<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000443">
<title confidence="0.993695">
Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement
Detector for an Uncertainty-Adaptive Spoken Dialogue System
</title>
<author confidence="0.922621">
Kate Forbes-Riley and Diane Litman and Heather Friedberg and Joanna Drummond*
</author>
<affiliation confidence="0.9030375">
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.997763">
forbesk@pitt.edu, litman@pitt.edu, haf13@pitt.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971043478261">
We present a model for detecting user dis-
engagement during spoken dialogue interac-
tions. Intrinsic evaluation of our model (i.e.,
with respect to a gold standard) yields results
on par with prior work. However, since our
goal is immediate implementation in a sys-
tem that already detects and adapts to user un-
certainty, we go further than prior work and
present an extrinsic evaluation of our model
(i.e., with respect to the real-world task). Cor-
relation analyses show crucially that our au-
tomatic disengagement labels correlate with
system performance in the same way as the
gold standard (manual) labels, while regres-
sion analyses show that detecting user disen-
gagement adds value over and above detecting
only user uncertainty when modeling perfor-
mance. Our results suggest that automatically
detecting and adapting to user disengagement
has the potential to significantly improve per-
formance even in the presence of noise, when
compared with only adapting to one affective
state or ignoring affect entirely.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9955925">
Spoken dialogue systems that can detect and adapt
to user affect1 are fast becoming reality (Schuller
et al., 2009b; Batliner et al., 2008; Prendinger and
Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee
</bodyText>
<note confidence="0.485336">
Now at Univ. Toronto: jdrummond@cs.toronto.edu
</note>
<footnote confidence="0.844984">
1We use affect for emotions and attitudes that affect how
users communicate. Other speech researchers also combine
concepts of emotion, arousal, and attitudes where emotion is
not full-blown (Cowie and Cornelius, 2003).
</footnote>
<page confidence="0.993486">
91
</page>
<bodyText confidence="0.999952411764706">
and Narayanan, 2005; Shafran et al., 2003). The
benefits are clear: affect-adaptive systems have been
shown to increase task success (Forbes-Riley and
Litman, 2011a; D’Mello et al., 2010; Wang et al.,
2008) or improve other system performance met-
rics such as user satisfaction (Liu and Picard, 2005;
Klein et al., 2002). However, to date most affec-
tive systems researchers have focused either only on
affect detection, or only on detecting and adapting
to a single affective state. The next step is thus to
develop and evaluate spoken dialogue systems that
detect and respond to multiple affective states.
We previously showed that detecting and re-
sponding to user uncertainty during spoken dialogue
computer tutoring significantly improves task suc-
cess (Forbes-Riley and Litman, 2011a). We are
now taking the next step: incorporating automatic
detection and adaptation to user disengagement as
well, with the goal of further improving task suc-
cess. We targeted user uncertainty and disengage-
ment because manual annotation showed them to be
the two most common user affective states in our
system and both are negatively correlated with task
success (Litman and Forbes-Riley, 2009; Forbes-
Riley and Litman, 2011b). Thus, we hypothesize
that providing appropriate responses to these states
would reduce their frequency, consequently improv-
ing task success. Although we address these user
states in the tutoring domain, spoken dialogue re-
searchers across domains and applications have in-
vestigated the automatic detection of both user un-
certainty (e.g. (Drummond and Litman, 2011; Pon-
Barry and Shieber, 2011; Paek and Ju, 2008; Alwan
et al., 2007)) and user disengagement (e.g., (Schuller
</bodyText>
<page confidence="0.465564">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999944842105263">
et al., 2010; Wang and Hirschberg, 2011; Schuller
et al., 2009a)), to improve system performance.
The detection of user disengagement in particular
has received substantial attention in recent years,
due to growing awareness of its potential for neg-
atively impacting commercial applications (Wang
and Hirschberg, 2011; Schuller et al., 2009a).
In this paper we present a model for automati-
cally detecting user disengagement during spoken
dialogue interactions. Intrinsic evaluation of our
model yields results on par with those of prior work.
However, we argue that while intrinsic evaluations
are necessary, they aren’t sufficient when immedi-
ate implementation is the goal, because there is no a
priori way to know when the model’s performance is
acceptable to use in a working system. This problem
is particularly relevant to affect detection because it
is such a difficult task, where no one achieves near-
perfect results. We argue that for such tasks some
extrinsic evaluation is also necessary, to show that
the automatic labels are useful and/or are a reason-
able substitute for a gold standard before undertak-
ing a labor-intensive and time-consuming evaluation
with real users. Here we use correlational analy-
ses to show that our automatic disengagement la-
bels are related to system performance in the same
way as the gold standard (manual) labels. We fur-
ther show through regression analyses that detecting
user disengagement adds value over and above de-
tecting only user uncertainty when modeling perfor-
mance. These results provide strong evidence that
enhancing a spoken dialogue system to detect and
adapt to multiple affective states (specifically, user
disengagement and uncertainty) has the potential to
significantly improve performance even in the pres-
ence of noise due to automatic detection, when com-
pared with only adapting to one affective state or ig-
noring affect entirely.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999935236363637">
Our focus in this paper is on first using machine
learning to develop a detector of user disengagement
for spoken dialogue systems, and then evaluating its
usefulness as fully as possible prior to its implemen-
tation and deployment with real users.
Disengaged users are highly undesirable in
human-computer interaction because they increase
the potential for user dissatisfaction and task fail-
ure; thus over the past decade there has already been
substantial prior work focused on detecting user dis-
engagement and the closely related states of bore-
dom, motivation and lack of interest (e.g., (Schuller
et al., 2010; Wang and Hirschberg, 2011; Jeon et
al., 2010; Schuller et al., 2009a; Bohus and Horvitz,
2009; Martalo et al., 2008; Porayska-Pomsta et al.,
2008; Kapoor and Picard, 2005; Sidner and Lee,
2003; Forbes-Riley and Litman, 2011b)).
Within this work, specific affect definitions vary
slightly with the intention of being coherent within
the application and domain and being relevant to the
specific adaptation goal (Martalo et al., 2008). How-
ever, affective systems researchers generally agree
that disengaged users show little involvement in the
interaction, and often display facial, gestural and lin-
guistic signals such as gaze avoidance, finger tap-
ping, humming, sarcasm, etcetera.
The features used to detect disengagement also
vary depending on system domain and applica-
tion. For example, Sidner &amp; Lee (2003) are in-
terested in modeling more natural and collabora-
tive human-robot interactions during basic conver-
sations. They define an algorithm for the engage-
ment process that involves appropriate eye gaze and
turn-taking. Martalo et al. (2008) study how user
engagement influences dialogue patterns during in-
teractions with an embodied agent that gives ad-
vice about healthy dieting. They model engage-
ment using manually coded dialogue acts based on
the SWBDL-DAMSL scheme (Stolcke et al., 2000).
Bohus and Horvitz (2009) study systems that attract
and engage users for dynamic, multi-party dialogues
in open-world settings. They model user intentions
to engage the system with cues from facial sensors
and the dialogue. Within recent spoken dialogue
research, acoustic-prosodic, lexical and contextual
features have been found to be effective detectors
of disengagement (Schuller et al., 2010; Wang and
Hirschberg, 2011; Jeon et al., 2010); we will briefly
compare our own results with these in Section 5.
While all of the above-mentioned research has
presented intrinsic evaluations of their disengage-
ment modeling efforts that indicate a reasonable de-
gree of accuracy as compared to a gold standard
(e.g., manual coding), only a few have yet demon-
strated that the model’s detected values are useful
</bodyText>
<page confidence="0.992435">
92
</page>
<bodyText confidence="0.99998435483871">
in practice and/or are a reasonable substitute for
the gold standard with respect to some practical
objective (e.g., a relationship to performance). In
particular, two studies (Bohus and Horvitz, 2009;
Schuller et al., 2009a) have gone directly from in-
trinsic evaluation of (dis)engagement models to per-
forming user studies with the implemented model,
thereby bypassing other less expensive and less
labor-intensive means of extrinsic evaluation to
quantify their model’s usefulness–and potentially in-
dicate its need to be further improved–before de-
ployment with real users. Neither study reports sta-
tistically significant improvements in system perfor-
mance as a result of detecting user (dis)engagement.
Finally, while substantial spoken dialogue and af-
fective systems research has shown that users dis-
play a range of affective states while interacting with
a system (e.g. (Schuller et al., 2009b; Conati and
Maclaren, 2009; Batliner et al., 2008; Devillers and
Vidrascu, 2006; Lee and Narayanan, 2005; Shafran
et al., 2003; Ang et al., 2002)), to date only a few af-
fective systems have been built that detect and adapt
to multiple user affective states (e.g., (D’Mello et al.,
2010; Aist et al., 2002; Tsukahara and Ward, 2001)),
and most of these have been deployed with cru-
cial natural language processing components “wiz-
arded” by a hidden human agent (e.g., who performs
speech recognition or affect annotation on the user
turns); moreover, none have yet shown significant
improvements in system performance as a result of
adapting to multiple user affective states.
</bodyText>
<sectionHeader confidence="0.997062" genericHeader="method">
3 ITSPOKE: Spoken Dialogue Tutor
</sectionHeader>
<bodyText confidence="0.998922363636364">
We develop and evaluate our disengagement detec-
tor using a corpus of spoken dialogues from a 2008
controlled experiment evaluating our uncertainty-
adaptive spoken dialogue tutoring system, IT-
SPOKE (Intelligent Tutoring SPOKEn dialog sys-
tem) (Forbes-Riley and Litman, 2011a).2
ITSPOKE tutors 5 Newtonian physics problems
(one per dialogue), using a Tutor Question - Stu-
dent Answer - Tutor Response format. After
each tutor question, the student speech is digi-
tized from head-mounted microphone input and sent
</bodyText>
<footnote confidence="0.977708666666667">
2ITSPOKE is a speech-enhanced and otherwise modified
version of the Why2-Atlas text-based qualitative physics tu-
tor (VanLehn et al., 2002).
</footnote>
<bodyText confidence="0.99996047826087">
to the Sphinx2 recognizer, which yields an auto-
matic transcript (Huang et al., 1993). This an-
swer’s (in)correctness is then automatically classi-
fied based on this transcript, using the TuTalk se-
mantic analyzer (Jordan et al., 2007), and the an-
swer’s (un)certainty is automatically classified by
inputting features of the speech signal, the automatic
transcript, and the dialogue context into a logistic
regression model. We will discuss these features
further in Section 5. All natural language process-
ing components were trained using prior ITSPOKE
corpora. The appropriate tutor response is deter-
mined based on the answer’s automatically labeled
(in)correctness and (un)certainty and then sent to the
Cepstral text-to-speech system3, whose audio output
is played through the student headphones and is also
displayed on a web-based interface.
The experimental procedure was as follows: col-
lege students with no college-level physics (1) read
a short physics text, (2) took a pretest, (3) worked
5 “training” problems with ITSPOKE, where each
user received a varying level of uncertainty adapta-
tion based on condition, (4) took a user satisfaction
survey, (5) took a posttest isomorphic to the pretest,
and (6) worked a “test” problem with ITSPOKE that
was isomorphic to the 5th training problem, where
no user received any uncertainty adaptation.
The resulting corpus contains 432 dialogues (6
per student) and 7216 turns from 72 students, 47
female and 25 male. All turns are used in the dis-
engagement detection experiments described next.
However, only the training problem dialogues (360,
5 per student, 6044 student turns) are used for the
performance analyses in Sections 6-7, because the
final test problem was given after the instruments
measuring performance (survey and posttest).
Our survey and tests are the same as those used in
multiple prior ITSPOKE experiments (c.f., (Forbes-
Riley and Litman, 2011a)). The pretest and posttest
each contain 26 multiple choice questions querying
knowledge of the topics covered in the dialogues.
Average pretest and posttest scores in the corpus
were 51.0% and 73.1% (out of 100%) with stan-
dard deviations of 14.5% and 13.8%, respectively.
The user satisfaction survey contains 16 statements
rated on a 5-point Likert scale. Average total sur-
</bodyText>
<footnote confidence="0.745744">
3an outgrowth of Festival (Black and Taylor, 1997).
</footnote>
<page confidence="0.996346">
93
</page>
<bodyText confidence="0.999820666666667">
vey score was 60.9 (out of 80), with a standard de-
viation of 8.5. While the statements themselves are
listed elsewhere (Forbes-Riley and Litman, 2009),
9 statements concern the tutoring domain (e.g., The
tutor was effective/precise/useful), 7 of which were
taken from (Baylor et al., 2003) and 2 of which
were created for our system. 3 statements concern
user uncertainty levels and were created for our sys-
tem. 4 statements concern the spoken dialogue in-
teraction (e.g., It was easy to understand the tutor’s
speech) and were taken from (Walker et al., 2002).
Our survey has also been incorporated into other re-
cent work exploring user satisfaction in spoken dia-
logue computer tutors (Dzikovska et al., 2011). In
Section 6 we discuss how user scores on these in-
struments are used to measure system performance.
See (Forbes-Riley and Litman, 2011a) for further
details of ITSPOKE and the 2008 experiment.
Following the experiment, the entire corpus
was manually labeled for (in)correctness (cor-
rect, incorrect), (un)certainty (CER, UNC) and
(dis)engagement (ENG, DISE) by one trained an-
notator. Table 1 shows the distribution of the la-
beled turns in the 2008 ITSPOKE corpus. In prior
ITSPOKE corpora, our annotator displayed interan-
notator agreement of 0.85 and 0.62 Kappa on cor-
rectness and uncertainty, respectively (Forbes-Riley
and Litman, 2011a). For the disengagement label,
a reliability analysis was performed over several an-
notation rounds on subsets of the 2008 ITSPOKE
corpus by this and a second trained annotator, yield-
ing 0.55 Kappa (this analysis is described in detail
elsewhere (Forbes-Riley et al., 2011)). Our Kap-
pas indicate that user uncertainty and disengage-
ment can both be annotated with moderate reliabil-
ity in our dataset, on par with prior emotion anno-
tation work (c.f., (Pon-Barry and Shieber, 2011)).
Note however that the best way to label users’ in-
ternal affective state(s) is still an open question.
Many system researchers (including ourselves) rely
on trained labelers (e.g., (Pon-Barry et al., 2006;
Porayska-Pomsta et al., 2008)) while others use self-
reports (e.g., (Conati and Maclaren, 2009; Gratch et
al., 2009; McQuiggan et al., 2008)). Both meth-
ods are problematic; for example both can be ren-
dered inaccurate when users mask their true feel-
ings. Two studies that have compared self-reports,
peer labelers, trained labelers, and combinations of
labelers (Afzal and Robinson, 2011; D’Mello et al.,
2008) both illustrate the common finding that hu-
man annotators display low to moderate interannota-
tor reliability for affect annotation, and both studies
show that trained labelers yield the highest reliabil-
ity on this task. Despite the lack of high interan-
notator reliability, responding to affect detected by
trained human labels has still been shown to improve
system performance (see Section 1).
</bodyText>
<tableCaption confidence="0.999181">
Table 1: 2008 ITSPOKE Corpus Description (N=7216)
</tableCaption>
<table confidence="0.9990322">
Turn Label Total Percent
Disengaged 1170 16.21%
Correct 5330 73.86%
Uncertain 1483 20.55%
Uncertain+Disengaged 373 5.17%
</table>
<sectionHeader confidence="0.815376" genericHeader="method">
4 Automatically Detecting User
Disengagement (DISE) in ITSPOKE
</sectionHeader>
<bodyText confidence="0.9996978">
As noted in Section 1, we have developed a user dis-
engagement detector to incorporate into our existing
uncertainty-adaptive spoken dialogue system. The
result will be a state of the art system that adapts to
multiple affective states during the dialogue.
</bodyText>
<subsectionHeader confidence="0.98271">
4.1 Binary DISE Label
</subsectionHeader>
<bodyText confidence="0.999797411764706">
Our disengagement annotation scheme (Forbes-
Riley et al., 2011) was derived from empirical ob-
servations in our data but draws on prior work,
including work mentioned in Section 2, appraisal
theory-based emotion models (e.g., Conati and Ma-
claren (2009))4, and prior approaches to annotating
disengagement or related states in tutoring (Lehman
et al., 2008; Porayska-Pomsta et al., 2008).
Briefly, our overall Disengagement label (DISE)
is used for turns expressing moderate to strong dis-
engagement towards the interaction, i.e., responses
given without much effort or without caring about
appropriateness. Responses might also be accompa-
nied by signs of inattention, boredom, or irritation.
Clear examples include answers spoken quickly in
leaden monotone, with sarcastic or playful tones,
or with off-task sounds such as rhythmic tapping or
</bodyText>
<footnote confidence="0.823096333333333">
4Appraisal theorists distinguish emotional behaviors from
their underlying causes, arguing that emotions result from an
evaluation of a context.
</footnote>
<page confidence="0.9989">
94
</page>
<bodyText confidence="0.857636923076923">
electronics usage.5 Note that our DISE label is de-
fined independently of the tutoring domain and thus
should generalize across spoken dialogue systems.
Figure 1 illustrates the DISE, (in)correctness, and
(un)certainty labels across 3 tutor/student turn pairs.
U1 is labeled DISE and UNC because the student
gave up immediately and with irritation when too
much prior knowledge was required. U2 is labeled
DISE and UNC because the student avoided giv-
ing a specific numerical value, offering instead a
vague (and obviously incorrect) answer. U3 is la-
beled DISE and CER because the student sang the
correct answer, indicating a lack of interest in the
larger purpose of the material being discussed.6
T1: What is the definition of Newton’s Second Law?
U1: I have no idea &lt;sigh&gt;. (DISE, incorrect, UNC)
. . .
T2: What’s the numerical value of the man’s accelera-
tion? Please specify the units too.
U2: The speed of the elevator. Meters per second. (DISE,
incorrect, UNC)
. . .
T3: What are the forces acting on the keys after the man
releases them?
U3: graaa-vi-tyyyyy &lt;sings the answer&gt; (DISE, cor-
rect, CER)
</bodyText>
<figureCaption confidence="0.993784">
Figure 1: Corpus Example Illustrating the User Turn La-
bels ((Dis)Engagement, (In)Correctness, (Un)Certainty)
</figureCaption>
<subsectionHeader confidence="0.871819">
4.2 DISE Detection Method
</subsectionHeader>
<bodyText confidence="0.9999728">
Machine learning classification was done at the turn
level using WEKA software7 and 10-fold cross val-
idation. A J48 decision tree was chosen because of
its easily read output and the fact that previous ex-
periments with our data showed little variance be-
</bodyText>
<footnote confidence="0.895671923076923">
5Affective systems research has found total disengagement
rare in laboratory settings (Lehman et al., 2008; Martalo et al.,
2008). As in that research, we equate the DISE label with no
or low engagement. Since total disengagement is common in
real-world unobserved human-computer interactions (deleting
unsatisfactory software being an extreme example) it remains
an open question as to how well laboratory findings generalize.
6Our original scheme distinguished six DISE subtypes
that trained annotators distinguished with a reliability of .43
Kappa (Forbes-Riley et al., 2011). However, pilot experiments
indicated that our models cannot accurately distinguish them,
thus our DISE detector focuses on the DISE label.
7http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<bodyText confidence="0.999893142857143">
tween different machine learning algorithms (Drum-
mond and Litman, 2011). We also use a cost matrix,
which heavily penalizes classifying a true DISE in-
stance as false, because our class distributions are
highly skewed (16.21% DISE turns) and the cost
matrix successfully mitigated the skew’s effect in
our prior work, where the uncertainty distribution is
also skewed (20.55% UNC turns) (Drummond and
Litman, 2011).
To train our DISE model, we first extracted the set
of speech and dialogue features shown in Figure 2
from the user turns in our corpus. As shown, the
acoustic-prosodic features represent duration, paus-
ing, pitch, and energy, and were normalized by the
first user turn, as well as totaled and averaged over
each dialogue. The lexical and dialogue features
consist of the current dialogue name (i.e., one of the
six physics problems) and turn number, the current
ITSPOKE question’s name (e.g.,T3 in Figure 1 has
a unique identifier) and depth in the discourse struc-
ture (e.g., an ITSPOKE remediation question after
an incorrect user answer would be at one greater
depth than the prior question), a word occurrence
vector for the automatically recognized text of the
user turn, an automatic (in)correctness label, and
lastly, the number of user turns since the last cor-
rect turn (“incorrect runs”). We also included two
user-based features, gender and pretest score.
</bodyText>
<listItem confidence="0.723529">
• Acoustic-Prosodic Features
temporal features: turn duration, prior pause dura-
</listItem>
<bodyText confidence="0.82352775">
tion, turn-internal silence
fundamental frequency (f0) and energy (RMS) fea-
tures: maximum, minimum, mean, std. deviation
running totals and averages for all features
</bodyText>
<listItem confidence="0.998812375">
• Lexical and Dialogue Features
dialogue name and turn number
question name and question depth
ITSPOKE-recognized lexical items in turn
ITSPOKE-labeled turn (in)correctness
incorrect runs
• User Identifier Features:
gender and pretest score
</listItem>
<figureCaption confidence="0.9466995">
Figure 2: Features Used to Detect Disengagement (DISE)
for each User Turn
</figureCaption>
<page confidence="0.995488">
95
</page>
<tableCaption confidence="0.9557245">
Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE
Label in the 2008 ITSPOKE Corpus (N=7216 user turns)
</tableCaption>
<table confidence="0.999851666666667">
Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE
Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25
Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27
</table>
<bodyText confidence="0.999709565217391">
Note that although our feature set was drawn pri-
marily from our prior uncertainty detection exper-
iments (Forbes-Riley and Litman, 2011a; Drum-
mond and Litman, 2011), we have also experi-
mented with other features, including state-of-the-
art acoustic-prosodic features used in the last Inter-
speech Challenges (Schuller et al., 2010; Schuller et
al., 2009b) and made freely available in the openS-
MILE Toolkit (Florian et al., 2010). To date, how-
ever, these features have only decreased the cross-
validation performance of our models.8 While some
of our features are tutoring-specific, these have sim-
ilar counterparts in other applications (i.e., answer
(in)correctness corresponds to a more general no-
tion of “response appropriateness” in other domains,
while pretest score corresponds to the general no-
tion of domain expertise). Moreover, all of our fea-
tures are fully automatic and available in real-time,
so that the model can be directly implemented and
deployed. To that end, we now describe the results
of our intrinsic and extrinsic evaluations of our DISE
model, aimed at determining whether it is ready to
be evaluated with real users.
</bodyText>
<sectionHeader confidence="0.99829" genericHeader="method">
5 Intrinsic Evaluation: Cross-Validation
</sectionHeader>
<bodyText confidence="0.999937583333333">
Table 2 shows the averaged results of the cross-
validation with the J48 decision tree algorithm. In
addition to accuracy, we use Unweighted Aver-
age (UA) Precision9, Recall, and F-measure be-
cause they are the standard measures used to eval-
uate current affect recognition technology, particu-
larly for unbalanced two-class problems (Schuller
et al., 2009b). In addition, we use the cross corre-
lation (CC) measure and mean linear error (MLE)
because these metrics were used in recent work for
evaluating disengagement (level of interest) detec-
tors for the Interspeech 2010 challenge (Schuller et
</bodyText>
<footnote confidence="0.998428">
8We also tried using our automatic UNC label as a feature in
our DISE model, but our results weren’t significantly improved.
9simply ((Precision(DISE) + Precision(ENG))/2)
</footnote>
<bodyText confidence="0.9947412">
al., 2010; Wang and Hirschberg, 2011; Jeon et al.,
2010)).10 Note however that the Interspeech 2010
task differs from ours not only in the corpus and fea-
tures, but also in the learning task: they used regres-
sion to detect a continuous level of interest ranging
from 0 to 1, while we detect a binary class. Thus
comparison between our results and those are only
suggestive rather than conclusive.
As shown in Table 2, we also compare our results
with those of majority class (ENG) labeling of the
same turns. Since (7216-1170)/7216 user turns in
the corpus are engaged (recall Table 1), always se-
lecting the majority class (ENG) label for these turns
thus yields 83.8% accuracy (with 0% precision and
recall for DISE, and 83.8% precision and 100% re-
call for ENG). While our DISE model does not out-
perform majority class labeling with respect to ac-
curacy, this is not surprising given the steep skew
in class distribution, and our learned model signif-
icantly outperforms the baseline with respect to all
the other measures (p&lt;.001).11
Our CC and MLE results are on par with the best
results from the state-of-the-art systems competing
in the 2010 Interspeech Challenge, where the task
was to detect level of interest. In particular, the win-
ner obtained a CC of 0.428 (higher numbers are bet-
ter) and an MLE of 0.146 (lower numbers are bet-
ter) (Jeon et al., 2010), while a subsequent study
yielded a CC of 0.480 and an MLE of 0.131 on
the same corpus (Wang and Hirschberg, 2011). Our
results are also on par with the best results of the
other prior research on detecting disengagement dis-
cussed in Section 2 that detects a small number of
disengagement classes and reports accuracy and/or
recall and precision. For example, (Martalo et al.,
2008) report average precision of 75% and recall
10Pearson product-moment correlation coefficient (CC) is a
measure of the linear dependence that is widely used in regres-
sion settings. MLE is a regression performance measure for the
mean absolute error between an estimator and the true value.
</bodyText>
<footnote confidence="0.972803">
11CC is undefined for majority class labeling.
</footnote>
<page confidence="0.997398">
96
</page>
<bodyText confidence="0.999967454545455">
of 74% (detecting three levels of disengagement),
while (Kapoor and Picard, 2005) report an accuracy
of 86% for detecting binary (dis)interest.
Our final DISE model was produced by running
the J48 algorithm over our entire corpus. The re-
sulting decision tree contains 141 nodes and 75
leaves. Inspection of the tree reveals that all of the
feature types in Figure 2 (acoustic-prosodic, lexi-
cal/dialogue, user identifier) are used as decision
nodes in the tree, although not all variations on these
types were used. The upper-level nodes of the tree
are usually considered to be more informative fea-
tures as compared to lower-level nodes, since they
are queried for more leaves. The upper level of
the DISE model consists entirely of temporal, lex-
ical, pitch and energy features as well as question
name and depth and incorrect runs, while features
such as gender, turn number, and dialogue name
appear only near the leaves, and pretest score and
turn (in)correctness don’t appear at all. The amount
of pausing prior to the start of the user turn is the
most important feature for determining disengage-
ment, with pauses shorter than a quarter second be-
ing labeled DISE, suggesting that fast answers are a
strong signal of disengagement in our system. Users
who answer quickly may do so without taking the
time to think it through; the more engaged user, in
contrast, takes more time to prepare an answer.
Three lexical items from the student turns, “fric-
tion”, “light”, and “greater”, are the next most im-
portant features in the tree, suggesting that particular
concepts and question types can be typically associ-
ated with user disengagement in a system. For ex-
ample, open-ended system questions may lead users
to disengage due to frustration from not knowing
when their answer is complete. One common case
in ITSPOKE involves asking users to name all the
forces on an object; some users don’t know how
many to list, so they start listing random forces, such
as “friction.” On the other hand, multiple choice
questions can also lead users to disengage; they be-
gin with a reasonable chance of being correct and
thus don’t take the time to think through their an-
swer. One common case in ITSPOKE involves ask-
ing users to determine which of two objects has the
greater or lesser force, acceleration, and velocity.
While our feature set is highly generalizable to
other domains, it is an empirical question as to
whether the feature values we found maximally ef-
fective for predicting disengagement also general-
ize to other domains. Intuition is often unreliable,
and it has been widely shown in affect prediction
that the answer can depend on domain, dataset, and
learning algorithm employed. Moreover, there are
many types of spoken dialogue systems with dif-
ferent styles and no single type can represent the
entire field. That said, it is also important to note
that there are lessons to be learned from the features
selected for one particular domain, in terms of the
take-home message for other domains. For example,
the fact that ”prior pause” is selected as a strong sig-
nal of disengagement in ITSPOKE dialogues may
indicate that the feature itself (regardless of its se-
lected value) could be transferred to different do-
mains, alone or in the demonstrated combinations
with the other selected features.
</bodyText>
<sectionHeader confidence="0.997404" genericHeader="method">
6 Extrinsic Evaluation: Correlation
</sectionHeader>
<bodyText confidence="0.997139851851852">
Next we use extrinsic evaluation to confirm that our
final DISE model is both useful and a reasonable
substitute for our gold standard manual DISE la-
bels. With respect to showing the utility of detecting
DISE, we use a correlational analysis to show that
the gold standard (manual) DISE values are signif-
icantly predictive of two different measures of sys-
tem performance.12 With respect to showing the ad-
equacy of our current level of detection performance
for the learned DISE model, we demonstrate that af-
ter replacing the manual DISE labels with the au-
tomatic DISE labels when running our correlations,
the automatic labels are related to performance in
the same way as the gold standard labels.
Thus for both our automatically detected DISE la-
bels (auto) and our gold standard DISE labels (man-
ual), we first computed the total number of occur-
rences for each student, and then computed a bivari-
ate Pearson’s correlation between this total and two
different metrics of performance: learning gain (LG)
and user satisfaction (US). In the tutoring domain,
learning is the primary performance metric and as is
common in this domain we compute it as normal-
ized learning gain ((posttest score-pretest score)/(1-
12Spoken dialogue research has shown that redesigning a sys-
tem in light of such correlational analysis can indeed yield per-
formance improvements (Rotaru and Litman, 2009).
</bodyText>
<page confidence="0.999859">
97
</page>
<tableCaption confidence="0.999902">
Table 3: Correlations between Disengagement and both Satisfaction and Learning in ITSPOKE Corpus (N=72 users)
</tableCaption>
<table confidence="0.9996015">
Measure Mean (SD) User Satisfaction Learning Gain
R p R p
Total Manual DISE 12.3 (7.3) -0.25 0.031 -0.35 0.002
Total Automatic DISE 12.6 (7.4) -0.26 0.029 -0.31 0.009
</table>
<bodyText confidence="0.994501823529412">
pretest score)). In spoken dialogue systems, user sat-
isfaction is the primary performance metric and as
is common in this domain we compute it by totaling
over the user satisfaction survey scores.13
Table 3 shows first the mean and standard devia-
tion for the DISE label over all students, the Pear-
son’s Correlation coefficient (R) and its significance
(p). As shown, both our manual and automatic DISE
labels are significantly related to performance, re-
gardless of whether we measure it as user satisfac-
tion or learning gain.14 Moreover, in both cases the
correlations are nearly identical between the man-
ual and automatic labels. These results indicate that
the detected DISE values are a useful substitute for
the gold standard, and suggest that redesigning IT-
SPOKE to recognize and respond to DISE can sig-
nificantly improve system performance.
</bodyText>
<sectionHeader confidence="0.743011" genericHeader="method">
7 Extrinsic Evaluation: Affective State
Multiple Regression
</sectionHeader>
<bodyText confidence="0.998316193548387">
Because we are adding our disengagement detector
to a spoken dialogue system that already detects and
adapts to user uncertainty, we argue that it is also
necessary to evaluate whether greater performance
benefits are likely to be obtained by adapting to a
second state. In other words, given how difficult it is
to effectively detect and adapt to one user affective
state, is performance likely to improve by detecting
and adapting to multiple affective states?
To answer this question, we performed a multi-
ple linear regression analysis aimed at quantifying
the relative usefulness of the automatically detected
13Identical results were obtained by using an average instead
of a total, and only slightly weaker results were obtained when
normalizing the DISE totals as the percentages of total turns.
14We previously found a related correlation between different
DISE and learning measures, during the analysis of our DISE
annotation scheme (Forbes-Riley and Litman, 2011b). In par-
ticular, we showed a significant partial correlation between the
percentage of manual DISE labels and posttest controlled for
pretest score.
disengagement and uncertainty labels when predict-
ing our system performance metrics. We ran four
stepwise linear regressions. The first regression pre-
dicted learning gain, and gave the model two possi-
ble inputs: the total number of automatic DISE la-
bels and UNC labels per user. We then ran the same
regression again, this time predicting user satisfac-
tion. For comparison, we ran the same two regres-
sions using the manual DISE and UNC labels.
As the trained regression models in Figure 3 show,
when predicting learning gain, selecting both auto-
matically detected affective state metrics as inputs
significantly increases the model’s predictive power
as compared to only selecting one.15 The (stan-
dardized) feature weights indicate relative predic-
tive power in accounting for the variance in learn-
ing gain. As shown, both automatic affect metrics
have the same weight in the final model. This re-
sult suggests that adapting to our automatically de-
tected disengagement and uncertainty labels can fur-
ther improve learning over and above adapting to un-
certainty alone. Although the final model’s predic-
tive power is low (R2=0.15), our interest here is only
in investigating whether the two affective states are
more useful in combination than in isolation for pre-
dicting performance. In similar types of stepwise re-
gressions on prior ITSPOKE corpora, we’ve shown
that more complete models of system performance
incorporating many predictors of learning (i.e. af-
fective states in conjunction with other dialogue fea-
tures) can yield R2 values of over .5 (Forbes-Riley
et al., 2008).16
15Using the stepwise method, Automatic DISE was the first
feature selected, and Automatic UNC the second. However,
note that a model consisting of only the Automatic UNC metric
also yields significantly worse predictive power than selecting
both affective state metrics. Further note that almost identical
models were produced using percentages rather than totals.
16R2 is the standard reported metric for linear regressions.
However, for consistency with Table 3, note that the two models
in Figure 3 yield R values of -.31 and -.38, respectively.
</bodyText>
<page confidence="0.98935">
98
</page>
<figure confidence="0.777202">
Learning Gain= -.31 *Total Automatic DISE (R2=.09, p=.009)
Learning Gain= -.24 *Total Automatic DISE - .24 * Total Automatic UNC (R2=.15, p=.004)
</figure>
<figureCaption confidence="0.993649">
Figure 3: Performance Model’s Predictive Power Increases Significantly with Multiple Affective Features
</figureCaption>
<bodyText confidence="0.999959684210526">
Interestingly, for the regression models of learn-
ing gain that used manual affect metrics, only the
DISE metric was selected as an input. This indi-
cates that the automatic affective state labels are use-
ful in combination for predicting performance in a
way that is not reflected in their gold standard coun-
terparts. Detecting multiple affective states might
thus be one way to compensate for the noise that is
introduced in a fully-automated affective spoken di-
alogue system.
Similarly, only the DISE metric was selected
for inclusion in the regression model of user sat-
isfaction, regardless of whether manual or auto-
matic labels were used. A separate correlation
analysis showed that user uncertainty is not sig-
nificantly correlated with user satisfaction in our
system, though we previously found that multiple
uncertainty-related metrics do significantly correlate
with learning (Litman and Forbes-Riley, 2009).
</bodyText>
<sectionHeader confidence="0.926146" genericHeader="conclusions">
8 Summary and Current Directions
</sectionHeader>
<bodyText confidence="0.999701428571429">
In this paper we used extrinsic evaluations to pro-
vide evidence for the utility of a new system de-
sign involving the complex task of user affect de-
tection, prior to undertaking an expensive and time-
consuming evaluation of an affect-adaptive system
with real users. In particular, we first presented a
novel model for automatically detecting user disen-
gagement in spoken dialogue systems. We showed
through intrinsic evaluations (i.e., cross-validation
experiments using gold-standard labels) that the
model yields results on par with prior work. We
then showed crucially through novel extrinsic eval-
uation that the resulting automatically detected dis-
engagement labels correlate with two primary per-
formance metrics (user satisfaction and learning) in
the same way as gold standard (manual) labels. This
suggests that adapting to the automatic disengage-
ment labels has the potential to significantly improve
performance even in the presence of noise from the
automatic labeling. Finally, further extrinsic anal-
yses using multiple regression suggest that adapt-
ing to our automatic disengagement labels can im-
prove learning (though not user satisfaction) over
and above the improvement achieved by only adapt-
ing to automatically detected user uncertainty.
We have already developed and implemented an
adaptation for user disengagement in ITSPOKE.
The disengagement adaptation draws on empiri-
cal analyses of our data and effective responses
to user disengagement presented in prior work
(c.f., (Forbes-Riley and Litman, 2011b)), We are
currently evaluating our disengagement adaptation
in the “ideal” environment of a Wizard of Oz exper-
iment, where user disengagement, uncertainty, and
correctness are labeled by a hidden human during
user interactions with ITSPOKE.
Based on the evaluations here, we believe our dis-
engagement model is ready for implementation in
ITSPOKE. We will then evaluate the resulting spo-
ken dialogue system for detecting and adapting to
multiple affective states in an upcoming controlled
experiment with real users.
</bodyText>
<sectionHeader confidence="0.997874" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8714905">
This work is funded by NSF award 0914615. We
thank Scott Silliman for systems support.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997395666666667">
S. Afzal and P. Robinson. 2011. Natural affect data:
Collection and annotation. In Sidney D’Mello and
Rafael Calvo, editors, Affect and Learning Technolo-
gies. Springer.
G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Pi-
card. 2002. Experimentally augmenting an intelli-
gent tutoring system with human-supplied capabili-
ties: Adding human-provided emotional scaffolding to
an automated reading tutor that listens. In Proc. In-
telligent Tutoring Systems Conference (ITS) Workshop
on Empirical Methods for Tutorial Dialogue Systems,
pages 16–28, San Sebastian, Spain.
A. Alwan, Y. Bai, M. Black, L. Caseyz, M. Gerosa,
M. Heritagez, M. Iseliy, M. Jonesz, A. Kazemzadeh,
S. Lee, S. Narayanan, P. Pricex, J. Tepperman, and
</reference>
<page confidence="0.992289">
99
</page>
<reference confidence="0.99383153271028">
S. Wangy. 2007. A system for technology based as-
sessment of language and literacy in young children:
the role of multiple information sources. In Proceed-
ings of the 9th IEEE International Workshop on Multi-
media Signal Processing (MMSP), pages 26–30, Cha-
nia, Greece, October.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
J. H. L. Hansen and B. Pellom, editors, Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), pages 2037–2039, Denver, USA.
A. Batliner, S. Steidl, C. Hacker, and E. Noth. 2008.
Private emotions vs. social interaction - a data-driven
approach towards analysing emotion in speech. User
Modeling and User-Adapted Interaction: The Journal
of Personalization Research, 18:175–206.
A. L. Baylor, J. Ryu, and E. Shen. 2003. The effect
of pedagogical agent voice and animation on learning,
motivation, and perceived persona. In Proceedings of
the ED-MEDIA Conference, Honolulu, Hawaii, June.
A. Black and P. Taylor. 1997. Festival speech synthe-
sis system: system documentation (1.1.1). The Centre
for Speech Technology Research, University of Edin-
burgh, http://www.cstr.ed.ac.uk/projects/festival/.
D. Bohus and E. Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proceedings of
SIGdial, London, UK.
C. Conati and H. Maclaren. 2009. Empirically build-
ing and evaluating a probabilistic model of user af-
fect. User Modeling and User-Adapted Interaction,
19(3):267–303.
R. Cowie and R. R. Cornelius. 2003. Describing the
emotional states that are expressed in speech. Speech
Communication, 40(1-2):5–32.
L. Devillers and L. Vidrascu. 2006. Real-life emo-
tions detection with lexical and paralinguistic cues on
human-human call center dialogs. In Ninth Inter-
national Conference on Spoken Language Processing
(ICSLP, pages 801–804, Pittsburgh, PA, September.
S. D’Mello, S. Craig, A. Witherspoon, B. McDaniel, and
A. Graesser. 2008. Automatic detection of learner’s
affect from conversational cues. User Modeling and
User-Adapted Interaction: The Journal of Personal-
ization Research, 18:45–80.
S. D’Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs,
K. Vogt, L. Perkins, and A. Graesser. 2010. A time
for emoting: When affect-sensitivity is and isn’t effec-
tive at promoting deep learning. In Intelligent Tutoring
Systems Conference, pages 245–254, Pittsburgh, PA,
USA, June.
J. Drummond and D. Litman. 2011. Examining the im-
pacts of dialogue content and system automation on
affect models in a spoken tutorial dialogue system.
In Proc. 12th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), pages
312–318, Portland, Oregon, June.
M. Dzikovska, J. Moore, N. Steinhauser, and G. Camp-
bell. 2011. Exploring user satisfaction in a tutorial
dialogue system. In Proc. 12th Annual Meeting of
the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 162–172, Portland, Oregon, June.
E. Florian, M. Wollmer, and B. Schuller. 2010. The Mu-
nich versatile and fast open-source audio feature ex-
tractor. In Proc. ACM Multimedia (MM), pages 1459–
1462, Florence, Italy.
K. Forbes-Riley and D. Litman. 2009. A user modeling-
based performance analysis of a wizarded uncertainty-
adaptive dialogue system corpus. In Proc. Inter-
speech, Brighton, UK, September.
K. Forbes-Riley and D. Litman. 2011a. Benefits and
challenges of real-time uncertainty detection and adap-
tation in a spoken dialogue computer tutor. Speech
Communication, 53(9–10):1115–1136.
K. Forbes-Riley and D. Litman. 2011b. When does
disengagement correlate with learning in spoken dia-
log computer tutoring? In Proceedings 15th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Auckland, NZ, June.
K. Forbes-Riley, M. Rotaru, and D. Litman. 2008. The
relative impact of student affect on performance mod-
els in a spoken dialogue tutoring system. User Model-
ing and User-Adapted Interaction, 18(1-2):11–43.
K. Forbes-Riley, D. Litman, and H. Friedberg. 2011. An-
notating disengagement for spoken dialogue computer
tutoring. In Sidney D’Mello and Rafael Calvo, editors,
Affect and Learning Technologies. Springer.
Jonathan Gratch, Stacy Marsella, Ning Wang, and
Brooke Stankovic. 2009. Assessing the validity of
appraisal-based models of emotion. In Proceedings of
ACII, Amsterdam, Netherlands.
X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.
Lee, and R. Rosenfeld. 1993. The SphinxII speech
recognition system: An Overview. Computer, Speech
and Language.
J. H. Jeon, R. Xia, and Y. Liu. 2010. Level of interest
sensing in spoken dialog using multi-level fusion of
acoustic and lexical evidence. In INTERSPEECH’10,
pages 2802–2805.
P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P. Rose.
2007. Tools for authoring a dialogue agent that par-
ticipates in learning studies. In Proc. Artificial Intelli-
gence in Education (AIED), pages 43–50.
A. Kapoor and R. W. Picard. 2005. Multimodal affect
recognition in learning environments. In 13th Annual
ACM International Conference on Multimedia, pages
677–682, Singapore.
</reference>
<page confidence="0.870596">
100
</page>
<reference confidence="0.999630859813084">
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-
sults. Interacting with Computers, 14:119–140.
C. M. Lee and S. Narayanan. 2005. Towards detect-
ing emotions in spoken dialogs. IEEE Transactions
on Speech and Audio Processing, 13(2), March.
B. Lehman, M. Matthews, S. D’Mello, and N. Per-
son. 2008. What are you feeling? Investigating
student affective states during expert human tutoring
sessions. In Intelligent Tutoring Systems Conference
(ITS), pages 50–59, Montreal, Canada, June.
D. Litman and K. Forbes-Riley. 2009. Spoken tutorial
dialogue and the feeling of another’s knowing. In Pro-
ceedings 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), Lon-
don, UK, September.
K. Liu and R. W. Picard. 2005. Embedded empathy
in continuous, interactive health assessment. In CHI
Workshop on HCI Challenges in Health Assessment.
A. Martalo, N. Novielli, and F. de Rosis. 2008. Attitude
display in dialogue patterns. In Proc. AISB 2008 Sym-
posium on Affective Language in Human and Machine,
pages 1–8, Aberdeen, Scotland, April.
S. McQuiggan, B. Mott, and J. Lester. 2008. Model-
ing self-efficacy in intelligent tutoring systems: An in-
ductive approach. User Modeling and User-Adapted
Interaction (UMUAI), 18(1-2):81–123, February.
T. Paek and Y.-C. Ju. 2008. Accommodating explicit
user expressions of uncertainty in voice search or
something like that. In Proceedings of the 9th Annual
Conference of the International Speech Communica-
tion Association (INTERSPEECH 08), pages 1165–
1168, Brisbane, Australia, September.
H. Pon-Barry and S. Shieber. 2011. Recognizing uncer-
tainty in speech. EURASIP Journal on Advances in
Signal Processing.
H. Pon-Barry, K. Schultz, E. Owen Bratt, B. Clark, and
S. Peters. 2006. Responding to student uncertainty in
spoken tutorial dialogue systems. International Jour-
nal ofArtificial Intelligence in Education, 16:171–194.
K. Porayska-Pomsta, M. Mavrikis, and H. Pain. 2008.
Diagnosing and acting on student affect: the tutor’s
perspective. User Modeling and User-Adapted In-
teraction: The Journal of Personalization Research,
18:125–173.
H. Prendinger and M. Ishizuka. 2005. The Empa-
thetic Companion: A character-based interface that ad-
dresses users’ affective states. International Journal of
Applied Artificial Intelligence, 19(3):267–285.
M. Rotaru and D. Litman. 2009. Discourse structure and
performance analysis: Beyond the correlation. In Pro-
ceedings 10th Annual Meeting of the Special Interest
Group on Discourse and Dialogue (SIGDIAL), Lon-
don, UK.
B. Schuller, R. Muller, F. Eyben, J. Gast, B. Hrnler,
M. Wollmer, G. Rigoll, A. Hthker, and H. Konosu.
2009a. Being bored? recognising natural interest by
extensive audiovisual integration for real-life applica-
tion. Image and Vision Computing Journal, Special
Issue on Visual and Multimodal Analysis of Human
Spontaneous Behavior, 27:1760–1774.
B. Schuller, S. Steidl, and A. Batliner. 2009b. The
Interspeech 2009 Emotion Challenge. In Proceed-
ings of the 10th Annual Conference of the Inter-
national Speech Communication Association (Inter-
speech), ISCA, Brighton, UK, September.
B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev-
illers, C. Muller, and S. Narayanan. 2010. The
Interspeech 2010 Paralinguistic Challenge. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Assocation (Inter-
speech), pages 2794–2797, Chiba, Japan, September.
I. Shafran, M. Riley, and M. Mohri. 2003. Voice signa-
tures. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 31–36, St. Thomas, US Virgin Islands.
C. Sidner and C. Lee. 2003. An architecture for engage-
ment in collaborative conversations between a robot
and a human. Technical Report TR2003-12, MERL.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3).
W. Tsukahara and N. Ward. 2001. Responding to subtle,
fleeting changes in the user’s internal state. In Pro-
ceedings of the SIG-CHI on Human factors in comput-
ing systems, pages 77–84, Seattle, WA. ACM.
K. VanLehn, P. W. Jordan, C. Ros´e, D. Bhembe,
M. B¨ottner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
L. Vidrascu and L. Devillers. 2005. Detection of real-
life emotions in dialogs recorded in a call center. In
Proceedings of INTERSPEECH, Lisbon, Portugal.
M. Walker, A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt,
J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potami-
anos, R. Passonneau, S. Roukos, G. Sanders, S. Sen-
eff, and D. Stallard. 2002. DARPA communicator:
Cross-system results for the 2001 evaluation. In Proc.
ICSLP.
W. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fusion
</reference>
<page confidence="0.97603">
101
</page>
<reference confidence="0.997562142857143">
learning. In Proc. 12th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue (SIG-
DIAL), pages 152–161, Portland, Oregon, June.
N. Wang, W.L. Johnson, R. E. Mayer, P. Rizzo, E. Shaw,
and H. Collins. 2008. The politeness effect: Peda-
gogical agents and learning outcomes. International
Journal of Human-Computer Studies, 66(2):98–112.
</reference>
<page confidence="0.998623">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965565">
<title confidence="0.999529">Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-Adaptive Spoken Dialogue System</title>
<author confidence="0.999481">Forbes-Riley Litman Friedberg</author>
<affiliation confidence="0.999769">University of</affiliation>
<address confidence="0.988796">Pittsburgh, PA 15260,</address>
<email confidence="0.992951">forbesk@pitt.edu,litman@pitt.edu,haf13@pitt.edu</email>
<abstract confidence="0.999315083333333">We present a model for detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model (i.e., with respect to a gold standard) yields results on par with prior work. However, since our goal is immediate implementation in a system that already detects and adapts to user uncertainty, we go further than prior work and present an extrinsic evaluation of our model (i.e., with respect to the real-world task). Correlation analyses show crucially that our automatic disengagement labels correlate with system performance in the same way as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Afzal</author>
<author>P Robinson</author>
</authors>
<title>Natural affect data: Collection and annotation.</title>
<date>2011</date>
<booktitle>In Sidney D’Mello and Rafael Calvo, editors, Affect and Learning Technologies.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="15376" citStr="Afzal and Robinson, 2011" startWordPosition="2365" endWordPosition="2368">, (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mello et al., 2008) both illustrate the common finding that human annotators display low to moderate interannotator reliability for affect annotation, and both studies show that trained labelers yield the highest reliability on this task. Despite the lack of high interannotator reliability, responding to affect detected by trained human labels has still been shown to improve system performance (see Section 1). Table 1: 2008 ITSPOKE Corpus Description (N=7216) Turn Label Total Percent Disengaged 1170 16.21% Correct 5330 73.86% Uncertain 1483 20.55% Uncertain+Disengaged 373 5.17% 4 Automatic</context>
</contexts>
<marker>Afzal, Robinson, 2011</marker>
<rawString>S. Afzal and P. Robinson. 2011. Natural affect data: Collection and annotation. In Sidney D’Mello and Rafael Calvo, editors, Affect and Learning Technologies. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>B Kort</author>
<author>R Reilly</author>
<author>J Mostow</author>
<author>R Picard</author>
</authors>
<title>Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding human-provided emotional scaffolding to an automated reading tutor that listens.</title>
<date>2002</date>
<booktitle>In Proc. Intelligent Tutoring Systems Conference (ITS) Workshop on Empirical Methods for Tutorial Dialogue Systems,</booktitle>
<pages>16--28</pages>
<location>San Sebastian,</location>
<contexts>
<context position="9561" citStr="Aist et al., 2002" startWordPosition="1458" endWordPosition="1461">udy reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tutor We develop and evaluate our disengagement detector using a corpus of spoken dialogues from a 2008 controlled experiment evaluating our uncertaintyadaptive spoken dialogue tutoring system, ITSPOKE (</context>
</contexts>
<marker>Aist, Kort, Reilly, Mostow, Picard, 2002</marker>
<rawString>G. Aist, B. Kort, R. Reilly, J. Mostow, and R. Picard. 2002. Experimentally augmenting an intelligent tutoring system with human-supplied capabilities: Adding human-provided emotional scaffolding to an automated reading tutor that listens. In Proc. Intelligent Tutoring Systems Conference (ITS) Workshop on Empirical Methods for Tutorial Dialogue Systems, pages 16–28, San Sebastian, Spain.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Alwan</author>
<author>Y Bai</author>
<author>M Black</author>
<author>L Caseyz</author>
<author>M Gerosa</author>
<author>M Heritagez</author>
<author>M Iseliy</author>
<author>M Jonesz</author>
<author>A Kazemzadeh</author>
<author>S Lee</author>
<author>S Narayanan</author>
<author>P Pricex</author>
<author>J Tepperman</author>
<author>S Wangy</author>
</authors>
<title>A system for technology based assessment of language and literacy in young children: the role of multiple information sources.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th IEEE International Workshop on Multimedia Signal Processing (MMSP),</booktitle>
<pages>26--30</pages>
<location>Chania, Greece,</location>
<contexts>
<context position="3464" citStr="Alwan et al., 2007" startWordPosition="521" endWordPosition="524">owed them to be the two most common user affective states in our system and both are negatively correlated with task success (Litman and Forbes-Riley, 2009; ForbesRiley and Litman, 2011b). Thus, we hypothesize that providing appropriate responses to these states would reduce their frequency, consequently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting commercial applications (Wang and Hirschberg, 2011; Schuller et al.,</context>
</contexts>
<marker>Alwan, Bai, Black, Caseyz, Gerosa, Heritagez, Iseliy, Jonesz, Kazemzadeh, Lee, Narayanan, Pricex, Tepperman, Wangy, 2007</marker>
<rawString>A. Alwan, Y. Bai, M. Black, L. Caseyz, M. Gerosa, M. Heritagez, M. Iseliy, M. Jonesz, A. Kazemzadeh, S. Lee, S. Narayanan, P. Pricex, J. Tepperman, and S. Wangy. 2007. A system for technology based assessment of language and literacy in young children: the role of multiple information sources. In Proceedings of the 9th IEEE International Workshop on Multimedia Signal Processing (MMSP), pages 26–30, Chania, Greece, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog.</title>
<date>2002</date>
<booktitle>Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>2037--2039</pages>
<editor>In J. H. L. Hansen and B. Pellom, editors,</editor>
<location>Denver, USA.</location>
<contexts>
<context position="9402" citStr="Ang et al., 2002" startWordPosition="1429" endWordPosition="1432">xtrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tutor We develop and evaluate our disengagem</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In J. H. L. Hansen and B. Pellom, editors, Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 2037–2039, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>S Steidl</author>
<author>C Hacker</author>
<author>E Noth</author>
</authors>
<title>Private emotions vs. social interaction - a data-driven approach towards analysing emotion in speech. User Modeling and User-Adapted Interaction:</title>
<date>2008</date>
<journal>The Journal of Personalization Research,</journal>
<pages>18--175</pages>
<contexts>
<context position="1483" citStr="Batliner et al., 2008" startWordPosition="219" endWordPosition="222">formance in the same way as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user</context>
<context position="9306" citStr="Batliner et al., 2008" startWordPosition="1413" endWordPosition="1416">th the implemented model, thereby bypassing other less expensive and less labor-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multipl</context>
</contexts>
<marker>Batliner, Steidl, Hacker, Noth, 2008</marker>
<rawString>A. Batliner, S. Steidl, C. Hacker, and E. Noth. 2008. Private emotions vs. social interaction - a data-driven approach towards analysing emotion in speech. User Modeling and User-Adapted Interaction: The Journal of Personalization Research, 18:175–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Baylor</author>
<author>J Ryu</author>
<author>E Shen</author>
</authors>
<title>The effect of pedagogical agent voice and animation on learning, motivation, and perceived persona.</title>
<date>2003</date>
<booktitle>In Proceedings of the ED-MEDIA Conference,</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="13245" citStr="Baylor et al., 2003" startWordPosition="2027" endWordPosition="2030">ics covered in the dialogues. Average pretest and posttest scores in the corpus were 51.0% and 73.1% (out of 100%) with standard deviations of 14.5% and 13.8%, respectively. The user satisfaction survey contains 16 statements rated on a 5-point Likert scale. Average total sur3an outgrowth of Festival (Black and Taylor, 1997). 93 vey score was 60.9 (out of 80), with a standard deviation of 8.5. While the statements themselves are listed elsewhere (Forbes-Riley and Litman, 2009), 9 statements concern the tutoring domain (e.g., The tutor was effective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorporated into other recent work exploring user satisfaction in spoken dialogue computer tutors (Dzikovska et al., 2011). In Section 6 we discuss how user scores on these instruments are used to measure system performance. See (Forbes-Riley and Litman, 2011a) for further details of ITSPOKE and the 2</context>
</contexts>
<marker>Baylor, Ryu, Shen, 2003</marker>
<rawString>A. L. Baylor, J. Ryu, and E. Shen. 2003. The effect of pedagogical agent voice and animation on learning, motivation, and perceived persona. In Proceedings of the ED-MEDIA Conference, Honolulu, Hawaii, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Black</author>
<author>P Taylor</author>
</authors>
<title>Festival speech synthesis system: system documentation (1.1.1). The Centre for Speech Technology Research,</title>
<date>1997</date>
<location>University of Edinburgh, http://www.cstr.ed.ac.uk/projects/festival/.</location>
<contexts>
<context position="12951" citStr="Black and Taylor, 1997" startWordPosition="1980" endWordPosition="1983"> given after the instruments measuring performance (survey and posttest). Our survey and tests are the same as those used in multiple prior ITSPOKE experiments (c.f., (ForbesRiley and Litman, 2011a)). The pretest and posttest each contain 26 multiple choice questions querying knowledge of the topics covered in the dialogues. Average pretest and posttest scores in the corpus were 51.0% and 73.1% (out of 100%) with standard deviations of 14.5% and 13.8%, respectively. The user satisfaction survey contains 16 statements rated on a 5-point Likert scale. Average total sur3an outgrowth of Festival (Black and Taylor, 1997). 93 vey score was 60.9 (out of 80), with a standard deviation of 8.5. While the statements themselves are listed elsewhere (Forbes-Riley and Litman, 2009), 9 statements concern the tutoring domain (e.g., The tutor was effective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorpora</context>
</contexts>
<marker>Black, Taylor, 1997</marker>
<rawString>A. Black and P. Taylor. 1997. Festival speech synthesis system: system documentation (1.1.1). The Centre for Speech Technology Research, University of Edinburgh, http://www.cstr.ed.ac.uk/projects/festival/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>Models for multiparty engagement in open-world dialog.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGdial,</booktitle>
<location>London, UK.</location>
<contexts>
<context position="6345" citStr="Bohus and Horvitz, 2009" startWordPosition="965" endWordPosition="968">ser disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features</context>
<context position="7575" citStr="Bohus and Horvitz (2009)" startWordPosition="1151" endWordPosition="1154">to detect disengagement also vary depending on system domain and application. For example, Sidner &amp; Lee (2003) are interested in modeling more natural and collaborative human-robot interactions during basic conversations. They define an algorithm for the engagement process that involves appropriate eye gaze and turn-taking. Martalo et al. (2008) study how user engagement influences dialogue patterns during interactions with an embodied agent that gives advice about healthy dieting. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al., 2000). Bohus and Horvitz (2009) study systems that attract and engage users for dynamic, multi-party dialogues in open-world settings. They model user intentions to engage the system with cues from facial sensors and the dialogue. Within recent spoken dialogue research, acoustic-prosodic, lexical and contextual features have been found to be effective detectors of disengagement (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010); we will briefly compare our own results with these in Section 5. While all of the above-mentioned research has presented intrinsic evaluations of their disengagement modeling effo</context>
</contexts>
<marker>Bohus, Horvitz, 2009</marker>
<rawString>D. Bohus and E. Horvitz. 2009. Models for multiparty engagement in open-world dialog. In Proceedings of SIGdial, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Conati</author>
<author>H Maclaren</author>
</authors>
<title>Empirically building and evaluating a probabilistic model of user affect. User Modeling and User-Adapted Interaction,</title>
<date>2009</date>
<contexts>
<context position="9283" citStr="Conati and Maclaren, 2009" startWordPosition="1409" endWordPosition="1412"> performing user studies with the implemented model, thereby bypassing other less expensive and less labor-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result</context>
<context position="15084" citStr="Conati and Maclaren, 2009" startWordPosition="2319" endWordPosition="2322">cond trained annotator, yielding 0.55 Kappa (this analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mello et al., 2008) both illustrate the common finding that human annotators display low to moderate interannotator reliability for affect annotation, and both studies show that trained labelers yield the highest reliability on this task. Despite the lack of high interannotator reliability, responding t</context>
<context position="16556" citStr="Conati and Maclaren (2009)" startWordPosition="2544" endWordPosition="2548">55% Uncertain+Disengaged 373 5.17% 4 Automatically Detecting User Disengagement (DISE) in ITSPOKE As noted in Section 1, we have developed a user disengagement detector to incorporate into our existing uncertainty-adaptive spoken dialogue system. The result will be a state of the art system that adapts to multiple affective states during the dialogue. 4.1 Binary DISE Label Our disengagement annotation scheme (ForbesRiley et al., 2011) was derived from empirical observations in our data but draws on prior work, including work mentioned in Section 2, appraisal theory-based emotion models (e.g., Conati and Maclaren (2009))4, and prior approaches to annotating disengagement or related states in tutoring (Lehman et al., 2008; Porayska-Pomsta et al., 2008). Briefly, our overall Disengagement label (DISE) is used for turns expressing moderate to strong disengagement towards the interaction, i.e., responses given without much effort or without caring about appropriateness. Responses might also be accompanied by signs of inattention, boredom, or irritation. Clear examples include answers spoken quickly in leaden monotone, with sarcastic or playful tones, or with off-task sounds such as rhythmic tapping or 4Appraisal</context>
</contexts>
<marker>Conati, Maclaren, 2009</marker>
<rawString>C. Conati and H. Maclaren. 2009. Empirically building and evaluating a probabilistic model of user affect. User Modeling and User-Adapted Interaction, 19(3):267–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cowie</author>
<author>R R Cornelius</author>
</authors>
<title>Describing the emotional states that are expressed in speech.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>40--1</pages>
<contexts>
<context position="1815" citStr="Cowie and Cornelius, 2003" startWordPosition="265" endWordPosition="268">ntly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affecti</context>
</contexts>
<marker>Cowie, Cornelius, 2003</marker>
<rawString>R. Cowie and R. R. Cornelius. 2003. Describing the emotional states that are expressed in speech. Speech Communication, 40(1-2):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Devillers</author>
<author>L Vidrascu</author>
</authors>
<title>Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs.</title>
<date>2006</date>
<booktitle>In Ninth International Conference on Spoken Language Processing (ICSLP,</booktitle>
<pages>801--804</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="9336" citStr="Devillers and Vidrascu, 2006" startWordPosition="1417" endWordPosition="1420">l, thereby bypassing other less expensive and less labor-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITS</context>
</contexts>
<marker>Devillers, Vidrascu, 2006</marker>
<rawString>L. Devillers and L. Vidrascu. 2006. Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs. In Ninth International Conference on Spoken Language Processing (ICSLP, pages 801–804, Pittsburgh, PA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D’Mello</author>
<author>S Craig</author>
<author>A Witherspoon</author>
<author>B McDaniel</author>
<author>A Graesser</author>
</authors>
<title>Automatic detection of learner’s affect from conversational cues. User Modeling and User-Adapted Interaction:</title>
<date>2008</date>
<journal>The Journal of Personalization Research,</journal>
<pages>18--45</pages>
<marker>D’Mello, Craig, Witherspoon, McDaniel, Graesser, 2008</marker>
<rawString>S. D’Mello, S. Craig, A. Witherspoon, B. McDaniel, and A. Graesser. 2008. Automatic detection of learner’s affect from conversational cues. User Modeling and User-Adapted Interaction: The Journal of Personalization Research, 18:45–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D’Mello</author>
<author>B Lehman</author>
<author>J Sullins</author>
<author>R Daigle</author>
<author>R Combs</author>
<author>K Vogt</author>
<author>L Perkins</author>
<author>A Graesser</author>
</authors>
<title>A time for emoting: When affect-sensitivity is and isn’t effective at promoting deep learning.</title>
<date>2010</date>
<booktitle>In Intelligent Tutoring Systems Conference,</booktitle>
<pages>245--254</pages>
<location>Pittsburgh, PA, USA,</location>
<marker>D’Mello, Lehman, Sullins, Daigle, Combs, Vogt, Perkins, Graesser, 2010</marker>
<rawString>S. D’Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs, K. Vogt, L. Perkins, and A. Graesser. 2010. A time for emoting: When affect-sensitivity is and isn’t effective at promoting deep learning. In Intelligent Tutoring Systems Conference, pages 245–254, Pittsburgh, PA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Drummond</author>
<author>D Litman</author>
</authors>
<title>Examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system.</title>
<date>2011</date>
<booktitle>In Proc. 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>312--318</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="3396" citStr="Drummond and Litman, 2011" startWordPosition="508" endWordPosition="511">e targeted user uncertainty and disengagement because manual annotation showed them to be the two most common user affective states in our system and both are negatively correlated with task success (Litman and Forbes-Riley, 2009; ForbesRiley and Litman, 2011b). Thus, we hypothesize that providing appropriate responses to these states would reduce their frequency, consequently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting </context>
<context position="19614" citStr="Drummond and Litman, 2011" startWordPosition="3008" endWordPosition="3012">. Since total disengagement is common in real-world unobserved human-computer interactions (deleting unsatisfactory software being an extreme example) it remains an open question as to how well laboratory findings generalize. 6Our original scheme distinguished six DISE subtypes that trained annotators distinguished with a reliability of .43 Kappa (Forbes-Riley et al., 2011). However, pilot experiments indicated that our models cannot accurately distinguish them, thus our DISE detector focuses on the DISE label. 7http://www.cs.waikato.ac.nz/ml/weka/ tween different machine learning algorithms (Drummond and Litman, 2011). We also use a cost matrix, which heavily penalizes classifying a true DISE instance as false, because our class distributions are highly skewed (16.21% DISE turns) and the cost matrix successfully mitigated the skew’s effect in our prior work, where the uncertainty distribution is also skewed (20.55% UNC turns) (Drummond and Litman, 2011). To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus. As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the fir</context>
<context position="21972" citStr="Drummond and Litman, 2011" startWordPosition="3376" endWordPosition="3380">rect runs • User Identifier Features: gender and pretest score Figure 2: Features Used to Detect Disengagement (DISE) for each User Turn 95 Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE Label in the 2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in other domains, while pretest s</context>
</contexts>
<marker>Drummond, Litman, 2011</marker>
<rawString>J. Drummond and D. Litman. 2011. Examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system. In Proc. 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 312–318, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dzikovska</author>
<author>J Moore</author>
<author>N Steinhauser</author>
<author>G Campbell</author>
</authors>
<title>Exploring user satisfaction in a tutorial dialogue system.</title>
<date>2011</date>
<booktitle>In Proc. 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>162--172</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="13665" citStr="Dzikovska et al., 2011" startWordPosition="2098" endWordPosition="2101"> themselves are listed elsewhere (Forbes-Riley and Litman, 2009), 9 statements concern the tutoring domain (e.g., The tutor was effective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorporated into other recent work exploring user satisfaction in spoken dialogue computer tutors (Dzikovska et al., 2011). In Section 6 we discuss how user scores on these instruments are used to measure system performance. See (Forbes-Riley and Litman, 2011a) for further details of ITSPOKE and the 2008 experiment. Following the experiment, the entire corpus was manually labeled for (in)correctness (correct, incorrect), (un)certainty (CER, UNC) and (dis)engagement (ENG, DISE) by one trained annotator. Table 1 shows the distribution of the labeled turns in the 2008 ITSPOKE corpus. In prior ITSPOKE corpora, our annotator displayed interannotator agreement of 0.85 and 0.62 Kappa on correctness and uncertainty, resp</context>
</contexts>
<marker>Dzikovska, Moore, Steinhauser, Campbell, 2011</marker>
<rawString>M. Dzikovska, J. Moore, N. Steinhauser, and G. Campbell. 2011. Exploring user satisfaction in a tutorial dialogue system. In Proc. 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 162–172, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Florian</author>
<author>M Wollmer</author>
<author>B Schuller</author>
</authors>
<title>The Munich versatile and fast open-source audio feature extractor.</title>
<date>2010</date>
<booktitle>In Proc. ACM Multimedia (MM),</booktitle>
<pages>1459--1462</pages>
<location>Florence, Italy.</location>
<contexts>
<context position="22235" citStr="Florian et al., 2010" startWordPosition="3418" endWordPosition="3421">2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and ex</context>
</contexts>
<marker>Florian, Wollmer, Schuller, 2010</marker>
<rawString>E. Florian, M. Wollmer, and B. Schuller. 2010. The Munich versatile and fast open-source audio feature extractor. In Proc. ACM Multimedia (MM), pages 1459– 1462, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>A user modelingbased performance analysis of a wizarded uncertaintyadaptive dialogue system corpus.</title>
<date>2009</date>
<booktitle>In Proc. Interspeech,</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="13106" citStr="Forbes-Riley and Litman, 2009" startWordPosition="2006" endWordPosition="2009">experiments (c.f., (ForbesRiley and Litman, 2011a)). The pretest and posttest each contain 26 multiple choice questions querying knowledge of the topics covered in the dialogues. Average pretest and posttest scores in the corpus were 51.0% and 73.1% (out of 100%) with standard deviations of 14.5% and 13.8%, respectively. The user satisfaction survey contains 16 statements rated on a 5-point Likert scale. Average total sur3an outgrowth of Festival (Black and Taylor, 1997). 93 vey score was 60.9 (out of 80), with a standard deviation of 8.5. While the statements themselves are listed elsewhere (Forbes-Riley and Litman, 2009), 9 statements concern the tutoring domain (e.g., The tutor was effective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorporated into other recent work exploring user satisfaction in spoken dialogue computer tutors (Dzikovska et al., 2011). In Section 6 we discuss how user scores</context>
</contexts>
<marker>Forbes-Riley, Litman, 2009</marker>
<rawString>K. Forbes-Riley and D. Litman. 2009. A user modelingbased performance analysis of a wizarded uncertaintyadaptive dialogue system corpus. In Proc. Interspeech, Brighton, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor.</title>
<date>2011</date>
<journal>Speech Communication,</journal>
<pages>53--9</pages>
<contexts>
<context position="1983" citStr="Forbes-Riley and Litman, 2011" startWordPosition="290" endWordPosition="293">alogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affective states. We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-R</context>
<context position="6475" citStr="Forbes-Riley and Litman, 2011" startWordPosition="985" endWordPosition="988">tation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features used to detect disengagement also vary depending on system domain and application. For example, Sidner &amp; Lee (2003) are intereste</context>
<context position="10234" citStr="Forbes-Riley and Litman, 2011" startWordPosition="1559" endWordPosition="1562">ese have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tutor We develop and evaluate our disengagement detector using a corpus of spoken dialogues from a 2008 controlled experiment evaluating our uncertaintyadaptive spoken dialogue tutoring system, ITSPOKE (Intelligent Tutoring SPOKEn dialog system) (Forbes-Riley and Litman, 2011a).2 ITSPOKE tutors 5 Newtonian physics problems (one per dialogue), using a Tutor Question - Student Answer - Tutor Response format. After each tutor question, the student speech is digitized from head-mounted microphone input and sent 2ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002). to the Sphinx2 recognizer, which yields an automatic transcript (Huang et al., 1993). This answer’s (in)correctness is then automatically classified based on this transcript, using the TuTalk semantic analyzer (Jordan et al</context>
<context position="13802" citStr="Forbes-Riley and Litman, 2011" startWordPosition="2121" endWordPosition="2124">ective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorporated into other recent work exploring user satisfaction in spoken dialogue computer tutors (Dzikovska et al., 2011). In Section 6 we discuss how user scores on these instruments are used to measure system performance. See (Forbes-Riley and Litman, 2011a) for further details of ITSPOKE and the 2008 experiment. Following the experiment, the entire corpus was manually labeled for (in)correctness (correct, incorrect), (un)certainty (CER, UNC) and (dis)engagement (ENG, DISE) by one trained annotator. Table 1 shows the distribution of the labeled turns in the 2008 ITSPOKE corpus. In prior ITSPOKE corpora, our annotator displayed interannotator agreement of 0.85 and 0.62 Kappa on correctness and uncertainty, respectively (Forbes-Riley and Litman, 2011a). For the disengagement label, a reliability analysis was performed over several annotation roun</context>
<context position="21943" citStr="Forbes-Riley and Litman, 2011" startWordPosition="3372" endWordPosition="3375">beled turn (in)correctness incorrect runs • User Identifier Features: gender and pretest score Figure 2: Features Used to Detect Disengagement (DISE) for each User Turn 95 Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE Label in the 2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in o</context>
<context position="32712" citStr="Forbes-Riley and Litman, 2011" startWordPosition="5130" endWordPosition="5133"> to one user affective state, is performance likely to improve by detecting and adapting to multiple affective states? To answer this question, we performed a multiple linear regression analysis aimed at quantifying the relative usefulness of the automatically detected 13Identical results were obtained by using an average instead of a total, and only slightly weaker results were obtained when normalizing the DISE totals as the percentages of total turns. 14We previously found a related correlation between different DISE and learning measures, during the analysis of our DISE annotation scheme (Forbes-Riley and Litman, 2011b). In particular, we showed a significant partial correlation between the percentage of manual DISE labels and posttest controlled for pretest score. disengagement and uncertainty labels when predicting our system performance metrics. We ran four stepwise linear regressions. The first regression predicted learning gain, and gave the model two possible inputs: the total number of automatic DISE labels and UNC labels per user. We then ran the same regression again, this time predicting user satisfaction. For comparison, we ran the same two regressions using the manual DISE and UNC labels. As th</context>
<context position="37713" citStr="Forbes-Riley and Litman, 2011" startWordPosition="5895" endWordPosition="5898">icantly improve performance even in the presence of noise from the automatic labeling. Finally, further extrinsic analyses using multiple regression suggest that adapting to our automatic disengagement labels can improve learning (though not user satisfaction) over and above the improvement achieved by only adapting to automatically detected user uncertainty. We have already developed and implemented an adaptation for user disengagement in ITSPOKE. The disengagement adaptation draws on empirical analyses of our data and effective responses to user disengagement presented in prior work (c.f., (Forbes-Riley and Litman, 2011b)), We are currently evaluating our disengagement adaptation in the “ideal” environment of a Wizard of Oz experiment, where user disengagement, uncertainty, and correctness are labeled by a hidden human during user interactions with ITSPOKE. Based on the evaluations here, we believe our disengagement model is ready for implementation in ITSPOKE. We will then evaluate the resulting spoken dialogue system for detecting and adapting to multiple affective states in an upcoming controlled experiment with real users. Acknowledgments This work is funded by NSF award 0914615. We thank Scott Silliman </context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>K. Forbes-Riley and D. Litman. 2011a. Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor. Speech Communication, 53(9–10):1115–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>When does disengagement correlate with learning in spoken dialog computer tutoring?</title>
<date>2011</date>
<booktitle>In Proceedings 15th International Conference on Artificial Intelligence in Education (AIED),</booktitle>
<location>Auckland, NZ,</location>
<contexts>
<context position="1983" citStr="Forbes-Riley and Litman, 2011" startWordPosition="290" endWordPosition="293">alogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affective states. We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-R</context>
<context position="6475" citStr="Forbes-Riley and Litman, 2011" startWordPosition="985" endWordPosition="988">tation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features used to detect disengagement also vary depending on system domain and application. For example, Sidner &amp; Lee (2003) are intereste</context>
<context position="10234" citStr="Forbes-Riley and Litman, 2011" startWordPosition="1559" endWordPosition="1562">ese have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tutor We develop and evaluate our disengagement detector using a corpus of spoken dialogues from a 2008 controlled experiment evaluating our uncertaintyadaptive spoken dialogue tutoring system, ITSPOKE (Intelligent Tutoring SPOKEn dialog system) (Forbes-Riley and Litman, 2011a).2 ITSPOKE tutors 5 Newtonian physics problems (one per dialogue), using a Tutor Question - Student Answer - Tutor Response format. After each tutor question, the student speech is digitized from head-mounted microphone input and sent 2ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002). to the Sphinx2 recognizer, which yields an automatic transcript (Huang et al., 1993). This answer’s (in)correctness is then automatically classified based on this transcript, using the TuTalk semantic analyzer (Jordan et al</context>
<context position="13802" citStr="Forbes-Riley and Litman, 2011" startWordPosition="2121" endWordPosition="2124">ective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorporated into other recent work exploring user satisfaction in spoken dialogue computer tutors (Dzikovska et al., 2011). In Section 6 we discuss how user scores on these instruments are used to measure system performance. See (Forbes-Riley and Litman, 2011a) for further details of ITSPOKE and the 2008 experiment. Following the experiment, the entire corpus was manually labeled for (in)correctness (correct, incorrect), (un)certainty (CER, UNC) and (dis)engagement (ENG, DISE) by one trained annotator. Table 1 shows the distribution of the labeled turns in the 2008 ITSPOKE corpus. In prior ITSPOKE corpora, our annotator displayed interannotator agreement of 0.85 and 0.62 Kappa on correctness and uncertainty, respectively (Forbes-Riley and Litman, 2011a). For the disengagement label, a reliability analysis was performed over several annotation roun</context>
<context position="21943" citStr="Forbes-Riley and Litman, 2011" startWordPosition="3372" endWordPosition="3375">beled turn (in)correctness incorrect runs • User Identifier Features: gender and pretest score Figure 2: Features Used to Detect Disengagement (DISE) for each User Turn 95 Table 2: Results of 10-fold Cross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE Label in the 2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in o</context>
<context position="32712" citStr="Forbes-Riley and Litman, 2011" startWordPosition="5130" endWordPosition="5133"> to one user affective state, is performance likely to improve by detecting and adapting to multiple affective states? To answer this question, we performed a multiple linear regression analysis aimed at quantifying the relative usefulness of the automatically detected 13Identical results were obtained by using an average instead of a total, and only slightly weaker results were obtained when normalizing the DISE totals as the percentages of total turns. 14We previously found a related correlation between different DISE and learning measures, during the analysis of our DISE annotation scheme (Forbes-Riley and Litman, 2011b). In particular, we showed a significant partial correlation between the percentage of manual DISE labels and posttest controlled for pretest score. disengagement and uncertainty labels when predicting our system performance metrics. We ran four stepwise linear regressions. The first regression predicted learning gain, and gave the model two possible inputs: the total number of automatic DISE labels and UNC labels per user. We then ran the same regression again, this time predicting user satisfaction. For comparison, we ran the same two regressions using the manual DISE and UNC labels. As th</context>
<context position="37713" citStr="Forbes-Riley and Litman, 2011" startWordPosition="5895" endWordPosition="5898">icantly improve performance even in the presence of noise from the automatic labeling. Finally, further extrinsic analyses using multiple regression suggest that adapting to our automatic disengagement labels can improve learning (though not user satisfaction) over and above the improvement achieved by only adapting to automatically detected user uncertainty. We have already developed and implemented an adaptation for user disengagement in ITSPOKE. The disengagement adaptation draws on empirical analyses of our data and effective responses to user disengagement presented in prior work (c.f., (Forbes-Riley and Litman, 2011b)), We are currently evaluating our disengagement adaptation in the “ideal” environment of a Wizard of Oz experiment, where user disengagement, uncertainty, and correctness are labeled by a hidden human during user interactions with ITSPOKE. Based on the evaluations here, we believe our disengagement model is ready for implementation in ITSPOKE. We will then evaluate the resulting spoken dialogue system for detecting and adapting to multiple affective states in an upcoming controlled experiment with real users. Acknowledgments This work is funded by NSF award 0914615. We thank Scott Silliman </context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>K. Forbes-Riley and D. Litman. 2011b. When does disengagement correlate with learning in spoken dialog computer tutoring? In Proceedings 15th International Conference on Artificial Intelligence in Education (AIED), Auckland, NZ, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>D Litman</author>
</authors>
<title>The relative impact of student affect on performance models in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction,</title>
<date>2008</date>
<pages>18--1</pages>
<contexts>
<context position="34438" citStr="Forbes-Riley et al., 2008" startWordPosition="5403" endWordPosition="5406">ted disengagement and uncertainty labels can further improve learning over and above adapting to uncertainty alone. Although the final model’s predictive power is low (R2=0.15), our interest here is only in investigating whether the two affective states are more useful in combination than in isolation for predicting performance. In similar types of stepwise regressions on prior ITSPOKE corpora, we’ve shown that more complete models of system performance incorporating many predictors of learning (i.e. affective states in conjunction with other dialogue features) can yield R2 values of over .5 (Forbes-Riley et al., 2008).16 15Using the stepwise method, Automatic DISE was the first feature selected, and Automatic UNC the second. However, note that a model consisting of only the Automatic UNC metric also yields significantly worse predictive power than selecting both affective state metrics. Further note that almost identical models were produced using percentages rather than totals. 16R2 is the standard reported metric for linear regressions. However, for consistency with Table 3, note that the two models in Figure 3 yield R values of -.31 and -.38, respectively. 98 Learning Gain= -.31 *Total Automatic DISE (R</context>
</contexts>
<marker>Forbes-Riley, Rotaru, Litman, 2008</marker>
<rawString>K. Forbes-Riley, M. Rotaru, and D. Litman. 2008. The relative impact of student affect on performance models in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction, 18(1-2):11–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
<author>H Friedberg</author>
</authors>
<title>Annotating disengagement for spoken dialogue computer tutoring.</title>
<date>2011</date>
<booktitle>In Sidney D’Mello and Rafael Calvo, editors, Affect and Learning Technologies.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="14578" citStr="Forbes-Riley et al., 2011" startWordPosition="2239" endWordPosition="2242">ct, incorrect), (un)certainty (CER, UNC) and (dis)engagement (ENG, DISE) by one trained annotator. Table 1 shows the distribution of the labeled turns in the 2008 ITSPOKE corpus. In prior ITSPOKE corpora, our annotator displayed interannotator agreement of 0.85 and 0.62 Kappa on correctness and uncertainty, respectively (Forbes-Riley and Litman, 2011a). For the disengagement label, a reliability analysis was performed over several annotation rounds on subsets of the 2008 ITSPOKE corpus by this and a second trained annotator, yielding 0.55 Kappa (this analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example bot</context>
<context position="19364" citStr="Forbes-Riley et al., 2011" startWordPosition="2978" endWordPosition="2981">eriments with our data showed little variance be5Affective systems research has found total disengagement rare in laboratory settings (Lehman et al., 2008; Martalo et al., 2008). As in that research, we equate the DISE label with no or low engagement. Since total disengagement is common in real-world unobserved human-computer interactions (deleting unsatisfactory software being an extreme example) it remains an open question as to how well laboratory findings generalize. 6Our original scheme distinguished six DISE subtypes that trained annotators distinguished with a reliability of .43 Kappa (Forbes-Riley et al., 2011). However, pilot experiments indicated that our models cannot accurately distinguish them, thus our DISE detector focuses on the DISE label. 7http://www.cs.waikato.ac.nz/ml/weka/ tween different machine learning algorithms (Drummond and Litman, 2011). We also use a cost matrix, which heavily penalizes classifying a true DISE instance as false, because our class distributions are highly skewed (16.21% DISE turns) and the cost matrix successfully mitigated the skew’s effect in our prior work, where the uncertainty distribution is also skewed (20.55% UNC turns) (Drummond and Litman, 2011). To tra</context>
</contexts>
<marker>Forbes-Riley, Litman, Friedberg, 2011</marker>
<rawString>K. Forbes-Riley, D. Litman, and H. Friedberg. 2011. Annotating disengagement for spoken dialogue computer tutoring. In Sidney D’Mello and Rafael Calvo, editors, Affect and Learning Technologies. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Gratch</author>
<author>Stacy Marsella</author>
<author>Ning Wang</author>
<author>Brooke Stankovic</author>
</authors>
<title>Assessing the validity of appraisal-based models of emotion.</title>
<date>2009</date>
<booktitle>In Proceedings of ACII,</booktitle>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="15105" citStr="Gratch et al., 2009" startWordPosition="2323" endWordPosition="2326">lding 0.55 Kappa (this analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mello et al., 2008) both illustrate the common finding that human annotators display low to moderate interannotator reliability for affect annotation, and both studies show that trained labelers yield the highest reliability on this task. Despite the lack of high interannotator reliability, responding to affect detected by </context>
</contexts>
<marker>Gratch, Marsella, Wang, Stankovic, 2009</marker>
<rawString>Jonathan Gratch, Stacy Marsella, Ning Wang, and Brooke Stankovic. 2009. Assessing the validity of appraisal-based models of emotion. In Proceedings of ACII, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X D Huang</author>
<author>F Alleva</author>
<author>H W Hon</author>
<author>M Y Hwang</author>
<author>K F Lee</author>
<author>R Rosenfeld</author>
</authors>
<title>The SphinxII speech recognition system: An Overview. Computer, Speech and Language.</title>
<date>1993</date>
<contexts>
<context position="10695" citStr="Huang et al., 1993" startWordPosition="1630" endWordPosition="1633">eriment evaluating our uncertaintyadaptive spoken dialogue tutoring system, ITSPOKE (Intelligent Tutoring SPOKEn dialog system) (Forbes-Riley and Litman, 2011a).2 ITSPOKE tutors 5 Newtonian physics problems (one per dialogue), using a Tutor Question - Student Answer - Tutor Response format. After each tutor question, the student speech is digitized from head-mounted microphone input and sent 2ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002). to the Sphinx2 recognizer, which yields an automatic transcript (Huang et al., 1993). This answer’s (in)correctness is then automatically classified based on this transcript, using the TuTalk semantic analyzer (Jordan et al., 2007), and the answer’s (un)certainty is automatically classified by inputting features of the speech signal, the automatic transcript, and the dialogue context into a logistic regression model. We will discuss these features further in Section 5. All natural language processing components were trained using prior ITSPOKE corpora. The appropriate tutor response is determined based on the answer’s automatically labeled (in)correctness and (un)certainty an</context>
</contexts>
<marker>Huang, Alleva, Hon, Hwang, Lee, Rosenfeld, 1993</marker>
<rawString>X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F. Lee, and R. Rosenfeld. 1993. The SphinxII speech recognition system: An Overview. Computer, Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Jeon</author>
<author>R Xia</author>
<author>Y Liu</author>
</authors>
<title>Level of interest sensing in spoken dialog using multi-level fusion of acoustic and lexical evidence.</title>
<date>2010</date>
<booktitle>In INTERSPEECH’10,</booktitle>
<pages>2802--2805</pages>
<contexts>
<context position="6296" citStr="Jeon et al., 2010" startWordPosition="957" endWordPosition="960">machine learning to develop a detector of user disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger </context>
<context position="7994" citStr="Jeon et al., 2010" startWordPosition="1212" endWordPosition="1215">th an embodied agent that gives advice about healthy dieting. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al., 2000). Bohus and Horvitz (2009) study systems that attract and engage users for dynamic, multi-party dialogues in open-world settings. They model user intentions to engage the system with cues from facial sensors and the dialogue. Within recent spoken dialogue research, acoustic-prosodic, lexical and contextual features have been found to be effective detectors of disengagement (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010); we will briefly compare our own results with these in Section 5. While all of the above-mentioned research has presented intrinsic evaluations of their disengagement modeling efforts that indicate a reasonable degree of accuracy as compared to a gold standard (e.g., manual coding), only a few have yet demonstrated that the model’s detected values are useful 92 in practice and/or are a reasonable substitute for the gold standard with respect to some practical objective (e.g., a relationship to performance). In particular, two studies (Bohus and Horvitz, 2009; Schuller et al., 2009a) have gone</context>
<context position="23808" citStr="Jeon et al., 2010" startWordPosition="3663" endWordPosition="3666">e the standard measures used to evaluate current affect recognition technology, particularly for unbalanced two-class problems (Schuller et al., 2009b). In addition, we use the cross correlation (CC) measure and mean linear error (MLE) because these metrics were used in recent work for evaluating disengagement (level of interest) detectors for the Interspeech 2010 challenge (Schuller et 8We also tried using our automatic UNC label as a feature in our DISE model, but our results weren’t significantly improved. 9simply ((Precision(DISE) + Precision(ENG))/2) al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010)).10 Note however that the Interspeech 2010 task differs from ours not only in the corpus and features, but also in the learning task: they used regression to detect a continuous level of interest ranging from 0 to 1, while we detect a binary class. Thus comparison between our results and those are only suggestive rather than conclusive. As shown in Table 2, we also compare our results with those of majority class (ENG) labeling of the same turns. Since (7216-1170)/7216 user turns in the corpus are engaged (recall Table 1), always selecting the majority class (ENG) label for these turns thus y</context>
<context position="25111" citStr="Jeon et al., 2010" startWordPosition="3890" endWordPosition="3893">100% recall for ENG). While our DISE model does not outperform majority class labeling with respect to accuracy, this is not surprising given the steep skew in class distribution, and our learned model significantly outperforms the baseline with respect to all the other measures (p&lt;.001).11 Our CC and MLE results are on par with the best results from the state-of-the-art systems competing in the 2010 Interspeech Challenge, where the task was to detect level of interest. In particular, the winner obtained a CC of 0.428 (higher numbers are better) and an MLE of 0.146 (lower numbers are better) (Jeon et al., 2010), while a subsequent study yielded a CC of 0.480 and an MLE of 0.131 on the same corpus (Wang and Hirschberg, 2011). Our results are also on par with the best results of the other prior research on detecting disengagement discussed in Section 2 that detects a small number of disengagement classes and reports accuracy and/or recall and precision. For example, (Martalo et al., 2008) report average precision of 75% and recall 10Pearson product-moment correlation coefficient (CC) is a measure of the linear dependence that is widely used in regression settings. MLE is a regression performance measu</context>
</contexts>
<marker>Jeon, Xia, Liu, 2010</marker>
<rawString>J. H. Jeon, R. Xia, and Y. Liu. 2010. Level of interest sensing in spoken dialog using multi-level fusion of acoustic and lexical evidence. In INTERSPEECH’10, pages 2802–2805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>B Hall</author>
<author>M Ringenberg</author>
<author>Y Cui</author>
<author>C P Rose</author>
</authors>
<title>Tools for authoring a dialogue agent that participates in learning studies.</title>
<date>2007</date>
<booktitle>In Proc. Artificial Intelligence in Education (AIED),</booktitle>
<pages>43--50</pages>
<contexts>
<context position="10842" citStr="Jordan et al., 2007" startWordPosition="1653" endWordPosition="1656">Litman, 2011a).2 ITSPOKE tutors 5 Newtonian physics problems (one per dialogue), using a Tutor Question - Student Answer - Tutor Response format. After each tutor question, the student speech is digitized from head-mounted microphone input and sent 2ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002). to the Sphinx2 recognizer, which yields an automatic transcript (Huang et al., 1993). This answer’s (in)correctness is then automatically classified based on this transcript, using the TuTalk semantic analyzer (Jordan et al., 2007), and the answer’s (un)certainty is automatically classified by inputting features of the speech signal, the automatic transcript, and the dialogue context into a logistic regression model. We will discuss these features further in Section 5. All natural language processing components were trained using prior ITSPOKE corpora. The appropriate tutor response is determined based on the answer’s automatically labeled (in)correctness and (un)certainty and then sent to the Cepstral text-to-speech system3, whose audio output is played through the student headphones and is also displayed on a web-base</context>
</contexts>
<marker>Jordan, Hall, Ringenberg, Cui, Rose, 2007</marker>
<rawString>P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P. Rose. 2007. Tools for authoring a dialogue agent that participates in learning studies. In Proc. Artificial Intelligence in Education (AIED), pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kapoor</author>
<author>R W Picard</author>
</authors>
<title>Multimodal affect recognition in learning environments.</title>
<date>2005</date>
<booktitle>In 13th Annual ACM International Conference on Multimedia,</booktitle>
<pages>677--682</pages>
<contexts>
<context position="6422" citStr="Kapoor and Picard, 2005" startWordPosition="977" endWordPosition="980">ness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features used to detect disengagement also vary depending on system domain and applic</context>
<context position="25914" citStr="Kapoor and Picard, 2005" startWordPosition="4021" endWordPosition="4024"> prior research on detecting disengagement discussed in Section 2 that detects a small number of disengagement classes and reports accuracy and/or recall and precision. For example, (Martalo et al., 2008) report average precision of 75% and recall 10Pearson product-moment correlation coefficient (CC) is a measure of the linear dependence that is widely used in regression settings. MLE is a regression performance measure for the mean absolute error between an estimator and the true value. 11CC is undefined for majority class labeling. 96 of 74% (detecting three levels of disengagement), while (Kapoor and Picard, 2005) report an accuracy of 86% for detecting binary (dis)interest. Our final DISE model was produced by running the J48 algorithm over our entire corpus. The resulting decision tree contains 141 nodes and 75 leaves. Inspection of the tree reveals that all of the feature types in Figure 2 (acoustic-prosodic, lexical/dialogue, user identifier) are used as decision nodes in the tree, although not all variations on these types were used. The upper-level nodes of the tree are usually considered to be more informative features as compared to lower-level nodes, since they are queried for more leaves. The</context>
</contexts>
<marker>Kapoor, Picard, 2005</marker>
<rawString>A. Kapoor and R. W. Picard. 2005. Multimodal affect recognition in learning environments. In 13th Annual ACM International Conference on Multimedia, pages 677–682, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klein</author>
<author>Y Moon</author>
<author>R Picard</author>
</authors>
<title>This computer responds to user frustration: Theory, design, and results. Interacting with Computers,</title>
<date>2002</date>
<pages>14--119</pages>
<contexts>
<context position="2139" citStr="Klein et al., 2002" startWordPosition="317" endWordPosition="320">ascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affective states. We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-Riley and Litman, 2011a). We are now taking the next step: incorporating automatic detection and adaptation to user disengagement as well, with the goal of f</context>
</contexts>
<marker>Klein, Moon, Picard, 2002</marker>
<rawString>J. Klein, Y. Moon, and R. Picard. 2002. This computer responds to user frustration: Theory, design, and results. Interacting with Computers, 14:119–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>S Narayanan</author>
</authors>
<title>Towards detecting emotions in spoken dialogs.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="9361" citStr="Lee and Narayanan, 2005" startWordPosition="1421" endWordPosition="1424">s expensive and less labor-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tut</context>
</contexts>
<marker>Lee, Narayanan, 2005</marker>
<rawString>C. M. Lee and S. Narayanan. 2005. Towards detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lehman</author>
<author>M Matthews</author>
<author>S D’Mello</author>
<author>N Person</author>
</authors>
<title>What are you feeling? Investigating student affective states during expert human tutoring sessions.</title>
<date>2008</date>
<booktitle>In Intelligent Tutoring Systems Conference (ITS),</booktitle>
<pages>50--59</pages>
<location>Montreal, Canada,</location>
<marker>Lehman, Matthews, D’Mello, Person, 2008</marker>
<rawString>B. Lehman, M. Matthews, S. D’Mello, and N. Person. 2008. What are you feeling? Investigating student affective states during expert human tutoring sessions. In Intelligent Tutoring Systems Conference (ITS), pages 50–59, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>K Forbes-Riley</author>
</authors>
<title>Spoken tutorial dialogue and the feeling of another’s knowing.</title>
<date>2009</date>
<booktitle>In Proceedings 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<location>London, UK,</location>
<contexts>
<context position="3000" citStr="Litman and Forbes-Riley, 2009" startWordPosition="450" endWordPosition="453"> that detect and respond to multiple affective states. We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-Riley and Litman, 2011a). We are now taking the next step: incorporating automatic detection and adaptation to user disengagement as well, with the goal of further improving task success. We targeted user uncertainty and disengagement because manual annotation showed them to be the two most common user affective states in our system and both are negatively correlated with task success (Litman and Forbes-Riley, 2009; ForbesRiley and Litman, 2011b). Thus, we hypothesize that providing appropriate responses to these states would reduce their frequency, consequently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics</context>
<context position="36157" citStr="Litman and Forbes-Riley, 2009" startWordPosition="5663" endWordPosition="5666">heir gold standard counterparts. Detecting multiple affective states might thus be one way to compensate for the noise that is introduced in a fully-automated affective spoken dialogue system. Similarly, only the DISE metric was selected for inclusion in the regression model of user satisfaction, regardless of whether manual or automatic labels were used. A separate correlation analysis showed that user uncertainty is not significantly correlated with user satisfaction in our system, though we previously found that multiple uncertainty-related metrics do significantly correlate with learning (Litman and Forbes-Riley, 2009). 8 Summary and Current Directions In this paper we used extrinsic evaluations to provide evidence for the utility of a new system design involving the complex task of user affect detection, prior to undertaking an expensive and timeconsuming evaluation of an affect-adaptive system with real users. In particular, we first presented a novel model for automatically detecting user disengagement in spoken dialogue systems. We showed through intrinsic evaluations (i.e., cross-validation experiments using gold-standard labels) that the model yields results on par with prior work. We then showed cruc</context>
</contexts>
<marker>Litman, Forbes-Riley, 2009</marker>
<rawString>D. Litman and K. Forbes-Riley. 2009. Spoken tutorial dialogue and the feeling of another’s knowing. In Proceedings 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), London, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Liu</author>
<author>R W Picard</author>
</authors>
<title>Embedded empathy in continuous, interactive health assessment.</title>
<date>2005</date>
<booktitle>In CHI Workshop on HCI Challenges in Health Assessment.</booktitle>
<contexts>
<context position="2118" citStr="Liu and Picard, 2005" startWordPosition="313" endWordPosition="316">d Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affective states. We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-Riley and Litman, 2011a). We are now taking the next step: incorporating automatic detection and adaptation to user disengagement as wel</context>
</contexts>
<marker>Liu, Picard, 2005</marker>
<rawString>K. Liu and R. W. Picard. 2005. Embedded empathy in continuous, interactive health assessment. In CHI Workshop on HCI Challenges in Health Assessment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Martalo</author>
<author>N Novielli</author>
<author>F de Rosis</author>
</authors>
<title>Attitude display in dialogue patterns.</title>
<date>2008</date>
<booktitle>In Proc. AISB 2008 Symposium on Affective Language in Human and Machine,</booktitle>
<pages>1--8</pages>
<location>Aberdeen, Scotland,</location>
<marker>Martalo, Novielli, de Rosis, 2008</marker>
<rawString>A. Martalo, N. Novielli, and F. de Rosis. 2008. Attitude display in dialogue patterns. In Proc. AISB 2008 Symposium on Affective Language in Human and Machine, pages 1–8, Aberdeen, Scotland, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McQuiggan</author>
<author>B Mott</author>
<author>J Lester</author>
</authors>
<title>Modeling self-efficacy in intelligent tutoring systems: An inductive approach. User Modeling and User-Adapted Interaction (UMUAI),</title>
<date>2008</date>
<pages>18--1</pages>
<contexts>
<context position="15130" citStr="McQuiggan et al., 2008" startWordPosition="2327" endWordPosition="2330">s analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mello et al., 2008) both illustrate the common finding that human annotators display low to moderate interannotator reliability for affect annotation, and both studies show that trained labelers yield the highest reliability on this task. Despite the lack of high interannotator reliability, responding to affect detected by trained human labels has </context>
</contexts>
<marker>McQuiggan, Mott, Lester, 2008</marker>
<rawString>S. McQuiggan, B. Mott, and J. Lester. 2008. Modeling self-efficacy in intelligent tutoring systems: An inductive approach. User Modeling and User-Adapted Interaction (UMUAI), 18(1-2):81–123, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>Y-C Ju</author>
</authors>
<title>Accommodating explicit user expressions of uncertainty in voice search or something like that.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Annual Conference of the International Speech Communication Association (INTERSPEECH 08),</booktitle>
<pages>1165--1168</pages>
<location>Brisbane, Australia,</location>
<contexts>
<context position="3443" citStr="Paek and Ju, 2008" startWordPosition="517" endWordPosition="520">anual annotation showed them to be the two most common user affective states in our system and both are negatively correlated with task success (Litman and Forbes-Riley, 2009; ForbesRiley and Litman, 2011b). Thus, we hypothesize that providing appropriate responses to these states would reduce their frequency, consequently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting commercial applications (Wang and Hirschberg, 2</context>
</contexts>
<marker>Paek, Ju, 2008</marker>
<rawString>T. Paek and Y.-C. Ju. 2008. Accommodating explicit user expressions of uncertainty in voice search or something like that. In Proceedings of the 9th Annual Conference of the International Speech Communication Association (INTERSPEECH 08), pages 1165– 1168, Brisbane, Australia, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pon-Barry</author>
<author>S Shieber</author>
</authors>
<title>Recognizing uncertainty in speech.</title>
<date>2011</date>
<booktitle>EURASIP Journal on Advances in Signal Processing.</booktitle>
<contexts>
<context position="14783" citStr="Pon-Barry and Shieber, 2011" startWordPosition="2273" endWordPosition="2276">, our annotator displayed interannotator agreement of 0.85 and 0.62 Kappa on correctness and uncertainty, respectively (Forbes-Riley and Litman, 2011a). For the disengagement label, a reliability analysis was performed over several annotation rounds on subsets of the 2008 ITSPOKE corpus by this and a second trained annotator, yielding 0.55 Kappa (this analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mel</context>
</contexts>
<marker>Pon-Barry, Shieber, 2011</marker>
<rawString>H. Pon-Barry and S. Shieber. 2011. Recognizing uncertainty in speech. EURASIP Journal on Advances in Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pon-Barry</author>
<author>K Schultz</author>
<author>E Owen Bratt</author>
<author>B Clark</author>
<author>S Peters</author>
</authors>
<title>Responding to student uncertainty in spoken tutorial dialogue systems.</title>
<date>2006</date>
<booktitle>International Journal ofArtificial Intelligence in Education,</booktitle>
<pages>16--171</pages>
<contexts>
<context position="14989" citStr="Pon-Barry et al., 2006" startWordPosition="2305" endWordPosition="2308">formed over several annotation rounds on subsets of the 2008 ITSPOKE corpus by this and a second trained annotator, yielding 0.55 Kappa (this analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mello et al., 2008) both illustrate the common finding that human annotators display low to moderate interannotator reliability for affect annotation, and both studies show that trained labelers yield the high</context>
</contexts>
<marker>Pon-Barry, Schultz, Bratt, Clark, Peters, 2006</marker>
<rawString>H. Pon-Barry, K. Schultz, E. Owen Bratt, B. Clark, and S. Peters. 2006. Responding to student uncertainty in spoken tutorial dialogue systems. International Journal ofArtificial Intelligence in Education, 16:171–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Porayska-Pomsta</author>
<author>M Mavrikis</author>
<author>H Pain</author>
</authors>
<title>Diagnosing and acting on student affect: the tutor’s perspective. User Modeling and User-Adapted Interaction:</title>
<date>2008</date>
<journal>The Journal of Personalization Research,</journal>
<pages>18--125</pages>
<contexts>
<context position="6397" citStr="Porayska-Pomsta et al., 2008" startWordPosition="973" endWordPosition="976">and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features used to detect disengagement also vary depending on</context>
<context position="15020" citStr="Porayska-Pomsta et al., 2008" startWordPosition="2309" endWordPosition="2312">tation rounds on subsets of the 2008 ITSPOKE corpus by this and a second trained annotator, yielding 0.55 Kappa (this analysis is described in detail elsewhere (Forbes-Riley et al., 2011)). Our Kappas indicate that user uncertainty and disengagement can both be annotated with moderate reliability in our dataset, on par with prior emotion annotation work (c.f., (Pon-Barry and Shieber, 2011)). Note however that the best way to label users’ internal affective state(s) is still an open question. Many system researchers (including ourselves) rely on trained labelers (e.g., (Pon-Barry et al., 2006; Porayska-Pomsta et al., 2008)) while others use selfreports (e.g., (Conati and Maclaren, 2009; Gratch et al., 2009; McQuiggan et al., 2008)). Both methods are problematic; for example both can be rendered inaccurate when users mask their true feelings. Two studies that have compared self-reports, peer labelers, trained labelers, and combinations of labelers (Afzal and Robinson, 2011; D’Mello et al., 2008) both illustrate the common finding that human annotators display low to moderate interannotator reliability for affect annotation, and both studies show that trained labelers yield the highest reliability on this task. D</context>
<context position="16690" citStr="Porayska-Pomsta et al., 2008" startWordPosition="2564" endWordPosition="2567">veloped a user disengagement detector to incorporate into our existing uncertainty-adaptive spoken dialogue system. The result will be a state of the art system that adapts to multiple affective states during the dialogue. 4.1 Binary DISE Label Our disengagement annotation scheme (ForbesRiley et al., 2011) was derived from empirical observations in our data but draws on prior work, including work mentioned in Section 2, appraisal theory-based emotion models (e.g., Conati and Maclaren (2009))4, and prior approaches to annotating disengagement or related states in tutoring (Lehman et al., 2008; Porayska-Pomsta et al., 2008). Briefly, our overall Disengagement label (DISE) is used for turns expressing moderate to strong disengagement towards the interaction, i.e., responses given without much effort or without caring about appropriateness. Responses might also be accompanied by signs of inattention, boredom, or irritation. Clear examples include answers spoken quickly in leaden monotone, with sarcastic or playful tones, or with off-task sounds such as rhythmic tapping or 4Appraisal theorists distinguish emotional behaviors from their underlying causes, arguing that emotions result from an evaluation of a context.</context>
</contexts>
<marker>Porayska-Pomsta, Mavrikis, Pain, 2008</marker>
<rawString>K. Porayska-Pomsta, M. Mavrikis, and H. Pain. 2008. Diagnosing and acting on student affect: the tutor’s perspective. User Modeling and User-Adapted Interaction: The Journal of Personalization Research, 18:125–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>The Empathetic Companion: A character-based interface that addresses users’ affective states.</title>
<date>2005</date>
<journal>International Journal of Applied Artificial Intelligence,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="1514" citStr="Prendinger and Ishizuka, 2005" startWordPosition="223" endWordPosition="226">y as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, </context>
</contexts>
<marker>Prendinger, Ishizuka, 2005</marker>
<rawString>H. Prendinger and M. Ishizuka. 2005. The Empathetic Companion: A character-based interface that addresses users’ affective states. International Journal of Applied Artificial Intelligence, 19(3):267–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rotaru</author>
<author>D Litman</author>
</authors>
<title>Discourse structure and performance analysis: Beyond the correlation.</title>
<date>2009</date>
<booktitle>In Proceedings 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<location>London, UK.</location>
<contexts>
<context position="30550" citStr="Rotaru and Litman, 2009" startWordPosition="4790" endWordPosition="4793">uto) and our gold standard DISE labels (manual), we first computed the total number of occurrences for each student, and then computed a bivariate Pearson’s correlation between this total and two different metrics of performance: learning gain (LG) and user satisfaction (US). In the tutoring domain, learning is the primary performance metric and as is common in this domain we compute it as normalized learning gain ((posttest score-pretest score)/(1- 12Spoken dialogue research has shown that redesigning a system in light of such correlational analysis can indeed yield performance improvements (Rotaru and Litman, 2009). 97 Table 3: Correlations between Disengagement and both Satisfaction and Learning in ITSPOKE Corpus (N=72 users) Measure Mean (SD) User Satisfaction Learning Gain R p R p Total Manual DISE 12.3 (7.3) -0.25 0.031 -0.35 0.002 Total Automatic DISE 12.6 (7.4) -0.26 0.029 -0.31 0.009 pretest score)). In spoken dialogue systems, user satisfaction is the primary performance metric and as is common in this domain we compute it by totaling over the user satisfaction survey scores.13 Table 3 shows first the mean and standard deviation for the DISE label over all students, the Pearson’s Correlation coe</context>
</contexts>
<marker>Rotaru, Litman, 2009</marker>
<rawString>M. Rotaru and D. Litman. 2009. Discourse structure and performance analysis: Beyond the correlation. In Proceedings 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schuller</author>
<author>R Muller</author>
<author>F Eyben</author>
<author>J Gast</author>
<author>B Hrnler</author>
<author>M Wollmer</author>
<author>G Rigoll</author>
<author>A Hthker</author>
<author>H Konosu</author>
</authors>
<title>Being bored? recognising natural interest by extensive audiovisual integration for real-life application.</title>
<date>2009</date>
<journal>Image and Vision Computing Journal, Special Issue on Visual and Multimodal Analysis of Human Spontaneous Behavior,</journal>
<pages>27--1760</pages>
<contexts>
<context position="1459" citStr="Schuller et al., 2009" startWordPosition="215" endWordPosition="218">orrelate with system performance in the same way as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performa</context>
<context position="3791" citStr="Schuller et al., 2009" startWordPosition="565" endWordPosition="568">ess. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting commercial applications (Wang and Hirschberg, 2011; Schuller et al., 2009a). In this paper we present a model for automatically detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model yields results on par with those of prior work. However, we argue that while intrinsic evaluations are necessary, they aren’t sufficient when immediate implementation </context>
<context position="6319" citStr="Schuller et al., 2009" startWordPosition="961" endWordPosition="964"> develop a detector of user disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarca</context>
<context position="8582" citStr="Schuller et al., 2009" startWordPosition="1306" endWordPosition="1309">schberg, 2011; Jeon et al., 2010); we will briefly compare our own results with these in Section 5. While all of the above-mentioned research has presented intrinsic evaluations of their disengagement modeling efforts that indicate a reasonable degree of accuracy as compared to a gold standard (e.g., manual coding), only a few have yet demonstrated that the model’s detected values are useful 92 in practice and/or are a reasonable substitute for the gold standard with respect to some practical objective (e.g., a relationship to performance). In particular, two studies (Bohus and Horvitz, 2009; Schuller et al., 2009a) have gone directly from intrinsic evaluation of (dis)engagement models to performing user studies with the implemented model, thereby bypassing other less expensive and less labor-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affe</context>
<context position="22159" citStr="Schuller et al., 2009" startWordPosition="3405" endWordPosition="3408">ent with J48 Decision Tree Algorithm Detecting the Binary DISE Label in the 2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented an</context>
</contexts>
<marker>Schuller, Muller, Eyben, Gast, Hrnler, Wollmer, Rigoll, Hthker, Konosu, 2009</marker>
<rawString>B. Schuller, R. Muller, F. Eyben, J. Gast, B. Hrnler, M. Wollmer, G. Rigoll, A. Hthker, and H. Konosu. 2009a. Being bored? recognising natural interest by extensive audiovisual integration for real-life application. Image and Vision Computing Journal, Special Issue on Visual and Multimodal Analysis of Human Spontaneous Behavior, 27:1760–1774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schuller</author>
<author>S Steidl</author>
<author>A Batliner</author>
</authors>
<title>The Interspeech</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th Annual Conference of the International Speech Communication Association (Interspeech), ISCA,</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="1459" citStr="Schuller et al., 2009" startWordPosition="215" endWordPosition="218">orrelate with system performance in the same way as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performa</context>
<context position="3791" citStr="Schuller et al., 2009" startWordPosition="565" endWordPosition="568">ess. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting commercial applications (Wang and Hirschberg, 2011; Schuller et al., 2009a). In this paper we present a model for automatically detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model yields results on par with those of prior work. However, we argue that while intrinsic evaluations are necessary, they aren’t sufficient when immediate implementation </context>
<context position="6319" citStr="Schuller et al., 2009" startWordPosition="961" endWordPosition="964"> develop a detector of user disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarca</context>
<context position="8582" citStr="Schuller et al., 2009" startWordPosition="1306" endWordPosition="1309">schberg, 2011; Jeon et al., 2010); we will briefly compare our own results with these in Section 5. While all of the above-mentioned research has presented intrinsic evaluations of their disengagement modeling efforts that indicate a reasonable degree of accuracy as compared to a gold standard (e.g., manual coding), only a few have yet demonstrated that the model’s detected values are useful 92 in practice and/or are a reasonable substitute for the gold standard with respect to some practical objective (e.g., a relationship to performance). In particular, two studies (Bohus and Horvitz, 2009; Schuller et al., 2009a) have gone directly from intrinsic evaluation of (dis)engagement models to performing user studies with the implemented model, thereby bypassing other less expensive and less labor-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affe</context>
<context position="22159" citStr="Schuller et al., 2009" startWordPosition="3405" endWordPosition="3408">ent with J48 Decision Tree Algorithm Detecting the Binary DISE Label in the 2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented an</context>
</contexts>
<marker>Schuller, Steidl, Batliner, 2009</marker>
<rawString>B. Schuller, S. Steidl, and A. Batliner. 2009b. The Interspeech 2009 Emotion Challenge. In Proceedings of the 10th Annual Conference of the International Speech Communication Association (Interspeech), ISCA, Brighton, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schuller</author>
<author>S Steidl</author>
<author>A Batliner</author>
<author>F Burkhardt</author>
<author>L Devillers</author>
<author>C Muller</author>
<author>S Narayanan</author>
</authors>
<title>The Interspeech 2010 Paralinguistic Challenge.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the International Speech Communication Assocation (Interspeech),</booktitle>
<pages>2794--2797</pages>
<location>Chiba, Japan,</location>
<contexts>
<context position="6250" citStr="Schuller et al., 2010" startWordPosition="949" endWordPosition="952">ed Work Our focus in this paper is on first using machine learning to develop a detector of user disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and ling</context>
<context position="7947" citStr="Schuller et al., 2010" startWordPosition="1204" endWordPosition="1207">nfluences dialogue patterns during interactions with an embodied agent that gives advice about healthy dieting. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al., 2000). Bohus and Horvitz (2009) study systems that attract and engage users for dynamic, multi-party dialogues in open-world settings. They model user intentions to engage the system with cues from facial sensors and the dialogue. Within recent spoken dialogue research, acoustic-prosodic, lexical and contextual features have been found to be effective detectors of disengagement (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010); we will briefly compare our own results with these in Section 5. While all of the above-mentioned research has presented intrinsic evaluations of their disengagement modeling efforts that indicate a reasonable degree of accuracy as compared to a gold standard (e.g., manual coding), only a few have yet demonstrated that the model’s detected values are useful 92 in practice and/or are a reasonable substitute for the gold standard with respect to some practical objective (e.g., a relationship to performance). In particular, two studies (Bohus and H</context>
<context position="22136" citStr="Schuller et al., 2010" startWordPosition="3401" endWordPosition="3404">ross-Validation Experiment with J48 Decision Tree Algorithm Detecting the Binary DISE Label in the 2008 ITSPOKE Corpus (N=7216 user turns) Algorithm Accuracy UA Precision UA Recall UA Fmeasure CC MLE Decision Tree 83.1% 68.9% 68.7% 68.8% 0.52 0.25 Majority Label 83.8% 41.9% 50.0% 45.6% – 0.27 Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges (Schuller et al., 2010; Schuller et al., 2009b) and made freely available in the openSMILE Toolkit (Florian et al., 2010). To date, however, these features have only decreased the crossvalidation performance of our models.8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of “response appropriateness” in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be </context>
</contexts>
<marker>Schuller, Steidl, Batliner, Burkhardt, Devillers, Muller, Narayanan, 2010</marker>
<rawString>B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers, C. Muller, and S. Narayanan. 2010. The Interspeech 2010 Paralinguistic Challenge. In Proceedings of the 11th Annual Conference of the International Speech Communication Assocation (Interspeech), pages 2794–2797, Chiba, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Shafran</author>
<author>M Riley</author>
<author>M Mohri</author>
</authors>
<title>Voice signatures.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>31--36</pages>
<publisher>St. Thomas, US Virgin Islands.</publisher>
<contexts>
<context position="1862" citStr="Shafran et al., 2003" startWordPosition="273" endWordPosition="276">se, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affective states. We previously showed that detecting </context>
<context position="9383" citStr="Shafran et al., 2003" startWordPosition="1425" endWordPosition="1428">r-intensive means of extrinsic evaluation to quantify their model’s usefulness–and potentially indicate its need to be further improved–before deployment with real users. Neither study reports statistically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tutor We develop and eval</context>
</contexts>
<marker>Shafran, Riley, Mohri, 2003</marker>
<rawString>I. Shafran, M. Riley, and M. Mohri. 2003. Voice signatures. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 31–36, St. Thomas, US Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sidner</author>
<author>C Lee</author>
</authors>
<title>An architecture for engagement in collaborative conversations between a robot and a human.</title>
<date>2003</date>
<tech>Technical Report TR2003-12, MERL.</tech>
<contexts>
<context position="6444" citStr="Sidner and Lee, 2003" startWordPosition="981" endWordPosition="984"> prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features used to detect disengagement also vary depending on system domain and application. For example, Si</context>
<context position="7061" citStr="Sidner &amp; Lee (2003)" startWordPosition="1072" endWordPosition="1075">03; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze avoidance, finger tapping, humming, sarcasm, etcetera. The features used to detect disengagement also vary depending on system domain and application. For example, Sidner &amp; Lee (2003) are interested in modeling more natural and collaborative human-robot interactions during basic conversations. They define an algorithm for the engagement process that involves appropriate eye gaze and turn-taking. Martalo et al. (2008) study how user engagement influences dialogue patterns during interactions with an embodied agent that gives advice about healthy dieting. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al., 2000). Bohus and Horvitz (2009) study systems that attract and engage users for dynamic, multi-party dialogues in ope</context>
</contexts>
<marker>Sidner, Lee, 2003</marker>
<rawString>C. Sidner and C. Lee. 2003. An architecture for engagement in collaborative conversations between a robot and a human. Technical Report TR2003-12, MERL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>N Coccaro</author>
<author>R Bates</author>
<author>P Taylor</author>
<author>C Van EssDykema</author>
<author>K Ries</author>
<author>E Shriberg</author>
<author>D Jurafsky</author>
<author>R Martin</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Coccaro, Bates, Taylor, Van EssDykema, Ries, Shriberg, Jurafsky, Martin, Meteer, 2000</marker>
<rawString>A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van EssDykema, K. Ries, E. Shriberg, D. Jurafsky, R. Martin, and M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Tsukahara</author>
<author>N Ward</author>
</authors>
<title>Responding to subtle, fleeting changes in the user’s internal state.</title>
<date>2001</date>
<booktitle>In Proceedings of the SIG-CHI on Human factors in computing systems,</booktitle>
<pages>77--84</pages>
<publisher>ACM.</publisher>
<location>Seattle, WA.</location>
<contexts>
<context position="9588" citStr="Tsukahara and Ward, 2001" startWordPosition="1462" endWordPosition="1465">ically significant improvements in system performance as a result of detecting user (dis)engagement. Finally, while substantial spoken dialogue and affective systems research has shown that users display a range of affective states while interacting with a system (e.g. (Schuller et al., 2009b; Conati and Maclaren, 2009; Batliner et al., 2008; Devillers and Vidrascu, 2006; Lee and Narayanan, 2005; Shafran et al., 2003; Ang et al., 2002)), to date only a few affective systems have been built that detect and adapt to multiple user affective states (e.g., (D’Mello et al., 2010; Aist et al., 2002; Tsukahara and Ward, 2001)), and most of these have been deployed with crucial natural language processing components “wizarded” by a hidden human agent (e.g., who performs speech recognition or affect annotation on the user turns); moreover, none have yet shown significant improvements in system performance as a result of adapting to multiple user affective states. 3 ITSPOKE: Spoken Dialogue Tutor We develop and evaluate our disengagement detector using a corpus of spoken dialogues from a 2008 controlled experiment evaluating our uncertaintyadaptive spoken dialogue tutoring system, ITSPOKE (Intelligent Tutoring SPOKEn</context>
</contexts>
<marker>Tsukahara, Ward, 2001</marker>
<rawString>W. Tsukahara and N. Ward. 2001. Responding to subtle, fleeting changes in the user’s internal state. In Proceedings of the SIG-CHI on Human factors in computing systems, pages 77–84, Seattle, WA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>P W Jordan</author>
<author>C Ros´e</author>
<author>D Bhembe</author>
<author>M B¨ottner</author>
<author>A Gaydos</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>M Ringenberg</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>R Wilson</author>
</authors>
<title>The architecture of Why2-Atlas: A coach for qualitative physics essay writing.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Intelligent Tutoring Systems.</booktitle>
<marker>VanLehn, Jordan, Ros´e, Bhembe, B¨ottner, Gaydos, Makatchev, Pappuswamy, Ringenberg, Roque, Siler, Srivastava, Wilson, 2002</marker>
<rawString>K. VanLehn, P. W. Jordan, C. Ros´e, D. Bhembe, M. B¨ottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. Intl. Conf. on Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vidrascu</author>
<author>L Devillers</author>
</authors>
<title>Detection of reallife emotions in dialogs recorded in a call center.</title>
<date>2005</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1544" citStr="Vidrascu and Devillers, 2005" startWordPosition="227" endWordPosition="230"> labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to significantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 1 Introduction Spoken dialogue systems that can detect and adapt to user affect1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). How</context>
</contexts>
<marker>Vidrascu, Devillers, 2005</marker>
<rawString>L. Vidrascu and L. Devillers. 2005. Detection of reallife emotions in dialogs recorded in a call center. In Proceedings of INTERSPEECH, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>A Rudnicky</author>
<author>R Prasad</author>
<author>J Aberdeen</author>
<author>E Bratt</author>
<author>J Garofolo</author>
<author>H Hastie</author>
<author>A Le</author>
<author>B Pellom</author>
<author>A Potamianos</author>
<author>R Passonneau</author>
<author>S Roukos</author>
<author>G Sanders</author>
<author>S Seneff</author>
<author>D Stallard</author>
</authors>
<title>DARPA communicator: Cross-system results for the 2001 evaluation.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="13515" citStr="Walker et al., 2002" startWordPosition="2074" endWordPosition="2077">al sur3an outgrowth of Festival (Black and Taylor, 1997). 93 vey score was 60.9 (out of 80), with a standard deviation of 8.5. While the statements themselves are listed elsewhere (Forbes-Riley and Litman, 2009), 9 statements concern the tutoring domain (e.g., The tutor was effective/precise/useful), 7 of which were taken from (Baylor et al., 2003) and 2 of which were created for our system. 3 statements concern user uncertainty levels and were created for our system. 4 statements concern the spoken dialogue interaction (e.g., It was easy to understand the tutor’s speech) and were taken from (Walker et al., 2002). Our survey has also been incorporated into other recent work exploring user satisfaction in spoken dialogue computer tutors (Dzikovska et al., 2011). In Section 6 we discuss how user scores on these instruments are used to measure system performance. See (Forbes-Riley and Litman, 2011a) for further details of ITSPOKE and the 2008 experiment. Following the experiment, the entire corpus was manually labeled for (in)correctness (correct, incorrect), (un)certainty (CER, UNC) and (dis)engagement (ENG, DISE) by one trained annotator. Table 1 shows the distribution of the labeled turns in the 2008 </context>
</contexts>
<marker>Walker, Rudnicky, Prasad, Aberdeen, Bratt, Garofolo, Hastie, Le, Pellom, Potamianos, Passonneau, Roukos, Sanders, Seneff, Stallard, 2002</marker>
<rawString>M. Walker, A. Rudnicky, R. Prasad, J. Aberdeen, E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potamianos, R. Passonneau, S. Roukos, G. Sanders, S. Seneff, and D. Stallard. 2002. DARPA communicator: Cross-system results for the 2001 evaluation. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Hirschberg</author>
</authors>
<title>Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning.</title>
<date>2011</date>
<booktitle>In Proc. 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>152--161</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="3768" citStr="Wang and Hirschberg, 2011" startWordPosition="561" endWordPosition="564">quently improving task success. Although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated the automatic detection of both user uncertainty (e.g. (Drummond and Litman, 2011; PonBarry and Shieber, 2011; Paek and Ju, 2008; Alwan et al., 2007)) and user disengagement (e.g., (Schuller 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics et al., 2010; Wang and Hirschberg, 2011; Schuller et al., 2009a)), to improve system performance. The detection of user disengagement in particular has received substantial attention in recent years, due to growing awareness of its potential for negatively impacting commercial applications (Wang and Hirschberg, 2011; Schuller et al., 2009a). In this paper we present a model for automatically detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model yields results on par with those of prior work. However, we argue that while intrinsic evaluations are necessary, they aren’t sufficient when im</context>
<context position="6277" citStr="Wang and Hirschberg, 2011" startWordPosition="953" endWordPosition="956">is paper is on first using machine learning to develop a detector of user disengagement for spoken dialogue systems, and then evaluating its usefulness as fully as possible prior to its implementation and deployment with real users. Disengaged users are highly undesirable in human-computer interaction because they increase the potential for user dissatisfaction and task failure; thus over the past decade there has already been substantial prior work focused on detecting user disengagement and the closely related states of boredom, motivation and lack of interest (e.g., (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010; Schuller et al., 2009a; Bohus and Horvitz, 2009; Martalo et al., 2008; Porayska-Pomsta et al., 2008; Kapoor and Picard, 2005; Sidner and Lee, 2003; Forbes-Riley and Litman, 2011b)). Within this work, specific affect definitions vary slightly with the intention of being coherent within the application and domain and being relevant to the specific adaptation goal (Martalo et al., 2008). However, affective systems researchers generally agree that disengaged users show little involvement in the interaction, and often display facial, gestural and linguistic signals such as gaze</context>
<context position="7974" citStr="Wang and Hirschberg, 2011" startWordPosition="1208" endWordPosition="1211">erns during interactions with an embodied agent that gives advice about healthy dieting. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al., 2000). Bohus and Horvitz (2009) study systems that attract and engage users for dynamic, multi-party dialogues in open-world settings. They model user intentions to engage the system with cues from facial sensors and the dialogue. Within recent spoken dialogue research, acoustic-prosodic, lexical and contextual features have been found to be effective detectors of disengagement (Schuller et al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010); we will briefly compare our own results with these in Section 5. While all of the above-mentioned research has presented intrinsic evaluations of their disengagement modeling efforts that indicate a reasonable degree of accuracy as compared to a gold standard (e.g., manual coding), only a few have yet demonstrated that the model’s detected values are useful 92 in practice and/or are a reasonable substitute for the gold standard with respect to some practical objective (e.g., a relationship to performance). In particular, two studies (Bohus and Horvitz, 2009; Schuller et a</context>
<context position="23788" citStr="Wang and Hirschberg, 2011" startWordPosition="3659" endWordPosition="3662">d F-measure because they are the standard measures used to evaluate current affect recognition technology, particularly for unbalanced two-class problems (Schuller et al., 2009b). In addition, we use the cross correlation (CC) measure and mean linear error (MLE) because these metrics were used in recent work for evaluating disengagement (level of interest) detectors for the Interspeech 2010 challenge (Schuller et 8We also tried using our automatic UNC label as a feature in our DISE model, but our results weren’t significantly improved. 9simply ((Precision(DISE) + Precision(ENG))/2) al., 2010; Wang and Hirschberg, 2011; Jeon et al., 2010)).10 Note however that the Interspeech 2010 task differs from ours not only in the corpus and features, but also in the learning task: they used regression to detect a continuous level of interest ranging from 0 to 1, while we detect a binary class. Thus comparison between our results and those are only suggestive rather than conclusive. As shown in Table 2, we also compare our results with those of majority class (ENG) labeling of the same turns. Since (7216-1170)/7216 user turns in the corpus are engaged (recall Table 1), always selecting the majority class (ENG) label fo</context>
<context position="25226" citStr="Wang and Hirschberg, 2011" startWordPosition="3912" endWordPosition="3915">racy, this is not surprising given the steep skew in class distribution, and our learned model significantly outperforms the baseline with respect to all the other measures (p&lt;.001).11 Our CC and MLE results are on par with the best results from the state-of-the-art systems competing in the 2010 Interspeech Challenge, where the task was to detect level of interest. In particular, the winner obtained a CC of 0.428 (higher numbers are better) and an MLE of 0.146 (lower numbers are better) (Jeon et al., 2010), while a subsequent study yielded a CC of 0.480 and an MLE of 0.131 on the same corpus (Wang and Hirschberg, 2011). Our results are also on par with the best results of the other prior research on detecting disengagement discussed in Section 2 that detects a small number of disengagement classes and reports accuracy and/or recall and precision. For example, (Martalo et al., 2008) report average precision of 75% and recall 10Pearson product-moment correlation coefficient (CC) is a measure of the linear dependence that is widely used in regression settings. MLE is a regression performance measure for the mean absolute error between an estimator and the true value. 11CC is undefined for majority class labeli</context>
</contexts>
<marker>Wang, Hirschberg, 2011</marker>
<rawString>W. Wang and J. Hirschberg. 2011. Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning. In Proc. 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 152–161, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Wang</author>
<author>W L Johnson</author>
<author>R E Mayer</author>
<author>P Rizzo</author>
<author>E Shaw</author>
<author>H Collins</author>
</authors>
<title>The politeness effect: Pedagogical agents and learning outcomes.</title>
<date>2008</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>66</volume>
<issue>2</issue>
<contexts>
<context position="2026" citStr="Wang et al., 2008" startWordPosition="298" endWordPosition="301">t1 are fast becoming reality (Schuller et al., 2009b; Batliner et al., 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee Now at Univ. Toronto: jdrummond@cs.toronto.edu 1We use affect for emotions and attitudes that affect how users communicate. Other speech researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). 91 and Narayanan, 2005; Shafran et al., 2003). The benefits are clear: affect-adaptive systems have been shown to increase task success (Forbes-Riley and Litman, 2011a; D’Mello et al., 2010; Wang et al., 2008) or improve other system performance metrics such as user satisfaction (Liu and Picard, 2005; Klein et al., 2002). However, to date most affective systems researchers have focused either only on affect detection, or only on detecting and adapting to a single affective state. The next step is thus to develop and evaluate spoken dialogue systems that detect and respond to multiple affective states. We previously showed that detecting and responding to user uncertainty during spoken dialogue computer tutoring significantly improves task success (Forbes-Riley and Litman, 2011a). We are now taking </context>
</contexts>
<marker>Wang, Johnson, Mayer, Rizzo, Shaw, Collins, 2008</marker>
<rawString>N. Wang, W.L. Johnson, R. E. Mayer, P. Rizzo, E. Shaw, and H. Collins. 2008. The politeness effect: Pedagogical agents and learning outcomes. International Journal of Human-Computer Studies, 66(2):98–112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>