<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000599">
<title confidence="0.9853205">
The (Non)Utility of Predicate-Argument Frequencies for Pronoun
Interpretation
</title>
<author confidence="0.943065">
Andrew Kehler* Douglas Appelt Lara Taylor* Aleksandr Simma†
</author>
<address confidence="0.348035">
UC San Diego SRI International UC San Diego UC San Diego
</address>
<email confidence="0.997044">
akehler@ucsd.edu appelt@ai.sri.com lmtaylor@ucsd.edu asimma@ucsd.edu
</email>
<sectionHeader confidence="0.980015" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917857142857">
State-of-the-art pronoun interpretation sys-
tems rely predominantly on morphosyntac-
tic contextual features. While the use of
deep knowledge and inference to improve
these models would appear technically in-
feasible, previous work has suggested that
predicate-argument statistics mined from
naturally-occurring data could provide a
useful approximation to such knowledge.
We test this idea in several system configu-
rations, and conclude from our results and
subsequent error analysis that such statis-
tics offer little or no predictive information
above that provided by morphosyntax.
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926952380952">
The last several years has seen a number of works
that use weight-based systems (trained either man-
ually or via supervised learning) for pronoun in-
terpretation, in addition to others that have ad-
dressed the broader task of entity-level coreference
(see Mitkov (2002) for a useful survey). These sys-
tems typically rely on a variety of morphosyntactic
factors that have been posited in the literature to
affect the interpretation of pronouns in naturally-
occurring discourse, including gender and number
agreement, the distance between the pronoun and
antecedent, the grammatical positions of the pro-
noun and antecedent, and the linguistic form of the
antecedent, among others. A common refrain is that
the performance of systems that rely on such fea-
tures is plateauing, and that further progress will re-
quire the use of world knowledge and inference (ibid.,
Ch. 9, inter alia). World knowledge, after all, would
seem to play a role in determining that the referent of
it in example (1) is the entity denoted by his industry
rather than Glendening’s initiative or the edge.
</bodyText>
<affiliation confidence="0.9245245">
∗Department of Linguistics.
†Department of Computer Science and Engineering.
</affiliation>
<bodyText confidence="0.993221492957747">
(1) He worries that Glendening’s initiative could
push his industry over the edge, forcing it to
shift operations elsewhere.
Of course, no well-suited knowledge base and accom-
panying inference procedure exists that can deliver
such a capability robustly in an open domain.
In lieu of this capability, previous authors have
suggested that what can be viewed as a more su-
perficial form of semantic information – predicate-
argument statistics mined from naturally-occurring
data – could be used to capture certain selectional
regularities. For instance, such statistics might re-
veal that forcing industry is a more likely verb-
object combination in naturally-occurring data than
forcing initiative or forcing edge. Assuming that
such statistics imply that industries are more likely
to be forced in the real world than are initiatives or
edges, this information could be taken to establish a
preference for his industry as the antecedent of it in
(1). While there will always be cases that require ar-
bitrarily deep knowledge for their interpretation, the
empirical question of how far one can go by relying
on this sort of selectional information remains.
Our point of departure is the work of Lappin
and Leass (1994, henceforth L&amp;L) and Dagan et
al. (1995). (See also Dagan and Itai (1990).) L&amp;L
demonstrated with a system called RAP that a
(manually-tuned) weight-based scheme for integrat-
ing pronoun interpretation preferences can achieve
high performance on real data, in their case, 86%
accuracy on a corpus of computer training manu-
als.&apos; Dagan et al. (1995) then developed a postpro-
cessor based on predicate-argument statistics that
was used to override RAP’s decision when it failed
to express a clear preference between two or more
antecedents, which resulted in a modest rise in per-
&apos;Kennedy and Boguraev (1996, henceforth, K&amp;B)
adapted L&amp;L’s algorithm to rely on far less syntac-
tic analysis (noun phrase identification and rudimentary
grammatical role marking), with performance in the 75%
range on mixed genres.
formance (2.5%).2 Because RAP is symbolic, the
two systems were necessarily coupled in a black-
box manner. They noted, however, that if one had
a statistically-driven pronoun interpretation system,
co-occurrence information could be modeled along-
side morphosyntactic information:
“A promising direction for future research
is the development of an empirically based
model for salience criteria analogous to the
one that we constructed for lexical prefer-
ence. The integration of these models using
a probabilistic decision procedure will hope-
fully yield an optimized integrated system
for anaphora resolution.” (p. 643)
In this work we set out to evaluate Dagan et al.’s
proposal. Indeed, the weight-combination scheme of
L&amp;L is suggestive of a particular approach to super-
vised learning – maximum entropy (MaxEnt) – in
which such a system of weights is inferred from max-
imum likelihood counts on annotated data. Using
MaxEnt, we trained a system based on an optimized
set of morphosyntactic features and augmented it
with predicate-argument statistics in two scenarios:
(i) one mimicking the Dagan et al. postprocessor,
and (ii) one in which the predicate-argument statis-
tics were represented as features alongside the mor-
phosyntactic features. Our results and subsequent
error analysis suggest, however, that such statistics
offer little or no predictive information above that
provided by morphosyntax.
</bodyText>
<sectionHeader confidence="0.936108" genericHeader="introduction">
2 Corpora Used
</sectionHeader>
<bodyText confidence="0.999872266666667">
The training and test data sets came from the news-
paper and newswire segments of the Automatic Con-
tent Extraction (ACE) program corpus. The train-
ing data contained 2773 annotated third-person pro-
nouns, and the test data (the February 2002 evalua-
tion set) contained 762 annotated third-person pro-
nouns. The performance statistics on the test data
reported here are from the only time an evaluation
with this data was performed; progress during devel-
opment was estimated solely via jackknifing on the
training data.
The annotated pronouns included only those that
were ACE “markables”, i.e., ones that referred to en-
tities of the following types: PERSONS, ORGANIZA-
TIONS, GEOPOLITICALENTITIES (politically defined
</bodyText>
<footnote confidence="0.925468">
2The difference amounted to 9 additional correct pre-
dictions in a corpus of 360 examples. They express a be-
lief that the improvement is real, but acknowledge that
they would need twice as many examples in their corpus
to reach statistical significance.
</footnote>
<bodyText confidence="0.999922176470588">
geographical regions, their governments, or their
people), LOCATIONS, and FACILITIES. Thus, there
were pronouns in both the development and (pre-
sumably) test sets for which there were no annota-
tions. As such, certain problems that real-world sys-
tems face, such as non-referential (e.g., ‘pleonastic’)
pronouns and pronouns that refer to eventualities,
did not have to be dealt with. (However, these pro-
nouns were possible antecedents to other pronouns,
and thus were sometimes mistakenly selected as the
correct antecedent.) Thus, our results are not nec-
essarily comparable to those of a system that deals
with these difficulties (although previous work varies
a fair bit on how their datasets were filtered in this re-
gard). Our main purpose here is to establish a state-
of-the-art baseline with which to assess the contribu-
tion of predicate-argument frequency information.
</bodyText>
<sectionHeader confidence="0.989449" genericHeader="method">
3 Learning Algorithms
</sectionHeader>
<bodyText confidence="0.997226444444444">
We implemented three pronoun interpretation sys-
tems: a MaxEnt model, a Naive Bayes model, and
a version of the Hobbs algorithm as a baseline. Our
experimentation was driven predominantly using the
MaxEnt system using an n-fold jackknifing paradigm
(n was typically three). Naive Bayes was imple-
mented toward the end of the project as a machine
learning baseline. Both machine learning algorithms
were trained as binary coreference classifiers, that
is, the examples provided to them consisted of pair-
ings of a pronoun and a possible antecedent phrase,
along with a binary coreference outcome determined
from the annotated keys. Thus, for a given pronoun
there was one example generated for each possible
antecedent phrase. So as to focus learning on only
the coreferential phrase that is most likely to have
been directly responsible for a given pronominaliza-
tion, all coreferential phrases except the closest in
terms of Hobbs distance (discussed later) were elimi-
nated before training. Because we are ultimately in-
terested in identifying the correct antecedent among
a list of possible ones, during testing the antecedent
assigned the highest probability was chosen.
These systems received as input the results of
SRI’s TExTPRO system, a chunk-style shallow parser
capable of recognizing low-level constituents (noun
groups, verb groups, etc.). No difficult attachments
are attempted, and the results are errorful. There
was no human-annotated linguistic information in
the input. The systems are described further below.
Maximum Entropy Modeling As previously in-
dicated, the weight-based scheme of L&amp;L suggests
MaxEnt modeling (Berger et al., 1996) as a particu-
larly natural choice for a machine learning approach.
In MaxEnt, the parameters of an exponential model
of the following form are estimated:
</bodyText>
<equation confidence="0.776384">
eK λifi(x,y)
</equation>
<bodyText confidence="0.9999276">
The variable y represents the outcome (coreference
or not) and x represents the context. There is
one value for each feature that predicts coreference
behavior, represented by the parameters λ1, ..., λn,
which are Lagrange multipliers that constrain the ex-
pected value of each feature in the model to be the
values found in the distribution of the training data.
(The fi(x, y) are indicator functions which equal 1
when the corresponding feature is present, and 0
otherwise.) The desired values for these parame-
ters are obtained by maximizing the likelihood of
the training data with respect to the model.3 Thus,
whereas L&amp;L’s RAP system uses an additive system
of weights that is trained manually, the MaxEnt sys-
tem learns a multiplicative system of weights auto-
matically. One can view the MaxEnt system as yield-
ing a probabilistic notion of antecedent salience: The
salience value assigned to a potential antecedent of
a given pronoun is just the probability that Maxent
assigns to the outcome of coreference.
Naive Bayes In Naive Bayes modeling, a Bayesian
probability distribution is estimated under a strong
assumption: that all of the features are conditionally
independent given the target value. Thus given n
features xi with respect to the context x, we have:
</bodyText>
<equation confidence="0.99922525">
p(y|x) = p(y)p(x|y) ≈ p(y) Hn i=1 p(xi|y)
n
argmax p(y) p(xi|y)
y∈{0,1} i=1
</equation>
<bodyText confidence="0.9998966">
For most natural language processing scenarios, in-
cluding ours, this independence assumption is almost
certainly false. Nonetheless, Naive Bayes models
seem to work well in practice when used as classifiers.
That is, the choice that receives the highest probabil-
ity (relative to the other choices) is often the correct
one even though the actual probabilities the model
generates may not be very good. These models have
the advantage that they are efficiently trained; only
a single pass through the training data is necessary.
</bodyText>
<footnote confidence="0.84053775">
3The results reported here were produced by using the
IMPROVED ITERATIVE SCALING algorithm with binary-
valued features. We also experimented with real-valued
features, with highly similar results on jackknifed data.
</footnote>
<bodyText confidence="0.999831666666667">
Hobbs Algorithm We also implemented a version
of Hobbs’s (1978) well-known pronoun interpretation
algorithm as a baseline, in which no machine learning
is involved. His algorithm takes the syntactic repre-
sentations of the sentences up to and including the
current sentence as input, and performs a search for
an antecedent noun phrase on these trees. Since our
shallow parsing system does not build full syntactic
trees for the input, we developed a version that does
a simple search through the list of noun groups recog-
nized. In accordance with Hobbs’s search procedure,
noun groups are searched in the following order: (i)
in the current sentence from right-to-left, starting
with the first noun group to the left of the pronoun,
(ii) in the previous sentence from left-to-right, (iii) in
two sentences prior from left-to-right, and (iv) in the
current sentence from left-to-right, starting with the
first noun group to the right of the pronoun (for cat-
aphora). The first noun group that agrees with the
pronoun with respect to number, gender, and person
is chosen as the antecedent.
</bodyText>
<sectionHeader confidence="0.996711" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.977626285714286">
Our automatically trained systems employed a set of
HARD CONSTRAINTS and SOFT FEATURES. Hard con-
straints are used to weed out potential antecedents
before they are sent to the machine learning algo-
rithm. There are only two such constraints, one
based on number agreement and one based on gen-
der agreement. Both are conservative in their appli-
cation. The soft features are used by the machine
learning algorithm. After considerable experimenta-
tion we settled on a set of forty such features, not
including predicate-argument features that will be
described in Section 5. These features fall into five
categories, listed here with abbreviations that will be
used in the tables given in Section 6:
Gender Agreement (gend): Includes features to
test a strict match of gender (e.g., a male pro-
noun and male antecedent), as well as mere
compatibility (e.g., a male pronoun with an an-
tecedent of unknown gender). These features
are more liberal than the gender-based hard con-
straint mentioned above.
Number Agreement (num): Includes features to
test a strict match of number (e.g., a singu-
lar pronoun and singular antecedent), as well
as mere compatibility (e.g., a singular pro-
noun with an antecedent of unknown number).
These features are likewise more liberal than the
number-based hard constraint mentioned above.
</bodyText>
<equation confidence="0.998340666666667">
p(y|x) = i λifi(x,y)
y e
p(x) p(x)
</equation>
<bodyText confidence="0.991191541666667">
The context x is constant for each outcome y, so we
only need to find:
Distance (dist): Includes features pertaining to
the distance between the pronoun and the po-
tential antecedent. Examples include the num-
ber of sentences between them and the “Hobbs
distance”, that is, the number of noun groups
that Hobbs’s search algorithm has to skip be-
fore the potential antecedent is found (Hobbs,
1978; Ge et al., 1998).
Grammatical Role (pos): Includes features per-
taining to the syntactic position of the potential
antecedent. Examples include whether the po-
tential antecedent appears to be the subject or
object of a verb, and whether the potential an-
tecedent is embedded in a prepositional phrase.
Linguistic Form (lform): Includes features per-
taining to the referential form of the potential
antecedent, e.g., whether it is a proper name,
definite description, indefinite NP, or a pronoun.
The values of these features – computed from our
system’s errorful shallow constituent parses – com-
prised the input to the learning algorithms, along
with the outcome as indicated by the annotated key.
</bodyText>
<sectionHeader confidence="0.990948" genericHeader="method">
5 Predicate-Argument Frequencies
</sectionHeader>
<bodyText confidence="0.997413725806452">
With a trained statistical model for pronoun inter-
pretation in hand, we can now consider the use of
predicate-argument statistics to improve it. Con-
sider sentence (1) again, repeated as (2).
(2) He worries that Glendening’s initiative could
push his industry over the edge, forcing it to
shift operations elsewhere.
Suppose that our system selects the edge as the an-
tecedent of it instead of his industry. It turns out
that in a large corpus of shallowly-parsed data (par-
ticularly the newswire subset of the TDT-2 corpus,
see below), industr(y|ies) appears nine times as the
head of the object noun phrase of force (in its var-
ious number/tense combinations), whereas edge(s)
never does.4 So by collecting predicate-argument co-
occurrence statistics, one could extract the “knowl-
edge” that industries are (statistically speaking)
more likely to be forced than edges are, and pos-
sibly use this information to change the prediction
of the statistical model.
We utilized three types of predicate-argument
statistics in our experiments: subject-verb, verb-
object, and possessive-noun. We processed the entire
4Likewise, the subject-verb combination industr(ylies)
shift occurs three times in the corpus whereas edge(s)
shift does not.
newswire subset of the Topic Detection and Tracking
(TDT-2) corpus with TEXTPRO, which resulted in
1,321,072 subject-verb relationships, 1,167,189 verb-
object relationships, and 301,477 possessive-noun re-
lationships. Words were categorized by their lem-
mas when available, and proper names for each of
the ACE entity types were classified into respective
classes (i.e., proper person names all counted as in-
stances of PROPER PERSON).
While counts were collected for a broad range
of predicate-argument combinations, there were still
many combinations that were only seen once or
twice, and certainly other possible combinations ex-
ist that were not seen at all. The distribution
that these statistics yield therefore needed to be
smoothed. We took two approaches to smooth-
ing. First, because Dagan et al. used Good-Turing
smoothing in their experiments, we did likewise so
as to replicate their work as closely as possible. Sec-
ond, we tried an approach based on the distributional
clustering method of Pereira et al. (1993). This
method yielded word classes that offered more ro-
bust count approximations for their member words.
However, both methods yielded similar results when
embedded in the larger system, and so we will report
on the results of using Good-Turing so as to remain
more directly comparable to Dagan et al.
The smoothed predicate-argument statistics were
employed in two ways. First, we built a postpro-
cessing filter modeled directly on Dagan et al.’s sys-
tem. Their implementation made use of two equa-
tions. The first computes the frequency with which
a candidate head noun C is found with the predi-
cate word A, normalized by the number of times C
is found alone, so as to not bias the statistic towards
words that are common in isolation:
</bodyText>
<equation confidence="0.996247">
stat(C) = P(tuple(C, A)  |C) = freq[tuple(C, A)]
freq(C)
</equation>
<bodyText confidence="0.99996275">
The second equation then weighs the difference in
statistical co-occurrence against the different salience
values assigned by the pronoun interpretation mod-
ule for two competing candidates C1 and C2:
</bodyText>
<listItem confidence="0.4230855">
ln Cstat(C2)1 &gt; K × [salience(C1) − salience(C2)]
stat(C1) J
</listItem>
<bodyText confidence="0.999907590909091">
The parameter K determines the threshold at which
statistical preferences supersede salience preferences.
In our implementation, the measure of salience is
simply the probability of coreference assigned by the
statistical model. Another parameter max sets a
threshold for the maximum difference between the
salience values for the two candidates; any pair for
which this difference exceeds max will not be con-
sidered. For each combination of feature sets that
we evaluated (see Section 6), we performed addi-
tional experiments to determine the optimal values
of K and max. To keep consistent with Dagan et al.,
statistics were not used (here or in the other MaxEnt
system) for potential antecedents that were them-
selves pronominal. To properly use statistics in such
cases, the system would need to have access to an
antecedent for the pronoun that has a lexical head;
neither model was given access to such information.
In our second approach, we simply developed fea-
tures that represent the magnitude of the predicate-
argument statistics and utilized them during Max-
Ent training along with the morphosyntactic features
described earlier. The statistics were normalized by
dividing them by the total counts for the head of
the potential antecedent in the relevant predicate-
argument configuration.5
In certain respects these different system config-
urations mirror questions about pronoun interpre-
tation that linger in the theoretical and psycholin-
guistics literature. A result showing that the post-
processing filter version works best might provide
evidence, as has been suggested, that people pri-
marily use morphosyntactic features to resolve pro-
nouns, relying on semantic information only when
more than one possibility remains active. A result
showing that the integrated version works best might
suggest that semantic information is used in concert
with morphosyntactic information. Finally, a result
showing that neither version improves performance
might suggest that morphosyntactic information is
the dominant determinant of pronoun interpretation,
and/or that any semantic information utilized is not
obtained primarily from superficial cues. The results
are reported in the next section.
</bodyText>
<sectionHeader confidence="0.999829" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.992301459016394">
Our final MaxEnt system used 40 features, which
were categorized into five classes in Section 4. To
get a sense for the relative contributions of each fea-
ture type, we ran evaluations with all 25 (32) possible
combinations of these five groups. We first report re-
sults on the held-out training data, and then provide
the blind test results. Table 1 provides the results
on the held-out sections of the training data dur-
ing 3-fold jackknifing for a sample of five of these 32
combinations. The four rightmost columns represent
the results from: (i) MaxEnt with no frequency fea-
tures (MaxEnt), (ii) MaxEnt with frequency features
5Experiments with unnormalized counts were also run
on jackknifed data with similar results.
included during training (MaxEnt-Features), (iii)
MaxEnt with Dagan et al. postprocessing (MaxEnt-
Postprocessing), and (iv) Naive Bayes without fre-
quencies. Experiments with n-fold jackknifing for
other values of n produced similar results. Dagan
et al. postprocessing was not attempted with Naive
Bayes since the postprocessor makes crucial use of
the probabilities the model assigns to competing an-
tecedents, and as previously mentioned, the actual
probabilities assigned by Naive Bayes are not neces-
sarily reliable due to the independence assumptions
it makes.
The testing phase breaks ties with respect to the
order imposed by Hobbs’s algorithm. In the case in
which no features were used during “training” (see
the first row of Table 1, columns 2 and 5), the models
will produce the same probability for each possible
antecedent. Thus, these experiments reduce to using
the Hobbs algorithm, which, performing at 68.23%
accuracy,6 provides a nontrivial baseline. As can be
seen, adding groups of additional features incremen-
tally improves performance, up to a final result of
76.16% for MaxEnt using all morphosyntactic fea-
tures, and a comparable 76.24% for Naive Bayes.
In the end, the predicate-argument statistics pro-
vided little if any value, used either as features dur-
ing MaxEnt training or for Dagan et al. postpro-
cessing. In the best-performing MaxEnt system con-
figuration (see bottom row), the statistics improve
performance by less than 0.5%. Interestingly, perfor-
mance was hurt when only statistical features were
used (65.71% in MaxEnt-Features and 66.25% in
MaxEnt-Postprocessing) as compared to none at all
(68.23%). Whereas the Hobbs algorithm ranks all of
the potential antecedents when no features are used,
it only breaks ties in the MaxEnt-Features system
that remain after statistical features order the po-
tential antecedents, and the MaxEnt-Postprocessor
system uses statistics to rerank the Hobbs ordering
between potential antecedents after the fact. This
reranking proved detrimental in both cases.
Table 2 provides the final results of blind test eval-
uation for the same five combinations of feature sets.
The final result of the system without predicate-
argument statistics was 75.72%, which is presumably
reasonable performance considering that the system
does not rely on fully-parsed input and lacks access
</bodyText>
<footnote confidence="0.925948">
6All results are reported here in terms of accuracy,
that is, the number of pronouns correctly resolved divided
by the total number of pronouns read in. Correctness is
defined with respect to anaphor-antecedent relationships:
a chosen antecedent is correct if the ACE keys place the
pronoun and antecedent in the same coreference class.
</footnote>
<table confidence="0.998853">
Features MaxEnt MaxEnt-Features MaxEnt-Postprocessing Naive Bayes
none .6823 .6571 .6625 .6823
num, gend .6870 .6863 .6841 .6859
num, gend, dist .7274 .7386 .7461 .7313
num, gend, dist, pos .7425 .7465 .7505 .7436
num, gend, dist, pos, lform .7616 .7663 .7656 .7624
</table>
<tableCaption confidence="0.999967">
Table 1: Results from jackknifing on training data
</tableCaption>
<bodyText confidence="0.999943076923077">
to world knowledge.7 In this case, the integrated fea-
ture system performed identically, whereas the post-
processor system displayed a performance improve-
ment of about 1% (a difference of 8 pronouns).
The MaxEnt results on the test data suffered only
a minimal (and in a few cases, no) loss from those
on the held-out data. Overtraining appears to have
been kept to a minimum; the generality of the fea-
tures was perhaps responsible for this.8 The results
from Naive Bayes generalized less well, exhibiting a
2% decrement on the test evaluation. The Hobbs al-
gorithm, which is not trained, exhibited similar per-
formance on both sets of data.
</bodyText>
<sectionHeader confidence="0.993521" genericHeader="method">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.862516111111111">
There are a variety of possible reasons why the
predicate-argument statistics failed to markedly im-
prove performance in each of the system configura-
tions. While it could be that such statistics are sim-
ply not good predictors for pronoun interpretation,
data sparsity in the collected predicate-argument
statistics could also be to blame.
We carried out an error analysis to gain further in-
sight into this question. To address the data-sparsity
issue, we employed the technique used in Keller and
Lapata (2003, K&amp;L) to get a more robust approx-
imation of predicate-argument counts.9 We wrote
7These performance results include 64 “impossible”
cases in which, due to misparsing, no correct antecedents
were provided to the model; hence 91.6% accuracy is the
best that could be achieved. The results likewise include
errors in which the model selected a bogus antecedent
that resulted from a misparse.
8As such, informal post-hoc experiments with Gaus-
sian smoothing (Chen and Rosenfeld, 2000) failed to im-
prove performance.
9K&amp;L use this technique to obtain frequencies for
predicate-argument bigrams that were unseen in a given
corpus, showing that the massive size of the web out-
weighs the noisy and unbalanced nature of searches per-
formed on it to produce statistics that correlate well with
corpus data. We are admittedly extending this reason-
ing to relations between the heads of predicates and ar-
guments without establishing that K&amp;L’s technique so
generalizes, but we nonetheless feel that it is sufficient
for the purpose of an exploratory error analysis. The re-
a script to collect the number of pages that the
AltaVista search engine found for each predicate-
argument combination and its variants per the fol-
lowing schema, modeled directly after K&amp;L:
Subject-Verb: Search for occurrences of the com-
binations N V where N is the singular or plural
form of the subject head noun and V is the in-
finitive, singular or plural present, past, perfect,
or gerund of the head verb.
Verb-Object: Search for occurrences of the com-
binations V Det N, where V and N are as
above for the verb and object head noun respec-
tively, and Det is the determiner the, a(n), or
the empty string.
Possessive-Noun: Search for occurrences of the
combinations Poss N, where Poss is the sin-
gular or plural form of the possessive and N is
the singular or plural form of the noun.
As in K&amp;L, all searches were done as exact matches.
The results for all of the different form combinations
totaled together comprised the unnormalized counts.
We also computed normalized counts, in which the
unnormalized count was divided by the total num-
ber of pages AltaVista returned for the head of the
candidate antecedent, so that, as before, the counts
would not unduly bias antecedents with head words
that occurred frequently in isolation.
We created a list of those examples for which the
MaxEnt model – trained with all 5 groups of the
morphosyntactic features activated, but not any sta-
tistical ones – made incorrect predictions during 3-
fold jackknifing on the training data. (We used held-
out data so that our test data would remain blind.)
We then pared the list down to a reasonable size
for manual analysis in a variety of ways. First, of
course, only those examples that fall into one of the
three predicate-argument configurations with which
we are concerned were included (most were). Second,
we filtered out the cases in which either the most
proximal correct antecedent (with proximity defined
sults we received from this technique held few surprises.
</bodyText>
<table confidence="0.992048833333333">
Features MaxEnt MaxEnt-Features MaxEnt-Postprocessing Naive Bayes
none .6877 .6496 .6627 .6877
num, gend .6667 .6745 .6719 .6654
num, gend, dist .7336 .7415 .7428 .7297
num, gend, dist, pos .7441 .7507 .7520 .7441
num, gend, dist, pos, lform .7572 .7572 .7677 .7415
</table>
<tableCaption confidence="0.999651">
Table 2: Results of final blind test evaluation
</tableCaption>
<bodyText confidence="0.999982512195122">
with respect to the Hobbs algorithm’s search order)
or the antecedent chosen by the model was a proper
name. Because all proper names of each ACE type
were classed together in our experiments, statistics
would not make different predictions for two such
names. While statistics could differentiate between
a potential proper name antecedent and one headed
by a common noun (and presumably did in our ex-
periments), we could not use K&amp;L’s method on those
cases unless we used the actual proper name instead
of the category in the search query – this would likely
create an undue bias to the other antecedent; con-
sider comparing counts for lawyer argued with those
for Snodgrass argued. Third, we filtered out cases in
which either the chosen antecedent or most proximal
correct antecedent was itself a pronominal, for the
reasons given in Section 5. Lastly, we eliminated a
small set of cases in which the chosen antecedent was
headless, as no predicate-argument statistics could
be collected for such a case.
These filters pared down the errors to a corpus of
45 examples; in all cases the chosen antecedent and
most proximal correct antecedent were each headed
by common nouns. Upon manual inspection, a fur-
ther subset of the cases were found to be caused by
factors irrelevant to the question at hand: 9 cases in
which the antecedent chosen should have been ruled
out as impossible (e.g., the collocation these days as
the antecedent of they), 5 cases in which either the
annotated keys were incorrect or our mapping system
failed to assign credit for a correct answer where it
was due, and 11 cases in which our shallow parser
misparsed either the chosen antecedent or the cor-
rect antecedent. This left a corpus of 20 cases to
examine using the K&amp;L methodology.
The preferences embodied by the statistics col-
lected split these cases down the middle: in 10 cases
the correct antecedent had a higher normalized prob-
ability than the chosen one, and in the other 10 cases
the opposite was true.10 To get a sense for the data,
we consider two examples, the first being a case in
</bodyText>
<footnote confidence="0.946919333333333">
10The unnormalized counts disagreed with the normal-
ized ones in only one case; the unnormalized one favored
the correct antecedent for that example.
</footnote>
<listItem confidence="0.671798">
which predicate-argument statistics were definitive:
(3) After the endowment was publicly excoriated for
</listItem>
<bodyText confidence="0.99834117948718">
having the temerity to award some of its money
to art that addressed changing views of gender
and race, many institutions lost the will to show
any art that was rambunctious or edgy.
The MaxEnt model selected the temerity as the an-
tecedent of its (salience value: 0.30), preferring it
to the correct antecedent the endowment (salience
value: 0.10). However, AltaVista found no occur-
rences of temerity’s money or its variants on the web,
and thus the unnormalized and normalized counts
were 0. On the other hand, endowment’s money and
its variants had unnormalized and normalized statis-
tics of 1583 and 1.47 × 10−3 respectively.
Example (4), on the other hand, is a case in which
the statistics merely strengthened the bias to the
wrong antecedent:
(4) The dancers were joined by about 70 supporters
as they marched around a fountain not far from
the mayor’s office, chanting: “Giuliani scared
of sex! Who’s he going to censor next?”
The model preferred the supporters as the antecedent
of they (salience value: 0.54) over the correct an-
tecedent the dancers (salience value: 0.45). Statis-
tics support the same conclusion, with unnormalized
and normalized counts of 2283 and 1.18 × 10−3 for
supporters marched and its variants, and of 334 and
1.72 × 10−4 for dancers marched and its variants.
The analysis of this sample therefore suggests that
predicate-argument statistics are unlikely to be of
much help when used in a model trained with a state-
of-the-art set of morphosyntactic features, even if
robust counts were available. While the statistical
preferences for our data sample were split down the
middle, it is important to understand that the cases
for which statistics hurt are potentially more damn-
ing than those for which they helped. In the cases in
which statistics reinforced a wrong answer, no (rea-
sonable) manipulation of statistical features or filters
can rescue the prediction. On the other hand, for
the cases in which statistics could help, their suc-
cessful use will depend on the existence of a formula
that can capture these cases without changing the
predictions for examples that the model currently
classifies correctly. Although our informal analysis
admittedly has certain limits – the web counts we
collected are only approximations of true counts, and
the size of our manually-inspected corpus ended up
being fairly small – our experience leads us to believe
that predicate-argument statistics are a poor substi-
tute for world knowledge, and more to the point, they
do not offer much predictive power to a state-of-the-
art morphosyntactically-driven pronoun interpreta-
tion system. Indeed, crisp “textbook” examples such
as (3) appear to be empirically rare; the help pro-
vided by statistics for several of the examples seemed
to be more due to fortuity than the capturing of an
actual world knowledge relationship. Consider (5):
(5) Chung, as part of a plea bargain deal with
the department, has claimed that then-DNC fi-
nance director Richard Sullivan personally asked
him for a $125,000 donation in April 1995, the
sources said. Sullivan took the money despite
having previously voiced suspicions that Chung
was acting as a conduit for illegal contributions
from Chinese business executives, they added.
Our system selected Chinese business executives as
the antecedent of they (salience value: 0.34), over the
correct the sources (salience value: 0.10). Predicate-
argument statistics support sources (normalized and
unnormalized values of 29662 and 5.78 × 10−4) over
executives (2391 and 1.50 × 10−4), and thus this ex-
ample was classified as one of the 10 for which statis-
tics helped. In actuality, however, the correct an-
tecedent is determined by unrelated factors, demon-
strated by the fact that if the head nouns executives
and sources were switched in (5), the preferred an-
tecedent would be the executives, contrary to what
predicate-argument statistics would predict.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999819666666667">
In conclusion, our experimental results and error
analysis suggest that predicate-argument statistics
offer little predictive power to a pronoun interpre-
tation system trained on a state-of-the-art set of
morphosyntactic features. On the one hand, it ap-
pears that the distribution of pronouns in discourse
allows for a system to correctly resolve a majority
of them using only morphosyntactic cues. On the
other hand, predicate-argument statistics appear to
provide a poor substitute for the world knowledge
that may be necessary to correctly interpret the re-
maining cases.
</bodyText>
<sectionHeader confidence="0.997692" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988295">
This work was supported by the ACE program
(www.nist.gov/speech/tests/ACE/).
</bodyText>
<sectionHeader confidence="0.996717" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893857142857">
Adam Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39–71.
Stanley F. Chen and Ronard Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37–50.
Ido Dagan and Alon Itai. 1990. Automatic acquisi-
tion of constraints for the resolution of anaphora
references and syntactic ambiguities. In Proceed-
ings of the 13th International Conference on Com-
putational Linguistics (COLING-90), pages 330–
332.
Ido Dagan, John Justenson, Shalom Lappin, Her-
bert Leass, and Amnon Ribak. 1995. Syntax and
lexical statistics in anaphora resolution. Applied
Artificial Intelligence, 9(6):633–644, Nov/Dec.
Niyu Ge, John Hale, and Eugene Charniak. 1998.
A statistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very Large
Corpora, Montreal, Quebec.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311–338.
Frank Keller and Mirella Lapata. 2003. Using
the web to obtain frequencies for unseen bigrams.
Computational Linguistics, 29(3).
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora res-
olution without a parser. In Proceedings of the
16th International Conference on Computational
Linguistics (COLING-96).
Shalom Lappin and Herbert Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535–561.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics (ACL-
93), pages 183–190.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872233">
<title confidence="0.997184">The (Non)Utility of Predicate-Argument Frequencies for Pronoun Interpretation</title>
<author confidence="0.961413">Appelt Lara</author>
<affiliation confidence="0.924097">UC San Diego SRI International UC San Diego UC San</affiliation>
<email confidence="0.998168">akehler@ucsd.eduappelt@ai.sri.comlmtaylor@ucsd.eduasimma@ucsd.edu</email>
<abstract confidence="0.998407733333333">State-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features. While the use of deep knowledge and inference to improve these models would appear technically infeasible, previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system configurations, and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8964" citStr="Berger et al., 1996" startWordPosition="1369" endWordPosition="1372">ntifying the correct antecedent among a list of possible ones, during testing the antecedent assigned the highest probability was chosen. These systems received as input the results of SRI’s TExTPRO system, a chunk-style shallow parser capable of recognizing low-level constituents (noun groups, verb groups, etc.). No difficult attachments are attempted, and the results are errorful. There was no human-annotated linguistic information in the input. The systems are described further below. Maximum Entropy Modeling As previously indicated, the weight-based scheme of L&amp;L suggests MaxEnt modeling (Berger et al., 1996) as a particularly natural choice for a machine learning approach. In MaxEnt, the parameters of an exponential model of the following form are estimated: eK λifi(x,y) The variable y represents the outcome (coreference or not) and x represents the context. There is one value for each feature that predicts coreference behavior, represented by the parameters λ1, ..., λn, which are Lagrange multipliers that constrain the expected value of each feature in the model to be the values found in the distribution of the training data. (The fi(x, y) are indicator functions which equal 1 when the correspon</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronard Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="25647" citStr="Chen and Rosenfeld, 2000" startWordPosition="4004" endWordPosition="4007">nalysis to gain further insight into this question. To address the data-sparsity issue, we employed the technique used in Keller and Lapata (2003, K&amp;L) to get a more robust approximation of predicate-argument counts.9 We wrote 7These performance results include 64 “impossible” cases in which, due to misparsing, no correct antecedents were provided to the model; hence 91.6% accuracy is the best that could be achieved. The results likewise include errors in which the model selected a bogus antecedent that resulted from a misparse. 8As such, informal post-hoc experiments with Gaussian smoothing (Chen and Rosenfeld, 2000) failed to improve performance. 9K&amp;L use this technique to obtain frequencies for predicate-argument bigrams that were unseen in a given corpus, showing that the massive size of the web outweighs the noisy and unbalanced nature of searches performed on it to produce statistics that correlate well with corpus data. We are admittedly extending this reasoning to relations between the heads of predicates and arguments without establishing that K&amp;L’s technique so generalizes, but we nonetheless feel that it is sufficient for the purpose of an exploratory error analysis. The rea script to collect th</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F. Chen and Ronard Rosenfeld. 2000. A survey of smoothing techniques for ME models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Automatic acquisition of constraints for the resolution of anaphora references and syntactic ambiguities.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING-90),</booktitle>
<pages>330--332</pages>
<contexts>
<context position="3303" citStr="Dagan and Itai (1990)" startWordPosition="502" endWordPosition="505">data than forcing initiative or forcing edge. Assuming that such statistics imply that industries are more likely to be forced in the real world than are initiatives or edges, this information could be taken to establish a preference for his industry as the antecedent of it in (1). While there will always be cases that require arbitrarily deep knowledge for their interpretation, the empirical question of how far one can go by relying on this sort of selectional information remains. Our point of departure is the work of Lappin and Leass (1994, henceforth L&amp;L) and Dagan et al. (1995). (See also Dagan and Itai (1990).) L&amp;L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals.&apos; Dagan et al. (1995) then developed a postprocessor based on predicate-argument statistics that was used to override RAP’s decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per&apos;Kennedy and Boguraev (1996, henceforth, K&amp;B) adapted L&amp;L’s algorithm to rely on far less syntactic</context>
</contexts>
<marker>Dagan, Itai, 1990</marker>
<rawString>Ido Dagan and Alon Itai. 1990. Automatic acquisition of constraints for the resolution of anaphora references and syntactic ambiguities. In Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), pages 330– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>John Justenson</author>
<author>Shalom Lappin</author>
<author>Herbert Leass</author>
<author>Amnon Ribak</author>
</authors>
<title>Syntax and lexical statistics in anaphora resolution.</title>
<date>1995</date>
<journal>Applied Artificial Intelligence,</journal>
<volume>9</volume>
<issue>6</issue>
<pages>Nov/Dec.</pages>
<contexts>
<context position="3270" citStr="Dagan et al. (1995)" startWordPosition="496" endWordPosition="499">ination in naturally-occurring data than forcing initiative or forcing edge. Assuming that such statistics imply that industries are more likely to be forced in the real world than are initiatives or edges, this information could be taken to establish a preference for his industry as the antecedent of it in (1). While there will always be cases that require arbitrarily deep knowledge for their interpretation, the empirical question of how far one can go by relying on this sort of selectional information remains. Our point of departure is the work of Lappin and Leass (1994, henceforth L&amp;L) and Dagan et al. (1995). (See also Dagan and Itai (1990).) L&amp;L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals.&apos; Dagan et al. (1995) then developed a postprocessor based on predicate-argument statistics that was used to override RAP’s decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per&apos;Kennedy and Boguraev (1996, henceforth, K&amp;B) adapted L&amp;L’s algori</context>
</contexts>
<marker>Dagan, Justenson, Lappin, Leass, Ribak, 1995</marker>
<rawString>Ido Dagan, John Justenson, Shalom Lappin, Herbert Leass, and Amnon Ribak. 1995. Syntax and lexical statistics in anaphora resolution. Applied Artificial Intelligence, 9(6):633–644, Nov/Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<location>Montreal, Quebec.</location>
<contexts>
<context position="14074" citStr="Ge et al., 1998" startWordPosition="2199" endWordPosition="2202">tibility (e.g., a singular pronoun with an antecedent of unknown number). These features are likewise more liberal than the number-based hard constraint mentioned above. p(y|x) = i λifi(x,y) y e p(x) p(x) The context x is constant for each outcome y, so we only need to find: Distance (dist): Includes features pertaining to the distance between the pronoun and the potential antecedent. Examples include the number of sentences between them and the “Hobbs distance”, that is, the number of noun groups that Hobbs’s search algorithm has to skip before the potential antecedent is found (Hobbs, 1978; Ge et al., 1998). Grammatical Role (pos): Includes features pertaining to the syntactic position of the potential antecedent. Examples include whether the potential antecedent appears to be the subject or object of a verb, and whether the potential antecedent is embedded in a prepositional phrase. Linguistic Form (lform): Includes features pertaining to the referential form of the potential antecedent, e.g., whether it is a proper name, definite description, indefinite NP, or a pronoun. The values of these features – computed from our system’s errorful shallow constituent parses – comprised the input to the l</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proceedings of the Sixth Workshop on Very Large Corpora, Montreal, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<pages>44--311</pages>
<contexts>
<context position="14056" citStr="Hobbs, 1978" startWordPosition="2197" endWordPosition="2198">as mere compatibility (e.g., a singular pronoun with an antecedent of unknown number). These features are likewise more liberal than the number-based hard constraint mentioned above. p(y|x) = i λifi(x,y) y e p(x) p(x) The context x is constant for each outcome y, so we only need to find: Distance (dist): Includes features pertaining to the distance between the pronoun and the potential antecedent. Examples include the number of sentences between them and the “Hobbs distance”, that is, the number of noun groups that Hobbs’s search algorithm has to skip before the potential antecedent is found (Hobbs, 1978; Ge et al., 1998). Grammatical Role (pos): Includes features pertaining to the syntactic position of the potential antecedent. Examples include whether the potential antecedent appears to be the subject or object of a verb, and whether the potential antecedent is embedded in a prepositional phrase. Linguistic Form (lform): Includes features pertaining to the referential form of the potential antecedent, e.g., whether it is a proper name, definite description, indefinite NP, or a pronoun. The values of these features – computed from our system’s errorful shallow constituent parses – comprised </context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Jerry R. Hobbs. 1978. Resolving pronoun references. Lingua, 44:311–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="25167" citStr="Keller and Lapata (2003" startWordPosition="3930" endWordPosition="3933">n. The Hobbs algorithm, which is not trained, exhibited similar performance on both sets of data. 7 Error Analysis There are a variety of possible reasons why the predicate-argument statistics failed to markedly improve performance in each of the system configurations. While it could be that such statistics are simply not good predictors for pronoun interpretation, data sparsity in the collected predicate-argument statistics could also be to blame. We carried out an error analysis to gain further insight into this question. To address the data-sparsity issue, we employed the technique used in Keller and Lapata (2003, K&amp;L) to get a more robust approximation of predicate-argument counts.9 We wrote 7These performance results include 64 “impossible” cases in which, due to misparsing, no correct antecedents were provided to the model; hence 91.6% accuracy is the best that could be achieved. The results likewise include errors in which the model selected a bogus antecedent that resulted from a misparse. 8As such, informal post-hoc experiments with Gaussian smoothing (Chen and Rosenfeld, 2000) failed to improve performance. 9K&amp;L use this technique to obtain frequencies for predicate-argument bigrams that were u</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Kennedy</author>
<author>Branimir Boguraev</author>
</authors>
<title>Anaphora for everyone: Pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96).</booktitle>
<contexts>
<context position="3831" citStr="Kennedy and Boguraev (1996" startWordPosition="584" endWordPosition="588">f Lappin and Leass (1994, henceforth L&amp;L) and Dagan et al. (1995). (See also Dagan and Itai (1990).) L&amp;L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals.&apos; Dagan et al. (1995) then developed a postprocessor based on predicate-argument statistics that was used to override RAP’s decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per&apos;Kennedy and Boguraev (1996, henceforth, K&amp;B) adapted L&amp;L’s algorithm to rely on far less syntactic analysis (noun phrase identification and rudimentary grammatical role marking), with performance in the 75% range on mixed genres. formance (2.5%).2 Because RAP is symbolic, the two systems were necessarily coupled in a blackbox manner. They noted, however, that if one had a statistically-driven pronoun interpretation system, co-occurrence information could be modeled alongside morphosyntactic information: “A promising direction for future research is the development of an empirically based model for salience criteria ana</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>Christopher Kennedy and Branimir Boguraev. 1996. Anaphora for everyone: Pronominal anaphora resolution without a parser. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="3229" citStr="Lappin and Leass (1994" startWordPosition="489" endWordPosition="492">ng industry is a more likely verbobject combination in naturally-occurring data than forcing initiative or forcing edge. Assuming that such statistics imply that industries are more likely to be forced in the real world than are initiatives or edges, this information could be taken to establish a preference for his industry as the antecedent of it in (1). While there will always be cases that require arbitrarily deep knowledge for their interpretation, the empirical question of how far one can go by relying on this sort of selectional information remains. Our point of departure is the work of Lappin and Leass (1994, henceforth L&amp;L) and Dagan et al. (1995). (See also Dagan and Itai (1990).) L&amp;L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals.&apos; Dagan et al. (1995) then developed a postprocessor based on predicate-argument statistics that was used to override RAP’s decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per&apos;Kennedy and Boguraev (19</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora Resolution.</title>
<date>2002</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="1135" citStr="Mitkov (2002)" startWordPosition="156" endWordPosition="157">edicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system configurations, and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax. 1 Introduction The last several years has seen a number of works that use weight-based systems (trained either manually or via supervised learning) for pronoun interpretation, in addition to others that have addressed the broader task of entity-level coreference (see Mitkov (2002) for a useful survey). These systems typically rely on a variety of morphosyntactic factors that have been posited in the literature to affect the interpretation of pronouns in naturallyoccurring discourse, including gender and number agreement, the distance between the pronoun and antecedent, the grammatical positions of the pronoun and antecedent, and the linguistic form of the antecedent, among others. A common refrain is that the performance of systems that rely on such features is plateauing, and that further progress will require the use of world knowledge and inference (ibid., Ch. 9, in</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov. 2002. Anaphora Resolution. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL93),</booktitle>
<pages>183--190</pages>
<contexts>
<context position="17033" citStr="Pereira et al. (1993)" startWordPosition="2655" endWordPosition="2658"> as instances of PROPER PERSON). While counts were collected for a broad range of predicate-argument combinations, there were still many combinations that were only seen once or twice, and certainly other possible combinations exist that were not seen at all. The distribution that these statistics yield therefore needed to be smoothed. We took two approaches to smoothing. First, because Dagan et al. used Good-Turing smoothing in their experiments, we did likewise so as to replicate their work as closely as possible. Second, we tried an approach based on the distributional clustering method of Pereira et al. (1993). This method yielded word classes that offered more robust count approximations for their member words. However, both methods yielded similar results when embedded in the larger system, and so we will report on the results of using Good-Turing so as to remain more directly comparable to Dagan et al. The smoothed predicate-argument statistics were employed in two ways. First, we built a postprocessing filter modeled directly on Dagan et al.’s system. Their implementation made use of two equations. The first computes the frequency with which a candidate head noun C is found with the predicate w</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL93), pages 183–190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>