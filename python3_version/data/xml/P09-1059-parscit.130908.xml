<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.9629305">
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging – A Case Study
</title>
<author confidence="0.999049">
Wenbin Jiang † Liang Huang t Qun Liu †
</author>
<affiliation confidence="0.971868666666667">
†Key Lab. of Intelligent Information Processing $Google Research
Institute of Computing Technology 1350 Charleston Rd.
Chinese Academy of Sciences Mountain View, CA 94043, USA
</affiliation>
<address confidence="0.776151">
P.O. Box 2704, Beijing 100190, China lianghuang@google.com
</address>
<email confidence="0.9958">
{jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.com
</email>
<sectionHeader confidence="0.997309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999763115384615">
Manually annotated corpora are valuable
but scarce resources, yet for many anno-
tation tasks such as treebanking and se-
quence labeling there exist multiple cor-
pora with different and incompatible anno-
tation guidelines or standards. This seems
to be a great waste of human efforts, and
it would be nice to automatically adapt
one annotation standard to another. We
present a simple yet effective strategy that
transfers knowledge from a differently an-
notated corpus to the corpus with desired
annotation. We test the efficacy of this
method in the context of Chinese word
segmentation and part-of-speech tagging,
where no segmentation and POS tagging
standards are widely accepted due to the
lack of morphology in Chinese. Experi-
ments show that adaptation from the much
larger People’s Daily corpus to the smaller
but more popular Penn Chinese Treebank
results in significant improvements in both
segmentation and tagging accuracies (with
error reductions of 30.2% and 14%, re-
spectively), which in turn helps improve
Chinese parsing accuracy.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999827272727273">
Much of statistical NLP research relies on some
sort of manually annotated corpora to train their
models, but these resources are extremely expen-
sive to build, especially at a large scale, for ex-
ample in treebanking (Marcus et al., 1993). How-
ever the linguistic theories underlying these anno-
tation efforts are often heavily debated, and as a re-
sult there often exist multiple corpora for the same
task with vastly different and incompatible anno-
tation philosophies. For example just for English
treebanking there have been the Chomskian-style
</bodyText>
<figure confidence="0.823969333333333">
U.S. Vice-President visited China
ns b n v
U.S. Vice President visited-China
</figure>
<figureCaption confidence="0.923650666666667">
Figure 1: Incompatible word segmentation and
POS tagging standards between CTB (upper) and
People’s Daily (below).
</figureCaption>
<bodyText confidence="0.996455692307692">
Penn Treebank (Marcus et al., 1993) the HPSG
LinGo Redwoods Treebank (Oepen et al., 2002),
and a smaller dependency treebank (Buchholz and
Marsi, 2006). A second, related problem is that
the raw texts are also drawn from different do-
mains, which for the above example range from
financial news (PTB/WSJ) to transcribed dialog
(LinGo). These two problems seem be a great
waste in human efforts, and it would be nice if
one could automatically adapt from one annota-
tion standard and/or domain to another in order
to exploit much larger datasets for better train-
ing. The second problem, domain adaptation, is
very well-studied, e.g. by Blitzer et al. (2006)
and Daum´e III (2007) (and see below for discus-
sions), so in this paper we focus on the less stud-
ied, but equally important problem of annotation-
style adaptation.
We present a very simple yet effective strategy
that enables us to utilize knowledge from a differ-
ently annotated corpora for the training of a model
on a corpus with desired annotation. The basic
idea is very simple: we first train on a source cor-
pus, resulting in a source classifier, which is used
to label the target corpus and results in a “source-
style” annotation of the target corpus. We then
</bodyText>
<figure confidence="0.737756714285714">
NR NN VV NR
*E1 9112 43 AA4
i; 5
*6
43 AA4
i; 5 -1&apos;6
*E1 9112
</figure>
<page confidence="0.964111">
522
</page>
<note confidence="0.9996">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522–530,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999954835294117">
train a second model on the target corpus with the
first classifier’s prediction as additional features
for guided learning.
This method is very similar to some ideas in
domain adaptation (Daum´e III and Marcu, 2006;
Daum´e III, 2007), but we argue that the underly-
ing problems are quite different. Domain adapta-
tion assumes the labeling guidelines are preserved
between the two domains, e.g., an adjective is al-
ways labeled as JJ regardless of from Wall Street
Journal (WSJ) or Biomedical texts, and only the
distributions are different, e.g., the word “control”
is most likely a verb in WSJ but often a noun
in Biomedical texts (as in “control experiment”).
Annotation-style adaptation, however, tackles the
problem where the guideline itself is changed, for
example, one treebank might distinguish between
transitive and intransitive verbs, while merging the
different noun types (NN, NNS, etc.), and for ex-
ample one treebank (PTB) might be much flatter
than the other (LinGo), not to mention the fun-
damental disparities between their underlying lin-
guistic representations (CFG vs. HPSG). In this
sense, the problem we study in this paper seems
much harder and more motivated from a linguistic
(rather than statistical) point of view. More inter-
estingly, our method, without any assumption on
the distributions, can be simultaneously applied to
both domain and annotation standards adaptation
problems, which is very appealing in practice be-
cause the latter problem often implies the former,
as in our case study.
To test the efficacy of our method we choose
Chinese word segmentation and part-of-speech
tagging, where the problem of incompatible an-
notation standards is one of the most evident: so
far no segmentation standard is widely accepted
due to the lack of a clear definition of Chinese
words, and the (almost complete) lack of mor-
phology results in much bigger ambiguities and
heavy debates in tagging philosophies for Chi-
nese parts-of-speech. The two corpora used in
this study are the much larger People’s Daily (PD)
(5.86M words) corpus (Yu et al., 2001) and the
smaller but more popular Penn Chinese Treebank
(CTB) (0.47M words) (Xue et al., 2005). They
used very different segmentation standards as well
as different POS tagsets and tagging guidelines.
For example, in Figure 1, People’s Daily breaks
“Vice-President” into two words while combines
the phrase “visited-China” as a compound. Also
CTB has four verbal categories (VV for normal
verbs, and VC for copulas, etc.) while PD has only
one verbal tag (v) (Xia, 2000). It is preferable to
transfer knowledge from PD to CTB because the
latter also annotates tree structures which is very
useful for downstream applications like parsing,
summarization, and machine translation, yet it is
much smaller in size. Indeed, many recent efforts
on Chinese-English translation and Chinese pars-
ing use the CTB as the de facto segmentation and
tagging standards, but suffers from the limited size
of training data (Chiang, 2007; Bikel and Chiang,
2000). We believe this is also a reason why state-
of-the-art accuracy for Chinese parsing is much
lower than that of English (CTB is only half the
size of PTB).
Our experiments show that adaptation from PD
to CTB results in a significant improvement in seg-
mentation and POS tagging, with error reductions
of 30.2% and 14%, respectively. In addition, the
improved accuracies from segmentation and tag-
ging also lead to an improved parsing accuracy on
CTB, reducing 38% of the error propagation from
word segmentation to parsing. We envision this
technique to be general and widely applicable to
many other sequence labeling tasks.
In the rest of the paper we first briefly review
the popular classification-based method for word
segmentation and tagging (Section 2), and then
describe our idea of annotation adaptation (Sec-
tion 3). We then discuss other relevant previous
work including co-training and classifier combina-
tion (Section 4) before presenting our experimen-
tal results (Section 5).
</bodyText>
<sectionHeader confidence="0.6835275" genericHeader="introduction">
2 Segmentation and Tagging as
Character Classification
</sectionHeader>
<bodyText confidence="0.999617142857143">
Before describing the adaptation algorithm, we
give a brief introduction of the baseline character
classification strategy for segmentation, as well as
joint segmenation and tagging (henceforth “Joint
S&amp;T”). following our previous work (Jiang et al.,
2008). Given a Chinese sentence as sequence of n
characters:
</bodyText>
<equation confidence="0.769685">
C1 C2 .. Cn
</equation>
<bodyText confidence="0.9858735">
where Ci is a character, word segmentation aims
to split the sequence into m(&lt; n) words:
</bodyText>
<footnote confidence="0.321313333333333">
C1:e1 Ce1+1:e2 .. Cem−1+1:em
where each subsequence Ci:j indicates a Chinese
word spanning from characters Ci to Cj (both in-
</footnote>
<page confidence="0.996502">
523
</page>
<construct confidence="0.486018">
Algorithm 1 Perceptron training algorithm.
</construct>
<listItem confidence="0.9596177">
1: Input: Training examples (xi, yi)
2: α�+— 0
3: fort +— 1 .. T do
4: for i +— 1 .. N do
5: zi +— argmaxzEGEN(xi) 4&apos;(xi, z) · α�
6: if zi # yi then
7: α� +— α� + 4&apos;(xi, yi) − 4&apos;(xi, zi)
8: Output: Parameters α�
clusive). While in Joint S&amp;T, each word is further
annotated with a POS tag:
</listItem>
<equation confidence="0.485566">
C1:e1/t1 Ce1+1:e2/t2 .. Ce,,,_1+1:e,/tm
</equation>
<bodyText confidence="0.837403">
where tk(k = L.m) denotes the POS tag for the
word Cek_1+1:ek.
</bodyText>
<subsectionHeader confidence="0.942286">
2.1 Character Classification Method
</subsectionHeader>
<bodyText confidence="0.999358266666667">
Xue and Shen (2003) describe for the first time
the character classification approach for Chinese
word segmentation, where each character is given
a boundary tag denoting its relative position in a
word. In Ng and Low (2004), Joint S&amp;T can also
be treated as a character classification problem,
where a boundary tag is combined with a POS tag
in order to give the POS information of the word
containing these characters. In addition, Ng and
Low (2004) find that, compared with POS tagging
after word segmentation, Joint S&amp;T can achieve
higher accuracy on both segmentation and POS
tagging. This paper adopts the tag representation
of Ng and Low (2004). For word segmentation
only, there are four boundary tags:
</bodyText>
<listItem confidence="0.99999475">
• b: the begin of the word
• m: the middle of the word
• e: the end of the word
• s: a single-character word
</listItem>
<bodyText confidence="0.999716454545455">
while for Joint S&amp;T, a POS tag is attached to the
tail of a boundary tag, to incorporate the word
boundary information and POS information to-
gether. For example, b-NN indicates that the char-
acter is the begin of a noun. After all charac-
ters of a sentence are assigned boundary tags (or
with POS postfix) by a classifier, the correspond-
ing word sequence (or with POS) can be directly
derived. Take segmentation for example, a char-
acter assigned a tag s or a subsequence of words
assigned a tag sequence bm*e indicates a word.
</bodyText>
<subsectionHeader confidence="0.999707">
2.2 Training Algorithm and Features
</subsectionHeader>
<bodyText confidence="0.999249416666667">
Now we will show the training algorithm of the
classifier and the features used. Several classi-
fication models can be adopted here, however,
we choose the averaged perceptron algorithm
(Collins, 2002) because of its simplicity and high
accuracy. It is an online training algorithm and
has been successfully used in many NLP tasks,
such as POS tagging (Collins, 2002), parsing
(Collins and Roark, 2004), Chinese word segmen-
tation (Zhang and Clark, 2007; Jiang et al., 2008),
and so on.
Similar to the situation in other sequence label-
ing problems, the training procedure is to learn a
discriminative model mapping from inputs x E X
to outputs y E Y , where X is the set of sentences
in the training corpus and Y is the set of corre-
sponding labelled results. Following Collins, we
use a function GEN(x) enumerating the candi-
date results of an input x , a representation 4) map-
ping each training example (x, y) E X x Y to a
feature vector 4)(x, y) E Rd, and a parameter vec-
tor α� E Rd corresponding to the feature vector.
For an input character sequence x, we aim to find
an output F(x) that satisfies:
</bodyText>
<equation confidence="0.9973465">
F(x) = argmax 4)(x, y) · α� (1)
yEGEN(x)
</equation>
<bodyText confidence="0.999993944444445">
where 4)(x, y)·a denotes the inner product of fea-
ture vector 4)(x, y) and the parameter vector a.
Algorithm 1 depicts the pseudo code to tune the
parameter vector a. In addition, the “averaged pa-
rameters” technology (Collins, 2002) is used to al-
leviate overfitting and achieve stable performance.
Table 1 lists the feature template and correspond-
ing instances. Following Ng and Low (2004),
the current considering character is denoted as C0,
while the ith character to the left of C0 as C_Z,
and to the right as CZ. There are additional two
functions of which each returns some property of a
character. Pu(·) is a boolean function that checks
whether a character is a punctuation symbol (re-
turns 1 for a punctuation, 0 for not). T(·) is a
multi-valued function, it classifies a character into
four classifications: number, date, English letter
and others (returns 1, 2, 3 and 4, respectively).
</bodyText>
<sectionHeader confidence="0.991377" genericHeader="method">
3 Automatic Annotation Adaptation
</sectionHeader>
<bodyText confidence="0.99928">
From this section, several shortened forms are
adopted for representation inconvenience. We use
source corpus to denote the corpus with the anno-
tation standard that we don’t require, which is of
</bodyText>
<page confidence="0.993579">
524
</page>
<table confidence="0.900368166666667">
Feature Template Instances
Ci (i = −2..2) C−2 = It, C−1 = O, C0= *, C1 = ,R, C2 = R
CiCi+1 (i = −2..1) C−2C−1 = -to, C−1C0 = O*, C0C1 = *,R, C1C2 = ,RR
C−1C1 C−1C1 = O,R
Pu(C0) Pu(C0) = 0
T(C−2)T(C−1)T(C0)T(C1)T(C2) T(C−2)T(C−1)T(C0)T(C1)T(C2) = 11243
</table>
<tableCaption confidence="0.965992">
Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004). Suppose we are
</tableCaption>
<bodyText confidence="0.984011588235294">
considering the third character “*” in “JLO * ,RR”.
course the source of the adaptation, while target
corpus denoting the corpus with the desired stan-
dard. And correspondingly, the two annotation
standards are naturally denoted as source standard
and target standard, while the classifiers follow-
ing the two annotation standards are respectively
named as source classifier and target classifier, if
needed.
Considering that word segmentation and Joint
S&amp;T can be conducted in the same character clas-
sification manner, we can design an unified stan-
dard adaptation framework for the two tasks, by
taking the source classifier’s classification result
as the guide information for the target classifier’s
classification decision. The following section de-
picts this adaptation strategy in detail.
</bodyText>
<subsectionHeader confidence="0.992901">
3.1 General Adaptation Strategy
</subsectionHeader>
<bodyText confidence="0.999933">
In detail, in order to adapt knowledge from the
source corpus, first, a source classifier is trained
on it and therefore captures the knowledge it con-
tains; then, the source classifier is used to clas-
sify the characters in the target corpus, although
the classification result follows a standard that we
don’t desire; finally, a target classifier is trained
on the target corpus, with the source classifier’s
classification result as additional guide informa-
tion. The training procedure of the target clas-
sifier automatically learns the regularity to trans-
fer the source classifier’s predication result from
source standard to target standard. This regular-
ity is incorporated together with the knowledge
learnt from the target corpus itself, so as to ob-
tain enhanced predication accuracy. For a given
un-classified character sequence, the decoding is
analogous to the training. First, the character se-
quence is input into the source classifier to ob-
tain an source standard annotated classification
result, then it is input into the target classifier
with this classification result as additional infor-
mation to get the final result. This coincides with
the stacking method for combining dependency
parsers (Martins et al., 2008; Nivre and McDon-
</bodyText>
<figure confidence="0.638963">
target classifier
</figure>
<figureCaption confidence="0.9999935">
Figure 2: The pipeline for training.
Figure 3: The pipeline for decoding.
</figureCaption>
<bodyText confidence="0.9960109375">
ald, 2008), and is also similar to the Pred baseline
for domain adaptation in (Daum´e III and Marcu,
2006; Daum´e III, 2007). Figures 2 and 3 show
the flow charts for training and decoding.
The utilization of the source classifier’s classi-
fication result as additional guide information re-
sorts to the introduction of new features. For the
current considering character waiting for classi-
fication, the most intuitive guide features is the
source classifier’s classification result itself. How-
ever, our effort isn’t limited to this, and more spe-
cial features are introduced: the source classifier’s
classification result is attached to every feature
listed in Table 1 to get combined guide features.
This is similar to feature design in discriminative
dependency parsing (McDonald et al., 2005; Mc-
</bodyText>
<figure confidence="0.997324875">
source corpus
train with
normal features
source classifier
train with
additional features
target corpus
source annotation
classification result
raw sentence
source classifier
target classifier
source annotation
classification result
target annotation
classification result
</figure>
<page confidence="0.991036">
525
</page>
<bodyText confidence="0.999558967741936">
Donald and Pereira, 2006), where the basic fea-
tures, composed of words and POSs in the context,
are also conjoined with link direction and distance
in order to obtain more special features. Table 2
shows an example of guide features and basic fea-
tures, where “α = b ” represents that the source
classifier classifies the current character as b, the
beginning of a word.
Such combination method derives a series of
specific features, which helps the target classifier
to make more precise classifications. The parame-
ter tuning procedure of the target classifier will au-
tomatically learn the regularity of using the source
classifier’s classification result to guide its deci-
sion making. For example, if a current consid-
ering character shares some basic features in Ta-
ble 2 and it is classified as b, then the target clas-
sifier will probably classify it as m. In addition,
the training procedure of the target classifier also
learns the relative weights between the guide fea-
tures and the basic features, so that the knowledge
from both the source corpus and the target corpus
are automatically integrated together.
In fact, more complicated features can be
adopted as guide information. For error tolerance,
guide features can be extracted from n-best re-
sults or compacted lattices of the source classifier;
while for the best use of the source classifier’s out-
put, guide features can also be the classification
results of several successive characters. We leave
them as future research.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="method">
4 Related Works
</sectionHeader>
<bodyText confidence="0.9997955">
Co-training (Sarkar, 2001) and classifier com-
bination (Vivre and McDonald, 2008) are two
technologies for training improved dependency
parsers. The co-training technology lets two dif-
ferent parsing models learn from each other dur-
ing parsing an unlabelled corpus: one model
selects some unlabelled sentences it can confi-
dently parse, and provide them to the other model
as additional training corpus in order to train
more powerful parsers. The classifier combina-
tion lets graph-based and transition-based depen-
dency parsers to utilize the features extracted from
each other’s parsing results, to obtain combined,
enhanced parsers. The two technologies aim to
let two models learn from each other on the same
corpora with the same distribution and annota-
tion standard, while our strategy aims to integrate
the knowledge in multiple corpora with different
</bodyText>
<equation confidence="0.987388777777778">
Baseline Features
C−2 = -k
C−1 = B
C0 = o
C1 = Ú
C2 = i3
C−2C−1 = -kB
C−1C0 = Bo
C0C1 = oÚ
C1C2 = Úi3
C−1C1 = BÚ
Pu(C0) = 0
T (C−2)T (C−1)T(C0)T(C1)T(C2) = 44444
Guide Features
α = b
C−2 = -k o α = b
C−1 = B o α = b
C0 = o o α = b
C1 = Ú o α = b
C2 = i3 o α = b
C−2C−1 = -kB o α = b
C−1C0 = Bo o α = b
C0C1 = oÚ o α = b
C1C2 = Úi3 o α = b
C−1C1 = BÚ o α = b
Pu(C0) = 0 o α = b
T(C−2)T(C−1)T(C0)T(C1)T(C2) = 44444 o α = b
</equation>
<bodyText confidence="0.897969357142857">
Table 2: An example of basic features and guide
features of standard-adaptation for word segmen-
tation. Suppose we are considering the third char-
acter “o” in “-kB o Úi3*”.
annotation-styles.
Gao et al. (2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
They design some class-type transformation tem-
plates and use the transformation-based error-
driven learning method of Brill (1995) to learn
what word delimiters should be modified. How-
ever, this converter need human designed transfor-
mation templates, and is hard to be generalized to
POS tagging, not to mention other structure label-
ing tasks. Moreover, the processing procedure is
divided into two isolated steps, conversion after
segmentation, which suffers from error propaga-
tion and wastes the knowledge in the corpora. On
the contrary, our strategy is automatic, generaliz-
able and effective.
In addition, many efforts have been devoted
to manual treebank adaptation, where they adapt
PTB to other grammar formalisms, such as such
as CCG and LFG (Hockenmaier and Steedman,
2008; Cahill and Mccarthy, 2007). However, they
are heuristics-based and involve heavy human en-
gineering.
</bodyText>
<page confidence="0.998484">
526
</page>
<sectionHeader confidence="0.999473" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999770147058824">
Our adaptation experiments are conducted from
People’s Daily (PD) to Penn Chinese Treebank 5.0
(CTB). These two corpora are segmented follow-
ing different segmentation standards and labeled
with different POS sets (see for example Figure 1).
PD is much bigger in size, with about 100K sen-
tences, while CTB is much smaller, with only
about 18K sentences. Thus a classifier trained on
CTB usually falls behind that trained on PD, but
CTB is preferable because it also annotates tree
structures, which is very useful for downstream
applications like parsing and translation. For ex-
ample, currently, most Chinese constituency and
dependency parsers are trained on some version
of CTB, using its segmentation and POS tagging
as the de facto standards. Therefore, we expect the
knowledge adapted from PD will lead to more pre-
cise CTB-style segmenter and POS tagger, which
would in turn reduce the error propagation to pars-
ing (and translation).
Experiments adapting from PD to CTB are con-
ducted for two tasks: word segmentation alone,
and joint segmentation and POS tagging (Joint
S&amp;T). The performance measurement indicators
for word segmentation and Joint S&amp;T are bal-
anced F-measure, F = 2PR/(P + R), a function
of Precision P and Recall R. For word segmen-
tation, P indicates the percentage of words in seg-
mentation result that are segmented correctly, and
R indicates the percentage of correctly segmented
words in gold standard words. For Joint S&amp;T, P
and R mean nearly the same except that a word
is correctly segmented only if its POS is also cor-
rectly labelled.
</bodyText>
<subsectionHeader confidence="0.987785">
5.1 Baseline Perceptron Classifier
</subsectionHeader>
<bodyText confidence="0.999991714285714">
We first report experimental results of the single
perceptron classifier on CTB 5.0. The original
corpus is split according to former works: chap-
ters 271 − 300 for testing, chapters 301 − 325 for
development, and others for training. Figure 4
shows the learning curves for segmentation only
and Joint S&amp;T, we find all curves tend to moder-
ate after 7 iterations. The data splitting conven-
tion of other two corpora, People’s Daily doesn’t
reserve the development sets, so in the following
experiments, we simply choose the model after 7
iterations when training on this corpus.
The first 3 rows in each sub-table of Table 3
show the performance of the single perceptron
</bodyText>
<figure confidence="0.7867035">
1 2 3 4 5 6 7 8 9 10
number of iterations
</figure>
<figureCaption confidence="0.968135">
Figure 4: Averaged perceptron learning curves for
segmentation and Joint S&amp;T.
</figureCaption>
<table confidence="0.990777181818182">
Train on Test on Seg F1% JST F1%
Word Segmentation
PD PD 97.45 —
PD CTB 91.71 —
CTB CTB 97.35 —
PD —* CTB CTB 98.15 —
Joint S&amp;T
PD PD 97.57 94.54
PD CTB 91.68 —
CTB CTB 97.58 93.06
PD —* CTB CTB 98.23 94.03
</table>
<tableCaption confidence="0.995935">
Table 3: Experimental results for both baseline
</tableCaption>
<bodyText confidence="0.901604409090909">
models and final systems with annotation adap-
tation. PD —* CTB means annotation adaptation
from PD to CTB. For the upper sub-table, items of
JST F1 are undefined since only segmentation is
performs. While in the sub-table below, JST F1
is also undefined since the model trained on PD
gives a POS set different from that of CTB.
models. Comparing row 1 and 3 in the sub-table
below with the corresponding rows in the upper
sub-table, we validate that when word segmenta-
tion and POS tagging are conducted jointly, the
performance for segmentation improves since the
POS tags provide additional information to word
segmentation (Ng and Low, 2004). We also see
that for both segmentation and Joint S&amp;T, the per-
formance sharply declines when a model trained
on PD is tested on CTB (row 2 in each sub-table).
In each task, only about 92% F1 is achieved. This
obviously fall behind those of the models trained
on CTB itself (row 3 in each sub-table), about 97%
F1, which are used as the baselines of the follow-
ing annotation adaptation experiments.
</bodyText>
<figure confidence="0.980471466666667">
segmentation only
segmentation in Joint S&amp;T
Joint S&amp;T
0.980
0.970
0.960
0.950
F measure
0.940
0.930
0.920
0.910
0.900
0.890
0.880
</figure>
<page confidence="0.984369">
527
</page>
<table confidence="0.99995178125">
POS #Word #BaseErr #AdaErr ErrDec%
AD 305 30 19 36.67 1
AS 76 0 0
BA 4 1 1
CC 135 8 8
CD 356 21 14 33.33 1
CS 6 0 0
DEC 137 31 23 25.81 1
DEG 197 32 37 1
DEV 10 0 0
DT 94 3 1 66.67 1
ETC 12 0 0
FW 1 1 1
JJ 127 41 44 1
LB 2 1 1
LC 106 3 2 33.33 1
M 349 18 4 77.78 1
MSP 8 2 1 50.00 1
NN 1715 151 126 16.56 1
NR 713 59 50 15.25 1
NT 178 1 2 1
OD 84 0 0
P 251 10 6 40.00 1
PN 81 1 1
PU 997 0 1 1
SB 2 0 0
SP 2 2 2
VA 98 23 21 08.70 1
VC 61 0 0
VE 25 1 0 100.00 1
VV 689 64 40 37.50 1
SUM 6821 213 169 20.66 1
</table>
<tableCaption confidence="0.979438">
Table 4: Error analysis for Joint S&amp;T on the devel-
</tableCaption>
<bodyText confidence="0.88439675">
oping set of CTB. #BaseErr and #AdaErr denote
the count of words that can’t be recalled by the
baseline model and adapted model, respectively.
ErrDec denotes the error reduction of Recall.
</bodyText>
<subsectionHeader confidence="0.98709">
5.2 Adaptation for Segmentation and
Tagging
</subsectionHeader>
<bodyText confidence="0.999577555555556">
Table 3 also lists the results of annotation adap-
tation experiments. For word segmentation, the
model after annotation adaptation (row 4 in upper
sub-table) achieves an F-measure increment of 0.8
points over the baseline model, corresponding to
an error reduction of 30.2%; while for Joint S&amp;T,
the F-measure increment of the adapted model
(row 4 in sub-table below) is 1 point, which cor-
responds to an error reduction of 14%. In addi-
tion, the performance of the adapted model for
Joint S&amp;T obviously surpass that of (Jiang et al.,
2008), which achieves an F1 of 93.41% for Joint
S&amp;T, although with more complicated models and
features.
Due to the obvious improvement brought by an-
notation adaptation to both word segmentation and
Joint S&amp;T, we can safely conclude that the knowl-
edge can be effectively transferred from on an-
</bodyText>
<table confidence="0.7745145">
Input Type Parsing F1%
gold-standard segmentation 82.35
baseline segmentation 80.28
adapted segmentation 81.07
</table>
<tableCaption confidence="0.750514">
Table 5: Chinese parsing results with different
word segmentation results as input.
</tableCaption>
<bodyText confidence="0.9996964">
notation standard to another, although using such
a simple strategy. To obtain further information
about what kind of errors be alleviated by annota-
tion adaptation, we conduct an initial error analy-
sis for Joint S&amp;T on the developing set of CTB. It
is reasonable to investigate the error reduction of
Recall for each word cluster grouped together ac-
cording to their POS tags. From Table 4 we find
that out of 30 word clusters appeared in the devel-
oping set of CTB, 13 clusters benefit from the an-
notation adaptation strategy, while 4 clusters suf-
fer from it. However, the compositive error rate of
Recall for all word clusters is reduced by 20.66%,
such a fact invalidates the effectivity of annotation
adaptation.
</bodyText>
<subsectionHeader confidence="0.997069">
5.3 Contribution to Chinese Parsing
</subsectionHeader>
<bodyText confidence="0.999995476190476">
We adopt the Chinese parser of Xiong et al.
(2005), and train it on the training set of CTB 5.0
as described before. To sketch the error propaga-
tion to parsing from word segmentation, we rede-
fine the constituent span as a constituent subtree
from a start character to a end character, rather
than from a start word to a end word. Note that if
we input the gold-standard segmented test set into
the parser, the F-measure under the two definitions
are the same.
Table 5 shows the parsing accuracies with dif-
ferent word segmentation results as the parser’s
input. The parsing F-measure corresponding to
the gold-standard segmentation, 82.35, represents
the “oracle” accuracy (i.e., upperbound) of pars-
ing on top of automatic word segmention. After
integrating the knowledge from PD, the enhanced
word segmenter gains an F-measure increment of
0.8 points, which indicates that 38% of the error
propagation from word segmentation to parsing is
reduced by our annotation adaptation strategy.
</bodyText>
<sectionHeader confidence="0.998671" genericHeader="conclusions">
6 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.998823333333333">
This paper presents an automatic annotation adap-
tation strategy, and conducts experiments on a
classic problem: word segmentation and Joint
</bodyText>
<page confidence="0.992318">
528
</page>
<bodyText confidence="0.999905066666666">
S&amp;T. To adapt knowledge from a corpus with an
annotation standard that we don’t require, a clas-
sifier trained on this corpus is used to pre-process
the corpus with the desired annotated standard, on
which a second classifier is trained with the first
classifier’s predication results as additional guide
information. Experiments of annotation adapta-
tion from PD to CTB 5.0 for word segmentation
and POS tagging show that, this strategy can make
effective use of the knowledge from the corpus
with different annotations. It obtains considerable
F-measure increment, about 0.8 point for word
segmentation and 1 point for Joint S&amp;T, with cor-
responding error reductions of 30.2% and 14%.
The final result outperforms the latest work on the
same corpus which uses more complicated tech-
nologies, and achieves the state-of-the-art. More-
over, such improvement further brings striking F-
measure increment for Chinese parsing, about 0.8
points, corresponding to an error propagation re-
duction of 38%.
In the future, we will continue to research on
annotation adaptation for other NLP tasks which
have different annotation-style corpora. Espe-
cially, we will pay efforts to the annotation stan-
dard adaptation between different treebanks, for
example, from HPSG LinGo Redwoods Treebank
to PTB, or even from a dependency treebank
to PTB, in order to obtain more powerful PTB
annotation-style parsers.
</bodyText>
<sectionHeader confidence="0.970243" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99995125">
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are especially grateful to
Fernando Pereira and the anonymous reviewers
for pointing us to relevant domain adaption refer-
ences. We also thank Yang Liu and Haitao Mi for
helpful discussions.
</bodyText>
<sectionHeader confidence="0.998708" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99811295">
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. In Computational
Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Aoife Cahill and Mairead Mccarthy. 2007. Auto-
matic annotation of the penn treebank with lfg f-
structure information. In in Proceedings of the
LREC Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated
Language Data.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201–228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42th Annual Meeting of the Association
for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1–8, Philadelphia, USA.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal of Artifi-
cial Intelligence Research.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings ofACL.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings ofACL.
Julia Hockenmaier and Mark Steedman. 2008. Ccg-
bank: a corpus of ccg derivations and dependency
structures extracted from the penn treebank. In
Computational Linguistics, volume 33(3), pages
355–396.
Wenbin Jiang, Liang Huang, Yajuan L¨u, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81–88.
</reference>
<page confidence="0.983885">
529
</page>
<reference confidence="0.999529372093024">
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings ofACL, pages 91–
98.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics (COLING 2002).
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings ofNAACL.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0). In Technical
Reports.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70–81.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
</reference>
<page confidence="0.997101">
530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382970">
<title confidence="0.795094">Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study</title>
<author confidence="0.940492">Jiang Huang Liu</author>
<affiliation confidence="0.972831">Lab. of Intelligent Information Processing Research Institute of Computing Technology 1350 Charleston Rd.</affiliation>
<address confidence="0.828486">Chinese Academy of Sciences Mountain View, CA 94043, USA Box 2704, Beijing 100190, China lianghuang@google.com</address>
<email confidence="0.998663">liang.huang.sh@gmail.com</email>
<abstract confidence="0.998693555555556">Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corwith annotation guidelines or standards. This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another. We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation. We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the chinese treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the second workshop on Chinese language processing.</booktitle>
<contexts>
<context position="6687" citStr="Bikel and Chiang, 2000" startWordPosition="1073" endWordPosition="1076">a” as a compound. Also CTB has four verbal categories (VV for normal verbs, and VC for copulas, etc.) while PD has only one verbal tag (v) (Xia, 2000). It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures which is very useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. Indeed, many recent efforts on Chinese-English translation and Chinese parsing use the CTB as the de facto segmentation and tagging standards, but suffers from the limited size of training data (Chiang, 2007; Bikel and Chiang, 2000). We believe this is also a reason why stateof-the-art accuracy for Chinese parsing is much lower than that of English (CTB is only half the size of PTB). Our experiments show that adaptation from PD to CTB results in a significant improvement in segmentation and POS tagging, with error reductions of 30.2% and 14%, respectively. In addition, the improved accuracies from segmentation and tagging also lead to an improved parsing accuracy on CTB, reducing 38% of the error propagation from word segmentation to parsing. We envision this technique to be general and widely applicable to many other se</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the chinese treebank. In Proceedings of the second workshop on Chinese language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2889" citStr="Blitzer et al. (2006)" startWordPosition="444" endWordPosition="447">3) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo). These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training. The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al. (2006) and Daum´e III (2007) (and see below for discussions), so in this paper we focus on the less studied, but equally important problem of annotationstyle adaptation. We present a very simple yet effective strategy that enables us to utilize knowledge from a differently annotated corpora for the training of a model on a corpus with desired annotation. The basic idea is very simple: we first train on a source corpus, resulting in a source classifier, which is used to label the target corpus and results in a “sourcestyle” annotation of the target corpus. We then NR NN VV NR *E1 9112 43 AA4 i; 5 *6 </context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="19205" citStr="Brill (1995)" startWordPosition="3184" endWordPosition="3185">= -kB o α = b C−1C0 = Bo o α = b C0C1 = oÚ o α = b C1C2 = Úi3 o α = b C−1C1 = BÚ o α = b Pu(C0) = 0 o α = b T(C−2)T(C−1)T(C0)T(C1)T(C2) = 44444 o α = b Table 2: An example of basic features and guide features of standard-adaptation for word segmentation. Suppose we are considering the third character “o” in “-kB o Úi3*”. annotation-styles. Gao et al. (2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. They design some class-type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. In addition, many efforts have been devoted to manual treebank adaptation, where they adapt PTB to other grammar formalisms, such as </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2386" citStr="Buchholz and Marsi, 2006" startWordPosition="359" endWordPosition="362">theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example just for English treebanking there have been the Chomskian-style U.S. Vice-President visited China ns b n v U.S. Vice President visited-China Figure 1: Incompatible word segmentation and POS tagging standards between CTB (upper) and People’s Daily (below). Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo). These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training. The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al. (2006) and Daum´e III (2007) (and see below for discussions), so in this paper we focus on the less stu</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Mairead Mccarthy</author>
</authors>
<title>Automatic annotation of the penn treebank with lfg fstructure information.</title>
<date>2007</date>
<booktitle>In in Proceedings of the LREC Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data.</booktitle>
<contexts>
<context position="19884" citStr="Cahill and Mccarthy, 2007" startWordPosition="3288" endWordPosition="3291">wever, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. In addition, many efforts have been devoted to manual treebank adaptation, where they adapt PTB to other grammar formalisms, such as such as CCG and LFG (Hockenmaier and Steedman, 2008; Cahill and Mccarthy, 2007). However, they are heuristics-based and involve heavy human engineering. 526 5 Experiments Our adaptation experiments are conducted from People’s Daily (PD) to Penn Chinese Treebank 5.0 (CTB). These two corpora are segmented following different segmentation standards and labeled with different POS sets (see for example Figure 1). PD is much bigger in size, with about 100K sentences, while CTB is much smaller, with only about 18K sentences. Thus a classifier trained on CTB usually falls behind that trained on PD, but CTB is preferable because it also annotates tree structures, which is very us</context>
</contexts>
<marker>Cahill, Mccarthy, 2007</marker>
<rawString>Aoife Cahill and Mairead Mccarthy. 2007. Automatic annotation of the penn treebank with lfg fstructure information. In in Proceedings of the LREC Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>201--228</pages>
<contexts>
<context position="6662" citStr="Chiang, 2007" startWordPosition="1071" endWordPosition="1072"> “visited-China” as a compound. Also CTB has four verbal categories (VV for normal verbs, and VC for copulas, etc.) while PD has only one verbal tag (v) (Xia, 2000). It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures which is very useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. Indeed, many recent efforts on Chinese-English translation and Chinese parsing use the CTB as the de facto segmentation and tagging standards, but suffers from the limited size of training data (Chiang, 2007; Bikel and Chiang, 2000). We believe this is also a reason why stateof-the-art accuracy for Chinese parsing is much lower than that of English (CTB is only half the size of PTB). Our experiments show that adaptation from PD to CTB results in a significant improvement in segmentation and POS tagging, with error reductions of 30.2% and 14%, respectively. In addition, the improved accuracies from segmentation and tagging also lead to an improved parsing accuracy on CTB, reducing 38% of the error propagation from word segmentation to parsing. We envision this technique to be general and widely ap</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, pages 201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10522" citStr="Collins and Roark, 2004" startWordPosition="1723" endWordPosition="1726"> word sequence (or with POS) can be directly derived. Take segmentation for example, a character assigned a tag s or a subsequence of words assigned a tag sequence bm*e indicates a word. 2.2 Training Algorithm and Features Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x E X to outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation 4) mapping each training example (x, y) E X x Y to a feature vector 4)(x, y) E Rd, and a parameter vector α� E Rd corresponding to th</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10321" citStr="Collins, 2002" startWordPosition="1692" endWordPosition="1693">. For example, b-NN indicates that the character is the begin of a noun. After all characters of a sentence are assigned boundary tags (or with POS postfix) by a classifier, the corresponding word sequence (or with POS) can be directly derived. Take segmentation for example, a character assigned a tag s or a subsequence of words assigned a tag sequence bm*e indicates a word. 2.2 Training Algorithm and Features Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x E X to outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x)</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 1–8, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>In Journal of Artificial Intelligence Research.</journal>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. In Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Andi Wu</author>
<author>Mu Li</author>
</authors>
<title>Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="18952" citStr="Gao et al. (2004)" startWordPosition="3147" endWordPosition="3150">atures C−2 = -k C−1 = B C0 = o C1 = Ú C2 = i3 C−2C−1 = -kB C−1C0 = Bo C0C1 = oÚ C1C2 = Úi3 C−1C1 = BÚ Pu(C0) = 0 T (C−2)T (C−1)T(C0)T(C1)T(C2) = 44444 Guide Features α = b C−2 = -k o α = b C−1 = B o α = b C0 = o o α = b C1 = Ú o α = b C2 = i3 o α = b C−2C−1 = -kB o α = b C−1C0 = Bo o α = b C0C1 = oÚ o α = b C1C2 = Úi3 o α = b C−1C1 = BÚ o α = b Pu(C0) = 0 o α = b T(C−2)T(C−1)T(C0)T(C1)T(C2) = 44444 o α = b Table 2: An example of basic features and guide features of standard-adaptation for word segmentation. Suppose we are considering the third character “o” in “-kB o Úi3*”. annotation-styles. Gao et al. (2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. They design some class-type transformation templates and use the transformation-based errordriven learning method of Brill (1995) to learn what word delimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propag</context>
</contexts>
<marker>Gao, Wu, Li, 2004</marker>
<rawString>Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004. Adaptive chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank.</title>
<date>2008</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>3</issue>
<pages>355--396</pages>
<contexts>
<context position="19856" citStr="Hockenmaier and Steedman, 2008" startWordPosition="3284" endWordPosition="3287">elimiters should be modified. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging, not to mention other structure labeling tasks. Moreover, the processing procedure is divided into two isolated steps, conversion after segmentation, which suffers from error propagation and wastes the knowledge in the corpora. On the contrary, our strategy is automatic, generalizable and effective. In addition, many efforts have been devoted to manual treebank adaptation, where they adapt PTB to other grammar formalisms, such as such as CCG and LFG (Hockenmaier and Steedman, 2008; Cahill and Mccarthy, 2007). However, they are heuristics-based and involve heavy human engineering. 526 5 Experiments Our adaptation experiments are conducted from People’s Daily (PD) to Penn Chinese Treebank 5.0 (CTB). These two corpora are segmented following different segmentation standards and labeled with different POS sets (see for example Figure 1). PD is much bigger in size, with about 100K sentences, while CTB is much smaller, with only about 18K sentences. Thus a classifier trained on CTB usually falls behind that trained on PD, but CTB is preferable because it also annotates tree </context>
</contexts>
<marker>Hockenmaier, Steedman, 2008</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2008. Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank. In Computational Linguistics, volume 33(3), pages 355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Jiang, Huang, L¨u, Liu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Yajuan L¨u, and Qun Liu. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="1736" citStr="Marcus et al., 1993" startWordPosition="260" endWordPosition="263">epted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy. 1 Introduction Much of statistical NLP research relies on some sort of manually annotated corpora to train their models, but these resources are extremely expensive to build, especially at a large scale, for example in treebanking (Marcus et al., 1993). However the linguistic theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example just for English treebanking there have been the Chomskian-style U.S. Vice-President visited China ns b n v U.S. Vice President visited-China Figure 1: Incompatible word segmentation and POS tagging standards between CTB (upper) and People’s Daily (below). Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smal</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="14784" citStr="Martins et al., 2008" startWordPosition="2432" endWordPosition="2435"> source standard to target standard. This regularity is incorporated together with the knowledge learnt from the target corpus itself, so as to obtain enhanced predication accuracy. For a given un-classified character sequence, the decoding is analogous to the training. First, the character sequence is input into the source classifier to obtain an source standard annotated classification result, then it is input into the target classifier with this classification result as additional information to get the final result. This coincides with the stacking method for combining dependency parsers (Martins et al., 2008; Nivre and McDontarget classifier Figure 2: The pipeline for training. Figure 3: The pipeline for decoding. ald, 2008), and is also similar to the Pred baseline for domain adaptation in (Daum´e III and Marcu, 2006; Daum´e III, 2007). Figures 2 and 3 show the flow charts for training and decoding. The utilization of the source classifier’s classification result as additional guide information resorts to the introduction of new features. For the current considering character waiting for classification, the most intuitive guide features is the source classifier’s classification result itself. Ho</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="15685" citStr="McDonald et al., 2005" startWordPosition="2573" endWordPosition="2576">ning and decoding. The utilization of the source classifier’s classification result as additional guide information resorts to the introduction of new features. For the current considering character waiting for classification, the most intuitive guide features is the source classifier’s classification result itself. However, our effort isn’t limited to this, and more special features are introduced: the source classifier’s classification result is attached to every feature listed in Table 1 to get combined guide features. This is similar to feature design in discriminative dependency parsing (McDonald et al., 2005; Mcsource corpus train with normal features source classifier train with additional features target corpus source annotation classification result raw sentence source classifier target classifier source annotation classification result target annotation classification result 525 Donald and Pereira, 2006), where the basic features, composed of words and POSs in the context, are also conjoined with link direction and distance in order to obtain more special features. Table 2 shows an example of guide features and basic features, where “α = b ” represents that the source classifier classifies th</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL, pages 91– 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference.</booktitle>
<contexts>
<context position="8964" citStr="Ng and Low (2004)" startWordPosition="1452" endWordPosition="1455">) 2: α�+— 0 3: fort +— 1 .. T do 4: for i +— 1 .. N do 5: zi +— argmaxzEGEN(xi) 4&apos;(xi, z) · α� 6: if zi # yi then 7: α� +— α� + 4&apos;(xi, yi) − 4&apos;(xi, zi) 8: Output: Parameters α� clusive). While in Joint S&amp;T, each word is further annotated with a POS tag: C1:e1/t1 Ce1+1:e2/t2 .. Ce,,,_1+1:e,/tm where tk(k = L.m) denotes the POS tag for the word Cek_1+1:ek. 2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In Ng and Low (2004), Joint S&amp;T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters. In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&amp;T can achieve higher accuracy on both segmentation and POS tagging. This paper adopts the tag representation of Ng and Low (2004). For word segmentation only, there are four boundary tags: • b: the begin of the word • m: the middle of the word • e: the end of the word • s: a single-character word whil</context>
<context position="11650" citStr="Ng and Low (2004)" startWordPosition="1930" endWordPosition="1933">x Y to a feature vector 4)(x, y) E Rd, and a parameter vector α� E Rd corresponding to the feature vector. For an input character sequence x, we aim to find an output F(x) that satisfies: F(x) = argmax 4)(x, y) · α� (1) yEGEN(x) where 4)(x, y)·a denotes the inner product of feature vector 4)(x, y) and the parameter vector a. Algorithm 1 depicts the pseudo code to tune the parameter vector a. In addition, the “averaged parameters” technology (Collins, 2002) is used to alleviate overfitting and achieve stable performance. Table 1 lists the feature template and corresponding instances. Following Ng and Low (2004), the current considering character is denoted as C0, while the ith character to the left of C0 as C_Z, and to the right as CZ. There are additional two functions of which each returns some property of a character. Pu(·) is a boolean function that checks whether a character is a punctuation symbol (returns 1 for a punctuation, 0 for not). T(·) is a multi-valued function, it classifies a character into four classifications: number, date, English letter and others (returns 1, 2, 3 and 4, respectively). 3 Automatic Annotation Adaptation From this section, several shortened forms are adopted for r</context>
<context position="23259" citStr="Ng and Low, 2004" startWordPosition="3863" endWordPosition="3866">ith annotation adaptation. PD —* CTB means annotation adaptation from PD to CTB. For the upper sub-table, items of JST F1 are undefined since only segmentation is performs. While in the sub-table below, JST F1 is also undefined since the model trained on PD gives a POS set different from that of CTB. models. Comparing row 1 and 3 in the sub-table below with the corresponding rows in the upper sub-table, we validate that when word segmentation and POS tagging are conducted jointly, the performance for segmentation improves since the POS tags provide additional information to word segmentation (Ng and Low, 2004). We also see that for both segmentation and Joint S&amp;T, the performance sharply declines when a model trained on PD is tested on CTB (row 2 in each sub-table). In each task, only about 92% F1 is achieved. This obviously fall behind those of the models trained on CTB itself (row 3 in each sub-table), about 97% F1, which are used as the baselines of the following annotation adaptation experiments. segmentation only segmentation in Joint S&amp;T Joint S&amp;T 0.980 0.970 0.960 0.950 F measure 0.940 0.930 0.920 0.910 0.900 0.890 0.880 527 POS #Word #BaseErr #AdaErr ErrDec% AD 305 30 19 36.67 1 AS 76 0 0 B</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Proceedings of the Empirical Methods in Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christopher Manning Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The lingo redwoods treebank: Motivation and preliminary applications. In</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="2324" citStr="Oepen et al., 2002" startWordPosition="350" endWordPosition="353">eebanking (Marcus et al., 1993). However the linguistic theories underlying these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example just for English treebanking there have been the Chomskian-style U.S. Vice-President visited China ns b n v U.S. Vice President visited-China Figure 1: Incompatible word segmentation and POS tagging standards between CTB (upper) and People’s Daily (below). Penn Treebank (Marcus et al., 1993) the HPSG LinGo Redwoods Treebank (Oepen et al., 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from financial news (PTB/WSJ) to transcribed dialog (LinGo). These two problems seem be a great waste in human efforts, and it would be nice if one could automatically adapt from one annotation standard and/or domain to another in order to exploit much larger datasets for better training. The second problem, domain adaptation, is very well-studied, e.g. by Blitzer et al. (2006) and Daum´e III (2007) (and see bel</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Flickinger, Brants, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning Dan Flickinger, and Thorsten Brants. 2002. The lingo redwoods treebank: Motivation and preliminary applications. In In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="17495" citStr="Sarkar, 2001" startWordPosition="2859" endWordPosition="2860">er also learns the relative weights between the guide features and the basic features, so that the knowledge from both the source corpus and the target corpus are automatically integrated together. In fact, more complicated features can be adopted as guide information. For error tolerance, guide features can be extracted from n-best results or compacted lattices of the source classifier; while for the best use of the source classifier’s output, guide features can also be the classification results of several successive characters. We leave them as future research. 4 Related Works Co-training (Sarkar, 2001) and classifier combination (Vivre and McDonald, 2008) are two technologies for training improved dependency parsers. The co-training technology lets two different parsing models learn from each other during parsing an unlabelled corpus: one model selects some unlabelled sentences it can confidently parse, and provide them to the other model as additional training corpus in order to train more powerful parsers. The classifier combination lets graph-based and transition-based dependency parsers to utilize the features extracted from each other’s parsing results, to obtain combined, enhanced par</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the penn chinese treebank (3.0). In</title>
<date>2000</date>
<tech>Technical Reports.</tech>
<contexts>
<context position="6214" citStr="Xia, 2000" startWordPosition="1001" endWordPosition="1002">r Chinese parts-of-speech. The two corpora used in this study are the much larger People’s Daily (PD) (5.86M words) corpus (Yu et al., 2001) and the smaller but more popular Penn Chinese Treebank (CTB) (0.47M words) (Xue et al., 2005). They used very different segmentation standards as well as different POS tagsets and tagging guidelines. For example, in Figure 1, People’s Daily breaks “Vice-President” into two words while combines the phrase “visited-China” as a compound. Also CTB has four verbal categories (VV for normal verbs, and VC for copulas, etc.) while PD has only one verbal tag (v) (Xia, 2000). It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures which is very useful for downstream applications like parsing, summarization, and machine translation, yet it is much smaller in size. Indeed, many recent efforts on Chinese-English translation and Chinese parsing use the CTB as the de facto segmentation and tagging standards, but suffers from the limited size of training data (Chiang, 2007; Bikel and Chiang, 2000). We believe this is also a reason why stateof-the-art accuracy for Chinese parsing is much lower than that of English (CTB is </context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>Fei Xia. 2000. The part-of-speech tagging guidelines for the penn chinese treebank (3.0). In Technical Reports.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Parsing the penn chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>70--81</pages>
<contexts>
<context position="26402" citStr="Xiong et al. (2005)" startWordPosition="4465" endWordPosition="4468">tial error analysis for Joint S&amp;T on the developing set of CTB. It is reasonable to investigate the error reduction of Recall for each word cluster grouped together according to their POS tags. From Table 4 we find that out of 30 word clusters appeared in the developing set of CTB, 13 clusters benefit from the annotation adaptation strategy, while 4 clusters suffer from it. However, the compositive error rate of Recall for all word clusters is reduced by 20.66%, such a fact invalidates the effectivity of annotation adaptation. 5.3 Contribution to Chinese Parsing We adopt the Chinese parser of Xiong et al. (2005), and train it on the training set of CTB 5.0 as described before. To sketch the error propagation to parsing from word segmentation, we redefine the constituent span as a constituent subtree from a start character to a end character, rather than from a start word to a end word. Note that if we input the gold-standard segmented test set into the parser, the F-measure under the two definitions are the same. Table 5 shows the parsing accuracies with different word segmentation results as the parser’s input. The parsing F-measure corresponding to the gold-standard segmentation, 82.35, represents </context>
</contexts>
<marker>Xiong, Li, Liu, Lin, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proceedings of IJCNLP 2005, pages 70–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGHAN Workshop.</booktitle>
<contexts>
<context position="8759" citStr="Xue and Shen (2003)" startWordPosition="1419" endWordPosition="1422"> C1:e1 Ce1+1:e2 .. Cem−1+1:em where each subsequence Ci:j indicates a Chinese word spanning from characters Ci to Cj (both in523 Algorithm 1 Perceptron training algorithm. 1: Input: Training examples (xi, yi) 2: α�+— 0 3: fort +— 1 .. T do 4: for i +— 1 .. N do 5: zi +— argmaxzEGEN(xi) 4&apos;(xi, z) · α� 6: if zi # yi then 7: α� +— α� + 4&apos;(xi, yi) − 4&apos;(xi, zi) 8: Output: Parameters α� clusive). While in Joint S&amp;T, each word is further annotated with a POS tag: C1:e1/t1 Ce1+1:e2/t2 .. Ce,,,_1+1:e,/tm where tk(k = L.m) denotes the POS tag for the word Cek_1+1:ek. 2.1 Character Classification Method Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In Ng and Low (2004), Joint S&amp;T can also be treated as a character classification problem, where a boundary tag is combined with a POS tag in order to give the POS information of the word containing these characters. In addition, Ng and Low (2004) find that, compared with POS tagging after word segmentation, Joint S&amp;T can achieve higher accuracy on both segmentation and POS tagging. This paper adopts the tag rep</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as lmr tagging. In Proceedings of SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="5838" citStr="Xue et al., 2005" startWordPosition="939" endWordPosition="942">thod we choose Chinese word segmentation and part-of-speech tagging, where the problem of incompatible annotation standards is one of the most evident: so far no segmentation standard is widely accepted due to the lack of a clear definition of Chinese words, and the (almost complete) lack of morphology results in much bigger ambiguities and heavy debates in tagging philosophies for Chinese parts-of-speech. The two corpora used in this study are the much larger People’s Daily (PD) (5.86M words) corpus (Yu et al., 2001) and the smaller but more popular Penn Chinese Treebank (CTB) (0.47M words) (Xue et al., 2005). They used very different segmentation standards as well as different POS tagsets and tagging guidelines. For example, in Figure 1, People’s Daily breaks “Vice-President” into two words while combines the phrase “visited-China” as a compound. Also CTB has four verbal categories (VV for normal verbs, and VC for copulas, etc.) while PD has only one verbal tag (v) (Xia, 2000). It is preferable to transfer knowledge from PD to CTB because the latter also annotates tree structures which is very useful for downstream applications like parsing, summarization, and machine translation, yet it is much </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
</authors>
<title>Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan.</title>
<date>2001</date>
<tech>Technical report.</tech>
<marker>Yu, 2001</marker>
<rawString>Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan, Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao, and Weidong Zhan. 2001. Processing norms of modern chinese corpus. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10572" citStr="Zhang and Clark, 2007" startWordPosition="1731" endWordPosition="1734"> Take segmentation for example, a character assigned a tag s or a subsequence of words assigned a tag sequence bm*e indicates a word. 2.2 Training Algorithm and Features Now we will show the training algorithm of the classifier and the features used. Several classification models can be adopted here, however, we choose the averaged perceptron algorithm (Collins, 2002) because of its simplicity and high accuracy. It is an online training algorithm and has been successfully used in many NLP tasks, such as POS tagging (Collins, 2002), parsing (Collins and Roark, 2004), Chinese word segmentation (Zhang and Clark, 2007; Jiang et al., 2008), and so on. Similar to the situation in other sequence labeling problems, the training procedure is to learn a discriminative model mapping from inputs x E X to outputs y E Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results. Following Collins, we use a function GEN(x) enumerating the candidate results of an input x , a representation 4) mapping each training example (x, y) E X x Y to a feature vector 4)(x, y) E Rd, and a parameter vector α� E Rd corresponding to the feature vector. For an input character sequence </context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>