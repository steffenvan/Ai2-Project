<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.899437">
Shrinking Exponential Language Models
</title>
<note confidence="0.750516">
Stanley F. Chen
IBM T.J. Watson Research Center
P.O. Box 218, Yorktown Heights, NY 10598
</note>
<email confidence="0.997">
stanchen@watson.ibm.com
</email>
<sectionHeader confidence="0.993865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999498444444445">
In (Chen, 2009), we show that for a vari-
ety of language models belonging to the ex-
ponential family, the test set cross-entropy of
a model can be accurately predicted from its
training set cross-entropy and its parameter
values. In this work, we show how this rela-
tionship can be used to motivate two heuristics
for “shrinking” the size of a language model
to improve its performance. We use the first
heuristic to develop a novel class-based lan-
guage model that outperforms a baseline word
trigram model by 28% in perplexity and 1.9%
absolute in speech recognition word-error rate
on Wall Street Journal data. We use the second
heuristic to motivate a regularized version of
minimum discrimination information models
and show that this method outperforms other
techniques for domain adaptation.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998816">
An exponential model pΛ(y|x) is a model with a set
of features {f1(x, y), ... , fF (x, y)} and equal num-
ber of parameters A = {A1, ... , AF} where
</bodyText>
<equation confidence="0.998807166666667">
exp(�F i=1 Aifi(x, y))
pΛ(y|x) = (1)
Z
F
Htest ≈ Htrain + -yD |˜Ai |(2)
i=1
</equation>
<bodyText confidence="0.999894555555556">
where Htest denotes test set cross-entropy; Htrain de-
notes training set cross-entropy; D is the number of
events in the training data; the ˜Ai are regularized pa-
rameter estimates; and -y is a constant independent
of domain, training set size, and model type.1 This
relationship is strongest if the A˜ = {˜Ai} are esti-
mated using j1+j2 2 regularization (Kazama and Tsu-
jii, 2003). In j1 + j2 2 regularization, parameters are
chosen to optimize
</bodyText>
<equation confidence="0.7251725">
O`, +`. (A) = Ht a
rain + D
</equation>
<bodyText confidence="0.9998694">
for some α and σ. With (α = 0.5, σ2 = 6) and
taking -y = 0.938, test set cross-entropy can be pre-
dicted with eq. (2) for a wide range of models with a
mean error of a few hundredths of a nat, equivalent
to a few percent in perplexity.2
In this paper, we show how eq. (2) can be applied
to improve language model performance. First, we
use eq. (2) to analyze backoff features in exponential
n-gram models. We find that backoff features im-
prove test set performance by reducing the “size” of
a model 1D �D 1  |Ai  |rather than by improving train-
ing set performance. This suggests the following
principle for improving exponential language mod-
els: if a model can be “shrunk” without increasing
its training set cross-entropy, test set cross-entropy
should improve. We apply this idea to motivate
two language models: a novel class-based language
model and regularized minimum discrimination in-
formation (MDI) models. We show how these mod-
els outperform other models in both perplexity and
word-error rate on Wall Street Journal (WSJ) data.
The organization of this paper is as follows: In
Section 2, we analyze the use of backoff features in
n-gram models to motivate a heuristic for model de-
sign. In Sections 3 and 4, we introduce our novel
</bodyText>
<footnote confidence="0.912077428571428">
1The cross-entropy of a model PΛ(y|x) on some data D =
(x1, y1), ... , (xD, yD) is defined as − 1D PD
log PΛ(yj |xj ).
It is equivalent to the negative mean log-likelihood per event as
well as to log perplexity.
2A nat is a “natural” bit and is equivalent to loge a regular
bits. We use nats to be consistent with (Chen, 2009).
</footnote>
<equation confidence="0.626998">
Λ(x)
</equation>
<bodyText confidence="0.9998755">
and where ZΛ(x) is a normalization factor. In
(Chen, 2009), we show that for many types of ex-
ponential language models, if a training and test set
are drawn from the same distribution, we have
</bodyText>
<equation confidence="0.9918155">
F �F A2i (3)
i=1 1
|Ai |+ 2σ2D
i=1
</equation>
<page confidence="0.996834">
468
</page>
<note confidence="0.729847142857143">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 468–476,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
features Heval Hpred Htrain |˜λi|
� D
3g 2.681 2.724 2.341 0.408
2g+3g 2.528 2.513 2.248 0.282
1g+2g+3g 2.514 2.474 2.241 0.249
</note>
<tableCaption confidence="0.997776">
Table 1: Various statistics for letter trigram models built
</tableCaption>
<bodyText confidence="0.8296858">
on a 1k-word training set. Heval is the cross-entropy of
the evaluation data; Hpred is the predicted test set cross-
entropy according to eq. (2); and Htrain is the training
set cross-entropy. The evaluation data is drawn from the
same distribution as the training; H values are in nats.
</bodyText>
<figure confidence="0.462162">
predicted letter
</figure>
<figureCaption confidence="0.620547714285714">
Figure 1: Nonzero ˜λi values for bigram features in let-
ter bigram model without unigram backoff features. If
we denote bigrams as wj−1wj, each column contains the
˜λi’s corresponding to all bigrams with a particular wj.
The ‘×’ marks represent the average |˜λi |in each column;
this average includes history words for which no feature
exists or for which ˜λi = 0.
</figureCaption>
<bodyText confidence="0.99838825">
class-based model and discuss MDI domain adapta-
tion, and compare these methods against other tech-
niques on WSJ data. Finally, in Sections 5 and 6 we
discuss related work and conclusions.3
</bodyText>
<sectionHeader confidence="0.880116" genericHeader="method">
2 N-Gram Models and Backoff Features
</sectionHeader>
<bodyText confidence="0.999283111111111">
In this section, we use eq. (2) to explain why backoff
features in exponential n-gram models improve per-
formance, and use this analysis to motivate a general
heuristic for model design. An exponential n-gram
model contains a binary feature fω for each n&apos;-gram
ω occurring in the training data for n&apos; ≤ n, where
fω(x, y) = 1 iff xy ends in ω. We refer to features
corresponding to n&apos;-grams for n&apos; &lt; n as backoff
features; it is well known that backoff features help
</bodyText>
<footnote confidence="0.853392">
3A long version of this paper can be found at (Chen, 2008).
predicted letter
</footnote>
<figureCaption confidence="0.9984495">
Figure 2: Like Figure 1, but for model with unigram
backoff features.
</figureCaption>
<bodyText confidence="0.996108">
performance a great deal. We present statistics in
Table 1 for various letter trigram models built on the
same data set. In these and all later experiments, all
models are regularized with `1 + `2 2 regularization
with (α = 0.5, σ2 = 6). The last row corresponds to
a normal trigram model; the second row corresponds
to a model lacking unigram features; and the first
row corresponds to a model with no unigram or bi-
gram features. As backoff features are added, we see
that the training set cross-entropy improves, which
is not surprising since the number of features is in-
creasing. More surprising is that as we add features,
the “size” of the model D EF1  |λi  |decreases.
We can explain these results by examining a sim-
ple example. Consider an exponential model con-
sisting of the features f1(x, y) and f2(x, y) with pa-
rameter values ˜λ1 = 3 and ˜λ2 = 4. From eq. (1),
this model has the form
</bodyText>
<equation confidence="0.98651175">
p˜Λ(y |x) = exp (3f1(x, y) + 4f2 (x, y)) (4)
ZΛ (x)
Now, consider creating a new feature f3(x, y) =
f1(x, y)+f2(x, y) and setting our parameters as fol-
lows: λnew
1 = 0, λnew
2 = 1, and λnew
3 = 3. Substitut-
</equation>
<bodyText confidence="0.999365142857143">
ing into eq. (1), we see that pΛnew(y|x) = p˜Λ(y|x)
for all x, y. As the distribution this model de-
scribes does not change, neither will its training per-
formance. However, the (unscaled) size EFi=1 |λi|
of the model has been reduced from 3+4=7 to
0+1+3=4, and consequently by eq. (2) we predict
that test performance will improve.4
</bodyText>
<figure confidence="0.982285260869565">
4When sgn(˜λ1) = sgn(˜λ2), PFi=1 |λi |is reduced most by
a
-1
-2
-3
-4
5
4
3
2
0
1
a
-1
-2
-3
-4
5
4
3
2
0
1
</figure>
<page confidence="0.999721">
469
</page>
<bodyText confidence="0.998605714285714">
In fact, since pΛnew = p˜Λ, test performance will
remain the same. The catch is that eq. (2) applies
only to the regularized parameter estimates for a
model, and in general, Λnew will not be the regu-
larized parameter estimates for the expanded feature
set. We can compute the actual regularized parame-
ters ˜Λnew for which eq. (2) will apply; this may im-
prove predicted performance even more.
Hence, by adding “redundant” features to a model
to shrink its total size EFi=1 |˜λi|, we can improve
predicted performance (and perhaps also actual per-
formance). This analysis suggests the following
technique for improving model performance:
Heuristic 1 Identify groups offeatures which will
tend to have similar ˜λi values. For each such fea-
ture group, add a new feature to the model that is
the sum of the original features.
The larger the original ˜λi’s, the larger the reduction
in model size and the higher the predicted gain.
Given this perspective, we can explain why back-
off features improve n-gram model performance.
For simplicity, consider a bigram model, one with-
out unigram backoff features. It seems intuitive
that probabilities of the form p(wj|wj−1) are sim-
ilar across different wj−1, and thus so are the ˜λi for
the corresponding bigram features. (If a word has
a high unigram probability, it will also tend to have
high bigram probabilities.) In Figure 1, we plot the
nonzero ˜λi values for all (bigram) features in a bi-
gram model without unigram features. Each column
contains the ˜λi values for a different predicted word
wj, and the ‘×’ mark in each column is the average
value of |˜λi |over all history words wj−1. We see
that the average |˜λi |for each word wj is often quite
far from zero, which suggests creating features
</bodyText>
<equation confidence="0.99684">
fwj(x,y) = � fwj_1wj(x,y) (5)
wj_1
</equation>
<bodyText confidence="0.868017777777778">
to reduce the overall size of the model.
In fact, these features are exactly unigram backoff
features. In Figure 2, we plot the nonzero ˜λi values
for all bigram features after adding unigram backoff
features. We see that the average |˜λi|’s are closer
to zero, implying that the model size EFi=1 |˜λi |has
setting anew
3 to the ˜ai with the smaller magnitude, and the size
of the reduction is equal to |anew
</bodyText>
<page confidence="0.861509">
3 1. If sgn(˜a1) =� sgn(˜a2), no
</page>
<bodyText confidence="0.448695">
reduction is possible through this transformation.
</bodyText>
<table confidence="0.9723">
|˜λi|
� D
Heval Hpred Htrain
word n-gram 4.649 4.672 3.354 1.405
model M 4.536 4.544 3.296 1.330
</table>
<tableCaption confidence="0.993883">
Table 2: Various statistics for word and class trigram
models built on 100k sentences of WSJ training data.
</tableCaption>
<bodyText confidence="0.999860666666667">
been significantly decreased. We can extend this
idea to higher-order n-gram models as well; e.g., bi-
gram parameters can shrink trigram parameters, and
can in turn be shrunk by unigram parameters. As
shown in Table 1, both training set cross-entropy and
model size can be reduced by this technique.
</bodyText>
<sectionHeader confidence="0.986186" genericHeader="method">
3 Class-Based Language Models
</sectionHeader>
<bodyText confidence="0.944194">
In this section, we show how we can use Heuris-
tic 1 to design a novel class-based model that outper-
forms existing models in both perplexity and speech
recognition word-error rate. We assume a word w is
always mapped to the same class c(w). For a sen-
tence w1 · · · wl, we have
</bodyText>
<equation confidence="0.999551666666667">
p(w1 ··· wl) = Hl+1
j=1 p(cj|c1 ··· cj−1, w1 ··· wj−1)×
Hlj=1 p(wj|c1 ··· cj, w1 ··· wj−1) (6)
</equation>
<bodyText confidence="0.99992175">
where cj = c(wj) and cl+1 is the end-of-sentence
token. We use the notation png(y|ω) to denote an ex-
ponential n-gram model, a model containing a fea-
ture for each suffix of each ωy occurring in the train-
ing set. We use png(y|ω1, ω2) to denote a model con-
taining all features in png(y|ω1) and png(y|ω2).
We can define a class-based n-gram model by
choosing parameterizations for the distributions
p(cj |· · · ) and p(wj |· · · ) in eq. (6) above. For exam-
ple, the most widely-used class-based n-gram model
is the one introduced by Brown et al. (1992); we re-
fer to this model as the IBM class model:
</bodyText>
<equation confidence="0.999108">
p(cj|c1 ··· cj−1, w1 ··· wj−1)= png(cj|cj−2cj−1)
p(wj|c1 ··· cj, w1 ··· wj−1)= png(wj|cj) (7)
</equation>
<bodyText confidence="0.999919285714286">
(In the original work, non-exponential n-gram mod-
els are used.) Clearly, there is a large space of pos-
sible class-based models.
Now, we discuss how we can use Heuristic 1 to
design a novel class-based model by using class in-
formation to “shrink” a word-based n-gram model.
The basic idea is as follows: if we have an n-gram ω
</bodyText>
<page confidence="0.986162">
470
</page>
<bodyText confidence="0.999917636363637">
and another n-gram ω0 created by replacing a word
in ω with a similar word, then the two correspond-
ing features should have similar ˜λi’s. For exam-
ple, it seems intuitive that the n-grams on Monday
morning and on Tuesday morning should have sim-
ilar ˜λi’s. Heuristic 1 tells us how to take advantage
of this observation to improve model performance.
Let’s begin with a word trigram model
png(wj|wj−2wj−1). First, we would like to
convert this model into a class-based model.
Without loss of generality, we have
</bodyText>
<equation confidence="0.999039">
p(wj|wj−2wj−1) = Ecj p(wj, cj|wj−2wj−1)
= Ecj p(cj|wj−2wj−1)p(wj|wj−2wj−1cj) (8)
</equation>
<bodyText confidence="0.999934909090909">
Thus, it seems reasonable to use the distributions
png(cj|wj−2wj−1) and png(wj|wj−2wj−1cj) as the
starting point for our class model. This model can
express the same set of word distributions as our
original model, and hence may have a similar train-
ing cross-entropy. In addition, this transformation
can be viewed as shrinking together word n-grams
that differ only in wj. That is, we expect that pairs
of n-grams wj−2wj−1wj that differ only in wj (be-
longing to the same class) should have similar ˜λi.
From Heuristic 1, we can make new features
</bodyText>
<equation confidence="0.9698045">
fwj−2wj−1cj(x,y) = � fwj−2wj−1wj(x,y) (9)
wjEcj
</equation>
<bodyText confidence="0.999939">
These are exactly the features in png(cj|wj−2wj−1).
When applying Heuristic 1, all features typically be-
long to the same model, but even when they don’t
one can achieve the same net effect.
Then, we can use Heuristic 1 to also shrink to-
gether n-gram features for n-grams that differ only
in their histories. For example, we can create new
features of the form
</bodyText>
<equation confidence="0.991245">
fcj−2cj−1cj(x, y) = � fwj−2wj−1cj(x, y) (10)
wj−2Ecj−2,wj−1Ecj−1
</equation>
<bodyText confidence="0.999951">
This corresponds to replacing png(cj|wj−2wj−1)
with the distribution png(cj|cj−2cj−1,wj−2wj−1).
We refer to the resulting model as model M:
</bodyText>
<equation confidence="0.99292">
p(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1,wj−2wj−1)
p(wj|c1···cj,w1···wj−1)=png(wj|wj−2wj−1cj) (11)
</equation>
<bodyText confidence="0.999854866666667">
By design, it is meant to have similar training set
cross-entropy as a word n-gram model while being
significantly smaller.
To give an idea of whether this model behaves as
expected, in Table 2 we provide statistics for this
model (as well as for an exponential word n-gram
model) built on 100k WSJ training sentences with 50
classes using the same regularization as before. We
see that model M is both smaller than the baseline
and has a lower training set cross-entropy, similar to
the behavior found when adding backoff features to
word n-gram models in Section 2. As long as eq. (2)
holds, model M should have good test performance;
in (Chen, 2009), we show that eq. (2) does indeed
hold for models of this type.
</bodyText>
<subsectionHeader confidence="0.8579">
3.1 Class-Based Model Comparison
</subsectionHeader>
<bodyText confidence="0.999963107142858">
In this section, we compare model M against other
class-based models in perplexity and word-error
rate. The training data is 1993 WSJ text with verbal-
ized punctuation from the CSR-III Text corpus, and
the vocabulary is the union of the training vocabu-
lary and 20k-word “closed” test vocabulary from the
first WSJ CSR corpus (Paul and Baker, 1992). We
evaluate training set sizes of 1k, 10k, 100k, and 900k
sentences. We create three different word classings
containing 50, 150, and 500 classes using the algo-
rithm of Brown et al. (1992) on the largest training
set.5 For each training set and number of classes, we
build 3-gram and 4-gram versions of each model.
From the verbalized punctuation data from the
training and test portions of the WSJ CSR corpus,
we randomly select 2439 unique utterances (46888
words) as our evaluation set. From the remaining
verbalized punctuation data, we select 977 utter-
ances (18279 words) as our development set.
We compare the following model types: con-
ventional (i.e., non-exponential) word n-gram mod-
els; conventional IBM class n-gram models in-
terpolated with conventional word n-gram models
(Brown et al., 1992); and model M. All conven-
tional n-gram models are smoothed with modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
except we also evaluate word n-gram models with
Katz smoothing (Katz, 1987). Note: Because word
</bodyText>
<footnote confidence="0.9796765">
5One can imagine choosing word classes to optimize model
shrinkage; however, this is not an avenue we pursued.
</footnote>
<page confidence="0.985436">
471
</page>
<table confidence="0.9981693">
training set (sents.)
1k 10k 100k 900k
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, modified KN
3g 488.4 270.6 168.2 121.5
4g 486.8 267.4 163.6 114.4
model M
3g, 50c 341.5 210.0 144.5 110.9
3g, 150c 342.6 203.7 140.0 108.0
3g, 500c 387.5 212.7 142.2 108.1
4g, 50c 345.8 209.0 139.1 101.6
4g, 150c 344.1 202.8 135.7 99.1
4g, 500c 390.7 211.1 138.5 100.6
conventional word n-gram, Katz
3g 579.3 317.1 196.7 137.5
4g 592.6 325.6 202.4 136.7
interpolated IBM class model
3g, 50c
3g, 150c
3g, 500c
4g, 50c
4g, 150c
4g, 500c
358.4 224.5 156.8 117.8
346.5 210.5 149.0 114.7
372.6 210.9 145.8 112.3
362.1 220.4 149.6 109.1
346.3 207.8 142.5 105.2
371.5 207.9 140.5 103.6
</table>
<tableCaption confidence="0.9920815">
Table 3: WSJ perplexity results. The best performance for each training set for each model type is highlighted in bold.
Table 4: WSJ lattice rescoring results; all values are word-error rates. The best performance for each training set size
</tableCaption>
<table confidence="0.989067548387097">
for each model type is highlighted in bold. Each 0.1% in error rate corresponds to about 47 errors.
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, Katz
3g 35.5% 30.7% 26.2% 22.7%
4g 35.6% 30.9% 26.3% 22.7%
interpolated IBM class model
3g, 50c
3g, 150c
3g, 500c
4g, 50c
4g, 150c
4g, 500c
32.2% 28.7% 25.2% 22.5%
31.8% 28.1% 25.0% 22.3%
32.5% 28.5% 24.5% 22.1%
32.2% 28.6% 25.0% 22.0%
31.8% 28.0% 24.6% 21.8%
32.7% 28.3% 24.5% 21.6%
training set (sents.)
1k 10k 100k 900k
conventional word n-gram, modified KN
3g 34.5% 30.5% 26.1% 22.6%
4g 34.5% 30.4% 25.7% 22.3%
model M
3g, 50c 30.8% 27.4% 24.0% 21.7%
3g, 150c 31.0% 27.1% 23.8% 21.5%
3g, 500c 32.3% 27.8% 23.9% 21.4%
4g, 50c 30.8% 27.5% 23.9% 21.2%
4g, 150c 31.0% 27.1% 23.5% 20.8%
4g, 500c 32.4% 27.9% 24.1% 21.1%
</table>
<bodyText confidence="0.999147368421052">
classes are derived from the largest training set, re-
sults for word models and class models are compa-
rable only for this data set. The interpolated model is
the most popular state-of-the-art class-based model
in the literature, and is the only model here using the
development set to tune interpolation weights.
We display the perplexities of these models on the
evaluation set in Table 3. Model M performs best of
all (even without interpolating with a word n-gram
model), outperforming the interpolated model with
every training set and achieving its largest reduction
in perplexity (4%) on the largest training set. While
these perplexity reductions are quite modest, what
matters more is speech recognition performance.
For the speech recognition experiments, we use
a cross-word quinphone system built from 50 hours
of Broadcast News data. The system contains 2176
context-dependent states and a total of 50336 Gaus-
sians. To evaluate our language models, we use lat-
tice rescoring. We generate lattices on both our de-
velopment and evaluation data sets using the Latt-
AIX decoder (Saon et al., 2005) in the Attila speech
recognition system (Soltau et al., 2005). The lan-
guage model for lattice generation is created by
building a modified Kneser-Ney-smoothed word tri-
gram model on our largest training set; this model is
pruned to contain a total of 350k n-grams using the
algorithm of Stolcke (1998). We choose the acoustic
weight for each model to optimize word-error rate
on the development set.
In Table 4, we display the word-error rates for
each model. If we compare the best performance
of model M for each training set with that of the
state-of-the-art interpolated class model, we find that
model M is superior by 0.8–1.0% absolute. These
gains are much larger than are suggested by the
perplexity gains of model M over the interpolated
model; as has been observed earlier, perplexity is
</bodyText>
<page confidence="0.993908">
472
</page>
<figure confidence="0.964000272727273">
Heval
E |˜a.|
Hpred Htrain D
baseline n-gram model
1k 5.915 5.875 2.808 3.269
10k 5.212 5.231 3.106 2.265
100k 4.649 4.672 3.354 1.405
MDI n-gram model
1k 5.444 5.285 2.678 2.780
10k 5.031 4.973 3.053 2.046
100k 4.611 4.595 3.339 1.339
</figure>
<tableCaption confidence="0.502813666666667">
Table 5: Various statistics for WSJ trigram models, with
and without a Broadcast News prior model. The first col-
umn is the size of the in-domain training set in sentences.
</tableCaption>
<bodyText confidence="0.9998514">
not a reliable predictor of speech recognition perfor-
mance. While we can only compare class models
with word models on the largest training set, for this
training set model M outperforms the baseline Katz-
smoothed word trigram model by 1.9% absolute.6
</bodyText>
<sectionHeader confidence="0.994753" genericHeader="method">
4 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.999972888888889">
In this section, we introduce another heuristic for
improving exponential models and show how this
heuristic can be used to motivate a regularized ver-
sion of minimum discrimination information (MDI)
models (Della Pietra et al., 1992). Let’s say we have
a model p˜Λ estimated from one training set and a
“similar” model q estimated from an independent
training set. Imagine we use q as a prior model for
pΛ; i.e., we make a new model
</bodyText>
<equation confidence="0.9930695">
e exp(EF (12)
1 Anew fi(x, y))
(�x� =I x�
pΛnewy q(y ZΛnew(x)
</equation>
<bodyText confidence="0.893414384615385">
Then, choose Λnew such that pq
Λnew(y|x) = p˜Λ(y|x)
for all x, y (assuming this is possible). If q is “simi-
lar” to p˜Λ, then we expect the size D Ei= 1  |λZ ew  |of
pqto be less than that of p˜Λ. Since they describe
Λnew
the same distribution, their training set cross-entropy
will be the same. By eq. (2), we expect pqΛnew to
have better test set performance than p˜Λ after reesti-
mation.7 In (Chen, 2009), we show that eq. (2) does
indeed hold for models with priors; q need not be
accounted for in computing model size as long as it
is estimated on a separate training set.
</bodyText>
<footnote confidence="0.941296">
6Results for several other baseline language models and with
a different acoustic model are given in (Chen, 2008).
7That is, we expect the regularized parameters ˜Λnew to yield
improved performance.
</footnote>
<bodyText confidence="0.999752323529412">
This analysis suggests the following method for
improving model performance:
Heuristic 2 Find a “similar” distribution estimated
from an independent training set, and use this distri-
bution as a prior.
It is straightforward to apply this heuristic to the task
of domain adaptation for language modeling. In the
usual formulation of this task, we have a test set and
a small training set from the same domain, and a
large training set from a different domain. The goal
is to use the data from the outside domain to max-
imally improve language modeling performance on
the target domain. By Heuristic 2, we can build a
language model on the outside domain, and use this
model as the prior model for a language model built
on the in-domain data. This method is identical to
the MDI method for domain adaptation, except that
we also apply regularization.
In our domain adaptation experiments, our out-
of-domain data is a 100k-sentence Broadcast News
training set. For our in-domain WSJ data, we use
training set sizes of 1k, 10k, and 100k sentences. We
build an exponential n-gram model on the Broad-
cast News data and use this model as the prior model
q(y|x) in eq. (12) when building an exponential n-
gram model on the in-domain data. In Table 5, we
display various statistics for trigram models built on
varying amounts of in-domain data when using a
Broadcast News prior and not. Across training sets,
the MDI models are both smaller in D EF1 |˜λi |and
have better training set cross-entropy than the un-
adapted models built on the same data. By eq. (2),
the adapted models should have better test perfor-
mance and we verify this in the next section.
</bodyText>
<subsectionHeader confidence="0.997298">
4.1 Domain Adaptation Method Comparison
</subsectionHeader>
<bodyText confidence="0.999864272727273">
In this section, we examine how MDI adapta-
tion compares to other state-of-the-art methods for
domain adaptation in both perplexity and speech
recognition word-error rate. For these experiments,
we use the same development and evaluation sets
and lattice rescoring setup from Section 3.1.
The most widely-used techniques for domain
adaptation are linear interpolation and count merg-
ing. In linear interpolation, separate n-gram models
are built on the in-domain and out-of-domain data
and are interpolated together. In count merging, the
</bodyText>
<figure confidence="0.374286">
pqas follows:
Λnew
</figure>
<page confidence="0.996914">
473
</page>
<tableCaption confidence="0.980201666666667">
Table 6: WSJ perplexity and lattice rescoring results for
domain adaptation models. Values on the left are perplex-
ities and values on the right are word-error rates.
</tableCaption>
<bodyText confidence="0.999933130434783">
in-domain and out-of-domain data are concatenated
into a single training set, and a single n-gram model
is built on the combined data set. The in-domain
data set may be replicated several times to more
heavily weight this data. We also consider the base-
line of not using the out-of-domain data.
In Table 6, we display perplexity and word-error
rates for each method, for both trigram and 4-gram
models and with varying amounts of in-domain
training data. The last method corresponds to the
exponential MDI model; all other methods employ
conventional (non-exponential) n-gram models with
modified Kneser-Ney smoothing. In count merging,
only one copy of the in-domain data is included in
the training set; including more copies does not im-
prove evaluation set word-error rate.
Looking first at perplexity, MDI models outper-
form the next best method, linear interpolation, by
about 10% in perplexity on the smallest data set and
3% in perplexity on the largest. In terms of word-
error rate, MDI models again perform best of all,
outperforming interpolation by 0.3–0.7% absolute
and count merging by 0.1–0.4% absolute.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999722">
5.1 Class-Based Language Models
</subsectionHeader>
<bodyText confidence="0.999942428571429">
In past work, the most common baseline models are
Katz-smoothed word trigram models. Compared to
this baseline, model M achieves a perplexity reduc-
tion of 28% and word-error rate reduction of 1.9%
absolute with a 900k-sentence training set. The most
closely-related existing model to model M is the
model fullibmpredict proposed by Goodman (2001):
</bodyText>
<equation confidence="0.99857725">
p(cj|cj−2cj−1,wj−2wj−1)=
λ p(cj|wj−2wj−1)+(1−λ) p(cj|cj−2cj−1)
p(wj|cj−2cj−1cj,wj−2wj−1)=
µ p(wj|wj−2wj−1cj)+(1−µ) p(wj|cj−2cj−1cj) (13)
</equation>
<bodyText confidence="0.999963552631579">
This is similar to model M except that linear in-
terpolation is used to combine word and class his-
tory information, and there is no analog to the fi-
nal term in eq. (13) in model M. Using the North
American Business news corpus, the largest perplex-
ity reduction achieved over a Katz-smoothed trigram
model baseline by fullibmpredict is about 25%, with
a training set of 1M words. In N-best list rescor-
ing with a 284M-word training set, the best result
achieved for an individual class-based model is an
0.5% absolute reduction in word-error rate.
To situate the quality of our results, we also re-
view the best perplexity and word-error rate results
reported for class-based language models relative
to conventional word n-gram model baselines. In
terms of absolute word-error rate, the best gains we
found in the literature are from multi-class com-
posite n-gram models, a variant of the IBM class
model (Yamamoto and Sagisaka, 1999; Yamamoto
et al., 2003). These are called composite models
because frequent word sequences can be concate-
nated into single units within the model; the term
multi-class refers to choosing different word clus-
terings depending on word position. In experiments
on the ATR spoken language database, Yamamoto et
al. (2003) report a reduction in perplexity of 9% and
an increase in word accuracy of 2.2% absolute over
a Katz-smoothed trigram model.
In terms of perplexity, the best gains we found
are from SuperARV language models (Wang and
Harper, 2002; Wang et al., 2002; Wang et al., 2004).
In these models, classes are based on abstract role
values as given by a Constraint Dependency Gram-
mar. The class and word prediction distributions are
n-gram models that back off to a variety of mixed
word/class histories in a specific order. With a WSJ
training set of 37M words and a Katz-smoothed tri-
gram model baseline, a perplexity reduction of up to
</bodyText>
<table confidence="0.998455214285714">
in-domain data (sents.) in-domain data (sents.)
1k 10k 100k 1k 10k 100k
in-domain data only
3g 488.4 270.6 168.2 34.5% 30.5% 26.1%
4g 486.8 267.4 163.6 34.5% 30.4% 25.7%
count merging
3g 503.1 290.9 170.7 30.4% 28.3% 25.2%
4g 497.1 284.9 165.3 30.0% 28.0% 25.3%
linear interpolation
3g 328.3 234.8 162.6 30.3% 28.5% 25.8%
4g 325.3 230.8 157.6 30.3% 28.4% 25.2%
MDI model
3g 296.3 218.7 157.0 30.0% 28.0% 24.9%
4g 293.7 215.8 152.5 29.6% 27.9% 24.9%
</table>
<page confidence="0.998461">
474
</page>
<bodyText confidence="0.999623875">
53% is achieved as well as a decrease in word-error
rate of up to 1.0% absolute.
All other perplexity and absolute word-error rate
gains we found in the literature are considerably
smaller than those listed here. While different data
sets are used in previous work so results are not di-
rectly comparable, our results appear very competi-
tive with the body of existing results in the literature.
</bodyText>
<subsectionHeader confidence="0.995163">
5.2 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999982625">
Here, we discuss methods for supervised domain
adaptation that involve only the simple static combi-
nation of in-domain and out-of-domain data or mod-
els. For a survey of techniques using word classes,
topic, syntax, etc., refer to (Bellegarda, 2004).
Linear interpolation is the most widely-used
method for domain adaptation. Jelinek et al. (1991)
describe its use for combining a cache language
model and static language model. Another popular
method is count merging; this has been motivated
as an instance of MAP adaptation (Federico, 1996;
Masataki et al., 1997). In terms of word-error rate,
Iyer et al. (1997) found linear interpolation to give
better speech recognition performance while Bac-
chiani et al. (2006) found count merging to be su-
perior. Klakow (1998) proposes log-linear interpo-
lation for domain adaptation. As compared to reg-
ular linear interpolation for bigram models, an im-
provement of 4% in perplexity and 0.2% absolute in
word-error rate is found.
Della Pietra et al. (1992) introduce the idea of
minimum discrimination information distributions.
Given a prior model q(y|x), the goal is to find
the nearest model in Kullback-Liebler divergence
that satisfies a set of linear constraints derived from
adaptation data. The model satisfying these condi-
tions is an exponential model containing one fea-
ture per constraint with q(y|x) as its prior as in
eq. (12). While MDI models have been used many
times for language model adaptation, e.g., (Kneser et
al., 1997; Federico, 1999), they have not performed
as well as linear interpolation in perplexity or word-
error rate (Rao et al., 1995; Rao et al., 1997).
One important issue with MDI models is how to
select the feature set specifying the model. With a
small amount of adaptation data, one should intu-
itively use a small feature set, e.g., containing just
unigram features. However, the use of regulariza-
tion can obviate the need for intelligent feature se-
lection. In this work, we include all n-gram fea-
tures present in the adaptation data for n E {3, 4}.
Chueh and Chien (2008) propose the use of inequal-
ity constraints for regularization (Kazama and Tsu-
jii, 2003); here, we use�,+e22regularization instead.
We hypothesize that the use of state-of-the-art regu-
larization is the primary reason why we achieve bet-
ter performance relative to interpolation and count
merging as compared to earlier work.
</bodyText>
<sectionHeader confidence="0.999741" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999988034482758">
For exponential language models, eq. (2) tells us
that with respect to test set performance, the num-
ber of model parameters seems to matter not at all;
all that matters are the magnitudes of the parame-
ter values. Consequently, one can improve exponen-
tial language models by adding features (or a prior
model) that shrink parameter values while maintain-
ing training performance, and from this observa-
tion we develop Heuristics 1 and 2. We use these
ideas to motivate a novel and simple class-based
language model that achieves perplexity and word-
error rate improvements competitive with the best
reported results for class-based models in the litera-
ture. In addition, we show that with regularization,
MDI models can outperform both linear interpola-
tion and count merging in language model combina-
tion. Still, Heuristics 1 and 2 are quite vague, and
it remains to be seen how to determine when these
heuristics will be effective.
In summary, we have demonstrated how the trade-
off between training set performance and model size
impacts aspects of language modeling as diverse as
backoff n-gram features, class-based models, and
domain adaptation. In particular, we can frame
performance improvements in all of these areas as
methods that shrink models without degrading train-
ing set performance. All in all, eq. (2) is an impor-
tant tool for both understanding and improving lan-
guage model performance.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997701">
We thank Bhuvana Ramabhadran and the anony-
mous reviewers for their comments on this and ear-
lier versions of the paper.
</bodyText>
<page confidence="0.998882">
475
</page>
<sectionHeader confidence="0.995883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999911096153846">
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41–68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42(1):93–108.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479, December.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard University.
Stanley F. Chen. 2008. Performance prediction for expo-
nential language models. Technical Report RC 24671,
IBM Research Division, October.
Stanley F. Chen. 2009. Performance prediction for expo-
nential language models. In Proc. ofHLT-NAACL.
Chuang-Hua Chueh and Jen-Tzung Chien. 2008. Reli-
able feature selection for language model adaptation.
In Proc. ofICASSP, pp. 5089–5092.
Stephen Della Pietra, Vincent Della Pietra, Robert L.
Mercer, and Salim Roukos. 1992. Adaptive language
modeling using minimum discriminant estimation. In
Proc. of the Speech and Natural Language DARPA
Workshop, February.
Marcello Federico. 1996. Bayesian estimation methods
for n-gram language model adaptation. In Proc. ofIC-
SLP, pp. 240–243.
Marcello Federico. 1999. Efficient language model
adaptation through MDI estimation. In Proc. of Eu-
rospeech, pp. 1583–1586.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical Report MSR-TR-2001-72, Mi-
crosoft Research.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish. 1997.
Using out-of-domain data to improve in-domain lan-
guage models. IEEE Signal Processing Letters,
4(8):221–223, August.
Frederick Jelinek, Bernard Merialdo, Salim Roukos, and
Martin Strauss. 1991. A dynamic language model for
speech recognition. In Proc. of the DARPA Workshop
on Speech and Natural Language, pp. 293–295, Mor-
ristown, NJ, USA.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400–401, March.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. ofEMNLP, pp. 137–144.
Dietrich Klakow. 1998. Log-linear interpolation of lan-
guage models. In Proc. ofICSLP.
Reinhard Kneser, Jochen Peters, and Dietrich Klakow.
1997. Language model adaptation using dynamic
marginals. In Proc. of Eurospeech.
Hirokazu Masataki, Yoshinori Sagisaka, Kazuya Hisaki,
and Tatsuya Kawahara. 1997. Task adaptation us-
ing MAP estimation in n-gram language modeling. In
Proc. ofICASSP, volume 2, pp. 783–786, Washington,
DC, USA. IEEE Computer Society.
Douglas B. Paul and Janet M. Baker. 1992. The de-
sign for the Wall Street Journal-based CSR corpus.
In Proc. of the DARPA Speech and Natural Language
Workshop, pp. 357–362, February.
P. Srinivasa Rao, Michael D. Monkowski, and Salim
Roukos. 1995. Language model adaptation via mini-
mum discrimination information. In Proc. ofICASSP,
volume 1, pp. 161–164.
P. Srinivasa Rao, Satya Dharanipragada, and Salim
Roukos. 1997. MDI adaptation of language models
across corpora. In Proc. of Eurospeech, pp. 1979–
1982.
George Saon, Daniel Povey, and Geoffrey Zweig. 2005.
Anatomy of an extremely fast LVCSR decoder. In
Proc. ofInterspeech, pp. 549–552.
Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, and Geoffrey Zweig. 2005. The
IBM 2004 conversational telephony system for rich
transcription. In Proc. ofICASSP, pp. 205–208.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. of the DARPA Broad-
cast News Transcription and Understanding Work-
shop, pp. 270–274, Lansdowne, VA, February.
Wen Wang and Mary P. Harper. 2002. The Super-
ARV language model: Investigating the effectiveness
of tightly integrating multiple knowledge sources. In
Proc. of EMNLP, pp. 238–247.
Wen Wang, Yang Liu, and Mary P. Harper. 2002.
Rescoring effectiveness of language models using dif-
ferent levels of knowledge and their integration. In
Proc. ofICASSP, pp. 785–788.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pp. 261–264.
Hirofumi Yamamoto and Yoshinori Sagisaka. 1999.
Multi-class composite n-gram based on connection di-
rection. In Proc. ofICASSP, pp. 533–536.
Hirofumi Yamamoto, Shuntaro Isogai, and Yoshinori
Sagisaka. 2003. Multi-class composite n-gram lan-
guage model. Speech Communication, 41(2-3):369–
379.
</reference>
<page confidence="0.999105">
476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929471">
<title confidence="0.999862">Shrinking Exponential Language Models</title>
<author confidence="0.996251">F Stanley</author>
<affiliation confidence="0.970876">IBM T.J. Watson Research</affiliation>
<address confidence="0.956734">P.O. Box 218, Yorktown Heights, NY</address>
<email confidence="0.999658">stanchen@watson.ibm.com</email>
<abstract confidence="0.999528263157895">In (Chen, 2009), we show that for a variety of language models belonging to the exponential family, the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values. In this work, we show how this relationship can be used to motivate two heuristics for “shrinking” the size of a language model to improve its performance. We use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by 28% in perplexity and 1.9% absolute in speech recognition word-error rate on Wall Street Journal data. We use the second heuristic to motivate a regularized version of minimum discrimination information models and show that this method outperforms other techniques for domain adaptation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="28245" citStr="Bacchiani et al. (2006)" startWordPosition="4807" endWordPosition="4811">combination of in-domain and out-of-domain data or models. For a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one f</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="27776" citStr="Bellegarda, 2004" startWordPosition="4737" endWordPosition="4738">rror rate of up to 1.0% absolute. All other perplexity and absolute word-error rate gains we found in the literature are considerably smaller than those listed here. While different data sets are used in previous work so results are not directly comparable, our results appear very competitive with the body of existing results in the literature. 5.2 Domain Adaptation Here, we discuss methods for supervised domain adaptation that involve only the simple static combination of in-domain and out-of-domain data or models. For a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular </context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42(1):93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="10614" citStr="Brown et al. (1992)" startWordPosition="1886" endWordPosition="1889">p(cj|c1 ··· cj−1, w1 ··· wj−1)× Hlj=1 p(wj|c1 ··· cj, w1 ··· wj−1) (6) where cj = c(wj) and cl+1 is the end-of-sentence token. We use the notation png(y|ω) to denote an exponential n-gram model, a model containing a feature for each suffix of each ωy occurring in the training set. We use png(y|ω1, ω2) to denote a model containing all features in png(y|ω1) and png(y|ω2). We can define a class-based n-gram model by choosing parameterizations for the distributions p(cj |· · · ) and p(wj |· · · ) in eq. (6) above. For example, the most widely-used class-based n-gram model is the one introduced by Brown et al. (1992); we refer to this model as the IBM class model: p(cj|c1 ··· cj−1, w1 ··· wj−1)= png(cj|cj−2cj−1) p(wj|c1 ··· cj, w1 ··· wj−1)= png(wj|cj) (7) (In the original work, non-exponential n-gram models are used.) Clearly, there is a large space of possible class-based models. Now, we discuss how we can use Heuristic 1 to design a novel class-based model by using class information to “shrink” a word-based n-gram model. The basic idea is as follows: if we have an n-gram ω 470 and another n-gram ω0 created by replacing a word in ω with a similar word, then the two corresponding features should have sim</context>
<context position="14224" citStr="Brown et al. (1992)" startWordPosition="2479" endWordPosition="2482">eq. (2) does indeed hold for models of this type. 3.1 Class-Based Model Comparison In this section, we compare model M against other class-based models in perplexity and word-error rate. The training data is 1993 WSJ text with verbalized punctuation from the CSR-III Text corpus, and the vocabulary is the union of the training vocabulary and 20k-word “closed” test vocabulary from the first WSJ CSR corpus (Paul and Baker, 1992). We evaluate training set sizes of 1k, 10k, 100k, and 900k sentences. We create three different word classings containing 50, 150, and 500 classes using the algorithm of Brown et al. (1992) on the largest training set.5 For each training set and number of classes, we build 3-gram and 4-gram versions of each model. From the verbalized punctuation data from the training and test portions of the WSJ CSR corpus, we randomly select 2439 unique utterances (46888 words) as our evaluation set. From the remaining verbalized punctuation data, we select 977 utterances (18279 words) as our development set. We compare the following model types: conventional (i.e., non-exponential) word n-gram models; conventional IBM class n-gram models interpolated with conventional word n-gram models (Brow</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="14957" citStr="Chen and Goodman, 1998" startWordPosition="2594" endWordPosition="2597">s of each model. From the verbalized punctuation data from the training and test portions of the WSJ CSR corpus, we randomly select 2439 unique utterances (46888 words) as our evaluation set. From the remaining verbalized punctuation data, we select 977 utterances (18279 words) as our development set. We compare the following model types: conventional (i.e., non-exponential) word n-gram models; conventional IBM class n-gram models interpolated with conventional word n-gram models (Brown et al., 1992); and model M. All conventional n-gram models are smoothed with modified Kneser-Ney smoothing (Chen and Goodman, 1998), except we also evaluate word n-gram models with Katz smoothing (Katz, 1987). Note: Because word 5One can imagine choosing word classes to optimize model shrinkage; however, this is not an avenue we pursued. 471 training set (sents.) 1k 10k 100k 900k training set (sents.) 1k 10k 100k 900k conventional word n-gram, modified KN 3g 488.4 270.6 168.2 121.5 4g 486.8 267.4 163.6 114.4 model M 3g, 50c 341.5 210.0 144.5 110.9 3g, 150c 342.6 203.7 140.0 108.0 3g, 500c 387.5 212.7 142.2 108.1 4g, 50c 345.8 209.0 139.1 101.6 4g, 150c 344.1 202.8 135.7 99.1 4g, 500c 390.7 211.1 138.5 100.6 conventional w</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Performance prediction for exponential language models.</title>
<date>2008</date>
<tech>Technical Report RC 24671,</tech>
<institution>IBM Research Division,</institution>
<contexts>
<context position="5260" citStr="Chen, 2008" startWordPosition="919" endWordPosition="920"> 5 and 6 we discuss related work and conclusions.3 2 N-Gram Models and Backoff Features In this section, we use eq. (2) to explain why backoff features in exponential n-gram models improve performance, and use this analysis to motivate a general heuristic for model design. An exponential n-gram model contains a binary feature fω for each n&apos;-gram ω occurring in the training data for n&apos; ≤ n, where fω(x, y) = 1 iff xy ends in ω. We refer to features corresponding to n&apos;-grams for n&apos; &lt; n as backoff features; it is well known that backoff features help 3A long version of this paper can be found at (Chen, 2008). predicted letter Figure 2: Like Figure 1, but for model with unigram backoff features. performance a great deal. We present statistics in Table 1 for various letter trigram models built on the same data set. In these and all later experiments, all models are regularized with `1 + `2 2 regularization with (α = 0.5, σ2 = 6). The last row corresponds to a normal trigram model; the second row corresponds to a model lacking unigram features; and the first row corresponds to a model with no unigram or bigram features. As backoff features are added, we see that the training set cross-entropy improv</context>
<context position="20642" citStr="Chen, 2008" startWordPosition="3573" endWordPosition="3574">his is possible). If q is “similar” to p˜Λ, then we expect the size D Ei= 1 |λZ ew |of pqto be less than that of p˜Λ. Since they describe Λnew the same distribution, their training set cross-entropy will be the same. By eq. (2), we expect pqΛnew to have better test set performance than p˜Λ after reestimation.7 In (Chen, 2009), we show that eq. (2) does indeed hold for models with priors; q need not be accounted for in computing model size as long as it is estimated on a separate training set. 6Results for several other baseline language models and with a different acoustic model are given in (Chen, 2008). 7That is, we expect the regularized parameters ˜Λnew to yield improved performance. This analysis suggests the following method for improving model performance: Heuristic 2 Find a “similar” distribution estimated from an independent training set, and use this distribution as a prior. It is straightforward to apply this heuristic to the task of domain adaptation for language modeling. In the usual formulation of this task, we have a test set and a small training set from the same domain, and a large training set from a different domain. The goal is to use the data from the outside domain to m</context>
</contexts>
<marker>Chen, 2008</marker>
<rawString>Stanley F. Chen. 2008. Performance prediction for exponential language models. Technical Report RC 24671, IBM Research Division, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Performance prediction for exponential language models.</title>
<date>2009</date>
<booktitle>In Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="3226" citStr="Chen, 2009" startWordPosition="568" endWordPosition="569">r models in both perplexity and word-error rate on Wall Street Journal (WSJ) data. The organization of this paper is as follows: In Section 2, we analyze the use of backoff features in n-gram models to motivate a heuristic for model design. In Sections 3 and 4, we introduce our novel 1The cross-entropy of a model PΛ(y|x) on some data D = (x1, y1), ... , (xD, yD) is defined as − 1D PD log PΛ(yj |xj ). It is equivalent to the negative mean log-likelihood per event as well as to log perplexity. 2A nat is a “natural” bit and is equivalent to loge a regular bits. We use nats to be consistent with (Chen, 2009). Λ(x) and where ZΛ(x) is a normalization factor. In (Chen, 2009), we show that for many types of exponential language models, if a training and test set are drawn from the same distribution, we have F �F A2i (3) i=1 1 |Ai |+ 2σ2D i=1 468 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 468–476, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics features Heval Hpred Htrain |˜λi| � D 3g 2.681 2.724 2.341 0.408 2g+3g 2.528 2.513 2.248 0.282 1g+2g+3g 2.514 2.474 2.241 0.249 Table 1: Various statistics for letter tr</context>
<context position="13590" citStr="Chen, 2009" startWordPosition="2373" endWordPosition="2374">aining set cross-entropy as a word n-gram model while being significantly smaller. To give an idea of whether this model behaves as expected, in Table 2 we provide statistics for this model (as well as for an exponential word n-gram model) built on 100k WSJ training sentences with 50 classes using the same regularization as before. We see that model M is both smaller than the baseline and has a lower training set cross-entropy, similar to the behavior found when adding backoff features to word n-gram models in Section 2. As long as eq. (2) holds, model M should have good test performance; in (Chen, 2009), we show that eq. (2) does indeed hold for models of this type. 3.1 Class-Based Model Comparison In this section, we compare model M against other class-based models in perplexity and word-error rate. The training data is 1993 WSJ text with verbalized punctuation from the CSR-III Text corpus, and the vocabulary is the union of the training vocabulary and 20k-word “closed” test vocabulary from the first WSJ CSR corpus (Paul and Baker, 1992). We evaluate training set sizes of 1k, 10k, 100k, and 900k sentences. We create three different word classings containing 50, 150, and 500 classes using th</context>
<context position="20358" citStr="Chen, 2009" startWordPosition="3522" endWordPosition="3523">ining set and a “similar” model q estimated from an independent training set. Imagine we use q as a prior model for pΛ; i.e., we make a new model e exp(EF (12) 1 Anew fi(x, y)) (�x� =I x� pΛnewy q(y ZΛnew(x) Then, choose Λnew such that pq Λnew(y|x) = p˜Λ(y|x) for all x, y (assuming this is possible). If q is “similar” to p˜Λ, then we expect the size D Ei= 1 |λZ ew |of pqto be less than that of p˜Λ. Since they describe Λnew the same distribution, their training set cross-entropy will be the same. By eq. (2), we expect pqΛnew to have better test set performance than p˜Λ after reestimation.7 In (Chen, 2009), we show that eq. (2) does indeed hold for models with priors; q need not be accounted for in computing model size as long as it is estimated on a separate training set. 6Results for several other baseline language models and with a different acoustic model are given in (Chen, 2008). 7That is, we expect the regularized parameters ˜Λnew to yield improved performance. This analysis suggests the following method for improving model performance: Heuristic 2 Find a “similar” distribution estimated from an independent training set, and use this distribution as a prior. It is straightforward to appl</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley F. Chen. 2009. Performance prediction for exponential language models. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuang-Hua Chueh</author>
<author>Jen-Tzung Chien</author>
</authors>
<title>Reliable feature selection for language model adaptation.</title>
<date>2008</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<pages>5089--5092</pages>
<contexts>
<context position="29577" citStr="Chueh and Chien (2008)" startWordPosition="5031" endWordPosition="5034">language model adaptation, e.g., (Kneser et al., 1997; Federico, 1999), they have not performed as well as linear interpolation in perplexity or worderror rate (Rao et al., 1995; Rao et al., 1997). One important issue with MDI models is how to select the feature set specifying the model. With a small amount of adaptation data, one should intuitively use a small feature set, e.g., containing just unigram features. However, the use of regularization can obviate the need for intelligent feature selection. In this work, we include all n-gram features present in the adaptation data for n E {3, 4}. Chueh and Chien (2008) propose the use of inequality constraints for regularization (Kazama and Tsujii, 2003); here, we use�,+e22regularization instead. We hypothesize that the use of state-of-the-art regularization is the primary reason why we achieve better performance relative to interpolation and count merging as compared to earlier work. 6 Discussion For exponential language models, eq. (2) tells us that with respect to test set performance, the number of model parameters seems to matter not at all; all that matters are the magnitudes of the parameter values. Consequently, one can improve exponential language </context>
</contexts>
<marker>Chueh, Chien, 2008</marker>
<rawString>Chuang-Hua Chueh and Jen-Tzung Chien. 2008. Reliable feature selection for language model adaptation. In Proc. ofICASSP, pp. 5089–5092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Adaptive language modeling using minimum discriminant estimation.</title>
<date>1992</date>
<booktitle>In Proc. of the Speech and Natural Language DARPA Workshop,</booktitle>
<contexts>
<context position="19693" citStr="Pietra et al., 1992" startWordPosition="3391" endWordPosition="3394"> and without a Broadcast News prior model. The first column is the size of the in-domain training set in sentences. not a reliable predictor of speech recognition performance. While we can only compare class models with word models on the largest training set, for this training set model M outperforms the baseline Katzsmoothed word trigram model by 1.9% absolute.6 4 Domain Adaptation In this section, we introduce another heuristic for improving exponential models and show how this heuristic can be used to motivate a regularized version of minimum discrimination information (MDI) models (Della Pietra et al., 1992). Let’s say we have a model p˜Λ estimated from one training set and a “similar” model q estimated from an independent training set. Imagine we use q as a prior model for pΛ; i.e., we make a new model e exp(EF (12) 1 Anew fi(x, y)) (�x� =I x� pΛnewy q(y ZΛnew(x) Then, choose Λnew such that pq Λnew(y|x) = p˜Λ(y|x) for all x, y (assuming this is possible). If q is “similar” to p˜Λ, then we expect the size D Ei= 1 |λZ ew |of pqto be less than that of p˜Λ. Since they describe Λnew the same distribution, their training set cross-entropy will be the same. By eq. (2), we expect pqΛnew to have better t</context>
<context position="28524" citStr="Pietra et al. (1992)" startWordPosition="4854" endWordPosition="4857">g a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one feature per constraint with q(y|x) as its prior as in eq. (12). While MDI models have been used many times for language model adaptation, e.g., (Kneser et al., 1997; Federico, 1999), they have not performed as well as linear interpolation in perplexity or worderror rate (Rao et a</context>
</contexts>
<marker>Pietra, Pietra, Mercer, Roukos, 1992</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, Robert L. Mercer, and Salim Roukos. 1992. Adaptive language modeling using minimum discriminant estimation. In Proc. of the Speech and Natural Language DARPA Workshop, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
</authors>
<title>Bayesian estimation methods for n-gram language model adaptation.</title>
<date>1996</date>
<booktitle>In Proc. ofICSLP,</booktitle>
<pages>240--243</pages>
<contexts>
<context position="28069" citStr="Federico, 1996" startWordPosition="4782" endWordPosition="4783">th the body of existing results in the literature. 5.2 Domain Adaptation Here, we discuss methods for supervised domain adaptation that involve only the simple static combination of in-domain and out-of-domain data or models. For a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kul</context>
</contexts>
<marker>Federico, 1996</marker>
<rawString>Marcello Federico. 1996. Bayesian estimation methods for n-gram language model adaptation. In Proc. ofICSLP, pp. 240–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
</authors>
<title>Efficient language model adaptation through MDI estimation.</title>
<date>1999</date>
<booktitle>In Proc. of Eurospeech,</booktitle>
<pages>1583--1586</pages>
<contexts>
<context position="29025" citStr="Federico, 1999" startWordPosition="4935" endWordPosition="4936">dels, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one feature per constraint with q(y|x) as its prior as in eq. (12). While MDI models have been used many times for language model adaptation, e.g., (Kneser et al., 1997; Federico, 1999), they have not performed as well as linear interpolation in perplexity or worderror rate (Rao et al., 1995; Rao et al., 1997). One important issue with MDI models is how to select the feature set specifying the model. With a small amount of adaptation data, one should intuitively use a small feature set, e.g., containing just unigram features. However, the use of regularization can obviate the need for intelligent feature selection. In this work, we include all n-gram features present in the adaptation data for n E {3, 4}. Chueh and Chien (2008) propose the use of inequality constraints for r</context>
</contexts>
<marker>Federico, 1999</marker>
<rawString>Marcello Federico. 1999. Efficient language model adaptation through MDI estimation. In Proc. of Eurospeech, pp. 1583–1586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72, Microsoft Research.</tech>
<contexts>
<context position="24646" citStr="Goodman (2001)" startWordPosition="4226" endWordPosition="4227">ity on the smallest data set and 3% in perplexity on the largest. In terms of worderror rate, MDI models again perform best of all, outperforming interpolation by 0.3–0.7% absolute and count merging by 0.1–0.4% absolute. 5 Related Work 5.1 Class-Based Language Models In past work, the most common baseline models are Katz-smoothed word trigram models. Compared to this baseline, model M achieves a perplexity reduction of 28% and word-error rate reduction of 1.9% absolute with a 900k-sentence training set. The most closely-related existing model to model M is the model fullibmpredict proposed by Goodman (2001): p(cj|cj−2cj−1,wj−2wj−1)= λ p(cj|wj−2wj−1)+(1−λ) p(cj|cj−2cj−1) p(wj|cj−2cj−1cj,wj−2wj−1)= µ p(wj|wj−2wj−1cj)+(1−µ) p(wj|cj−2cj−1cj) (13) This is similar to model M except that linear interpolation is used to combine word and class history information, and there is no analog to the final term in eq. (13) in model M. Using the North American Business news corpus, the largest perplexity reduction achieved over a Katz-smoothed trigram model baseline by fullibmpredict is about 25%, with a training set of 1M words. In N-best list rescoring with a 284M-word training set, the best result achieved fo</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling. Technical Report MSR-TR-2001-72, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini Iyer</author>
<author>Mari Ostendorf</author>
<author>Herbert Gish</author>
</authors>
<title>Using out-of-domain data to improve in-domain language models.</title>
<date>1997</date>
<journal>IEEE Signal Processing Letters,</journal>
<volume>4</volume>
<issue>8</issue>
<contexts>
<context position="28142" citStr="Iyer et al. (1997)" startWordPosition="4793" endWordPosition="4796">ion Here, we discuss methods for supervised domain adaptation that involve only the simple static combination of in-domain and out-of-domain data or models. For a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints deriv</context>
</contexts>
<marker>Iyer, Ostendorf, Gish, 1997</marker>
<rawString>Rukmini Iyer, Mari Ostendorf, and Herbert Gish. 1997. Using out-of-domain data to improve in-domain language models. IEEE Signal Processing Letters, 4(8):221–223, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Bernard Merialdo</author>
<author>Salim Roukos</author>
<author>Martin Strauss</author>
</authors>
<title>A dynamic language model for speech recognition.</title>
<date>1991</date>
<booktitle>In Proc. of the DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>293--295</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27874" citStr="Jelinek et al. (1991)" startWordPosition="4749" endWordPosition="4752">ound in the literature are considerably smaller than those listed here. While different data sets are used in previous work so results are not directly comparable, our results appear very competitive with the body of existing results in the literature. 5.2 Domain Adaptation Here, we discuss methods for supervised domain adaptation that involve only the simple static combination of in-domain and out-of-domain data or models. For a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in wo</context>
</contexts>
<marker>Jelinek, Merialdo, Roukos, Strauss, 1991</marker>
<rawString>Frederick Jelinek, Bernard Merialdo, Salim Roukos, and Martin Strauss. 1991. A dynamic language model for speech recognition. In Proc. of the DARPA Workshop on Speech and Natural Language, pp. 293–295, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="15034" citStr="Katz, 1987" startWordPosition="2608" endWordPosition="2609"> of the WSJ CSR corpus, we randomly select 2439 unique utterances (46888 words) as our evaluation set. From the remaining verbalized punctuation data, we select 977 utterances (18279 words) as our development set. We compare the following model types: conventional (i.e., non-exponential) word n-gram models; conventional IBM class n-gram models interpolated with conventional word n-gram models (Brown et al., 1992); and model M. All conventional n-gram models are smoothed with modified Kneser-Ney smoothing (Chen and Goodman, 1998), except we also evaluate word n-gram models with Katz smoothing (Katz, 1987). Note: Because word 5One can imagine choosing word classes to optimize model shrinkage; however, this is not an avenue we pursued. 471 training set (sents.) 1k 10k 100k 900k training set (sents.) 1k 10k 100k 900k conventional word n-gram, modified KN 3g 488.4 270.6 168.2 121.5 4g 486.8 267.4 163.6 114.4 model M 3g, 50c 341.5 210.0 144.5 110.9 3g, 150c 342.6 203.7 140.0 108.0 3g, 500c 387.5 212.7 142.2 108.1 4g, 50c 345.8 209.0 139.1 101.6 4g, 150c 344.1 202.8 135.7 99.1 4g, 500c 390.7 211.1 138.5 100.6 conventional word n-gram, Katz 3g 579.3 317.1 196.7 137.5 4g 592.6 325.6 202.4 136.7 interp</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, 35(3):400–401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="1570" citStr="Kazama and Tsujii, 2003" startWordPosition="261" endWordPosition="265">ptation. 1 Introduction An exponential model pΛ(y|x) is a model with a set of features {f1(x, y), ... , fF (x, y)} and equal number of parameters A = {A1, ... , AF} where exp(�F i=1 Aifi(x, y)) pΛ(y|x) = (1) Z F Htest ≈ Htrain + -yD |˜Ai |(2) i=1 where Htest denotes test set cross-entropy; Htrain denotes training set cross-entropy; D is the number of events in the training data; the ˜Ai are regularized parameter estimates; and -y is a constant independent of domain, training set size, and model type.1 This relationship is strongest if the A˜ = {˜Ai} are estimated using j1+j2 2 regularization (Kazama and Tsujii, 2003). In j1 + j2 2 regularization, parameters are chosen to optimize O`, +`. (A) = Ht a rain + D for some α and σ. With (α = 0.5, σ2 = 6) and taking -y = 0.938, test set cross-entropy can be predicted with eq. (2) for a wide range of models with a mean error of a few hundredths of a nat, equivalent to a few percent in perplexity.2 In this paper, we show how eq. (2) can be applied to improve language model performance. First, we use eq. (2) to analyze backoff features in exponential n-gram models. We find that backoff features improve test set performance by reducing the “size” of a model 1D �D 1 |</context>
<context position="29664" citStr="Kazama and Tsujii, 2003" startWordPosition="5044" endWordPosition="5048"> performed as well as linear interpolation in perplexity or worderror rate (Rao et al., 1995; Rao et al., 1997). One important issue with MDI models is how to select the feature set specifying the model. With a small amount of adaptation data, one should intuitively use a small feature set, e.g., containing just unigram features. However, the use of regularization can obviate the need for intelligent feature selection. In this work, we include all n-gram features present in the adaptation data for n E {3, 4}. Chueh and Chien (2008) propose the use of inequality constraints for regularization (Kazama and Tsujii, 2003); here, we use�,+e22regularization instead. We hypothesize that the use of state-of-the-art regularization is the primary reason why we achieve better performance relative to interpolation and count merging as compared to earlier work. 6 Discussion For exponential language models, eq. (2) tells us that with respect to test set performance, the number of model parameters seems to matter not at all; all that matters are the magnitudes of the parameter values. Consequently, one can improve exponential language models by adding features (or a prior model) that shrink parameter values while maintai</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proc. ofEMNLP, pp. 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Klakow</author>
</authors>
<title>Log-linear interpolation of language models.</title>
<date>1998</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="28295" citStr="Klakow (1998)" startWordPosition="4819" endWordPosition="4820">or a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one feature per constraint with q(y|x) as its prior as </context>
</contexts>
<marker>Klakow, 1998</marker>
<rawString>Dietrich Klakow. 1998. Log-linear interpolation of language models. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Jochen Peters</author>
<author>Dietrich Klakow</author>
</authors>
<title>Language model adaptation using dynamic marginals.</title>
<date>1997</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="29008" citStr="Kneser et al., 1997" startWordPosition="4931" endWordPosition="4934">olation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one feature per constraint with q(y|x) as its prior as in eq. (12). While MDI models have been used many times for language model adaptation, e.g., (Kneser et al., 1997; Federico, 1999), they have not performed as well as linear interpolation in perplexity or worderror rate (Rao et al., 1995; Rao et al., 1997). One important issue with MDI models is how to select the feature set specifying the model. With a small amount of adaptation data, one should intuitively use a small feature set, e.g., containing just unigram features. However, the use of regularization can obviate the need for intelligent feature selection. In this work, we include all n-gram features present in the adaptation data for n E {3, 4}. Chueh and Chien (2008) propose the use of inequality </context>
</contexts>
<marker>Kneser, Peters, Klakow, 1997</marker>
<rawString>Reinhard Kneser, Jochen Peters, and Dietrich Klakow. 1997. Language model adaptation using dynamic marginals. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirokazu Masataki</author>
<author>Yoshinori Sagisaka</author>
<author>Kazuya Hisaki</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Task adaptation using MAP estimation in n-gram language modeling.</title>
<date>1997</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<volume>2</volume>
<pages>783--786</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="28093" citStr="Masataki et al., 1997" startWordPosition="4784" endWordPosition="4787">xisting results in the literature. 5.2 Domain Adaptation Here, we discuss methods for supervised domain adaptation that involve only the simple static combination of in-domain and out-of-domain data or models. For a survey of techniques using word classes, topic, syntax, etc., refer to (Bellegarda, 2004). Linear interpolation is the most widely-used method for domain adaptation. Jelinek et al. (1991) describe its use for combining a cache language model and static language model. Another popular method is count merging; this has been motivated as an instance of MAP adaptation (Federico, 1996; Masataki et al., 1997). In terms of word-error rate, Iyer et al. (1997) found linear interpolation to give better speech recognition performance while Bacchiani et al. (2006) found count merging to be superior. Klakow (1998) proposes log-linear interpolation for domain adaptation. As compared to regular linear interpolation for bigram models, an improvement of 4% in perplexity and 0.2% absolute in word-error rate is found. Della Pietra et al. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence</context>
</contexts>
<marker>Masataki, Sagisaka, Hisaki, Kawahara, 1997</marker>
<rawString>Hirokazu Masataki, Yoshinori Sagisaka, Kazuya Hisaki, and Tatsuya Kawahara. 1997. Task adaptation using MAP estimation in n-gram language modeling. In Proc. ofICASSP, volume 2, pp. 783–786, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Paul</author>
<author>Janet M Baker</author>
</authors>
<title>The design for the Wall Street Journal-based CSR corpus.</title>
<date>1992</date>
<booktitle>In Proc. of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>357--362</pages>
<contexts>
<context position="14034" citStr="Paul and Baker, 1992" startWordPosition="2446" endWordPosition="2449">milar to the behavior found when adding backoff features to word n-gram models in Section 2. As long as eq. (2) holds, model M should have good test performance; in (Chen, 2009), we show that eq. (2) does indeed hold for models of this type. 3.1 Class-Based Model Comparison In this section, we compare model M against other class-based models in perplexity and word-error rate. The training data is 1993 WSJ text with verbalized punctuation from the CSR-III Text corpus, and the vocabulary is the union of the training vocabulary and 20k-word “closed” test vocabulary from the first WSJ CSR corpus (Paul and Baker, 1992). We evaluate training set sizes of 1k, 10k, 100k, and 900k sentences. We create three different word classings containing 50, 150, and 500 classes using the algorithm of Brown et al. (1992) on the largest training set.5 For each training set and number of classes, we build 3-gram and 4-gram versions of each model. From the verbalized punctuation data from the training and test portions of the WSJ CSR corpus, we randomly select 2439 unique utterances (46888 words) as our evaluation set. From the remaining verbalized punctuation data, we select 977 utterances (18279 words) as our development se</context>
</contexts>
<marker>Paul, Baker, 1992</marker>
<rawString>Douglas B. Paul and Janet M. Baker. 1992. The design for the Wall Street Journal-based CSR corpus. In Proc. of the DARPA Speech and Natural Language Workshop, pp. 357–362, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Srinivasa Rao</author>
<author>Michael D Monkowski</author>
<author>Salim Roukos</author>
</authors>
<title>Language model adaptation via minimum discrimination information.</title>
<date>1995</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<volume>1</volume>
<pages>161--164</pages>
<contexts>
<context position="29132" citStr="Rao et al., 1995" startWordPosition="4952" endWordPosition="4955">. (1992) introduce the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one feature per constraint with q(y|x) as its prior as in eq. (12). While MDI models have been used many times for language model adaptation, e.g., (Kneser et al., 1997; Federico, 1999), they have not performed as well as linear interpolation in perplexity or worderror rate (Rao et al., 1995; Rao et al., 1997). One important issue with MDI models is how to select the feature set specifying the model. With a small amount of adaptation data, one should intuitively use a small feature set, e.g., containing just unigram features. However, the use of regularization can obviate the need for intelligent feature selection. In this work, we include all n-gram features present in the adaptation data for n E {3, 4}. Chueh and Chien (2008) propose the use of inequality constraints for regularization (Kazama and Tsujii, 2003); here, we use�,+e22regularization instead. We hypothesize that the </context>
</contexts>
<marker>Rao, Monkowski, Roukos, 1995</marker>
<rawString>P. Srinivasa Rao, Michael D. Monkowski, and Salim Roukos. 1995. Language model adaptation via minimum discrimination information. In Proc. ofICASSP, volume 1, pp. 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Srinivasa Rao</author>
<author>Satya Dharanipragada</author>
<author>Salim Roukos</author>
</authors>
<title>MDI adaptation of language models across corpora.</title>
<date>1997</date>
<booktitle>In Proc. of Eurospeech,</booktitle>
<pages>1979--1982</pages>
<contexts>
<context position="29151" citStr="Rao et al., 1997" startWordPosition="4956" endWordPosition="4959"> the idea of minimum discrimination information distributions. Given a prior model q(y|x), the goal is to find the nearest model in Kullback-Liebler divergence that satisfies a set of linear constraints derived from adaptation data. The model satisfying these conditions is an exponential model containing one feature per constraint with q(y|x) as its prior as in eq. (12). While MDI models have been used many times for language model adaptation, e.g., (Kneser et al., 1997; Federico, 1999), they have not performed as well as linear interpolation in perplexity or worderror rate (Rao et al., 1995; Rao et al., 1997). One important issue with MDI models is how to select the feature set specifying the model. With a small amount of adaptation data, one should intuitively use a small feature set, e.g., containing just unigram features. However, the use of regularization can obviate the need for intelligent feature selection. In this work, we include all n-gram features present in the adaptation data for n E {3, 4}. Chueh and Chien (2008) propose the use of inequality constraints for regularization (Kazama and Tsujii, 2003); here, we use�,+e22regularization instead. We hypothesize that the use of state-of-the</context>
</contexts>
<marker>Rao, Dharanipragada, Roukos, 1997</marker>
<rawString>P. Srinivasa Rao, Satya Dharanipragada, and Salim Roukos. 1997. MDI adaptation of language models across corpora. In Proc. of Eurospeech, pp. 1979– 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Saon</author>
<author>Daniel Povey</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Anatomy of an extremely fast LVCSR decoder.</title>
<date>2005</date>
<booktitle>In Proc. ofInterspeech,</booktitle>
<pages>549--552</pages>
<contexts>
<context position="17979" citStr="Saon et al., 2005" startWordPosition="3104" endWordPosition="3107">ming the interpolated model with every training set and achieving its largest reduction in perplexity (4%) on the largest training set. While these perplexity reductions are quite modest, what matters more is speech recognition performance. For the speech recognition experiments, we use a cross-word quinphone system built from 50 hours of Broadcast News data. The system contains 2176 context-dependent states and a total of 50336 Gaussians. To evaluate our language models, we use lattice rescoring. We generate lattices on both our development and evaluation data sets using the LattAIX decoder (Saon et al., 2005) in the Attila speech recognition system (Soltau et al., 2005). The language model for lattice generation is created by building a modified Kneser-Ney-smoothed word trigram model on our largest training set; this model is pruned to contain a total of 350k n-grams using the algorithm of Stolcke (1998). We choose the acoustic weight for each model to optimize word-error rate on the development set. In Table 4, we display the word-error rates for each model. If we compare the best performance of model M for each training set with that of the state-of-the-art interpolated class model, we find that</context>
</contexts>
<marker>Saon, Povey, Zweig, 2005</marker>
<rawString>George Saon, Daniel Povey, and Geoffrey Zweig. 2005. Anatomy of an extremely fast LVCSR decoder. In Proc. ofInterspeech, pp. 549–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Soltau</author>
<author>Brian Kingsbury</author>
<author>Lidia Mangu</author>
<author>Daniel Povey</author>
<author>George Saon</author>
<author>Geoffrey Zweig</author>
</authors>
<title>conversational telephony system for rich transcription.</title>
<date>2005</date>
<booktitle>The IBM</booktitle>
<pages>205--208</pages>
<contexts>
<context position="18041" citStr="Soltau et al., 2005" startWordPosition="3114" endWordPosition="3117">eving its largest reduction in perplexity (4%) on the largest training set. While these perplexity reductions are quite modest, what matters more is speech recognition performance. For the speech recognition experiments, we use a cross-word quinphone system built from 50 hours of Broadcast News data. The system contains 2176 context-dependent states and a total of 50336 Gaussians. To evaluate our language models, we use lattice rescoring. We generate lattices on both our development and evaluation data sets using the LattAIX decoder (Saon et al., 2005) in the Attila speech recognition system (Soltau et al., 2005). The language model for lattice generation is created by building a modified Kneser-Ney-smoothed word trigram model on our largest training set; this model is pruned to contain a total of 350k n-grams using the algorithm of Stolcke (1998). We choose the acoustic weight for each model to optimize word-error rate on the development set. In Table 4, we display the word-error rates for each model. If we compare the best performance of model M for each training set with that of the state-of-the-art interpolated class model, we find that model M is superior by 0.8–1.0% absolute. These gains are muc</context>
</contexts>
<marker>Soltau, Kingsbury, Mangu, Povey, Saon, Zweig, 2005</marker>
<rawString>Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel Povey, George Saon, and Geoffrey Zweig. 2005. The IBM 2004 conversational telephony system for rich transcription. In Proc. ofICASSP, pp. 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<location>Lansdowne, VA,</location>
<contexts>
<context position="18280" citStr="Stolcke (1998)" startWordPosition="3156" endWordPosition="3157">phone system built from 50 hours of Broadcast News data. The system contains 2176 context-dependent states and a total of 50336 Gaussians. To evaluate our language models, we use lattice rescoring. We generate lattices on both our development and evaluation data sets using the LattAIX decoder (Saon et al., 2005) in the Attila speech recognition system (Soltau et al., 2005). The language model for lattice generation is created by building a modified Kneser-Ney-smoothed word trigram model on our largest training set; this model is pruned to contain a total of 350k n-grams using the algorithm of Stolcke (1998). We choose the acoustic weight for each model to optimize word-error rate on the development set. In Table 4, we display the word-error rates for each model. If we compare the best performance of model M for each training set with that of the state-of-the-art interpolated class model, we find that model M is superior by 0.8–1.0% absolute. These gains are much larger than are suggested by the perplexity gains of model M over the interpolated model; as has been observed earlier, perplexity is 472 Heval E |˜a.| Hpred Htrain D baseline n-gram model 1k 5.915 5.875 2.808 3.269 10k 5.212 5.231 3.106</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop, pp. 270–274, Lansdowne, VA, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>The SuperARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="26261" citStr="Wang and Harper, 2002" startWordPosition="4478" endWordPosition="4481">gram models, a variant of the IBM class model (Yamamoto and Sagisaka, 1999; Yamamoto et al., 2003). These are called composite models because frequent word sequences can be concatenated into single units within the model; the term multi-class refers to choosing different word clusterings depending on word position. In experiments on the ATR spoken language database, Yamamoto et al. (2003) report a reduction in perplexity of 9% and an increase in word accuracy of 2.2% absolute over a Katz-smoothed trigram model. In terms of perplexity, the best gains we found are from SuperARV language models (Wang and Harper, 2002; Wang et al., 2002; Wang et al., 2004). In these models, classes are based on abstract role values as given by a Constraint Dependency Grammar. The class and word prediction distributions are n-gram models that back off to a variety of mixed word/class histories in a specific order. With a WSJ training set of 37M words and a Katz-smoothed trigram model baseline, a perplexity reduction of up to in-domain data (sents.) in-domain data (sents.) 1k 10k 100k 1k 10k 100k in-domain data only 3g 488.4 270.6 168.2 34.5% 30.5% 26.1% 4g 486.8 267.4 163.6 34.5% 30.4% 25.7% count merging 3g 503.1 290.9 170</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wen Wang and Mary P. Harper. 2002. The SuperARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources. In Proc. of EMNLP, pp. 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Yang Liu</author>
<author>Mary P Harper</author>
</authors>
<title>Rescoring effectiveness of language models using different levels of knowledge and their integration.</title>
<date>2002</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<pages>785--788</pages>
<contexts>
<context position="26280" citStr="Wang et al., 2002" startWordPosition="4482" endWordPosition="4485">of the IBM class model (Yamamoto and Sagisaka, 1999; Yamamoto et al., 2003). These are called composite models because frequent word sequences can be concatenated into single units within the model; the term multi-class refers to choosing different word clusterings depending on word position. In experiments on the ATR spoken language database, Yamamoto et al. (2003) report a reduction in perplexity of 9% and an increase in word accuracy of 2.2% absolute over a Katz-smoothed trigram model. In terms of perplexity, the best gains we found are from SuperARV language models (Wang and Harper, 2002; Wang et al., 2002; Wang et al., 2004). In these models, classes are based on abstract role values as given by a Constraint Dependency Grammar. The class and word prediction distributions are n-gram models that back off to a variety of mixed word/class histories in a specific order. With a WSJ training set of 37M words and a Katz-smoothed trigram model baseline, a perplexity reduction of up to in-domain data (sents.) in-domain data (sents.) 1k 10k 100k 1k 10k 100k in-domain data only 3g 488.4 270.6 168.2 34.5% 30.5% 26.1% 4g 486.8 267.4 163.6 34.5% 30.4% 25.7% count merging 3g 503.1 290.9 170.7 30.4% 28.3% 25.2</context>
</contexts>
<marker>Wang, Liu, Harper, 2002</marker>
<rawString>Wen Wang, Yang Liu, and Mary P. Harper. 2002. Rescoring effectiveness of language models using different levels of knowledge and their integration. In Proc. ofICASSP, pp. 785–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Andreas Stolcke</author>
<author>Mary P Harper</author>
</authors>
<title>The use of a linguistically motivated language model in conversational speech recognition.</title>
<date>2004</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>261--264</pages>
<contexts>
<context position="26300" citStr="Wang et al., 2004" startWordPosition="4486" endWordPosition="4489">del (Yamamoto and Sagisaka, 1999; Yamamoto et al., 2003). These are called composite models because frequent word sequences can be concatenated into single units within the model; the term multi-class refers to choosing different word clusterings depending on word position. In experiments on the ATR spoken language database, Yamamoto et al. (2003) report a reduction in perplexity of 9% and an increase in word accuracy of 2.2% absolute over a Katz-smoothed trigram model. In terms of perplexity, the best gains we found are from SuperARV language models (Wang and Harper, 2002; Wang et al., 2002; Wang et al., 2004). In these models, classes are based on abstract role values as given by a Constraint Dependency Grammar. The class and word prediction distributions are n-gram models that back off to a variety of mixed word/class histories in a specific order. With a WSJ training set of 37M words and a Katz-smoothed trigram model baseline, a perplexity reduction of up to in-domain data (sents.) in-domain data (sents.) 1k 10k 100k 1k 10k 100k in-domain data only 3g 488.4 270.6 168.2 34.5% 30.5% 26.1% 4g 486.8 267.4 163.6 34.5% 30.4% 25.7% count merging 3g 503.1 290.9 170.7 30.4% 28.3% 25.2% 4g 497.1 284.9 165</context>
</contexts>
<marker>Wang, Stolcke, Harper, 2004</marker>
<rawString>Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The use of a linguistically motivated language model in conversational speech recognition. In Proc. of ICASSP, pp. 261–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>Multi-class composite n-gram based on connection direction.</title>
<date>1999</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<pages>533--536</pages>
<contexts>
<context position="25714" citStr="Yamamoto and Sagisaka, 1999" startWordPosition="4390" endWordPosition="4393">l baseline by fullibmpredict is about 25%, with a training set of 1M words. In N-best list rescoring with a 284M-word training set, the best result achieved for an individual class-based model is an 0.5% absolute reduction in word-error rate. To situate the quality of our results, we also review the best perplexity and word-error rate results reported for class-based language models relative to conventional word n-gram model baselines. In terms of absolute word-error rate, the best gains we found in the literature are from multi-class composite n-gram models, a variant of the IBM class model (Yamamoto and Sagisaka, 1999; Yamamoto et al., 2003). These are called composite models because frequent word sequences can be concatenated into single units within the model; the term multi-class refers to choosing different word clusterings depending on word position. In experiments on the ATR spoken language database, Yamamoto et al. (2003) report a reduction in perplexity of 9% and an increase in word accuracy of 2.2% absolute over a Katz-smoothed trigram model. In terms of perplexity, the best gains we found are from SuperARV language models (Wang and Harper, 2002; Wang et al., 2002; Wang et al., 2004). In these mod</context>
</contexts>
<marker>Yamamoto, Sagisaka, 1999</marker>
<rawString>Hirofumi Yamamoto and Yoshinori Sagisaka. 1999. Multi-class composite n-gram based on connection direction. In Proc. ofICASSP, pp. 533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Shuntaro Isogai</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>Multi-class composite n-gram language model.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>41--2</pages>
<contexts>
<context position="25738" citStr="Yamamoto et al., 2003" startWordPosition="4394" endWordPosition="4397">is about 25%, with a training set of 1M words. In N-best list rescoring with a 284M-word training set, the best result achieved for an individual class-based model is an 0.5% absolute reduction in word-error rate. To situate the quality of our results, we also review the best perplexity and word-error rate results reported for class-based language models relative to conventional word n-gram model baselines. In terms of absolute word-error rate, the best gains we found in the literature are from multi-class composite n-gram models, a variant of the IBM class model (Yamamoto and Sagisaka, 1999; Yamamoto et al., 2003). These are called composite models because frequent word sequences can be concatenated into single units within the model; the term multi-class refers to choosing different word clusterings depending on word position. In experiments on the ATR spoken language database, Yamamoto et al. (2003) report a reduction in perplexity of 9% and an increase in word accuracy of 2.2% absolute over a Katz-smoothed trigram model. In terms of perplexity, the best gains we found are from SuperARV language models (Wang and Harper, 2002; Wang et al., 2002; Wang et al., 2004). In these models, classes are based o</context>
</contexts>
<marker>Yamamoto, Isogai, Sagisaka, 2003</marker>
<rawString>Hirofumi Yamamoto, Shuntaro Isogai, and Yoshinori Sagisaka. 2003. Multi-class composite n-gram language model. Speech Communication, 41(2-3):369– 379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>