<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.996606">
A Word Reordering Model for Improved Machine Translation
</title>
<author confidence="0.726372">
Karthik Visweswariah Rajakrishnan Rajkumar Ankur Gandhe
</author>
<affiliation confidence="0.634748">
IBM Research India Dept. of Linguistics IBM Research India
Bangalore, India Ohio State University Bangalore, India
</affiliation>
<email confidence="0.94545">
v-karthik@in.ibm.com raja@ling.osu.edu ankugand@in.ibm.com
</email>
<author confidence="0.923077">
Ananthakrishnan Ramanathan
</author>
<affiliation confidence="0.8004545">
IBM Research India
Bangalore, India
</affiliation>
<email confidence="0.99035">
aramana2@in.ibm.com
</email>
<sectionHeader confidence="0.998535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999687642857143">
Preordering of source side sentences has
proved to be useful in improving statistical
machine translation. Most work has used a
parser in the source language along with rules
to map the source language word order into
the target language word order. The require-
ment to have a source language parser is a ma-
jor drawback, which we seek to overcome in
this paper. Instead of using a parser and then
using rules to order the source side sentence
we learn a model that can directly reorder
source side sentences to match target word or-
der using a small parallel corpus with high-
quality word alignments. Our model learns
pairwise costs of a word immediately preced-
ing another word. We use the Lin-Kernighan
heuristic to find the best source reordering ef-
ficiently during training and testing and show
that it suffices to provide good quality reorder-
ing.
We show gains in translation performance
based on our reordering model for translating
from Hindi to English, Urdu to English (with
a public dataset), and English to Hindi. For
English to Hindi we show that our technique
achieves better performance than a method
that uses rules applied to the source side En-
glish parse.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99686625">
Languages differ in the way they order words to pro-
duce sentences representing the same meaning. Ma-
chine translation systems need to reorder words in
the source sentence to produce fluent output in the
</bodyText>
<author confidence="0.334389">
Jiri Navratil
</author>
<affiliation confidence="0.352201">
IBM T.J. Watson Research Center
Yorktown Heights, New York
</affiliation>
<email confidence="0.905111">
jiri@us.ibm.com
</email>
<bodyText confidence="0.999939029411764">
target language that preserves the meaning of the
source sentence.
Current phrase based machine translation systems
can capture short range reorderings via the phrase
table. Even the capturing of these local reordering
phenomena is constrained by the amount of training
data available. For example, if adjectives precede
nouns in the source language and follow nouns in the
target language we still need to see a particular ad-
jective noun pair in the parallel corpus to handle the
reordering via the phrase table. Phrase based sys-
tems also rely on the target side language model to
produce the right target side order. This is known
to be inadequate (Al-Onaizan and Papineni, 2006),
and this inadequacy has spurred various attempts to
overcome the problem of handling differing word
order in languages.
One approach is through distortion models, that
try to model which reorderings are more likely
than others. The simplest models just penalize
long jumps in the source sentence when producing
the target sentence. These models have also been
generalized (Al-Onaizan and Papineni, 2006; Till-
man, 2004) to allow for lexical dependencies on the
source. While these models are simple, and can
be integrated with the decoder they are insufficient
to capture long-range reordering phenomena espe-
cially for language pairs that differ significantly.
The weakness of these simple distortion models
has been overcome using syntax of either the source
or target sentence (Yamada and Knight, 2002; Gal-
ley et al., 2006; Liu et al., 2006; Zollmann and Venu-
gopal, 2006). While these methods have shown to
be useful in improving machine translation perfor-
</bodyText>
<page confidence="0.984793">
486
</page>
<note confidence="0.957904">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999969509803922">
mance they generally involve joint parsing of the
source and target language which is significantly
more computationally expensive when compared to
phrase based translation systems. Another approach
that overcomes this weakness, is to to reorder the
source sentence based on rules applied on the source
parse (either hand written or learned from data) both
when training and testing (Collins et al., 2005; Gen-
zel, 2010; Visweswariah et al., 2010).
In this paper we propose a novel method for deal-
ing with the word order problem that is efficient and
does not rely on a source or target side parse being
available. We cast the word ordering problem as a
Traveling Salesman Problem (TSP) based on previ-
ous work on word-based and phrased-based statis-
tical machine translation (Tillmann and Ney, 2003;
Zaslavskiy et al., 2009). Words are the cities in the
TSP and the objective is to learn the distance be-
tween words so that the shortest tour corresponds to
the ordering of the words in the source sentence in
the target language. We show that the TSP distances
for reordering can be learned from a small amount
of high-quality word alignment data by means of
pairwise word comparisons and an informative fea-
ture set involving words and part-of-speech (POS)
tags adapted and extended from prior work on de-
pendency parsing (McDonald et al., 2005b). Ob-
taining high-quality word alignments that we require
for training is fairly easy compared with obtaining a
treebank required to obtain parses for use in syntax
based methods.
We show experimentally that our reordering
model, even when used to reorder sentences for
training and testing (rather than being used as an
additional score in the decoder) improves machine
translation performance for: Hindi → English, En-
glish → Hindi, and Urdu → English. Although Urdu
is similar to Hindi from the point of reordering phe-
nomena we include it in our experiments since there
are publicly available datasets for Urdu-English. For
English → Hindi we obtained better machine trans-
lation performance with our reordering model as
compared to a method that uses reordering rules ap-
plied to the source side parse.
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work and places our work in
context. Section 3 outlines reordering issues due
to syntactic differences between Hindi and English.
Section 4 presents our reordering model, Section 5
presents experimental results and Section 6 presents
our conclusions and possible future work.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999973452380953">
There have been several studies demonstrating im-
proved machine translation performance by reorder-
ing source side sentences based on rules applied to
the source side parse during training and decoding.
Much of this work has used hand written rules and
several language pairs have been studied e.g German
to English (Collins et al., 2005), Chinese to English
(Wang et al., 2007), English to Hindi (Ramanathan
et al., 2009), English to Arabic (Badr et al., 2009)
and Japanese to English (Lee et al., 2010). There
have also been some studies where the rules are
learned from the data (Genzel, 2010; Visweswariah
et al., 2010; Xia and McCord, 2004). In addition
there has been work (Yamada and Knight, 2002;
Zollmann and Venugopal, 2006; Galley et al., 2006;
Liu et al., 2006) which uses source and/or target
side syntax in a Context Free Grammar framework
which results in machine translation decoding being
considered as a parsing problem. In this paper we
propose a model that does not require either source
or target side syntax while also preserving the effi-
ciency of reordering techniques based on rules ap-
plied to the source side parse.
In work that is closely related to ours, (Tromble
and Eisner, 2009) formulated word reordering as a
Linear Ordering Problem (LOP), an NP-hard permu-
tation problem. They learned LOP model weights
capable of assigning a score to every possible per-
mutation of the source language sentence from an
aligned corpus by using a averaged perceptron learn-
ing model. The key difference between our model
and the model in (Tromble and Eisner, 2009) is that
while they learn costs of a word wz appearing any-
where before wj, we learn costs of wz immediately
preceding wj. This results in more compact models
and (as we show in Section 5) better models.
Our model results in us having to solve a TSP
instance. The relation between the TSP and ma-
chine translation decoding has been explored before.
(Knight, 1999) showed that TSP is a sub-class of MT
decoding and thus established that the latter is NP-
hard. (Zaslavskiy et al., 2009) casts phrase-based
</bodyText>
<page confidence="0.978664">
487
</page>
<bodyText confidence="0.98864">
decoding as a TSP and they show favorable speed The cost c(m, n) can be thought of as the cost of the
performance trade-offs compared with Moses, an word at index m immediately preceding the word
existing state-of-the-art decoder. In (Tillmann and with index n in the candidate reordering. In this pa-
Ney, 2003), a beam-search algorithm used for TSP per, we parametrize the costs as:
is adapted to work with an IBM-4 word-based model c(m, n) = 0TΦ(w, m, n),
and phrase-based model respectively. As opposed where 0 is a learned vector of weights and Φ is a
to calculating TSP distances from existing machine vector of feature functions.
translation components ( viz. the translation, dis- Given a source sentence w we reorder it accord-
tortion and language model probabilities) we learn ing to the permutation 7r that minimizes the cost
model weights to reorder source sentences to match C(7r|w). Thus, we would like our cost function
target word order using an informative feature set C(7r|w) to be such that the correct reordering 7r∗ has
adapted from graph-based dependency parsing (Mc- the lowest cost of all possible reorderings 7r. In Sec-
Donald et al., 2005a). tion 4.1 we describe the features Φ that we use, and
3 Hindi-English reordering issues in Section 4.2 we describe how we train the weights
This section provides a brief survey of constructions 0 to obtain a good reordering model.
that the two languages in question differ as well as Given our model structure, the minimization
have in common. (Ramanathan et al., 2009) notes problem that we need to solve is identical to solving
the following divergences: a Asymmetric Traveling Salesman Problem (ATSP)
</bodyText>
<listItem confidence="0.781926857142857">
• English follows SVO order while Hindi follows with each word corresponding to a city, and the costs
SOV order c(m, n) representing the pairwise distances between
• English uses prepositions while Hindi uses the cities. Consider the following example:
post-positions English input: John eats apples
• Hindi allows greater word order freedom Hindi: John seba(apples) khaataa hai(eats)
• Hindi has a relatively richer case-marking sys- Desired reordered English: John apples eats
tem The ATSP that we need to solve is represented
In addition to these differences, (Visweswariah et pictorially in Figure 1 with sample costs. Note that
al., 2010) mention the similarity in word order in we have one extra node numbered 0. We start and
the case of adjective noun sequences (some books end the tour at node 0, and this determines the first
vs. kuch kitab). word in the reordered sentence. In this example the
4 Reordering model minimum cost tour is:
Consider a source sentence w consisting of a se- Start → John → apple → eats
quence of n words w1, w2, ... wn that we would recovering the right reordering for translation into
</listItem>
<bodyText confidence="0.997712571428571">
like to reorder into the target language order. Given Hindi.
a permutation 7r of the indices 1..n, let the candi- Solving the ATSP (which is a well known NP hard
date reordering be wπ1, wπ2, ..., wπn. Thus, 7ri de- problem) efficiently is crucial for the efficiency of
notes the index of the word in the source sentence our reordering model. To solve the ATSP, we first
that maps to position i in the candidate reordering. convert the ATSP to a symmetric TSP and then use
Clearly there are n! such permutations. Our reorder- the Lin-Kernighan heuristic as implemented in Con-
ing model assigns costs to candidate permutations corde, a state-of-the-art TSP solver (Applegate et al.,
as: 2005). We also experimented with using the exact
C(7r|w) = � c(7ri−1, 7ri). TSP solver in Concorde but since it was slower and
i did not improve performance we preferred using the
488 Lin-Kernighan heuristic. To convert the ATSP to
a symmetric TSP we double the size of the orig-
inal problem creating a node N&apos; for every node
N in the original graph. Following (Hornik and
</bodyText>
<figureCaption confidence="0.998861">
Figure 1: Example of an ATSP for reordering the sen-
tence: John eats apples.
</figureCaption>
<bodyText confidence="0.9997769">
Hahsler, 2009), we then set new costs as follows:
c0(A, B) = oo, c0(A, B0) = c0(B0, A) = c(A, B)
and C(A, A0) = −oo. Even with this doubling of
the number of nodes, we observed that solving the
TSPs with the Lin-Kernighan heuristic is very fast,
taking roughly 10 milliseconds per sentence on av-
erage. Overall, this means that our reordering model
is as fast as parsing and hence our model is compara-
ble in performance to techniques based on applying
rules to the parse tree.
</bodyText>
<subsectionHeader confidence="0.946795">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999995388888889">
Since we would like to model reordering phenomena
which are largely related to analyzing the syntax of
the source sentence, we chose to use features based
on those that have in the past been used for parsing
(McDonald et al., 2005a). A subset of the features
we use was also used for reordering in (Tromble and
Eisner, 2009).
To be able to generalize from relatively small
amounts of data, we use features that in addition to
depending on the words in the input sentence w de-
pend on the part-of-speech (POS) tags of the words
in the input sentence. All features Φ(w, i, j) we use
are binary features, that fire based on the identities
of the words and POS tags at or surrounding posi-
tions i and j in the source sentence. The first set of
feature templates we use are given in Table 1. These
features depend only on the identities of the word
and POS tag of the two positions i and j and we call
</bodyText>
<equation confidence="0.998095153846154">
wi pi wj pj
X X X X
X
X
X
X
X X
X X
X X
X X
X X
X X
X X X
</equation>
<tableCaption confidence="0.878265">
Table 1: Bigram feature templates used to calculate the
</tableCaption>
<bodyText confidence="0.993782625">
cost that word at position i immediately precedes word at
position j in the target word order. wi (pi) denotes the
word (POS tag) at position i in the source sentence. Each
of the templates is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
these Bigram features.
The second set of feature templates we use are
given in Table 2. These features, in addition to ex-
amining positions i and j examine the surround-
ing positions. We instantiate these feature templates
separately for the POS tag sequence and for the
word sequence. We call these two feature sets Con-
textPOS and ContextWord respectively. When in-
stantiated with POS tags, the first row of Table 2
looks at all POS tags between positions i and j.
(Tromble and Eisner, 2009) use Bigram and Con-
textPOS features, while we extend their feature set
with the use of ContextWord features. Since Hindi
is verb final, in Hindi sentences with multiple verb
groups it is rare for words with a verb in between
to be placed together in the reordering to match En-
glish. Looking at the POS tags of words between
positions i and j allows us to penalize such reorder-
ings.
Each of the templates described in Table 1 and
Table 2 is also conjoined with i-j the signed dis-
tance between the two words in the source sentence.
The values of i-j between 5 and 10, and greater than
10 are quantized (negative values are similarly quan-
tized).
In Section 5.2 we report on experiments showing
the relative performance of these different feature
</bodyText>
<figure confidence="0.9976227">
c(0,3)=5
c(3,0)=2
0
Start
3
apples
c(0,2)=5
c(0,1)=-1
c(1,0)= 5
c(3,1)=5
c(2,3)=3
c(3,2)=1
c(1,3)=0
c(2,0)=-2
c(1,2)=3
c(2,1)=5
1
John
2
eats
</figure>
<page confidence="0.989276">
489
</page>
<table confidence="0.991698611111111">
oi−1 oi oi+1 ob oj−1 oj oj+1
× × ×
× × × ×
× × ×
× × ×
× × ×
× × ×
× × × ×
× × ×
× × ×
× × ×
× × ×
× × × ×
× × ×
× × ×
× × × ×
× × ×
× × ×
</table>
<tableCaption confidence="0.997466">
Table 2: Context feature templates used to calculate the
</tableCaption>
<bodyText confidence="0.859777545454545">
cost that word at position i immediately precedes word
at position j in the target word order. oi denotes the ob-
servation at position i in the source sentence and ob de-
notes an observation at a position between i and j (i.e
i + 1 ≤ b ≤ j − 1). Each of the templates is instan-
tiated with the observation sequence o taken to be the
word sequence w and the POS tag sequence p. Each of
the templates is also conjoined with i-j the signed dis-
tance between the two positions in the source sentence.
types for the task of reordering Hindi sentences to
be in English word order.
</bodyText>
<subsectionHeader confidence="0.986042">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.99991147826087">
To train the weights θ in our model, we need a
collection of sentences, where we have the desired
reference reordering π*(x) for each input sentence
x. To obtain these reference reorderings we use
word aligned source-target sentence pairs. The qual-
ity and consistency of these reference reorderings
will depend on the quality of the word alignments
that we use. Given word aligned source and tar-
get sentences, we drop the source words that are not
aligned. Let mi be the mean of the target word po-
sitions that the source word at index i is aligned to.
We then sort the source indices in increasing order
of mi. If mi = mj (for example, because wi and wj
are aligned to the same set of words) we keep them
in the same order that they occurred in the source
sentence. Obtaining the target ordering in this man-
ner, is certainly not the only possible way and we
would like to explore better treatment of this in fu-
ture work.
We used the single best Margin Infused Re-
laxed Algorithm (MIRA) ((McDonald et al., 2005b),
(Crammer and Singer, 2003)) with the online up-
dates to our parameters being given by
</bodyText>
<equation confidence="0.9730715">
θi+1 = arg min
θ
s.t. C(π*|w) &lt; C(ˆπ|w) − L(π*, ˆπ).
In the equation above,
πˆ = arg min C(π|x)
π
</equation>
<bodyText confidence="0.999839111111111">
is the best reordering based on the current parameter
value and L is a loss function. We take the loss of
a reordering to be the number of words for which
the preceding word is wrong relative to the reference
target order.
We also experimented with the averaged percep-
tron algorithm (Collins, 2002), but found single best
MIRA to work slightly better and hence used MIRA
for all our experiments.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.976008181818182">
In this section we report on experiments to evalu-
ate our reordering model. The first method we use
for evaluation (monolingual BLEU) is by generat-
ing the desired reordering of the source sentence (as
described in Section 4.2) and compare the reordered
output to this desired reordered sentence using the
BLEU metric. In addition, to these monolingual
BLEU results, we also evaluate (in Section 5.5) the
reordering by its effect on eventual machine transla-
tion performance.
We note that our reordering techniques uses POS
information for the input sentence. The POS taggers
used in this paper are Maximum Entropy Markov
models trained using manually annotated POS cor-
pora. For Hindi, we used roughly fifty thousand
words with twenty six tags from the corpus de-
scribed in (Dalal et al., 2007). For Urdu we used
roughly fifty thousand words and forty six tags from
the CRULP corpus (Hussain, 2008) and for English
we used the Wall Street Journal section of the Penn
Treebank.
||θ − θi||
</bodyText>
<page confidence="0.951232">
490
</page>
<subsectionHeader confidence="0.8031595">
5.1 Reordering model training data and
alignment quality
</subsectionHeader>
<bodyText confidence="0.999908575">
To train our reordering models we need training data
where we have the input source language sentence
and the desired reordering in the target language.
As described in Section 4.2 we derive the refer-
ence reordered sentence using word alignments. Ta-
ble 3 presents our monolingual BLEU results for
Hindi to English reordering as the source of the
word alignments is varied. All results in Table 3
are with Bigram and ContextPOS features. We have
word alignments from three sources: A small set
of hand aligned sentences, HMM alignments (Vo-
gel et al., 1996) and alignments obtained using a su-
pervised Maximum Entropy aligner (Ittycheriah and
Roukos, 2005) trained on the hand alignments. The
F-measure for the HMM alignments were 65% and
78% for the Maximum Entropy model alignments.
We see that the quality of the alignments is an im-
portant determiner of reordering performance. Row
1 shows the BLEU for unreordered (baseline) Hindi
compared with the Hindi sentences reordered in En-
glish Order. Using just HMM alignments to train
our model we do worse than unreordered Hindi. Al-
though using the Maximum Entropy alignments is
better than using HMM alignments, we do not im-
prove upon a small number of hand alignments by
using all the Maximum Entropy alignments.
To improve upon the model trained with only
hand alignments we selected a small number of snip-
pets of sentences from our Maximum Entropy align-
ments. The goal was to pick parts of sentences
where the alignment is reliable enough to use for
training. The heuristic we used in the selection of
snippets was to pick maximal snippets of at least
7 consecutive Hindi words with all Hindi words
aligned to a consecutive span of English words,
with no unaligned English words in the span and no
English words aligned to Hindi words outside the
span. Adding snippets selected with this heuristic
improves the reordering performance of our model
as seen in the last row of Table 3.
</bodyText>
<subsectionHeader confidence="0.99653">
5.2 Feature set comparison
</subsectionHeader>
<bodyText confidence="0.976673666666667">
In this section we report on experiments to deter-
mine the performance of the different classes of fea-
tures (Bigram, ContextPos and ContextWord) dis-
</bodyText>
<table confidence="0.999551">
HMM MaxEnt Hand BLEU
- - - 35.9
220K - - 35.4
- 220K - 47.0
- 220K 6K 48.4
- - 6K 49.0
- Good 17K 6K 51.3
</table>
<tableCaption confidence="0.99058525">
Table 3: Monolingual BLEU scores for Hindi to English
reordering using models trained on different alignment
types and tested on a development set of 280 Hindi sen-
tences (5590 tokens).
</tableCaption>
<table confidence="0.999760833333333">
Feature template
Bigram ContextPOS ContextWord BLEU
- - - 35.9
× - - 43.8
× × - 49.0
× × × 51.3
</table>
<tableCaption confidence="0.9514895">
Table 4: Monolingual BLEU scores for Hindi to En-
glish reordering using models trained with different fea-
ture sets and tested on a development set of 280 Hindi
sentences (5590 tokens).
</tableCaption>
<bodyText confidence="0.999658454545455">
cussed in Section 4.1. Table 4 shows monolingual
BLEU results for training with different features sets
for Hindi to English reordering. In all cases, we
use a set of 6000 sentence pairs which were hand
aligned to generate the training data. It is clear that
all three sets of features contribute to performance of
the reordering model, however the number of Con-
textWord features is larger than the number of Bi-
gram and ContextPOS features put together, and it
may be desirable to select from this set of features
especially when training on large amounts of data.
</bodyText>
<subsectionHeader confidence="0.998765">
5.3 Monolingual reordering comparisons
</subsectionHeader>
<bodyText confidence="0.999838909090909">
Table 5 compares our reordering model with a reim-
plementation of the reordering model proposed in
(Tromble and Eisner, 2009). Both the models use
exactly the same features (bigram features and Con-
textPOS features) and are trained on the same data.
To generate our training data, for Hindi to English
and English to Hindi we use a set of 6000 hand
aligned sentences, for Urdu to English we use a set
of 8500 hand aligned sentences and for English to
French we use a set of 10000 hand aligned sentences
(a subset of Europarl and Hansards corpus). Our
</bodyText>
<page confidence="0.997305">
491
</page>
<table confidence="0.999419333333333">
Language pair Monolingual BLEU
Source Target Unreordered LOP TSP
Hindi English 35.9 36.6 49.0
English Hindi 34.4 48.4 56.7
Urdu English 35.6 39.5 49.9
English French 64.4 78.2 81.2
</table>
<tableCaption confidence="0.994501">
Table 5: Monolingual BLEU scores comparing the orig-
inal source order with desired target reorder without re-
ordering, and reordering using our model (TSP) and the
model proposed in (Tromble and Eisner, 2009) (LOP).
</tableCaption>
<bodyText confidence="0.999262888888889">
test data consisted of 280 sentences for Hindi to En-
glish and 400 sentences for all other language pairs
generated from hand aligned sentences. We include
English-French here to compare on a fairly similar
language pair with local reordering phenomena (the
main difference being that in French adjectives gen-
erally follow nouns). We note that our model outper-
forms the model proposed in (Tromble and Eisner,
2009) in all cases.
</bodyText>
<subsectionHeader confidence="0.999412">
5.4 Analysis of reordering performance
</subsectionHeader>
<bodyText confidence="0.99977125">
To get a feel for the qualitative performance of our
reordering algorithm and the kind of phenomena it
is able to capture, we analyze the reordering per-
formance in terms of (i) whether the clause restruc-
turing is done correctly – these can be thought of
as medium-to-long range reorderings, (ii) whether
clause boundaries are respected, and (iii) whether lo-
cal (short range) reordering is performed correctly.
The following analysis is for Hindi to English re-
ordering with the best model (this is also the model
used for Machine Translation experiments reported
on in Section 5.5).
</bodyText>
<listItem confidence="0.791297333333333">
• Clause structure: As discussed in Section 3,
the canonical clause order in Hindi is SOV,
while in English it is SVO. However, variations
</listItem>
<bodyText confidence="0.980955086956522">
on this structure are possible and quite frequent
(e.g., clauses with two objects). To evaluate
clause restructuring, we compared sequences
of subjects, objects and verbs in the output and
reference reorderings.
We had a set of 70 sentences annotated with
subject, direct object, indirect object and verb
information – these annotations were made on
the head word of each phrase, and the compar-
isons were on sequences of these words alone
and not the entire constituent phrase. 52 sen-
tences were reordered by the model to match
the order of the corresponding reference. Eight
sentences were ordered correctly but differently
from the reference, because the reference was
expressed in non-canonical fashion (e.g., in the
passive) – note that these cases negatively im-
pact the monolingual BLEU score. The follow-
ing example shows a sentence being reordered
correctly, where, however, the reference is ex-
pressed differently (note the position of the
subject “policy” (niiti) in the reference and the
reordered output)&apos;:
</bodyText>
<equation confidence="0.928537777777778">
Input: aba1 (now) taka2 (till) aisii3 (this) niiti4
(policy) kabhii5 (ever) nahii6 (not) rahii7 (has)
hai8 (been)
Reordered: taka2 (till) aba1 (now) aisii3 (this)
niiti4 (policy) hai8 (been) kabhii5 (ever) nahii6
(not) rahii7 (has)
Reference: taka2 (till) aba1 (now) aisii3 (this)
kabhii5 (ever) nahii6 (not) rahii7 (has) hai8
(been) niiti4 (policy)
</equation>
<bodyText confidence="0.9470525">
English: Till now this never has been the policy
The remaining ten sentences were reordered in-
correctly. These errors are largely in clauses
which deviate from the SVO order in some
way – clauses with multiple subjects or objects,
clauses with no object, etc.. For example, the
following sentence with two subjects and ob-
jects corresponding to the verb wearing has not
been reordered correctly.
Input: sabhii1 (all) purusha2 (men) safeda3
(white) evama4 (and) mahilaaen5 (women)
kesariyaa6 (saffron) vastra7 (clothes) dhaarana8
(wear) kiyeg hue10 (- ing) thiin11 (were)
Reordered: sabhii1 (all) purusha2 (men)
safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron) vastra7 (clothes)
dhaarana8 (wear) thiin11 (were) kiyeg hue10 (-
ing)
Reference: sabhii1 (all) purusha2 (men)
thiin11 (were) dhaarana8 (wear) kiyeg hue10 (-
&apos;The numeric subscripts in the examples indicate word po-
sitions in the input.
</bodyText>
<page confidence="0.995263">
492
</page>
<bodyText confidence="0.849564470588235">
ing) safeda3 (white) evama4 (and) mahilaaen5
(women) kesariyaa6 (saffron)
English: All men were wearing white and the
women saffron
The model possibly needs more data with pat-
terns that deviate from the standard SOV order
to learn to reorder them correctly. We could
also add to the model, features pertaining to
subject, object, etc.
• Clause boundaries: Measured on a set of
844 sentences which were marked with clause
boundaries, 37 sentences (4.4 %) had reorder-
ings that violated these boundaries. An exam-
ple of such a clause-boundary violation is be-
low:
Input: main1 (I) sarakaara2 (government) kaa3
(of) dhyaana4 (attention) maananiiya5 (hon-
</bodyText>
<equation confidence="0.990789238095238">
ourable) pradhaana6 (prime) mantri7 (min-
ister) dvaaraa8 (by) isa9 (this) sabhaa10
(house) me11 (in) kiye12 gaye13 (made) isa14
(this) vaade15 (promise) ki16 ora17 (towards)
dilaanaa18 (to bring) chaahuungaa19 (would
like)
Reordered: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) kii16 ora17 (to-
wards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) dhyaana4 (attention) kaa3 (of)
sarakaara2 (government) men11 (in) isa14 (this)
sabhaa10 (house)
Reference: main1 (I) chahuungaa19 (would
like) dilaanaa18 (to bring) dhyaana4 (attention)
kaa3 (of) sarakaara2 (government) kii16 ora17
(towards) isa9 (this) vaade15 (promise) kiye12
gaye13 (made) dvaaraa8 (by) maananiiya5
(honourable) mantri7 (minister) pradhaana6
(prime) men11 (in) isa9 (this) sabhaa10 (house)
</equation>
<bodyText confidence="0.999706153846154">
English I would like to bring the attention of
the government towards this promise made by
the honourable prime minister in this house.
Note how the italicized clause, which is kept
together in the reference, is split up incorrectly
in the reordered output. The proportion of such
boundary violations is, however, quite low, be-
cause Hindi being a verb-final language, most
clauses end with a verb and it is probably quite
straightforward for the model to keep clauses
separate. A clause boundary detection program
should make it possible to eliminate the re-
maining errors.
</bodyText>
<listItem confidence="0.862677333333333">
• Local reordering: To estimate the short range
reordering performance, we consider how of-
ten different POS bigrams in the input are re-
ordered correctly. Here, we expect the model
to reorder prepositions correctly, and to avoid
any reordering that moves apart nouns and their
</listItem>
<bodyText confidence="0.951467419354839">
adjectival pre-modifiers or components of com-
pound nouns (see Section 3). Table 6 sum-
marizes the reordering performance for these
categories for a set of 280 sentences (same as
the test set used in Section 5.1). Each row
in Table 6 indicates the total number of cor-
rect instances for the pair, i.e., the number of
instances of the pair in the reference (column
titled Total), the number of instances that al-
ready appear in the correct order in the input
(column Input), and the number that are or-
dered correctly by the reordering model (col-
umn Reordered). The first two rows show that
adjective-noun and noun-noun (compounds)
are in most cases correctly retained in the orig-
inal order by the model. The final row shows
that while many prepositions have been moved
into their correct positions, there are still quite a
few mismatches with the reference. An impor-
tant reason why this happens is that nouns mod-
ified by prepositional phrases can often also be
expressed as noun compounds. For example,
vidyuta (electricity) kii (oJ) aavashyakataaen
(requirements) in Hindi can be expressed either
as “requirements of electricity” or “electricity
requirements”. The latter expression results in
a match with the input (explaining many of the
104 correct orders in the input) and a mismatch
with the model’s reordering. The same problem
in the training data would also adversely impact
the learning of the preposition reordering rule.
</bodyText>
<page confidence="0.998058">
493
</page>
<table confidence="0.999585333333333">
Language pair BLEU
Source Target Unreordered Reordered
Hindi English 14.7 16.7
Urdu English 23.3 24.8
English Hindi 20.7 22.5
POS pair Total Input Reordered
adj-noun 234 192 196
noun-noun 46 44 42
prep-noun 436 104 250
</table>
<tableCaption confidence="0.955487">
Table 6: An analyis of reordering for a few POS bigrams
</tableCaption>
<subsectionHeader confidence="0.991373">
5.5 Machine translation results
</subsectionHeader>
<bodyText confidence="0.999615717948718">
We now present experiments in incorporating the re-
ordering model in machine translation systems. For
all results presented here, we reorder the training and
test data using the single best reordering based on
our reordering model for each sentence. For each of
the language pairs we evaluated, we trained Direct
Translation Model 2 (DTM) systems (Ittycheriah
and Roukos, 2007) with and without reordering and
compared performance on test data. We note that the
DTM system includes features that allow it to model
lexicalized reordering phenomena. The reordering
window size was set to +/-8 words for both the base-
line and our reordered input. In our experiments, we
left the word alignments fixed, i.e we reordered the
existing word alignments rather than realigning the
sentences after reordering. Redoing the word align-
ments with the reordered data could potentially give
further small improvements. We note that we ob-
tained better baseline performance using DTM sys-
tems than the standard Moses/Giza++ pipeline (e.g
we obtained a BLEU of 14.9 for English to Hindi
with a standard Moses/Giza++ pipeline). For all of
our systems we used a combination of HMM (Vo-
gel et al., 1996) and MaxEnt alignments (Ittycheriah
and Roukos, 2005).
For our Hindi-English experiments we use a train-
ing set of roughly 250k sentences (5.5M words) con-
sisting of the Darpa-TIDES dataset (Bojar et al.,
2010) and an internal dataset from several domains
but dominated by news. Our test set was roughly
1.2K sentences from the news domain with a sin-
gle reference. To train our reordering model, we
used roughly 6K alignments plus 17K snippets se-
lected from MaxEnt alignments as described in Sec-
tion 5.1 with bigram, ContextPOS and ContextWord
features. The monolingual reordering BLEU (on the
same data reported on in Section 5.3) was 54.0 for
Hindi to English and 60.8 for English to Hindi.
For our Urdu-English experiments we used 70k
</bodyText>
<tableCaption confidence="0.990292333333333">
Table 7: Translation performance without reordering
(baseline) compared with performance after preordering
with our reordering model.
</tableCaption>
<bodyText confidence="0.999834142857143">
sentences from the NIST MT-08 training corpus
and used the MT-08 eval set for testing. We note
that the MT-08 eval set has four references as com-
pared to one reference for our Hindi-English test
set. This largely explains the improved baseline per-
formance for Urdu-English as compared to Hindi-
English. We present averaged results for the Web
and News part of the test sets. To train the reorder-
ing model we used 9K hand alignments and 11K
snippets extracted from MaxEnt alignments as de-
scribed in Section 5.1 with bigram, ContextPOS and
ContextWord context feature. The monolingual re-
ordering BLEU for the reordering model thus ob-
tained (on the same data reported on in Section 5.3)
was 52.7.
Table 7 shows that for Hindi to English, English
to Hindi and for Urdu to English we see a gain
of 1.5 - 2 BLEU points. For English → Hindi
we also experimented with a system that uses rules
(learned from the data using the methods described
in (Visweswariah et al., 2010)) applied to a parse to
reorder source side English sentences. This system
had a BLEU score of 21.2, which is an improvement
over the baseline, but our reordering model is better
by 1.3 BLEU points.
An added benefit of our reordering model is that
the decoder can be run with a smaller search space
exploring only a small amount of reordering with-
out losing accuracy but running substantially faster.
Table 8 shows the variation in machine Hindi to En-
glish translation performance with varying skip size
(this parameter sets the maximum number of words
skipped during decoding, lower values are associ-
ated with a restricted decoder search space and in-
creased speed).
</bodyText>
<page confidence="0.996454">
494
</page>
<table confidence="0.9986755">
skip Unreordered Reordered
2 12.2 16.7
4 13.4 16.7
8 14.7 16.4
</table>
<tableCaption confidence="0.9949905">
Table 8: Translation performance with/without reorder-
ing with varying decoder search space.
</tableCaption>
<sectionHeader confidence="0.987616" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999623416666667">
In this paper we presented a reordering model to
reorder source language data to make it resemble
the target language word order without using either
a source or target parser. We showed consistent
gains of up to 2 BLEU points in machine transla-
tion performance using this model to preorder train-
ing and test data. We show better performance com-
pared to syntax based reordering rules for English
to Hindi translation. Our model used only a part of
speech tagger (sometimes trained with fairly small
amounts of data) and a small corpus of word align-
ments. Considering the fact that treebanks required
to build high quality parsers are costly to obtain, we
think that our reordering model is a viable alterna-
tive to using syntax for reordering. We also note,
that with the preordering based on our reordering
model we can achieve the best BLEU scores with
a much tighter search space in the decoder. Even ac-
counting for the cost of finding the best reordering
according to our model, this usually results in faster
processing than if we did not have the reordering in
place.
In future work we plan to explore using more data
from automatic alignments, perhaps by considering
a joint model for aligning and reordering. We would
also like to explore doing away with the requirement
of having a POS tagger, using completely unsuper-
vised methods to class words. We currently only
look at word pairs in calculating the loss function
used in MIRA updates. We would like to investigate
the use of other loss functions and their effect on re-
ordering performance. We also would like to explore
whether the use of scores from our reordering model
directly in machine translation systems can improve
performance relative to using just the single best re-
ordering.
</bodyText>
<sectionHeader confidence="0.998459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99881305882353">
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529–536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
David L. Applegate, Robert E. Bixby, Vasek Chvatal, and
William J. Cook. 2005. Concorde tsp solver. In
http://www.tsp.gatech.edu/.
Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syn-
tactic phrase reordering for English-to-Arabic statisti-
cal machine translation. In Proceedings of EACL.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In LREC.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,
Morristown, NJ, USA. Association for Computational
Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research.
Aniket Dalal, Kumar Nagaraj, Uma Sawant, Sandeep
Shelke, and Pushpak Bhattacharyya. 2007. Building
feature rich pos tagger for morphologically rich lan-
guages: Experiences in Hindi. In Proceedings of In-
ternational Conference on Natural Language Process-
ing.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proceedings of ACL.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Kurt Hornik and Michael Hahsler. 2009. TSP–
infrastructure for the traveling salesperson problem.
Journal of Statistical Software, 23(i02).
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP’08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ’05, pages 89–96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.988109">
495
</page>
<reference confidence="0.999310344262295">
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Proceedings of HLT-NAACL,
pages 57–64.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25:607–615, December.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In COL-
ING.
Y. Liu, Q. Liu, and S. Lin. 2006. Tree-to-String align-
ment template for statistical machine translation. In
Proceedings of ACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97–133.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of ACL-IJCNLP.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
</reference>
<page confidence="0.999106">
496
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.538503">
<title confidence="0.999949">A Word Reordering Model for Improved Machine Translation</title>
<author confidence="0.990787">Karthik Visweswariah Rajakrishnan Rajkumar Ankur Gandhe</author>
<affiliation confidence="0.992508">IBM Research India Dept. of Linguistics IBM Research India Bangalore, India Ohio State University Bangalore,</affiliation>
<email confidence="0.997685">v-karthik@in.ibm.comraja@ling.osu.eduankugand@in.ibm.com</email>
<author confidence="0.895282">Ananthakrishnan</author>
<affiliation confidence="0.8601495">IBM Research Bangalore,</affiliation>
<email confidence="0.997692">aramana2@in.ibm.com</email>
<abstract confidence="0.995035793103448">Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preceding another word. We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL, ACL-44,</booktitle>
<pages>529--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2535" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="397" endWordPosition="400">ource sentence. Current phrase based machine translation systems can capture short range reorderings via the phrase table. Even the capturing of these local reordering phenomena is constrained by the amount of training data available. For example, if adjectives precede nouns in the source language and follow nouns in the target language we still need to see a particular adjective noun pair in the parallel corpus to handle the reordering via the phrase table. Phrase based systems also rely on the target side language model to produce the right target side order. This is known to be inadequate (Al-Onaizan and Papineni, 2006), and this inadequacy has spurred various attempts to overcome the problem of handling differing word order in languages. One approach is through distortion models, that try to model which reorderings are more likely than others. The simplest models just penalize long jumps in the source sentence when producing the target sentence. These models have also been generalized (Al-Onaizan and Papineni, 2006; Tillman, 2004) to allow for lexical dependencies on the source. While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering pheno</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of ACL, ACL-44, pages 529–536, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Applegate</author>
<author>Robert E Bixby</author>
<author>Vasek Chvatal</author>
<author>William J Cook</author>
</authors>
<title>Concorde tsp solver.</title>
<date>2005</date>
<note>In http://www.tsp.gatech.edu/.</note>
<marker>Applegate, Bixby, Chvatal, Cook, 2005</marker>
<rawString>David L. Applegate, Robert E. Bixby, Vasek Chvatal, and William J. Cook. 2005. Concorde tsp solver. In http://www.tsp.gatech.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ibrahim Badr</author>
<author>Rabih Zbib</author>
<author>James Glass</author>
</authors>
<title>Syntactic phrase reordering for English-to-Arabic statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="6661" citStr="Badr et al., 2009" startWordPosition="1060" endWordPosition="1063">ction 4 presents our reordering model, Section 5 presents experimental results and Section 6 presents our conclusions and possible future work. 2 Related work There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the effic</context>
</contexts>
<marker>Badr, Zbib, Glass, 2009</marker>
<rawString>Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syntactic phrase reordering for English-to-Arabic statistical machine translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Pavel Stranak</author>
<author>Daniel Zeman</author>
</authors>
<title>Data issues in English-to-Hindi machine translation.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="31965" citStr="Bojar et al., 2010" startWordPosition="5380" endWordPosition="5383">the sentences after reordering. Redoing the word alignments with the reordered data could potentially give further small improvements. We note that we obtained better baseline performance using DTM systems than the standard Moses/Giza++ pipeline (e.g we obtained a BLEU of 14.9 for English to Hindi with a standard Moses/Giza++ pipeline). For all of our systems we used a combination of HMM (Vogel et al., 1996) and MaxEnt alignments (Ittycheriah and Roukos, 2005). For our Hindi-English experiments we use a training set of roughly 250k sentences (5.5M words) consisting of the Darpa-TIDES dataset (Bojar et al., 2010) and an internal dataset from several domains but dominated by news. Our test set was roughly 1.2K sentences from the news domain with a single reference. To train our reordering model, we used roughly 6K alignments plus 17K snippets selected from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord features. The monolingual reordering BLEU (on the same data reported on in Section 5.3) was 54.0 for Hindi to English and 60.8 for English to Hindi. For our Urdu-English experiments we used 70k Table 7: Translation performance without reordering (baseline) compared </context>
</contexts>
<marker>Bojar, Stranak, Zeman, 2010</marker>
<rawString>Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010. Data issues in English-to-Hindi machine translation. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL, pages 531–540, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17634" citStr="Collins, 2002" startWordPosition="3052" endWordPosition="3053"> better treatment of this in future work. We used the single best Margin Infused Relaxed Algorithm (MIRA) ((McDonald et al., 2005b), (Crammer and Singer, 2003)) with the online updates to our parameters being given by θi+1 = arg min θ s.t. C(π*|w) &lt; C(ˆπ|w) − L(π*, ˆπ). In the equation above, πˆ = arg min C(π|x) π is the best reordering based on the current parameter value and L is a loss function. We take the loss of a reordering to be the number of words for which the preceding word is wrong relative to the reference target order. We also experimented with the averaged perceptron algorithm (Collins, 2002), but found single best MIRA to work slightly better and hence used MIRA for all our experiments. 5 Experiments In this section we report on experiments to evaluate our reordering model. The first method we use for evaluation (monolingual BLEU) is by generating the desired reordering of the source sentence (as described in Section 4.2) and compare the reordered output to this desired reordered sentence using the BLEU metric. In addition, to these monolingual BLEU results, we also evaluate (in Section 5.5) the reordering by its effect on eventual machine translation performance. We note that ou</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="17179" citStr="Crammer and Singer, 2003" startWordPosition="2964" endWordPosition="2967">drop the source words that are not aligned. Let mi be the mean of the target word positions that the source word at index i is aligned to. We then sort the source indices in increasing order of mi. If mi = mj (for example, because wi and wj are aligned to the same set of words) we keep them in the same order that they occurred in the source sentence. Obtaining the target ordering in this manner, is certainly not the only possible way and we would like to explore better treatment of this in future work. We used the single best Margin Infused Relaxed Algorithm (MIRA) ((McDonald et al., 2005b), (Crammer and Singer, 2003)) with the online updates to our parameters being given by θi+1 = arg min θ s.t. C(π*|w) &lt; C(ˆπ|w) − L(π*, ˆπ). In the equation above, πˆ = arg min C(π|x) π is the best reordering based on the current parameter value and L is a loss function. We take the loss of a reordering to be the number of words for which the preceding word is wrong relative to the reference target order. We also experimented with the averaged perceptron algorithm (Collins, 2002), but found single best MIRA to work slightly better and hence used MIRA for all our experiments. 5 Experiments In this section we report on expe</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Dalal</author>
<author>Kumar Nagaraj</author>
<author>Uma Sawant</author>
<author>Sandeep Shelke</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Building feature rich pos tagger for morphologically rich languages: Experiences in Hindi.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="18536" citStr="Dalal et al., 2007" startWordPosition="3199" endWordPosition="3202">g of the source sentence (as described in Section 4.2) and compare the reordered output to this desired reordered sentence using the BLEU metric. In addition, to these monolingual BLEU results, we also evaluate (in Section 5.5) the reordering by its effect on eventual machine translation performance. We note that our reordering techniques uses POS information for the input sentence. The POS taggers used in this paper are Maximum Entropy Markov models trained using manually annotated POS corpora. For Hindi, we used roughly fifty thousand words with twenty six tags from the corpus described in (Dalal et al., 2007). For Urdu we used roughly fifty thousand words and forty six tags from the CRULP corpus (Hussain, 2008) and for English we used the Wall Street Journal section of the Penn Treebank. ||θ − θi|| 490 5.1 Reordering model training data and alignment quality To train our reordering models we need training data where we have the input source language sentence and the desired reordering in the target language. As described in Section 4.2 we derive the reference reordered sentence using word alignments. Table 3 presents our monolingual BLEU results for Hindi to English reordering as the source of the</context>
</contexts>
<marker>Dalal, Nagaraj, Sawant, Shelke, Bhattacharyya, 2007</marker>
<rawString>Aniket Dalal, Kumar Nagaraj, Uma Sawant, Sandeep Shelke, and Pushpak Bhattacharyya. 2007. Building feature rich pos tagger for morphologically rich languages: Experiences in Hindi. In Proceedings of International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3360" citStr="Galley et al., 2006" startWordPosition="524" endWordPosition="528">ore likely than others. The simplest models just penalize long jumps in the source sentence when producing the target sentence. These models have also been generalized (Al-Onaizan and Papineni, 2006; Tillman, 2004) to allow for lexical dependencies on the source. While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly. The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). While these methods have shown to be useful in improving machine translation perfor486 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems. Another approach that overcomes this weakness, is to to reorder the source sentence b</context>
<context position="6954" citStr="Galley et al., 2006" startWordPosition="1110" endWordPosition="1113">ules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Linear Ordering Problem (LOP), an NP-hard permutation problem. They learned LOP model weights capable of assignin</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Genzel</author>
</authors>
<title>Automatically learning source-side reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4111" citStr="Genzel, 2010" startWordPosition="639" endWordPosition="641">roceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems. Another approach that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either hand written or learned from data) both when training and testing (Collins et al., 2005; Genzel, 2010; Visweswariah et al., 2010). In this paper we propose a novel method for dealing with the word order problem that is efficient and does not rely on a source or target side parse being available. We cast the word ordering problem as a Traveling Salesman Problem (TSP) based on previous work on word-based and phrased-based statistical machine translation (Tillmann and Ney, 2003; Zaslavskiy et al., 2009). Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target l</context>
<context position="6795" citStr="Genzel, 2010" startWordPosition="1086" endWordPosition="1087">rk. 2 Related work There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and </context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>D. Genzel. 2010. Automatically learning source-side reordering rules for large scale machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Hornik</author>
<author>Michael Hahsler</author>
</authors>
<title>TSP– infrastructure for the traveling salesperson problem.</title>
<date>2009</date>
<journal>Journal of Statistical Software,</journal>
<volume>23</volume>
<marker>Hornik, Hahsler, 2009</marker>
<rawString>Kurt Hornik and Michael Hahsler. 2009. TSP– infrastructure for the traveling salesperson problem. Journal of Statistical Software, 23(i02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarmad Hussain</author>
</authors>
<title>Resources for Urdu language processing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Workshop on Asian Language Resources, IJCNLP’08.</booktitle>
<contexts>
<context position="18640" citStr="Hussain, 2008" startWordPosition="3219" endWordPosition="3220">ered sentence using the BLEU metric. In addition, to these monolingual BLEU results, we also evaluate (in Section 5.5) the reordering by its effect on eventual machine translation performance. We note that our reordering techniques uses POS information for the input sentence. The POS taggers used in this paper are Maximum Entropy Markov models trained using manually annotated POS corpora. For Hindi, we used roughly fifty thousand words with twenty six tags from the corpus described in (Dalal et al., 2007). For Urdu we used roughly fifty thousand words and forty six tags from the CRULP corpus (Hussain, 2008) and for English we used the Wall Street Journal section of the Penn Treebank. ||θ − θi|| 490 5.1 Reordering model training data and alignment quality To train our reordering models we need training data where we have the input source language sentence and the desired reordering in the target language. As described in Section 4.2 we derive the reference reordered sentence using word alignments. Table 3 presents our monolingual BLEU results for Hindi to English reordering as the source of the word alignments is varied. All results in Table 3 are with Bigram and ContextPOS features. We have word</context>
</contexts>
<marker>Hussain, 2008</marker>
<rawString>Sarmad Hussain. 2008. Resources for Urdu language processing. In Proceedings of the 6th Workshop on Asian Language Resources, IJCNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP, HLT ’05,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19444" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3351" endWordPosition="3354">models we need training data where we have the input source language sentence and the desired reordering in the target language. As described in Section 4.2 we derive the reference reordered sentence using word alignments. Table 3 presents our monolingual BLEU results for Hindi to English reordering as the source of the word alignments is varied. All results in Table 3 are with Bigram and ContextPOS features. We have word alignments from three sources: A small set of hand aligned sentences, HMM alignments (Vogel et al., 1996) and alignments obtained using a supervised Maximum Entropy aligner (Ittycheriah and Roukos, 2005) trained on the hand alignments. The F-measure for the HMM alignments were 65% and 78% for the Maximum Entropy model alignments. We see that the quality of the alignments is an important determiner of reordering performance. Row 1 shows the BLEU for unreordered (baseline) Hindi compared with the Hindi sentences reordered in English Order. Using just HMM alignments to train our model we do worse than unreordered Hindi. Although using the Maximum Entropy alignments is better than using HMM alignments, we do not improve upon a small number of hand alignments by using all the Maximum Entropy align</context>
<context position="31810" citStr="Ittycheriah and Roukos, 2005" startWordPosition="5354" endWordPosition="5357">th the baseline and our reordered input. In our experiments, we left the word alignments fixed, i.e we reordered the existing word alignments rather than realigning the sentences after reordering. Redoing the word alignments with the reordered data could potentially give further small improvements. We note that we obtained better baseline performance using DTM systems than the standard Moses/Giza++ pipeline (e.g we obtained a BLEU of 14.9 for English to Hindi with a standard Moses/Giza++ pipeline). For all of our systems we used a combination of HMM (Vogel et al., 1996) and MaxEnt alignments (Ittycheriah and Roukos, 2005). For our Hindi-English experiments we use a training set of roughly 250k sentences (5.5M words) consisting of the Darpa-TIDES dataset (Bojar et al., 2010) and an internal dataset from several domains but dominated by news. Our test set was roughly 1.2K sentences from the news domain with a single reference. To train our reordering model, we used roughly 6K alignments plus 17K snippets selected from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord features. The monolingual reordering BLEU (on the same data reported on in Section 5.3) was 54.0 for Hindi to E</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proceedings of HLT/EMNLP, HLT ’05, pages 89–96, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="30955" citStr="Ittycheriah and Roukos, 2007" startWordPosition="5215" endWordPosition="5218"> Hindi English 14.7 16.7 Urdu English 23.3 24.8 English Hindi 20.7 22.5 POS pair Total Input Reordered adj-noun 234 192 196 noun-noun 46 44 42 prep-noun 436 104 250 Table 6: An analyis of reordering for a few POS bigrams 5.5 Machine translation results We now present experiments in incorporating the reordering model in machine translation systems. For all results presented here, we reorder the training and test data using the single best reordering based on our reordering model for each sentence. For each of the language pairs we evaluated, we trained Direct Translation Model 2 (DTM) systems (Ittycheriah and Roukos, 2007) with and without reordering and compared performance on test data. We note that the DTM system includes features that allow it to model lexicalized reordering phenomena. The reordering window size was set to +/-8 words for both the baseline and our reordered input. In our experiments, we left the word alignments fixed, i.e we reordered the existing word alignments rather than realigning the sentences after reordering. Redoing the word alignments with the reordered data could potentially give further small improvements. We note that we obtained better baseline performance using DTM systems tha</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In Proceedings of HLT-NAACL, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Comput. Linguist.,</journal>
<pages>25--607</pages>
<contexts>
<context position="8137" citStr="Knight, 1999" startWordPosition="1314" endWordPosition="1315">el weights capable of assigning a score to every possible permutation of the source language sentence from an aligned corpus by using a averaged perceptron learning model. The key difference between our model and the model in (Tromble and Eisner, 2009) is that while they learn costs of a word wz appearing anywhere before wj, we learn costs of wz immediately preceding wj. This results in more compact models and (as we show in Section 5) better models. Our model results in us having to solve a TSP instance. The relation between the TSP and machine translation decoding has been explored before. (Knight, 1999) showed that TSP is a sub-class of MT decoding and thus established that the latter is NPhard. (Zaslavskiy et al., 2009) casts phrase-based 487 decoding as a TSP and they show favorable speed The cost c(m, n) can be thought of as the cost of the performance trade-offs compared with Moses, an word at index m immediately preceding the word existing state-of-the-art decoder. In (Tillmann and with index n in the candidate reordering. In this paNey, 2003), a beam-search algorithm used for TSP per, we parametrize the costs as: is adapted to work with an IBM-4 word-based model c(m, n) = 0TΦ(w, m, n),</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Comput. Linguist., 25:607–615, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Bing Zhao</author>
<author>Xiaoqian Luo</author>
</authors>
<title>Constituent reordering and syntax models for Englishto-Japanese statistical machine translation.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="6704" citStr="Lee et al., 2010" startWordPosition="1068" endWordPosition="1071">on 5 presents experimental results and Section 6 presents our conclusions and possible future work. 2 Related work There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rul</context>
</contexts>
<marker>Lee, Zhao, Luo, 2010</marker>
<rawString>Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent reordering and syntax models for Englishto-Japanese statistical machine translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Tree-to-String alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3378" citStr="Liu et al., 2006" startWordPosition="529" endWordPosition="532">s. The simplest models just penalize long jumps in the source sentence when producing the target sentence. These models have also been generalized (Al-Onaizan and Papineni, 2006; Tillman, 2004) to allow for lexical dependencies on the source. While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly. The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). While these methods have shown to be useful in improving machine translation perfor486 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems. Another approach that overcomes this weakness, is to to reorder the source sentence based on rules appl</context>
<context position="6973" citStr="Liu et al., 2006" startWordPosition="1114" endWordPosition="1117">ource side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Linear Ordering Problem (LOP), an NP-hard permutation problem. They learned LOP model weights capable of assigning a score to every </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Y. Liu, Q. Liu, and S. Lin. 2006. Tree-to-String alignment template for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5033" citStr="McDonald et al., 2005" startWordPosition="797" endWordPosition="800">sed and phrased-based statistical machine translation (Tillmann and Ney, 2003; Zaslavskiy et al., 2009). Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target language. We show that the TSP distances for reordering can be learned from a small amount of high-quality word alignment data by means of pairwise word comparisons and an informative feature set involving words and part-of-speech (POS) tags adapted and extended from prior work on dependency parsing (McDonald et al., 2005b). Obtaining high-quality word alignments that we require for training is fairly easy compared with obtaining a treebank required to obtain parses for use in syntax based methods. We show experimentally that our reordering model, even when used to reorder sentences for training and testing (rather than being used as an additional score in the decoder) improves machine translation performance for: Hindi → English, English → Hindi, and Urdu → English. Although Urdu is similar to Hindi from the point of reordering phenomena we include it in our experiments since there are publicly available data</context>
<context position="12921" citStr="McDonald et al., 2005" startWordPosition="2134" endWordPosition="2137">nd C(A, A0) = −oo. Even with this doubling of the number of nodes, we observed that solving the TSPs with the Lin-Kernighan heuristic is very fast, taking roughly 10 milliseconds per sentence on average. Overall, this means that our reordering model is as fast as parsing and hence our model is comparable in performance to techniques based on applying rules to the parse tree. 4.1 Features Since we would like to model reordering phenomena which are largely related to analyzing the syntax of the source sentence, we chose to use features based on those that have in the past been used for parsing (McDonald et al., 2005a). A subset of the features we use was also used for reordering in (Tromble and Eisner, 2009). To be able to generalize from relatively small amounts of data, we use features that in addition to depending on the words in the input sentence w depend on the part-of-speech (POS) tags of the words in the input sentence. All features Φ(w, i, j) we use are binary features, that fire based on the identities of the words and POS tags at or surrounding positions i and j in the source sentence. The first set of feature templates we use are given in Table 1. These features depend only on the identities </context>
<context position="17149" citStr="McDonald et al., 2005" startWordPosition="2960" endWordPosition="2963"> and target sentences, we drop the source words that are not aligned. Let mi be the mean of the target word positions that the source word at index i is aligned to. We then sort the source indices in increasing order of mi. If mi = mj (for example, because wi and wj are aligned to the same set of words) we keep them in the same order that they occurred in the source sentence. Obtaining the target ordering in this manner, is certainly not the only possible way and we would like to explore better treatment of this in future work. We used the single best Margin Infused Relaxed Algorithm (MIRA) ((McDonald et al., 2005b), (Crammer and Singer, 2003)) with the online updates to our parameters being given by θi+1 = arg min θ s.t. C(π*|w) &lt; C(ˆπ|w) − L(π*, ˆπ). In the equation above, πˆ = arg min C(π|x) π is the best reordering based on the current parameter value and L is a loss function. We take the loss of a reordering to be the number of words for which the preceding word is wrong relative to the reference target order. We also experimented with the averaged perceptron algorithm (Collins, 2002), but found single best MIRA to work slightly better and hence used MIRA for all our experiments. 5 Experiments In </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Hansraj Choudhary</author>
<author>Avishek Ghosh</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Case markers and morphology: addressing the crux of the fluency problem in English-Hindi smt.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="6622" citStr="Ramanathan et al., 2009" startWordPosition="1053" endWordPosition="1056">tic differences between Hindi and English. Section 4 presents our reordering model, Section 5 presents experimental results and Section 6 presents our conclusions and possible future work. 2 Related work There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side</context>
<context position="9813" citStr="Ramanathan et al., 2009" startWordPosition="1596" endWordPosition="1599"> like our cost function target word order using an informative feature set C(7r|w) to be such that the correct reordering 7r∗ has adapted from graph-based dependency parsing (Mc- the lowest cost of all possible reorderings 7r. In SecDonald et al., 2005a). tion 4.1 we describe the features Φ that we use, and 3 Hindi-English reordering issues in Section 4.2 we describe how we train the weights This section provides a brief survey of constructions 0 to obtain a good reordering model. that the two languages in question differ as well as Given our model structure, the minimization have in common. (Ramanathan et al., 2009) notes problem that we need to solve is identical to solving the following divergences: a Asymmetric Traveling Salesman Problem (ATSP) • English follows SVO order while Hindi follows with each word corresponding to a city, and the costs SOV order c(m, n) representing the pairwise distances between • English uses prepositions while Hindi uses the cities. Consider the following example: post-positions English input: John eats apples • Hindi allows greater word order freedom Hindi: John seba(apples) khaataa hai(eats) • Hindi has a relatively richer case-marking sys- Desired reordered English: Joh</context>
</contexts>
<marker>Ramanathan, Choudhary, Ghosh, Bhattacharyya, 2009</marker>
<rawString>Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: addressing the crux of the fluency problem in English-Hindi smt. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2955" citStr="Tillman, 2004" startWordPosition="462" endWordPosition="464">ring via the phrase table. Phrase based systems also rely on the target side language model to produce the right target side order. This is known to be inadequate (Al-Onaizan and Papineni, 2006), and this inadequacy has spurred various attempts to overcome the problem of handling differing word order in languages. One approach is through distortion models, that try to model which reorderings are more likely than others. The simplest models just penalize long jumps in the source sentence when producing the target sentence. These models have also been generalized (Al-Onaizan and Papineni, 2006; Tillman, 2004) to allow for lexical dependencies on the source. While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly. The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). While these methods have shown to be useful in improving machine translation perfor486 Proceedings of the 2011 Conference on Empirical Methods i</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4489" citStr="Tillmann and Ney, 2003" startWordPosition="703" endWordPosition="706">ystems. Another approach that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either hand written or learned from data) both when training and testing (Collins et al., 2005; Genzel, 2010; Visweswariah et al., 2010). In this paper we propose a novel method for dealing with the word order problem that is efficient and does not rely on a source or target side parse being available. We cast the word ordering problem as a Traveling Salesman Problem (TSP) based on previous work on word-based and phrased-based statistical machine translation (Tillmann and Ney, 2003; Zaslavskiy et al., 2009). Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target language. We show that the TSP distances for reordering can be learned from a small amount of high-quality word alignment data by means of pairwise word comparisons and an informative feature set involving words and part-of-speech (POS) tags adapted and extended from prior work on dependency parsing (McDonald et al., 2005b). Obtaining high-quality word alignments that we requi</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7408" citStr="Tromble and Eisner, 2009" startWordPosition="1187" endWordPosition="1190">Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Linear Ordering Problem (LOP), an NP-hard permutation problem. They learned LOP model weights capable of assigning a score to every possible permutation of the source language sentence from an aligned corpus by using a averaged perceptron learning model. The key difference between our model and the model in (Tromble and Eisner, 2009) is that while they learn costs of a word wz appearing anywhere before wj, we learn costs of wz immediately preceding wj. This results in more compact models and (as we show in Section 5) better models. Our model results in us havin</context>
<context position="13015" citStr="Tromble and Eisner, 2009" startWordPosition="2151" endWordPosition="2154">g the TSPs with the Lin-Kernighan heuristic is very fast, taking roughly 10 milliseconds per sentence on average. Overall, this means that our reordering model is as fast as parsing and hence our model is comparable in performance to techniques based on applying rules to the parse tree. 4.1 Features Since we would like to model reordering phenomena which are largely related to analyzing the syntax of the source sentence, we chose to use features based on those that have in the past been used for parsing (McDonald et al., 2005a). A subset of the features we use was also used for reordering in (Tromble and Eisner, 2009). To be able to generalize from relatively small amounts of data, we use features that in addition to depending on the words in the input sentence w depend on the part-of-speech (POS) tags of the words in the input sentence. All features Φ(w, i, j) we use are binary features, that fire based on the identities of the words and POS tags at or surrounding positions i and j in the source sentence. The first set of feature templates we use are given in Table 1. These features depend only on the identities of the word and POS tag of the two positions i and j and we call wi pi wj pj X X X X X X X X X</context>
<context position="14472" citStr="Tromble and Eisner, 2009" startWordPosition="2435" endWordPosition="2438">the source sentence. Each of the templates is also conjoined with i-j the signed distance between the two words in the source sentence. these Bigram features. The second set of feature templates we use are given in Table 2. These features, in addition to examining positions i and j examine the surrounding positions. We instantiate these feature templates separately for the POS tag sequence and for the word sequence. We call these two feature sets ContextPOS and ContextWord respectively. When instantiated with POS tags, the first row of Table 2 looks at all POS tags between positions i and j. (Tromble and Eisner, 2009) use Bigram and ContextPOS features, while we extend their feature set with the use of ContextWord features. Since Hindi is verb final, in Hindi sentences with multiple verb groups it is rare for words with a verb in between to be placed together in the reordering to match English. Looking at the POS tags of words between positions i and j allows us to penalize such reorderings. Each of the templates described in Table 1 and Table 2 is also conjoined with i-j the signed distance between the two words in the source sentence. The values of i-j between 5 and 10, and greater than 10 are quantized </context>
<context position="22196" citStr="Tromble and Eisner, 2009" startWordPosition="3832" endWordPosition="3835"> for Hindi to English reordering. In all cases, we use a set of 6000 sentence pairs which were hand aligned to generate the training data. It is clear that all three sets of features contribute to performance of the reordering model, however the number of ContextWord features is larger than the number of Bigram and ContextPOS features put together, and it may be desirable to select from this set of features especially when training on large amounts of data. 5.3 Monolingual reordering comparisons Table 5 compares our reordering model with a reimplementation of the reordering model proposed in (Tromble and Eisner, 2009). Both the models use exactly the same features (bigram features and ContextPOS features) and are trained on the same data. To generate our training data, for Hindi to English and English to Hindi we use a set of 6000 hand aligned sentences, for Urdu to English we use a set of 8500 hand aligned sentences and for English to French we use a set of 10000 hand aligned sentences (a subset of Europarl and Hansards corpus). Our 491 Language pair Monolingual BLEU Source Target Unreordered LOP TSP Hindi English 35.9 36.6 49.0 English Hindi 34.4 48.4 56.7 Urdu English 35.6 39.5 49.9 English French 64.4 </context>
<context position="23433" citStr="Tromble and Eisner, 2009" startWordPosition="4041" endWordPosition="4044">ble 5: Monolingual BLEU scores comparing the original source order with desired target reorder without reordering, and reordering using our model (TSP) and the model proposed in (Tromble and Eisner, 2009) (LOP). test data consisted of 280 sentences for Hindi to English and 400 sentences for all other language pairs generated from hand aligned sentences. We include English-French here to compare on a fairly similar language pair with local reordering phenomena (the main difference being that in French adjectives generally follow nouns). We note that our model outperforms the model proposed in (Tromble and Eisner, 2009) in all cases. 5.4 Analysis of reordering performance To get a feel for the qualitative performance of our reordering algorithm and the kind of phenomena it is able to capture, we analyze the reordering performance in terms of (i) whether the clause restructuring is done correctly – these can be thought of as medium-to-long range reorderings, (ii) whether clause boundaries are respected, and (iii) whether local (short range) reordering is performed correctly. The following analysis is for Hindi to English reordering with the best model (this is also the model used for Machine Translation exper</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax based reordering with automatically derived rules for improved statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4139" citStr="Visweswariah et al., 2010" startWordPosition="642" endWordPosition="645">the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems. Another approach that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either hand written or learned from data) both when training and testing (Collins et al., 2005; Genzel, 2010; Visweswariah et al., 2010). In this paper we propose a novel method for dealing with the word order problem that is efficient and does not rely on a source or target side parse being available. We cast the word ordering problem as a Traveling Salesman Problem (TSP) based on previous work on word-based and phrased-based statistical machine translation (Tillmann and Ney, 2003; Zaslavskiy et al., 2009). Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target language. We show that the TS</context>
<context position="6822" citStr="Visweswariah et al., 2010" startWordPosition="1088" endWordPosition="1091">work There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated wo</context>
<context position="33592" citStr="Visweswariah et al., 2010" startWordPosition="5656" endWordPosition="5659">eb and News part of the test sets. To train the reordering model we used 9K hand alignments and 11K snippets extracted from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord context feature. The monolingual reordering BLEU for the reordering model thus obtained (on the same data reported on in Section 5.3) was 52.7. Table 7 shows that for Hindi to English, English to Hindi and for Urdu to English we see a gain of 1.5 - 2 BLEU points. For English → Hindi we also experimented with a system that uses rules (learned from the data using the methods described in (Visweswariah et al., 2010)) applied to a parse to reorder source side English sentences. This system had a BLEU score of 21.2, which is an improvement over the baseline, but our reordering model is better by 1.3 BLEU points. An added benefit of our reordering model is that the decoder can be run with a smaller search space exploring only a small amount of reordering without losing accuracy but running substantially faster. Table 8 shows the variation in machine Hindi to English translation performance with varying skip size (this parameter sets the maximum number of words skipped during decoding, lower values are assoc</context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational Linguistics.</booktitle>
<contexts>
<context position="19346" citStr="Vogel et al., 1996" startWordPosition="3336" endWordPosition="3340">|| 490 5.1 Reordering model training data and alignment quality To train our reordering models we need training data where we have the input source language sentence and the desired reordering in the target language. As described in Section 4.2 we derive the reference reordered sentence using word alignments. Table 3 presents our monolingual BLEU results for Hindi to English reordering as the source of the word alignments is varied. All results in Table 3 are with Bigram and ContextPOS features. We have word alignments from three sources: A small set of hand aligned sentences, HMM alignments (Vogel et al., 1996) and alignments obtained using a supervised Maximum Entropy aligner (Ittycheriah and Roukos, 2005) trained on the hand alignments. The F-measure for the HMM alignments were 65% and 78% for the Maximum Entropy model alignments. We see that the quality of the alignments is an important determiner of reordering performance. Row 1 shows the BLEU for unreordered (baseline) Hindi compared with the Hindi sentences reordered in English Order. Using just HMM alignments to train our model we do worse than unreordered Hindi. Although using the Maximum Entropy alignments is better than using HMM alignment</context>
<context position="31757" citStr="Vogel et al., 1996" startWordPosition="5346" endWordPosition="5350">ng window size was set to +/-8 words for both the baseline and our reordered input. In our experiments, we left the word alignments fixed, i.e we reordered the existing word alignments rather than realigning the sentences after reordering. Redoing the word alignments with the reordered data could potentially give further small improvements. We note that we obtained better baseline performance using DTM systems than the standard Moses/Giza++ pipeline (e.g we obtained a BLEU of 14.9 for English to Hindi with a standard Moses/Giza++ pipeline). For all of our systems we used a combination of HMM (Vogel et al., 1996) and MaxEnt alignments (Ittycheriah and Roukos, 2005). For our Hindi-English experiments we use a training set of roughly 250k sentences (5.5M words) consisting of the Darpa-TIDES dataset (Bojar et al., 2010) and an internal dataset from several domains but dominated by news. Our test set was roughly 1.2K sentences from the news domain with a single reference. To train our reordering model, we used roughly 6K alignments plus 17K snippets selected from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord features. The monolingual reordering BLEU (on the same dat</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6578" citStr="Wang et al., 2007" startWordPosition="1046" endWordPosition="1049">tlines reordering issues due to syntactic differences between Hindi and English. Section 4 presents our reordering model, Section 5 presents experimental results and Section 6 presents our conclusions and possible future work. 2 Related work There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that d</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="6845" citStr="Xia and McCord, 2004" startWordPosition="1092" endWordPosition="1095">l studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Line</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical mt.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3339" citStr="Yamada and Knight, 2002" startWordPosition="520" endWordPosition="523">l which reorderings are more likely than others. The simplest models just penalize long jumps in the source sentence when producing the target sentence. These models have also been generalized (Al-Onaizan and Papineni, 2006; Tillman, 2004) to allow for lexical dependencies on the source. While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly. The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). While these methods have shown to be useful in improving machine translation perfor486 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems. Another approach that overcomes this weakness, is to to reorder </context>
<context position="6903" citStr="Yamada and Knight, 2002" startWordPosition="1102" endWordPosition="1105">formance by reordering source side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Linear Ordering Problem (LOP), an NP-hard permutation problem.</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical mt. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Zaslavskiy</author>
<author>Marc Dymetman</author>
<author>Nicola Cancedda</author>
</authors>
<title>Phrase-based statistical machine translation as a traveling salesman problem.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="4515" citStr="Zaslavskiy et al., 2009" startWordPosition="707" endWordPosition="710"> that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either hand written or learned from data) both when training and testing (Collins et al., 2005; Genzel, 2010; Visweswariah et al., 2010). In this paper we propose a novel method for dealing with the word order problem that is efficient and does not rely on a source or target side parse being available. We cast the word ordering problem as a Traveling Salesman Problem (TSP) based on previous work on word-based and phrased-based statistical machine translation (Tillmann and Ney, 2003; Zaslavskiy et al., 2009). Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target language. We show that the TSP distances for reordering can be learned from a small amount of high-quality word alignment data by means of pairwise word comparisons and an informative feature set involving words and part-of-speech (POS) tags adapted and extended from prior work on dependency parsing (McDonald et al., 2005b). Obtaining high-quality word alignments that we require for training is fairly </context>
<context position="8257" citStr="Zaslavskiy et al., 2009" startWordPosition="1334" endWordPosition="1337"> aligned corpus by using a averaged perceptron learning model. The key difference between our model and the model in (Tromble and Eisner, 2009) is that while they learn costs of a word wz appearing anywhere before wj, we learn costs of wz immediately preceding wj. This results in more compact models and (as we show in Section 5) better models. Our model results in us having to solve a TSP instance. The relation between the TSP and machine translation decoding has been explored before. (Knight, 1999) showed that TSP is a sub-class of MT decoding and thus established that the latter is NPhard. (Zaslavskiy et al., 2009) casts phrase-based 487 decoding as a TSP and they show favorable speed The cost c(m, n) can be thought of as the cost of the performance trade-offs compared with Moses, an word at index m immediately preceding the word existing state-of-the-art decoder. In (Tillmann and with index n in the candidate reordering. In this paNey, 2003), a beam-search algorithm used for TSP per, we parametrize the costs as: is adapted to work with an IBM-4 word-based model c(m, n) = 0TΦ(w, m, n), and phrase-based model respectively. As opposed where 0 is a learned vector of weights and Φ is a to calculating TSP di</context>
</contexts>
<marker>Zaslavskiy, Dymetman, Cancedda, 2009</marker>
<rawString>Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation as a traveling salesman problem. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="3409" citStr="Zollmann and Venugopal, 2006" startWordPosition="533" endWordPosition="537">dels just penalize long jumps in the source sentence when producing the target sentence. These models have also been generalized (Al-Onaizan and Papineni, 2006; Tillman, 2004) to allow for lexical dependencies on the source. While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly. The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). While these methods have shown to be useful in improving machine translation perfor486 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486–496, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics mance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems. Another approach that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either</context>
<context position="6933" citStr="Zollmann and Venugopal, 2006" startWordPosition="1106" endWordPosition="1109">urce side sentences based on rules applied to the source side parse during training and decoding. Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan et al., 2009), English to Arabic (Badr et al., 2009) and Japanese to English (Lee et al., 2010). There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004). In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem. In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse. In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Linear Ordering Problem (LOP), an NP-hard permutation problem. They learned LOP model weight</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>