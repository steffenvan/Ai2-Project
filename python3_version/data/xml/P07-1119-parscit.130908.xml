<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000188">
<title confidence="0.951927">
Substring-Based Transliteration
</title>
<author confidence="0.99291">
Tarek Sherif and Grzegorz Kondrak
</author>
<affiliation confidence="0.875504333333333">
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada T6G 2E8
</affiliation>
<email confidence="0.993296">
{tarek,kondrak}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995225">
Transliteration is the task of converting a
word from one alphabetic script to another.
We present a novel, substring-based ap-
proach to transliteration, inspired by phrase-
based models of machine translation. We in-
vestigate two implementations of substring-
based transliteration: a dynamic program-
ming algorithm, and a finite-state transducer.
We show that our substring-based transducer
not only outperforms a state-of-the-art letter-
based approach by a significant margin, but
is also orders of magnitude faster.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947805555556">
A significant proportion of out-of-vocabulary words
in machine translation models or cross language in-
formation retrieval systems are named entities. If
the languages are written in different scripts, these
names must be transliterated. Transliteration is the
task of converting a word from one writing script to
another, usually based on the phonetics of the orig-
inal word. If the target language contains all the
phonemes used in the source language, the translit-
eration is straightforward. For example, the Arabic
transliteration of Amanda isIY KAÓ
tially pronounced in the same way. However, if
some of the sounds are missing in the target lan-
guage, they are generally mapped to the most pho-
netically similar letter. For example, the sound [p]
in the name Paul, does not exist in Arabic, and the
phonotactic constraints of Arabic disallow the sound
[a] in this context, so the word is transliterated as
ÈñK., pronounced [bul].
The information loss inherent in the process of
transliteration makes back-transliteration, which is
the restoration of a previously transliterated word,
a particularly difficult task. Any phonetically rea-
sonable forward transliteration is essentially correct,
although occasionally there is a standard translitera-
tion (e.g. Omar Sharif). In the original script, how-
ever, there is usually only a single correct form. For
example, both Naguib Mahfouz and Najib Mahfuz
are reasonable transliterations of�ñ�®m×I.J�m.��, but
Tsharlz Dykens is certainly not acceptable if one is
referring to the author of Oliver Twist.
In a statistical approach to machine translitera-
tion, given a foreign word F, we are interested in
finding the English word E� that maximizes P(E|F).
Using Bayes’ rule, and keeping in mind that F is
constant, we can formulate the task as follows:
</bodyText>
<equation confidence="0.981342666666667">
P(F|E)P(E)
P(F)
P(F|E)P(E)
</equation>
<bodyText confidence="0.972250928571429">
This is known as the noisy channel approach to
machine transliteration, which splits the task into
two parts. The language model provides an esti-
mate of the probability P(E) of an English word,
while the transliteration model provides an estimate
of the probability P(F|E) of a foreign word being a
transliteration of an English word. The probabilities
assigned by the transliteration and language mod-
els counterbalance each other. For example, sim-
ply concatenating the most common mapping for
each letter in the Arabic stringÉ3/4K�AÓ, produces the
string maykl, which is barely pronounceable. In or-
der to generate the correct Michael, a model needs
I, which is essen-
</bodyText>
<equation confidence="0.989962">
E� = arg max
E
= arg max
E
</equation>
<page confidence="0.982229">
944
</page>
<note confidence="0.9249765">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 944–951,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965620689656">
to know the relatively rare letter relationships ch/,J
and ae/ǫ, and to balance their unlikelihood against
the probability of the correct transliteration being an
actual English name.
The search for the optimal English transliteration
E� for a given foreign name F is referred to as de-
coding. An efficient approach to decoding is dy-
namic programming, in which solutions to subprob-
lems are maintained in a table and used to build up
the global solution in a bottom-up approach. Dy-
namic programming approaches are optimal as long
as the dynamic programming invariant assumption
holds. This assumption states that if the optimal path
through a graph happens to go through state q, then
this optimal path must include the best path up to and
including q. Thus, once an optimal path to state q is
found, all other paths to q can be eliminated from
the search. The validity of this assumption depends
on the state space used to define the model. Typ-
ically, for problems related to word comparison, a
dynamic programming approach will define states as
positions in the source and target words. As will be
shown later, however, not all models can be repre-
sented with such a state space.
The phrase-based approach developed for statis-
tical machine translation (Koehn et al., 2003) is
designed to overcome the restrictions on many-to-
many mappings in word-based translation models.
This approach is based on learning correspondences
between phrases, rather than words. Phrases are
generated on the basis of a word-to-word alignment,
with the constraint that no words within the phrase
pair are linked to words outside the phrase pair.
In this paper, we propose to apply phrase-based
translation methods to the task of machine translit-
eration, in an approach we refer to as substring-
based transliteration. We consider two implemen-
tations of these models. The first is an adaptation
of the monotone search algorithm outlined in (Zens
and Ney, 2004).The second encodes the substring-
based transliteration model as a transducer. The re-
sults of experiments on Arabic-to-English transliter-
ation show that the substring-based transducer out-
performs a state-of-the-art letter-based transducer,
while at the same time being orders of magnitude
smaller and faster.
The remainder of the paper is organized as fol-
lows. Section 2 discusses previous approaches
to machine transliteration. Section 3 presents the
letter-based transducer approach to Arabic-English
transliteration proposed in (Al-Onaizan and Knight,
2002), which we use as the main point of com-
parison for our substring-based models. Section 4
presents our substring-based approaches to translit-
eration. In Section 5, we outline the experiments
used to evaluate the models and present their results.
Finally, Section 6 contains our overall impressions
and conclusions.
</bodyText>
<sectionHeader confidence="0.995553" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999742085714286">
Arababi et al. (1994) propose to model forward
transliteration through a combination of neural net
and expert systems. Their main task was to vow-
elize the Arabic names as a preprocessing step for
transliteration. Their method is Arabic-specific and
requires that the Arabic names have a regular pattern
of vowelization.
Knight and Graehl (1998) model the translitera-
tion of Japanese syllabic katakana script into En-
glish with a sequence of finite-state transducers.
After performing a conversion of the English and
katakana sequences to their phonetic representa-
tions, the correspondences between the English and
Japanese phonemes are learned with the expectation
maximization (EM) algorithm. Stalls and Knight
(1998) adapt this approach to Arabic, with the mod-
ification that the English phonemes are mapped di-
rectly to Arabic letters. Al-Onaizan and Knight
(2002) find that a model mapping directly from En-
glish to Arabic letters outperforms the phoneme-to-
letter model.
AbdulJaleel and Larkey (2003) model forward
transliteration from Arabic to English by treating
the words as sentences and using a statistical word
alignment model to align the letters. They select
common English n-grams based on cases when the
alignment links an Arabic letter to several English
letters, and consider these n-grams as single letters
for the purpose of training. The English translitera-
tions are produced using probabilities, learned from
the training data, for the mappings between Arabic
letters and English letters/n-grams.
Li et al. (2004) propose a letter-to-letter n-gram
transliteration model for Chinese-English transliter-
ation in an attempt to allow for the encoding of more
</bodyText>
<page confidence="0.998145">
945
</page>
<bodyText confidence="0.999845166666667">
contextual information. The model isolates individ-
ual mapping operations between training pairs, and
then learns n-gram probabilities for sequences of
these mapping operations. Ekbal et al. (2006) adapt
this model to the transliteration of names from Ben-
gali to English.
</bodyText>
<sectionHeader confidence="0.996465" genericHeader="method">
3 Letter-based Transliteration
</sectionHeader>
<bodyText confidence="0.999989419354839">
The main point of comparison for the evaluation
of our substring-based models of transliteration is
the letter-based transducer proposed by (Al-Onaizan
and Knight, 2002). Their model is a composition
of a transliteration transducer and a language trans-
ducer. Mappings in the transliteration transducer are
defined between 1-3 English letters and 0-2 Arabic
letters, and their probabilities are learned by EM.
The transliteration transducer is split into three states
to allow mapping probabilities to be learned sepa-
rately for letters at the beginning, middle and end of
a word. Unlike the transducers proposed in (Stalls
and Knight, 1998) and (Knight and Graehl, 1998)
no attempt is made to model the pronunciation of
words. Although names are generally transliterated
based on how they sound, not how they look, the
letter-phoneme conversion itself is problematic as it
is not a trivial task. Many transliterated words are
proper names, whose pronunciation rules may vary
depending on the language of origin (Li et al., 2004).
For example, ch is generally pronounced as either
[1] or [k] in English names, but as [f] in French
names.
The language model is implemented as a finite
state acceptor using a combination of word unigram
and letter trigram probabilities. Essentially, the word
unigram model acts as a probabilistic lookup table,
allowing for words seen in the training data to be
produced with high accuracy, while the letter trigram
probabilities are used model words not seen in the
training data.
</bodyText>
<sectionHeader confidence="0.99788" genericHeader="method">
4 Substring-based Transliteration
</sectionHeader>
<bodyText confidence="0.9999714">
Our substring-based transliteration approach is an
adaptation of phrase-based models of machine trans-
lation to the domain of transliteration. In particular,
our methods are inspired by the monotone search
algorithm proposed in (Zens and Ney, 2004). We
introduce two models of substring-based translitera-
tion: the Viterbi substring decoder and the substring-
based transducer. Table 1 presents a comparison of
the substring-based models to the letter-based model
discussed in Section 3.
</bodyText>
<subsectionHeader confidence="0.99783">
4.1 The Monotone Search Algorithm
</subsectionHeader>
<bodyText confidence="0.999968393939394">
Zens and Ney (2004) propose a linear-time decoding
algorithm for phrase-based machine translation. The
algorithm requires that the translation of phrases be
sequential, disallowing any phrase reordering in the
translation.
Starting from a word-based alignment for each
pair of sentences, the training for the algorithm ac-
cepts all contiguous bilingual phrase pairs (up to a
predetermined maximum length) whose words are
only aligned with each other (Koehn et al., 2003).
The probabilities P(�f|e) for each foreign phrase f
and English phrase e� are calculated on the basis
of counts gleaned from a bitext. Since the count-
ing process is much simpler than trying to learn the
phrases with EM, the maximum phrase length can be
made arbitrarily long with minimal jumps in com-
plexity. This allows the model to actually encode
contextual information into the translation model in-
stead of leaving it completely to the language model.
There are no null (ǫ) phrases so the model does not
handle insertions or deletions explicitly. They can be
handled implicitly, however, by including inserted or
deleted words as members of a larger phrase.
Decoding in the monotone search algorithm is
performed with a Viterbi dynamic programming ap-
proach. For a foreign sentence of length J and a
phrase length maximum of M, a table is filled with a
row j for each position in the input foreign sentence,
representing a translation sequence ending at that
foreign word, and each column e represents possi-
ble final English words for that translation sequence.
Each entry in the table Q is filled according to the
following recursion:
</bodyText>
<equation confidence="0.995010166666667">
Q(0, $) = 1
Q(j, e) = max
e′,&amp;, �
f
Q(J + 1, $) = max Q(J, e′)P($|e′)
e′
</equation>
<bodyText confidence="0.997959333333333">
where f is a foreign phrase beginning at j′ + 1, end-
ing at j and consisting of up to M words. The ‘$’
symbol is the sentence boundary marker.
</bodyText>
<equation confidence="0.492846">
P( P�e)P(�e|e′)Q(j′, e′)
</equation>
<page confidence="0.704627">
946
</page>
<table confidence="0.999797666666667">
Letter Transducer Viterbi Substring Substring Transducer
Model Type Transducer Dynamic Programming Transducer
Transliteration Model Letter Substring Substring
Language Model Word/Letter Substring/Letter Word/Letter
Null Symbols Yes No No
Alignments All Most Probable Most Probable
</table>
<tableCaption confidence="0.99997">
Table 1: Comparison of statistical transliteration models.
</tableCaption>
<bodyText confidence="0.999857666666667">
In the above recursion, the language model is
represented as P(˜e|e′), the probability of the En-
glish phrase given the previous English word. Be-
cause of data sparseness issues in the context of
word phrases, the actual implementation approxi-
mates this probability using word n-grams.
</bodyText>
<subsectionHeader confidence="0.963268">
4.2 Viterbi Substring Decoder
</subsectionHeader>
<bodyText confidence="0.998407966666667">
We propose to adapt the monotone search algorithm
to the domain of transliteration by substituting let-
ters and substrings for the words and phrases of the
original model. There are, in fact, strong indica-
tions that the monotone search algorithm is better
suited to transliteration than it is to translation. Un-
like machine translation, where the constraint on re-
ordering required by monotone search is frequently
violated, transliteration is an inherently sequential
process. Also, the sparsity issue in training the lan-
guage model is much less pronounced, allowing us
to model P(˜e|e′) directly.
In order to train the model, we extract the one-
to-one Viterbi alignment of a training pair from a
stochastic transducer based on the model outlined
in (Ristad and Yianilos, 1998). Substrings are then
generated by iteratively appending adjacent links or
unlinked letters to the one-to-one links of the align-
ment. For example, assuming a maximum substring
length of 2, the &lt;r,,&gt; link in the alignment pre-
sented in Figure 1 would participate in the following
substring pairs: &lt;r,,&gt;, &lt;ur,,&gt;, and &lt;ra,1&gt;.
The fact that the Viterbi substring decoder em-
ploys a dynamic programming search through the
source/target letter state space described in Section 1
renders the use of a word unigram language model
impossible. This is due to the fact that alternate
paths to a given source/target letter pair are being
eliminated as the search proceeds. For example,
suppose the Viterbi substring decoder were given the
</bodyText>
<figureCaption confidence="0.830438333333333">
Figure 1: A one-to-one alignment of Mourad and
�. For clarity the Arabic name is written left to
right.
</figureCaption>
<bodyText confidence="0.999922">
Arabic stringf�� and there are two valid English
names in the language model, Karim (the correct
transliteration of the input) and Kristine (the Arabic
transliteration of which would be���������). The op-
timal path up to the second letter might go through
&lt;J,k&gt;, &lt;,,r&gt;. At this point, it is transliterating into
the name Kristine, but as soon as it hits the third let-
ter (S), it is clear that this is the incorrect choice.
In order to recover from the error, the search would
have to backtrack to the beginning and return to state
&lt;,,r&gt; from a different path, but this is an impos-
sibility since all other paths to that state have been
eliminated from the search.
</bodyText>
<subsectionHeader confidence="0.998298">
4.3 Substring-based Transducer
</subsectionHeader>
<bodyText confidence="0.999952692307692">
The major advantage the letter-based transducer pre-
sented in Section 3 has over the Viterbi substring de-
coder is its word unigram language model, which
allows it to reproduce words seen in the training
data with high accuracy. On the other hand, the
Viterbi substring decoder is able to encode con-
textual information in the transliteration model be-
cause of its ability to consider larger many-to-many
mappings. In a novel approach presented here, we
propose a substring-based transducer that draws on
both advantages. The substring transliteration model
learned for the Viterbi substring decoder is encoded
as a transducer, thus allowing it to use a word uni-
</bodyText>
<page confidence="0.955813">
947
</page>
<figureCaption confidence="0.657162870967742">
gram language model. Our model, which we refer 5.1 Data
to as the substring-based transducer, has several ad-
vantages over the previously presented models.
• The substring-based transducer can be com-
posed with a word unigram language model, al-
lowing it to transliterate names seen in training
for the language model with greater accuracy.
• Longer many-to-many mappings enable the
transducer to encode contextual information
into the transliteration model. Compared to the
letter-based transducer, it allows for the gener-
ation of longer well-formed substrings (or po-
tentially even entire words).
• The letter-based transducer considers all possi-
ble alignments of the training examples, mean-
ing that many low-probability mappings are en-
coded into the model. This issue is even more
pronounced in cases where the desired translit-
eration is not in the word unigram model, and
it is guided by the weaker letter trigram model.
The substring-based transducer can eliminate
many of these low-probability mappings be-
cause of its commitment to a single high-
probability one-to-one alignment during train-
ing.
• A major computational advantage this model
has over the letter-based transducer is the fact
that null characters (ǫ) are not encoded explic-
itly. Since the Arabic input to the letter-based
transducer could contain an arbitrary number
of nulls, the potential number of output strings
from the transliteration transducer is infinite.
Thus, the composition with the language trans-
ducer must be done in such a way that there
is a valid path for all of the strings output by
the transliteration transducer that have a pos-
itive probability in the language model. This
leads to prohibitively large transducers. On the
other hand, the substring-based transducer han-
dles nulls implicitly (e.g. the mapping ke:,,im-
plicitly represents e:ǫ after a k), so the trans-
ducer itself is not required to deal with them.
5 Experiments
In this section, we describe the evaluation of our
models on the task of Arabic-to-English transliter-
ation.
948
For our experiments, we required bilingual name
pairs for testing and development data, as well as
for the training of the transliteration models. To train
the language models, we simply needed a list of En-
glish names. Bilingual data was extracted from the
Arabic-English Parallel News part 1 (approx. 2.5M
words) and the Arabic Treebank Part 1-10k word
English Translation. Both bitexts contain Arabic
news articles and their English translations. The En-
glish name list for the language model training was
extracted from the English-Arabic Treebank v1.0
(approx. 52k words)1. The language model training
set consisted of all words labeled as proper names
in this corpus along with all the English names in
the transliteration training set. Any names in any of
the data sets that consisted of multiple words (e.g.
first name/last name pairs) were split and consid-
ered individually. Training data for the translitera-
tion model consisted of 2844 English-Arabic pairs.
The language model was trained on a separate set
of 10991 (4494 unique) English names. The final
test set of 300 English-Arabic transliteration pairs
contained no overlap with the set that was used to
induce the transliteration models.
5.2 Evaluation Methodology
For each of the 300 transliteration pairs in the test
set, the name written in Arabic served as input to the
models, while its English counterpart was consid-
ered a gold standard transliteration for the purpose
of evaluation. Two separate tests were performed on
the test set. In the first, the 300 English words in
the test set were added to the training data for the
language models (the seen test), while in the sec-
ond, all English words in the test set were removed
from the language model’s training data (the unseen
test). Both tests were run on the same set of words
to ensure that variations in performance for seen and
unseen words were solely due to whether or not they
appear in the language model (and not, for exam-
ple, their language of origin). The seen test is sim-
ilar to tests run in (Knight and Graehl, 1998) and
(Stalls and Knight, 1998) where the models could
not produce any words not included in the language
1All corpora are distributed by the Linguistic Data Consor-
tium. Despite the name, the English-Arabic Treebank v1.0 con-
tains only English data.
</figureCaption>
<bodyText confidence="0.999972">
model training data. The models were evaluated on
the seen test set in terms of exact matches to the gold
standard. Because the task of generating transliter-
ations for the unseen test set is much more difficult,
exact match accuracy will not provide a meaningful
metric for comparison. Thus, a softer measure of
performance was required to indicate how close the
generated transliterations are to the gold standard.
We used Levenshtein distance: the number of inser-
tions, deletions and substitutions required to convert
one string into another. We present the results sep-
arately for names of Arabic origin and for those of
non-Arabic origin.
We also performed a third test on words that ap-
pear in both the transliteration and language model
training data. This test was not indicative of the
overall strength of the models but was meant to give
a sense of how much each model depends on its lan-
guage model versus its transliteration model.
</bodyText>
<subsectionHeader confidence="0.996336">
5.3 Setup
</subsectionHeader>
<bodyText confidence="0.9996835">
Five approaches were evaluated on the Arabic-
English transliteration task.
</bodyText>
<listItem confidence="0.999794">
• Baseline: As a baseline for our experiments,
we used a simple deterministic mapping algo-
rithm which maps Arabic letters to the most
likely letter or sequence of letters in English.
• Letter-based Transducer: Mapping proba-
</listItem>
<bodyText confidence="0.8077565">
bilities were learned by running the forward-
backward algorithm until convergence. The
language model is a combination of word un-
igram and letter trigram models and selects a
word unigram or letter trigram modeling of the
English word depending on whichever one as-
signs the highest probability. The letter-based
transducer was implemented in Carmel2.
</bodyText>
<listItem confidence="0.82767775">
• Viterbi Substring Decoder: We experimented
with maximum substring lengths between 3
and 10 on the development set, and found that
a maximum length of 6 was optimal.
• Substring-based Transducer: The substring-
based transducer was also implemented in
Carmel. We found that this model worked best
with a maximum substring length of 4.
</listItem>
<footnote confidence="0.982518333333333">
2Carmel is a finite-state transducer package written by
Jonathan Graehl. It is available at http://www.isi.edu/licensed-
sw/carmel/.
</footnote>
<table confidence="0.9987275">
Method Arabic Non-Arabic All
Baseline 1.9 2.1 2.0
Letter trans. 45.9 64.3 54.7
Viterbi substring 15.9 30.1 22.7
Substring trans. 59.9 81.1 70.0
Human 33.1 40.6 36.7
</table>
<tableCaption confidence="0.8176155">
Table 2: Exact match accuracy percentage on the
seen test set for various methods.
</tableCaption>
<table confidence="0.9998825">
Method Arabic Non-Arabic All
Baseline 2.32 2.80 2.55
Letter trans. 2.46 2.63 2.54
Viterbi substring 1.90 2.13 2.01
Substring trans. 1.92 2.41 2.16
Human 1.24 1.42 1.33
</table>
<tableCaption confidence="0.737636">
Table 3: Average Levenshtein distance on the un-
seen test set for various methods.
</tableCaption>
<listItem confidence="0.513437142857143">
• Human: For the purpose of comparison, we
allowed an independent human subject (fluent
in Arabic, but a native speaker of English) to
perform the same task. The subject was asked
to transliterate the Arabic words in the test set
without any additional context. No additional
resources or collaboration were allowed.
</listItem>
<subsectionHeader confidence="0.92821">
5.4 Results on the Test Set
</subsectionHeader>
<bodyText confidence="0.999284722222223">
Table 2 presents the word accuracy performance of
each transliterator when the test set is available to the
language models. Table 3 shows the average Leven-
shtein distance results when the test set is unavail-
able to the language models. Exact match perfor-
mance by the automated approaches on the unseen
set did not exceed 10.3% (achieved by the Viterbi
substring decoder). Results on the seen test sug-
gest that non-Arabic words (back transliterations)
are easier to transliterate exactly, while results for
the unseen test suggest that errors on Arabic words
(forward transliterations) tend to be closer to the
gold standard.
Overall, our substring-based transducer clearly
outperforms the letter-based transducer. Its per-
formance is better in both tests, but its advantage
is particularly pronounced on words it has seen in
the training data for the language model (the task
</bodyText>
<page confidence="0.996272">
949
</page>
<table confidence="0.999335214285714">
Arabic LBT SBT Correct
1�àAÒ�J«Uthman Uthman Othman
¬Qå~...~
2 3�Iª�¯P Asharf Asharf Ashraf
5�àAÖß��
Rafeet Arafat Refaat
4�éÓAƒ�Istamaday Asuma Usama
�QÊJ�Ó
Erdman Aliman
Iman
6 �•��ððWortch Watch Watch
7 Mellis Mills Mills
8 ø February Firari Ferrari
P�Q��¯
</table>
<tableCaption confidence="0.961056">
Table 4: A sample of the errors made by the letter-
based (LBT) and segment-based (SBT) transducers.
</tableCaption>
<bodyText confidence="0.999962914285714">
for which the letter-based transducer was originally
designed). Since both transducers use exactly the
same language model, the fact that the substring-
based transducer outperforms the letter-based trans-
ducer indicates that it learns a stronger translitera-
tion model.
The Viterbi substring decoder seems to struggle
when it comes to recreating words seen the language
training data, as evidenced by its weak performance
on the seen test. Obviously, its substring/letter bi-
gram language model is no match for the word un-
igram model used by the transducers on this task.
On the other hand, its stronger performance on the
unseen test set suggests that its language model is
stronger than the letter trigram used by the transduc-
ers when it comes to generating completely novel
words.
A sample of the errors made by the letter- and
substring-based transducers is presented in Table 4.
In general, when both models err, the substring-
based transducer tends toward more phonetically
reasonable choices. The most common type of er-
ror is simply correct alternate English spellings of
an Arabic name (error 1). Error 2 is an example of
a learned mapping being misplaced (the deleted a).
Error 3 indicates that the letter-based transducer is
able to avoid these misplaced mappings at the be-
ginning or end of a word because of its three-state
transliteration transducer (i.e. it learns not to allow
vowel deletions at the beginning of a word). Errors
4 and 5 are cases where the letter-based transducer
produced particularly awkward transliterations. Er-
rors 6 and 7 are names that actually appear in the
word unigram model but were missed by the letter-
based transducer, while error 8 is an example of the
</bodyText>
<table confidence="0.99840225">
Method Exact match Avg Lev.
Letter transducer 81.2 0.46
Viterbi substring 83.2 0.24
Substring transducer 94.4 0.09
</table>
<tableCaption confidence="0.9209255">
Table 5: Results for testing on the transliteration
training set.
</tableCaption>
<bodyText confidence="0.99148575">
letter-based transducer incorrectly choosing a name
from the word unigram model. As discussed in Sec-
tion 4.3, this is likely due to mappings learned from
low-probability alignments.
</bodyText>
<subsectionHeader confidence="0.543932">
5.5 Results on the Training Set
</subsectionHeader>
<bodyText confidence="0.999991518518519">
The substring-based approaches encode a great deal
of contextual information into the transliteration
model. In order to assess how much the perfor-
mance of each approach depends on its language
model versus its transliteration model, we tested the
three statistical models on the set of 2844 names
seen in both the transliteration and language model
training. The results of this experiment are pre-
sented in Table 5. The Viterbi substring decoder re-
ceives the biggest boost, outperforming the letter-
based transducer, which indicates that its strength
lies mainly in its transliteration modeling as opposed
to its language modeling. The substring-based trans-
ducer, however, still outperforms it by a large mar-
gin, achieving near-perfect results. Most of the re-
maining errors can be attributed to names with alter-
nate correct spellings in English.
The results also suggest that the substring-based
transducer practically subsumes a naive “lookup ta-
ble” approach. Although the accuracy achieved is
less than 100%, the substring-based transducer has
the great advantage of being able to handle noise in
the input. In other words, if the spelling of an input
word does not match an Arabic word from the train-
ing data, a lookup table will generate nothing, while
the substring-based transducer could still search for
the correct transliteration.
</bodyText>
<subsectionHeader confidence="0.93325">
5.6 Computational Considerations
</subsectionHeader>
<bodyText confidence="0.98708525">
Another point of comparison between the models
is complexity. The letter-based transducer encodes
56144 mappings while the substring-based trans-
ducer encodes 13948, but as shown in Table 6, once
</bodyText>
<page confidence="0.988356">
950
</page>
<table confidence="0.992084333333333">
Method Size (states/arcs)
Letter transducer 86309/547184
Substring transducer 759/2131
</table>
<tableCaption confidence="0.8140855">
Table 6: Transducer sizes for composition with the
word�s.I-(Helmy).
</tableCaption>
<table confidence="0.99862425">
Method Time
Letter transducer 5h52min
Viterbi substring 3 sec
Substring transducer 11 sec
</table>
<tableCaption confidence="0.999794">
Table 7: Running times for the 300 word test set.
</tableCaption>
<bodyText confidence="0.999944153846154">
the transducers are fully composed, the difference
becomes even more pronounced. As discussed in
Section 4.3, the reason for the size explosion fac-
tor in the letter-based transducer is the possibility of
null characters in the input word.
The running times for the statistical approaches
on the 300 word test set are presented in Table 7.
The huge computational advantage of the substring-
based approach makes it a much more attractive op-
tion for any real-world application. Tests were per-
formed on an AMD Athlon 64 3500+ machine with
2GB of memory running Red Hat Enterprise Linux
release 4.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978769230769">
In this paper, we presented a new substring-based
approach to modeling transliteration inspired by
phrase-based models of machine translation. We
tested both dynamic programming and finite-state
transducer implementations, the latter of which en-
abled us to use a word unigram language model to
improve the accuracy of generated transliterations.
The results of evaluation on the task of Arabic-
English transliteration indicate that the substring-
based approach not only improves performance over
a state-of-the-art letter-based model, but also leads
to major gains in efficiency. Since no language-
specific information was encoded directly into the
models, they can also be used for transliteration be-
tween other language pairs.
In the future, we plan to consider more com-
plex language models in order to improve the re-
sults on unseen words, which should certainly be
feasible for the substring-based transducer because
of its efficient memory usage. Another feature of the
substring-based transducer that we have not yet ex-
plored is its ability to easily produce an n-best list of
transliterations. We plan to investigate whether us-
ing methods like discriminative reranking (Och and
Ney, 2002) on such an n-best list could improve per-
formance.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995424">
We would like to thank Colin Cherry and the other
members of the NLP research group at the Univer-
sity of Alberta for their helpful comments. This re-
search was supported by the Natural Sciences and
Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999723181818182">
N. AbdulJaleel and L. S. Larkey. 2003. Statistical
transliteration for English-Arabic cross language in-
formation retrieval. In CIKM, pages 139–146.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in Arabic text. In ACL Workshop on
Comp. Approaches to Semitic Languages.
M. Arababi, S.M. Fischthal, V.C. Cheng, and E. Bart.
1994. Algorithmns for Arabic name transliteration.
IBMJournal ofResearch and Development, 38(2).
A. Ekbal, S.K. Naskar, and S. Bandyopadhyay. 2006.
A modified joint source-channel model for transliter-
ation. In COLING/ACL Poster Sessions, pages 191–
198.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599–612.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL-HLT, pages 48–
54.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In ACL, pages 159–
166.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL, pages 295–302.
E. S. Ristad and P. N. Yianilos. 1998. Learning string-
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(5):522–532.
B. Stalls and K. Knight. 1998. Translating names and
technical terms in Arabic text. In COLING/ACL Work-
shop on Comp. Approaches to Semitic Languages.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In HLT-NAACL,
pages 257–264.
</reference>
<page confidence="0.998244">
951
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957291">
<title confidence="0.99906">Substring-Based Transliteration</title>
<author confidence="0.999533">Sherif Kondrak</author>
<affiliation confidence="0.9999765">Department of Computing Science University of Alberta</affiliation>
<address confidence="0.994321">Edmonton, Alberta, Canada T6G 2E8</address>
<abstract confidence="0.996499153846154">Transliteration is the task of converting a word from one alphabetic script to another. We present a novel, substring-based approach to transliteration, inspired by phrasebased models of machine translation. We investigate two implementations of substringbased transliteration: a dynamic programming algorithm, and a finite-state transducer. We show that our substring-based transducer not only outperforms a state-of-the-art letterbased approach by a significant margin, but is also orders of magnitude faster.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N AbdulJaleel</author>
<author>L S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>In CIKM,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="7250" citStr="AbdulJaleel and Larkey (2003)" startWordPosition="1116" endWordPosition="1119">of Japanese syllabic katakana script into English with a sequence of finite-state transducers. After performing a conversion of the English and katakana sequences to their phonetic representations, the correspondences between the English and Japanese phonemes are learned with the expectation maximization (EM) algorithm. Stalls and Knight (1998) adapt this approach to Arabic, with the modification that the English phonemes are mapped directly to Arabic letters. Al-Onaizan and Knight (2002) find that a model mapping directly from English to Arabic letters outperforms the phoneme-toletter model. AbdulJaleel and Larkey (2003) model forward transliteration from Arabic to English by treating the words as sentences and using a statistical word alignment model to align the letters. They select common English n-grams based on cases when the alignment links an Arabic letter to several English letters, and consider these n-grams as single letters for the purpose of training. The English transliterations are produced using probabilities, learned from the training data, for the mappings between Arabic letters and English letters/n-grams. Li et al. (2004) propose a letter-to-letter n-gram transliteration model for Chinese-E</context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>N. AbdulJaleel and L. S. Larkey. 2003. Statistical transliteration for English-Arabic cross language information retrieval. In CIKM, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Comp. Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="5920" citStr="Al-Onaizan and Knight, 2002" startWordPosition="913" endWordPosition="916">ion of the monotone search algorithm outlined in (Zens and Ney, 2004).The second encodes the substringbased transliteration model as a transducer. The results of experiments on Arabic-to-English transliteration show that the substring-based transducer outperforms a state-of-the-art letter-based transducer, while at the same time being orders of magnitude smaller and faster. The remainder of the paper is organized as follows. Section 2 discusses previous approaches to machine transliteration. Section 3 presents the letter-based transducer approach to Arabic-English transliteration proposed in (Al-Onaizan and Knight, 2002), which we use as the main point of comparison for our substring-based models. Section 4 presents our substring-based approaches to transliteration. In Section 5, we outline the experiments used to evaluate the models and present their results. Finally, Section 6 contains our overall impressions and conclusions. 2 Previous Work Arababi et al. (1994) propose to model forward transliteration through a combination of neural net and expert systems. Their main task was to vowelize the Arabic names as a preprocessing step for transliteration. Their method is Arabic-specific and requires that the Ara</context>
<context position="8396" citStr="Al-Onaizan and Knight, 2002" startWordPosition="1285" endWordPosition="1288">t al. (2004) propose a letter-to-letter n-gram transliteration model for Chinese-English transliteration in an attempt to allow for the encoding of more 945 contextual information. The model isolates individual mapping operations between training pairs, and then learns n-gram probabilities for sequences of these mapping operations. Ekbal et al. (2006) adapt this model to the transliteration of names from Bengali to English. 3 Letter-based Transliteration The main point of comparison for the evaluation of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan and Knight, 2002). Their model is a composition of a transliteration transducer and a language transducer. Mappings in the transliteration transducer are defined between 1-3 English letters and 0-2 Arabic letters, and their probabilities are learned by EM. The transliteration transducer is split into three states to allow mapping probabilities to be learned separately for letters at the beginning, middle and end of a word. Unlike the transducers proposed in (Stalls and Knight, 1998) and (Knight and Graehl, 1998) no attempt is made to model the pronunciation of words. Although names are generally transliterated</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Y. Al-Onaizan and K. Knight. 2002. Machine transliteration of names in Arabic text. In ACL Workshop on Comp. Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arababi</author>
<author>S M Fischthal</author>
<author>V C Cheng</author>
<author>E Bart</author>
</authors>
<title>Algorithmns for Arabic name transliteration. IBMJournal ofResearch and Development,</title>
<date>1994</date>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="6271" citStr="Arababi et al. (1994)" startWordPosition="968" endWordPosition="971">maller and faster. The remainder of the paper is organized as follows. Section 2 discusses previous approaches to machine transliteration. Section 3 presents the letter-based transducer approach to Arabic-English transliteration proposed in (Al-Onaizan and Knight, 2002), which we use as the main point of comparison for our substring-based models. Section 4 presents our substring-based approaches to transliteration. In Section 5, we outline the experiments used to evaluate the models and present their results. Finally, Section 6 contains our overall impressions and conclusions. 2 Previous Work Arababi et al. (1994) propose to model forward transliteration through a combination of neural net and expert systems. Their main task was to vowelize the Arabic names as a preprocessing step for transliteration. Their method is Arabic-specific and requires that the Arabic names have a regular pattern of vowelization. Knight and Graehl (1998) model the transliteration of Japanese syllabic katakana script into English with a sequence of finite-state transducers. After performing a conversion of the English and katakana sequences to their phonetic representations, the correspondences between the English and Japanese</context>
</contexts>
<marker>Arababi, Fischthal, Cheng, Bart, 1994</marker>
<rawString>M. Arababi, S.M. Fischthal, V.C. Cheng, and E. Bart. 1994. Algorithmns for Arabic name transliteration. IBMJournal ofResearch and Development, 38(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ekbal</author>
<author>S K Naskar</author>
<author>S Bandyopadhyay</author>
</authors>
<title>A modified joint source-channel model for transliteration.</title>
<date>2006</date>
<booktitle>In COLING/ACL Poster Sessions,</booktitle>
<pages>191--198</pages>
<contexts>
<context position="8121" citStr="Ekbal et al. (2006)" startWordPosition="1245" endWordPosition="1248">everal English letters, and consider these n-grams as single letters for the purpose of training. The English transliterations are produced using probabilities, learned from the training data, for the mappings between Arabic letters and English letters/n-grams. Li et al. (2004) propose a letter-to-letter n-gram transliteration model for Chinese-English transliteration in an attempt to allow for the encoding of more 945 contextual information. The model isolates individual mapping operations between training pairs, and then learns n-gram probabilities for sequences of these mapping operations. Ekbal et al. (2006) adapt this model to the transliteration of names from Bengali to English. 3 Letter-based Transliteration The main point of comparison for the evaluation of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan and Knight, 2002). Their model is a composition of a transliteration transducer and a language transducer. Mappings in the transliteration transducer are defined between 1-3 English letters and 0-2 Arabic letters, and their probabilities are learned by EM. The transliteration transducer is split into three states to allow mapping probabilit</context>
</contexts>
<marker>Ekbal, Naskar, Bandyopadhyay, 2006</marker>
<rawString>A. Ekbal, S.K. Naskar, and S. Bandyopadhyay. 2006. A modified joint source-channel model for transliteration. In COLING/ACL Poster Sessions, pages 191– 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="6594" citStr="Knight and Graehl (1998)" startWordPosition="1018" endWordPosition="1021"> our substring-based models. Section 4 presents our substring-based approaches to transliteration. In Section 5, we outline the experiments used to evaluate the models and present their results. Finally, Section 6 contains our overall impressions and conclusions. 2 Previous Work Arababi et al. (1994) propose to model forward transliteration through a combination of neural net and expert systems. Their main task was to vowelize the Arabic names as a preprocessing step for transliteration. Their method is Arabic-specific and requires that the Arabic names have a regular pattern of vowelization. Knight and Graehl (1998) model the transliteration of Japanese syllabic katakana script into English with a sequence of finite-state transducers. After performing a conversion of the English and katakana sequences to their phonetic representations, the correspondences between the English and Japanese phonemes are learned with the expectation maximization (EM) algorithm. Stalls and Knight (1998) adapt this approach to Arabic, with the modification that the English phonemes are mapped directly to Arabic letters. Al-Onaizan and Knight (2002) find that a model mapping directly from English to Arabic letters outperforms t</context>
<context position="8896" citStr="Knight and Graehl, 1998" startWordPosition="1363" endWordPosition="1366">on of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan and Knight, 2002). Their model is a composition of a transliteration transducer and a language transducer. Mappings in the transliteration transducer are defined between 1-3 English letters and 0-2 Arabic letters, and their probabilities are learned by EM. The transliteration transducer is split into three states to allow mapping probabilities to be learned separately for letters at the beginning, middle and end of a word. Unlike the transducers proposed in (Stalls and Knight, 1998) and (Knight and Graehl, 1998) no attempt is made to model the pronunciation of words. Although names are generally transliterated based on how they sound, not how they look, the letter-phoneme conversion itself is problematic as it is not a trivial task. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin (Li et al., 2004). For example, ch is generally pronounced as either [1] or [k] in English names, but as [f] in French names. The language model is implemented as a finite state acceptor using a combination of word unigram and letter trigram probabilities. Es</context>
<context position="19835" citStr="Knight and Graehl, 1998" startWordPosition="3136" endWordPosition="3139">e purpose of evaluation. Two separate tests were performed on the test set. In the first, the 300 English words in the test set were added to the training data for the language models (the seen test), while in the second, all English words in the test set were removed from the language model’s training data (the unseen test). Both tests were run on the same set of words to ensure that variations in performance for seen and unseen words were solely due to whether or not they appear in the language model (and not, for example, their language of origin). The seen test is similar to tests run in (Knight and Graehl, 1998) and (Stalls and Knight, 1998) where the models could not produce any words not included in the language 1All corpora are distributed by the Linguistic Data Consortium. Despite the name, the English-Arabic Treebank v1.0 contains only English data. model training data. The models were evaluated on the seen test set in terms of exact matches to the gold standard. Because the task of generating transliterations for the unseen test set is much more difficult, exact match accuracy will not provide a meaningful metric for comparison. Thus, a softer measure of performance was required to indicate how</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL-HLT,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="4697" citStr="Koehn et al., 2003" startWordPosition="732" endWordPosition="735">ns to go through state q, then this optimal path must include the best path up to and including q. Thus, once an optimal path to state q is found, all other paths to q can be eliminated from the search. The validity of this assumption depends on the state space used to define the model. Typically, for problems related to word comparison, a dynamic programming approach will define states as positions in the source and target words. As will be shown later, however, not all models can be represented with such a state space. The phrase-based approach developed for statistical machine translation (Koehn et al., 2003) is designed to overcome the restrictions on many-tomany mappings in word-based translation models. This approach is based on learning correspondences between phrases, rather than words. Phrases are generated on the basis of a word-to-word alignment, with the constraint that no words within the phrase pair are linked to words outside the phrase pair. In this paper, we propose to apply phrase-based translation methods to the task of machine transliteration, in an approach we refer to as substringbased transliteration. We consider two implementations of these models. The first is an adaptation o</context>
<context position="10760" citStr="Koehn et al., 2003" startWordPosition="1649" endWordPosition="1652">ucer. Table 1 presents a comparison of the substring-based models to the letter-based model discussed in Section 3. 4.1 The Monotone Search Algorithm Zens and Ney (2004) propose a linear-time decoding algorithm for phrase-based machine translation. The algorithm requires that the translation of phrases be sequential, disallowing any phrase reordering in the translation. Starting from a word-based alignment for each pair of sentences, the training for the algorithm accepts all contiguous bilingual phrase pairs (up to a predetermined maximum length) whose words are only aligned with each other (Koehn et al., 2003). The probabilities P(�f|e) for each foreign phrase f and English phrase e� are calculated on the basis of counts gleaned from a bitext. Since the counting process is much simpler than trying to learn the phrases with EM, the maximum phrase length can be made arbitrarily long with minimal jumps in complexity. This allows the model to actually encode contextual information into the translation model instead of leaving it completely to the language model. There are no null (ǫ) phrases so the model does not handle insertions or deletions explicitly. They can be handled implicitly, however, by inc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In NAACL-HLT, pages 48– 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>M Zhang</author>
<author>J Su</author>
</authors>
<title>A joint source-channel model for machine transliteration. In</title>
<date>2004</date>
<booktitle>ACL,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="7780" citStr="Li et al. (2004)" startWordPosition="1197" endWordPosition="1200">o Arabic letters outperforms the phoneme-toletter model. AbdulJaleel and Larkey (2003) model forward transliteration from Arabic to English by treating the words as sentences and using a statistical word alignment model to align the letters. They select common English n-grams based on cases when the alignment links an Arabic letter to several English letters, and consider these n-grams as single letters for the purpose of training. The English transliterations are produced using probabilities, learned from the training data, for the mappings between Arabic letters and English letters/n-grams. Li et al. (2004) propose a letter-to-letter n-gram transliteration model for Chinese-English transliteration in an attempt to allow for the encoding of more 945 contextual information. The model isolates individual mapping operations between training pairs, and then learns n-gram probabilities for sequences of these mapping operations. Ekbal et al. (2006) adapt this model to the transliteration of names from Bengali to English. 3 Letter-based Transliteration The main point of comparison for the evaluation of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan a</context>
<context position="9254" citStr="Li et al., 2004" startWordPosition="1421" endWordPosition="1424">e transliteration transducer is split into three states to allow mapping probabilities to be learned separately for letters at the beginning, middle and end of a word. Unlike the transducers proposed in (Stalls and Knight, 1998) and (Knight and Graehl, 1998) no attempt is made to model the pronunciation of words. Although names are generally transliterated based on how they sound, not how they look, the letter-phoneme conversion itself is problematic as it is not a trivial task. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin (Li et al., 2004). For example, ch is generally pronounced as either [1] or [k] in English names, but as [f] in French names. The language model is implemented as a finite state acceptor using a combination of word unigram and letter trigram probabilities. Essentially, the word unigram model acts as a probabilistic lookup table, allowing for words seen in the training data to be produced with high accuracy, while the letter trigram probabilities are used model words not seen in the training data. 4 Substring-based Transliteration Our substring-based transliteration approach is an adaptation of phrase-based mod</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>H. Li, M. Zhang, and J. Su. 2004. A joint source-channel model for machine transliteration. In ACL, pages 159– 166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>295--302</pages>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
<author>P N Yianilos</author>
</authors>
<title>Learning stringedit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="13574" citStr="Ristad and Yianilos, 1998" startWordPosition="2108" endWordPosition="2111">f the original model. There are, in fact, strong indications that the monotone search algorithm is better suited to transliteration than it is to translation. Unlike machine translation, where the constraint on reordering required by monotone search is frequently violated, transliteration is an inherently sequential process. Also, the sparsity issue in training the language model is much less pronounced, allowing us to model P(˜e|e′) directly. In order to train the model, we extract the oneto-one Viterbi alignment of a training pair from a stochastic transducer based on the model outlined in (Ristad and Yianilos, 1998). Substrings are then generated by iteratively appending adjacent links or unlinked letters to the one-to-one links of the alignment. For example, assuming a maximum substring length of 2, the &lt;r,,&gt; link in the alignment presented in Figure 1 would participate in the following substring pairs: &lt;r,,&gt;, &lt;ur,,&gt;, and &lt;ra,1&gt;. The fact that the Viterbi substring decoder employs a dynamic programming search through the source/target letter state space described in Section 1 renders the use of a word unigram language model impossible. This is due to the fact that alternate paths to a given source/targe</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>E. S. Ristad and P. N. Yianilos. 1998. Learning stringedit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Stalls</author>
<author>K Knight</author>
</authors>
<title>Translating names and technical terms in Arabic text.</title>
<date>1998</date>
<booktitle>In COLING/ACL Workshop on Comp. Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="6967" citStr="Stalls and Knight (1998)" startWordPosition="1071" endWordPosition="1074">l net and expert systems. Their main task was to vowelize the Arabic names as a preprocessing step for transliteration. Their method is Arabic-specific and requires that the Arabic names have a regular pattern of vowelization. Knight and Graehl (1998) model the transliteration of Japanese syllabic katakana script into English with a sequence of finite-state transducers. After performing a conversion of the English and katakana sequences to their phonetic representations, the correspondences between the English and Japanese phonemes are learned with the expectation maximization (EM) algorithm. Stalls and Knight (1998) adapt this approach to Arabic, with the modification that the English phonemes are mapped directly to Arabic letters. Al-Onaizan and Knight (2002) find that a model mapping directly from English to Arabic letters outperforms the phoneme-toletter model. AbdulJaleel and Larkey (2003) model forward transliteration from Arabic to English by treating the words as sentences and using a statistical word alignment model to align the letters. They select common English n-grams based on cases when the alignment links an Arabic letter to several English letters, and consider these n-grams as single lett</context>
<context position="8866" citStr="Stalls and Knight, 1998" startWordPosition="1358" endWordPosition="1361">of comparison for the evaluation of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan and Knight, 2002). Their model is a composition of a transliteration transducer and a language transducer. Mappings in the transliteration transducer are defined between 1-3 English letters and 0-2 Arabic letters, and their probabilities are learned by EM. The transliteration transducer is split into three states to allow mapping probabilities to be learned separately for letters at the beginning, middle and end of a word. Unlike the transducers proposed in (Stalls and Knight, 1998) and (Knight and Graehl, 1998) no attempt is made to model the pronunciation of words. Although names are generally transliterated based on how they sound, not how they look, the letter-phoneme conversion itself is problematic as it is not a trivial task. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin (Li et al., 2004). For example, ch is generally pronounced as either [1] or [k] in English names, but as [f] in French names. The language model is implemented as a finite state acceptor using a combination of word unigram and le</context>
<context position="19865" citStr="Stalls and Knight, 1998" startWordPosition="3141" endWordPosition="3144">eparate tests were performed on the test set. In the first, the 300 English words in the test set were added to the training data for the language models (the seen test), while in the second, all English words in the test set were removed from the language model’s training data (the unseen test). Both tests were run on the same set of words to ensure that variations in performance for seen and unseen words were solely due to whether or not they appear in the language model (and not, for example, their language of origin). The seen test is similar to tests run in (Knight and Graehl, 1998) and (Stalls and Knight, 1998) where the models could not produce any words not included in the language 1All corpora are distributed by the Linguistic Data Consortium. Despite the name, the English-Arabic Treebank v1.0 contains only English data. model training data. The models were evaluated on the seen test set in terms of exact matches to the gold standard. Because the task of generating transliterations for the unseen test set is much more difficult, exact match accuracy will not provide a meaningful metric for comparison. Thus, a softer measure of performance was required to indicate how close the generated translite</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>B. Stalls and K. Knight. 1998. Translating names and technical terms in Arabic text. In COLING/ACL Workshop on Comp. Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrasebased statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="5361" citStr="Zens and Ney, 2004" startWordPosition="837" endWordPosition="840">many-tomany mappings in word-based translation models. This approach is based on learning correspondences between phrases, rather than words. Phrases are generated on the basis of a word-to-word alignment, with the constraint that no words within the phrase pair are linked to words outside the phrase pair. In this paper, we propose to apply phrase-based translation methods to the task of machine transliteration, in an approach we refer to as substringbased transliteration. We consider two implementations of these models. The first is an adaptation of the monotone search algorithm outlined in (Zens and Ney, 2004).The second encodes the substringbased transliteration model as a transducer. The results of experiments on Arabic-to-English transliteration show that the substring-based transducer outperforms a state-of-the-art letter-based transducer, while at the same time being orders of magnitude smaller and faster. The remainder of the paper is organized as follows. Section 2 discusses previous approaches to machine transliteration. Section 3 presents the letter-based transducer approach to Arabic-English transliteration proposed in (Al-Onaizan and Knight, 2002), which we use as the main point of compa</context>
<context position="10020" citStr="Zens and Ney, 2004" startWordPosition="1540" endWordPosition="1543"> finite state acceptor using a combination of word unigram and letter trigram probabilities. Essentially, the word unigram model acts as a probabilistic lookup table, allowing for words seen in the training data to be produced with high accuracy, while the letter trigram probabilities are used model words not seen in the training data. 4 Substring-based Transliteration Our substring-based transliteration approach is an adaptation of phrase-based models of machine translation to the domain of transliteration. In particular, our methods are inspired by the monotone search algorithm proposed in (Zens and Ney, 2004). We introduce two models of substring-based transliteration: the Viterbi substring decoder and the substringbased transducer. Table 1 presents a comparison of the substring-based models to the letter-based model discussed in Section 3. 4.1 The Monotone Search Algorithm Zens and Ney (2004) propose a linear-time decoding algorithm for phrase-based machine translation. The algorithm requires that the translation of phrases be sequential, disallowing any phrase reordering in the translation. Starting from a word-based alignment for each pair of sentences, the training for the algorithm accepts al</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrasebased statistical machine translation. In HLT-NAACL, pages 257–264.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>