<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9980225">
PolyUCOMP-CORE TYPED: Computing Semantic Textual Similarity
using Overlapped Senses
</title>
<author confidence="0.964461">
Jian Xu Qin Lu
</author>
<affiliation confidence="0.9219375">
The Hong Kong Polytechnic University
Department of Computing
</affiliation>
<address confidence="0.57966">
Hung Hom, Kowloon, Hong Kong
</address>
<email confidence="0.993253">
{csjxu, csluqin}@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.998519" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997586933333333">
The Semantic Textual Similarity (STS)
task aims to exam the degree of semantic
equivalence between sentences (Agirre et
al., 2012). This paper presents the work
of the Hong Kong Polytechnic University
(PolyUCOMP) team which has participated
in the STS core and typed tasks of SemEval-
2013. For the STS core task, the PolyUCOMP
system disambiguates words senses using
contexts and then determine sentence
similarity by counting the number of senses
they shared. For the STS typed task, the string
kernel (Lodhi et al., 2002) is used to compute
similarity between two entities to avoid string
variations in entities.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999624375">
Sentence similarity computation plays an important
role in text summarization and social network
applications (Erkan et al., 2004; Jin et al., 2011).
The SemEval 2012 competition initiated a task
targeted at Semantic Textual Similarity (STS)
between sentence pairs (Agirre et al., 2012). Given
a set of sentence pairs, participants are required to
assign to each sentence pair a similarity score.
Because a sentence has only a limited amount of
content words, it is difficult to determine sentence
similarities. To solve this problem, Hatzivassiloglou
et al. (1999) proposed to use linguistic features
as indicators of text similarity to address the
problem of sparse representation of sentences.
Mihalcea et al. (2006) measured sentence similarity
using component words in sentences. Li et al.
</bodyText>
<page confidence="0.969687">
90
</page>
<bodyText confidence="0.999935088235294">
(2006) proposed to incorporate the semantic vector
and word order to calculate sentence similarity.
Biemann et al. (2012) applied the log-linear
regression model by combining the simple string
based measures, for example, word ngrams and
semantic similarity measures, for example, textual
entailment. Similarly, Saric et al. (2012) used a
support vector regression model which incorporates
features computed from sentence pairs. The features
are knowledge- and corpus-based word similarity,
ngram overlaps, WordNet augmented word overlap,
syntactic features and so on. Xu et al. (2012)
combined semantic vectors with skip bigrams to
determine sentence similarity, whereas the skip
bigrams take into the sequential order between
words.
In our approach to the STS task, words in
sentences are assigned with appropriate senses using
their contexts. Sentence similarity is computed by
calculating the number of shared senses in both
sentences since it is reasonable to assume that
similar sentences should have more overlapping
senses. For the STS-TYPED task, variations
might occur in author names, people involved,
time expression and location. Thus, string kernel
is applied to compute similarity between entities
because it can capture variations between entities.
Moreover, for the event similarity in STS-TYPED
task, semantic relatedness between verbs is derived
the WordNet.
The rest of this paper is structured as follows.
Section 2 describes sentence similarity using sense
overlapping and string kernel. Section 3 gives the
performance evaluation. Section 4 is the conclusion.
</bodyText>
<note confidence="0.2174095">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 90–95, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.715043" genericHeader="method">
2 Similarity between Sentences
</sectionHeader>
<bodyText confidence="0.99977075">
Words are used to convey meaning in a sentence.
They are tagged with appropriate senses initially and
then sentence similarity is calculated based on the
number of shared senses.
</bodyText>
<subsectionHeader confidence="0.997552">
2.1 Sense Overlapping
</subsectionHeader>
<bodyText confidence="0.996794272727273">
When comparing word features, we did not compare
their surface equality, but we first conceptualize
these words and then calculate their similarities
based on the hierarchial structure in WordNet. For a
word in a sentence, it will be assigned a WordNet
sense. In this paper, we focus on the Word
Sense Disambiguation (WSD) algorithm taken by
Banerjee and Pederson (2003). They measured the
semantic relatedness between concepts by counting
the shared words in their WordNet glosses.
In WordNet, a word sense is represented by a
synset which has a gloss that defines the concept
that it represents. For example, the words walking,
afoot, ambulate constitute a single synset which has
gloss representations as follows,
walking: the act of traveling by foot
afoot: traveling by foot
ambulate: walk about
To lift the limitations of dictionary glosses which
are fairly short with insufficient vocabulary, we
utilize the glosses of related senses since we assume
that words co-occur in one sentence share related
senses and the more glosses two senses share, the
more similar they are. Therefore, we extract not
only glosses of target synset, but also the glosses
of the hypernym, hyponym, meronym, holonym and
troponym synsets of the target synset to form a
synset context. Finally, we compare the sentence
contexts with different synset contexts to determine
which sense should be assigned to the words.
To disambiguate word senses, a window of
contexts surrounding the the target word is specified
and a set of candidate word senses are extracted for
the content word (noun, verb, adjective) within that
window. Let the current target word index i = 0 that
is, w0, the window size be 2n+1 and −n ≤ i ≤ +n.
Let |wi |be the number of senses for word wi and the
jth sense of wi is si,j, where 1 ≤ j ≤ |wi|. Next is
to assign an appropriate sense k to the target word.
We achieve this by adding together the relatedness
scores calculated by comparing the senses of the
target word and senses of every non-target word
within the window of context. The sense score for
the current target word w0 is defined as,
</bodyText>
<equation confidence="0.947372">
relatedness(s0,k, si,j) (1)
</equation>
<bodyText confidence="0.9997755">
The kth sense which has the biggest sense score
will be chosen as the right sense for the target word
w0. Now remains the question of how to define the
relatedness between two synsets. It is defined as,
</bodyText>
<equation confidence="0.993917833333334">
relatedness(s0,k, si,j) =
score(gloss(s0,k), gloss(si,j))
+score(hype(s0,k), hype(si,j))
+score(hypo(s0,k), hypo(si,j))
+score(hype(s0,k), gloss(si,j))
+score(gloss(s0,k), hype(si,j))
</equation>
<bodyText confidence="0.999919714285714">
In Equation 2, the score function counts the
number of overlapping words between two glosses.
However, if there is a phrasal n-word overlap, then
a score of n2 will be assigned, thus encouraging the
longer n-word overlap. Let V denote the set of n-
word overlaps shared between two glosses, the score
is defined as,
</bodyText>
<equation confidence="0.9877105">
�score = kwk2 (3)
wEV
</equation>
<bodyText confidence="0.9988645">
where kwk refers to the number of words in w. In
so doing, we can have corresponding senses for the
sentence Castro celebrates 86th birthday Monday
as follows,
</bodyText>
<equation confidence="0.4849465">
castro/10886929-n celebrate/02490877-v
birthday/15250178-n monday/15163979-n
</equation>
<bodyText confidence="0.9943615">
To find the n-word overlap, we found that
contiguous words in two glosses lie in the diagonal
of a matrix, take the senses walk and afoot for
example, their glosses are,
walking: the act of traveling by foot
afoot: traveling by foot
</bodyText>
<figure confidence="0.744776666666667">
5ensek = n � wi
i=−n j=1
(2)
</figure>
<page confidence="0.975345">
91
</page>
<bodyText confidence="0.955693666666667">
Place the walking glosses in rows and afoot
glosses in columns, we get the matrix representation
in Figure 1,
</bodyText>
<figureCaption confidence="0.61077">
Figure 1: n-word overlap representation
Figure 1 shows that travel by foot is a continuous
sequence of words shared by two glosses. Steps to
find n-word overlapping are:
</figureCaption>
<listItem confidence="0.996864142857143">
(1) Construct a matrix for two sentences;
(2) Get continuous n-word overlapping, n is
greater than 1;
(3) Set the cell values to 0 if they are contained in
continuous n-word.
(4) Get the words (unigrams) which are shared by
two sentences.
</listItem>
<bodyText confidence="0.5465085">
Take a b c d and b c a d for example, we will have
the matrix as follows,
</bodyText>
<figure confidence="0.796779333333333">
b c a d
a 0 0 1 0
b 1 0 0 0
</figure>
<table confidence="0.544243">
c 0 1 0 0
d 0 0 0 1
</table>
<tableCaption confidence="0.996502">
Table 1: Matrix representation for two sentences
</tableCaption>
<bodyText confidence="0.892660777777778">
By the step 2, we will get the b c and its
corresponding cells cell(1,0) and cell(2,1). We then
set the two cells to zero, and obtain an updated
matrix as follows,
b c a d
a 0 0 1 0
b 0 0 0 0
c 0 0 0 0
d 0 0 0 1
</bodyText>
<tableCaption confidence="0.990207">
Table 2: Updated matrix representation for two sentences
</tableCaption>
<bodyText confidence="0.9823728">
In Table 2, we found that cell(0,2) and cell(3,3)
have values greater than zero. Therefore, a and b
will be extracted the common terms.
This approach can also be applied to find common
n-word overlaps between sentences, for example,
</bodyText>
<listItem confidence="0.7290653">
s1: Olli Heinonen, the Head of the International
Atomic Energy Agency delegation to Iran, declared
yesterday that the agency has reached an agreement
with Tehran on the method of conducting the
negotiations pertaining to its nuclear program.
s2: leader of international atomic energy agency
delegation to iran , olli heinonen said yesterday ,
that the agency concluded a mutual understanding
with tehran on the way to manage talks depending
upon its atomic program.
</listItem>
<bodyText confidence="0.72330225">
We will have ngrams with n ranging from 1 to 7,
such as,
unigram: of, to, its, program, yesterday
bigram: olli heinonen
</bodyText>
<construct confidence="0.602137">
trigram: that the agency
four-gram: with tehran on the
seven-gram: international atomic energy agency
delegation to iran
</construct>
<bodyText confidence="0.9980485">
Similarity between two sentences is calculated by
counting the number of overlapped n-words. The
similarity for s1 and s2 is, (1 + 1 + 1 + 1 + 1) +
(2)2 + (3)2 + (4)2 + (7)2 = 83.
</bodyText>
<subsectionHeader confidence="0.999775">
2.2 String kernel
</subsectionHeader>
<bodyText confidence="0.9998555">
For the STS-TYPED task, when comparing whether
people or authors are similar or not, we found that
some entity mentions may have tiny variations, for
example,
</bodyText>
<subsectionHeader confidence="0.464317">
E Vincent Harris and E.Vincent Harris
</subsectionHeader>
<bodyText confidence="0.999931111111111">
The difference between the entities lies in fact that
the second entity has one more dot. In this case,
string kernel would be a good choice in verifying
they are similar or not. If we consider n=2, we obtain
79-dimensional feature space where the two entities
are mapped in Table 3.
In Table 3, A is the decay factor, in the range
of [0,1], that penalizes the longer distance of a
subsequence. Formally, string kernel is defined as,
</bodyText>
<equation confidence="0.8492125">
Kn(s, t) � � (Ou(s) - Ou(t)) (4)
uEE&amp;quot;
92
ev ei en ··· e. ··· rs is
O(evincentharris) A2 A3 + A13 A2 + A4 + A7 ··· 0 ··· A3 + A4 A2 + A12
O(e.vincentharris) A3 A4 + A14 A2 + A5 + A8 ··· A2 ··· A3 + A4 A2 + A12
</equation>
<tableCaption confidence="0.994966">
Table 3: Feature mapping for two entities
</tableCaption>
<table confidence="0.9837625">
TEAM headlines OnWN FNWN SMT mean rank
RUN1 0.5176 0.1517 0.2496 0.2914 0.3284 77
</table>
<tableCaption confidence="0.999635">
Table 4: Experimental results for STS-CORE
</tableCaption>
<bodyText confidence="0.97248575">
where En is the set of all possible subsequences
of length n. u indicates an item in the set, for
example, the subsequence ev in Table 3. Ou(s) is
the feature mapping of the subsequences in s. In
so doing, we can have similarity between entities in
Table 3 as follows:
Kn(s, t) = A2 × A3 + (A3 + A13) × (A4 + A14) +
···+(A3+A4)×(A3+A4)+(A2+A12)×(A2+A12)
To avoid enumeration of all subsequences for
similarity measurement, dynamic programming,
similar to the method by Lodhi et al. (2002) is used
here for similarity calculation.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.955863963636364">
The STS-CORE task is to quantify how similar
two sentences are. We simply use the sense
overlapping approach to compute the similarity.
Since this approach needs to find appropriate senses
for each word based on its contexts. The number
of contextual words is set to 5. Experiments
are conducted on four datasets. They are:
headlines mined from news sources by European
Media, OnWN extracted from from WordNet and
OntoNotes, FNWN from WordNet and FrameNet
and SMT dataset from DARPA GALE HTER and
HyTER. The results of our system (PolyUCOMP-
RUN1) are given in Table 4 ,
Our system achieves rather lower performance
in the OnWN and FNWN datasets. This is because
it is difficult to use contextual terms to find the
correct senses for words in sentences of these two
datasets. Take the two sentences in OnWN dataset
for example,
s1: the act of choosing among alternatives
s2: the act of changing one thing for another
thing.
The valid concepts for the two sentences are:
c1: 06532095-n 05790944-n
c2: 00030358-n 00126264-v 00002452-n
00002452-n
c1 and c2 have no shared senses, resulting in a
zero similarity between s1 and s2. However, s1 and
s2 should have the same meaning. Moreover, in the
FNWN dataset, the sentence lengths are unbalanced,
for example,
s1: there exist a number of different possible
events that may happen in the future. in most cases,
there is an agent involved who has to consider which
of the possible events will or should occur. a salient
entity which is deeply involved in the event may also
be mentioned.
s2: doing as one pleases or chooses;
s1 has 48 tokens with punctuations being
excluded and s2 has only 6 tokens. This would affect
our system performance as well.
For the STS-TYPED task, data set is taken
from Europeana, which provides millions of books,
paintings, films, museum objects and archival
records that have been digitised throughout Europe.
Each item has one line per type, where the type
can be the title of a record, list of subject terms,
textual description of the record, creator of the
record and date of the record. Participating systems
are supposed to compute similarities between semi-
structured items. In this task, we take the strategies
in Table 5,
Jaccard denotes the Jaccard similarity measure.
5tringkernel + Jaccard means that two types
are similar if they share many terms, for example,
</bodyText>
<page confidence="0.994977">
93
</page>
<table confidence="0.9772865">
TEAM general author people time location event subject description mean rank
RUN1 0.4888 0.6940 0.3223 0.3820 0.3621 0.1625 0.3962 0.4816 0.4112 12
RUN2 0.4893 0.6940 0.3253 0.3777 0.3628 0.1968 0.3962 0.4816 0.4155 11
RUN3 0.4915 0.6940 0.3254 0.3737 0.3667 0.2207 0.3962 0.4816 0.4187 10
</table>
<tableCaption confidence="0.807685">
Table 6: Experimental results for STS-TYPED
</tableCaption>
<table confidence="0.99973375">
Type Strategy
author String kernel
people String kernel + Jaccard
time String kernel + Jaccard
location String kernel + Jaccard
event WordNet + Jaccard
subject Sense overlapping
description Sense overlapping
</table>
<tableCaption confidence="0.996547">
Table 5: Strategies for computing similarity
</tableCaption>
<bodyText confidence="0.99967995">
location; and string kernel is used to determine
whether two locations are similar or not. For the
type of event, we extract verbs from records and
count the number of shared verbs between two
records. The verb similarity is obtained through
WordNet. The general similarity is equal to the
average of the 7 scores. Also, Stanford CoreNLP
tool1 is used to extract author, date, time, location
and handle part-of-speech tagging.
In this STS-TYPED task, we use string kernel and
WordNet to determine whether two terms are similar
and increase the number of counts if their similarity
exceeds a certain threshold. Therefore, we have
chosen 0.4, 0.5 and 0.6 in a heuristic manner and
obtained three different runs. Experimental results
are given in Table 6.
Since the types of author, subject and
description are not related to either string kernel
or WordNet, their performances remain unchanged
during three runs.
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999883666666667">
In the Semantic Textual Similarity task of SemEval-
2013, to capture the meaning between sentences,
we proposed to disambiguate word senses using
contexts and then determine sentence similarity
by counting the senses they shared. First, word
senses are disambiguated by means of the contextual
</bodyText>
<footnote confidence="0.882819">
1http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<bodyText confidence="0.999523333333333">
words. When determining similarity between two
senses (synsets), n-word overlapping approach is
used for counting the number of shared words
in two glosses. Besides, string kernel is used
to capture similarity between entities to avoid
variations between entities. Our approach is simple
and we will apply regression models to determine
sentence similarity on the basis of these features in
future work.
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992423125">
Daniel B., Chris Biemann, Iryna Gurevych and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics (*SEM 2012).
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot
on Semantic Textual Similarity. Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Frane Saric, Goran Glavas, Mladen Karan, Jan Snajder
and Bojana Dalbelo Basia. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. Proceedings
of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), in conjunction with the
First Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research, 22(2004):457–479.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
Classification using String Kernels. The Journal of
Machine Learning Research, 2(2002):419–444.
Jian Xu, Qin Lu and Zhengzhong Liu. 2012.
PolyUCOMP: Combining Semantic Vectors with
Skip-bigrams for Semantic Textual Similarity.
</reference>
<page confidence="0.990815">
94
</page>
<reference confidence="0.9988694">
Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang 2011.
Transferring Topical Knowledge from Auxiliary Long
Text for Short Text Understanding. Proceedings of the
20th ACM Conference on Information and Knowledge
Management (ACM CIKM 2011).
Rada Mihalcea and Courtney Corley. 2006. Corpusbased
and Knowledge-based Measures of Text Semantic
Similarity. Proceeding of the Twenty-First National
Conference on Artificial Intelligence and the
Eighteenth Innovative Applications of Artificial
Intelligence Conference..
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
Proceedings of the 18th International Joint
Conference on Artificial Intelligence.
Vasileios Hatzivassiloglou, Judith L. Klavans , Eleazar
Eskin. 1999. Detecting Text Similarity over Short
Passages: Exploring Linguistic Feature Combinations
via Machine Learning. Proceeding of Empirical
Methods in natural language processing and Very
Large Corpora.
Yuhua Li, David Mclean, Zuhair B, James D. O’shea
and Keeley Crockett. 2006. Sentence Similarity
Based on Semantic Nets and Corpus Statistics. IEEE
Transactions on Knowledge and Data Engineering,
18(8):1138–1149.
</reference>
<page confidence="0.999007">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.631773">
<title confidence="0.996187">PolyUCOMP-CORE TYPED: Computing Semantic Textual using Overlapped Senses</title>
<author confidence="0.994929">Jian Xu Qin Lu The Hong Kong Polytechnic</author>
<affiliation confidence="0.961217">Department of</affiliation>
<author confidence="0.683779">Hung Hom</author>
<author confidence="0.683779">Hong Kowloon</author>
<abstract confidence="0.9987828125">The Semantic Textual Similarity (STS) task aims to exam the degree of semantic equivalence between sentences (Agirre et al., 2012). This paper presents the work of the Hong Kong Polytechnic University (PolyUCOMP) team which has participated in the STS core and typed tasks of SemEval- 2013. For the STS core task, the PolyUCOMP system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses they shared. For the STS typed task, the string kernel (Lodhi et al., 2002) is used to compute similarity between two entities to avoid string variations in entities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Daniel</author>
<author>Chris Biemann</author>
</authors>
<title>Iryna Gurevych and Torsten Zesch.</title>
<date>2012</date>
<booktitle>Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<marker>Daniel, Biemann, 2012</marker>
<rawString>Daniel B., Chris Biemann, Iryna Gurevych and Torsten Zesch. 2012. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<date>2012</date>
<booktitle>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="1148" citStr="Agirre et al., 2012" startWordPosition="167" endWordPosition="170">e STS core task, the PolyUCOMP system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses they shared. For the STS typed task, the string kernel (Lodhi et al., 2002) is used to compute similarity between two entities to avoid string variations in entities. 1 Introduction Sentence similarity computation plays an important role in text summarization and social network applications (Erkan et al., 2004; Jin et al., 2011). The SemEval 2012 competition initiated a task targeted at Semantic Textual Similarity (STS) between sentence pairs (Agirre et al., 2012). Given a set of sentence pairs, participants are required to assign to each sentence pair a similarity score. Because a sentence has only a limited amount of content words, it is difficult to determine sentence similarities. To solve this problem, Hatzivassiloglou et al. (1999) proposed to use linguistic features as indicators of text similarity to address the problem of sparse representation of sentences. Mihalcea et al. (2006) measured sentence similarity using component words in sentences. Li et al. 90 (2006) proposed to incorporate the semantic vector and word order to calculate sentence </context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Saric</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder and Bojana Dalbelo Basia.</title>
<date></date>
<booktitle>Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<marker>Saric, Glavas, Karan, </marker>
<rawString>Frane Saric, Goran Glavas, Mladen Karan, Jan Snajder and Bojana Dalbelo Basia. 2012. TakeLab: Systems for Measuring Semantic Text Similarity. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<issue>2004</issue>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based Lexical Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research, 22(2004):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text Classification using String Kernels.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>2</volume>
<issue>2002</issue>
<contexts>
<context position="755" citStr="Lodhi et al., 2002" startWordPosition="109" endWordPosition="112">Department of Computing Hung Hom, Kowloon, Hong Kong {csjxu, csluqin}@comp.polyu.edu.hk Abstract The Semantic Textual Similarity (STS) task aims to exam the degree of semantic equivalence between sentences (Agirre et al., 2012). This paper presents the work of the Hong Kong Polytechnic University (PolyUCOMP) team which has participated in the STS core and typed tasks of SemEval2013. For the STS core task, the PolyUCOMP system disambiguates words senses using contexts and then determine sentence similarity by counting the number of senses they shared. For the STS typed task, the string kernel (Lodhi et al., 2002) is used to compute similarity between two entities to avoid string variations in entities. 1 Introduction Sentence similarity computation plays an important role in text summarization and social network applications (Erkan et al., 2004; Jin et al., 2011). The SemEval 2012 competition initiated a task targeted at Semantic Textual Similarity (STS) between sentence pairs (Agirre et al., 2012). Given a set of sentence pairs, participants are required to assign to each sentence pair a similarity score. Because a sentence has only a limited amount of content words, it is difficult to determine sent</context>
<context position="10657" citStr="Lodhi et al. (2002)" startWordPosition="1775" endWordPosition="1778">ities TEAM headlines OnWN FNWN SMT mean rank RUN1 0.5176 0.1517 0.2496 0.2914 0.3284 77 Table 4: Experimental results for STS-CORE where En is the set of all possible subsequences of length n. u indicates an item in the set, for example, the subsequence ev in Table 3. Ou(s) is the feature mapping of the subsequences in s. In so doing, we can have similarity between entities in Table 3 as follows: Kn(s, t) = A2 × A3 + (A3 + A13) × (A4 + A14) + ···+(A3+A4)×(A3+A4)+(A2+A12)×(A2+A12) To avoid enumeration of all subsequences for similarity measurement, dynamic programming, similar to the method by Lodhi et al. (2002) is used here for similarity calculation. 3 Experiments The STS-CORE task is to quantify how similar two sentences are. We simply use the sense overlapping approach to compute the similarity. Since this approach needs to find appropriate senses for each word based on its contexts. The number of contextual words is set to 5. Experiments are conducted on four datasets. They are: headlines mined from news sources by European Media, OnWN extracted from from WordNet and OntoNotes, FNWN from WordNet and FrameNet and SMT dataset from DARPA GALE HTER and HyTER. The results of our system (PolyUCOMPRUN1</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text Classification using String Kernels. The Journal of Machine Learning Research, 2(2002):419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Xu</author>
<author>Qin Lu</author>
<author>Zhengzhong Liu</author>
</authors>
<title>PolyUCOMP: Combining Semantic Vectors with Skip-bigrams for Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="2245" citStr="Xu et al. (2012)" startWordPosition="329" endWordPosition="332">rds in sentences. Li et al. 90 (2006) proposed to incorporate the semantic vector and word order to calculate sentence similarity. Biemann et al. (2012) applied the log-linear regression model by combining the simple string based measures, for example, word ngrams and semantic similarity measures, for example, textual entailment. Similarly, Saric et al. (2012) used a support vector regression model which incorporates features computed from sentence pairs. The features are knowledge- and corpus-based word similarity, ngram overlaps, WordNet augmented word overlap, syntactic features and so on. Xu et al. (2012) combined semantic vectors with skip bigrams to determine sentence similarity, whereas the skip bigrams take into the sequential order between words. In our approach to the STS task, words in sentences are assigned with appropriate senses using their contexts. Sentence similarity is computed by calculating the number of shared senses in both sentences since it is reasonable to assume that similar sentences should have more overlapping senses. For the STS-TYPED task, variations might occur in author names, people involved, time expression and location. Thus, string kernel is applied to compute </context>
</contexts>
<marker>Xu, Lu, Liu, 2012</marker>
<rawString>Jian Xu, Qin Lu and Zhengzhong Liu. 2012. PolyUCOMP: Combining Semantic Vectors with Skip-bigrams for Semantic Textual Similarity. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ou Jin</author>
<author>Nathan Nan Liu</author>
</authors>
<title>Yong Yu and Qiang Yang 2011. Transferring Topical Knowledge from Auxiliary Long Text for Short Text Understanding.</title>
<date>2011</date>
<booktitle>Proceedings of the 20th ACM Conference on Information and Knowledge Management (ACM CIKM</booktitle>
<marker>Jin, Liu, 2011</marker>
<rawString>Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang 2011. Transferring Topical Knowledge from Auxiliary Long Text for Short Text Understanding. Proceedings of the 20th ACM Conference on Information and Knowledge Management (ACM CIKM 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
</authors>
<title>Corpusbased and Knowledge-based Measures of Text Semantic</title>
<date>2006</date>
<booktitle>Similarity. Proceeding of the Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference..</booktitle>
<marker>Mihalcea, Corley, 2006</marker>
<rawString>Rada Mihalcea and Courtney Corley. 2006. Corpusbased and Knowledge-based Measures of Text Semantic Similarity. Proceeding of the Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended Gloss Overlaps as a Measure of Semantic Relatedness.</title>
<date>2003</date>
<booktitle>Proceedings of the 18th International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended Gloss Overlaps as a Measure of Semantic Relatedness. Proceedings of the 18th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
</authors>
<title>Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning. Proceeding of Empirical Methods in natural language processing and Very Large Corpora.</title>
<date>1999</date>
<marker>Hatzivassiloglou, Klavans, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans , Eleazar Eskin. 1999. Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning. Proceeding of Empirical Methods in natural language processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David Mclean</author>
<author>B Zuhair</author>
<author>James D O’shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, Mclean, Zuhair, O’shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David Mclean, Zuhair B, James D. O’shea and Keeley Crockett. 2006. Sentence Similarity Based on Semantic Nets and Corpus Statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138–1149.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>