<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038337">
<title confidence="0.998975">
A Weighting Scheme for Open Information Extraction
</title>
<author confidence="0.994889">
Yuval Merhav
</author>
<affiliation confidence="0.7359345">
Illinois Institute of Technology
Chicago, IL USA
</affiliation>
<email confidence="0.999034">
yuval@ir.iit.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999193470588235">
We study1 the problem of extracting all pos-
sible relations among named entities from un-
structured text, a task known as Open Infor-
mation Extraction (Open IE). A state-of-the-
art Open IE system consists of natural lan-
guage processing tools to identify entities and
extract sentences that relate such entities, fol-
lowed by using text clustering to identify the
relations among co-occurring entity pairs. In
particular, we study how the current weighting
scheme used for Open IE affects the clustering
results and propose a term weighting scheme
that significantly improves on the state-of-the-
art in the task of relation extraction both when
used in conjunction with the standard tf ·idf
scheme, and also when used as a pruning fil-
ter.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988725">
The extraction of structured information from text is
a long-standing challenge in Natural Language Pro-
cessing which has been re-invigorated with the ever-
increasing availability of user-generated textual con-
tent online. The large-scale extraction of unknown
relations has been termed as Open Information Ex-
traction (Open IE) (Banko et al., 2007) (also referred
to as Open Relationship Extraction, Relation Extrac-
tion, or Relation Discovery). Many challenges exist
in developing an Open IE solution, such as recogniz-
ing and disambiguating entities in a multi-document
setting, and identifying all so-called relational terms
</bodyText>
<footnote confidence="0.9524835">
1This thesis proposal has been accepted for publication
in (Merhav et al., 2012).
</footnote>
<page confidence="0.992494">
60
</page>
<bodyText confidence="0.999986558823529">
in the sentences connecting pairs of entities. Rela-
tional terms are words (usually one or two) that de-
scribe a relation between entities (for instance, terms
like “running mate”, “opponent”, “governor of” are
relational terms).
One approach for Open IE is based on cluster-
ing of entity pairs to produce relations, as intro-
duced by Hasegawa et al. (Hasegawa et al., 2004).
Their and follow-up works (e.g., (Mesquita et al.,
2010)) extract terms in a small window between two
named entities to build the context vector of each
entity pair, and then apply a clustering algorithm
to cluster together entity pairs that share the same
relation (e.g., Google–Youtube and Google–
Motorola Mobility in a cluster about the “ac-
quired” relation). Contexts of entity pairs are repre-
sented using the vector space model. The state-of-
the-art in clustering-based Open IE assigns weights
to the terms according to the standard tf·idf scheme.
Motivation. Intuitively, the justification for us-
ing idf is that a term appearing in many documents
(i.e., many contexts in our setting) would not be
a good discriminator (Robertson, 2004), and thus
should weigh proportionally less than other, more
rare terms. For the task of relation extraction how-
ever, we are interested specifically in terms that de-
scribe relations. In our settings, a single document
is a context vector of one entity pair, generated from
all articles discussing this pair, which means that the
fewer entity pairs a term appears in, the higher its
idf score would be. Consequently, it is not necessar-
ily the case that terms that are associated with high
idf weights would be good relation discriminators.
On the other hand, popular relational terms that ap-
</bodyText>
<note confidence="0.385585">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 60–65,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9982546875">
ply to many entity pairs would have relatively lower
idf weights.
It is natural to expect that the relations extracted
by an Open IE system are strongly correlated with
a given context. For instance, marriage is a relation
between two persons and thus belongs to the domain
PER–PER. We exploit this observation to boost the
weight of relational terms associated with marriage
(e.g., “wife”, “spouse”, etc.) in those entity pairs
where the domain is also PER–PER. The more dom-
inant a term in a given domain compared to other
domains, the higher its boosting score would be.
Our work resembles the work on selectional pref-
erences (Resnik, 1996). Selectional preferences are
semantic constraints on arguments (e.g. a verb like
“eat” prefers as object edible things).
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999899875">
Different approaches for Open IE have been pro-
posed in the literature, such as bootstrapping
(e.g., (Zhu et al., 2009) (Bunescu and Mooney,
2007)), self or distant supervision (e.g., (Banko
et al., 2007) (Mintz et al., 2009)) and rule based
(e.g., (Fader et al., 2011)). In this work we focus
on unsupervised approaches.
Fully unsupervised Open IE systems are mainly
based on clustering of entity pair contexts to pro-
duce clusters of entity pairs that share the same re-
lations, as introduced by Hasegawa et al. (Hasegawa
et al., 2004) (this is the system we use in this work
as our baseline). Hasegawa et al. used word uni-
grams weighted by tf ·idf to build the context vec-
tors and applied Hierarchical Agglomerative Clus-
tering (HAC) with complete linkage deployed on a
1995 New York Times corpus. Mesquita et al. ex-
tended this work by using other features such as part
of speech patterns (Mesquita et al., 2010). To re-
duce noise in the feature space, a common problem
with text mining, known feature selection and rank-
ing methods for clustering have been applied (Chen
et al., 2005; Rosenfeld and Feldman, 2007). Both
works used the K-Means clustering algorithm with
the stability-based criterion to automatically esti-
mate the number of clusters.
This work extends all previous clustering works
by utilizing domain frequency as a novel weight-
ing scheme for clustering entity pairs. The idea of
domain frequency was first proposed for predicting
entities which are erroneously typed by NER sys-
tems (Merhav et al., 2010).
</bodyText>
<sectionHeader confidence="0.974374" genericHeader="method">
3 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.999938676470588">
This work was implemented on top of the SONEX
system (Mesquita et al., 2010), deployed on the
ICWSM 2009 Spinn3r corpus (Burton et al., 2009),
focusing on posts in English (25 million out of 44
million in total), collected between August 1st, 2008
and October 1st, 2008. The system uses the Illi-
nois Entity Tagger (Ratinov and Roth, 2009) and Or-
thomatcher from the GATE framework2 for within-
a-document co-reference resolution.
Evaluating Open IE systems is a difficult prob-
lem. Mesquita et al. evaluated SONEX by auto-
matically matching a sample of the entity pairs their
system identified from the Spinn3r corpus against a
publicly available curated database3. Their approach
generated two datasets: INTER and 10PERC. IN-
TER contains the intersection pairs only (i.e., in-
tersection pairs are those from Spinn3r and Free-
base that match both entity names and types ex-
actly), while 10PERC contains 10% of the total pairs
SONEX identified, including the intersection pairs.
We extended these two datasets by adding more en-
tity pairs and relations. We call the resulting datasets
INTER (395 entity pairs and 20 different relations)
and NOISY (contains INTER plus approximately
30,000 entity pairs as compared to the 13,000 pairs
in 10PERC ).
We evaluate our system by reporting f-measure
numbers for our system running on INTER and
NOISY against the ground truth, using similar set-
tings used by (Hasegawa et al., 2004) and (Mesquita
et al., 2010). These include word unigrams as fea-
tures, HAC with average link (outperformed single
and complete link), and tf ·idf and cosine similarity
as the baseline.
</bodyText>
<sectionHeader confidence="0.941109" genericHeader="method">
4 Weighting Scheme
</sectionHeader>
<bodyText confidence="0.9999055">
Identifying the relationship (if any) between entities
e1, e2 is done by analyzing the sentences that men-
tion e1 and e2 together. An entity pair is defined by
two entities e1 and e2 together with the context in
</bodyText>
<footnote confidence="0.9994935">
2http://gate.ac.uk/
3http://www.freebase.com
</footnote>
<page confidence="0.999079">
61
</page>
<bodyText confidence="0.978694571428572">
which they co-occur. For our purposes, the context
can be any textual feature that allows the identifica-
tion of the relationship for the given pair. The con-
texts of entity pairs are represented using the vec-
tor space model with the common tf •idf weighting
scheme. More precisely, for each term t in the con-
text of an entity pair, tf is the frequency of the term
in the context, while
idf = log (|d :|D |t E d |/
where |D |is the total number of entity pairs, and
|d : t E d |is the number of entity pairs contain-
ing term t. The standard cosine similarity is used to
compute the similarity between context vectors dur-
ing clustering.
</bodyText>
<subsectionHeader confidence="0.997416">
4.1 Domain Frequency
</subsectionHeader>
<bodyText confidence="0.999844032258064">
We start with a motivating example before diving
into the details about how we compute domain fre-
quency. We initially built our system with the tra-
ditional tf • idf and were unsatisfied with the re-
sults. Consequently, we examined the data to find
a better way to score terms and filter noise. For
example, we noticed that the pair Youtube[ORG] –
Google[ORG] (associated with the “Acquired by”
relation) was not clustered correctly. In Table 1 we
listed all the Unigram features we extracted for the
pair from the entire collection sorted by their domain
frequency score for ORG–ORG (recall that these are
the intervening features between the pair for each
co-occurrence in the entire dataset). For clarity the
terms were not stemmed.
Clearly, most terms are irrelevant which make it
difficult to cluster the pair correctly. We listed in
bold all terms that we think are useful. Besides “be-
longs”, all these terms have high domain frequency
scores. However, most of these terms do not have
high idf scores. Term frequencies within a pair are
also not helpful in many cases since many pairs are
mentioned only a few times in the text. Next, we
define the domain frequency score (Merhav et al.,
2010).
Definition. Let P be the set of entity pairs, let T
be the set of all entity types, and let D = T x T be
the set of all possible relation domains. The domain
frequency (df ) of a term t, appearing in the context
of some entity pair in P, in a given relation domain
i E D, denoted df�(t), is defined as
</bodyText>
<equation confidence="0.9841195">
df�(t) = fi(t)
E1&lt;j&lt;n fj(t)�
</equation>
<bodyText confidence="0.999900692307692">
where fi(t) is the frequency with which term t ap-
pears in the context of entity pairs of domain i E
D, and n is the number of domains in D. When
computing the df score for a given term, it is pre-
ferred to consider each pair only once. For example,
“Google[ORG] acquired Youtube[ORG]” would be
counted only once (for “acquired” in the ORG–ORG
domain) even if this pair and context appear many
times in the collection. By doing so we eliminate
the problem of duplicates (common on the web).
Unlike the idf score, which is a global measure
of the discriminating power of a term, the df score
is domain-specific. Thus, intuitively, the df score
would favour specific relational terms (e.g., “wife”
which is specific to personal relations) as opposed
to generic ones (e.g., “member of” which applies to
several domains). To validate this hypothesis, we
computed the df scores of several relational terms
found in the clusters the system produced on the
main Spinn3r corpus.
Figure 1 shows the relative df scores of 4 rela-
tional terms (mayor, wife, CEO, and coach) which
illustrate well the strengths of the df score. We can
see that for the majority of terms (Figure 1(a)–(c)),
there is a single domain for which the term has a
clearly dominant df score: LOC–PER for mayor,
PER–PER for wife, and ORG–PER for CEO.
Dependency on NER Types. Looking again at
Figure 1, there is one case in which the df score does
not seem to discriminate a reasonable domain. For
coach, the dominant domain is LOC–PER, which
can be explained by the common use of the city (or
state) name as a proxy for a team as in the sentence
“Syracuse football coach Greg Robinson”. Note,
however, that the problem in this case is the dif-
ficulty for the NER to determine that “Syracuse”
refers to the university. These are some examples
of correctly identified pairs in the coach relation but
in which the NER types are misleading:
</bodyText>
<listItem confidence="0.882144333333333">
• LOC–PER domain: (England, Fabio Capello);
(Croatia, Slaven Bilic); (Sunderland, Roy
Keane).
</listItem>
<page confidence="0.99945">
62
</page>
<tableCaption confidence="0.999639">
Table 1: Unigram features for the pair Youtube[ORG] – Google[ORG] with idf and df (ORG–ORG) scores
</tableCaption>
<table confidence="0.999763555555556">
Term idf df (ORG–ORG) Term idf df (ORG–ORG)
ubiquitious 11.6 1.00 blogs 6.4 0.14
sale 5.9 0.80 services 5.9 0.13
parent 6.8 0.78 instead 4.0 0.12
uploader 10.5 0.66 free 5.0 0.12
purchase 6.3 0.62 similar 5.7 0.12
add 6.1 0.33 recently 4.2 0.12
traffic 7.0 0.55 disappointing 8.2 0.12
downloader 10.9 0.50 dominate 6.4 0.11
dailymotion 9.5 0.50 hosted 5.6 0.10
bought 5.2 0.49 hmmm 9.3 0.10
buying 5.8 0.47 giant 5.4 &lt; 0.1
integrated 7.3 0.44 various 5.7 &lt; 0.1
partnership 6.7 0.42 revealed 5.2 &lt; 0.1
pipped 8.9 0.37 experiencing 7.7 &lt; 0.1
embedded 7.6 0.36 fifth 6.5 &lt; 0.1
add 6.1 0.33 implication 8.5 &lt; 0.1
acquired 5.6 0.33 owner 6.0 &lt; 0.1
channel 6.3 0.28 corporate 6.4 &lt; 0.1
web 5.8 0.26 comments 5.2 &lt; 0.1
video 4.9 0.24 according 4.5 &lt; 0.1
sellout 9,2 0.23 resources 6.9 &lt; 0.1
revenues 8.6 0.21 grounds 7.8 &lt; 0.1
account 6.0 0.18 poked 6.9 &lt; 0.1
evading 9.8 0.16 belongs 6.2 &lt; 0.1
eclipsed 7.8 0.16 authors 7.4 &lt; 0.1
company 4.7 0.15 hooked 7.1 &lt; 0.1
</table>
<listItem confidence="0.993079">
• MISC–PER domain: (Titans, Jeff Fisher); (Jets,
Eric Mangini); (Texans, Gary Kubiak).
</listItem>
<subsectionHeader confidence="0.99819">
4.2 Using the df Score
</subsectionHeader>
<bodyText confidence="0.99993905">
We use the df score for two purposes in our work.
First, for clustering, we compute the weights of the
terms inside all vectors using the product tf •idf • df .
Second, we also use the df score as a filtering tool,
by removing terms from vectors whenever their df
scores lower than a threshold. Going back to the
Youtube[ORG] – Google[ORG] example in Table 1,
we can see that minimum df filtering helps with re-
moving many noisy terms. We also use maximum
idf filtering which helps with removing terms that
have high df scores only because they are rare and
appear only within one domain (e.g., ubiquitious
(misspelled in source) and uploader in this example).
As we shall see in the experimental evaluation,
even in the presence of incorrect type assignments
made by the NER tool, the use of df scores improves
the accuracy significantly. It is also worth mention-
ing that computing the df scores can be done fairly
efficiently, and as soon as all entity pairs are ex-
tracted.
</bodyText>
<sectionHeader confidence="0.999862" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999992111111111">
We now report the results on INTER and NOISY.
Our baseline run is similar to the systems pub-
lished by Hasegawa et al. (Hasegawa et al., 2004)
and Mesquita et al. (Mesquita et al., 2010); that
is HAC with average link using tf •idf and cosine
similarity, and stemmed word unigrams (excluding
stop words) as features extracted using a window
size of five words between pair of entities. Fig-
ure 2 shows that by integrating domain frequency
</bodyText>
<page confidence="0.994362">
63
</page>
<figure confidence="0.9986314375">
1
Domain Freq.
0.8
0.6
0.4
0.2
0
1
Domain Freq.
0.8
0.6
0.4
0.2
0
1
Domain Freq.
0.8
0.6
0.4
0.2
0
1
Domain Freq.
0.8
0.6
0.4
0.2
0
(a) mayor.
(c) CEO.
(b) wife.
(d) coach.
</figure>
<figureCaption confidence="0.99997">
Figure 1: Domain Frequency examples.
</figureCaption>
<bodyText confidence="0.9999855">
(df) we significantly outperformed this baseline on
both datasets (INTER: F-1 score of 0.87 compared
to 0.75; NOISY: F-1 score of 0.72 compared to
0.65). In addition, filtering terms by minimum df
and maximum idf thresholds improved the results
further on INTER. These results are promising since
a major challenge in text clustering is reducing the
noise in the data.
We also see a substantial decrease of the results
on NOISY compared to INTER. Such a decrease
is, of course, expected: NOISY contains not only
thousands more entity pairs than INTER, but also
hundreds (if not thousands) more relations as well,
making the clustering task harder in practice.
</bodyText>
<sectionHeader confidence="0.976512" genericHeader="evaluation">
6 Conclusion and Future Research
Directions
</sectionHeader>
<bodyText confidence="0.999992777777778">
We utilized the Domain Frequency (df ) score as a
term-weighting score designed for identifying rela-
tional terms for Open IE. We believe that df can
be utilized in various of applications, with the ad-
vantage that in practice, for many such applica-
tions, the list of terms and scores can be used off-
the-shelf with no further effort. One such applica-
tion is Named Entity Recognition (NER) – df helps
in identifying relational patterns that are associated
</bodyText>
<figure confidence="0.7214915">
0 0.01 0.02 0.03 0.04 0.05
clustering threshold
</figure>
<figureCaption confidence="0.99902025">
Figure 2: tf x idf Vs. tf x idf x df with and with-
out minimum df and maximum idf pruning on INTER
and NOISY. All results consistently dropped for cluster-
ing thresholds larger than 0.05.
</figureCaption>
<bodyText confidence="0.999522222222222">
with a certain domain (e.g., PER–PER). If the list of
words and phrases associated with their df scores is
generated using an external dataset annotated with
entities, it can be applied to improve results in other,
more difficult domains, where the performance of
the NER is poor.
It is also appealing that the df score is proba-
bilistic, and as such, it is, for the most part, lan-
guage independent. Obviously, not all languages
</bodyText>
<figure confidence="0.983992230769231">
f-measure
0.9
0.8
0.7
0.6
0.5
0.4
INTER: tf*idf*df &amp; pruning
INTER: tf*idf*df
INTER: tf*idf
NOISY: tf*idf*df &amp; pruning
NOISY: tf*idf*df
NOISY: tf*idf
</figure>
<page confidence="0.995309">
64
</page>
<bodyText confidence="0.999844375">
have the same structure as English and some adjust-
ments should be made. For example, df exploits
the fact that relational verbs are usually placed be-
tween two entities in a sentence, which may not be
always the case in other languages (e.g., German).
Investigating how df can be extended and utilized in
a multi-lingual environment is an interesting future
direction.
</bodyText>
<sectionHeader confidence="0.998529" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999966666666667">
The author would like to thank Professor Denilson
Barbosa from the University of Alberta, Professor
David Grossman and Gady Agam from Illinois In-
stitute of Technology, and Professor Ophir Frieder
from Georgetown University. All provided great
help in forming the ideas that led to this work.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997490442857143">
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Manuela M.
Veloso, editor, IJCAI, pages 2670–2676.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In ACL.
K. Burton, A. Java, and I. Soboroff. 2009. The icwsm
2009 spinn3r dataset. In Proceedings of the Annual
Conference on Weblogs and Social Media.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu
Niu. 2005. Unsupervised feature selection for re-
lation extraction. In IJCNLP-05: The 2nd Interna-
tional Joint Conference on Natural Language Process-
ing. Springer.
Anthony Fader, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2011. Identifying re-
lations for open information extraction. Manuscript
submitted for publication. University of Washington.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL ’04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 415, Morristown, NJ, USA.
Association for Computational Linguistics.
Yuval Merhav, Filipe Mesquita, Denilson Barbosa,
Wai Gen Yee, and Ophir Frieder. 2010. Incorporat-
ing global information into named entity recognition
systems using relational context. In Proceedings of
the 33rd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’10, pages 883–884, New York, NY, USA. ACM.
Yuval Merhav, Filipe Mesquita, Denilson Barbosa,
Wai Gen Yee, and Ophir Frieder. 2012. Extract-
ing information networks from the blogosphere. ACM
Transactions on the Web (TWEB). Accepted 2012.
Filipe Mesquita, Yuval Merhav, and Denilson Barbosa.
2010. Extracting information networks from the blo-
gosphere: State-of-the-art and challenges. In 4th Int’l
AAAI Conference on Weblogs and Social Media–Data
Challenge.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL-IJCNLP ’09: Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 2, pages 1003–1011, Morristown, NJ, USA.
Association for Computational Linguistics.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL ’09: Proceedings of the Thirteenth Conference
on Computational Natural Language Learning, pages
147–155, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Philip Resnik. 1996. Selectional constraints: an
information-theoretic model and its computational re-
alization.
Stephen Robertson. 2004. Understanding inverse doc-
ument frequency: On theoretical arguments for idf.
Journal of Documentation, 60:2004.
Benjamin Rosenfeld and Ronen Feldman. 2007. Cluster-
ing for unsupervised relation identification. In CIKM
’07, pages 411–418, New York, NY, USA. ACM.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. Statsnowball: a statistical approach
to extracting entity relationships. In Proceedings of
the 18th international conference on World wide web,
WWW ’09, pages 101–110, New York, NY, USA.
ACM.
</reference>
<page confidence="0.999602">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.294458">
<title confidence="0.995893">A Weighting Scheme for Open Information Extraction</title>
<author confidence="0.676746">Yuval</author>
<affiliation confidence="0.7379845">Illinois Institute of Chicago, IL</affiliation>
<email confidence="0.997451">yuval@ir.iit.edu</email>
<abstract confidence="0.987750777777778">the problem of extracting all possible relations among named entities from unstructured text, a task known as Open Information Extraction (Open IE). A state-of-theart Open IE system consists of natural language processing tools to identify entities and extract sentences that relate such entities, followed by using text clustering to identify the relations among co-occurring entity pairs. In particular, we study how the current weighting scheme used for Open IE affects the clustering results and propose a term weighting scheme that significantly improves on the state-of-theart in the task of relation extraction both when in conjunction with the standard scheme, and also when used as a pruning filter.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<pages>2670--2676</pages>
<editor>In Manuela M. Veloso, editor, IJCAI,</editor>
<contexts>
<context position="1231" citStr="Banko et al., 2007" startWordPosition="187" endWordPosition="190">E affects the clustering results and propose a term weighting scheme that significantly improves on the state-of-theart in the task of relation extraction both when used in conjunction with the standard tf ·idf scheme, and also when used as a pruning filter. 1 Introduction The extraction of structured information from text is a long-standing challenge in Natural Language Processing which has been re-invigorated with the everincreasing availability of user-generated textual content online. The large-scale extraction of unknown relations has been termed as Open Information Extraction (Open IE) (Banko et al., 2007) (also referred to as Open Relationship Extraction, Relation Extraction, or Relation Discovery). Many challenges exist in developing an Open IE solution, such as recognizing and disambiguating entities in a multi-document setting, and identifying all so-called relational terms 1This thesis proposal has been accepted for publication in (Merhav et al., 2012). 60 in the sentences connecting pairs of entities. Relational terms are words (usually one or two) that describe a relation between entities (for instance, terms like “running mate”, “opponent”, “governor of” are relational terms). One appro</context>
<context position="4436" citStr="Banko et al., 2007" startWordPosition="701" endWordPosition="704">ith marriage (e.g., “wife”, “spouse”, etc.) in those entity pairs where the domain is also PER–PER. The more dominant a term in a given domain compared to other domains, the higher its boosting score would be. Our work resembles the work on selectional preferences (Resnik, 1996). Selectional preferences are semantic constraints on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesqui</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Manuela M. Veloso, editor, IJCAI, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4378" citStr="Bunescu and Mooney, 2007" startWordPosition="692" endWordPosition="695">observation to boost the weight of relational terms associated with marriage (e.g., “wife”, “spouse”, etc.) in those entity pairs where the domain is also PER–PER. The more dominant a term in a given domain compared to other domains, the higher its boosting score would be. Our work resembles the work on selectional preferences (Resnik, 1996). Selectional preferences are semantic constraints on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complet</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Burton</author>
<author>A Java</author>
<author>I Soboroff</author>
</authors>
<title>The icwsm 2009 spinn3r dataset.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="5916" citStr="Burton et al., 2009" startWordPosition="952" endWordPosition="955">(Chen et al., 2005; Rosenfeld and Feldman, 2007). Both works used the K-Means clustering algorithm with the stability-based criterion to automatically estimate the number of clusters. This work extends all previous clustering works by utilizing domain frequency as a novel weighting scheme for clustering entity pairs. The idea of domain frequency was first proposed for predicting entities which are erroneously typed by NER systems (Merhav et al., 2010). 3 Data and Evaluation This work was implemented on top of the SONEX system (Mesquita et al., 2010), deployed on the ICWSM 2009 Spinn3r corpus (Burton et al., 2009), focusing on posts in English (25 million out of 44 million in total), collected between August 1st, 2008 and October 1st, 2008. The system uses the Illinois Entity Tagger (Ratinov and Roth, 2009) and Orthomatcher from the GATE framework2 for withina-document co-reference resolution. Evaluating Open IE systems is a difficult problem. Mesquita et al. evaluated SONEX by automatically matching a sample of the entity pairs their system identified from the Spinn3r corpus against a publicly available curated database3. Their approach generated two datasets: INTER and 10PERC. INTER contains the inte</context>
</contexts>
<marker>Burton, Java, Soboroff, 2009</marker>
<rawString>K. Burton, A. Java, and I. Soboroff. 2009. The icwsm 2009 spinn3r dataset. In Proceedings of the Annual Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxiu Chen</author>
<author>Donghong Ji</author>
<author>Chew Lim Tan</author>
<author>Zhengyu Niu</author>
</authors>
<title>Unsupervised feature selection for relation extraction.</title>
<date>2005</date>
<booktitle>In IJCNLP-05: The 2nd International Joint Conference on Natural Language Processing.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="5314" citStr="Chen et al., 2005" startWordPosition="856" endWordPosition="859">me relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesquita et al. extended this work by using other features such as part of speech patterns (Mesquita et al., 2010). To reduce noise in the feature space, a common problem with text mining, known feature selection and ranking methods for clustering have been applied (Chen et al., 2005; Rosenfeld and Feldman, 2007). Both works used the K-Means clustering algorithm with the stability-based criterion to automatically estimate the number of clusters. This work extends all previous clustering works by utilizing domain frequency as a novel weighting scheme for clustering entity pairs. The idea of domain frequency was first proposed for predicting entities which are erroneously typed by NER systems (Merhav et al., 2010). 3 Data and Evaluation This work was implemented on top of the SONEX system (Mesquita et al., 2010), deployed on the ICWSM 2009 Spinn3r corpus (Burton et al., 200</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2005</marker>
<rawString>Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2005. Unsupervised feature selection for relation extraction. In IJCNLP-05: The 2nd International Joint Conference on Natural Language Processing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Michael Schmitz</author>
<author>Robert Bart</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction. Manuscript submitted for publication.</title>
<date>2011</date>
<institution>University of Washington.</institution>
<contexts>
<context position="4501" citStr="Fader et al., 2011" startWordPosition="713" endWordPosition="716"> where the domain is also PER–PER. The more dominant a term in a given domain compared to other domains, the higher its boosting score would be. Our work resembles the work on selectional preferences (Resnik, 1996). Selectional preferences are semantic constraints on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesquita et al. extended this work by using other features such as part</context>
</contexts>
<marker>Fader, Schmitz, Bart, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. Manuscript submitted for publication. University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>415</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1964" citStr="Hasegawa et al., 2004" startWordPosition="302" endWordPosition="305">s exist in developing an Open IE solution, such as recognizing and disambiguating entities in a multi-document setting, and identifying all so-called relational terms 1This thesis proposal has been accepted for publication in (Merhav et al., 2012). 60 in the sentences connecting pairs of entities. Relational terms are words (usually one or two) that describe a relation between entities (for instance, terms like “running mate”, “opponent”, “governor of” are relational terms). One approach for Open IE is based on clustering of entity pairs to produce relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004). Their and follow-up works (e.g., (Mesquita et al., 2010)) extract terms in a small window between two named entities to build the context vector of each entity pair, and then apply a clustering algorithm to cluster together entity pairs that share the same relation (e.g., Google–Youtube and Google– Motorola Mobility in a cluster about the “acquired” relation). Contexts of entity pairs are represented using the vector space model. The state-ofthe-art in clustering-based Open IE assigns weights to the terms according to the standard tf·idf scheme. Motivation. Intuitively, the justification for</context>
<context position="4767" citStr="Hasegawa et al., 2004" startWordPosition="758" endWordPosition="761">s on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesquita et al. extended this work by using other features such as part of speech patterns (Mesquita et al., 2010). To reduce noise in the feature space, a common problem with text mining, known feature selection and ranking methods for clustering have been applied (Chen et al., 2005; Rosenfeld and Feldman, 2007). Both works used the K</context>
<context position="7187" citStr="Hasegawa et al., 2004" startWordPosition="1158" endWordPosition="1161"> those from Spinn3r and Freebase that match both entity names and types exactly), while 10PERC contains 10% of the total pairs SONEX identified, including the intersection pairs. We extended these two datasets by adding more entity pairs and relations. We call the resulting datasets INTER (395 entity pairs and 20 different relations) and NOISY (contains INTER plus approximately 30,000 entity pairs as compared to the 13,000 pairs in 10PERC ). We evaluate our system by reporting f-measure numbers for our system running on INTER and NOISY against the ground truth, using similar settings used by (Hasegawa et al., 2004) and (Mesquita et al., 2010). These include word unigrams as features, HAC with average link (outperformed single and complete link), and tf ·idf and cosine similarity as the baseline. 4 Weighting Scheme Identifying the relationship (if any) between entities e1, e2 is done by analyzing the sentences that mention e1 and e2 together. An entity pair is defined by two entities e1 and e2 together with the context in 2http://gate.ac.uk/ 3http://www.freebase.com 61 which they co-occur. For our purposes, the context can be any textual feature that allows the identification of the relationship for the </context>
<context position="14118" citStr="Hasegawa et al., 2004" startWordPosition="2395" endWordPosition="2398">t have high df scores only because they are rare and appear only within one domain (e.g., ubiquitious (misspelled in source) and uploader in this example). As we shall see in the experimental evaluation, even in the presence of incorrect type assignments made by the NER tool, the use of df scores improves the accuracy significantly. It is also worth mentioning that computing the df scores can be done fairly efficiently, and as soon as all entity pairs are extracted. 5 Results We now report the results on INTER and NOISY. Our baseline run is similar to the systems published by Hasegawa et al. (Hasegawa et al., 2004) and Mesquita et al. (Mesquita et al., 2010); that is HAC with average link using tf •idf and cosine similarity, and stemmed word unigrams (excluding stop words) as features extracted using a window size of five words between pair of entities. Figure 2 shows that by integrating domain frequency 63 1 Domain Freq. 0.8 0.6 0.4 0.2 0 1 Domain Freq. 0.8 0.6 0.4 0.2 0 1 Domain Freq. 0.8 0.6 0.4 0.2 0 1 Domain Freq. 0.8 0.6 0.4 0.2 0 (a) mayor. (c) CEO. (b) wife. (d) coach. Figure 1: Domain Frequency examples. (df) we significantly outperformed this baseline on both datasets (INTER: F-1 score of 0.87</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from large corpora. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 415, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Merhav</author>
<author>Filipe Mesquita</author>
<author>Denilson Barbosa</author>
<author>Wai Gen Yee</author>
<author>Ophir Frieder</author>
</authors>
<title>Incorporating global information into named entity recognition systems using relational context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’10,</booktitle>
<pages>883--884</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5751" citStr="Merhav et al., 2010" startWordPosition="923" endWordPosition="926">et al., 2010). To reduce noise in the feature space, a common problem with text mining, known feature selection and ranking methods for clustering have been applied (Chen et al., 2005; Rosenfeld and Feldman, 2007). Both works used the K-Means clustering algorithm with the stability-based criterion to automatically estimate the number of clusters. This work extends all previous clustering works by utilizing domain frequency as a novel weighting scheme for clustering entity pairs. The idea of domain frequency was first proposed for predicting entities which are erroneously typed by NER systems (Merhav et al., 2010). 3 Data and Evaluation This work was implemented on top of the SONEX system (Mesquita et al., 2010), deployed on the ICWSM 2009 Spinn3r corpus (Burton et al., 2009), focusing on posts in English (25 million out of 44 million in total), collected between August 1st, 2008 and October 1st, 2008. The system uses the Illinois Entity Tagger (Ratinov and Roth, 2009) and Orthomatcher from the GATE framework2 for withina-document co-reference resolution. Evaluating Open IE systems is a difficult problem. Mesquita et al. evaluated SONEX by automatically matching a sample of the entity pairs their syste</context>
<context position="9501" citStr="Merhav et al., 2010" startWordPosition="1561" endWordPosition="1564">–ORG (recall that these are the intervening features between the pair for each co-occurrence in the entire dataset). For clarity the terms were not stemmed. Clearly, most terms are irrelevant which make it difficult to cluster the pair correctly. We listed in bold all terms that we think are useful. Besides “belongs”, all these terms have high domain frequency scores. However, most of these terms do not have high idf scores. Term frequencies within a pair are also not helpful in many cases since many pairs are mentioned only a few times in the text. Next, we define the domain frequency score (Merhav et al., 2010). Definition. Let P be the set of entity pairs, let T be the set of all entity types, and let D = T x T be the set of all possible relation domains. The domain frequency (df ) of a term t, appearing in the context of some entity pair in P, in a given relation domain i E D, denoted df�(t), is defined as df�(t) = fi(t) E1&lt;j&lt;n fj(t)� where fi(t) is the frequency with which term t appears in the context of entity pairs of domain i E D, and n is the number of domains in D. When computing the df score for a given term, it is preferred to consider each pair only once. For example, “Google[ORG] acquir</context>
</contexts>
<marker>Merhav, Mesquita, Barbosa, Yee, Frieder, 2010</marker>
<rawString>Yuval Merhav, Filipe Mesquita, Denilson Barbosa, Wai Gen Yee, and Ophir Frieder. 2010. Incorporating global information into named entity recognition systems using relational context. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’10, pages 883–884, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Merhav</author>
<author>Filipe Mesquita</author>
<author>Denilson Barbosa</author>
<author>Wai Gen Yee</author>
<author>Ophir Frieder</author>
</authors>
<title>Extracting information networks from the blogosphere.</title>
<date>2012</date>
<journal>ACM Transactions on the Web (TWEB). Accepted</journal>
<contexts>
<context position="1589" citStr="Merhav et al., 2012" startWordPosition="239" endWordPosition="242">Natural Language Processing which has been re-invigorated with the everincreasing availability of user-generated textual content online. The large-scale extraction of unknown relations has been termed as Open Information Extraction (Open IE) (Banko et al., 2007) (also referred to as Open Relationship Extraction, Relation Extraction, or Relation Discovery). Many challenges exist in developing an Open IE solution, such as recognizing and disambiguating entities in a multi-document setting, and identifying all so-called relational terms 1This thesis proposal has been accepted for publication in (Merhav et al., 2012). 60 in the sentences connecting pairs of entities. Relational terms are words (usually one or two) that describe a relation between entities (for instance, terms like “running mate”, “opponent”, “governor of” are relational terms). One approach for Open IE is based on clustering of entity pairs to produce relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004). Their and follow-up works (e.g., (Mesquita et al., 2010)) extract terms in a small window between two named entities to build the context vector of each entity pair, and then apply a clustering algorithm to cluster together</context>
</contexts>
<marker>Merhav, Mesquita, Barbosa, Yee, Frieder, 2012</marker>
<rawString>Yuval Merhav, Filipe Mesquita, Denilson Barbosa, Wai Gen Yee, and Ophir Frieder. 2012. Extracting information networks from the blogosphere. ACM Transactions on the Web (TWEB). Accepted 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filipe Mesquita</author>
<author>Yuval Merhav</author>
<author>Denilson Barbosa</author>
</authors>
<title>Extracting information networks from the blogosphere: State-of-the-art and challenges.</title>
<date>2010</date>
<booktitle>In 4th Int’l AAAI Conference on Weblogs and Social Media–Data Challenge.</booktitle>
<contexts>
<context position="2022" citStr="Mesquita et al., 2010" startWordPosition="311" endWordPosition="314">zing and disambiguating entities in a multi-document setting, and identifying all so-called relational terms 1This thesis proposal has been accepted for publication in (Merhav et al., 2012). 60 in the sentences connecting pairs of entities. Relational terms are words (usually one or two) that describe a relation between entities (for instance, terms like “running mate”, “opponent”, “governor of” are relational terms). One approach for Open IE is based on clustering of entity pairs to produce relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004). Their and follow-up works (e.g., (Mesquita et al., 2010)) extract terms in a small window between two named entities to build the context vector of each entity pair, and then apply a clustering algorithm to cluster together entity pairs that share the same relation (e.g., Google–Youtube and Google– Motorola Mobility in a cluster about the “acquired” relation). Contexts of entity pairs are represented using the vector space model. The state-ofthe-art in clustering-based Open IE assigns weights to the terms according to the standard tf·idf scheme. Motivation. Intuitively, the justification for using idf is that a term appearing in many documents (i.e</context>
<context position="5144" citStr="Mesquita et al., 2010" startWordPosition="826" endWordPosition="829">cus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesquita et al. extended this work by using other features such as part of speech patterns (Mesquita et al., 2010). To reduce noise in the feature space, a common problem with text mining, known feature selection and ranking methods for clustering have been applied (Chen et al., 2005; Rosenfeld and Feldman, 2007). Both works used the K-Means clustering algorithm with the stability-based criterion to automatically estimate the number of clusters. This work extends all previous clustering works by utilizing domain frequency as a novel weighting scheme for clustering entity pairs. The idea of domain frequency was first proposed for predicting entities which are erroneously typed by NER systems (Merhav et al.</context>
<context position="7215" citStr="Mesquita et al., 2010" startWordPosition="1163" endWordPosition="1166">base that match both entity names and types exactly), while 10PERC contains 10% of the total pairs SONEX identified, including the intersection pairs. We extended these two datasets by adding more entity pairs and relations. We call the resulting datasets INTER (395 entity pairs and 20 different relations) and NOISY (contains INTER plus approximately 30,000 entity pairs as compared to the 13,000 pairs in 10PERC ). We evaluate our system by reporting f-measure numbers for our system running on INTER and NOISY against the ground truth, using similar settings used by (Hasegawa et al., 2004) and (Mesquita et al., 2010). These include word unigrams as features, HAC with average link (outperformed single and complete link), and tf ·idf and cosine similarity as the baseline. 4 Weighting Scheme Identifying the relationship (if any) between entities e1, e2 is done by analyzing the sentences that mention e1 and e2 together. An entity pair is defined by two entities e1 and e2 together with the context in 2http://gate.ac.uk/ 3http://www.freebase.com 61 which they co-occur. For our purposes, the context can be any textual feature that allows the identification of the relationship for the given pair. The contexts of </context>
<context position="14162" citStr="Mesquita et al., 2010" startWordPosition="2403" endWordPosition="2406">rare and appear only within one domain (e.g., ubiquitious (misspelled in source) and uploader in this example). As we shall see in the experimental evaluation, even in the presence of incorrect type assignments made by the NER tool, the use of df scores improves the accuracy significantly. It is also worth mentioning that computing the df scores can be done fairly efficiently, and as soon as all entity pairs are extracted. 5 Results We now report the results on INTER and NOISY. Our baseline run is similar to the systems published by Hasegawa et al. (Hasegawa et al., 2004) and Mesquita et al. (Mesquita et al., 2010); that is HAC with average link using tf •idf and cosine similarity, and stemmed word unigrams (excluding stop words) as features extracted using a window size of five words between pair of entities. Figure 2 shows that by integrating domain frequency 63 1 Domain Freq. 0.8 0.6 0.4 0.2 0 1 Domain Freq. 0.8 0.6 0.4 0.2 0 1 Domain Freq. 0.8 0.6 0.4 0.2 0 1 Domain Freq. 0.8 0.6 0.4 0.2 0 (a) mayor. (c) CEO. (b) wife. (d) coach. Figure 1: Domain Frequency examples. (df) we significantly outperformed this baseline on both datasets (INTER: F-1 score of 0.87 compared to 0.75; NOISY: F-1 score of 0.72 </context>
</contexts>
<marker>Mesquita, Merhav, Barbosa, 2010</marker>
<rawString>Filipe Mesquita, Yuval Merhav, and Denilson Barbosa. 2010. Extracting information networks from the blogosphere: State-of-the-art and challenges. In 4th Int’l AAAI Conference on Weblogs and Social Media–Data Challenge.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data. In</title>
<date>2009</date>
<booktitle>ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4457" citStr="Mintz et al., 2009" startWordPosition="705" endWordPosition="708">wife”, “spouse”, etc.) in those entity pairs where the domain is also PER–PER. The more dominant a term in a given domain compared to other domains, the higher its boosting score would be. Our work resembles the work on selectional preferences (Resnik, 1996). Selectional preferences are semantic constraints on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesquita et al. extended th</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003–1011, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In CoNLL ’09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6113" citStr="Ratinov and Roth, 2009" startWordPosition="986" endWordPosition="989">extends all previous clustering works by utilizing domain frequency as a novel weighting scheme for clustering entity pairs. The idea of domain frequency was first proposed for predicting entities which are erroneously typed by NER systems (Merhav et al., 2010). 3 Data and Evaluation This work was implemented on top of the SONEX system (Mesquita et al., 2010), deployed on the ICWSM 2009 Spinn3r corpus (Burton et al., 2009), focusing on posts in English (25 million out of 44 million in total), collected between August 1st, 2008 and October 1st, 2008. The system uses the Illinois Entity Tagger (Ratinov and Roth, 2009) and Orthomatcher from the GATE framework2 for withina-document co-reference resolution. Evaluating Open IE systems is a difficult problem. Mesquita et al. evaluated SONEX by automatically matching a sample of the entity pairs their system identified from the Spinn3r corpus against a publicly available curated database3. Their approach generated two datasets: INTER and 10PERC. INTER contains the intersection pairs only (i.e., intersection pairs are those from Spinn3r and Freebase that match both entity names and types exactly), while 10PERC contains 10% of the total pairs SONEX identified, inc</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL ’09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: an information-theoretic model and its computational realization.</title>
<date>1996</date>
<contexts>
<context position="4096" citStr="Resnik, 1996" startWordPosition="650" endWordPosition="651">d have relatively lower idf weights. It is natural to expect that the relations extracted by an Open IE system are strongly correlated with a given context. For instance, marriage is a relation between two persons and thus belongs to the domain PER–PER. We exploit this observation to boost the weight of relational terms associated with marriage (e.g., “wife”, “spouse”, etc.) in those entity pairs where the domain is also PER–PER. The more dominant a term in a given domain compared to other domains, the higher its boosting score would be. Our work resembles the work on selectional preferences (Resnik, 1996). Selectional preferences are semantic constraints on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the s</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional constraints: an information-theoretic model and its computational realization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
</authors>
<title>Understanding inverse document frequency: On theoretical arguments for idf.</title>
<date>2004</date>
<journal>Journal of Documentation,</journal>
<pages>60--2004</pages>
<contexts>
<context position="2706" citStr="Robertson, 2004" startWordPosition="422" endWordPosition="423">d the context vector of each entity pair, and then apply a clustering algorithm to cluster together entity pairs that share the same relation (e.g., Google–Youtube and Google– Motorola Mobility in a cluster about the “acquired” relation). Contexts of entity pairs are represented using the vector space model. The state-ofthe-art in clustering-based Open IE assigns weights to the terms according to the standard tf·idf scheme. Motivation. Intuitively, the justification for using idf is that a term appearing in many documents (i.e., many contexts in our setting) would not be a good discriminator (Robertson, 2004), and thus should weigh proportionally less than other, more rare terms. For the task of relation extraction however, we are interested specifically in terms that describe relations. In our settings, a single document is a context vector of one entity pair, generated from all articles discussing this pair, which means that the fewer entity pairs a term appears in, the higher its idf score would be. Consequently, it is not necessarily the case that terms that are associated with high idf weights would be good relation discriminators. On the other hand, popular relational terms that apProceeding</context>
</contexts>
<marker>Robertson, 2004</marker>
<rawString>Stephen Robertson. 2004. Understanding inverse document frequency: On theoretical arguments for idf. Journal of Documentation, 60:2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Clustering for unsupervised relation identification.</title>
<date>2007</date>
<booktitle>In CIKM ’07,</booktitle>
<pages>411--418</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5344" citStr="Rosenfeld and Feldman, 2007" startWordPosition="860" endWordPosition="863">troduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Clustering (HAC) with complete linkage deployed on a 1995 New York Times corpus. Mesquita et al. extended this work by using other features such as part of speech patterns (Mesquita et al., 2010). To reduce noise in the feature space, a common problem with text mining, known feature selection and ranking methods for clustering have been applied (Chen et al., 2005; Rosenfeld and Feldman, 2007). Both works used the K-Means clustering algorithm with the stability-based criterion to automatically estimate the number of clusters. This work extends all previous clustering works by utilizing domain frequency as a novel weighting scheme for clustering entity pairs. The idea of domain frequency was first proposed for predicting entities which are erroneously typed by NER systems (Merhav et al., 2010). 3 Data and Evaluation This work was implemented on top of the SONEX system (Mesquita et al., 2010), deployed on the ICWSM 2009 Spinn3r corpus (Burton et al., 2009), focusing on posts in Engli</context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman. 2007. Clustering for unsupervised relation identification. In CIKM ’07, pages 411–418, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Xiaojiang Liu</author>
<author>Bo Zhang</author>
<author>JiRong Wen</author>
</authors>
<title>Statsnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web, WWW ’09,</booktitle>
<pages>101--110</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4351" citStr="Zhu et al., 2009" startWordPosition="688" endWordPosition="691">R. We exploit this observation to boost the weight of relational terms associated with marriage (e.g., “wife”, “spouse”, etc.) in those entity pairs where the domain is also PER–PER. The more dominant a term in a given domain compared to other domains, the higher its boosting score would be. Our work resembles the work on selectional preferences (Resnik, 1996). Selectional preferences are semantic constraints on arguments (e.g. a verb like “eat” prefers as object edible things). 2 Related Work Different approaches for Open IE have been proposed in the literature, such as bootstrapping (e.g., (Zhu et al., 2009) (Bunescu and Mooney, 2007)), self or distant supervision (e.g., (Banko et al., 2007) (Mintz et al., 2009)) and rule based (e.g., (Fader et al., 2011)). In this work we focus on unsupervised approaches. Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al. (Hasegawa et al., 2004) (this is the system we use in this work as our baseline). Hasegawa et al. used word unigrams weighted by tf ·idf to build the context vectors and applied Hierarchical Agglomerative Cl</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and JiRong Wen. 2009. Statsnowball: a statistical approach to extracting entity relationships. In Proceedings of the 18th international conference on World wide web, WWW ’09, pages 101–110, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>