<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.986761">
Understanding Tables in Context Using Standard NLP Toolkits
</title>
<author confidence="0.993729">
Vidhya Govindaraju Ce Zhang Christopher R´e
</author>
<affiliation confidence="0.997179">
University of Wisconsin-Madison
</affiliation>
<email confidence="0.973601">
{vidhya, czhang, chrisre}@cs.wisc.edu
</email>
<sectionHeader confidence="0.938103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990785">
Tabular information in text documents
contains a wealth of information, and
so tables are a natural candidate for in-
formation extraction. There are many
cues buried in both a table and its sur-
rounding text that allow us to under-
stand the meaning of the data in a ta-
ble. We study how natural-language
tools, such as part-of-speech tagging,
dependency paths, and named-entity
recognition, can be used to improve the
quality of relation extraction from ta-
bles. In three domains we show that (1)
a model that performs joint probabilis-
tic inference across tabular and natural
language features achieves an F1 score
that is twice as high as either a pure-
table or pure-text system, and (2) us-
ing only shallower features or non-joint
inference results in lower quality.
</bodyText>
<sectionHeader confidence="0.98608" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810372881356">
Tabular data is ubiquitous and often contains
high-quality, structured relational data. Re-
cent studies found billions of high-quality re-
lations on the web in HTML (Cafarella et
al., 2008). In financial applications, a huge
amount of data is buried in the tables of cor-
porate filings and earnings reports; in science,
millions of journal articles contain billions of
scientific facts in tables. Although tables de-
scribe precise, structured relations, tables are
rarely written in a way that is self-describing,
e.g., tables may contain abbreviations or only
informal schema information; in turn, the con-
tents of tables are often ambiguously specified,
which makes extracting the relations implicit
in tabular data difficult.
Tables are, however, not written in isola-
tion. The text surrounding a table in a jour-
nal article explains its contents to its intended
audience, a human reader. For example, in
a simple study, we demonstrate that humans
can achieve more than 60% higher recall by
jointly reading the text and tables in a journal
article than by only looking at the tables. The
conclusion of this experiment is not surprising,
but it raises a question: How should a system
combine tabular and natural-language features
to understand tables in text?
The literature provides a broad spectrum of
answers to this question. Most previous ap-
proaches use textual or tabular features sepa-
rately, e.g., tabular approaches that do not use
text features (Dalvi et al., 2012; Wu and Lee,
2006; Pinto et al., 2003) or textual approaches
that do not use tabular features (Mintz et al.,
2009; Wu and Weld, 2010; Poon and Domin-
gos, 2007). In a prescient study, Liu et al.
(2007) proposed to learn the target relation in-
dependently from both table and surface tex-
tual features, and then combine the result us-
ing a linear combination of the predictions.
In a similar spirit, we propose to use both
types of features in our approach of relation
extraction. Our proposed approach differs
from prior approaches in two ways: (1) We
use deeper–but standard–NLP features than
prior approaches for table extraction. In con-
trast to the shallow, lexical features that prior
approaches have used, we use standard NLP
features, such as dependency paths, parts of
speech, etc. Our hypothesis is that a deeper
understanding of the text in which a table is
embedded will lead to higher quality table ex-
traction. (2) Our probabilistic model jointly
uses both tabular and textual features. One
advantage of a joint approach is that one can
predict portions of the complicated predicate
that is buried in a table. For example, in a ge-
ology journal article, we may read a measure-
</bodyText>
<note confidence="0.859757666666667">
658
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 658–664,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.998972850746269">
Data Joint Inference Results
1 Input
trace el
Table
Location Zhunuo
Sample ZM-1 ZM-2
SiO2 66.69 65.96
Context 064 Text
Zhunuo
Fe2O3 (ZN),
4.7 30
which
lithologically
MnO 002 0.06
corresp-­‐
ondMgOto granodiori7c
1.75 1.5
and a grani7c...
Na O 41
st-collision adakitic rock
Sample_Loca7on
SampleID Location
unuo Sample_RockType
ZM6 7 ZM8 ZM9 ZM
ZM-­‐1
ZM
Zh
5
...
9 0.
ZM-­‐1
40 0.45
Granodi
0 0.57 0.46
Loca7on_RockType
-­‐
35 1
orite
.5 .
Location 406 318431
RockType
8 0.08
Zhunuo
Granodi-­‐
0.04 0.0
orite
1.20
8 ...
04 ...
...
... 3
...SampleID RockType
09 6 7156 673 6937
Precision
3 3.78 Recall
377 F-­‐1
Text-­‐only Table-­‐only Whole document
1
0.8
azon
0.6
.6 0.4
45
.46 0.2
06
0
.
</figure>
<figureCaption confidence="0.98441">
Figure 1: An example of joint inference be-
</figureCaption>
<figure confidence="0.821128166666667">
Sum 99 99.8 99.6 99.7 9.7 99.7 97 99.8 99.7 99.8 Figure 3: Human quality to extract Sample-
99.8 99 00.3 99.9 98.7 90
9 5 1 . 5 7 1 5
tween a table and its context.
Sc 831 5.2 855 76 5.72 8.65 Rocktype relations in PetDB.
8.96 7.27 77 7.28 43 8.03
</figure>
<figureCaption confidence="0.909736">
Figure 2: Job assignments for the human
</figureCaption>
<equation confidence="0.916908">
Ho 0.4 034 487 0.43 0.41 0.4 0.3 0.23 0.29 0.35
11
study.
Tm
me
m
</equation>
<bodyText confidence="0.931698947368421">
where this rock was unearthed and in what ge-
g/Mg+
ological time interval this rock appeared may
not bemspecified in the table.
We consider tasks inrthree do
PETROLOGY,rFiNANCE,nand GEOlOGy.
each domain, we build a system to extract re-
lationsbefrom text, tables,yorlboth. We found
that a jointiinference system that uses
shallow, butostandardhNLP features can sig-
nificantly improve the0quality of the extracted
relations,oand that this resultseholdswconsis
acrosshall three domains. For example, in our
Petrology application to extract a know
base,vrcalled PETDB1, by usingninformation
extracted from both text and tables, w
achieve twiceCas high F1acomparedgtoOeither a
pure-table or pure-text system.
that
</bodyText>
<sectionHeader confidence="0.317094" genericHeader="method">
2 Motivating Human Study
</sectionHeader>
<figureCaption confidence="0.856061">
Figure 4: List of features we used in Text,
</figureCaption>
<bodyText confidence="0.956994703703704">
TAblE, and JOINt approaches. NER, EL,
and RE refer to named-entity recognition, en-
tity linking, and relation extraction, respec-
tively.
valuable in-
videnon-
formation even for a human r
fore, an ideal machine readin
also try to capture similar information.
We asked three geoscientists to manually
read journal articles and extract relations
for the PETROLOGY domain. We report
lation, Sample-
our results for the target re
RocKType, which associates a rock type with
a rock sample (see Figure 1 for an example).
We randomly sampled 21 journal articles. For
each journal article, we produced three vari-
ants: (1) the original document; (2) table-
only, which is the set of tables in the docu-
ment (without the text); (3) text-only, which
is the text of the document with the tables
removed from the document. Each geoscien-
tist was asked to read and extract the relations
from one of the three variants. We then judged
the precision and recall of their extraction, as
shown in Figure 2.
</bodyText>
<figure confidence="0.998836423728814">
Wh
r
Table-only
Nb
Text-only
La
C
Rbole doc.
232 021
631
1 9
. . . Geoscien/st 117 11 1 108 136 130 Geoscien/st 2 76 100 107 Geoscien/st 3
...
63 ...
...
715
7
7 6
88
988
58
...
...
9. ...
5314
669
14
1489
15
15
6
15
4 7
...
... 1
21
21
77
21
5
Task
17.1 TExT
11.6 TAblE
12.9 JOINT
POS42.2 tags
834
Bing9.83 query
1042 results 9
Freebase
37.5 3
6.19 Parser
6.25
Stanford
Dependency
4.64 4.50 path4
Term0.390 0.397
proximity
Word 0504 0511
sequence
Pd;otable
46.7
887
Bing993query1070
results
Freebase
37.2
Table 4.38 headers
463
Table0.374 0.414
subheaders
2.7 2
RE of 091 neighbor rows
0530
Subjec9ve men9ons
in the sentence near
a table
.
825 NER
ry3
1205
columns 12
1025 tags
POS 9
Stanford
12.012.1
NER
Regular
127 136
Expression
127
Dic9ona
883
366
pd;otable
950 9
NER11.6of neighbor
129 cells
Regular
139 expression
149
Dic9onary
8
360
#
Whether a men9on
in table also appears
in the text.
EL20
RE2.
Join between
rela9ons (See Figure
1 for an example)
in a table that9tells us the type of rock
d its weight—but5data such as9the location
an g
mains: esishthat wegwant toe validate
For
surrounding pro
</figure>
<bodyText confidence="0.999922833333333">
As shown in Figure 3, human readers not
surprisingly achieve perfect precision on each
of the variants, but lower recall on both
the table-only and text-only variants. How-
ever, summing the recall of table-only (60%)
and text-only (20%) variants together would
achieve only 80% recall; this implies that in
the best case more than 20% of the extrac-
tions require that the human reader read the
table and its surrounding text jointly. Figure 1
shows one representative example.
This motivates our approach, which uses a
joint inference system to model features from
a table and its surrounding text. We also pro-
pose to use deep linguistic features instead of
shallower features to get as close as possible to
the ability of human readers in understanding
the surrounding text of a table.
</bodyText>
<sectionHeader confidence="0.958848" genericHeader="method">
3 Empirical Study &amp; Experiments
</sectionHeader>
<bodyText confidence="0.999984083333333">
We describe our experiments to test the hy-
pothesis that (1) deeper linguistic features can
help to extract higher quality relations from
tables, and (2) joint inference across tables and
text improves extraction quality compared to
approaches that use pure-table, pure-text, and
non-joint ways of combining these two. We
briefly describe some experiments for a dataset
that we call GEOLOGY (Zhang et al., 2013).
The detailed experimental results in all three
domains are in the technical report version of
this paper.
</bodyText>
<subsectionHeader confidence="0.984435">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.996840956521739">
We consider the task of constructing a geol-
ogy knowledge base. Specifically, our goal is
to extract a ROCK-TOTALORGANICCARBON
relation that maps rock formations (e.g., “Bar-
nett Formation”) to their total organic carbon
(e.g., “6%”). Such data is important for es-
timating stored energy and for global climate
research.
Dataset. We selected 100 geology journal
articles.2 We asked three geoscientists to an-
notate these journal articles manually to ex-
tract the ROCK-TOTALORGANICCARBON re-
lation (1.5K tuples). We processed each doc-
ument using Stanford CoreNLP (de Marneffe
et al., 2006; Toutanova and Manning, 2000),
2We choose a set of documents that (1) are in En-
glish, and (2) contain at least one table.
PDFtoHTML3, and pdf2table (Yildiz, 2004).
We then extracted features following state-of-
the-art practices (see Figure 4).
Approaches. To validate our hypothesis,
we implement four systems, each of which has
access to different types of data:
</bodyText>
<listItem confidence="0.904295714285714">
(1) TABLE. This approach follows Pinto et
al. (2003) and Dalvi et al. (2012) and only uses
the tables in a document.
(2) TEXT. This approach only has access to
the text in a document and contains all the fea-
tures mentioned in Wu and Weld (2010) and
Mintz et al. (2009).
</listItem>
<bodyText confidence="0.9984534">
The features used in (1) and (2) are shown in
Figure 4. In both TABLE and TEXT, we use a
conditional random field (Lafferty et al., 2001)
model for the ROCK-TOTALORGANICCARBON
relation.
</bodyText>
<listItem confidence="0.9181025">
(3) MERGE. Using TABLE and TEXT, we
extract all facts and their associated probabil-
ity. Following Duin (2002), we combine these
two probabilities using a linear combination.
MERGE is a baseline approach that uses infor-
mation from both tables and text.
(4) JOINT. We build a joint approach that
uses information from both tables and text.
</listItem>
<bodyText confidence="0.999449333333333">
This approach is a large factor graph in which
we embed the CRFs developed in TABLE and
TEXT. Additionally, we allow JOINT to pre-
dict projections of each relation, as shown in
Figure 4. Recall that a key advantage of a joint
approach is that we do not need to predict all
arguments of the relation (if such a prediction
is unwarranted from the data). The inference
is done by Gibbs sampling using our inference
engine ELEMENTARY (Zhang and R´e, 2013).
We describe the JOINT system in more detail
in the technical report version of this paper.
</bodyText>
<subsectionHeader confidence="0.992277">
3.2 End-to-End Quality
</subsectionHeader>
<bodyText confidence="0.9994295">
We were able to validate that JOINT achieves
higher quality than the other three approaches
we considered. Figure 5 shows the P/R curve
of different approaches on three domains. We
analyzed the domain GEOLOGY.
JOINT dominates all other approaches. At
a recall of 10%, JOINT achieves 3x higher pre-
cision than all other approaches. In our error
analysis, we saw that tables in geology articles
often contain ambiguous words; for example,
</bodyText>
<figure confidence="0.997611114285714">
3http://pdftohtml.sourceforge.net/
660
Precision
0.8
0.6
0.4
0.2
0
1
Text
Joint
Table
Merge
0.8
0.6
0.4
0.2
0
1
Text Table
Merge
Joint
0.8
0.6
0.4
0.2
0
1
Joint
Merge
Table
Text
0 0.1 0.2 0.3 0 0.2 0.4 0 0.1 0.2
Recall Recall Recall
(a) Geology Domain (b) Petrology Domain (c) Finance Domain
</figure>
<figureCaption confidence="0.97191">
Figure 5: End-to-end extraction quality on PETROLOGY, FINANCE, and GEODEEPDIVE. The
recall is limited by the quality of state-of-the-art table recognition software on PDFs.
</figureCaption>
<bodyText confidence="0.999828464285714">
the word “Barnett” in a table may refer to ei-
ther a location or a rock formation. By using
features extracted from text, JOINT achieves
higher precision. For recall in the range of 0–
10%, MERGE outperforms both TEXT and TA-
BLE, with 3%–90% improvement in precision.
In GEOLOGY, MERGE has precision that is
similar to TEXT and TABLE for the higher re-
call range (&gt;10%). In this domain, we found
that relations that appeared in the text often
repeated relations described in the table. In
other domains, such as PETROLOGY, where
the relations in text and tables have lower de-
grees of overlap, MERGE significantly improves
over TEXT and TABLE (Figure 5(b)).
We conducted a statistical significance test
to check whether the improvement of JOINT
over the three other approaches is statistically
significant. For each of the three probability
thresholds, t E {.99, .90, .50}, we created the
set of predictions that JOINT assigns probabil-
ity greater than t. Figure 6 shows the results
of the statistical significance test in which the
null hypothesis is that the F1 scores of two ap-
proaches are the same. With p = 0.01, JOINT
has statistically significant improvement of F1
score over all three other approaches with each
probability threshold.
</bodyText>
<subsectionHeader confidence="0.998502">
3.3 Shallow vs. Linguistic Features
</subsectionHeader>
<bodyText confidence="0.999349875">
We validate the hypothesis that using
linguistic features, e.g., part-of-speech
tags (Toutanova and Manning, 2000),
named-entity tags (Finkel et al., 2005), and
dependency trees (de Marneffe et al., 2006),
helps improve the quality of our approach,
called JOINT. There are different ways to
use shallow and linguistic features; we select
</bodyText>
<figure confidence="0.7015065">
Approaches \ Prob. .99 .90 .50
TEXT + + +
TABLE + + +
MERGE + + +
</figure>
<figureCaption confidence="0.884428">
Figure 6: Approximate randomization test
</figureCaption>
<bodyText confidence="0.907618571428571">
from Chinchor (1992) of F1 score with p =
0.01 on the impact of joint inference compared
with pure-table or pure-text approaches for
different probability thresholds. A + sign in-
dicates that the F1 score of joint approach in-
creased significantly.
Linguistic POS tags (Wu et al., 2010)
</bodyText>
<note confidence="0.9282735">
Stanford NER tags (Mintz et al., 2009)
Dependence trees (Mintz et al., 2009)
</note>
<figureCaption confidence="0.999819">
Figure 7: Types of Features.
</figureCaption>
<bodyText confidence="0.99469225">
state-of-the-art approaches from the literature
(see Figure 7).
We created the following variants of JOINT.
JOINT(-PARSE) removes features generated by
the dependency parser and syntax parser.
Similarly, JOINT(-NER) (JOINT(-POS)) removes
all features related to NER (resp. POS).
JOINT(-POS) also removes NER and parser fea-
tures because the latter two are dependent on
POS features.
Figure 8 shows the P/R curve for all
these variants on GEOLOGY, and Figure 9
shows the results of statistical significance
test. For probability threshold .90, JOINT
outperforms JOINT(-POS) significantly. The
difference between JOINT, JOINT(-PARSE),
</bodyText>
<figure confidence="0.9016798">
Type Features
Shallow Regular Expressions (Dalvi et al., 2012)
Term proximity (Matsuo et al., 2003)
Dictionary and Freebase (Mintz et al., 2009)
661
</figure>
<figureCaption confidence="0.894935">
Figure 8: Lesion study of different features for
GEOLOGY.
</figureCaption>
<figure confidence="0.70662875">
Features \ Prob. .90 .50
JOINT(-PARSE) → JOINT 0 +
JOINT(-NER) →JOINT 0 +
JOINT(-POS) → JOINT + +
</figure>
<figureCaption confidence="0.945488">
Figure 9: Approximate randomization test of
</figureCaption>
<bodyText confidence="0.9654304">
F1 score with p = 0.01 on the impact of lin-
guistic features. For x → y, a + indicates that
the F1 score of y is significantly higher than x.
0 indicates that the F1 score does not change
significantly.
and JOINT(-NER) is not significant because
there are “easy-to-extract” facts in the high-
probability range. For probability threshold
.50, JOINT outperforms all three other vari-
ants significantly.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999984931818182">
The intuition that context features might help
table-related tasks has existed for decades. For
example, Hurst and Nasukawa (2000) men-
tioned (as future work) that context features
could be used to further improve their relation
extraction approaches from tables. Lin et al.
(2010) use bag-of-words features and hyper-
links to recommend new columns for web ta-
bles. Liu et al. (2007) extract features, includ-
ing font size and title, from PDF documents in
which a table appears to help the table rank-
ing task. They find that these features only
contribute less than 2% to precision. In con-
trast, in our approach linguistic features are
quite useful. The above approaches use con-
text features that can be extracted without
POS tagging or linguistic parsing. One aspect
of our work is to demonstrate that traditional
NLP tools can enhance the quality of table ex-
traction.
Extracting information from tables has been
discussed by different communities in the last
decade, including NLP (Wu and Lee, 2006;
Tengli et al., 2004; Chen et al., 2000), artifi-
cial intelligence (Fang et al., 2012; Pivk, 2006),
information retrieval (Wei et al., 2006; Pinto
et al., 2003), database (Cafarella et al., 2008),
and the web (Dalvi et al., 2012). This body of
work considers only features derived from ta-
bles and does not examine richer NLP features
as we do.
While joint inference is popular, it is not
clear when a joint inference system outper-
forms a more traditional NLP pipeline. Re-
cent studies have reached a variety of conclu-
sions: in some, joint inference helps extraction
quality (McCallum, 2009; Poon and Domin-
gos, 2007; Singh et al., 2009); and in some,
joint inference hurts extraction quality (Poon
and Domingos, 2007; Eisner, 2009). Our intu-
ition is that joint inference is helpful in this ap-
plication because our joint inference approach
combines non-redundant signals (textual ver-
sus tabular).
</bodyText>
<sectionHeader confidence="0.992117" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991">
To improve the quality of extractions of tabu-
lar data, we use standard NLP techniques to
more deeply understand the text in which a
table is embedded. We validate that deeper
NLP features combined with a joint proba-
bilistic model has a statistically significant im-
pact on quality, i.e., recall and precision. Our
ongoing work is to apply these ideas to a much
larger corpus from each of the three domains.
</bodyText>
<sectionHeader confidence="0.99687" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.991837785714286">
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) DEFT Program under Air Force
Research Laboratory (AFRL) prime contract
No. FA8750-13-2-0039, the National Science
Foundation EAGER Award under No. EAR-
1242902 and CAREER Award under No. IIS-
1054009, and the Sloan Research Fellowship.
Any opinions, findings, and conclusion or rec-
ommendations expressed in this material are
those of the authors and do not necessarily re-
flect the view of DARPA, AFRL, NSF, or the
US government. We are also grateful to Jude
W. Shavlik for his insightful comments.
</bodyText>
<figure confidence="0.992003692307692">
1
Joint
Joint(-parse) Joint(-ner)
Joint(-pos)
0
0.1 0.2 0.3
Recall
Precision
0.8
0.6
0.4
0.2
662
</figure>
<sectionHeader confidence="0.559219" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993298467889908">
Michael J. Cafarella, Alon Halevy, Daisy Zhe
Wang, Eugene Wu, and Yang Zhang. 2008.
WebTables: Exploring the power of tables on the
web. Proceedings of VLDB Endowment, 1(1).
Hsin-Hsi Chen, Shih-Chung Tsai, and Jin-He Tsai.
2000. Mining tables from large scale HTML
texts. In Proceedings of the 18th Conference on
Computational Linguistics, COLING ’00.
Nancy Chinchor. 1992. The statistical significance
of the MUC-4 results. In Proceedings of the 4th
Conference on Message Understanding, MUC4
’92.
Bhavana Bharat Dalvi, William Cohen, and Jamie
Callan. 2012. WebSets: Extracting sets of en-
tities from the web using unsupervised infor-
mation extraction. In Proceedings of the 5th
ACM International Conference on Web Search
and Data Mining, WSDM ’12.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalua-
tion.
Robert Duin. 2002. The combining classifier: to
train or not to train? In 16th International
Conference on Pattern Recognition.
Jason Eisner. 2009. Joint models with missing
data for semi-supervised learning. In NAACL
HLT Workshop on Semi-supervised Learning for
Natural Language Processing.
Jing Fang, Prasenjit Mitra, Zhi Tang, and C. Lee
Giles. 2012. Table header detection and classi-
fication. In Proceedings of the 26th AAAI Con-
ference on Artificial Intelligence, AAAI ’12.
Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by
Gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ’05.
Matthew Hurst and Tetsuya Nasukawa. 2000.
Layout and language: Integrating spatial and
linguistic knowledge for layout understanding
tasks. In Proceedings of the 18th Conference on
Computational Linguistics, COLING ’00.
John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning, ICML ’01, pages 282–289, San
Francisco, CA, USA. Morgan Kaufmann Pub-
lishers Inc.
Cindy Xide Lin, Bo Zhao, Tim Weninger, Jiawei
Han, and Bing Liu. 2010. Entity relation dis-
covery from web tables and links. In Proceedings
of the 19th International Conference on World
Wide Web, WWW ’10.
Ying Liu, Kun Bai, Prasenjit Mitra, and C. Lee
Giles. 2007. TableSeer: Automatic table meta-
data extraction and searching in digital libraries.
In Proceedings of the 7th ACM/IEEE-CS Joint
Conference on Digital Libraries, JCDL ’07.
Andrew McCallum. 2009. Joint inference for nat-
ural language processing. In Proceedings of the
13th Conference on Computational Natural Lan-
guage Learning, CoNLL ’09.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings
of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, ACL ’09.
David Pinto, Andrew McCallum, Xing Wei, and
W. Bruce Croft. 2003. Table extraction us-
ing conditional random fields. In Proceedings
of the 26th Annual International ACM SIGIR
Conference on Research and Development in In-
formaion Retrieval, SIGIR ’03.
Aleksander Pivk. 2006. Automatic ontology gen-
eration from web tabular structures. AI Com-
munication, 19(1).
Hoifung Poon and Pedro Domingos. 2007. Joint
inference in information extraction. In Proceed-
ings of the 22nd National Conference on Artifi-
cial intelligence, AAAI’07.
Sameer Singh, Karl Schultz, and Andrew Mc-
Callum. 2009. Bi-directional joint inference
for entity resolution and segmentation using
imperatively-defined factor graphs. In Pro-
ceedings of the European Conference on Ma-
chine Learning and Knowledge Discovery in
Databases, ECML PKDD ’09.
Ashwin Tengli, Yiming Yang, and Nian Li Ma.
2004. Learning table extraction from examples.
In Proceedings of the 20th International Con-
ference on Computational Linguistics, COLING
’04.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In
Proceedings of the 2000 Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’00.
Xing Wei, Bruce Croft, and Andrew McCallum.
2006. Table extraction for answer retrieval. In-
formation Retrieval, 9(5).
</reference>
<page confidence="0.633882">
663
</page>
<reference confidence="0.998581736842105">
Dekai Wu and Ken Wing Kuen Lee. 2006. A gram-
matical approach to understanding textual ta-
bles using two-dimensional scfgs. In Proceedings
of the COLING/ACL, COLING-ACL ’06.
Fei Wu and Daniel S. Weld. 2010. Open informa-
tion extraction using Wikipedia. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ’10.
Burcu Yildiz. 2004. Information extraction – uti-
lizing table patterns. Master’s thesis, Institutf¨ur
Softwaretechnik und Interaktive Systeme.
Ce Zhang and Christopher R´e. 2013. Towards
high-throughput Gibbs sampling at scale: A
study across storage managers. SIGMOD ’13.
Ce Zhang, Vidhya Govindaraju, Jackson Bor-
chardt, Tim Foltz, Christopher R´e, and Shanan
Peters. 2013. GeoDeepDive: Statistical infer-
ence using familiar data-processing languages.
SIGMOD ’13.
</reference>
<page confidence="0.926444">
664
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864334">
<title confidence="0.999823">Understanding Tables in Context Using Standard NLP Toolkits</title>
<author confidence="0.999725">Vidhya Govindaraju Ce Zhang Christopher R´e</author>
<affiliation confidence="0.998642">University of</affiliation>
<email confidence="0.874563">czhang,</email>
<abstract confidence="0.999497428571429">Tabular information in text documents contains a wealth of information, and so tables are a natural candidate for information extraction. There are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table. We study how natural-language tools, such as part-of-speech tagging, dependency paths, and named-entity recognition, can be used to improve the quality of relation extraction from tables. In three domains we show that (1) a model that performs joint probabilistic inference across tabular and natural language features achieves an F1 score that is twice as high as either a puretable or pure-text system, and (2) using only shallower features or non-joint inference results in lower quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael J Cafarella</author>
<author>Alon Halevy</author>
<author>Daisy Zhe Wang</author>
<author>Eugene Wu</author>
<author>Yang Zhang</author>
</authors>
<title>WebTables: Exploring the power of tables on the web.</title>
<date>2008</date>
<journal>Proceedings of VLDB Endowment,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1147" citStr="Cafarella et al., 2008" startWordPosition="177" endWordPosition="180">pendency paths, and named-entity recognition, can be used to improve the quality of relation extraction from tables. In three domains we show that (1) a model that performs joint probabilistic inference across tabular and natural language features achieves an F1 score that is twice as high as either a puretable or pure-text system, and (2) using only shallower features or non-joint inference results in lower quality. 1 Introduction Tabular data is ubiquitous and often contains high-quality, structured relational data. Recent studies found billions of high-quality relations on the web in HTML (Cafarella et al., 2008). In financial applications, a huge amount of data is buried in the tables of corporate filings and earnings reports; in science, millions of journal articles contain billions of scientific facts in tables. Although tables describe precise, structured relations, tables are rarely written in a way that is self-describing, e.g., tables may contain abbreviations or only informal schema information; in turn, the contents of tables are often ambiguously specified, which makes extracting the relations implicit in tabular data difficult. Tables are, however, not written in isolation. The text surroun</context>
<context position="17002" citStr="Cafarella et al., 2008" startWordPosition="2828" endWordPosition="2831">precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint inference is helpful in this applica</context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: Exploring the power of tables on the web. Proceedings of VLDB Endowment, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Shih-Chung Tsai</author>
<author>Jin-He Tsai</author>
</authors>
<title>Mining tables from large scale HTML texts.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics, COLING ’00.</booktitle>
<contexts>
<context position="16848" citStr="Chen et al., 2000" startWordPosition="2804" endWordPosition="2807">and title, from PDF documents in which a table appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and i</context>
</contexts>
<marker>Chen, Tsai, Tsai, 2000</marker>
<rawString>Hsin-Hsi Chen, Shih-Chung Tsai, and Jin-He Tsai. 2000. Mining tables from large scale HTML texts. In Proceedings of the 18th Conference on Computational Linguistics, COLING ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>The statistical significance of the MUC-4 results.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th Conference on Message Understanding, MUC4</booktitle>
<pages>92</pages>
<contexts>
<context position="14046" citStr="Chinchor (1992)" startWordPosition="2354" endWordPosition="2355">tatistically significant improvement of F1 score over all three other approaches with each probability threshold. 3.3 Shallow vs. Linguistic Features We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called JOINT. There are different ways to use shallow and linguistic features; we select Approaches \ Prob. .99 .90 .50 TEXT + + + TABLE + + + MERGE + + + Figure 6: Approximate randomization test from Chinchor (1992) of F1 score with p = 0.01 on the impact of joint inference compared with pure-table or pure-text approaches for different probability thresholds. A + sign indicates that the F1 score of joint approach increased significantly. Linguistic POS tags (Wu et al., 2010) Stanford NER tags (Mintz et al., 2009) Dependence trees (Mintz et al., 2009) Figure 7: Types of Features. state-of-the-art approaches from the literature (see Figure 7). We created the following variants of JOINT. JOINT(-PARSE) removes features generated by the dependency parser and syntax parser. Similarly, JOINT(-NER) (JOINT(-POS))</context>
</contexts>
<marker>Chinchor, 1992</marker>
<rawString>Nancy Chinchor. 1992. The statistical significance of the MUC-4 results. In Proceedings of the 4th Conference on Message Understanding, MUC4 ’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhavana Bharat Dalvi</author>
<author>William Cohen</author>
<author>Jamie Callan</author>
</authors>
<title>WebSets: Extracting sets of entities from the web using unsupervised information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Web Search and Data Mining, WSDM ’12.</booktitle>
<contexts>
<context position="2428" citStr="Dalvi et al., 2012" startWordPosition="383" endWordPosition="386">intended audience, a human reader. For example, in a simple study, we demonstrate that humans can achieve more than 60% higher recall by jointly reading the text and tables in a journal article than by only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior appr</context>
<context position="10122" citStr="Dalvi et al. (2012)" startWordPosition="1687" endWordPosition="1690">annotate these journal articles manually to extract the ROCK-TOTALORGANICCARBON relation (1.5K tuples). We processed each document using Stanford CoreNLP (de Marneffe et al., 2006; Toutanova and Manning, 2000), 2We choose a set of documents that (1) are in English, and (2) contain at least one table. PDFtoHTML3, and pdf2table (Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al., 2001) model for the ROCK-TOTALORGANICCARBON relation. (3) MERGE. Using TABLE and TEXT, we extract all facts and their associated probability. Following Duin (2002), we combine these two probabilities using a linear combination. MERGE is a baseline approach that uses informa</context>
<context position="15111" citStr="Dalvi et al., 2012" startWordPosition="2513" endWordPosition="2516">the following variants of JOINT. JOINT(-PARSE) removes features generated by the dependency parser and syntax parser. Similarly, JOINT(-NER) (JOINT(-POS)) removes all features related to NER (resp. POS). JOINT(-POS) also removes NER and parser features because the latter two are dependent on POS features. Figure 8 shows the P/R curve for all these variants on GEOLOGY, and Figure 9 shows the results of statistical significance test. For probability threshold .90, JOINT outperforms JOINT(-POS) significantly. The difference between JOINT, JOINT(-PARSE), Type Features Shallow Regular Expressions (Dalvi et al., 2012) Term proximity (Matsuo et al., 2003) Dictionary and Freebase (Mintz et al., 2009) 661 Figure 8: Lesion study of different features for GEOLOGY. Features \ Prob. .90 .50 JOINT(-PARSE) → JOINT 0 + JOINT(-NER) →JOINT 0 + JOINT(-POS) → JOINT + + Figure 9: Approximate randomization test of F1 score with p = 0.01 on the impact of linguistic features. For x → y, a + indicates that the F1 score of y is significantly higher than x. 0 indicates that the F1 score does not change significantly. and JOINT(-NER) is not significant because there are “easy-to-extract” facts in the highprobability range. For </context>
<context position="17036" citStr="Dalvi et al., 2012" startWordPosition="2835" endWordPosition="2838">h linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint inference is helpful in this application because our joint inference a</context>
</contexts>
<marker>Dalvi, Cohen, Callan, 2012</marker>
<rawString>Bhavana Bharat Dalvi, William Cohen, and Jamie Callan. 2012. WebSets: Extracting sets of entities from the web using unsupervised information extraction. In Proceedings of the 5th ACM International Conference on Web Search and Data Mining, WSDM ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Duin</author>
</authors>
<title>The combining classifier: to train or not to train?</title>
<date>2002</date>
<booktitle>In 16th International Conference on Pattern Recognition.</booktitle>
<contexts>
<context position="10611" citStr="Duin (2002)" startWordPosition="1777" endWordPosition="1778"> which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al., 2001) model for the ROCK-TOTALORGANICCARBON relation. (3) MERGE. Using TABLE and TEXT, we extract all facts and their associated probability. Following Duin (2002), we combine these two probabilities using a linear combination. MERGE is a baseline approach that uses information from both tables and text. (4) JOINT. We build a joint approach that uses information from both tables and text. This approach is a large factor graph in which we embed the CRFs developed in TABLE and TEXT. Additionally, we allow JOINT to predict projections of each relation, as shown in Figure 4. Recall that a key advantage of a joint approach is that we do not need to predict all arguments of the relation (if such a prediction is unwarranted from the data). The inference is don</context>
</contexts>
<marker>Duin, 2002</marker>
<rawString>Robert Duin. 2002. The combining classifier: to train or not to train? In 16th International Conference on Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Joint models with missing data for semi-supervised learning.</title>
<date>2009</date>
<booktitle>In NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing.</booktitle>
<contexts>
<context position="17536" citStr="Eisner, 2009" startWordPosition="2921" endWordPosition="2922">ieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint inference is helpful in this application because our joint inference approach combines non-redundant signals (textual versus tabular). 5 Conclusion To improve the quality of extractions of tabular data, we use standard NLP techniques to more deeply understand the text in which a table is embedded. We validate that deeper NLP features combined with a joint probabilistic model has a statistically significant impact on quality, i.e., recall and precision. Our ongoing work is to apply these ideas to a much larger corpus from each of the three domains. 6 Acknowledgment</context>
</contexts>
<marker>Eisner, 2009</marker>
<rawString>Jason Eisner. 2009. Joint models with missing data for semi-supervised learning. In NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Fang</author>
<author>Prasenjit Mitra</author>
<author>Zhi Tang</author>
<author>C Lee Giles</author>
</authors>
<title>Table header detection and classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th AAAI Conference on Artificial Intelligence, AAAI ’12.</booktitle>
<contexts>
<context position="16892" citStr="Fang et al., 2012" startWordPosition="2811" endWordPosition="2814">e appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction qua</context>
</contexts>
<marker>Fang, Mitra, Tang, Giles, 2012</marker>
<rawString>Jing Fang, Prasenjit Mitra, Zhi Tang, and C. Lee Giles. 2012. Table header detection and classification. In Proceedings of the 26th AAAI Conference on Artificial Intelligence, AAAI ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05.</booktitle>
<contexts>
<context position="13736" citStr="Finkel et al., 2005" startWordPosition="2297" endWordPosition="2300">ch of the three probability thresholds, t E {.99, .90, .50}, we created the set of predictions that JOINT assigns probability greater than t. Figure 6 shows the results of the statistical significance test in which the null hypothesis is that the F1 scores of two approaches are the same. With p = 0.01, JOINT has statistically significant improvement of F1 score over all three other approaches with each probability threshold. 3.3 Shallow vs. Linguistic Features We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called JOINT. There are different ways to use shallow and linguistic features; we select Approaches \ Prob. .99 .90 .50 TEXT + + + TABLE + + + MERGE + + + Figure 6: Approximate randomization test from Chinchor (1992) of F1 score with p = 0.01 on the impact of joint inference compared with pure-table or pure-text approaches for different probability thresholds. A + sign indicates that the F1 score of joint approach increased significantly. Linguistic POS tags (Wu et al., 2010) Stanford NER tags (Mintz </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hurst</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Layout and language: Integrating spatial and linguistic knowledge for layout understanding tasks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics, COLING ’00.</booktitle>
<contexts>
<context position="15941" citStr="Hurst and Nasukawa (2000)" startWordPosition="2651" endWordPosition="2654">R) →JOINT 0 + JOINT(-POS) → JOINT + + Figure 9: Approximate randomization test of F1 score with p = 0.01 on the impact of linguistic features. For x → y, a + indicates that the F1 score of y is significantly higher than x. 0 indicates that the F1 score does not change significantly. and JOINT(-NER) is not significant because there are “easy-to-extract” facts in the highprobability range. For probability threshold .50, JOINT outperforms all three other variants significantly. 4 Related Work The intuition that context features might help table-related tasks has existed for decades. For example, Hurst and Nasukawa (2000) mentioned (as future work) that context features could be used to further improve their relation extraction approaches from tables. Lin et al. (2010) use bag-of-words features and hyperlinks to recommend new columns for web tables. Liu et al. (2007) extract features, including font size and title, from PDF documents in which a table appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging </context>
</contexts>
<marker>Hurst, Nasukawa, 2000</marker>
<rawString>Matthew Hurst and Tetsuya Nasukawa. 2000. Layout and language: Integrating spatial and linguistic knowledge for layout understanding tasks. In Proceedings of the 18th Conference on Computational Linguistics, COLING ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10453" citStr="Lafferty et al., 2001" startWordPosition="1752" endWordPosition="1755">(Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al., 2001) model for the ROCK-TOTALORGANICCARBON relation. (3) MERGE. Using TABLE and TEXT, we extract all facts and their associated probability. Following Duin (2002), we combine these two probabilities using a linear combination. MERGE is a baseline approach that uses information from both tables and text. (4) JOINT. We build a joint approach that uses information from both tables and text. This approach is a large factor graph in which we embed the CRFs developed in TABLE and TEXT. Additionally, we allow JOINT to predict projections of each relation, as shown in Figure 4. Recall that a key advantage</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cindy Xide Lin</author>
<author>Bo Zhao</author>
<author>Tim Weninger</author>
<author>Jiawei Han</author>
<author>Bing Liu</author>
</authors>
<title>Entity relation discovery from web tables and links.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web, WWW ’10.</booktitle>
<contexts>
<context position="16091" citStr="Lin et al. (2010)" startWordPosition="2675" endWordPosition="2678">indicates that the F1 score of y is significantly higher than x. 0 indicates that the F1 score does not change significantly. and JOINT(-NER) is not significant because there are “easy-to-extract” facts in the highprobability range. For probability threshold .50, JOINT outperforms all three other variants significantly. 4 Related Work The intuition that context features might help table-related tasks has existed for decades. For example, Hurst and Nasukawa (2000) mentioned (as future work) that context features could be used to further improve their relation extraction approaches from tables. Lin et al. (2010) use bag-of-words features and hyperlinks to recommend new columns for web tables. Liu et al. (2007) extract features, including font size and title, from PDF documents in which a table appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting inf</context>
</contexts>
<marker>Lin, Zhao, Weninger, Han, Liu, 2010</marker>
<rawString>Cindy Xide Lin, Bo Zhao, Tim Weninger, Jiawei Han, and Bing Liu. 2010. Entity relation discovery from web tables and links. In Proceedings of the 19th International Conference on World Wide Web, WWW ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Liu</author>
<author>Kun Bai</author>
<author>Prasenjit Mitra</author>
<author>C Lee Giles</author>
</authors>
<title>TableSeer: Automatic table metadata extraction and searching in digital libraries.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL ’07.</booktitle>
<contexts>
<context position="2628" citStr="Liu et al. (2007)" startWordPosition="421" endWordPosition="424"> only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior approaches for table extraction. In contrast to the shallow, lexical features that prior approaches have used, we use standard NLP features, such as dependency paths, parts of speech, etc. Our hypothesis </context>
<context position="16191" citStr="Liu et al. (2007)" startWordPosition="2693" endWordPosition="2696">not change significantly. and JOINT(-NER) is not significant because there are “easy-to-extract” facts in the highprobability range. For probability threshold .50, JOINT outperforms all three other variants significantly. 4 Related Work The intuition that context features might help table-related tasks has existed for decades. For example, Hurst and Nasukawa (2000) mentioned (as future work) that context features could be used to further improve their relation extraction approaches from tables. Lin et al. (2010) use bag-of-words features and hyperlinks to recommend new columns for web tables. Liu et al. (2007) extract features, including font size and title, from PDF documents in which a table appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (</context>
</contexts>
<marker>Liu, Bai, Mitra, Giles, 2007</marker>
<rawString>Ying Liu, Kun Bai, Prasenjit Mitra, and C. Lee Giles. 2007. TableSeer: Automatic table metadata extraction and searching in digital libraries. In Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries, JCDL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Joint inference for natural language processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning, CoNLL ’09.</booktitle>
<contexts>
<context position="17395" citStr="McCallum, 2009" startWordPosition="2898" endWordPosition="2899">ding NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint inference is helpful in this application because our joint inference approach combines non-redundant signals (textual versus tabular). 5 Conclusion To improve the quality of extractions of tabular data, we use standard NLP techniques to more deeply understand the text in which a table is embedded. We validate that deeper NLP features combined with a joint probabilistic model has a statistically significant impact on quality, </context>
</contexts>
<marker>McCallum, 2009</marker>
<rawString>Andrew McCallum. 2009. Joint inference for natural language processing. In Proceedings of the 13th Conference on Computational Natural Language Learning, CoNLL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL ’09.</booktitle>
<contexts>
<context position="2542" citStr="Mintz et al., 2009" startWordPosition="404" endWordPosition="407">n 60% higher recall by jointly reading the text and tables in a journal article than by only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior approaches for table extraction. In contrast to the shallow, lexical features that prior approaches have used, we use </context>
<context position="10314" citStr="Mintz et al. (2009)" startWordPosition="1725" endWordPosition="1728">nd Manning, 2000), 2We choose a set of documents that (1) are in English, and (2) contain at least one table. PDFtoHTML3, and pdf2table (Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al., 2001) model for the ROCK-TOTALORGANICCARBON relation. (3) MERGE. Using TABLE and TEXT, we extract all facts and their associated probability. Following Duin (2002), we combine these two probabilities using a linear combination. MERGE is a baseline approach that uses information from both tables and text. (4) JOINT. We build a joint approach that uses information from both tables and text. This approach is a large factor graph in which we embed the CRFs developed</context>
<context position="14349" citStr="Mintz et al., 2009" startWordPosition="2404" endWordPosition="2407"> 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called JOINT. There are different ways to use shallow and linguistic features; we select Approaches \ Prob. .99 .90 .50 TEXT + + + TABLE + + + MERGE + + + Figure 6: Approximate randomization test from Chinchor (1992) of F1 score with p = 0.01 on the impact of joint inference compared with pure-table or pure-text approaches for different probability thresholds. A + sign indicates that the F1 score of joint approach increased significantly. Linguistic POS tags (Wu et al., 2010) Stanford NER tags (Mintz et al., 2009) Dependence trees (Mintz et al., 2009) Figure 7: Types of Features. state-of-the-art approaches from the literature (see Figure 7). We created the following variants of JOINT. JOINT(-PARSE) removes features generated by the dependency parser and syntax parser. Similarly, JOINT(-NER) (JOINT(-POS)) removes all features related to NER (resp. POS). JOINT(-POS) also removes NER and parser features because the latter two are dependent on POS features. Figure 8 shows the P/R curve for all these variants on GEOLOGY, and Figure 9 shows the results of statistical significance test. For probability thres</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>Table extraction using conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03.</booktitle>
<contexts>
<context position="2467" citStr="Pinto et al., 2003" startWordPosition="391" endWordPosition="394"> example, in a simple study, we demonstrate that humans can achieve more than 60% higher recall by jointly reading the text and tables in a journal article than by only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior approaches for table extraction. In contras</context>
<context position="10098" citStr="Pinto et al. (2003)" startWordPosition="1682" endWordPosition="1685"> three geoscientists to annotate these journal articles manually to extract the ROCK-TOTALORGANICCARBON relation (1.5K tuples). We processed each document using Stanford CoreNLP (de Marneffe et al., 2006; Toutanova and Manning, 2000), 2We choose a set of documents that (1) are in English, and (2) contain at least one table. PDFtoHTML3, and pdf2table (Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al., 2001) model for the ROCK-TOTALORGANICCARBON relation. (3) MERGE. Using TABLE and TEXT, we extract all facts and their associated probability. Following Duin (2002), we combine these two probabilities using a linear combination. MERGE is a baseline ap</context>
<context position="16967" citStr="Pinto et al., 2003" startWordPosition="2823" endWordPosition="2826">nly contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint i</context>
</contexts>
<marker>Pinto, McCallum, Wei, Croft, 2003</marker>
<rawString>David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003. Table extraction using conditional random fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleksander Pivk</author>
</authors>
<title>Automatic ontology generation from web tabular structures.</title>
<date>2006</date>
<journal>AI Communication,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="16905" citStr="Pivk, 2006" startWordPosition="2815" endWordPosition="2816">he table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon an</context>
</contexts>
<marker>Pivk, 2006</marker>
<rawString>Aleksander Pivk. 2006. Automatic ontology generation from web tabular structures. AI Communication, 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National Conference on Artificial intelligence, AAAI’07.</booktitle>
<contexts>
<context position="2587" citStr="Poon and Domingos, 2007" startWordPosition="412" endWordPosition="416">the text and tables in a journal article than by only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior approaches for table extraction. In contrast to the shallow, lexical features that prior approaches have used, we use standard NLP features, such as dependency pat</context>
<context position="17420" citStr="Poon and Domingos, 2007" startWordPosition="2900" endWordPosition="2904"> Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint inference is helpful in this application because our joint inference approach combines non-redundant signals (textual versus tabular). 5 Conclusion To improve the quality of extractions of tabular data, we use standard NLP techniques to more deeply understand the text in which a table is embedded. We validate that deeper NLP features combined with a joint probabilistic model has a statistically significant impact on quality, i.e., recall and precisio</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In Proceedings of the 22nd National Conference on Artificial intelligence, AAAI’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Karl Schultz</author>
<author>Andrew McCallum</author>
</authors>
<title>Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD ’09.</booktitle>
<contexts>
<context position="17441" citStr="Singh et al., 2009" startWordPosition="2905" endWordPosition="2908">, 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intuition is that joint inference is helpful in this application because our joint inference approach combines non-redundant signals (textual versus tabular). 5 Conclusion To improve the quality of extractions of tabular data, we use standard NLP techniques to more deeply understand the text in which a table is embedded. We validate that deeper NLP features combined with a joint probabilistic model has a statistically significant impact on quality, i.e., recall and precision. Our ongoing work i</context>
</contexts>
<marker>Singh, Schultz, McCallum, 2009</marker>
<rawString>Sameer Singh, Karl Schultz, and Andrew McCallum. 2009. Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashwin Tengli</author>
<author>Yiming Yang</author>
<author>Nian Li Ma</author>
</authors>
<title>Learning table extraction from examples.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04.</booktitle>
<contexts>
<context position="16828" citStr="Tengli et al., 2004" startWordPosition="2800" endWordPosition="2803"> including font size and title, from PDF documents in which a table appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh </context>
</contexts>
<marker>Tengli, Yang, Ma, 2004</marker>
<rawString>Ashwin Tengli, Yiming Yang, and Nian Li Ma. 2004. Learning table extraction from examples. In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing, EMNLP ’00.</booktitle>
<contexts>
<context position="9712" citStr="Toutanova and Manning, 2000" startWordPosition="1619" endWordPosition="1622">perimental Setup We consider the task of constructing a geology knowledge base. Specifically, our goal is to extract a ROCK-TOTALORGANICCARBON relation that maps rock formations (e.g., “Barnett Formation”) to their total organic carbon (e.g., “6%”). Such data is important for estimating stored energy and for global climate research. Dataset. We selected 100 geology journal articles.2 We asked three geoscientists to annotate these journal articles manually to extract the ROCK-TOTALORGANICCARBON relation (1.5K tuples). We processed each document using Stanford CoreNLP (de Marneffe et al., 2006; Toutanova and Manning, 2000), 2We choose a set of documents that (1) are in English, and (2) contain at least one table. PDFtoHTML3, and pdf2table (Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (200</context>
<context position="13695" citStr="Toutanova and Manning, 2000" startWordPosition="2291" endWordPosition="2294">r approaches is statistically significant. For each of the three probability thresholds, t E {.99, .90, .50}, we created the set of predictions that JOINT assigns probability greater than t. Figure 6 shows the results of the statistical significance test in which the null hypothesis is that the F1 scores of two approaches are the same. With p = 0.01, JOINT has statistically significant improvement of F1 score over all three other approaches with each probability threshold. 3.3 Shallow vs. Linguistic Features We validate the hypothesis that using linguistic features, e.g., part-of-speech tags (Toutanova and Manning, 2000), named-entity tags (Finkel et al., 2005), and dependency trees (de Marneffe et al., 2006), helps improve the quality of our approach, called JOINT. There are different ways to use shallow and linguistic features; we select Approaches \ Prob. .99 .90 .50 TEXT + + + TABLE + + + MERGE + + + Figure 6: Approximate randomization test from Chinchor (1992) of F1 score with p = 0.01 on the impact of joint inference compared with pure-table or pure-text approaches for different probability thresholds. A + sign indicates that the F1 score of joint approach increased significantly. Linguistic POS tags (W</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing, EMNLP ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wei</author>
<author>Bruce Croft</author>
<author>Andrew McCallum</author>
</authors>
<title>Table extraction for answer retrieval.</title>
<date>2006</date>
<journal>Information Retrieval,</journal>
<volume>9</volume>
<issue>5</issue>
<contexts>
<context position="16946" citStr="Wei et al., 2006" startWordPosition="2819" endWordPosition="2822">t these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and Domingos, 2007; Singh et al., 2009); and in some, joint inference hurts extraction quality (Poon and Domingos, 2007; Eisner, 2009). Our intu</context>
</contexts>
<marker>Wei, Croft, McCallum, 2006</marker>
<rawString>Xing Wei, Bruce Croft, and Andrew McCallum. 2006. Table extraction for answer retrieval. Information Retrieval, 9(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Ken Wing Kuen Lee</author>
</authors>
<title>A grammatical approach to understanding textual tables using two-dimensional scfgs.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL, COLING-ACL ’06.</booktitle>
<contexts>
<context position="2446" citStr="Wu and Lee, 2006" startWordPosition="387" endWordPosition="390"> human reader. For example, in a simple study, we demonstrate that humans can achieve more than 60% higher recall by jointly reading the text and tables in a journal article than by only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior approaches for table e</context>
<context position="16807" citStr="Wu and Lee, 2006" startWordPosition="2796" endWordPosition="2799"> extract features, including font size and title, from PDF documents in which a table appears to help the table ranking task. They find that these features only contribute less than 2% to precision. In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction. Extracting information from tables has been discussed by different communities in the last decade, including NLP (Wu and Lee, 2006; Tengli et al., 2004; Chen et al., 2000), artificial intelligence (Fang et al., 2012; Pivk, 2006), information retrieval (Wei et al., 2006; Pinto et al., 2003), database (Cafarella et al., 2008), and the web (Dalvi et al., 2012). This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality (McCallum, 2009; Poon and D</context>
</contexts>
<marker>Wu, Lee, 2006</marker>
<rawString>Dekai Wu and Ken Wing Kuen Lee. 2006. A grammatical approach to understanding textual tables using two-dimensional scfgs. In Proceedings of the COLING/ACL, COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10.</booktitle>
<contexts>
<context position="2561" citStr="Wu and Weld, 2010" startWordPosition="408" endWordPosition="411">by jointly reading the text and tables in a journal article than by only looking at the tables. The conclusion of this experiment is not surprising, but it raises a question: How should a system combine tabular and natural-language features to understand tables in text? The literature provides a broad spectrum of answers to this question. Most previous approaches use textual or tabular features separately, e.g., tabular approaches that do not use text features (Dalvi et al., 2012; Wu and Lee, 2006; Pinto et al., 2003) or textual approaches that do not use tabular features (Mintz et al., 2009; Wu and Weld, 2010; Poon and Domingos, 2007). In a prescient study, Liu et al. (2007) proposed to learn the target relation independently from both table and surface textual features, and then combine the result using a linear combination of the predictions. In a similar spirit, we propose to use both types of features in our approach of relation extraction. Our proposed approach differs from prior approaches in two ways: (1) We use deeper–but standard–NLP features than prior approaches for table extraction. In contrast to the shallow, lexical features that prior approaches have used, we use standard NLP featur</context>
<context position="10290" citStr="Wu and Weld (2010)" startWordPosition="1720" endWordPosition="1723"> al., 2006; Toutanova and Manning, 2000), 2We choose a set of documents that (1) are in English, and (2) contain at least one table. PDFtoHTML3, and pdf2table (Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al., 2001) model for the ROCK-TOTALORGANICCARBON relation. (3) MERGE. Using TABLE and TEXT, we extract all facts and their associated probability. Following Duin (2002), we combine these two probabilities using a linear combination. MERGE is a baseline approach that uses information from both tables and text. (4) JOINT. We build a joint approach that uses information from both tables and text. This approach is a large factor graph in which we </context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burcu Yildiz</author>
</authors>
<title>Information extraction – utilizing table patterns. Master’s thesis, Institutf¨ur Softwaretechnik und Interaktive Systeme.</title>
<date>2004</date>
<contexts>
<context position="9845" citStr="Yildiz, 2004" startWordPosition="1645" endWordPosition="1646">ion that maps rock formations (e.g., “Barnett Formation”) to their total organic carbon (e.g., “6%”). Such data is important for estimating stored energy and for global climate research. Dataset. We selected 100 geology journal articles.2 We asked three geoscientists to annotate these journal articles manually to extract the ROCK-TOTALORGANICCARBON relation (1.5K tuples). We processed each document using Stanford CoreNLP (de Marneffe et al., 2006; Toutanova and Manning, 2000), 2We choose a set of documents that (1) are in English, and (2) contain at least one table. PDFtoHTML3, and pdf2table (Yildiz, 2004). We then extracted features following state-ofthe-art practices (see Figure 4). Approaches. To validate our hypothesis, we implement four systems, each of which has access to different types of data: (1) TABLE. This approach follows Pinto et al. (2003) and Dalvi et al. (2012) and only uses the tables in a document. (2) TEXT. This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al. (2009). The features used in (1) and (2) are shown in Figure 4. In both TABLE and TEXT, we use a conditional random field (Lafferty et al</context>
</contexts>
<marker>Yildiz, 2004</marker>
<rawString>Burcu Yildiz. 2004. Information extraction – utilizing table patterns. Master’s thesis, Institutf¨ur Softwaretechnik und Interaktive Systeme.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ce Zhang</author>
<author>Christopher R´e</author>
</authors>
<title>Towards high-throughput Gibbs sampling at scale: A study across storage managers.</title>
<date>2013</date>
<journal>SIGMOD</journal>
<volume>13</volume>
<marker>Zhang, R´e, 2013</marker>
<rawString>Ce Zhang and Christopher R´e. 2013. Towards high-throughput Gibbs sampling at scale: A study across storage managers. SIGMOD ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ce Zhang</author>
<author>Vidhya Govindaraju</author>
<author>Jackson Borchardt</author>
<author>Tim Foltz</author>
<author>Christopher R´e</author>
<author>Shanan Peters</author>
</authors>
<title>GeoDeepDive: Statistical inference using familiar data-processing languages.</title>
<date>2013</date>
<journal>SIGMOD</journal>
<volume>13</volume>
<marker>Zhang, Govindaraju, Borchardt, Foltz, R´e, Peters, 2013</marker>
<rawString>Ce Zhang, Vidhya Govindaraju, Jackson Borchardt, Tim Foltz, Christopher R´e, and Shanan Peters. 2013. GeoDeepDive: Statistical inference using familiar data-processing languages. SIGMOD ’13.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>