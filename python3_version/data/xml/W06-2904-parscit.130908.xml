<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003633">
<title confidence="0.997726">
Improved Large Margin Dependency Parsing
via Local Constraints and Laplacian Regularization
</title>
<author confidence="0.972817">
Qin Iris Wang Colin Cherry Dan Lizotte Dale Schuurmans
</author>
<affiliation confidence="0.975204">
Department of Computing Science
University of Alberta
</affiliation>
<email confidence="0.992627">
wqin,colinc,dlizotte,dale@cs.ualberta.ca
</email>
<sectionHeader confidence="0.993796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922263157895">
We present an improved approach for
learning dependency parsers from tree-
bank data. Our technique is based on two
ideas for improving large margin train-
ing in the context of dependency parsing.
First, we incorporate local constraints that
enforce the correctness of each individ-
ual link, rather than just scoring the global
parse tree. Second, to cope with sparse
data, we smooth the lexical parameters ac-
cording to their underlying word similar-
ities using Laplacian Regularization. To
demonstrate the benefits of our approach,
we consider the problem of parsing Chi-
nese treebank data using only lexical fea-
tures, that is, without part-of-speech tags
or grammatical categories. We achieve
state of the art performance, improving
upon current large margin approaches.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945714285714">
Over the past decade, there has been tremendous
progress on learning parsing models from treebank
data (Collins, 1997; Charniak, 2000; Wang et al.,
2005; McDonald et al., 2005). Most of the early
work in this area was based on postulating gener-
ative probability models of language that included
parse structure (Collins, 1997). Learning in this con-
text consisted of estimating the parameters of the
model with simple likelihood based techniques, but
incorporating various smoothing and back-off esti-
mation tricks to cope with the sparse data problems
(Collins, 1997; Bikel, 2004). Subsequent research
began to focus more on conditional models of parse
structure given the input sentence, which allowed
</bodyText>
<page confidence="0.996348">
21
</page>
<bodyText confidence="0.999966054054054">
discriminative training techniques such as maximum
conditional likelihood (i.e. “maximum entropy”)
to be applied (Ratnaparkhi, 1999; Charniak, 2000).
In fact, recently, effective conditional parsing mod-
els have been learned using relatively straightfor-
ward “plug-in” estimates, augmented with similar-
ity based smoothing (Wang et al., 2005). Currently,
the work on conditional parsing models appears to
have culminated in large margin training (Taskar
et al., 2003; Taskar et al., 2004; Tsochantaridis et
al., 2004; McDonald et al., 2005), which currently
demonstrates the state of the art performance in En-
glish dependency parsing (McDonald et al., 2005).
Despite the realization that maximum margin
training is closely related to maximum conditional
likelihood for conditional models (McDonald et
al., 2005), a sufficiently unified view has not yet
been achieved that permits the easy exchange of
improvements between the probabilistic and non-
probabilistic approaches. For example, smoothing
methods have played a central role in probabilistic
approaches (Collins, 1997; Wang et al., 2005), and
yet they are not being used in current large margin
training algorithms. However, as we demonstrate,
not only can smoothing be applied in a large mar-
gin training framework, it leads to generalization im-
provements in much the same way as probabilistic
approaches. The second key observation we make is
somewhat more subtle. It turns out that probabilistic
approaches pay closer attention to the individual er-
rors made by each component of a parse, whereas
the training error minimized in the large margin
approach—the “structured margin loss” (Taskar et
al., 2003; Tsochantaridis et al., 2004; McDonald et
al., 2005)—is a coarse measure that only assesses
the total error of an entire parse rather than focusing
on the error of any particular component.
</bodyText>
<note confidence="0.878838">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL -X),
pages 21–28, New York City, June 2006. @c 2006 Association for Computational Linguistics
Investors continue to pour cash into money funds
</note>
<figureCaption confidence="0.99947">
Figure 1: A dependency tree
</figureCaption>
<bodyText confidence="0.999816681818182">
In this paper, we make two contributions to the
large margin approach to learning parsers from su-
pervised data. First, we show that smoothing based
on lexical similarity is not only possible in the large
margin framework, but more importantly, allows
better generalization to new words not encountered
during training. Second, we show that the large mar-
gin training objective can be significantly refined to
assess the error of each component of a given parse,
rather than just assess a global score. We show that
these two extensions together lead to greater train-
ing accuracy and better generalization to novel input
sentences than current large margin methods.
To demonstrate the benefit of combining useful
learning principles from both the probabilistic and
large margin frameworks, we consider the prob-
lem of learning a dependency parser for Chinese.
This is an interesting test domain because Chinese
does not have clearly defined parts-of-speech, which
makes lexical smoothing one of the most natural ap-
proaches to achieving reasonable results (Wang et
al., 2005).
</bodyText>
<sectionHeader confidence="0.930925" genericHeader="introduction">
2 Lexicalized Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999964950000001">
A dependency tree specifies which words in a sen-
tence are directly related. That is, the dependency
structure of a sentence is a directed tree where the
nodes are the words in the sentence and links rep-
resent the direct dependency relationships between
the words; see Figure 1. There has been a grow-
ing interest in dependency parsing in recent years.
(Fox, 2002) found that the dependency structures
of a pair of translated sentences have a greater de-
gree of cohesion than phrase structures. (Cherry and
Lin, 2003) exploited such cohesion between the de-
pendency structures to improve the quality of word
alignment of parallel sentences. Dependency rela-
tions have also been found to be useful in informa-
tion extraction (Culotta and Sorensen, 2004; Yan-
garber et al., 2000).
A key aspect of a dependency tree is that it does
not necessarily report parts-of-speech or phrase la-
bels. Not requiring parts-of-speech is especially
beneficial for languages such as Chinese, where
parts-of-speech are not as clearly defined as En-
glish. In Chinese, clear indicators of a word’s part-
of-speech such as suffixes “-ment”, “-ous” or func-
tion words such as “the”, are largely absent. One
of our motivating goals is to develop an approach to
learning dependency parsers that is strictly lexical.
Hence the parser can be trained with a treebank that
only contains the dependency relationships, making
annotation much easier.
Of course, training a parser with bare word-to-
word relationships presents a serious challenge due
to data sparseness. It was found in (Bikel, 2004) that
Collins’ parser made use of bi-lexical statistics only
1.49% of the time. The parser has to compute back-
off probability using parts-of-speech in vast majority
of the cases. In fact, it was found in (Gildea, 2001)
that the removal of bi-lexical statistics from a state
of the art PCFG parser resulted in very little change
in the output. (Klein and Manning, 2003) presented
an unlexicalized parser that eliminated all lexical-
ized parameters. Its performance was close to the
state of the art lexicalized parsers.
Nevertheless, in this paper we follow the re-
cent work of (Wang et al., 2005) and consider a
completely lexicalized parser that uses no parts-of-
speech or grammatical categories of any kind. Even
though a part-of-speech lexicon has always been
considered to be necessary in any natural language
parser, (Wang et al., 2005) showed that distributional
word similarities from a large unannotated corpus
can be used to supplant part-of-speech smoothing
with word similarity smoothing, to still achieve state
of the art dependency parsing accuracy for Chinese.
Before discussing our modifications to large mar-
gin training for parsing in detail, we first present the
dependency parsing model we use. We then give
a brief overview of large margin training, and then
present our two modifications. Subsequently, we
present our experimental results on fully lexical de-
pendency parsing for Chinese.
</bodyText>
<sectionHeader confidence="0.999088" genericHeader="method">
3 Dependency Parsing Model
</sectionHeader>
<subsectionHeader confidence="0.657701">
Given a sentence we are in-
</subsectionHeader>
<bodyText confidence="0.971519">
terested in computing a directed dependency tree,
</bodyText>
<page confidence="0.993786">
22
</page>
<bodyText confidence="0.9863332">
, over . In particular, we assume that a di-
rected dependency tree consists of ordered pairs
of words in such that each word ap-
pears in at least one pair and each word has in-degree
at most one. Dependency trees are usually assumed
to be projective (no crossing arcs), which means that
if there is an arc , then is an ancestor
of all the words between and . Let de-
note the set of all the directed, projective trees that
span .
Given an input sentence , we would like to be
able to compute the best parse; that is, a projective
tree, , that obtains the highest “score”.
In particular, we follow (Eisner, 1996; Eisner and
Satta, 1999; McDonald et al., 2005) and assume that
the score of a complete spanning tree for a given
sentence, whether probabilistically motivated or not,
can be decomposed as a sum of local scores for each
link (a word pair). In which case, the parsing prob-
lem reduces to
</bodyText>
<equation confidence="0.731255">
s (1)
</equation>
<bodyText confidence="0.998869361702128">
where the score s can depend on any
measurable property of and within the tree
. This formulation is sufficiently general to capture
most dependency parsing models, including proba-
bilistic dependency models (Wang et al., 2005; Eis-
ner, 1996) as well as non-probabilistic models (Mc-
Donald et al., 2005). For standard scoring functions,
parsing requires an dynamic programming
algorithm to compute a projective tree that obtains
the maximum score (Eisner and Satta, 1999; Wang
et al., 2005; McDonald et al., 2005).
For the purpose of learning, we decompose each
link score into a weighted linear combination of fea-
tures
for dependency parsing are indicators of each possi-
ble word pair
which allows one to represent the tendency of two
words, and, to be directly linked in a parse. In
this case, there is a corresponding parameter to
be learned for each word pair, which represents the
strength of the possible linkage.
A large number of features leads to a serious risk
of over-fitting due to sparse data problems. The stan-
dard mechanisms for mitigating such effects are to
combine features via abstraction (e.g. using parts-
of-speech) or smoothing (e.g. using word similarity
based smoothing). For abstraction, a common strat-
egy is to use parts-of-speech to compress the feature
set, for example by only considering the tag of the
parent
✺POs
However, rather than use abstraction, we will follow
a purely lexical approach and only consider features
that are directly computable from the words them-
selves (or statistical quantities that are directly mea-
surable from these words).
In general, the most important aspect of a link
feature is simply that it measures something about
a candidate word pair that is predictive of whether
the words will actually be linked in a given sen-
tence. Thus, many other natural features, beyond
parts-of-speech and abstract grammatical categories,
immediately suggest themselves as being predictive
of link existence. For example, one very useful fea-
ture is simply the degree of association between the
two words as measured by their pointwise mutual
information
</bodyText>
<equation confidence="0.975805333333333">
s (2)
PMI
PMI
</equation>
<bodyText confidence="0.985218625">
where are the weight parameters to be estimated
during training.
Of course, the specific features used in any real
situation are critical for obtaining a reasonable de-
pendency parser. The natural sets of features to con-
sider in this setting are very large, consisting at the
very least of features indexed by all possible lexical
items (words). For example, natural features to use
(We describe in Section 6 below how we compute
this association measure on an auxiliary corpus of
unannotated text.) Another useful link feature is
simply the distance between the two words in the
sentence; that is, how many words they have be-
tween them
position position
dist
</bodyText>
<page confidence="0.910954">
23
</page>
<bodyText confidence="0.999817714285714">
Using the techniques of (Hastie et al., 2004) one
can show that minimizing (4) is equivalent to solving
the quadratic program
In fact, the likelihood of a direct link between two
words diminishes quickly with distance, which mo-
tivates using more rapidly increasing functions of
distance, such as the square
</bodyText>
<figure confidence="0.710138">
subject to (4)
dist2 position position
s s
</figure>
<bodyText confidence="0.992949357142857">
In our experiments below, we used only these sim-
ple, lexically determined features, , PMI, dist
and dist2, without the parts-of-speech . Cur-
rently, we only use undirected forms of these fea-
tures, where, for example, for all pairs
(or, put another way, we tie the parameters
together for all ). Ideally, we would like
to use directed features, but we have already found
that these simple undirected features permit state of
the art accuracy in predicting (undirected) depen-
dencies. Nevertheless, extending our approach to di-
rected features and contextual features, as in (Wang
et al., 2005), remains an important direction for fu-
ture research.
</bodyText>
<sectionHeader confidence="0.994429" genericHeader="method">
4 Large Margin Training
</sectionHeader>
<subsectionHeader confidence="0.513262">
Given a training set of sentences annotated with their
</subsectionHeader>
<bodyText confidence="0.990598205128205">
correct dependency parses, ,
the goal of learning is to estimate the parameters of
the parsing model,. In particular, we seek values
for the parameters that can accurately reconstruct the
training parses, but more importantly, are also able
to accurately predict the dependency parse structure
on future test sentences.
To train we follow the large margin training ap-
proach of (Taskar et al., 2003; Tsochantaridis et al.,
2004), which has been applied with great success to
dependency parsing (Taskar et al., 2004; McDonald
et al., 2005). Large margin training can be expressed
as minimizing a regularized loss (Hastie et al., 2004)
for all
which corresponds to the training problem posed in
(McDonald et al., 2005).
Unfortunately, the quadratic program (4) has three
problems one must address. First, there are expo-
nentially many constraints—corresponding to each
possible parse of each training sentence—which
forces one to use alternative training procedures,
such as incremental constraint generation, to slowly
converge to a solution (McDonald et al., 2005;
Tsochantaridis et al., 2004). Second, and related,
the original loss (4) is only evaluated at the global
parse tree level, and is not targeted at penalizing any
specific component in an incorrect parse. Although
(McDonald et al., 2005) explicitly describes this
as an advantage over previous approaches (Ratna-
parkhi, 1999; Yamada and Matsumoto, 2003), below
we find that changing the loss to enforce a more de-
tailed set of constraints leads to a more effective ap-
proach. Third, given the large number of bi-lexical
features in our model, solving (4) directly will
over-fit any reasonable training corpus. (Moreover,
using a large to shrink the values does not mit-
igate the sparse data problem introduced by having
so many features.) We now present our refinements
that address each of these issues in turn.
</bodyText>
<sectionHeader confidence="0.927452" genericHeader="method">
5 Training with Local Constraints
</sectionHeader>
<bodyText confidence="0.998834083333333">
We are initially focusing on training on just an
undirected link model, where each parameter in the
model is a weight between two words, and
, respectively. Since links are undirected, these
weights are symmetric , and we can
also write the score in an undirected fashion as:
s . The main advantage of
working with the undirected link model is that the
constraints needed to ensure correct parses on the
training data are much easier to specify in this case.
Ignoring the projective (no crossing arcs) constraint
for the moment, an undirected dependency parse can
</bodyText>
<figure confidence="0.863407833333333">
(3)
s s
where is the target tree for sentence ;
ranges over all possible alternative trees in ;
s
; and
</figure>
<figureCaption confidence="0.464812">
is a measure of distance between the two
trees and.
</figureCaption>
<page confidence="0.994896">
24
</page>
<bodyText confidence="0.999903785714286">
be equated with a maximum score spanning tree of a
sentence. Given a target parse, the set of constraints
needed to ensure the target parse is in fact the max-
imum score spanning tree under the weights, by
at least a minimum amount, is a simple set of lin-
ear constraints: for any edge that is not in the
target parse, one simply adds two constraints
Nevertheless, the constraints in capture much
of the relevant structure in the data, and are easy
to enforce. Therefore, we wish to maintain them.
However, rather than impose the constraints exactly,
we enforce them approximately through the intro-
duction of slack variables. The relaxed constraints
can then be expressed as
</bodyText>
<figure confidence="0.650886">
(6)
(5)
</figure>
<bodyText confidence="0.998619070175439">
where the edges and are the adjacent
edges that actually occur in the target parse that are
also on the path between and . (These would
have to be the only such edges, or there would be
a loop in the parse tree.) These constraints behave
very naturally by forcing the weight of an omitted
edge to be smaller than the adjacent included edges
that would form a loop, which ensures that the omit-
ted edge would not be added to the maximum score
spanning tree before the included edges.
In this way, one can simply accumulate the set of
linear constraints (5) for every edge that fails to be
included in the target parse for the sentences where
it is a candidate. We denote this set of constraints by
Importantly, the constraint set is convex in the link
weight parameters, as it consists only of linear
constraints.
Ignoring the non-crossing condition, the con-
straint set is exact. However, because of the
non-crossing condition, the constraint set is more
restrictive than necessary. For example, consider
the word sequence , where the
edge is in the target parse. Then the edge
can be ruled out of the parse in one of two
ways: it can be ruled out by making its score less
than the adjacent scores as specified in (5), or it
can be ruled out by making its score smaller than
the score of . Thus, the exact constraint
contains a disjunction of two different constraints,
which creates a non-convex constraint in. (The
union of two convex sets is not necessarily convex.)
This is a weakening of the original constraint set .
Unfortunately, this means that, given a large train-
ing corpus, the constraint set can easily become
infeasible.
and therefore a maximum soft margin solution can
then be expressed as a quadratic program
where denotes the vector of all 1’s.
Even though the slacks are required because we
have slightly over-constrained the parameters, given
that there are so many parameters and a sparse data
problem as well, it seems desirable to impose a
stronger set of constraints. A set of solution pa-
rameters achieved in this way will allow maximum
weight spanning trees to correctly parse nearly all
of the training sentences, even without the non-
crossing condition (see the results in Section 8).
This quadratic program has the advantage of pro-
ducing link parameters that will correctly parse most
of the training data. Unfortunately, the main draw-
back of this method thus far is that it does not of-
fer any mechanism by which the link weights
can be generalized to new or rare words. Given the
sparse data problem, some form of generalization is
necessary to achieve good test results. We achieve
this by exploiting distributional similarities between
words to smooth the parameters.
</bodyText>
<sectionHeader confidence="0.997007" genericHeader="method">
6 Distributional Word Similarity
</sectionHeader>
<bodyText confidence="0.999564222222222">
Treebanks are an extremely precious resource. The
average cost of producing a treebank parse can run
as high as 30 person-minutes per sentence (20 words
on average). Similarity-based smoothing, on the
other hand, allows one to tap into auxiliary sources
of raw unannotated text, which is practically unlim-
ited. With this extra data, one can estimate parame-
ters for words that have never appeared in the train-
ing corpus.
</bodyText>
<figure confidence="0.8832365">
subject to (7)
for all constraints in
</figure>
<page confidence="0.99261">
25
</page>
<bodyText confidence="0.9994464">
The basic intuition behind similarity smoothing
is that words that tend to appear in the same con-
texts tend to have similar meanings. This is known
as the Distributional Hypothesis in linguistics (Har-
ris, 1968). For example, the words test and exam are
similar because both of them can follow verbs such
as administer, cancel, cheat on, conduct, etc.
Many methods have been proposed to compute
distributional similarity between words, e.g., (Hin-
dle, 1990; Pereira et al., 1993; Grefenstette, 1994;
Lin, 1998). Almost all of the methods represent a
word by a feature vector where each feature corre-
sponds to a type of context in which the word ap-
peared. They differ in how the feature vectors are
constructed and how the similarity between two fea-
ture vectors is computed.
In our approach below, we define the features of
a word to be the set of words that occurred within
a small window of in a large corpus. The con-
text window of consists of the closest non-stop-
word on each side of and the stop-words in be-
tween. The value of a feature is defined as the
pointwise mutual information between the and
: PMI . The similarity
between two words, , is then defined as
the cosine of the angle between their feature vectors.
We use this similarity information both in training
and in parsing. For training, we smooth the parame-
ters according to their underlying word-pair similar-
ities by introducing a Laplacian regularizer, which
will be introduced in the next section. For parsing,
the link scores in (1) are smoothed by word similar-
ities (similar to the approach used by (Wang et al.,
2005)) before the maximum score projective depen-
dency tree is computed.
</bodyText>
<sectionHeader confidence="0.931448" genericHeader="method">
7 Laplacian Regularization
</sectionHeader>
<bodyText confidence="0.99978494">
We wish to incorporate similarity based smoothing
in large margin training, while using the more re-
fined constraints outlined in Section 5.
Recall that most of the features we use, and there-
fore most of the parameters we need to estimate are
based on bi-lexical parameters that serve as
undirected link weights between words and in
our dependency parsing model (Section 3). Here we
would like to ensure that two different link weights,
and , that involve similar words also
take on similar values. The previous optimization
(7) needs to be modified to take this into account.
Smoothing the link parameters requires us to first
extend the notion of word similarity to word-pair
similarities, since each link involves two words.
Given similarities between individual words, com-
puted above, we then define the similarity between
word pairs by the geometric mean of the similarities
between corresponding words.
where is defined as in Section 6 above.
Then, instead of just solving the constraint system
(7) we can also ensure that similar links take on sim-
ilar parameter values by introducing a penalty on
their deviations that is weighted by their similarity
value. Specifically, we use
Here is the Laplacian matrix of, which
is defined by where
is a diagonal matrix such that
. Also,corresponds to the
vector of bi-lexical parameters. In this penalty func-
tion, if two edges and have a high sim-
ilarity value, their parameters will be encouraged to
take on similar values. By contrast, if two edges
have low similarity, then there will be little mutual
attraction on their parameter values.
Note, however, that we do not smooth the param-
eters,PMI,aist,aist2, corresponding to the point-
wise mutual information, distance, and squared dis-
tance features described in Section 5, respectively.
We only apply similarity smoothing to the bi-lexical
parameters.
The Laplacian regularizer (9) provides a natural
smoother for the bi-lexical parameter estimates that
takes into account valuable word similarity informa-
tion computed as above. The Laplacian regularizer
also has a significant computational advantage: it is
guaranteed to be a convex quadratic function of the
parameters (Zhu et al., 2001). Therefore, by com-
bining the constraint system (7) with the Laplacian
smoother (9), we can obtain a convex optimization
</bodyText>
<page confidence="0.9986">
26
</page>
<tableCaption confidence="0.999759">
Table 1: Accuracy Results on CTB Test Set
</tableCaption>
<table confidence="0.9996655">
Features used Trained w/ Trained w/
local loss global loss
Pairs 0.6426 0.6184
+ Lap 0.6506 0.5622
+ Dist 0.6546 0.6466
+ Lap + Dist 0.6586 0.5542
+ MI + Dist 0.6707 0.6546
+ Lap + MI + Dist 0.6827 n/a
</table>
<tableCaption confidence="0.992035">
Table 3: Accuracy Results on CTB Training Set
</tableCaption>
<table confidence="0.99964575">
Features used Trained w/ Trained w/
local loss global loss
Pairs 0.9802 0.8393
+ Lap 0.9777 0.7216
+ Dist 0.9755 0.8376
+ Lap + Dist 0.9747 0.7216
+ MI + Dist 0.9768 0.7985
+ Lap + MI + Dist 0.9738 n/a
</table>
<tableCaption confidence="0.977468">
Table 2: Accuracy Results on CTB Dev Set
</tableCaption>
<table confidence="0.998922625">
Features used Trained w/ Trained w/
local loss global loss
Pairs 0.6130 0.5688
+ Lap 0.6390 0.4935
+ Dist 0.6364 0.6130
+ Lap + Dist 0.6494 0.5299
+ MI + Dist 0.6312 0.6182
+ Lap + MI + Dist 0.6571 n/a
</table>
<bodyText confidence="0.999137578947368">
procedure for estimating the link parameters
where does not apply smoothing toPMI,dist,
dist2.
Clearly, (10) describes a large margin training
program for dependency parsing, but one which uses
word similarity smoothing for the bi-lexical param-
eters, and a more refined set of constraints devel-
oped in Section 5. Although the constraints are
more refined, they are fewer in number than (4).
That is, we now only have a polynomial number of
constraints corresponding to each word pair in (5),
rather than the exponential number over every pos-
sible parse tree in (4). Thus, we obtain a polynomial
size quadratic program that can be solved for moder-
ately large problems using standard software pack-
ages. We used CPLEX in our experiments below.
As before, once optimized, the solution parameters
can be introduced into the dependency model (1)
according to (2).
</bodyText>
<sectionHeader confidence="0.99823" genericHeader="evaluation">
8 Experimental Results
</sectionHeader>
<bodyText confidence="0.99996955">
We tested our method experimentally on the Chinese
Treebank (CTB) (Xue et al., 2004). The parse trees
in CTB are constituency structures. We converted
them into dependency trees using the same method
and head-finding rules as in (Bikel, 2004). Follow-
ing (Bikel, 2004), we used Sections 1-270 for train-
ing, Sections 271-300 for testing and Sections 301-
325 for development. We experimented with two
sets of data: CTB-10 and CTB-15, which contains
sentences with no more than 10 and 15 words re-
spectively. Table 1, Table 2 and Table 3 show our
experimental results trained and evaluated on Chi-
nese Treebank sentences of length no more than 10,
using the standard split. For any unseen link in the
new sentences, the weight is computed as the simi-
larity weighted average of similar links seen in the
training corpus. The regularization parameter was
set by 5-fold cross-validation on the training set.
We evaluate parsing accuracy by comparing the
undirected dependency links in the parser outputs
against the undirected links in the treebank. We de-
fine the accuracy of the parser to be the percentage
of correct dependency links among the total set of
dependency links created by the parser.
Table 1 and Table 2 show that training based on
the more refined local loss is far superior to training
with the global loss of standard large margin train-
ing, on both the test and development sets. Parsing
accuracy also appears to increase with the introduc-
tion of each new feature. Notably, the pointwise mu-
tual information and distance features significantly
improve parsing accuracy—and yet we know of no
other research that has investigated these features in
this context. Finally, we note that Laplacian regular-
ization improved performance as expected, but not
for the global loss, where it appears to systemati-
cally degrade performance (n/a results did not com-
plete in time). It seems that the global loss model
may have been over-regularized (Table 3). However,
we have picked the parameter which gave us the
</bodyText>
<figure confidence="0.844349">
subject to (10)
for all constraints in
</figure>
<page confidence="0.989805">
27
</page>
<bodyText confidence="0.9999386">
best resutls in our experiments. One possible ex-
planation for this phenomenon is that the interaction
between the Laplician regularization in training and
the similarity smoothing in parsing, since distribu-
tional word similarities are used in both cases.
Finally, we compared our results to the probabilis-
tic parsing approach of (Wang et al., 2005), which on
this data obtained accuracies of 0.7631 on the CTB
test set and 0.6104 on the development set. How-
ever, we are using a much simpler feature set here.
</bodyText>
<sectionHeader confidence="0.997523" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99997925">
We have presented two improvements to the stan-
dard large margin training approach for dependency
parsing. To cope with the sparse data problem, we
smooth the parameters according to their underlying
word similarities by introducing a Laplacian regular-
izer. More significantly, we use more refined local
constraints in the large margin criterion, rather than
the global parse-level losses that are commonly con-
sidered. We achieve state of the art parsing accuracy
for predicting undirected dependencies in test data,
competitive with previous large margin and previous
probabilistic approaches in our experiments.
Much work remains to be done. One extension
is to consider directed features, and contextual fea-
tures like those used in current probabilistic parsers
(Wang et al., 2005). We would also like to apply our
approach to parsing English, investigate the confu-
sion showed in Table 3 more carefully, and possibly
re-investigate the use of parts-of-speech features in
this context.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947616666667">
Dan Bikel. 2004. Intricacies of collins’ parsing model. Com-
putational Linguistics, 30(4).
Eugene Charniak. 2000. A maximum entropy inspired parser.
In Proceedings of NAACL-2000, pages 132–139.
Colin Cherry and Dekang Lin. 2003. A probability model to
improve word alignment. In Proceedings of ACL-2003.
M. J. Collins. 1997. Three generative, lexicalized models for
statistical parsing. In Proceedings of ACL-1997.
Aron Culotta and Jeffery Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of ACL-2004.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
Proceedings of ACL-1999.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. of COLING-1996.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP-2002.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proceedings of EMNLP-2001, Pittsburgh, PA.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Press, Boston, MA.
Zelig S. Harris. 1968. Mathematical Structures of Language.
Wiley, New York.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The entire
regularization path for the support vector machine. JMLR, 5.
Donald Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-1990.
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of ACL-2003.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL-1998.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proceedings of
ACL-2005.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional cluster-
ing of english words. In Proceedings of ACL-1993.
Adwait Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-3).
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In Proc. of NIPS-2003.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In Proceedings of EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proceedings of ICML-2004.
Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly lexical
dependency parsing. In Proceedings of IWPT-2005.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2004. The penn chi-
nese treebank: Phrase structure annotation of a large corpus.
Natural Language Engineering, 10(4):1–30.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings of
IWPT-2003.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Unsupervised discovery of scenario-level patterns for
information extraction. In Proceedings of ANLP/NAACL-
2000.
Xiaojin Zhu, John Lafferty, and Zoublin Ghahramani. 2001.
Semi-supervised learning using gaussian fields and harmonic
functions. In Proceedings of ICML-2003.
</reference>
<page confidence="0.999071">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864317">
<title confidence="0.945185">Improved Large Margin Dependency via Local Constraints and Laplacian Regularization</title>
<author confidence="0.974588">Qin Iris Wang Colin Cherry Dan Lizotte Dale Schuurmans</author>
<affiliation confidence="0.9996355">Department of Computing University of Alberta</affiliation>
<email confidence="0.995222">wqin,colinc,dlizotte,dale@cs.ualberta.ca</email>
<abstract confidence="0.99982335">We present an improved approach for learning dependency parsers from treebank data. Our technique is based on two ideas for improving large margin training in the context of dependency parsing. First, we incorporate local constraints that enforce the correctness of each individual link, rather than just scoring the global parse tree. Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization. To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories. We achieve state of the art performance, improving upon current large margin approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bikel</author>
</authors>
<title>Intricacies of collins’ parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1612" citStr="Bikel, 2004" startWordPosition="238" endWordPosition="239">argin approaches. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Collins, 1997; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structure (Collins, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). In fact, recently, effective conditional parsing models have been learned using relatively straightforward “plug-in” estimates, augmented with similarity based smoothing (Wang et al., 2005). Currently, the work on conditional parsing models appears to have culminated in large margin training (Taskar et al., 2003; Taskar et</context>
<context position="6502" citStr="Bikel, 2004" startWordPosition="1001" endWordPosition="1002">guages such as Chinese, where parts-of-speech are not as clearly defined as English. In Chinese, clear indicators of a word’s partof-speech such as suffixes “-ment”, “-ous” or function words such as “the”, are largely absent. One of our motivating goals is to develop an approach to learning dependency parsers that is strictly lexical. Hence the parser can be trained with a treebank that only contains the dependency relationships, making annotation much easier. Of course, training a parser with bare word-toword relationships presents a serious challenge due to data sparseness. It was found in (Bikel, 2004) that Collins’ parser made use of bi-lexical statistics only 1.49% of the time. The parser has to compute backoff probability using parts-of-speech in vast majority of the cases. In fact, it was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state of the art PCFG parser resulted in very little change in the output. (Klein and Manning, 2003) presented an unlexicalized parser that eliminated all lexicalized parameters. Its performance was close to the state of the art lexicalized parsers. Nevertheless, in this paper we follow the recent work of (Wang et al., 2005) and c</context>
<context position="25100" citStr="Bikel, 2004" startWordPosition="4122" endWordPosition="4123">onential number over every possible parse tree in (4). Thus, we obtain a polynomial size quadratic program that can be solved for moderately large problems using standard software packages. We used CPLEX in our experiments below. As before, once optimized, the solution parameters can be introduced into the dependency model (1) according to (2). 8 Experimental Results We tested our method experimentally on the Chinese Treebank (CTB) (Xue et al., 2004). The parse trees in CTB are constituency structures. We converted them into dependency trees using the same method and head-finding rules as in (Bikel, 2004). Following (Bikel, 2004), we used Sections 1-270 for training, Sections 271-300 for testing and Sections 301- 325 for development. We experimented with two sets of data: CTB-10 and CTB-15, which contains sentences with no more than 10 and 15 words respectively. Table 1, Table 2 and Table 3 show our experimental results trained and evaluated on Chinese Treebank sentences of length no more than 10, using the standard split. For any unseen link in the new sentences, the weight is computed as the similarity weighted average of similar links seen in the training corpus. The regularization paramete</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Dan Bikel. 2004. Intricacies of collins’ parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1166" citStr="Charniak, 2000" startWordPosition="168" endWordPosition="169">ust scoring the global parse tree. Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization. To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories. We achieve state of the art performance, improving upon current large margin approaches. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Collins, 1997; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structure (Collins, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training tech</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy inspired parser. In Proceedings of NAACL-2000, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-2003.</booktitle>
<contexts>
<context position="5463" citStr="Cherry and Lin, 2003" startWordPosition="833" endWordPosition="836">t natural approaches to achieving reasonable results (Wang et al., 2005). 2 Lexicalized Dependency Parsing A dependency tree specifies which words in a sentence are directly related. That is, the dependency structure of a sentence is a directed tree where the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see Figure 1. There has been a growing interest in dependency parsing in recent years. (Fox, 2002) found that the dependency structures of a pair of translated sentences have a greater degree of cohesion than phrase structures. (Cherry and Lin, 2003) exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences. Dependency relations have also been found to be useful in information extraction (Culotta and Sorensen, 2004; Yangarber et al., 2000). A key aspect of a dependency tree is that it does not necessarily report parts-of-speech or phrase labels. Not requiring parts-of-speech is especially beneficial for languages such as Chinese, where parts-of-speech are not as clearly defined as English. In Chinese, clear indicators of a word’s partof-speech such as suffixes “-ment”, “-ous” </context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of ACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-1997.</booktitle>
<contexts>
<context position="1150" citStr="Collins, 1997" startWordPosition="166" endWordPosition="167">, rather than just scoring the global parse tree. Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization. To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories. We achieve state of the art performance, improving upon current large margin approaches. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Collins, 1997; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structure (Collins, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminati</context>
<context position="2808" citStr="Collins, 1997" startWordPosition="409" endWordPosition="410">, 2003; Taskar et al., 2004; Tsochantaridis et al., 2004; McDonald et al., 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al., 2005). Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not being used in current large margin training algorithms. However, as we demonstrate, not only can smoothing be applied in a large margin training framework, it leads to generalization improvements in much the same way as probabilistic approaches. The second key observation we make is somewhat more subtle. It turns out that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach—the “structured margin loss” (Taskar et al., 2003; Tsochantar</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. J. Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of ACL-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffery Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004.</booktitle>
<contexts>
<context position="5694" citStr="Culotta and Sorensen, 2004" startWordPosition="869" endWordPosition="872">ence is a directed tree where the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see Figure 1. There has been a growing interest in dependency parsing in recent years. (Fox, 2002) found that the dependency structures of a pair of translated sentences have a greater degree of cohesion than phrase structures. (Cherry and Lin, 2003) exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences. Dependency relations have also been found to be useful in information extraction (Culotta and Sorensen, 2004; Yangarber et al., 2000). A key aspect of a dependency tree is that it does not necessarily report parts-of-speech or phrase labels. Not requiring parts-of-speech is especially beneficial for languages such as Chinese, where parts-of-speech are not as clearly defined as English. In Chinese, clear indicators of a word’s partof-speech such as suffixes “-ment”, “-ous” or function words such as “the”, are largely absent. One of our motivating goals is to develop an approach to learning dependency parsers that is strictly lexical. Hence the parser can be trained with a treebank that only contains </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffery Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head-automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-1999.</booktitle>
<contexts>
<context position="8643" citStr="Eisner and Satta, 1999" startWordPosition="1361" endWordPosition="1364"> In particular, we assume that a directed dependency tree consists of ordered pairs of words in such that each word appears in at least one pair and each word has in-degree at most one. Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc , then is an ancestor of all the words between and . Let denote the set of all the directed, projective trees that span . Given an input sentence , we would like to be able to compute the best parse; that is, a projective tree, , that obtains the highest “score”. In particular, we follow (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005). For standard scoring function</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars. In Proceedings of ACL-1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING-1996.</booktitle>
<contexts>
<context position="8619" citStr="Eisner, 1996" startWordPosition="1359" endWordPosition="1360">e, 22 , over . In particular, we assume that a directed dependency tree consists of ordered pairs of words in such that each word appears in at least one pair and each word has in-degree at most one. Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc , then is an ancestor of all the words between and . Let denote the set of all the directed, projective trees that span . Given an input sentence , we would like to be able to compute the best parse; that is, a projective tree, , that obtains the highest “score”. In particular, we follow (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005). For s</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING-1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002.</booktitle>
<contexts>
<context position="5311" citStr="Fox, 2002" startWordPosition="810" endWordPosition="811">his is an interesting test domain because Chinese does not have clearly defined parts-of-speech, which makes lexical smoothing one of the most natural approaches to achieving reasonable results (Wang et al., 2005). 2 Lexicalized Dependency Parsing A dependency tree specifies which words in a sentence are directly related. That is, the dependency structure of a sentence is a directed tree where the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see Figure 1. There has been a growing interest in dependency parsing in recent years. (Fox, 2002) found that the dependency structures of a pair of translated sentences have a greater degree of cohesion than phrase structures. (Cherry and Lin, 2003) exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences. Dependency relations have also been found to be useful in information extraction (Culotta and Sorensen, 2004; Yangarber et al., 2000). A key aspect of a dependency tree is that it does not necessarily report parts-of-speech or phrase labels. Not requiring parts-of-speech is especially beneficial for languages such as Chines</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of EMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP-2001,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="6720" citStr="Gildea, 2001" startWordPosition="1038" endWordPosition="1039"> absent. One of our motivating goals is to develop an approach to learning dependency parsers that is strictly lexical. Hence the parser can be trained with a treebank that only contains the dependency relationships, making annotation much easier. Of course, training a parser with bare word-toword relationships presents a serious challenge due to data sparseness. It was found in (Bikel, 2004) that Collins’ parser made use of bi-lexical statistics only 1.49% of the time. The parser has to compute backoff probability using parts-of-speech in vast majority of the cases. In fact, it was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state of the art PCFG parser resulted in very little change in the output. (Klein and Manning, 2003) presented an unlexicalized parser that eliminated all lexicalized parameters. Its performance was close to the state of the art lexicalized parsers. Nevertheless, in this paper we follow the recent work of (Wang et al., 2005) and consider a completely lexicalized parser that uses no parts-ofspeech or grammatical categories of any kind. Even though a part-of-speech lexicon has always been considered to be necessary in any natural language parser,</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of EMNLP-2001, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Press,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="19745" citStr="Grefenstette, 1994" startWordPosition="3215" endWordPosition="3216">n estimate parameters for words that have never appeared in the training corpus. subject to (7) for all constraints in 25 The basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach below, we define the features of a word to be the set of words that occurred within a small window of in a large corpus. The context window of consists of the closest non-stopword on each side of and the stop-words in between. The value of a feature is defined as the pointwise mutual information between the</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Press, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="19459" citStr="Harris, 1968" startWordPosition="3169" endWordPosition="3171">cost of producing a treebank parse can run as high as 30 person-minutes per sentence (20 words on average). Similarity-based smoothing, on the other hand, allows one to tap into auxiliary sources of raw unannotated text, which is practically unlimited. With this extra data, one can estimate parameters for words that have never appeared in the training corpus. subject to (7) for all constraints in 25 The basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach below, we define the f</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zelig S. Harris. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>S Rosset</author>
<author>R Tibshirani</author>
<author>J Zhu</author>
</authors>
<title>The entire regularization path for the support vector machine.</title>
<date>2004</date>
<journal>JMLR,</journal>
<volume>5</volume>
<contexts>
<context position="11725" citStr="Hastie et al., 2004" startWordPosition="1870" endWordPosition="1873">c features used in any real situation are critical for obtaining a reasonable dependency parser. The natural sets of features to consider in this setting are very large, consisting at the very least of features indexed by all possible lexical items (words). For example, natural features to use (We describe in Section 6 below how we compute this association measure on an auxiliary corpus of unannotated text.) Another useful link feature is simply the distance between the two words in the sentence; that is, how many words they have between them position position dist 23 Using the techniques of (Hastie et al., 2004) one can show that minimizing (4) is equivalent to solving the quadratic program In fact, the likelihood of a direct link between two words diminishes quickly with distance, which motivates using more rapidly increasing functions of distance, such as the square subject to (4) dist2 position position s s In our experiments below, we used only these simple, lexically determined features, , PMI, dist and dist2, without the parts-of-speech . Currently, we only use undirected forms of these features, where, for example, for all pairs (or, put another way, we tie the parameters together for all ). I</context>
<context position="13384" citStr="Hastie et al., 2004" startWordPosition="2139" endWordPosition="2142">ndency parses, , the goal of learning is to estimate the parameters of the parsing model,. In particular, we seek values for the parameters that can accurately reconstruct the training parses, but more importantly, are also able to accurately predict the dependency parse structure on future test sentences. To train we follow the large margin training approach of (Taskar et al., 2003; Tsochantaridis et al., 2004), which has been applied with great success to dependency parsing (Taskar et al., 2004; McDonald et al., 2005). Large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004) for all which corresponds to the training problem posed in (McDonald et al., 2005). Unfortunately, the quadratic program (4) has three problems one must address. First, there are exponentially many constraints—corresponding to each possible parse of each training sentence—which forces one to use alternative training procedures, such as incremental constraint generation, to slowly converge to a solution (McDonald et al., 2005; Tsochantaridis et al., 2004). Second, and related, the original loss (4) is only evaluated at the global parse tree level, and is not targeted at penalizing any specific</context>
</contexts>
<marker>Hastie, Rosset, Tibshirani, Zhu, 2004</marker>
<rawString>T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The entire regularization path for the support vector machine. JMLR, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-1990.</booktitle>
<contexts>
<context position="19703" citStr="Hindle, 1990" startWordPosition="3208" endWordPosition="3210">imited. With this extra data, one can estimate parameters for words that have never appeared in the training corpus. subject to (7) for all constraints in 25 The basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach below, we define the features of a word to be the set of words that occurred within a small window of in a large corpus. The context window of consists of the closest non-stopword on each side of and the stop-words in between. The value of a feature is defined as th</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicateargument structures. In Proceedings of ACL-1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-2003.</booktitle>
<contexts>
<context position="6870" citStr="Klein and Manning, 2003" startWordPosition="1062" endWordPosition="1065">n be trained with a treebank that only contains the dependency relationships, making annotation much easier. Of course, training a parser with bare word-toword relationships presents a serious challenge due to data sparseness. It was found in (Bikel, 2004) that Collins’ parser made use of bi-lexical statistics only 1.49% of the time. The parser has to compute backoff probability using parts-of-speech in vast majority of the cases. In fact, it was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state of the art PCFG parser resulted in very little change in the output. (Klein and Manning, 2003) presented an unlexicalized parser that eliminated all lexicalized parameters. Its performance was close to the state of the art lexicalized parsers. Nevertheless, in this paper we follow the recent work of (Wang et al., 2005) and consider a completely lexicalized parser that uses no parts-ofspeech or grammatical categories of any kind. Even though a part-of-speech lexicon has always been considered to be necessary in any natural language parser, (Wang et al., 2005) showed that distributional word similarities from a large unannotated corpus can be used to supplant part-of-speech smoothing wit</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-1998.</booktitle>
<contexts>
<context position="19757" citStr="Lin, 1998" startWordPosition="3217" endWordPosition="3218">s for words that have never appeared in the training corpus. subject to (7) for all constraints in 25 The basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach below, we define the features of a word to be the set of words that occurred within a small window of in a large corpus. The context window of consists of the closest non-stopword on each side of and the stop-words in between. The value of a feature is defined as the pointwise mutual information between the and : PMI .</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online largemargin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="1209" citStr="McDonald et al., 2005" startWordPosition="174" endWordPosition="177">Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization. To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories. We achieve state of the art performance, improving upon current large margin approaches. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Collins, 1997; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structure (Collins, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training techniques such as maximum conditional likeliho</context>
<context position="2546" citStr="McDonald et al., 2005" startWordPosition="370" endWordPosition="373">onal parsing models have been learned using relatively straightforward “plug-in” estimates, augmented with similarity based smoothing (Wang et al., 2005). Currently, the work on conditional parsing models appears to have culminated in large margin training (Taskar et al., 2003; Taskar et al., 2004; Tsochantaridis et al., 2004; McDonald et al., 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al., 2005). Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not being used in current large margin training algorithms. However, as we demonstrate, not only can smoothing be applied in a large margin training framework, it leads to generalization improvements in much the same way as probabilistic approaches. The second key observation we make is somewhat mo</context>
<context position="8667" citStr="McDonald et al., 2005" startWordPosition="1365" endWordPosition="1368">e that a directed dependency tree consists of ordered pairs of words in such that each word appears in at least one pair and each word has in-degree at most one. Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc , then is an ancestor of all the words between and . Let denote the set of all the directed, projective trees that span . Given an input sentence , we would like to be able to compute the best parse; that is, a projective tree, , that obtains the highest “score”. In particular, we follow (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005). For standard scoring functions, parsing requires an d</context>
<context position="13289" citStr="McDonald et al., 2005" startWordPosition="2124" endWordPosition="2127">arch. 4 Large Margin Training Given a training set of sentences annotated with their correct dependency parses, , the goal of learning is to estimate the parameters of the parsing model,. In particular, we seek values for the parameters that can accurately reconstruct the training parses, but more importantly, are also able to accurately predict the dependency parse structure on future test sentences. To train we follow the large margin training approach of (Taskar et al., 2003; Tsochantaridis et al., 2004), which has been applied with great success to dependency parsing (Taskar et al., 2004; McDonald et al., 2005). Large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004) for all which corresponds to the training problem posed in (McDonald et al., 2005). Unfortunately, the quadratic program (4) has three problems one must address. First, there are exponentially many constraints—corresponding to each possible parse of each training sentence—which forces one to use alternative training procedures, such as incremental constraint generation, to slowly converge to a solution (McDonald et al., 2005; Tsochantaridis et al., 2004). Second, and related, the original loss (4) i</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online largemargin training of dependency parsers. In Proceedings of ACL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL-1993.</booktitle>
<contexts>
<context position="19725" citStr="Pereira et al., 1993" startWordPosition="3211" endWordPosition="3214">his extra data, one can estimate parameters for words that have never appeared in the training corpus. subject to (7) for all constraints in 25 The basic intuition behind similarity smoothing is that words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them can follow verbs such as administer, cancel, cheat on, conduct, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach below, we define the features of a word to be the set of words that occurred within a small window of in a large corpus. The context window of consists of the closest non-stopword on each side of and the stop-words in between. The value of a feature is defined as the pointwise mutual inf</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of english words. In Proceedings of ACL-1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1869" citStr="Ratnaparkhi, 1999" startWordPosition="273" endWordPosition="274">ased on postulating generative probability models of language that included parse structure (Collins, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). In fact, recently, effective conditional parsing models have been learned using relatively straightforward “plug-in” estimates, augmented with similarity based smoothing (Wang et al., 2005). Currently, the work on conditional parsing models appears to have culminated in large margin training (Taskar et al., 2003; Taskar et al., 2004; Tsochantaridis et al., 2004; McDonald et al., 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al., 2005). Despite the realization that maximum margin training is closely related to </context>
<context position="14136" citStr="Ratnaparkhi, 1999" startWordPosition="2252" endWordPosition="2254"> problems one must address. First, there are exponentially many constraints—corresponding to each possible parse of each training sentence—which forces one to use alternative training procedures, such as incremental constraint generation, to slowly converge to a solution (McDonald et al., 2005; Tsochantaridis et al., 2004). Second, and related, the original loss (4) is only evaluated at the global parse tree level, and is not targeted at penalizing any specific component in an incorrect parse. Although (McDonald et al., 2005) explicitly describes this as an advantage over previous approaches (Ratnaparkhi, 1999; Yamada and Matsumoto, 2003), below we find that changing the loss to enforce a more detailed set of constraints leads to a more effective approach. Third, given the large number of bi-lexical features in our model, solving (4) directly will over-fit any reasonable training corpus. (Moreover, using a large to shrink the values does not mitigate the sparse data problem introduced by having so many features.) We now present our refinements that address each of these issues in turn. 5 Training with Local Constraints We are initially focusing on training on just an undirected link model, where ea</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin markov networks.</title>
<date>2003</date>
<booktitle>In Proc. of NIPS-2003.</booktitle>
<contexts>
<context position="2201" citStr="Taskar et al., 2003" startWordPosition="319" endWordPosition="322">s (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). In fact, recently, effective conditional parsing models have been learned using relatively straightforward “plug-in” estimates, augmented with similarity based smoothing (Wang et al., 2005). Currently, the work on conditional parsing models appears to have culminated in large margin training (Taskar et al., 2003; Taskar et al., 2004; Tsochantaridis et al., 2004; McDonald et al., 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al., 2005). Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collin</context>
<context position="13149" citStr="Taskar et al., 2003" startWordPosition="2102" endWordPosition="2105">ding our approach to directed features and contextual features, as in (Wang et al., 2005), remains an important direction for future research. 4 Large Margin Training Given a training set of sentences annotated with their correct dependency parses, , the goal of learning is to estimate the parameters of the parsing model,. In particular, we seek values for the parameters that can accurately reconstruct the training parses, but more importantly, are also able to accurately predict the dependency parse structure on future test sentences. To train we follow the large margin training approach of (Taskar et al., 2003; Tsochantaridis et al., 2004), which has been applied with great success to dependency parsing (Taskar et al., 2004; McDonald et al., 2005). Large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004) for all which corresponds to the training problem posed in (McDonald et al., 2005). Unfortunately, the quadratic program (4) has three problems one must address. First, there are exponentially many constraints—corresponding to each possible parse of each training sentence—which forces one to use alternative training procedures, such as incremental constraint gen</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin markov networks. In Proc. of NIPS-2003. B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML-2004.</booktitle>
<contexts>
<context position="2251" citStr="Tsochantaridis et al., 2004" startWordPosition="327" endWordPosition="330"> research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). In fact, recently, effective conditional parsing models have been learned using relatively straightforward “plug-in” estimates, augmented with similarity based smoothing (Wang et al., 2005). Currently, the work on conditional parsing models appears to have culminated in large margin training (Taskar et al., 2003; Taskar et al., 2004; Tsochantaridis et al., 2004; McDonald et al., 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al., 2005). Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not </context>
<context position="13179" citStr="Tsochantaridis et al., 2004" startWordPosition="2106" endWordPosition="2109">directed features and contextual features, as in (Wang et al., 2005), remains an important direction for future research. 4 Large Margin Training Given a training set of sentences annotated with their correct dependency parses, , the goal of learning is to estimate the parameters of the parsing model,. In particular, we seek values for the parameters that can accurately reconstruct the training parses, but more importantly, are also able to accurately predict the dependency parse structure on future test sentences. To train we follow the large margin training approach of (Taskar et al., 2003; Tsochantaridis et al., 2004), which has been applied with great success to dependency parsing (Taskar et al., 2004; McDonald et al., 2005). Large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004) for all which corresponds to the training problem posed in (McDonald et al., 2005). Unfortunately, the quadratic program (4) has three problems one must address. First, there are exponentially many constraints—corresponding to each possible parse of each training sentence—which forces one to use alternative training procedures, such as incremental constraint generation, to slowly converge to</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of ICML-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>D Schuurmans</author>
<author>D Lin</author>
</authors>
<title>Strictly lexical dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT-2005.</booktitle>
<contexts>
<context position="1185" citStr="Wang et al., 2005" startWordPosition="170" endWordPosition="173">global parse tree. Second, to cope with sparse data, we smooth the lexical parameters according to their underlying word similarities using Laplacian Regularization. To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories. We achieve state of the art performance, improving upon current large margin approaches. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Collins, 1997; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structure (Collins, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed 21 discriminative training techniques such as maxi</context>
<context position="2828" citStr="Wang et al., 2005" startWordPosition="411" endWordPosition="414">et al., 2004; Tsochantaridis et al., 2004; McDonald et al., 2005), which currently demonstrates the state of the art performance in English dependency parsing (McDonald et al., 2005). Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not being used in current large margin training algorithms. However, as we demonstrate, not only can smoothing be applied in a large margin training framework, it leads to generalization improvements in much the same way as probabilistic approaches. The second key observation we make is somewhat more subtle. It turns out that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach—the “structured margin loss” (Taskar et al., 2003; Tsochantaridis et al., 2004; M</context>
<context position="4914" citStr="Wang et al., 2005" startWordPosition="741" endWordPosition="744"> parse, rather than just assess a global score. We show that these two extensions together lead to greater training accuracy and better generalization to novel input sentences than current large margin methods. To demonstrate the benefit of combining useful learning principles from both the probabilistic and large margin frameworks, we consider the problem of learning a dependency parser for Chinese. This is an interesting test domain because Chinese does not have clearly defined parts-of-speech, which makes lexical smoothing one of the most natural approaches to achieving reasonable results (Wang et al., 2005). 2 Lexicalized Dependency Parsing A dependency tree specifies which words in a sentence are directly related. That is, the dependency structure of a sentence is a directed tree where the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see Figure 1. There has been a growing interest in dependency parsing in recent years. (Fox, 2002) found that the dependency structures of a pair of translated sentences have a greater degree of cohesion than phrase structures. (Cherry and Lin, 2003) exploited such cohesion between the dependency str</context>
<context position="7096" citStr="Wang et al., 2005" startWordPosition="1099" endWordPosition="1102">s found in (Bikel, 2004) that Collins’ parser made use of bi-lexical statistics only 1.49% of the time. The parser has to compute backoff probability using parts-of-speech in vast majority of the cases. In fact, it was found in (Gildea, 2001) that the removal of bi-lexical statistics from a state of the art PCFG parser resulted in very little change in the output. (Klein and Manning, 2003) presented an unlexicalized parser that eliminated all lexicalized parameters. Its performance was close to the state of the art lexicalized parsers. Nevertheless, in this paper we follow the recent work of (Wang et al., 2005) and consider a completely lexicalized parser that uses no parts-ofspeech or grammatical categories of any kind. Even though a part-of-speech lexicon has always been considered to be necessary in any natural language parser, (Wang et al., 2005) showed that distributional word similarities from a large unannotated corpus can be used to supplant part-of-speech smoothing with word similarity smoothing, to still achieve state of the art dependency parsing accuracy for Chinese. Before discussing our modifications to large margin training for parsing in detail, we first present the dependency parsin</context>
<context position="9137" citStr="Wang et al., 2005" startWordPosition="1446" endWordPosition="1449">t is, a projective tree, , that obtains the highest “score”. In particular, we follow (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005). For standard scoring functions, parsing requires an dynamic programming algorithm to compute a projective tree that obtains the maximum score (Eisner and Satta, 1999; Wang et al., 2005; McDonald et al., 2005). For the purpose of learning, we decompose each link score into a weighted linear combination of features for dependency parsing are indicators of each possible word pair which allows one to represent the tendency of two words, and, to be directly linked in a parse. In this case, there is a corresponding paramete</context>
<context position="12619" citStr="Wang et al., 2005" startWordPosition="2017" endWordPosition="2020"> dist2 position position s s In our experiments below, we used only these simple, lexically determined features, , PMI, dist and dist2, without the parts-of-speech . Currently, we only use undirected forms of these features, where, for example, for all pairs (or, put another way, we tie the parameters together for all ). Ideally, we would like to use directed features, but we have already found that these simple undirected features permit state of the art accuracy in predicting (undirected) dependencies. Nevertheless, extending our approach to directed features and contextual features, as in (Wang et al., 2005), remains an important direction for future research. 4 Large Margin Training Given a training set of sentences annotated with their correct dependency parses, , the goal of learning is to estimate the parameters of the parsing model,. In particular, we seek values for the parameters that can accurately reconstruct the training parses, but more importantly, are also able to accurately predict the dependency parse structure on future test sentences. To train we follow the large margin training approach of (Taskar et al., 2003; Tsochantaridis et al., 2004), which has been applied with great succ</context>
<context position="20836" citStr="Wang et al., 2005" startWordPosition="3410" endWordPosition="3413">n each side of and the stop-words in between. The value of a feature is defined as the pointwise mutual information between the and : PMI . The similarity between two words, , is then defined as the cosine of the angle between their feature vectors. We use this similarity information both in training and in parsing. For training, we smooth the parameters according to their underlying word-pair similarities by introducing a Laplacian regularizer, which will be introduced in the next section. For parsing, the link scores in (1) are smoothed by word similarities (similar to the approach used by (Wang et al., 2005)) before the maximum score projective dependency tree is computed. 7 Laplacian Regularization We wish to incorporate similarity based smoothing in large margin training, while using the more refined constraints outlined in Section 5. Recall that most of the features we use, and therefore most of the parameters we need to estimate are based on bi-lexical parameters that serve as undirected link weights between words and in our dependency parsing model (Section 3). Here we would like to ensure that two different link weights, and , that involve similar words also take on similar values. The prev</context>
<context position="27256" citStr="Wang et al., 2005" startWordPosition="4476" endWordPosition="4479">here it appears to systematically degrade performance (n/a results did not complete in time). It seems that the global loss model may have been over-regularized (Table 3). However, we have picked the parameter which gave us the subject to (10) for all constraints in 27 best resutls in our experiments. One possible explanation for this phenomenon is that the interaction between the Laplician regularization in training and the similarity smoothing in parsing, since distributional word similarities are used in both cases. Finally, we compared our results to the probabilistic parsing approach of (Wang et al., 2005), which on this data obtained accuracies of 0.7631 on the CTB test set and 0.6104 on the development set. However, we are using a much simpler feature set here. 9 Conclusion We have presented two improvements to the standard large margin training approach for dependency parsing. To cope with the sparse data problem, we smooth the parameters according to their underlying word similarities by introducing a Laplacian regularizer. More significantly, we use more refined local constraints in the large margin criterion, rather than the global parse-level losses that are commonly considered. We achie</context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2005</marker>
<rawString>Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly lexical dependency parsing. In Proceedings of IWPT-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F Chiou</author>
<author>M Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="24942" citStr="Xue et al., 2004" startWordPosition="4095" endWordPosition="4098">efined, they are fewer in number than (4). That is, we now only have a polynomial number of constraints corresponding to each word pair in (5), rather than the exponential number over every possible parse tree in (4). Thus, we obtain a polynomial size quadratic program that can be solved for moderately large problems using standard software packages. We used CPLEX in our experiments below. As before, once optimized, the solution parameters can be introduced into the dependency model (1) according to (2). 8 Experimental Results We tested our method experimentally on the Chinese Treebank (CTB) (Xue et al., 2004). The parse trees in CTB are constituency structures. We converted them into dependency trees using the same method and head-finding rules as in (Bikel, 2004). Following (Bikel, 2004), we used Sections 1-270 for training, Sections 271-300 for testing and Sections 301- 325 for development. We experimented with two sets of data: CTB-10 and CTB-15, which contains sentences with no more than 10 and 15 words respectively. Table 1, Table 2 and Table 3 show our experimental results trained and evaluated on Chinese Treebank sentences of length no more than 10, using the standard split. For any unseen </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2004</marker>
<rawString>N. Xue, F. Xia, F. Chiou, and M. Palmer. 2004. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 10(4):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT-2003.</booktitle>
<contexts>
<context position="14165" citStr="Yamada and Matsumoto, 2003" startWordPosition="2255" endWordPosition="2258">address. First, there are exponentially many constraints—corresponding to each possible parse of each training sentence—which forces one to use alternative training procedures, such as incremental constraint generation, to slowly converge to a solution (McDonald et al., 2005; Tsochantaridis et al., 2004). Second, and related, the original loss (4) is only evaluated at the global parse tree level, and is not targeted at penalizing any specific component in an incorrect parse. Although (McDonald et al., 2005) explicitly describes this as an advantage over previous approaches (Ratnaparkhi, 1999; Yamada and Matsumoto, 2003), below we find that changing the loss to enforce a more detailed set of constraints leads to a more effective approach. Third, given the large number of bi-lexical features in our model, solving (4) directly will over-fit any reasonable training corpus. (Moreover, using a large to shrink the values does not mitigate the sparse data problem introduced by having so many features.) We now present our refinements that address each of these issues in turn. 5 Training with Local Constraints We are initially focusing on training on just an undirected link model, where each parameter in the model is </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Unsupervised discovery of scenario-level patterns for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP/NAACL2000.</booktitle>
<contexts>
<context position="5719" citStr="Yangarber et al., 2000" startWordPosition="873" endWordPosition="877">e the nodes are the words in the sentence and links represent the direct dependency relationships between the words; see Figure 1. There has been a growing interest in dependency parsing in recent years. (Fox, 2002) found that the dependency structures of a pair of translated sentences have a greater degree of cohesion than phrase structures. (Cherry and Lin, 2003) exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences. Dependency relations have also been found to be useful in information extraction (Culotta and Sorensen, 2004; Yangarber et al., 2000). A key aspect of a dependency tree is that it does not necessarily report parts-of-speech or phrase labels. Not requiring parts-of-speech is especially beneficial for languages such as Chinese, where parts-of-speech are not as clearly defined as English. In Chinese, clear indicators of a word’s partof-speech such as suffixes “-ment”, “-ous” or function words such as “the”, are largely absent. One of our motivating goals is to develop an approach to learning dependency parsers that is strictly lexical. Hence the parser can be trained with a treebank that only contains the dependency relationsh</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Unsupervised discovery of scenario-level patterns for information extraction. In Proceedings of ANLP/NAACL2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>John Lafferty</author>
<author>Zoublin Ghahramani</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-2003.</booktitle>
<contexts>
<context position="23118" citStr="Zhu et al., 2001" startWordPosition="3772" endWordPosition="3775">arameter values. Note, however, that we do not smooth the parameters,PMI,aist,aist2, corresponding to the pointwise mutual information, distance, and squared distance features described in Section 5, respectively. We only apply similarity smoothing to the bi-lexical parameters. The Laplacian regularizer (9) provides a natural smoother for the bi-lexical parameter estimates that takes into account valuable word similarity information computed as above. The Laplacian regularizer also has a significant computational advantage: it is guaranteed to be a convex quadratic function of the parameters (Zhu et al., 2001). Therefore, by combining the constraint system (7) with the Laplacian smoother (9), we can obtain a convex optimization 26 Table 1: Accuracy Results on CTB Test Set Features used Trained w/ Trained w/ local loss global loss Pairs 0.6426 0.6184 + Lap 0.6506 0.5622 + Dist 0.6546 0.6466 + Lap + Dist 0.6586 0.5542 + MI + Dist 0.6707 0.6546 + Lap + MI + Dist 0.6827 n/a Table 3: Accuracy Results on CTB Training Set Features used Trained w/ Trained w/ local loss global loss Pairs 0.9802 0.8393 + Lap 0.9777 0.7216 + Dist 0.9755 0.8376 + Lap + Dist 0.9747 0.7216 + MI + Dist 0.9768 0.7985 + Lap + MI + </context>
</contexts>
<marker>Zhu, Lafferty, Ghahramani, 2001</marker>
<rawString>Xiaojin Zhu, John Lafferty, and Zoublin Ghahramani. 2001. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of ICML-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>