<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000520">
<title confidence="0.9992635">
A System Demonstration of a Framework for Computer Assisted
Pronunciation Training
</title>
<author confidence="0.970512">
Renlong Ai, Feiyu Xu
</author>
<affiliation confidence="0.90273">
German Research Center for Artificial Intelligence, Language Technology Lab
</affiliation>
<address confidence="0.911695">
Alt-Moabit 91c, 10559 Berlin, Germany
</address>
<email confidence="0.998924">
{renlong.ai,feiyu}@dfki.de
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856944444445">
In this paper, we demonstrate a system im-
plementation of a framework for computer
assisted pronunciation training for second
language learner (L2). This framework
supports an iterative improvement of the
automatic pronunciation error recognition
and classification by allowing integration
of annotated error data. The annotated er-
ror data is acquired via an annotation tool
for linguists. This paper will give a de-
tailed description of the annotation tool
and explains the error types. Furthermore,
it will present the automatic error recogni-
tion method and the methods for automatic
visual and audio feedback. This system
demonstrates a novel approach to interac-
tive and individualized learning for pro-
nunciation training.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976218181818">
Second language (L2) acquisition is a much
greater challenge for human cognition than first
language (L1) acquisition during the critical pe-
riod. Especially by older children and adults, L2
is usually not acquired solely through imitation in
daily communication but by dedicated language
education. The teaching of second and further lan-
guages normally combines transmission of knowl-
edge with practicing of skills. One major hurdle is
the interference of L1 proficiency with L2.
Modern language learning courses are no longer
exclusively based on books or face-to-face lec-
tures. A clear trend is web-based, interactive, mul-
timedia and personalized learning. Learners want
to be flexible as to times and places for learn-
ing: home, trains, vacation, etc. Both face-to-
face teaching and 7/24 personal online language
learning services are very expensive. There is a
growing economic pressure to employ computer-
assisted methods for improving language learn-
ing in quality, efficiency and scalability. The
current philosophy of Computer Assisted Lan-
guage Learning (CALL) puts a strong emphasis on
learner-centered materials that enable learners to
work on their own. CALL tries to support two im-
portant features in language learning: interactive
learning and individualized learning.
Natural language processing (NLP) technolo-
gies play a growing important role for CALL
(Hubbard, 2009). They can help to identify errors
in student input and provide feedback so that the
learners can be aware of them (Heift, 2013; Na-
gata, 1993). Furthermore, they can help to build
models of the achieved proficiency of the learners
and provide materials and tasks appropriate.
In this paper, we demonstrate an implementa-
tion of a framework of computer assisted pronun-
ciation training for L2. The current version, which
is a further development of the Sprinter system (Ai
et al., 2014), automatically recognises and classi-
fies the pronunciation errors of learners and pro-
vides visual and audio feedback with respect to
the error types. The framework contains two sub-
systems: 1) the annotation tool which provides
linguists an easy environment for annotating pro-
nunciation errors in learners’ speech data; 2) the
speech verification tool which identifies learners’
prosody and pronunciation errors and helps to cor-
rect them.
The remainder of the paper is as follows: Sec-
tion 2 describes the system architecture; Section 3
and 4 explain how each subsystem works; Section
5 evaluates the speech verification tool; A brief
conclusion and future plan is mentioned at last in
Section 6.
</bodyText>
<sectionHeader confidence="0.984351" genericHeader="method">
2 System Description
</sectionHeader>
<figureCaption confidence="0.910165666666667">
Figure 1 depicts the framework and the interaction
between the annotation tool and the speech veri-
fication tool. As presented below, the speech ver-
</figureCaption>
<page confidence="0.757694">
1
</page>
<note confidence="0.694033">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 1–6,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<bodyText confidence="0.9342287">
ification tool is initialised with a language model
trained with audio data from 10 learners. In case
that learners’ audio data can be collected and an-
notated in the online system, the error database can
be updated on the fly and the language model can
be updated dynamically, hence speech verification
will improve itself iteratively until enough pronun-
ciation errors are gathered and no annotation will
be needed. Comparing to existing methods on
Feedback are provided to learners to improve their second language
</bodyText>
<figureCaption confidence="0.998924">
Figure 1: Overall Framework Architecture
</figureCaption>
<bodyText confidence="0.999670259259259">
pronunciation error detection, including building
classifiers with Linear Discriminant Analysis or
Decision Tree (Truong et al., 2004), or using Sup-
port Vector Machine (SVM) classifier based on ap-
plying transformation on Mel Frequency Cepstral
coefficients (MFCC) of learners’ audio data (Pi-
card et al., 2010; Ananthakrishnan et al., 2011),
our HMM classifier has the advantage that it is
trained from finely annotated L2 speech error data,
hence can recognise error types that are specifi-
cally defined in the annotations. Moreover, the re-
sults not only show the differences between gold
standard and learner data, but also contain infor-
mation of corrective feedback. Comparing to a
general Goodness of Pronunciation (GOP) score
or simply showing differences in waveform, our
feedback pinpoints different types of error down
to phoneme level and show learners how to pro-
nounce them correctly via various means. The
same annotating-training-verifying process can be
adapted to all languages as far as our open source
components support them, thus, with almost no
extra efforts. Since people with the same L1 back-
ground tend to make the same pronunciation errors
while learning a second language (Witt, 2012), we
choose a specific L1-L2 pair in our experiment,
namely, Germans who learn English.
</bodyText>
<sectionHeader confidence="0.967639" genericHeader="method">
3 Annotation Tool
</sectionHeader>
<bodyText confidence="0.9988655">
To provide annotators an easy and efficient in-
terface, we have further developed MAT (Ai and
</bodyText>
<figureCaption confidence="0.998545">
Figure 2: Screenshot of the Annotation Tool
</figureCaption>
<bodyText confidence="0.99793675">
Charfuelan, 2014), with which pronunciation er-
rors can be annotated via simple mouse clicks and
single phoneme inputs from keyboard. During the
annotation, errors are automatically stored in an
error database, which then updates the speech ver-
ification tool. We also add checking mechanisms
to validate the annotations and ask annotators to
deal with conflicts.
</bodyText>
<subsectionHeader confidence="0.984047">
3.1 Target Pronunciation Errors
</subsectionHeader>
<bodyText confidence="0.747855333333333">
Four types of phoneme-level pronunciation errors
can be annotated in the ways as shown in Figure
2. They are:
</bodyText>
<listItem confidence="0.876348818181818">
• Deletion: A phoneme is removed from
learner’s utterance, e.g. to omit /g/ in “Eng-
land”. Annotators only need to check the
checkbox in column deletion.
• Insertion: A phoneme is inserted after an-
other one, e.g. learner tries to pronounce a
silent letter, like ‘b’ in “dumb”. In this case
the annotator should check insertion and type
the inserted phoneme in column spoken.
• Substitution: A phoneme is replaced by an-
other one, e.g. replacing /z/ with /s/ in “was”.
The annotator should check substitution and
type the actually pronounced phoneme in
spoken.
• Distortion: A phoneme is not pronounced
fully correct, yet is not so wrong that it be-
comes another phoneme, e.g. pronouncing
/u/ with tongue slightly backward. In this
case the annotator should also select a hint
from the drop down list in hint column to
indicate how the error phoneme can be cor-
rected.
</listItem>
<figure confidence="0.998940090909091">
Learners
Annotators
Annotation Tool
Error Database
+
Trained Model for
Error Detection
Learner’s Audio
Data
Speech Verification
Tool
</figure>
<page confidence="0.986753">
2
</page>
<subsectionHeader confidence="0.999067">
3.2 Annotations
</subsectionHeader>
<bodyText confidence="0.999963555555556">
As depicted in Figure 2, annotations can be con-
ducted easily and directly with this tool. If an an-
notator finds a pronunciation error, he/she can sim-
ply check the checkbox at line: phoneme and col-
umn: error type from the table, and provide extra
information of the actually spoken phoneme and
also hints of how to pronounce correctly. To make
the annotation more convenient and efficient, sev-
eral functions are built in the tool:
</bodyText>
<listItem confidence="0.90894236">
• Phoneme sequences are generated via MARY
phonemizer. Phonemes are listed in the first
column of the table and also in the header of
the waveform panel. Clicking on a phoneme
in the header will highlight the row of the
corresponding phoneme in the table, and vice
versa. The word, which the clicked phoneme
belongs to, is also highlighted in the middle
panel where the sentence is shown. Syllables
in words are also retrieved from text analysis
and displayed in the first column.
• Phoneme boundaries are recognized via
forced alignment performed with HTK. An-
notators can play any single phoneme, sylla-
ble or word by double-clicking them in the
first column, or play any part of the sentence
by choosing a range in the waveform panel
with mouse and hit space on keyboard. In this
way annotators can focus on error phonemes
without bothering to listen to the whole sen-
tence.
• There is already a list of hints on articulation
provided. If a correction cannot be found in
the list, annotators can click hints button and
add the correction to the list.
</listItem>
<bodyText confidence="0.836185">
After the annotation for a single audio file is done,
the annotator clicks Commit to submit the annota-
tions. The Commit function will check the anno-
tation additionally:
</bodyText>
<listItem confidence="0.949906916666667">
• If insertion is marked, the inserted phoneme
should be also written in spoken.
• If substitution is marked, the phoneme, which
the original phoneme is substituted to, should
also be written in spoken.
• If distortion is marked, a comment should be
provided by annotators to indicate in which
way this phoneme is distorted.
• Only one kind of error can be annotated per
phoneme.
If the annotations are valid, the pronunciation
errors and their diphone or triphone informations
</listItem>
<figureCaption confidence="0.998999">
Figure 3: Components in annotation tool
</figureCaption>
<bodyText confidence="0.999769">
are stored in an error database, which is later used
in training the language model for error detection
and also in feedback generation.
</bodyText>
<sectionHeader confidence="0.996206" genericHeader="method">
4 Speech Verification Tool
</sectionHeader>
<bodyText confidence="0.9999755">
The speech verification tool identifies the pronun-
ciation errors in learners’ speech and provides
feedback on correction, and also analyze the dif-
ferences in pitch and duration between gold stan-
dard and learners’ speech and display them in
comprehensive ways. The goal is to help learners
speak second language error-free and with rhythm
and intonation like native speakers.
</bodyText>
<subsectionHeader confidence="0.998471">
4.1 Pronunciation Error Detection
</subsectionHeader>
<bodyText confidence="0.999967666666667">
Pronunciation error detection is realized by per-
forming phoneme recognition with HTK and com-
paring the correct and recognized phoneme se-
quence. Errors can be retrieved from the differ-
ences between the sequences. The sentence read
by learner is processed by MARY phonemizer to
generate the correct phoneme sequence. Possible
pronunciation errors are then fetched from the er-
ror database based on the phonemes and their di-
phone and triphone information in the sequences.
A grammar for phoneme recognition is then com-
posed from the errors and the phoneme sequence.
</bodyText>
<figureCaption confidence="0.99363">
Figure 4: Workflow of Pronunciation Error Detection
</figureCaption>
<bodyText confidence="0.7117275">
Errors are handled differently as they appear in
the grammar:
</bodyText>
<listItem confidence="0.9912755">
• Deletion means a phoneme can be optional in
the grammar.
</listItem>
<figure confidence="0.994723025641025">
Annotator
Annotations in
Extended MaryXML
Format
Speech
Model
Audio
Text
Signal Processing
Forced Alignment
Text Analysis
MARY TTS Annotations Tool
Acoustic parameters
Label Files
MaryXML
text
MARY phonemizer
correct phoneme
sequence
sequence
identical?
not identical
✗
grammar
Unmatched phonemes are
possibly mispronounced.
error database
dictionary
recognized
phoneme sequence
trained model
audio
feature extraction
features
✔
sequences are identical
No pronunciation
erroris
detected.
</figure>
<page confidence="0.929705">
3
</page>
<listItem confidence="0.859975375">
• Insertion means an extra phoneme, i.e. the
inserted one, can be optional in the grammar.
• Substitution means multiple phonemes can
appear at this position: the correct one and
the substituted ones.
• Distortion also means multiple phonemes can
appear at the same position: the correct and
their distorted alias, which are represented
with phoneme plus number. For example, the
phoneme /a/ can be distorted in two ways:
either tongue is placed backward or tongue
starts too forward. Since /a/ is represented as
/A/ in MARY, its two distorted versions are
/A1/ and /A2/.
If a given sentence with its MARY phoneme
sequence is:
</listItem>
<bodyText confidence="0.957591666666667">
I’ll be in London for the whole year.
A l b i I n l V n d @ n fOr D @ h @U l jIr
A grammar with following content
</bodyText>
<equation confidence="0.985422">
(silAlb(i |i1) I n l (V |A |O) n {d} @ n f
(O |O2) r (D |z) @ h @U l j (I |I1) {(r |A)} sil)
</equation>
<bodyText confidence="0.9977445">
will be generated, because the following pronun-
ciation errors have been learned:
</bodyText>
<listItem confidence="0.9945553">
1. /i/ in be can be distorted.
2. /A/ in London can be substituted with /a/ or
/O/.
3. /d/ in London can be removed by learners
while pronouncing.
4. /O:/ in for can be distorted.
5. /d/ in the can be replaced with /z/.
6. /i:/ in year can be distorted.
7. /a/ in year can be substituted with /a/, and can
also be deleted by learners.
</listItem>
<bodyText confidence="0.999325181818182">
If the predefined errors appear in learners’
speech, they will be identified by comparing the
different phonemes in the correct phoneme se-
quence generated with MARY phonemizer and the
recognized sequence with HTK. Moreover, cor-
rective feedback can also be created by fetching
the hint, which annotators provide, from the error
database. E.g. if /A1/ instead of /A/ is recognized
for word are, we can provide the hint for this dis-
torted /a/: Tongue needs to be slightly further for-
ward.
</bodyText>
<subsectionHeader confidence="0.967673">
4.2 Prosody Differences
</subsectionHeader>
<bodyText confidence="0.999582375">
Prosody differences between gold standard and
learners’ speech are calculated and shown to learn-
ers, not only visually, but also with audio feed-
back. The prosody from gold standard is applied
to learners’ speech, so learners can hear their own
voice with native prosody, in this way it might be
easier for them to mimic the gold standard prosody
(Flege, 1995).
</bodyText>
<figureCaption confidence="0.991616">
Figure 5: Workflow of Prosody Comparison and Transplan-
tation
</figureCaption>
<bodyText confidence="0.999987172413793">
Duration differences can be calculated directly
from the results of forced alignment. To gain more
accurate alignment, the language model is trained
with gold standard data and a selected set of learn-
ers’ data. Before displaying the differences to
learners as in Figure 6, the durations in learners’
speech are normalized so that only words with
large duration differences are shown.
Pitch differences are calculated with Snack
Sound Toolkit1. Dynamic time warping is ap-
plied to the gold standard against speech data from
learners, so that differences in pitch can be shown
per phoneme synchronously. Due to the fact that
each person has a distinct baseline in pitch, differ-
ences in pitch value are not considered as differ-
ences, we show only differences in pitch variation.
We use FD-PSOLA combined with DTW
(Latsch and Netto, 2011) to perform prosody
transplantation and generate audio feedback that
learners can perceive. Despite the high perfor-
mance of the method, there are still cases that
prosody transplantation yields faulty results, e.g.
the synthesized speech sounds very artificial or
there are significant gaps between words or even
phonemes. Therefore a scale factor calculator runs
before the transplantation to determine if it makes
sense to transplant the prosody, and will only do it
and provide it as feedback when the scale factors
do not exceed the threshold.
</bodyText>
<subsectionHeader confidence="0.998143">
4.3 User Interface
</subsectionHeader>
<bodyText confidence="0.993109">
Figure 6 shows a screenshot of the speech verifi-
cation tool. Learners can choose sentences they
want to practice from the list in the left, and
</bodyText>
<footnote confidence="0.823734">
1http://www.speech.kth.se/snack/
</footnote>
<figure confidence="0.987121846153846">
Audio Data from
Learner and Gold
Standard
Speech Model
Text
MaryTTS NLP Tools
MaryXML
Snack
HTK
Extracted Pitch
Contours
Scale Factor
Calculator
Visualized
Comparison of
Pitch and Duration
Forced Alignment
Results of both
Learners and
Gold Standard
Perform Prosody Transplantation
only when it’s safe to do so.
FD-PSOLA
Processor
Prosody
Transplantation
</figure>
<page confidence="0.986799">
4
</page>
<bodyText confidence="0.99914025">
click Record and speak to the microphone. Af-
ter the audio is recorded, they can click Check
to display the verification results. Pronunciation
errors are marked in the first panel with colors
other than green. Red, pink, yellow and pur-
ple are used for substitution, distortion, deletion
and insertion. The second panel shows the dif-
ferences in pitches, significant and medium dif-
ferences are rendered with red and yellow. The
panel below shows the differences in durations.
Hints of correction or improvement are shown as
text if learners click on the colored phonemes or
words. If prosody transplantation is feasible, the
button Transplanted is enabled and will play audio
with learners’ voice and transplanted gold stan-
dard prosody upon click.
</bodyText>
<figureCaption confidence="0.99737">
Figure 6: Screenshot of the Speech Verification Tool
</figureCaption>
<sectionHeader confidence="0.995429" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99995425">
Experiments are conducted both objectively and
subjectively, in order to evaluate the performance
of error detection and also how our feedback can
help learners improve their second language skills
</bodyText>
<subsectionHeader confidence="0.997159">
5.1 Objective Evaluation
</subsectionHeader>
<bodyText confidence="0.999669769230769">
To train the language model for pronunciation er-
ror detection, we let 1506 sentences read by native
Britons. Given these sentences, 96 sentences are
carefully selected, which cover almost all typical
pronunciation errors made by Germans who learn
English, and have been read by 10 Germans at dif-
ferent English skill levels. To evaluate our error
detection system, we let 4 additional German peo-
ple read these 96 sentences. The recordings were
sent to both annotators and the error detection sys-
tem. The results from error detection are trans-
formed to MARYXML format and compared with
the annotations.
</bodyText>
<table confidence="0.987991285714286">
true false false total recall precision
positive positive negative
deletion 46 0 4 50 92% 100%
insertion 17 0 1 18 94.4% 100%
substitution 1264 14 2 1266 99.8% 98.9%
distortion 745 102 26 771 96.6% 88.0%
total 2072 116 33 2105 98.4% 94.6%
</table>
<tableCaption confidence="0.71583375">
Table 1: A statistic of the error detection result. True pos-
itive: actually detected errors; false positive: correct pro-
nounced phonemes detected as errors; false negative: errors
not detected.
</tableCaption>
<bodyText confidence="0.99996824">
The results show a very high precision in recog-
nizing deletion and insertion. The recalls are also
very good considering that there are new deletion
phenomena in testers’ speech that are not involved
in old training data. Although a large amount of
substitution errors appear in the test data, they
have been detected accurately. This proves that
training a language model considering specific L1
background is important for correct error recogni-
tion. For example in our case, most typical sub-
stitution errors made by Germans are well trained,
like pronouncing /b/ like /z/ in the, or /z/ like /s/ in
was.
Detecting distortion errors seems a more dif-
ficult task for the system. Although a good re-
call is achieved, the precision is not satisfying. In
CALL, this is perhaps not helpful because learn-
ers will be discouraged if they make correct pro-
nunciation but are told wrong by the system. More
speech data is required for training the model. Our
annotators are experienced linguists but they may
still holds different criterion on judging distortion.
Having more linguists working on the annotation
should also help to improve the accuracy of error
detection.
</bodyText>
<subsectionHeader confidence="0.998158">
5.2 Subjective Evaluation
</subsectionHeader>
<bodyText confidence="0.999509466666667">
To evaluate whether and how our feedback can
help learners improve their second language, we
designed a progressive experiment. 4 learners are
chosen to read 30 sentences from the list. They
can listen to gold standard as many times as they
need, and record the speech when they are ready.
If there are pronunciation errors or prosody differ-
ences, they can view hints or listen to gold stan-
dard, and then try again. If there are still prosody
differences, they can then hear the transplanted
speech, as many times as they need, and then try
to record again. Insertions and deletions are easily
corrected by learners once they have been pointed
out. We examined the substitution errors that were
not corrected, and found most of them are be-
</bodyText>
<page confidence="0.978232">
5
</page>
<table confidence="0.992448666666667">
insertion deletion substitution distortion total
detected errors 13 4 467 220 704
corrected errors 13 4 429 116 562
</table>
<tableCaption confidence="0.9991935">
Table 2: Amount of detected and corrected pronunciation
errors from test speech data.
</tableCaption>
<bodyText confidence="0.993840666666667">
tween phoneme /æ/ and /e/, and also /@Ú/ and /O/.
The differences between the phoneme pairs were
not easily perceived by learners, and they need to
be taught systematically how to pronounce these
phonemes. It was difficult for learners to correct
distortions. Besides providing tutorial on how to
pronounce error phonemes correctly, our system
also needs to be modified so that it doesn’t handle
distortion so strictly.
</bodyText>
<table confidence="0.959926">
differences in count correct after corrected after listening
of phonemes (pitch) viewing hints to prosody transplantation
or words (duration)
pitch 474 343 438
duration 205 135 177
</table>
<tableCaption confidence="0.999436">
Table 3: Amount of detected and corrected prosody differ-
ences from test speech data.
</tableCaption>
<bodyText confidence="0.999925333333333">
The results on prosody differences show that
most of these differences can be perceived and ad-
justed by learners, if they are given enough infor-
mation. Almost all the remaining differences are
from long sentences. Learners couldn’t pay atten-
tion to all differences in one attempt. We believe
that learners should be able to read all sentences
in native intonation and rhythm if they try another
several times.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999744538461538">
In this paper we presents a framework for com-
puter assisted pronunciation training. Its anno-
tation tool substantially simplifies the acquisition
of speech error data, while its speech verifica-
tion tool automatically detects pronunciation er-
rors and prosody differences in learners’ speech
and provide corrective feedback. Objective and
subjective evaluations show that the system not
only performs accurately in error detection but
also helps learners realize and correct their errors.
In the future, we attempt to integrate more feed-
back content such as video tutorial or articulation
animation for teaching pronunciation.
</bodyText>
<sectionHeader confidence="0.999029" genericHeader="acknowledgments">
7 Acknowledgment
</sectionHeader>
<bodyText confidence="0.86726475">
This research was partially supported by the
German Federal Ministry of Education and Re-
search (BMBF) through the projects Deependance
(01IW11003) and ALL SIDES (01IW14002).
</bodyText>
<sectionHeader confidence="0.989496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99968475">
Renlong Ai and Marcela Charfuelan. 2014. Mat:
a tool for l2 pronunciation errors annotation. In
Proceedings of the 9th International Conference on
Language Resources and Evaluation (LREC-2014).
European Language Resources Association.
Renlong Ai, Marcela Charfuelan, Walter Kasper, Tina
Klwer, Hans Uszkoreit, Feiyu Xu, Sandra Gas-
ber, and Philip Gienandt. 2014. Sprinter: Lan-
guage technologies for interactive and multimedia
language learning. In Proceedings of the 9th In-
ternational Conference on Language Resources and
Evaluation (LREC-2014). European Language Re-
sources Association.
Gopal Ananthakrishnan, Preben Wik, Olov Engwall,
and Sherif Abdou. 2011. Using an ensemble
of classifiers for mispronunciation feedback. In
SLaTE, pages 49–52.
James E Flege. 1995. Second language speech learn-
ing: Theory, findings, and problems. Speech Per-
ception and Linguistic Experience: Theoretical and
Methodological Issues, pages 233–273.
Trude Heift. 2013. Learner control and error correc-
tion in icall: Browsers, peekers, and adamants. Cal-
ico Journal, 19(2):295–313.
Philip Hubbard. 2009. Computer Assisted Language
Learning: Critical Concepts in Linguistics, volume
I-IV. London &amp; New York: Routledge.
V. L. Latsch and S. L. Netto. 2011. Pitch-synchronous
time alignment of speech signals for prosody trans-
plantation. In IEEE International Symposium on
Circuits and Systems (ISCAS), Rio de Janeiro,
Brazil.
Noriko Nagata. 1993. Intelligent computer feedback
for second language instruction. The Modern Lan-
guage Journal, 77(3):330–339.
S´ebastien Picard, Gopal Ananthakrishnan, Preben Wik,
Olov Engwall, and Sherif Abdou. 2010. Detection
of specific mispronunciations using audiovisual fea-
tures. In AVSP, pages 7–2.
Khiet Truong, Ambra Neri, Catia Cucchiarini, and
Helmer Strik. 2004. Automatic pronunciation er-
ror detection: an acoustic-phonetic approach. In In-
STIL/ICALL Symposium 2004.
S. M. Witt. 2012. Automatic error detection in pronun-
ciation training: where we are and where we need to
go. In International Symposium on Automatic De-
tection of Errors in Pronunciation Training, Stock-
holm, Sweden.
</reference>
<page confidence="0.998777">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.355710">
<title confidence="0.9985025">A System Demonstration of a Framework for Computer Pronunciation Training</title>
<author confidence="0.691502">Renlong Ai</author>
<author confidence="0.691502">Feiyu</author>
<affiliation confidence="0.561495">German Research Center for Artificial Intelligence, Language Technology</affiliation>
<address confidence="0.470324">Alt-Moabit 91c, 10559 Berlin,</address>
<abstract confidence="0.997610052631579">In this paper, we demonstrate a system implementation of a framework for computer assisted pronunciation training for second language learner (L2). This framework supports an iterative improvement of the automatic pronunciation error recognition and classification by allowing integration of annotated error data. The annotated error data is acquired via an annotation tool for linguists. This paper will give a detailed description of the annotation tool and explains the error types. Furthermore, it will present the automatic error recognition method and the methods for automatic visual and audio feedback. This system demonstrates a novel approach to interactive and individualized learning for pronunciation training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Renlong Ai</author>
<author>Marcela Charfuelan</author>
</authors>
<title>Mat: a tool for l2 pronunciation errors annotation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014). European Language Resources Association.</booktitle>
<marker>Ai, Charfuelan, 2014</marker>
<rawString>Renlong Ai and Marcela Charfuelan. 2014. Mat: a tool for l2 pronunciation errors annotation. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014). European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renlong Ai</author>
<author>Marcela Charfuelan</author>
<author>Walter Kasper</author>
<author>Tina Klwer</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
<author>Sandra Gasber</author>
<author>Philip Gienandt</author>
</authors>
<title>Sprinter: Language technologies for interactive and multimedia language learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014). European Language Resources Association.</booktitle>
<contexts>
<context position="2854" citStr="Ai et al., 2014" startWordPosition="425" endWordPosition="428">ng and individualized learning. Natural language processing (NLP) technologies play a growing important role for CALL (Hubbard, 2009). They can help to identify errors in student input and provide feedback so that the learners can be aware of them (Heift, 2013; Nagata, 1993). Furthermore, they can help to build models of the achieved proficiency of the learners and provide materials and tasks appropriate. In this paper, we demonstrate an implementation of a framework of computer assisted pronunciation training for L2. The current version, which is a further development of the Sprinter system (Ai et al., 2014), automatically recognises and classifies the pronunciation errors of learners and provides visual and audio feedback with respect to the error types. The framework contains two subsystems: 1) the annotation tool which provides linguists an easy environment for annotating pronunciation errors in learners’ speech data; 2) the speech verification tool which identifies learners’ prosody and pronunciation errors and helps to correct them. The remainder of the paper is as follows: Section 2 describes the system architecture; Section 3 and 4 explain how each subsystem works; Section 5 evaluates the </context>
</contexts>
<marker>Ai, Charfuelan, Kasper, Klwer, Uszkoreit, Xu, Gasber, Gienandt, 2014</marker>
<rawString>Renlong Ai, Marcela Charfuelan, Walter Kasper, Tina Klwer, Hans Uszkoreit, Feiyu Xu, Sandra Gasber, and Philip Gienandt. 2014. Sprinter: Language technologies for interactive and multimedia language learning. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014). European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gopal Ananthakrishnan</author>
<author>Preben Wik</author>
<author>Olov Engwall</author>
<author>Sherif Abdou</author>
</authors>
<title>Using an ensemble of classifiers for mispronunciation feedback. In SLaTE,</title>
<date>2011</date>
<pages>49--52</pages>
<contexts>
<context position="4736" citStr="Ananthakrishnan et al., 2011" startWordPosition="717" endWordPosition="720">amically, hence speech verification will improve itself iteratively until enough pronunciation errors are gathered and no annotation will be needed. Comparing to existing methods on Feedback are provided to learners to improve their second language Figure 1: Overall Framework Architecture pronunciation error detection, including building classifiers with Linear Discriminant Analysis or Decision Tree (Truong et al., 2004), or using Support Vector Machine (SVM) classifier based on applying transformation on Mel Frequency Cepstral coefficients (MFCC) of learners’ audio data (Picard et al., 2010; Ananthakrishnan et al., 2011), our HMM classifier has the advantage that it is trained from finely annotated L2 speech error data, hence can recognise error types that are specifically defined in the annotations. Moreover, the results not only show the differences between gold standard and learner data, but also contain information of corrective feedback. Comparing to a general Goodness of Pronunciation (GOP) score or simply showing differences in waveform, our feedback pinpoints different types of error down to phoneme level and show learners how to pronounce them correctly via various means. The same annotating-training</context>
</contexts>
<marker>Ananthakrishnan, Wik, Engwall, Abdou, 2011</marker>
<rawString>Gopal Ananthakrishnan, Preben Wik, Olov Engwall, and Sherif Abdou. 2011. Using an ensemble of classifiers for mispronunciation feedback. In SLaTE, pages 49–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James E Flege</author>
</authors>
<title>Second language speech learning: Theory, findings, and problems. Speech Perception and Linguistic Experience: Theoretical and Methodological Issues,</title>
<date>1995</date>
<pages>233--273</pages>
<contexts>
<context position="13450" citStr="Flege, 1995" startWordPosition="2178" endWordPosition="2179">reated by fetching the hint, which annotators provide, from the error database. E.g. if /A1/ instead of /A/ is recognized for word are, we can provide the hint for this distorted /a/: Tongue needs to be slightly further forward. 4.2 Prosody Differences Prosody differences between gold standard and learners’ speech are calculated and shown to learners, not only visually, but also with audio feedback. The prosody from gold standard is applied to learners’ speech, so learners can hear their own voice with native prosody, in this way it might be easier for them to mimic the gold standard prosody (Flege, 1995). Figure 5: Workflow of Prosody Comparison and Transplantation Duration differences can be calculated directly from the results of forced alignment. To gain more accurate alignment, the language model is trained with gold standard data and a selected set of learners’ data. Before displaying the differences to learners as in Figure 6, the durations in learners’ speech are normalized so that only words with large duration differences are shown. Pitch differences are calculated with Snack Sound Toolkit1. Dynamic time warping is applied to the gold standard against speech data from learners, so th</context>
</contexts>
<marker>Flege, 1995</marker>
<rawString>James E Flege. 1995. Second language speech learning: Theory, findings, and problems. Speech Perception and Linguistic Experience: Theoretical and Methodological Issues, pages 233–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trude Heift</author>
</authors>
<title>Learner control and error correction in icall: Browsers, peekers, and adamants.</title>
<date>2013</date>
<journal>Calico Journal,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2498" citStr="Heift, 2013" startWordPosition="369" endWordPosition="370">loy computerassisted methods for improving language learning in quality, efficiency and scalability. The current philosophy of Computer Assisted Language Learning (CALL) puts a strong emphasis on learner-centered materials that enable learners to work on their own. CALL tries to support two important features in language learning: interactive learning and individualized learning. Natural language processing (NLP) technologies play a growing important role for CALL (Hubbard, 2009). They can help to identify errors in student input and provide feedback so that the learners can be aware of them (Heift, 2013; Nagata, 1993). Furthermore, they can help to build models of the achieved proficiency of the learners and provide materials and tasks appropriate. In this paper, we demonstrate an implementation of a framework of computer assisted pronunciation training for L2. The current version, which is a further development of the Sprinter system (Ai et al., 2014), automatically recognises and classifies the pronunciation errors of learners and provides visual and audio feedback with respect to the error types. The framework contains two subsystems: 1) the annotation tool which provides linguists an eas</context>
</contexts>
<marker>Heift, 2013</marker>
<rawString>Trude Heift. 2013. Learner control and error correction in icall: Browsers, peekers, and adamants. Calico Journal, 19(2):295–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Hubbard</author>
</authors>
<title>Computer Assisted Language Learning: Critical Concepts in Linguistics, volume I-IV. London &amp;</title>
<date>2009</date>
<publisher>Routledge.</publisher>
<location>New York:</location>
<contexts>
<context position="2371" citStr="Hubbard, 2009" startWordPosition="346" endWordPosition="347">oface teaching and 7/24 personal online language learning services are very expensive. There is a growing economic pressure to employ computerassisted methods for improving language learning in quality, efficiency and scalability. The current philosophy of Computer Assisted Language Learning (CALL) puts a strong emphasis on learner-centered materials that enable learners to work on their own. CALL tries to support two important features in language learning: interactive learning and individualized learning. Natural language processing (NLP) technologies play a growing important role for CALL (Hubbard, 2009). They can help to identify errors in student input and provide feedback so that the learners can be aware of them (Heift, 2013; Nagata, 1993). Furthermore, they can help to build models of the achieved proficiency of the learners and provide materials and tasks appropriate. In this paper, we demonstrate an implementation of a framework of computer assisted pronunciation training for L2. The current version, which is a further development of the Sprinter system (Ai et al., 2014), automatically recognises and classifies the pronunciation errors of learners and provides visual and audio feedback</context>
</contexts>
<marker>Hubbard, 2009</marker>
<rawString>Philip Hubbard. 2009. Computer Assisted Language Learning: Critical Concepts in Linguistics, volume I-IV. London &amp; New York: Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V L Latsch</author>
<author>S L Netto</author>
</authors>
<title>Pitch-synchronous time alignment of speech signals for prosody transplantation.</title>
<date>2011</date>
<booktitle>In IEEE International Symposium on Circuits and Systems (ISCAS), Rio de Janeiro,</booktitle>
<location>Brazil.</location>
<contexts>
<context position="14346" citStr="Latsch and Netto, 2011" startWordPosition="2321" endWordPosition="2324"> Before displaying the differences to learners as in Figure 6, the durations in learners’ speech are normalized so that only words with large duration differences are shown. Pitch differences are calculated with Snack Sound Toolkit1. Dynamic time warping is applied to the gold standard against speech data from learners, so that differences in pitch can be shown per phoneme synchronously. Due to the fact that each person has a distinct baseline in pitch, differences in pitch value are not considered as differences, we show only differences in pitch variation. We use FD-PSOLA combined with DTW (Latsch and Netto, 2011) to perform prosody transplantation and generate audio feedback that learners can perceive. Despite the high performance of the method, there are still cases that prosody transplantation yields faulty results, e.g. the synthesized speech sounds very artificial or there are significant gaps between words or even phonemes. Therefore a scale factor calculator runs before the transplantation to determine if it makes sense to transplant the prosody, and will only do it and provide it as feedback when the scale factors do not exceed the threshold. 4.3 User Interface Figure 6 shows a screenshot of th</context>
</contexts>
<marker>Latsch, Netto, 2011</marker>
<rawString>V. L. Latsch and S. L. Netto. 2011. Pitch-synchronous time alignment of speech signals for prosody transplantation. In IEEE International Symposium on Circuits and Systems (ISCAS), Rio de Janeiro, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Nagata</author>
</authors>
<title>Intelligent computer feedback for second language instruction.</title>
<date>1993</date>
<journal>The Modern Language Journal,</journal>
<volume>77</volume>
<issue>3</issue>
<contexts>
<context position="2513" citStr="Nagata, 1993" startWordPosition="371" endWordPosition="373">ssisted methods for improving language learning in quality, efficiency and scalability. The current philosophy of Computer Assisted Language Learning (CALL) puts a strong emphasis on learner-centered materials that enable learners to work on their own. CALL tries to support two important features in language learning: interactive learning and individualized learning. Natural language processing (NLP) technologies play a growing important role for CALL (Hubbard, 2009). They can help to identify errors in student input and provide feedback so that the learners can be aware of them (Heift, 2013; Nagata, 1993). Furthermore, they can help to build models of the achieved proficiency of the learners and provide materials and tasks appropriate. In this paper, we demonstrate an implementation of a framework of computer assisted pronunciation training for L2. The current version, which is a further development of the Sprinter system (Ai et al., 2014), automatically recognises and classifies the pronunciation errors of learners and provides visual and audio feedback with respect to the error types. The framework contains two subsystems: 1) the annotation tool which provides linguists an easy environment f</context>
</contexts>
<marker>Nagata, 1993</marker>
<rawString>Noriko Nagata. 1993. Intelligent computer feedback for second language instruction. The Modern Language Journal, 77(3):330–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien Picard</author>
<author>Gopal Ananthakrishnan</author>
<author>Preben Wik</author>
<author>Olov Engwall</author>
<author>Sherif Abdou</author>
</authors>
<title>Detection of specific mispronunciations using audiovisual features.</title>
<date>2010</date>
<booktitle>In AVSP,</booktitle>
<pages>7--2</pages>
<contexts>
<context position="4705" citStr="Picard et al., 2010" startWordPosition="712" endWordPosition="716">el can be updated dynamically, hence speech verification will improve itself iteratively until enough pronunciation errors are gathered and no annotation will be needed. Comparing to existing methods on Feedback are provided to learners to improve their second language Figure 1: Overall Framework Architecture pronunciation error detection, including building classifiers with Linear Discriminant Analysis or Decision Tree (Truong et al., 2004), or using Support Vector Machine (SVM) classifier based on applying transformation on Mel Frequency Cepstral coefficients (MFCC) of learners’ audio data (Picard et al., 2010; Ananthakrishnan et al., 2011), our HMM classifier has the advantage that it is trained from finely annotated L2 speech error data, hence can recognise error types that are specifically defined in the annotations. Moreover, the results not only show the differences between gold standard and learner data, but also contain information of corrective feedback. Comparing to a general Goodness of Pronunciation (GOP) score or simply showing differences in waveform, our feedback pinpoints different types of error down to phoneme level and show learners how to pronounce them correctly via various mean</context>
</contexts>
<marker>Picard, Ananthakrishnan, Wik, Engwall, Abdou, 2010</marker>
<rawString>S´ebastien Picard, Gopal Ananthakrishnan, Preben Wik, Olov Engwall, and Sherif Abdou. 2010. Detection of specific mispronunciations using audiovisual features. In AVSP, pages 7–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khiet Truong</author>
<author>Ambra Neri</author>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
</authors>
<title>Automatic pronunciation error detection: an acoustic-phonetic approach.</title>
<date>2004</date>
<booktitle>In InSTIL/ICALL Symposium</booktitle>
<contexts>
<context position="4531" citStr="Truong et al., 2004" startWordPosition="685" endWordPosition="688">data from 10 learners. In case that learners’ audio data can be collected and annotated in the online system, the error database can be updated on the fly and the language model can be updated dynamically, hence speech verification will improve itself iteratively until enough pronunciation errors are gathered and no annotation will be needed. Comparing to existing methods on Feedback are provided to learners to improve their second language Figure 1: Overall Framework Architecture pronunciation error detection, including building classifiers with Linear Discriminant Analysis or Decision Tree (Truong et al., 2004), or using Support Vector Machine (SVM) classifier based on applying transformation on Mel Frequency Cepstral coefficients (MFCC) of learners’ audio data (Picard et al., 2010; Ananthakrishnan et al., 2011), our HMM classifier has the advantage that it is trained from finely annotated L2 speech error data, hence can recognise error types that are specifically defined in the annotations. Moreover, the results not only show the differences between gold standard and learner data, but also contain information of corrective feedback. Comparing to a general Goodness of Pronunciation (GOP) score or si</context>
</contexts>
<marker>Truong, Neri, Cucchiarini, Strik, 2004</marker>
<rawString>Khiet Truong, Ambra Neri, Catia Cucchiarini, and Helmer Strik. 2004. Automatic pronunciation error detection: an acoustic-phonetic approach. In InSTIL/ICALL Symposium 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Witt</author>
</authors>
<title>Automatic error detection in pronunciation training: where we are and where we need to go.</title>
<date>2012</date>
<booktitle>In International Symposium on Automatic Detection of Errors in Pronunciation Training,</booktitle>
<location>Stockholm,</location>
<contexts>
<context position="5603" citStr="Witt, 2012" startWordPosition="857" endWordPosition="858">d and learner data, but also contain information of corrective feedback. Comparing to a general Goodness of Pronunciation (GOP) score or simply showing differences in waveform, our feedback pinpoints different types of error down to phoneme level and show learners how to pronounce them correctly via various means. The same annotating-training-verifying process can be adapted to all languages as far as our open source components support them, thus, with almost no extra efforts. Since people with the same L1 background tend to make the same pronunciation errors while learning a second language (Witt, 2012), we choose a specific L1-L2 pair in our experiment, namely, Germans who learn English. 3 Annotation Tool To provide annotators an easy and efficient interface, we have further developed MAT (Ai and Figure 2: Screenshot of the Annotation Tool Charfuelan, 2014), with which pronunciation errors can be annotated via simple mouse clicks and single phoneme inputs from keyboard. During the annotation, errors are automatically stored in an error database, which then updates the speech verification tool. We also add checking mechanisms to validate the annotations and ask annotators to deal with confli</context>
</contexts>
<marker>Witt, 2012</marker>
<rawString>S. M. Witt. 2012. Automatic error detection in pronunciation training: where we are and where we need to go. In International Symposium on Automatic Detection of Errors in Pronunciation Training, Stockholm, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>