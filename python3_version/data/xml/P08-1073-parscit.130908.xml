<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000138">
<title confidence="0.931068">
Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz
data: Bootstrapping and Evaluation
</title>
<author confidence="0.993263">
Verena Rieser
</author>
<affiliation confidence="0.997957">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.966135">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.998906">
vrieser@inf.ed.ac.uk
</email>
<author confidence="0.989841">
Oliver Lemon
</author>
<affiliation confidence="0.99784">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.966255">
Edinburgh, EH8 9LW, GB
</address>
<email confidence="0.999202">
olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968625">
We address two problems in the field of au-
tomatic optimization of dialogue strategies:
learning effective dialogue strategies when no
initial data or system exists, and evaluating the
result with real users. We use Reinforcement
Learning (RL) to learn multimodal dialogue
strategies by interaction with a simulated envi-
ronment which is “bootstrapped” from small
amounts of Wizard-of-Oz (WOZ) data. This
use of WOZ data allows development of op-
timal strategies for domains where no work-
ing prototype is available. We compare the
RL-based strategy against a supervised strat-
egy which mimics the wizards’ policies. This
comparison allows us to measure relative im-
provement over the training data. Our results
show that RL significantly outperforms Super-
vised Learning when interacting in simulation
as well as for interactions with real users. The
RL-based policy gains on average 50-times
more reward when tested in simulation, and
almost 18-times more reward when interacting
with real users. Users also subjectively rate
the RL-based policy on average 10% higher.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986169047619">
Designing a spoken dialogue system is a time-
consuming and challenging task. A developer may
spend a lot of time and effort anticipating the po-
tential needs of a specific application environment
and then deciding on the most appropriate system
action (e.g. confirm, present items,...). One of the
key advantages of statistical optimisation methods,
such as Reinforcement Learning (RL), for dialogue
strategy design is that the problem can be formu-
lated as a principled mathematical model which can
be automatically trained on real data (Lemon and
Pietquin, 2007; Frampton and Lemon, to appear). In
cases where a system is designed from scratch, how-
ever, there is often no suitable in-domain data. Col-
lecting dialogue data without a working prototype
is problematic, leaving the developer with a classic
chicken-and-egg problem.
We propose to learn dialogue strategies by
simulation-based RL (Sutton and Barto, 1998),
where the simulated environment is learned from
small amounts of Wizard-of-Oz (WOZ) data. Us-
ing WOZ data rather than data from real Human-
Computer Interaction (HCI) allows us to learn op-
timal strategies for domains where no working di-
alogue system already exists. To date, automatic
strategy learning has been applied to dialogue sys-
tems which have already been deployed using hand-
crafted strategies. In such work, strategy learning
was performed based on already present extensive
online operation experience, e.g. (Singh et al., 2002;
Henderson et al., 2005). In contrast to this preced-
ing work, our approach enables strategy learning in
domains where no prior system is available. Opti-
mised learned strategies are then available from the
first moment of online-operation, and tedious hand-
crafting of dialogue strategies is omitted. This inde-
pendence from large amounts of in-domain dialogue
data allows researchers to apply RL to new appli-
cation areas beyond the scope of existing dialogue
systems. We call this method ‘bootstrapping’.
In a WOZ experiment, a hidden human operator,
the so called “wizard”, simulates (partly or com-
</bodyText>
<page confidence="0.971045">
638
</page>
<note confidence="0.697544">
Proceedings ofACL-08: HLT, pages 638–646,
</note>
<page confidence="0.495437">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999894115384615">
pletely) the behaviour of the application, while sub-
jects are left in the belief that they are interacting
with a real system (Fraser and Gilbert, 1991). That
is, WOZ experiments only simulate HCI. We there-
fore need to show that a strategy bootstrapped from
WOZ data indeed transfers to real HCI. Further-
more, we also need to introduce methods to learn
useful user simulations (for training RL) from such
limited data.
The use of WOZ data has earlier been proposed
in the context of RL. (Williams and Young, 2004)
utilise WOZ data to discover the state and action
space for MDP design. (Prommer et al., 2006)
use WOZ data to build a simulated user and noise
model for simulation-based RL. While both stud-
ies show promising first results, their simulated en-
vironment still contains many hand-crafted aspects,
which makes it hard to evaluate whether the suc-
cess of the learned strategy indeed originates from
the WOZ data. (Schatzmann et al., 2007) propose to
‘bootstrap’ with a simulated user which is entirely
hand-crafted. In the following we propose an en-
tirely data-driven approach, where all components
of the simulated learning environment are learned
from WOZ data. We also show that the resulting
policy performs well for real users.
</bodyText>
<sectionHeader confidence="0.594835" genericHeader="method">
2 Wizard-of-Oz data collection
</sectionHeader>
<bodyText confidence="0.999977181818182">
Our domains of interest are information-seeking di-
alogues, for example a multimodal in-car interface
to a large database of music (MP3) files. The corpus
we use for learning was collected in a multimodal
study of German task-oriented dialogues for an in-
car music player application by (Rieser et al., 2005).
This study provides insights into natural methods
of information presentation as performed by human
wizards. 6 people played the role of an intelligent
interface (the “wizards”). The wizards were able
to speak freely and display search results on the
screen by clicking on pre-computed templates. Wiz-
ards’ outputs were not restricted, in order to explore
the different ways they intuitively chose to present
search results. Wizard’s utterances were immedi-
ately transcribed and played back to the user with
Text-To-Speech. 21 subjects (11 female, 10 male)
were given a set of predefined tasks to perform, as
well as a primary driving task, using a driving simu-
lator. The users were able to speak, as well as make
selections on the screen. We also introduced artifi-
cial noise in the setup, in order to closer resemble
the conditions of real HCI. Please see (Rieser et al.,
2005) for further detail.
The corpus gathered with this setup comprises 21
sessions and over 1600 turns. Example 1 shows a
typical multimodal presentation sub-dialogue from
the corpus (translated from German). Note that the
wizard displays quite a long list of possible candi-
dates on an (average sized) computer screen, while
the user is driving. This example illustrates that even
for humans it is difficult to find an “optimal” solu-
tion to the problem we are trying to solve.
</bodyText>
<listItem confidence="0.812824">
(1) User: Please search for music by Madonna.
</listItem>
<tableCaption confidence="0.52753825">
Wizard: I found seventeen hundred and eleven
items. The items are displayed on the screen.
[displays list]
User: Please select ‘Secret’.
</tableCaption>
<bodyText confidence="0.999855178571429">
For each session information was logged, e.g. the
transcriptions of the spoken utterances, the wizard’s
database query and the number of results, the screen
option chosen by the wizard, and a rich set of con-
textual dialogue features was also annotated, see
(Rieser et al., 2005).
Of the 793 wizard turns 22.3% were annotated
as presentation strategies, resulting in 177 instances
for learning, where the six wizards contributed about
equal proportions.
Information about user preferences was obtained,
using a questionnaire containing similar questions to
the PARADISE study (Walker et al., 2000). In gen-
eral, users report that they get distracted from driv-
ing if too much information is presented. On the
other hand, users prefer shorter dialogues (most of
the user ratings are negatively correlated with dia-
logue length). These results indicate that we need
to find a strategy given the competing trade-offs be-
tween the number of results (large lists are difficult
for users to process), the length of the dialogue (long
dialogues are tiring, but collecting more information
can result in more precise results), and the noise in
the speech recognition environment (in high noise
conditions accurate information is difficult to ob-
tain). In the following we utilise the ratings from the
user questionnaires to optimise a presentation strat-
egy using simulation-based RL.
</bodyText>
<page confidence="0.954126">
639
</page>
<equation confidence="0.980104117647059">
⎡ ⎡n o
⎡ ⎤
askASlot filledSlot 1  |2  |3  |4  |: 0,1
⎢ ⎢
⎢ ⎢ ⎥ ⎢ n o
⎢ implConfAskASlot
⎢ acquisition action: ⎢ ⎥ state: ⎢ ⎢ confirmedSlot 1  |2  |3  |4  |: 0,1
⎦
⎣
⎢ explConf ⎣ n o
⎢ ⎢ presentInfo DB: 1--438
⎢ ⎢ ⎡DB low:
low: {0,presentInfoVerbal⎢1}
rl lJ
presentation action: state: DB med: { 0, 1 }
presentInfoMM lr I
DB high C 0,1o
</equation>
<figureCaption confidence="0.996598">
Figure 1: State-Action space for hierarchical Reinforcement Learning
</figureCaption>
<sectionHeader confidence="0.994463" genericHeader="method">
3 Simulated Learning Environment
</sectionHeader>
<bodyText confidence="0.999711166666667">
Simulation-based RL (also know as “model-free”
RL) learns by interaction with a simulated environ-
ment. We obtain the simulated components from the
WOZ corpus using data-driven methods. The em-
ployed database contains 438 items and is similar in
retrieval ambiguity and structure to the one used in
the WOZ experiment. The dialogue system used for
learning comprises some obvious constraints reflect-
ing the system logic (e.g. that only filled slots can be
confirmed), implemented as Information State Up-
date (ISU) rules. All other actions are left for opti-
misation.
</bodyText>
<subsectionHeader confidence="0.997265">
3.1 MDP and problem representation
</subsectionHeader>
<bodyText confidence="0.999980146341464">
The structure of an information seeking dialogue
system consists of an information acquisition phase,
and an information presentation phase. For informa-
tion acquisition the task of the dialogue manager is
to gather ‘enough’ search constraints from the user,
and then, ‘at the right time’, to start the information
presentation phase, where the presentation task is to
present ‘the right amount’ of information in the right
way– either on the screen or listing the items ver-
bally. What ‘the right amount’ actually means de-
pends on the application, the dialogue context, and
the preferences of users. For optimising dialogue
strategies information acquisition and presentation
are two closely interrelated problems and need to
be optimised simultaneously: when to present in-
formation depends on the available options for how
to present them, and vice versa. We therefore for-
mulate the problem as a Markov Decision Process
(MDP), relating states to actions in a hierarchical
manner (see Figure 1): 4 actions are available for
the information acquisition phase; once the action
presentInfo is chosen, the information presen-
tation phase is entered, where 2 different actions
for output realisation are available. The state-space
comprises 8 binary features representing the task for
a4slotproblem: filledSlot indicates whether a
slots is filled, confirmedSlot indicates whether
a slot is confirmed. We also add features that hu-
man wizards pay attention to, using the feature se-
lection techniques of (Rieser and Lemon, 2006b).
Our results indicate that wizards only pay attention
to the number of retrieved items (DB). We there-
fore add the feature DB to the state space, which
takes integer values between 1 and 438, resulting in
28 x 438 = 112,128 distinct dialogue states. In to-
tal there are 4112,128 theoretically possible policies
for information acquisition. 1 For the presentation
phase the DB feature is discretised, as we will further
discuss in Section 3.6. For the information presenta-
tion phase there are 223 = 256 theoretically possible
policies.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2 Supervised Baseline
</subsectionHeader>
<bodyText confidence="0.99995425">
We create a baseline by applying Supervised Learn-
ing (SL). This baseline mimics the average wizard
behaviour and allows us to measure the relative im-
provements over the training data (cf. (Henderson et
al., 2005)). For these experiments we use the WEKA
toolkit (Witten and Frank, 2005). We learn with the
decision tree J4.8 classifier, WEKA’s implementation
of the C4.5 system (Quinlan, 1993), and rule induc-
</bodyText>
<footnote confidence="0.79825775">
1In practise, the policy space is smaller, as some of combi-
nations are not possible, e.g. a slot cannot be confirmed before
being filled. Furthermore, some incoherent action choices are
excluded by the basic system logic.
</footnote>
<page confidence="0.986994">
640
</page>
<table confidence="0.978199333333333">
baseline JRip J48
timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7)
modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)*
</table>
<tableCaption confidence="0.952111666666667">
Table 1: Predicted accuracy for presentation timing and
modality (with standard deviation ±), * denotes statisti-
cally significant improvement at p &lt; .05
</tableCaption>
<bodyText confidence="0.998935333333333">
tion JRIP, the WEKA implementation of RIPPER (Co-
hen, 1995). In particular, we learn models which
predict the following wizard actions:
</bodyText>
<listItem confidence="0.9971">
• Presentation timing: when the ‘average’ wizard
starts the presentation phase
• Presentation modality: in which modality the
list is presented.
</listItem>
<bodyText confidence="0.999292285714286">
As input features we use annotated dialogue con-
text features, see (Rieser and Lemon, 2006b). Both
models are trained using 10-fold cross validation.
Table 1 presents the results for comparing the ac-
curacy of the learned classifiers against the major-
ity baseline. For presentation timing, none of the
classifiers produces significantly improved results.
Hence, we conclude that there is no distinctive pat-
tern the wizards follow for when to present informa-
tion. For strategy implementation we therefore use a
frequency-based approach following the distribution
in the WOZ data: in 0.48 of cases the baseline policy
decides to present the retrieved items; for the rest of
the time the system follows a hand-coded strategy.
For learning presentation modality, both classifiers
significantly outperform the baseline. The learned
models can be rewritten as in Algorithm 1. Note that
this rather simple algorithm is meant to represent the
average strategy as present in the initial data (which
then allows us to measure the relative improvements
of the RL-based strategy).
</bodyText>
<listItem confidence="0.864417666666667">
Algorithm 15upervised5trategy
1: if DB &lt; 3 then
2: return presentInfoVerbal
3: else
4: return presentInfoMM
5: end if
</listItem>
<subsectionHeader confidence="0.981804">
3.3 Noise simulation
</subsectionHeader>
<bodyText confidence="0.999918592592593">
One of the fundamental characteristics of HCI is an
error prone communication channel. Therefore, the
simulation of channel noise is an important aspect of
the learning environment. Previous work uses data-
intensive simulations of ASR errors, e.g. (Pietquin
and Dutoit, 2006). We use a simple model simulat-
ing the effects of non- and misunderstanding on the
interaction, rather than the noise itself. This method
is especially suited to learning from small data sets.
From our data we estimate a 30% chance of user
utterances to be misunderstood, and 4% to be com-
plete non-understandings. We simulate the effects
noise has on the user behaviour, as well as for the
task accuracy. For the user side, the noise model de-
fines the likelihood of the user accepting or rejecting
the system’s hypothesis (for example when the sys-
tem utters a confirmation), i.e. in 30% of the cases
the user rejects, in 70% the user agrees. These prob-
abilities are combined with the probabilities for user
actions from the user simulation, as described in the
next section. For non-understandings we have the
user simulation generating Out-of-Vocabulary utter-
ances with a chance of 4%. Furthermore, the noise
model determines the likelihood of task accuracy as
calculated in the reward function for learning. A
filled slot which is not confirmed by the user has a
30% chance of having been mis-recognised.
</bodyText>
<subsectionHeader confidence="0.961694">
3.4 User simulation
</subsectionHeader>
<bodyText confidence="0.999570529411765">
A user simulation is a predictive model of real user
behaviour used for automatic dialogue strategy de-
velopment and testing. For our domain, the user
can either add information (add), repeat or para-
phrase information which was already provided at
an earlier stage (repeat), give a simple yes-no an-
swer (y/n), or change to a different topic by pro-
viding a different slot value than the one asked for
(change). These actions are annotated manually
(n = .7). We build two different types of user
simulations, one is used for strategy training, and
one for testing. Both are simple bi-gram models
which predict the next user action based on the pre-
vious system action (P(a,,se�|asystem)). We face
the problem of learning such models when train-
ing data is sparse. For training, we therefore use
a cluster-based user simulation method, see (Rieser
</bodyText>
<page confidence="0.994629">
641
</page>
<bodyText confidence="0.999869461538462">
and Lemon, 2006a). For testing, we apply smooth-
ing to the bi-gram model. The simulations are evalu-
ated using the SUPER metric proposed earlier (Rieser
and Lemon, 2006a), which measures variance and
consistency of the simulated behaviour with respect
to the observed behaviour in the original data set.
This technique is used because for training we need
more variance to facilitate the exploration of large
state-action spaces, whereas for testing we need sim-
ulations which are more realistic. Both user simula-
tions significantly outperform random and majority
class baselines. See (Rieser, 2008) for further de-
tails.
</bodyText>
<subsectionHeader confidence="0.912773">
3.5 Reward modelling
</subsectionHeader>
<bodyText confidence="0.998312181818182">
The reward function defines the goal of the over-
all dialogue. For example, if it is most important
for the dialogue to be efficient, the reward penalises
dialogue length, while rewarding task success. In
most previous work the reward function is manu-
ally set, which makes it “the most hand-crafted as-
pect” of RL (Paek, 2006). In contrast, we learn the
reward model from data, using a modified version
of the PARADISE framework (Walker et al., 2000),
following pioneering work by (Walker et al., 1998).
In PARADISE multiple linear regression is used to
build a predictive model of subjective user ratings
(from questionnaires) from objective dialogue per-
formance measures (such as dialogue length). We
use PARADISE to predict Task Ease (a variable ob-
tained by taking the average of two questions in the
questionnaire) 2 from various input variables, via
stepwise regression. The chosen model comprises
dialogue length in turns, task completion (as manu-
ally annotated in the WOZ data), and the multimodal
user score from the user questionnaire, as shown in
Equation 2.
</bodyText>
<equation confidence="0.9289605">
TaskEase = − 20.2 * dialogueLength +
11.8 * taskCompletion + 8.7 * multimodalScore; (2)
</equation>
<bodyText confidence="0.9910756">
This equation is used to calculate the overall re-
ward for the information acquisition phase. Dur-
ing learning, Task Completion is calculated online
according to the noise model, penalising all slots
which are filled but not confirmed.
2“The task was easy to solve.”, “I had no problems finding
the information I wanted.”
For the information presentation phase, we com-
pute a local reward. We relate the multimodal score
(a variable obtained by taking the average of 4 ques-
tions) 3 to the number of items presented (DB) for
each modality, using curve fitting. In contrast to
linear regression, curve fitting does not assume a
linear inductive bias, but it selects the most likely
model (given the data points) by function interpo-
lation. The resulting models are shown in Figure
3.5. The reward for multimodal presentation is a
quadratic function that assigns a maximal score to
a strategy displaying 14.8 items (curve inflection
point). The reward for verbal presentation is a linear
function assigning negative scores to all presented
items &lt; 4. The reward functions for information
presentation intersect at no. items=3. A comprehen-
sive evaluation of this reward function can be found
in (Rieser and Lemon, 2008a).
</bodyText>
<figureCaption confidence="0.944699">
Figure 2: Evaluation functions relating number of items
presented in different modalities to multimodal score
</figureCaption>
<subsectionHeader confidence="0.995325">
3.6 State space discretisation
</subsectionHeader>
<bodyText confidence="0.962549090909091">
We use linear function approximation in order to
learn with large state-action spaces. Linear func-
tion approximation learns linear estimates for ex-
pected reward values of actions in states represented
as feature vectors. This is inconsistent with the idea
3“I liked the combination of information being displayed on
the screen and presented verbally.”, “Switching between modes
did not distract me.”, “The displayed lists and tables contained
on average the right amount of information.”, “The information
presented verbally was easy to remember.”
reward function for information presentation
</bodyText>
<figure confidence="0.999191764705882">
user score
-10
-20
-30
-40
-50
-60
-70
-80
10
0
intersection point
turning point:14.8
multimodal presentation: MM(x)
verbal presentation: Speech(x)
0 10 20 30 40 50 60 70
no. items
</figure>
<page confidence="0.992718">
642
</page>
<bodyText confidence="0.999875333333333">
of non-linear reward functions (as introduced in the
previous section). We therefore quantise the state
space for information presentation. We partition
the database feature into 3 bins, taking the first in-
tersection point between verbal and multimodal re-
ward and the turning point of the multimodal func-
tion as discretisation boundaries. Previous work
on learning with large databases commonly quan-
tises the database feature in order to learn with large
state spaces using manual heuristics, e.g. (Levin et
al., 2000; Heeman, 2007). Our quantisation tech-
nique is more principled as it reflects user prefer-
ences for multi-modal output. Furthermore, in pre-
vious work database items were not only quantised
in the state-space, but also in the reward function,
resulting in a direct mapping between quantised re-
trieved items and discrete reward values, whereas
our reward function still operates on the continuous
values. In addition, the decision when to present a
list (information acquisition phase) is still based on
continuous DB values. In future work we plan to en-
gineer new state features in order to learn with non-
linear rewards while the state space is still continu-
ous. A continuous representation of the state space
allows learning of more fine-grained local trade-offs
between the parameters, as demonstrated by (Rieser
and Lemon, 2008b).
</bodyText>
<subsectionHeader confidence="0.997598">
3.7 Testing the Learned Policies in Simulation
</subsectionHeader>
<bodyText confidence="0.999993169811321">
We now train and test the multimodal presentation
strategies by interacting with the simulated learn-
ing environment. For the following RL experiments
we used the REALL-DUDE toolkit of (Lemon et al.,
2006b). The SHARSHA algorithm is employed for
training, which adds hierarchical structure to the
well known SARSA algorithm (Shapiro and Langley,
2002). The policy is trained with the cluster-based
user simulation over 180k system cycles, which re-
sults in about 20k simulated dialogues. In total, the
learned strategy has 371 distinct state-action pairs
(see (Rieser, 2008) for details).
We test the RL-based and supervised baseline
policies by running 500 test dialogues with a
smoothed user simulation (so that we are not train-
ing and testing on the same simulation). We then
compare quantitative dialogue measures performing
a paired t-test. In particular, we compare mean val-
ues of the final rewards, number of filled and con-
firmed slots, dialog length, and items presented mul-
timodally (MM items) and items presented ver-
bally (verbal items). RL performs signifi-
cantly better (p &lt; .001) than the baseline strategy.
The only non-significant difference is the number
of items presented verbally, where both RL and SL
strategy settled on a threshold of less than 4 items.
The mean performance measures for simulation-
based testing are shown in Table 2 and Figure 3.
The major strength of the learned policy is that
it learns to keep the dialogues reasonably short (on
average 5.9 system turns for RL versus 8.4 turns
for SL) by presenting lists as soon as the number
of retrieved items is within tolerance range for the
respective modality (as reflected in the reward func-
tion). The SL strategy in contrast has not learned the
right timing nor an upper bound for displaying items
on the screen. The results show that simulation-
based RL with an environment bootstrapped from
WOZ data allows learning of robust strategies which
significantly outperform the strategies contained in
the initial data set.
One major advantage of RL is that it allows us
to provide additional information about user pref-
erences in the reward function, whereas SL simply
mimics the data. In addition, RL is based on de-
layed rewards, i.e. the optimisation of a final goal.
For dialogue systems we often have measures indi-
cating how successful and/or satisfying the overall
performance of a strategy was, but it is hard to tell
how things should have been exactly done in a spe-
cific situation. This is what makes RL specifically
attractive for dialogue strategy learning. In the next
section we test the learned strategy with real users.
</bodyText>
<sectionHeader confidence="0.997397" genericHeader="method">
4 User Tests
</sectionHeader>
<subsectionHeader confidence="0.999412">
4.1 Experimental design
</subsectionHeader>
<bodyText confidence="0.9998689">
For the user tests the RL policy is ported to a work-
ing ISU-based dialogue system via table look-up,
which indicates the action with the highest expected
reward for each state (cf. (Singh et al., 2002)). The
supervised baseline is implemented using standard
threshold-based update rules. The experimental con-
ditions are similar to the WOZ study, i.e. we ask the
users to solve similar tasks, and use similar ques-
tionnaires. Furthermore, we decided to use typed
user input rather than ASR. The use of text input
</bodyText>
<page confidence="0.997958">
643
</page>
<table confidence="0.988482666666667">
Measure SL baseline RL Strategy
SIM REAL SIM REAL
av. turns 8.42(±3.04) 5.86(±3.2) 5.9(±2.4)*** 5.07(±2.9)***
av. speech items 1.04(±.2) 1.29(±.4) 1.1(±.3) 1.2(±.4)
av. MM items 61.37(±82.5) 52.2(±68.5) 11.2(±2.4)*** 8.73(±4.4)***
av. reward -1741.3(±566.2) -628.2(±178.6) 44.06(±51.5)*** 37.62(±60.7)***
</table>
<tableCaption confidence="0.9762205">
Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strate-
gies; *** denotes significant difference between SL and RL at p &lt; .001
</tableCaption>
<figureCaption confidence="0.55365925">
Figure 3: Graph comparison of objective measures: SLs
= SL policy in simulation; SLr = SL policy with real
users; RLs = RL policy in simulation; RLr = RL policy
with real users.
</figureCaption>
<bodyText confidence="0.9972825">
allows us to target the experiments to the dialogue
management decisions, and block ASR quality from
interfering with the experimental results (Hajdinjak
and Mihelic, 2006). 17 subjects (8 female, 9 male)
are given a set of 6×2 predefined tasks, which they
solve by interaction with the RL-based and the SL-
based system in controlled order. As a secondary
task users are asked to count certain objects in a driv-
ing simulation. In total, 204 dialogues with 1,115
turns are gathered in this setup.
</bodyText>
<subsectionHeader confidence="0.581365">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.997456333333333">
In general, the users rate the RL-based significantly
higher (p &lt; .001) than the SL-based policy. The re-
sults from a paired t-test on the user questionnaire
data show significantly improved Task Ease, better
presentation timing, more agreeable verbal and mul-
timodal presentation, and that more users would use
the RL-based system in the future (Future Use). All
the observed differences have a medium effects size
(r &gt; |.3|).
We also observe that female participants clearly
favour the RL-based strategy, whereas the ratings by
male participants are more indifferent. Similar gen-
der effects are also reported by other studies on mul-
timodal output presentation, e.g. (Foster and Ober-
lander, 2006).
Furthermore, we compare objective dialogue per-
formance measures. The dialogues of the RL strat-
egy are significantly shorter (p &lt; .005), while fewer
items are displayed (p &lt; .001), and the help func-
tion is used significantly less (p &lt; .003). The mean
performance measures for testing with real users are
shown in Table 2 and Figure 3. However, there is
no significant difference for the performance of the
secondary driving task.
</bodyText>
<sectionHeader confidence="0.649116" genericHeader="method">
5 Comparison of Results
</sectionHeader>
<bodyText confidence="0.999983571428571">
We finally test whether the results obtained in sim-
ulation transfer to tests with real users, following
(Lemon et al., 2006a). We evaluate the quality of
the simulated learning environment by directly com-
paring the dialogue performance measures between
simulated and real interaction. This comparison en-
ables us to make claims regarding whether a policy
which is ‘bootstrapped’ from WOZ data is transfer-
able to real HCI. We first evaluate whether objective
dialogue measures are transferable, using a paired
t-test. For the RL policy there is no statistical dif-
ference in overall performance (reward), dialogue
length (turns), and the number of presented items
(verbal and multimodal items) between simulated
</bodyText>
<page confidence="0.995693">
644
</page>
<table confidence="0.84818">
Measure WOZ SL RL
av. Task Ease .53±.14 .63±.26 .79±.21***
av. Future Use .56±.16 .55±.21 .67±.20***
</table>
<tableCaption confidence="0.997498">
Table 3: Improved user ratings over the WOZ study
</tableCaption>
<bodyText confidence="0.998632551724138">
where *** denotes p &lt; .001
and real interaction (see Table 2, Figure 3). This in-
dicates that the learned strategy transfers well to real
settings. For the SL policy the dialogue length for
real users is significantly shorter than in simulation.
From an error analysis we conclude that real users
intelligently adapt to poor policies, e.g. by changing
topic, whereas the simulated users do not react in
this way.
Furthermore, we want to know whether the sub-
jective user ratings for the RL strategy improved
over the WOZ study. We therefore compare the user
ratings from the WOZ questionnaire to the user rat-
ings of the final user tests using a independent t-test
and a Wilcoxon Signed Ranks Test. Users rate the
RL-policy on average 10% higher. We are especially
interested in the ratings for Task Ease (as this was
the ultimate measure optimised with PARADISE) and
Future Use, as we believe this measure to be an im-
portant indicator of acceptance of the technology.
The results show that only the RL strategy leads to
significantly improved user ratings (increasing av-
erage Task Ease by 49% and Future Use by 19%),
whereas the ratings for the SL policy are not signifi-
cantly better than those for the WOZ data, see Table
3. 4 This indicates that the observed difference is in-
deed due to the improved strategy (and not to other
factors like the different user population or the em-
bedded dialogue system).
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999983333333333">
We addressed two problems in the field of automatic
optimization of dialogue strategies: learning effec-
tive dialogue strategies when no initial data or sys-
tem exists, and evaluating the result with real users.
We learned optimal strategies by interaction with a
simulated environment which is bootstrapped from
</bodyText>
<footnote confidence="0.71569">
4The ratings are normalised as some of the questions were
on different scales.
</footnote>
<bodyText confidence="0.999905321428571">
a small amount of Wizard-of-Oz data, and we evalu-
ated the result with real users. The use of WOZ data
allows us to develop optimal strategies for domains
where no working prototype is available. The de-
veloped simulations are entirely data driven and the
reward function reflects real user preferences. We
compare the Reinforcement Learning-based strategy
against a supervised strategy which mimics the (hu-
man) wizards’ policies from the original data. This
comparison allows us to measure relative improve-
ment over the training data. Our results show that
RL significantly outperforms SL in simulation as
well as in interactions with real users. The RL-based
policy gains on average 50-times more reward when
tested in simulation, and almost 18-times more re-
ward when interacting with real users. The human
users also subjectively rate the RL-based policy on
average 10% higher, and 49% higher for Task Ease.
We also show that results obtained in simulation are
comparable to results for real users. We conclude
that a strategy trained from WOZ data via boot-
strapping is transferable to real Human-Computer-
Interaction.
In future work will apply similar techniques to
statistical planning for Natural Language Generation
in spoken dialogue (Lemon, 2008; Janarthanam and
Lemon, 2008), (see the EC FP7 CLASSiC project:
www.classic-project.org).
</bodyText>
<sectionHeader confidence="0.995136" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998615">
The research leading to these results has re-
ceived funding from the European Community’s
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (CLASSiC project
www.classic-project.org), the EC FP6
project “TALK: Talk and Look, Tools for Am-
bient Linguistic Knowledge (IST 507802, www.
talk-project.org), from the EPSRC, project
no. EP/E019501/1, and from the IRTG Saarland
University.
</bodyText>
<page confidence="0.998526">
645
</page>
<sectionHeader confidence="0.925382" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99772560952381">
W. W. Cohen. 1995. Fast effective rule induction. In
Proc. of the 12th ICML-95.
M. E. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. In Proc. of EACL.
M. Frampton and O. Lemon. (to appear). Recent re-
search advances in Reinforcement Learning in Spoken
Dialogue Systems. Knowledge Engineering Review.
N. M. Fraser and G. N. Gilbert. 1991. Simulating speech
systems. Computer Speech and Language, 5:81–99.
M. Hajdinjak and F. Mihelic. 2006. The PARADISE
evaluation framework: Issues and findings. Computa-
tional Linguistics, 32(2):263–272.
P. Heeman. 2007. Combining reinforcement learn-
ing with information-state update rules. In Proc. of
NAACL.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hy-
brid Reinforcement/Supervised Learning for Dialogue
Policies from COMMUNICATOR data. In Proc. of IJ-
CAI workshop on Knowledge and Reasoning in Prac-
tical Dialogue Systems, pages 68–75.
S. Janarthanam and O. Lemon. 2008. User simula-
tions for online adaptation and knowledge-alignment
in Troubleshooting dialogue systems. In Proc. of the
12th SEMDIAL Workshop (LONdial).
O. Lemon and O. Pietquin. 2007. Machine learning for
spoken dialogue systems. In Proc. of Interspeech.
O. Lemon, K. Georgila, and J. Henderson. 2006a.
Evaluating Effectiveness and Portability of Reinforce-
ment Learned Dialogue Strategies with real users: the
TALK TownInfo Evaluation. In Proc. of IEEE/ACL
workshop on Spoken Language Technology (SLT).
O. Lemon, X. Liu, D. Shapiro, and C. Tollander. 2006b.
Hierarchical reinforcement learning of dialogue poli-
cies in a development environment for dialogue sys-
tems: REALL-DUDE. In Proc. of the 10th SEMDIAL
Workshop (BRANdial).
O. Lemon. 2008. Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
Proc. of the 12th SEMDIAL Workshop (LONdial).
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1).
T. Paek. 2006. Reinforcement learning for spoken dia-
logue systems: Comparing strengths and weaknesses
for practical deployment. In Proc. Dialog-on-Dialog
Workshop, Interspeech.
O. Pietquin and T. Dutoit. 2006. A probabilistic
framework for dialog simulation and optimal strategy
learnin. IEEE Transactions on Audio, Speech and
Language Processing, 14(2):589–599.
T. Prommer, H. Holzapfel, and A. Waibel. 2006. Rapid
simulation-driven reinforcement learning of multi-
modal dialog strategies in human-robot interaction. In
Proc. of Interspeech/ICSLP.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
V. Rieser and O. Lemon. 2006a. Cluster-based user sim-
ulations for learning dialogue strategies. In Proc. of
Interspeech/ICSLP.
V. Rieser and O. Lemon. 2006b. Using machine learning
to explore human multimodal clarification strategies.
In Proc. of ACL.
V. Rieser and O. Lemon. 2008a. Automatic learning
and evaluation of user-centered objective functions for
dialogue system optimisation. In LREC.
V. Rieser and O. Lemon. 2008b. Does this list con-
tain what you were searching for? Learning adaptive
dialogue strategies for interactive question answering.
Journal of Natural Language Engineering (special is-
sue on Interactive Question answering, to appear).
V. Rieser, I. Kruijff-Korbayov´a, and O. Lemon. 2005. A
corpus collection and annotation framework for learn-
ing multimodal clarification strategies. In Proc. of the
6th SIGdial Workshop.
V. Rieser. 2008. Bootstrapping Reinforcement Learning-
based Dialogue Strategies from Wizard-of-Oz data (to
appear). Ph.D. thesis, Saarland University.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based user simulation for
bootstrapping a POMDP dialogue system. In Proc. of
HLT/NAACL.
D. Shapiro and P. Langley. 2002. Separating skills from
preference: Using learning to program by reward. In
Proc. of the 19th ICML.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with reinforcement
learning: Experiments with the NJFun system. JAIR,
16.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press.
M. Walker, J. Fromer, and S. Narayanan. 1998. Learn-
ing optimal dialogue strategies: A case study of a
spoken dialogue agent for email. In Proceedings of
ACL/COLING.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Journal of Natural Language Engineering, 6(3).
J. Williams and S. Young. 2004. Using Wizard-of-Oz
simulations to bootstrap reinforcement-learning-based
dialog management systems. In Proc. of the 4th SIG-
dial Workshop.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques (2nd Edi-
tion). Morgan Kaufmann.
</reference>
<page confidence="0.998867">
646
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923415">
<title confidence="0.9986665">Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz data: Bootstrapping and Evaluation</title>
<author confidence="0.999969">Verena Rieser</author>
<affiliation confidence="0.9999625">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997521">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.988608">vrieser@inf.ed.ac.uk</email>
<author confidence="0.998751">Oliver Lemon</author>
<affiliation confidence="0.999968">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.997832">Edinburgh, EH8 9LW, GB</address>
<email confidence="0.996879">olemon@inf.ed.ac.uk</email>
<abstract confidence="0.99776044">We address two problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and evaluating the result with real users. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is “bootstrapped” from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows development of optimal strategies for domains where no working prototype is available. We compare the RL-based strategy against a supervised strategy which mimics the wizards’ policies. This comparison allows us to measure relative improvement over the training data. Our results show that RL significantly outperforms Supervised Learning when interacting in simulation as well as for interactions with real users. The RL-based policy gains on average 50-times more reward when tested in simulation, and almost 18-times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. of the 12th ICML-95.</booktitle>
<contexts>
<context position="12118" citStr="Cohen, 1995" startWordPosition="1920" endWordPosition="1922"> implementation of the C4.5 system (Quinlan, 1993), and rule induc1In practise, the policy space is smaller, as some of combinations are not possible, e.g. a slot cannot be confirmed before being filled. Furthermore, some incoherent action choices are excluded by the basic system logic. 640 baseline JRip J48 timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7) modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)* Table 1: Predicted accuracy for presentation timing and modality (with standard deviation ±), * denotes statistically significant improvement at p &lt; .05 tion JRIP, the WEKA implementation of RIPPER (Cohen, 1995). In particular, we learn models which predict the following wizard actions: • Presentation timing: when the ‘average’ wizard starts the presentation phase • Presentation modality: in which modality the list is presented. As input features we use annotated dialogue context features, see (Rieser and Lemon, 2006b). Both models are trained using 10-fold cross validation. Table 1 presents the results for comparing the accuracy of the learned classifiers against the majority baseline. For presentation timing, none of the classifiers produces significantly improved results. Hence, we conclude that t</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. W. Cohen. 1995. Fast effective rule induction. In Proc. of the 12th ICML-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>J Oberlander</author>
</authors>
<title>Data-driven generation of emphatic facial displays.</title>
<date>2006</date>
<booktitle>In Proc. of</booktitle>
<contexts>
<context position="26148" citStr="Foster and Oberlander, 2006" startWordPosition="4168" endWordPosition="4172">001) than the SL-based policy. The results from a paired t-test on the user questionnaire data show significantly improved Task Ease, better presentation timing, more agreeable verbal and multimodal presentation, and that more users would use the RL-based system in the future (Future Use). All the observed differences have a medium effects size (r &gt; |.3|). We also observe that female participants clearly favour the RL-based strategy, whereas the ratings by male participants are more indifferent. Similar gender effects are also reported by other studies on multimodal output presentation, e.g. (Foster and Oberlander, 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are significantly shorter (p &lt; .005), while fewer items are displayed (p &lt; .001), and the help function is used significantly less (p &lt; .003). The mean performance measures for testing with real users are shown in Table 2 and Figure 3. However, there is no significant difference for the performance of the secondary driving task. 5 Comparison of Results We finally test whether the results obtained in simulation transfer to tests with real users, following (Lemon et al., 2006a). We evaluate the qu</context>
</contexts>
<marker>Foster, Oberlander, 2006</marker>
<rawString>M. E. Foster and J. Oberlander. 2006. Data-driven generation of emphatic facial displays. In Proc. of EACL. M. Frampton and O. Lemon. (to appear). Recent research advances in Reinforcement Learning in Spoken Dialogue Systems. Knowledge Engineering Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N M Fraser</author>
<author>G N Gilbert</author>
</authors>
<title>Simulating speech systems.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--81</pages>
<contexts>
<context position="3724" citStr="Fraser and Gilbert, 1991" startWordPosition="561" endWordPosition="564">handcrafting of dialogue strategies is omitted. This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems. We call this method ‘bootstrapping’. In a WOZ experiment, a hidden human operator, the so called “wizard”, simulates (partly or com638 Proceedings ofACL-08: HLT, pages 638–646, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics pletely) the behaviour of the application, while subjects are left in the belief that they are interacting with a real system (Fraser and Gilbert, 1991). That is, WOZ experiments only simulate HCI. We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. Furthermore, we also need to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. (Williams and Young, 2004) utilise WOZ data to discover the state and action space for MDP design. (Prommer et al., 2006) use WOZ data to build a simulated user and noise model for simulation-based RL. While both studies show promising first results, their simulate</context>
</contexts>
<marker>Fraser, Gilbert, 1991</marker>
<rawString>N. M. Fraser and G. N. Gilbert. 1991. Simulating speech systems. Computer Speech and Language, 5:81–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hajdinjak</author>
<author>F Mihelic</author>
</authors>
<title>The PARADISE evaluation framework: Issues and findings.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="25118" citStr="Hajdinjak and Mihelic, 2006" startWordPosition="3999" endWordPosition="4002">4)*** 8.73(±4.4)*** av. reward -1741.3(±566.2) -628.2(±178.6) 44.06(±51.5)*** 37.62(±60.7)*** Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strategies; *** denotes significant difference between SL and RL at p &lt; .001 Figure 3: Graph comparison of objective measures: SLs = SL policy in simulation; SLr = SL policy with real users; RLs = RL policy in simulation; RLr = RL policy with real users. allows us to target the experiments to the dialogue management decisions, and block ASR quality from interfering with the experimental results (Hajdinjak and Mihelic, 2006). 17 subjects (8 female, 9 male) are given a set of 6×2 predefined tasks, which they solve by interaction with the RL-based and the SLbased system in controlled order. As a secondary task users are asked to count certain objects in a driving simulation. In total, 204 dialogues with 1,115 turns are gathered in this setup. 4.2 Results In general, the users rate the RL-based significantly higher (p &lt; .001) than the SL-based policy. The results from a paired t-test on the user questionnaire data show significantly improved Task Ease, better presentation timing, more agreeable verbal and multimodal</context>
</contexts>
<marker>Hajdinjak, Mihelic, 2006</marker>
<rawString>M. Hajdinjak and F. Mihelic. 2006. The PARADISE evaluation framework: Issues and findings. Computational Linguistics, 32(2):263–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heeman</author>
</authors>
<title>Combining reinforcement learning with information-state update rules.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="20252" citStr="Heeman, 2007" startWordPosition="3217" endWordPosition="3218">ation: MM(x) verbal presentation: Speech(x) 0 10 20 30 40 50 60 70 no. items 642 of non-linear reward functions (as introduced in the previous section). We therefore quantise the state space for information presentation. We partition the database feature into 3 bins, taking the first intersection point between verbal and multimodal reward and the turning point of the multimodal function as discretisation boundaries. Previous work on learning with large databases commonly quantises the database feature in order to learn with large state spaces using manual heuristics, e.g. (Levin et al., 2000; Heeman, 2007). Our quantisation technique is more principled as it reflects user preferences for multi-modal output. Furthermore, in previous work database items were not only quantised in the state-space, but also in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewar</context>
</contexts>
<marker>Heeman, 2007</marker>
<rawString>P. Heeman. 2007. Combining reinforcement learning with information-state update rules. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>O Lemon</author>
<author>K Georgila</author>
</authors>
<title>Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="2872" citStr="Henderson et al., 2005" startWordPosition="429" endWordPosition="432">gue strategies by simulation-based RL (Sutton and Barto, 1998), where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data. Using WOZ data rather than data from real HumanComputer Interaction (HCI) allows us to learn optimal strategies for domains where no working dialogue system already exists. To date, automatic strategy learning has been applied to dialogue systems which have already been deployed using handcrafted strategies. In such work, strategy learning was performed based on already present extensive online operation experience, e.g. (Singh et al., 2002; Henderson et al., 2005). In contrast to this preceding work, our approach enables strategy learning in domains where no prior system is available. Optimised learned strategies are then available from the first moment of online-operation, and tedious handcrafting of dialogue strategies is omitted. This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems. We call this method ‘bootstrapping’. In a WOZ experiment, a hidden human operator, the so called “wizard”, simulates (partly or com638 Proceedings ofACL-08: H</context>
<context position="11376" citStr="Henderson et al., 2005" startWordPosition="1803" endWordPosition="1806">hich takes integer values between 1 and 438, resulting in 28 x 438 = 112,128 distinct dialogue states. In total there are 4112,128 theoretically possible policies for information acquisition. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presentation phase there are 223 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Supervised Learning (SL). This baseline mimics the average wizard behaviour and allows us to measure the relative improvements over the training data (cf. (Henderson et al., 2005)). For these experiments we use the WEKA toolkit (Witten and Frank, 2005). We learn with the decision tree J4.8 classifier, WEKA’s implementation of the C4.5 system (Quinlan, 1993), and rule induc1In practise, the policy space is smaller, as some of combinations are not possible, e.g. a slot cannot be confirmed before being filled. Furthermore, some incoherent action choices are excluded by the basic system logic. 640 baseline JRip J48 timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7) modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)* Table 1: Predicted accuracy for presentation timing and modality (wit</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data. In Proc. of IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems, pages 68–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>User simulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems.</title>
<date>2008</date>
<booktitle>In Proc. of the 12th SEMDIAL Workshop (LONdial).</booktitle>
<marker>Janarthanam, Lemon, 2008</marker>
<rawString>S. Janarthanam and O. Lemon. 2008. User simulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems. In Proc. of the 12th SEMDIAL Workshop (LONdial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>O Pietquin</author>
</authors>
<title>Machine learning for spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="1957" citStr="Lemon and Pietquin, 2007" startWordPosition="287" endWordPosition="290"> rate the RL-based policy on average 10% higher. 1 Introduction Designing a spoken dialogue system is a timeconsuming and challenging task. A developer may spend a lot of time and effort anticipating the potential needs of a specific application environment and then deciding on the most appropriate system action (e.g. confirm, present items,...). One of the key advantages of statistical optimisation methods, such as Reinforcement Learning (RL), for dialogue strategy design is that the problem can be formulated as a principled mathematical model which can be automatically trained on real data (Lemon and Pietquin, 2007; Frampton and Lemon, to appear). In cases where a system is designed from scratch, however, there is often no suitable in-domain data. Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic chicken-and-egg problem. We propose to learn dialogue strategies by simulation-based RL (Sutton and Barto, 1998), where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data. Using WOZ data rather than data from real HumanComputer Interaction (HCI) allows us to learn optimal strategies for domains where no working dialogue syst</context>
</contexts>
<marker>Lemon, Pietquin, 2007</marker>
<rawString>O. Lemon and O. Pietquin. 2007. Machine learning for spoken dialogue systems. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>K Georgila</author>
<author>J Henderson</author>
</authors>
<title>Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation.</title>
<date>2006</date>
<booktitle>In Proc. of IEEE/ACL workshop on Spoken Language Technology (SLT).</booktitle>
<contexts>
<context position="21319" citStr="Lemon et al., 2006" startWordPosition="3384" endWordPosition="3387">acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more fine-grained local trade-offs between the parameters, as demonstrated by (Rieser and Lemon, 2008b). 3.7 Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pairs (see (Rieser, 2008) for details). We test the RL-based and supervised baseline policies by running 500 test dialogues with a smoothed user simulation (so that we are not training and testing on the same simulation). We then compare quantitative dialo</context>
<context position="26726" citStr="Lemon et al., 2006" startWordPosition="4266" endWordPosition="4269">tion, e.g. (Foster and Oberlander, 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are significantly shorter (p &lt; .005), while fewer items are displayed (p &lt; .001), and the help function is used significantly less (p &lt; .003). The mean performance measures for testing with real users are shown in Table 2 and Figure 3. However, there is no significant difference for the performance of the secondary driving task. 5 Comparison of Results We finally test whether the results obtained in simulation transfer to tests with real users, following (Lemon et al., 2006a). We evaluate the quality of the simulated learning environment by directly comparing the dialogue performance measures between simulated and real interaction. This comparison enables us to make claims regarding whether a policy which is ‘bootstrapped’ from WOZ data is transferable to real HCI. We first evaluate whether objective dialogue measures are transferable, using a paired t-test. For the RL policy there is no statistical difference in overall performance (reward), dialogue length (turns), and the number of presented items (verbal and multimodal items) between simulated 644 Measure WO</context>
</contexts>
<marker>Lemon, Georgila, Henderson, 2006</marker>
<rawString>O. Lemon, K. Georgila, and J. Henderson. 2006a. Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation. In Proc. of IEEE/ACL workshop on Spoken Language Technology (SLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>X Liu</author>
<author>D Shapiro</author>
<author>C Tollander</author>
</authors>
<title>Hierarchical reinforcement learning of dialogue policies in a development environment for dialogue systems: REALL-DUDE.</title>
<date>2006</date>
<booktitle>In Proc. of the 10th SEMDIAL Workshop (BRANdial).</booktitle>
<contexts>
<context position="21319" citStr="Lemon et al., 2006" startWordPosition="3384" endWordPosition="3387">acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more fine-grained local trade-offs between the parameters, as demonstrated by (Rieser and Lemon, 2008b). 3.7 Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pairs (see (Rieser, 2008) for details). We test the RL-based and supervised baseline policies by running 500 test dialogues with a smoothed user simulation (so that we are not training and testing on the same simulation). We then compare quantitative dialo</context>
<context position="26726" citStr="Lemon et al., 2006" startWordPosition="4266" endWordPosition="4269">tion, e.g. (Foster and Oberlander, 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are significantly shorter (p &lt; .005), while fewer items are displayed (p &lt; .001), and the help function is used significantly less (p &lt; .003). The mean performance measures for testing with real users are shown in Table 2 and Figure 3. However, there is no significant difference for the performance of the secondary driving task. 5 Comparison of Results We finally test whether the results obtained in simulation transfer to tests with real users, following (Lemon et al., 2006a). We evaluate the quality of the simulated learning environment by directly comparing the dialogue performance measures between simulated and real interaction. This comparison enables us to make claims regarding whether a policy which is ‘bootstrapped’ from WOZ data is transferable to real HCI. We first evaluate whether objective dialogue measures are transferable, using a paired t-test. For the RL policy there is no statistical difference in overall performance (reward), dialogue length (turns), and the number of presented items (verbal and multimodal items) between simulated 644 Measure WO</context>
</contexts>
<marker>Lemon, Liu, Shapiro, Tollander, 2006</marker>
<rawString>O. Lemon, X. Liu, D. Shapiro, and C. Tollander. 2006b. Hierarchical reinforcement learning of dialogue policies in a development environment for dialogue systems: REALL-DUDE. In Proc. of the 10th SEMDIAL Workshop (BRANdial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
</authors>
<title>Adaptive Natural Language Generation in Dialogue using Reinforcement Learning.</title>
<date>2008</date>
<booktitle>In Proc. of the 12th SEMDIAL Workshop (LONdial).</booktitle>
<contexts>
<context position="18797" citStr="Lemon, 2008" startWordPosition="2997" endWordPosition="2998">tting does not assume a linear inductive bias, but it selects the most likely model (given the data points) by function interpolation. The resulting models are shown in Figure 3.5. The reward for multimodal presentation is a quadratic function that assigns a maximal score to a strategy displaying 14.8 items (curve inflection point). The reward for verbal presentation is a linear function assigning negative scores to all presented items &lt; 4. The reward functions for information presentation intersect at no. items=3. A comprehensive evaluation of this reward function can be found in (Rieser and Lemon, 2008a). Figure 2: Evaluation functions relating number of items presented in different modalities to multimodal score 3.6 State space discretisation We use linear function approximation in order to learn with large state-action spaces. Linear function approximation learns linear estimates for expected reward values of actions in states represented as feature vectors. This is inconsistent with the idea 3“I liked the combination of information being displayed on the screen and presented verbally.”, “Switching between modes did not distract me.”, “The displayed lists and tables contained on average t</context>
<context position="21065" citStr="Lemon, 2008" startWordPosition="3347" endWordPosition="3348">in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more fine-grained local trade-offs between the parameters, as demonstrated by (Rieser and Lemon, 2008b). 3.7 Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pa</context>
</contexts>
<marker>Lemon, 2008</marker>
<rawString>O. Lemon. 2008. Adaptive Natural Language Generation in Dialogue using Reinforcement Learning. In Proc. of the 12th SEMDIAL Workshop (LONdial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="20237" citStr="Levin et al., 2000" startWordPosition="3213" endWordPosition="3216">8 multimodal presentation: MM(x) verbal presentation: Speech(x) 0 10 20 30 40 50 60 70 no. items 642 of non-linear reward functions (as introduced in the previous section). We therefore quantise the state space for information presentation. We partition the database feature into 3 bins, taking the first intersection point between verbal and multimodal reward and the turning point of the multimodal function as discretisation boundaries. Previous work on learning with large databases commonly quantises the database feature in order to learn with large state spaces using manual heuristics, e.g. (Levin et al., 2000; Heeman, 2007). Our quantisation technique is more principled as it reflects user preferences for multi-modal output. Furthermore, in previous work database items were not only quantised in the state-space, but also in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with </context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment.</title>
<date>2006</date>
<booktitle>In Proc. Dialog-on-Dialog Workshop,</booktitle>
<location>Interspeech.</location>
<contexts>
<context position="16756" citStr="Paek, 2006" startWordPosition="2667" endWordPosition="2668">d more variance to facilitate the exploration of large state-action spaces, whereas for testing we need simulations which are more realistic. Both user simulations significantly outperform random and majority class baselines. See (Rieser, 2008) for further details. 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be efficient, the reward penalises dialogue length, while rewarding task success. In most previous work the reward function is manually set, which makes it “the most hand-crafted aspect” of RL (Paek, 2006). In contrast, we learn the reward model from data, using a modified version of the PARADISE framework (Walker et al., 2000), following pioneering work by (Walker et al., 1998). In PARADISE multiple linear regression is used to build a predictive model of subjective user ratings (from questionnaires) from objective dialogue performance measures (such as dialogue length). We use PARADISE to predict Task Ease (a variable obtained by taking the average of two questions in the questionnaire) 2 from various input variables, via stepwise regression. The chosen model comprises dialogue length in turn</context>
</contexts>
<marker>Paek, 2006</marker>
<rawString>T. Paek. 2006. Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment. In Proc. Dialog-on-Dialog Workshop, Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pietquin</author>
<author>T Dutoit</author>
</authors>
<title>A probabilistic framework for dialog simulation and optimal strategy learnin.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="13820" citStr="Pietquin and Dutoit, 2006" startWordPosition="2179" endWordPosition="2182"> Algorithm 1. Note that this rather simple algorithm is meant to represent the average strategy as present in the initial data (which then allows us to measure the relative improvements of the RL-based strategy). Algorithm 15upervised5trategy 1: if DB &lt; 3 then 2: return presentInfoVerbal 3: else 4: return presentInfoMM 5: end if 3.3 Noise simulation One of the fundamental characteristics of HCI is an error prone communication channel. Therefore, the simulation of channel noise is an important aspect of the learning environment. Previous work uses dataintensive simulations of ASR errors, e.g. (Pietquin and Dutoit, 2006). We use a simple model simulating the effects of non- and misunderstanding on the interaction, rather than the noise itself. This method is especially suited to learning from small data sets. From our data we estimate a 30% chance of user utterances to be misunderstood, and 4% to be complete non-understandings. We simulate the effects noise has on the user behaviour, as well as for the task accuracy. For the user side, the noise model defines the likelihood of the user accepting or rejecting the system’s hypothesis (for example when the system utters a confirmation), i.e. in 30% of the cases </context>
</contexts>
<marker>Pietquin, Dutoit, 2006</marker>
<rawString>O. Pietquin and T. Dutoit. 2006. A probabilistic framework for dialog simulation and optimal strategy learnin. IEEE Transactions on Audio, Speech and Language Processing, 14(2):589–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Prommer</author>
<author>H Holzapfel</author>
<author>A Waibel</author>
</authors>
<title>Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech/ICSLP.</booktitle>
<contexts>
<context position="4180" citStr="Prommer et al., 2006" startWordPosition="641" endWordPosition="644"> Linguistics pletely) the behaviour of the application, while subjects are left in the belief that they are interacting with a real system (Fraser and Gilbert, 1991). That is, WOZ experiments only simulate HCI. We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. Furthermore, we also need to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. (Williams and Young, 2004) utilise WOZ data to discover the state and action space for MDP design. (Prommer et al., 2006) use WOZ data to build a simulated user and noise model for simulation-based RL. While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. (Schatzmann et al., 2007) propose to ‘bootstrap’ with a simulated user which is entirely hand-crafted. In the following we propose an entirely data-driven approach, where all components of the simulated learning environment are learned from WOZ data. We also show that the resulting poli</context>
</contexts>
<marker>Prommer, Holzapfel, Waibel, 2006</marker>
<rawString>T. Prommer, H. Holzapfel, and A. Waibel. 2006. Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction. In Proc. of Interspeech/ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11556" citStr="Quinlan, 1993" startWordPosition="1833" endWordPosition="1834">n. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presentation phase there are 223 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Supervised Learning (SL). This baseline mimics the average wizard behaviour and allows us to measure the relative improvements over the training data (cf. (Henderson et al., 2005)). For these experiments we use the WEKA toolkit (Witten and Frank, 2005). We learn with the decision tree J4.8 classifier, WEKA’s implementation of the C4.5 system (Quinlan, 1993), and rule induc1In practise, the policy space is smaller, as some of combinations are not possible, e.g. a slot cannot be confirmed before being filled. Furthermore, some incoherent action choices are excluded by the basic system logic. 640 baseline JRip J48 timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7) modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)* Table 1: Predicted accuracy for presentation timing and modality (with standard deviation ±), * denotes statistically significant improvement at p &lt; .05 tion JRIP, the WEKA implementation of RIPPER (Cohen, 1995). In particular, we learn models which</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Cluster-based user simulations for learning dialogue strategies.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech/ICSLP.</booktitle>
<contexts>
<context position="10604" citStr="Rieser and Lemon, 2006" startWordPosition="1677" endWordPosition="1680"> Markov Decision Process (MDP), relating states to actions in a hierarchical manner (see Figure 1): 4 actions are available for the information acquisition phase; once the action presentInfo is chosen, the information presentation phase is entered, where 2 different actions for output realisation are available. The state-space comprises 8 binary features representing the task for a4slotproblem: filledSlot indicates whether a slots is filled, confirmedSlot indicates whether a slot is confirmed. We also add features that human wizards pay attention to, using the feature selection techniques of (Rieser and Lemon, 2006b). Our results indicate that wizards only pay attention to the number of retrieved items (DB). We therefore add the feature DB to the state space, which takes integer values between 1 and 438, resulting in 28 x 438 = 112,128 distinct dialogue states. In total there are 4112,128 theoretically possible policies for information acquisition. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presentation phase there are 223 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Supervi</context>
<context position="12429" citStr="Rieser and Lemon, 2006" startWordPosition="1966" endWordPosition="1969">JRip J48 timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7) modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)* Table 1: Predicted accuracy for presentation timing and modality (with standard deviation ±), * denotes statistically significant improvement at p &lt; .05 tion JRIP, the WEKA implementation of RIPPER (Cohen, 1995). In particular, we learn models which predict the following wizard actions: • Presentation timing: when the ‘average’ wizard starts the presentation phase • Presentation modality: in which modality the list is presented. As input features we use annotated dialogue context features, see (Rieser and Lemon, 2006b). Both models are trained using 10-fold cross validation. Table 1 presents the results for comparing the accuracy of the learned classifiers against the majority baseline. For presentation timing, none of the classifiers produces significantly improved results. Hence, we conclude that there is no distinctive pattern the wizards follow for when to present information. For strategy implementation we therefore use a frequency-based approach following the distribution in the WOZ data: in 0.48 of cases the baseline policy decides to present the retrieved items; for the rest of the time the system</context>
<context position="15959" citStr="Rieser and Lemon, 2006" startWordPosition="2538" endWordPosition="2541">sked for (change). These actions are annotated manually (n = .7). We build two different types of user simulations, one is used for strategy training, and one for testing. Both are simple bi-gram models which predict the next user action based on the previous system action (P(a,,se�|asystem)). We face the problem of learning such models when training data is sparse. For training, we therefore use a cluster-based user simulation method, see (Rieser 641 and Lemon, 2006a). For testing, we apply smoothing to the bi-gram model. The simulations are evaluated using the SUPER metric proposed earlier (Rieser and Lemon, 2006a), which measures variance and consistency of the simulated behaviour with respect to the observed behaviour in the original data set. This technique is used because for training we need more variance to facilitate the exploration of large state-action spaces, whereas for testing we need simulations which are more realistic. Both user simulations significantly outperform random and majority class baselines. See (Rieser, 2008) for further details. 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be effic</context>
</contexts>
<marker>Rieser, Lemon, 2006</marker>
<rawString>V. Rieser and O. Lemon. 2006a. Cluster-based user simulations for learning dialogue strategies. In Proc. of Interspeech/ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Using machine learning to explore human multimodal clarification strategies.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10604" citStr="Rieser and Lemon, 2006" startWordPosition="1677" endWordPosition="1680"> Markov Decision Process (MDP), relating states to actions in a hierarchical manner (see Figure 1): 4 actions are available for the information acquisition phase; once the action presentInfo is chosen, the information presentation phase is entered, where 2 different actions for output realisation are available. The state-space comprises 8 binary features representing the task for a4slotproblem: filledSlot indicates whether a slots is filled, confirmedSlot indicates whether a slot is confirmed. We also add features that human wizards pay attention to, using the feature selection techniques of (Rieser and Lemon, 2006b). Our results indicate that wizards only pay attention to the number of retrieved items (DB). We therefore add the feature DB to the state space, which takes integer values between 1 and 438, resulting in 28 x 438 = 112,128 distinct dialogue states. In total there are 4112,128 theoretically possible policies for information acquisition. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presentation phase there are 223 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Supervi</context>
<context position="12429" citStr="Rieser and Lemon, 2006" startWordPosition="1966" endWordPosition="1969">JRip J48 timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7) modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)* Table 1: Predicted accuracy for presentation timing and modality (with standard deviation ±), * denotes statistically significant improvement at p &lt; .05 tion JRIP, the WEKA implementation of RIPPER (Cohen, 1995). In particular, we learn models which predict the following wizard actions: • Presentation timing: when the ‘average’ wizard starts the presentation phase • Presentation modality: in which modality the list is presented. As input features we use annotated dialogue context features, see (Rieser and Lemon, 2006b). Both models are trained using 10-fold cross validation. Table 1 presents the results for comparing the accuracy of the learned classifiers against the majority baseline. For presentation timing, none of the classifiers produces significantly improved results. Hence, we conclude that there is no distinctive pattern the wizards follow for when to present information. For strategy implementation we therefore use a frequency-based approach following the distribution in the WOZ data: in 0.48 of cases the baseline policy decides to present the retrieved items; for the rest of the time the system</context>
<context position="15959" citStr="Rieser and Lemon, 2006" startWordPosition="2538" endWordPosition="2541">sked for (change). These actions are annotated manually (n = .7). We build two different types of user simulations, one is used for strategy training, and one for testing. Both are simple bi-gram models which predict the next user action based on the previous system action (P(a,,se�|asystem)). We face the problem of learning such models when training data is sparse. For training, we therefore use a cluster-based user simulation method, see (Rieser 641 and Lemon, 2006a). For testing, we apply smoothing to the bi-gram model. The simulations are evaluated using the SUPER metric proposed earlier (Rieser and Lemon, 2006a), which measures variance and consistency of the simulated behaviour with respect to the observed behaviour in the original data set. This technique is used because for training we need more variance to facilitate the exploration of large state-action spaces, whereas for testing we need simulations which are more realistic. Both user simulations significantly outperform random and majority class baselines. See (Rieser, 2008) for further details. 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be effic</context>
</contexts>
<marker>Rieser, Lemon, 2006</marker>
<rawString>V. Rieser and O. Lemon. 2006b. Using machine learning to explore human multimodal clarification strategies. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Automatic learning and evaluation of user-centered objective functions for dialogue system optimisation.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="18797" citStr="Rieser and Lemon, 2008" startWordPosition="2995" endWordPosition="2998">n, curve fitting does not assume a linear inductive bias, but it selects the most likely model (given the data points) by function interpolation. The resulting models are shown in Figure 3.5. The reward for multimodal presentation is a quadratic function that assigns a maximal score to a strategy displaying 14.8 items (curve inflection point). The reward for verbal presentation is a linear function assigning negative scores to all presented items &lt; 4. The reward functions for information presentation intersect at no. items=3. A comprehensive evaluation of this reward function can be found in (Rieser and Lemon, 2008a). Figure 2: Evaluation functions relating number of items presented in different modalities to multimodal score 3.6 State space discretisation We use linear function approximation in order to learn with large state-action spaces. Linear function approximation learns linear estimates for expected reward values of actions in states represented as feature vectors. This is inconsistent with the idea 3“I liked the combination of information being displayed on the screen and presented verbally.”, “Switching between modes did not distract me.”, “The displayed lists and tables contained on average t</context>
<context position="21065" citStr="Rieser and Lemon, 2008" startWordPosition="3345" endWordPosition="3348">, but also in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more fine-grained local trade-offs between the parameters, as demonstrated by (Rieser and Lemon, 2008b). 3.7 Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pa</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>V. Rieser and O. Lemon. 2008a. Automatic learning and evaluation of user-centered objective functions for dialogue system optimisation. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Does this list contain what you were searching for? Learning adaptive dialogue strategies for interactive question answering.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering (special issue on Interactive Question</journal>
<note>answering, to appear).</note>
<contexts>
<context position="18797" citStr="Rieser and Lemon, 2008" startWordPosition="2995" endWordPosition="2998">n, curve fitting does not assume a linear inductive bias, but it selects the most likely model (given the data points) by function interpolation. The resulting models are shown in Figure 3.5. The reward for multimodal presentation is a quadratic function that assigns a maximal score to a strategy displaying 14.8 items (curve inflection point). The reward for verbal presentation is a linear function assigning negative scores to all presented items &lt; 4. The reward functions for information presentation intersect at no. items=3. A comprehensive evaluation of this reward function can be found in (Rieser and Lemon, 2008a). Figure 2: Evaluation functions relating number of items presented in different modalities to multimodal score 3.6 State space discretisation We use linear function approximation in order to learn with large state-action spaces. Linear function approximation learns linear estimates for expected reward values of actions in states represented as feature vectors. This is inconsistent with the idea 3“I liked the combination of information being displayed on the screen and presented verbally.”, “Switching between modes did not distract me.”, “The displayed lists and tables contained on average t</context>
<context position="21065" citStr="Rieser and Lemon, 2008" startWordPosition="3345" endWordPosition="3348">, but also in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more fine-grained local trade-offs between the parameters, as demonstrated by (Rieser and Lemon, 2008b). 3.7 Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pa</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>V. Rieser and O. Lemon. 2008b. Does this list contain what you were searching for? Learning adaptive dialogue strategies for interactive question answering. Journal of Natural Language Engineering (special issue on Interactive Question answering, to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>I Kruijff-Korbayov´a</author>
<author>O Lemon</author>
</authors>
<title>A corpus collection and annotation framework for learning multimodal clarification strategies.</title>
<date>2005</date>
<booktitle>In Proc. of the 6th SIGdial Workshop.</booktitle>
<marker>Rieser, Kruijff-Korbayov´a, Lemon, 2005</marker>
<rawString>V. Rieser, I. Kruijff-Korbayov´a, and O. Lemon. 2005. A corpus collection and annotation framework for learning multimodal clarification strategies. In Proc. of the 6th SIGdial Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
</authors>
<title>Bootstrapping Reinforcement Learningbased Dialogue Strategies from Wizard-of-Oz data (to appear).</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="16389" citStr="Rieser, 2008" startWordPosition="2604" endWordPosition="2605"> see (Rieser 641 and Lemon, 2006a). For testing, we apply smoothing to the bi-gram model. The simulations are evaluated using the SUPER metric proposed earlier (Rieser and Lemon, 2006a), which measures variance and consistency of the simulated behaviour with respect to the observed behaviour in the original data set. This technique is used because for training we need more variance to facilitate the exploration of large state-action spaces, whereas for testing we need simulations which are more realistic. Both user simulations significantly outperform random and majority class baselines. See (Rieser, 2008) for further details. 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be efficient, the reward penalises dialogue length, while rewarding task success. In most previous work the reward function is manually set, which makes it “the most hand-crafted aspect” of RL (Paek, 2006). In contrast, we learn the reward model from data, using a modified version of the PARADISE framework (Walker et al., 2000), following pioneering work by (Walker et al., 1998). In PARADISE multiple linear regression is used to build</context>
<context position="21688" citStr="Rieser, 2008" startWordPosition="3441" endWordPosition="3442">sting the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pairs (see (Rieser, 2008) for details). We test the RL-based and supervised baseline policies by running 500 test dialogues with a smoothed user simulation (so that we are not training and testing on the same simulation). We then compare quantitative dialogue measures performing a paired t-test. In particular, we compare mean values of the final rewards, number of filled and confirmed slots, dialog length, and items presented multimodally (MM items) and items presented verbally (verbal items). RL performs significantly better (p &lt; .001) than the baseline strategy. The only non-significant difference is the number of i</context>
</contexts>
<marker>Rieser, 2008</marker>
<rawString>V. Rieser. 2008. Bootstrapping Reinforcement Learningbased Dialogue Strategies from Wizard-of-Oz data (to appear). Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S Young</author>
</authors>
<title>Agenda-based user simulation for bootstrapping a POMDP dialogue system.</title>
<date>2007</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="4518" citStr="Schatzmann et al., 2007" startWordPosition="696" endWordPosition="699"> to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. (Williams and Young, 2004) utilise WOZ data to discover the state and action space for MDP design. (Prommer et al., 2006) use WOZ data to build a simulated user and noise model for simulation-based RL. While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. (Schatzmann et al., 2007) propose to ‘bootstrap’ with a simulated user which is entirely hand-crafted. In the following we propose an entirely data-driven approach, where all components of the simulated learning environment are learned from WOZ data. We also show that the resulting policy performs well for real users. 2 Wizard-of-Oz data collection Our domains of interest are information-seeking dialogues, for example a multimodal in-car interface to a large database of music (MP3) files. The corpus we use for learning was collected in a multimodal study of German task-oriented dialogues for an incar music player appl</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. Young. 2007. Agenda-based user simulation for bootstrapping a POMDP dialogue system. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shapiro</author>
<author>P Langley</author>
</authors>
<title>Separating skills from preference: Using learning to program by reward.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th ICML.</booktitle>
<contexts>
<context position="21466" citStr="Shapiro and Langley, 2002" startWordPosition="3405" endWordPosition="3408">linear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more fine-grained local trade-offs between the parameters, as demonstrated by (Rieser and Lemon, 2008b). 3.7 Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALL-DUDE toolkit of (Lemon et al., 2006b). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley, 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-action pairs (see (Rieser, 2008) for details). We test the RL-based and supervised baseline policies by running 500 test dialogues with a smoothed user simulation (so that we are not training and testing on the same simulation). We then compare quantitative dialogue measures performing a paired t-test. In particular, we compare mean values of the final rewards, number of filled and confirmed slots, dialog l</context>
</contexts>
<marker>Shapiro, Langley, 2002</marker>
<rawString>D. Shapiro and P. Langley. 2002. Separating skills from preference: Using learning to program by reward. In Proc. of the 19th ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>D Litman</author>
<author>M Kearns</author>
<author>M Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system.</title>
<date>2002</date>
<journal>JAIR,</journal>
<volume>16</volume>
<contexts>
<context position="2847" citStr="Singh et al., 2002" startWordPosition="425" endWordPosition="428">opose to learn dialogue strategies by simulation-based RL (Sutton and Barto, 1998), where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data. Using WOZ data rather than data from real HumanComputer Interaction (HCI) allows us to learn optimal strategies for domains where no working dialogue system already exists. To date, automatic strategy learning has been applied to dialogue systems which have already been deployed using handcrafted strategies. In such work, strategy learning was performed based on already present extensive online operation experience, e.g. (Singh et al., 2002; Henderson et al., 2005). In contrast to this preceding work, our approach enables strategy learning in domains where no prior system is available. Optimised learned strategies are then available from the first moment of online-operation, and tedious handcrafting of dialogue strategies is omitted. This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems. We call this method ‘bootstrapping’. In a WOZ experiment, a hidden human operator, the so called “wizard”, simulates (partly or com63</context>
<context position="23965" citStr="Singh et al., 2002" startWordPosition="3823" endWordPosition="3826">of a final goal. For dialogue systems we often have measures indicating how successful and/or satisfying the overall performance of a strategy was, but it is hard to tell how things should have been exactly done in a specific situation. This is what makes RL specifically attractive for dialogue strategy learning. In the next section we test the learned strategy with real users. 4 User Tests 4.1 Experimental design For the user tests the RL policy is ported to a working ISU-based dialogue system via table look-up, which indicates the action with the highest expected reward for each state (cf. (Singh et al., 2002)). The supervised baseline is implemented using standard threshold-based update rules. The experimental conditions are similar to the WOZ study, i.e. we ask the users to solve similar tasks, and use similar questionnaires. Furthermore, we decided to use typed user input rather than ASR. The use of text input 643 Measure SL baseline RL Strategy SIM REAL SIM REAL av. turns 8.42(±3.04) 5.86(±3.2) 5.9(±2.4)*** 5.07(±2.9)*** av. speech items 1.04(±.2) 1.29(±.4) 1.1(±.3) 1.2(±.4) av. MM items 61.37(±82.5) 52.2(±68.5) 11.2(±2.4)*** 8.73(±4.4)*** av. reward -1741.3(±566.2) -628.2(±178.6) 44.06(±51.5)*</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. JAIR, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2311" citStr="Sutton and Barto, 1998" startWordPosition="341" endWordPosition="344"> the key advantages of statistical optimisation methods, such as Reinforcement Learning (RL), for dialogue strategy design is that the problem can be formulated as a principled mathematical model which can be automatically trained on real data (Lemon and Pietquin, 2007; Frampton and Lemon, to appear). In cases where a system is designed from scratch, however, there is often no suitable in-domain data. Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic chicken-and-egg problem. We propose to learn dialogue strategies by simulation-based RL (Sutton and Barto, 1998), where the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data. Using WOZ data rather than data from real HumanComputer Interaction (HCI) allows us to learn optimal strategies for domains where no working dialogue system already exists. To date, automatic strategy learning has been applied to dialogue systems which have already been deployed using handcrafted strategies. In such work, strategy learning was performed based on already present extensive online operation experience, e.g. (Singh et al., 2002; Henderson et al., 2005). In contrast to this preceding work, o</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. Sutton and A. Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>J Fromer</author>
<author>S Narayanan</author>
</authors>
<title>Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/COLING.</booktitle>
<contexts>
<context position="16932" citStr="Walker et al., 1998" startWordPosition="2694" endWordPosition="2697">nificantly outperform random and majority class baselines. See (Rieser, 2008) for further details. 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be efficient, the reward penalises dialogue length, while rewarding task success. In most previous work the reward function is manually set, which makes it “the most hand-crafted aspect” of RL (Paek, 2006). In contrast, we learn the reward model from data, using a modified version of the PARADISE framework (Walker et al., 2000), following pioneering work by (Walker et al., 1998). In PARADISE multiple linear regression is used to build a predictive model of subjective user ratings (from questionnaires) from objective dialogue performance measures (such as dialogue length). We use PARADISE to predict Task Ease (a variable obtained by taking the average of two questions in the questionnaire) 2 from various input variables, via stepwise regression. The chosen model comprises dialogue length in turns, task completion (as manually annotated in the WOZ data), and the multimodal user score from the user questionnaire, as shown in Equation 2. TaskEase = − 20.2 * dialogueLengt</context>
</contexts>
<marker>Walker, Fromer, Narayanan, 1998</marker>
<rawString>M. Walker, J. Fromer, and S. Narayanan. 1998. Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. In Proceedings of ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>C Kamm</author>
<author>D Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="7280" citStr="Walker et al., 2000" startWordPosition="1139" endWordPosition="1142">Please select ‘Secret’. For each session information was logged, e.g. the transcriptions of the spoken utterances, the wizard’s database query and the number of results, the screen option chosen by the wizard, and a rich set of contextual dialogue features was also annotated, see (Rieser et al., 2005). Of the 793 wizard turns 22.3% were annotated as presentation strategies, resulting in 177 instances for learning, where the six wizards contributed about equal proportions. Information about user preferences was obtained, using a questionnaire containing similar questions to the PARADISE study (Walker et al., 2000). In general, users report that they get distracted from driving if too much information is presented. On the other hand, users prefer shorter dialogues (most of the user ratings are negatively correlated with dialogue length). These results indicate that we need to find a strategy given the competing trade-offs between the number of results (large lists are difficult for users to process), the length of the dialogue (long dialogues are tiring, but collecting more information can result in more precise results), and the noise in the speech recognition environment (in high noise conditions accu</context>
<context position="16880" citStr="Walker et al., 2000" startWordPosition="2686" endWordPosition="2689"> which are more realistic. Both user simulations significantly outperform random and majority class baselines. See (Rieser, 2008) for further details. 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be efficient, the reward penalises dialogue length, while rewarding task success. In most previous work the reward function is manually set, which makes it “the most hand-crafted aspect” of RL (Paek, 2006). In contrast, we learn the reward model from data, using a modified version of the PARADISE framework (Walker et al., 2000), following pioneering work by (Walker et al., 1998). In PARADISE multiple linear regression is used to build a predictive model of subjective user ratings (from questionnaires) from objective dialogue performance measures (such as dialogue length). We use PARADISE to predict Task Ease (a variable obtained by taking the average of two questions in the questionnaire) 2 from various input variables, via stepwise regression. The chosen model comprises dialogue length in turns, task completion (as manually annotated in the WOZ data), and the multimodal user score from the user questionnaire, as sh</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>M. Walker, C. Kamm, and D. Litman. 2000. Towards developing general models of usability with PARADISE. Journal of Natural Language Engineering, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Using Wizard-of-Oz simulations to bootstrap reinforcement-learning-based dialog management systems.</title>
<date>2004</date>
<booktitle>In Proc. of the 4th SIGdial Workshop.</booktitle>
<contexts>
<context position="4085" citStr="Williams and Young, 2004" startWordPosition="624" endWordPosition="627"> ofACL-08: HLT, pages 638–646, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics pletely) the behaviour of the application, while subjects are left in the belief that they are interacting with a real system (Fraser and Gilbert, 1991). That is, WOZ experiments only simulate HCI. We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. Furthermore, we also need to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. (Williams and Young, 2004) utilise WOZ data to discover the state and action space for MDP design. (Prommer et al., 2006) use WOZ data to build a simulated user and noise model for simulation-based RL. While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. (Schatzmann et al., 2007) propose to ‘bootstrap’ with a simulated user which is entirely hand-crafted. In the following we propose an entirely data-driven approach, where all components of the</context>
</contexts>
<marker>Williams, Young, 2004</marker>
<rawString>J. Williams and S. Young. 2004. Using Wizard-of-Oz simulations to bootstrap reinforcement-learning-based dialog management systems. In Proc. of the 4th SIGdial Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques (2nd Edition).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11449" citStr="Witten and Frank, 2005" startWordPosition="1815" endWordPosition="1818">128 distinct dialogue states. In total there are 4112,128 theoretically possible policies for information acquisition. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presentation phase there are 223 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Supervised Learning (SL). This baseline mimics the average wizard behaviour and allows us to measure the relative improvements over the training data (cf. (Henderson et al., 2005)). For these experiments we use the WEKA toolkit (Witten and Frank, 2005). We learn with the decision tree J4.8 classifier, WEKA’s implementation of the C4.5 system (Quinlan, 1993), and rule induc1In practise, the policy space is smaller, as some of combinations are not possible, e.g. a slot cannot be confirmed before being filled. Furthermore, some incoherent action choices are excluded by the basic system logic. 640 baseline JRip J48 timing 52.0(± 2.2) 50.2(± 9.7) 53.5(±11.7) modality 51.0(± 7.0) 93.5(±11.5)* 94.6(± 10.0)* Table 1: Predicted accuracy for presentation timing and modality (with standard deviation ±), * denotes statistically significant improvement </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques (2nd Edition). Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>