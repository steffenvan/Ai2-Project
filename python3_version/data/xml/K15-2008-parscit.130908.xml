<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029501">
<title confidence="0.9807">
The CLaC Discourse Parser at CoNLL-2015
</title>
<author confidence="0.991475">
Majid Laali Elnaz Davoodi Leila Kosseim
</author>
<affiliation confidence="0.9943775">
Department of Computer Science and Software Engineering,
Concordia University, Montreal, Quebec, Canada
</affiliation>
<email confidence="0.955812">
{m laali, e davoo, kosseim}@encs.concordia.ca
</email>
<sectionHeader confidence="0.992787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997569">
This paper describes our submission (kos-
seim15) to the CoNLL-2015 shared task
on shallow discourse parsing. We used the
UIMA framework to develop our parser
and used ClearTK to add machine learn-
ing functionality to the UIMA framework.
Overall, our parser achieves a result of
17.3 F1 on the identification of discourse
relations on the blind CoNLL-2015 test
set, ranking in sixth place.
</bodyText>
<sectionHeader confidence="0.998726" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988530333333333">
Today, discourse parsers typically consist of sev-
eral independent components that address the fol-
lowing problems:
</bodyText>
<listItem confidence="0.997613333333334">
1. Discourse Connective Classification: The
concern of this problem is the identification
of discourse usage of discourse connectives
within a text.
2. Argument Labeling: This problem focuses on
labeling the text spans of the two discourse
arguments, namely ARG1 and ARG2.
3. Explicit Sense Classification: This problem
can be reduced to the sense disambiguation
of the discourse connective in an explicit dis-
course relation.
4. Non-Explicit Sense Classification: The target
of this problem is the identification of im-
plicit discourse relations between two con-
secutive sentences.
</listItem>
<bodyText confidence="0.556856">
To illustrate these tasks, consider Example (1):
</bodyText>
<footnote confidence="0.909197666666667">
(1) We would stop index arbitrage when the
market is under stress.1
1The example is taken from the CoNLL 2015 trial dataset.
</footnote>
<bodyText confidence="0.970407684210526">
The task of Discourse Connective Classification
is to determine if the marker “when” is used to
mark a discourse relation or not. Argument La-
beling should segment the two arguments ARG1
and ARG2 (in this example, ARG1 is italicized
while ARG2 is bolded). Finally, Explicit Sense
Classification should identify which discourse re-
lation is signaled by “when” - in this case CON-
TINGENCY.CONDITION.
In this paper, we report on the development
and results of our discourse parser for the CoNLL
2015 shared task. Our parser, named CLaC Dis-
course Parser, was built from scratch and took
about 3 person-month to code. The focus of the
CLaC Discourse Parser is the treatment of explicit
discourse relations (i.e. problem 1 to 3 above).
2 Architecture of the CLaC Discourse
Parser
We developed our parser based on the UIMA
framework (Ferrucci and Lally, 2004) and we used
ClearTK (Bethard et al., 2014) to add machine
learning functionality to the UIMA framework.
The parser was written in Java and its source code
is distributed under the BSD license2.
Figure 1 shows the architecture of the CLaC
Discourse Parser. Motivated by Lin et al. (2014),
the architecture of the CLaC Discourse Parser is a
pipeline that consists in five components: CoNLL
Syntax Reader, Discourse Connective Annotator,
Argument Labeler, Discourse Sense Annotator
and CoNLL JSON Exporter. Due to lack of time,
we did not implement a Non-Explicit Classifica-
tion in our pipeline and only focused on explicit
discourse relations.
The CoNLL Syntax Reader and the CoNLL
JSON Exporter were added to the CLaC Dis-
course Parser in order for the input and the output
of the parser to be compatible with the CoNLL
</bodyText>
<footnote confidence="0.997197">
2All the source codes can be downloaded from
https://github.com/mjlaali/CLaCDiscourseParser.git
</footnote>
<page confidence="0.96224">
56
</page>
<note confidence="0.9358825">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 56–60,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999921">
Figure 1: Components of the CLaC Discourse Parser
</figureCaption>
<bodyText confidence="0.99923445">
2015 Shared Task specifications. The CoNLL
Syntax Reader parses syntactic information (i.e.
POS tags, constituent parse trees and dependency
parses). CoNLL organisers and adds this syntac-
tic information to the documents in the UIMA
framework. To create a stand-alone parser, the
CoNLL Syntax Reader can be easily replaced with
the cleartk-berkeleyparser component
in the CLaC discourse Parser pipeline. This com-
ponent is a wrapper around the Berkeley syntac-
tic parser (Petrov and Klein, 2007) and distributed
with ClearTK. The Berkeley syntactic parser was
actually used in the CoNLL shared task to parse
texts and generate the syntactic information.
The CoNLL JSON Exporter reads the output
discourse relations annotated in the UIMA doc-
uments and generates a JSON file in the format
required for the CoNLL shared task. We will dis-
cuss the other components in details in the next
sections.
</bodyText>
<subsectionHeader confidence="0.980711">
2.1 Discourse Connective Annotator
</subsectionHeader>
<bodyText confidence="0.999990266666667">
To annotate discourse connectives, the Discourse
Connective Annotator first searches the input texts
for terms that match a pre-defined list of discourse
connectives. This list of discourse connectives was
built solely from the CoNLL training dataset of
around 30K explicit discourse relations and con-
tains 100 discourse connectives. Each match of
discourse connective is then checked to see if it
occurs in discourse usage or not.
Inspired by (Pitler et al., 2009), we built a bi-
nary classifier with six local syntactic and lexical-
ized features of discourse connectives to classify
discourse connectives as discourse usage or non-
discourse usage. These features are listed in Ta-
ble 1 in the row labeled Connective Features.
</bodyText>
<subsectionHeader confidence="0.99989">
2.2 Argument Labeler
</subsectionHeader>
<bodyText confidence="0.999962326086956">
When ARG1 and ARG2 appear in the same sen-
tence, we can exploit the syntactic tree to label
boundaries of the discourse arguments. Motivated
by (Lin et al., 2014), we first classify each con-
stituent in the parse tree into to three categories:
part of ARG1, part of ARG2 or NON (i.e. is not
part of any discourse argument). Then, all con-
stituents which were tagged as part of ARG1 or
as part of ARG2 are merged to obtain the actual
boundaries of ARG1 and ARG2.
Previous studies have shown that learning an ar-
gument labeler classifier when all syntactic con-
stituents are considered suffers from many in-
stances being labeled as NON (Kong et al., 2014).
In order to avoid this, we used the approach pro-
posed by Kong et al. (2014) to prune constituents
with a NON label. This approach uses only the
nodes in the path from the discourse connective
(or SelfCat see Table 1) to the root of the sentence
(Connective-Root path nodes) to limit the number
of the candidate constituents. More formally, only
constituents that are directly connected to one of
the Connective-Root path nodes are considered for
the classification.
For example, consider the parse tree of Exam-
ple (1) shown in Figure 2. The path from the dis-
course connective “when” to the root of the sen-
tence contains these nodes: {WRB, WHADVP,
SBAR, VP2, VP1, S1}. Therefore, we only con-
sider {S2, NP2, VB, MD, NP1} for obtaining dis-
course arguments.
If the classifier does not classify any constituent
as a part of ARG1, we assume that the ARG1 is
not in the same sentence as ARG2. In such a sce-
nario, we consider the whole text of the previous
sentence as ARG1.
In the current implementation, we made the
assumption that discourse connectives cannot be
multiword expressions. Therefore, the Argument
Labeler cannot identify the arguments of paral-
lel discourse connectives (e.g. either..or, on one
hand..on the other hand, etc.)
We used a sub-set of 9 features proposed by
Kong et al. (2014) for the Argument Labeler clas-
sifier. The complete list of features is listed in Ta-
ble 1.
</bodyText>
<subsectionHeader confidence="0.994792">
2.3 Discourse Sense Annotator
</subsectionHeader>
<bodyText confidence="0.990375666666667">
Although some discourse connectives can signal
different discourse relations, the naive approach
that labels each discourse connective with its most
</bodyText>
<page confidence="0.983843">
57
</page>
<figure confidence="0.995800034482759">
S1
NP1 VP1
PRP
We
MD
VP2
would
VB NP2 SBAR
NN2
arbitrage
WHADVP
WRB
when
stop
NN1
index
S2
NP3
DT NN3
the market
VBZ
is
VP3
under
IN
PP
NP4
NN4
stress
</figure>
<figureCaption confidence="0.99914775">
Figure 2: The Parse Tree Provided by CoNLL 2015 for Example (1)
Category Description Example
Connective 1. The discourse connective text in lowercase. when
Features
2. The categorization of the case of the connective: all lowercase
all lowercase, all uppercase and initial uppercase
3. The highest node in the parse tree that covers the WRB
connective words but nothing more
4. The parent of SelfCat WHADVP
5. The left sibling of SelfCat null
6. The right sibling of SelfCat S
Syntactic 7. The path from the node to the SelfCat node in the S ↑ SBAR ↓
Node parser tree WHADVP
Features
8. The context of the node in the parse tree. The S-SBAR-
context of a node is defined by its label the label of WHADVP-null
its parent, the label of left and right sibling in the
parse tree.
9. The position of the node relative to the SelfCat left
node: left or right
</figureCaption>
<tableCaption confidence="0.984363">
Table 1: Features Used in the CLaC Discourse Parser
</tableCaption>
<bodyText confidence="0.999657875">
frequent relation performs rather well. Accord-
ing to Pitler et al. (2009), such an approach can
achieve an accuracy of 85.86%. Due to lack of
time, we implemented this naive approach for the
Discourse Sense Annotator, using the 100 con-
nectives mined from the dataset (see Section 2.1)
and their most frequent relation as mined from the
CoNLL training dataset.
</bodyText>
<sectionHeader confidence="0.995805" genericHeader="introduction">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999927428571429">
As explained in Section 2, the CLaC Discourse
Parser contains two main classifiers, one for the
Discourse Connective Annotator and one for the
Argument Labeler. We used the off-the-shelf im-
plementation of the C4.5 decision tree classifier
(Quinlan, 1993) available in WEKA (Hall et al.,
2009) for the two classifiers and trained them us-
</bodyText>
<page confidence="0.997798">
58
</page>
<table confidence="0.999897857142857">
Discourse Classifier Discourse Parsing Discourse Parsing
Connective Argument (explicit only) (explicit and
Labeler implicit)
Best Result 91.86% 41.35% 30.58% 24.00%
CLaC Parser 90.19% 36.60% 27.32% 17.38%
Average 74.20% 23.89% 18.28% 13.25%
Standard deviation 23.24% 13.01% 9.93% 6.41%
</table>
<tableCaption confidence="0.999554">
Table 2: Summary of the Results of the CLaC Discourse Parser in the CoNLL 2015 Shared Task.
</tableCaption>
<bodyText confidence="0.997871242424242">
ing the CoNLL training dataset.
Although the CLaC discourse parser only con-
siders explicit discourse relations (which only ac-
counts for about half of the relations), the parser
ranked 6th among the 17 submitted discourse
parsers. The overall F1 score of the parser and
the individual performance of the Discourse Con-
nective Classifier and the Argument Labeler in the
blind CoNLL test data are shown in Table 2. As
Table 2 shows, the performance of the parser is
consistently above the average. In addition, the
performance of the Discourse Connective Classi-
fier is very close to the best result.
Note that all numbers presented in Table 2
were obtained when errors propagate through the
pipeline. That is to say, if a discourse connective is
not correctly identified by the Discourse Connec-
tive Classifier for example, the arguments of this
discourse connective will not be identified. Thus,
the recall of the Argument Labeler will be affected.
The CoNLL 2015 results of the submitted
parsers show that the identification of ARG1 is
more difficult than ARG2. In line with this,
the CLaC Discourse Parser performed better on
the identification of ARG2 (with the F1 score
of 69.18%) than ARG1 (with the F1 score of
45.18%). Table 3 provides a summary of the re-
sults for the identification of Arg1 and Arg2. An
important source of errors in the identification of
ARG1 is that attribute spans are contained within
ARG1. For example in (2), the CLaC Discourse
Parser incorrectly includes the text “But the RTC
also requires “working” capital” within ARG1.
</bodyText>
<table confidence="0.9982618">
Arg1 Arg2
Best Result 49.68% 74.29%
CLaC Parser 45.18% 69.18%
Average 30.77% 50.91%
Std. deviation 15.31% 20.58%
</table>
<tableCaption confidence="0.6737465">
Table 3: Results of the Identification of ARG1 and
ARG2.
</tableCaption>
<bodyText confidence="0.960422681818182">
(2) But the RTC also requires “working” capi-
tal to maintain the bad assets of thrifts that
are sold until the assets can be sold sepa-
rately.3
With regards to the identification of ARG2, we ob-
served that subordinate and coordinate clauses are
an important source of errors. For example in (3),
the subordinate clause “before we can move for-
ward” is erroneously included in the ARG2 span
when the CLaC Discourse Parser parses the text.
The cause of such errors are usually rooted in
an incorrect syntax parse tree that was fed to the
parser. For instance in (3), the text “we have col-
lected on those assets before we can move for-
ward” was incorrectly parsed as a single clause
covered by an S node with the subordinate “before
we can move forward” as a child of this S node.
However, in the correct parse tree the subordinate
clause should be a sibling of the S node.
(3) We would have to wait until we have col-
lected on those assets before we can move
forward.3
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9998546">
In this paper, we described the CLaC Discourse
Parser which was developed from scratch for the
CoNLL 2015 shared task. This 3 person-month
effort focused on the task of the Discourse Con-
nective Classification and Argument Labeler. We
used a naive approach for sense labelling and
consider only explicit relations. Yet, the parser
achieves an overall F1 measure of 17.38%, rank-
ing in 6th place out of the 17 parsers submitted to
the CoNLL 2015 shared task.
</bodyText>
<sectionHeader confidence="0.998652" genericHeader="method">
5 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9987395">
The authors would like to thank the CoNLL 2015
organisers and the anonymous reviewers. This
</bodyText>
<footnote confidence="0.9975315">
3The example is taken from the CoNLL 2015 develop-
ment dataset.
</footnote>
<page confidence="0.998802">
59
</page>
<bodyText confidence="0.940999">
work was financially supported by NSERC.
</bodyText>
<sectionHeader confidence="0.992405" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999754324324325">
[Bethard et al.2014] Steven Bethard, Philip Ogren, and
Lee Becker. 2014. ClearTK 2.0: Design patterns for
machine learning in UIMA. LREC.
[Ferrucci and Lally2004] David Ferrucci and Adam
Lally. 2004. UIMA: An architectural approach to
unstructured information processing in the corporate
research environment. Natural Language Engineer-
ing, 10(3-4):327–348.
[Hall et al.2009] Mark Hall, Eibe Frank, Geoffrey
Holmes, Bernhard Pfahringer, Peter Reutemann, and
Ian H. Witten. 2009. The WEKA data mining
software: An update. ACM SIGKDD explorations
newsletter, 11(1):10–18.
[Kong et al.2014] Fang Kong, Hwee Tou Ng, and
Guodong Zhou. 2014. A Constituent-Based Ap-
proach to Argument Labeling with Joint Inference in
Discourse Parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 68–77, Doha, Qatar,
October.
[Lin et al.2014] Ziheng Lin, Hwee Tou Ng, and Min-
Yen Kan. 2014. A PDTB-styled end-to-end
discourse parser. Natural Language Engineering,
20(02):151–184.
[Petrov and Klein2007] Slav Petrov and Dan Klein.
2007. Improved Inference for Unlexicalized Pars-
ing. In Proceedings of NAACL HLT 2007, page
404–411, Rochester, NY, April.
[Pitler et al.2009] Emily Pitler, Annie Louis, and Ani
Nenkova. 2009. Automatic sense prediction for im-
plicit discourse relations in text. In Proceedings of
the 47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP, page 683–691, Suntec, Singa-
pore, August.
[Quinlan1993] J. Ross Quinlan. 1993. C4.5: Programs
for Machine Learning. Morgan Kaufmann Publish-
ers Inc., San Francisco, CA, USA.
</reference>
<page confidence="0.998414">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.250886">
<title confidence="0.645911">The CLaC Discourse Parser at CoNLL-2015</title>
<author confidence="0.992072">Majid Laali Elnaz Davoodi Leila Kosseim</author>
<affiliation confidence="0.732879333333333">Department of Computer Science and Software Concordia University, Montreal, Quebec, Canada laali, e davoo,</affiliation>
<abstract confidence="0.999447090909091">paper describes our submission to the CoNLL-2015 shared task on shallow discourse parsing. We used the UIMA framework to develop our parser and used ClearTK to add machine learning functionality to the UIMA framework. Overall, our parser achieves a result of the identification of discourse relations on the blind CoNLL-2015 test set, ranking in sixth place.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Philip Ogren</author>
<author>Lee Becker</author>
</authors>
<title>ClearTK 2.0: Design patterns for machine learning in UIMA.</title>
<date>2014</date>
<publisher>LREC.</publisher>
<marker>[Bethard et al.2014]</marker>
<rawString>Steven Bethard, Philip Ogren, and Lee Becker. 2014. ClearTK 2.0: Design patterns for machine learning in UIMA. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Adam Lally</author>
</authors>
<title>UIMA: An architectural approach to unstructured information processing in the corporate research environment.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<pages>10--3</pages>
<marker>[Ferrucci and Lally2004]</marker>
<rawString>David Ferrucci and Adam Lally. 2004. UIMA: An architectural approach to unstructured information processing in the corporate research environment. Natural Language Engineering, 10(3-4):327–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<marker>[Hall et al.2009]</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. ACM SIGKDD explorations newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Hwee Tou Ng</author>
<author>Guodong Zhou</author>
</authors>
<title>A Constituent-Based Approach to Argument Labeling with Joint Inference in Discourse Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>68--77</pages>
<location>Doha, Qatar,</location>
<marker>[Kong et al.2014]</marker>
<rawString>Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014. A Constituent-Based Approach to Argument Labeling with Joint Inference in Discourse Parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>MinYen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>02</issue>
<marker>[Lin et al.2014]</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and MinYen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, 20(02):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT 2007,</booktitle>
<pages>404--411</pages>
<location>Rochester, NY,</location>
<marker>[Petrov and Klein2007]</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of NAACL HLT 2007, page 404–411, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>683--691</pages>
<location>Suntec, Singapore,</location>
<marker>[Pitler et al.2009]</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, page 683–691, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<marker>[Quinlan1993]</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>