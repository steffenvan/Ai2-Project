<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98688">
Clique-Based Clustering for improving Named Entity Recognition systems
</title>
<author confidence="0.814591">
Julien Ah-Pine
</author>
<affiliation confidence="0.744968">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.685415">
6, chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.990043">
julien.ah-pine@xrce.xerox.com
</email>
<author confidence="0.567236">
Guillaume Jacquet
</author>
<affiliation confidence="0.520482">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.515109">
6, chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.991844">
guillaume.jacquet@xrce.xerox.com
</email>
<sectionHeader confidence="0.99465" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999062">
We propose a system which builds, in a
semi-supervised manner, a resource that
aims at helping a NER system to anno-
tate corpus-specific named entities. This
system is based on a distributional ap-
proach which uses syntactic dependen-
cies for measuring similarities between
named entities. The specificity of the
presented method however, is to combine
a clique-based approach and a clustering
technique that amounts to a soft clustering
method. Our experiments show that the
resource constructed by using this clique-
based clustering system allows to improve
different NER systems.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999365777777778">
In Information Extraction domain, named entities
(NEs) are one of the most important textual units
as they express an important part of the meaning
of a document. Named entity recognition (NER)
is not a new domain (see MUC1 and ACE2 confer-
ences) but some new needs appeared concerning
NEs processing. For instance the NE Oxford illus-
trates the different ambiguity types that are inter-
esting to address:
</bodyText>
<listItem confidence="0.998832142857143">
• intra-annotation ambiguity: Wikipedia lists
more than 25 cities named Oxford in the world
• systematic inter-annotation ambiguity: the
name of cities could be used to refer to the uni-
versity of this city or the football club of this
city. This is the case for Oxford or Newcastle
• non-systematic inter-annotation ambiguity:
</listItem>
<bodyText confidence="0.7787015">
Oxford is also a company unlike Newcastle.
The main goal of our system is to act in a com-
plementary way with an existing NER system, in
order to enhance its results. We address two kinds
</bodyText>
<footnote confidence="0.99985">
1http://www-nlpir.nist.gov/related projects/muc/
2http://www.nist.gov/speech/tests/ace
</footnote>
<bodyText confidence="0.999680631578947">
of issues: first, we want to detect and correctly
annotate corpus-specific NEs3 that the NER sys-
tem could have missed; second, we want to correct
some wrong annotations provided by the existing
NER system due to ambiguity. In section 3, we
give some examples of such corrections.
The paper is organized as follows. We present,
in section 2, the global architecture of our system
and from §2.1 to §2.6, we give details about each
of its steps. In section 3, we present the evalu-
ation of our approach when it is combined with
other classic NER systems. We show that the re-
sulting hybrid systems perform better with respect
to F-measure. In the best case, the latter increased
by 4.84 points. Furthermore, we give examples of
successful correction of NEs annotation thanks to
our approach. Then, in section 4, we discuss about
related works. Finally we sum up the main points
of this paper in section 5.
</bodyText>
<sectionHeader confidence="0.686776" genericHeader="method">
2 Description of the system
</sectionHeader>
<bodyText confidence="0.9803486875">
Given a corpus, the main objectives of our system
are: to detect potential NEs; to compute the possi-
ble annotations for each NE and then; to annotate
each occurrence of these NEs with the right anno-
tation by analyzing its local context.
We assume that this corpus dependent approach
allows an easier NE annotation. Indeed, even if
a NE such as Oxford can have many annotation
types, it will certainly have less annotation possi-
bilities in a specific corpus.
Figure 1 presents the global architecture of our
system. The most important part concerns steps
3 (§2.3) and 4 (§2.4). The aim of these sub-
processes is to group NEs which have the same
annotation with respect to a given context. On
the one hand, clique-based methods (see §2.3 for
</bodyText>
<footnote confidence="0.9965745">
3In our definition a corpus-specific NE is the one which
does not appear in a classic NEs lexicon. Recent news articles
for instance, are often constituted of NEs that are not in a
classic NEs lexicon.
</footnote>
<note confidence="0.962175">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 51–59,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99688">
51
</page>
<figureCaption confidence="0.99968">
Figure 1: General description of our system
</figureCaption>
<bodyText confidence="0.986948421052632">
details on cliques) are interesting as they allow
the same NE to be in different cliques. In other
words, cliques allow to represent the different pos-
sible annotations of a NE. The clique-based ap-
proach drawback however, is the over production
of cliques which corresponds to an artificial over
production of possible annotations for a NE. On
the other hand, clustering methods aim at struc-
turing a data set and such techniques can be seen
as data compression processes. However, a sim-
ple NEs hard clustering doesn’t allow a NE to be
in several clusters and thus to express its differ-
ent annotations. Then, our proposal is to combine
both methods in a clique-based clustering frame-
work. This combination leads to a soft-clustering
approach that we denote CBC system. The fol-
lowing paragraphs, from 2.1 to 2.6, describe the
respective steps mentioned in Figure 1.
relation with a noun as governee argument (e.g.
</bodyText>
<equation confidence="0.393525">
attribute
president −−−−→ George Bush)
</equation>
<listItem confidence="0.9350795">
• a governee argument of a modifier syntactic re-
lation with a noun as a governor argument (e.g.
</listItem>
<equation confidence="0.632644">
modifier
company ←−−−− Coca-Cola).
</equation>
<bodyText confidence="0.99607">
The list of potential NEs extracted from the cor-
pus will be denoted NE and the number of NEs
|NE|.
</bodyText>
<subsectionHeader confidence="0.999512">
2.2 Distributional space of NEs
</subsectionHeader>
<bodyText confidence="0.949991766666667">
The distributional approach aims at evaluating a
distance between words based on their syntac-
tic distribution. This method assumes that words
which appear in the same contexts are semanti-
cally similar (Harris, 1951).
To construct the distributional space associated
to a corpus, we use a robust parser (in our ex-
periments, we used XIP parser (Ait et al., 2002))
to extract chunks (i.e. nouns, noun phrases, ... )
and syntactic dependencies between these chunks.
Given this parser’s output, we identify triple in-
stances. Each triple has the form w1.R.w2 where
w1 and w2 are chunks and R is a syntactic relation
(Lin, 1998), (Kilgarriff et al., 2004).
One triple gives two contexts (1.w1.R and
2.w2.R) and two chunks (w1 and w2). Then, we
only select chunks w which belong to NE. Each
point in the distributional space is a NE and each
dimension is a syntactic context. CT denotes the
set of all syntactic contexts and |CT |represents its
cardinal.
We illustrate this construction on the sentence
“provide Albania with food aid”. We obtain the
three following triples (note that aid and food aid
are considered as two different chunks):
provide VERB•I-OBJ•Albania NOUN
provide VERB•PREP WITH•aid NOUN
provide VERB•PREP WITH•food aid NP
From these triples, we have the following
chunks and contexts4:
</bodyText>
<sectionHeader confidence="0.567819" genericHeader="method">
Chunks:
Contexts:
</sectionHeader>
<subsectionHeader confidence="0.999781">
2.1 Detection of potential Named Entities
</subsectionHeader>
<bodyText confidence="0.999940375">
Different methods exist for detecting potential
NEs. In our system, we used some lexico-
syntactic constraints to extract expressions from a
corpus because it allows to detect some corpus-
specific NEs. In our approach, a potential NE is a
noun starting with an upper-case letter or a noun
phrase which is (see (Ehrmann and Jacquet, 2007)
for similar use):
</bodyText>
<listItem confidence="0.983018">
• a governor argument of an attribute syntactic
</listItem>
<footnote confidence="0.925705533333333">
1.provide VERB.I-OBJ
1.provide VERB.PREP WITH
2.Albania NOUN.I-OBJ
2.aid NOUN.PREP WITH
2.food aid NP.PREP WITH
According to the NEs detection method de-
scribed previously, we only keep the chunks and
contexts which are in bold in the above table.
4In the context 1.VERB:provide.I-OBJ, the figure 1
means that the verb provide is the governor argument of the
Indirect OBJect relation.
provide VERB
Albania NOUN
aid NOUN
food aid NP
</footnote>
<page confidence="0.999113">
52
</page>
<bodyText confidence="0.938402428571429">
We also use an heuristic in order to reduce the
over production of chunks and contexts: in our ex-
periments for example, each NE and each context
should appear more than 10 times in the corpus for
being considered.
D is the resulting (|NE |x |CT|) NE-Context
matrix where ei : i = 1, ... , |NE |is a NE and
</bodyText>
<equation confidence="0.952641333333333">
cj : j = 1, ... , |CT |is a syntactic context. Then
we have:
D(ei, cj) = Nb. of occ. of cj associated to ei (1)
</equation>
<subsectionHeader confidence="0.99986">
2.3 Cliques of NEs computation
</subsectionHeader>
<bodyText confidence="0.999835782608696">
A clique in a graph is a set of pairwise adja-
cent nodes which is equivalent to a complete sub-
graph. A maximal clique is a clique that is not a
subset of any other clique. Maximal cliques com-
putation was already employed for semantic space
representation (Ploux and Victorri, 1998). In this
work, cliques of lexical units are used to represent
a precise meaning. Similarly, we compute cliques
of NEs in order to represent a precise annotation.
For example, Oxford is an ambiguous NE
but a clique such as &lt;Cambridge, Oxford, Ed-
inburgh University, Edinburgh, Oxford Univer-
sity&gt; allows to focus on the specific annota-
tion &lt;organization&gt; (see (Ehrmann and Jacquet,
2007) for similar use).
Given the distributional space described in the
previous paragraph, we use a probabilistic frame-
work for computing similarities between NEs.
The approach that we propose is inspired from
the language modeling framework introduced in
the information retrieval field (see for example
(Lavrenko and Croft, 2003)). Then, we construct
cliques of NEs based on these similarities.
</bodyText>
<subsectionHeader confidence="0.711913">
2.3.1 Similarity measures between NEs
</subsectionHeader>
<bodyText confidence="0.999925">
We first compute the maximum likelihood esti-
mation for a NE ei to be associated with a con-
</bodyText>
<equation confidence="0.98130775">
text cj: Pml(cj|ei) = D(ei,cj)
, where |ei |=
|ei
P|CT|
</equation>
<bodyText confidence="0.910204857142857">
j=1 D(ei, cj) is the total occurrences of the NE
ei in the corpus.
This leads to sparse data which is not suitable
for measuring similarities. In order to counter
this problem, we use the Jelinek-Mercer smooth-
ing method: D0(ei, cj) = λPml(cj|ei) + (1 −
λ)Pml(cj|CORP) where CORP is the corpus and
</bodyText>
<equation confidence="0.8972485">
Pml (cj  |CORP) = Ei D(ee,cj ). In our experi-
Ei,j i,cj)
</equation>
<bodyText confidence="0.992067">
ments we took λ = 0.5.
Given D0, we then use the cross-entropy as a
similarity measure between NEs. Let us denote by
</bodyText>
<equation confidence="0.871266666666667">
s this similarity matrix, we have:
Xs(ei, e0i) = − D0(ei, cj) log(D0(ei0, cj)) (2)
cj∈CT
</equation>
<bodyText confidence="0.909167666666667">
2.3.2 From similarity matrix to adjacency
matrix
Next, we convert s into an adjacency matrix de-
noted s. In a first step, we binarize s as fol-
lows. Let us denote fei1, ... , ei|NE|}, the list of NEs
ranked according to the descending order of their
similarity with ei. Then, L(ei) is the list of NEs
which are considered as the nearest neighbors of
ei according to the following definition:
</bodyText>
<equation confidence="0.9995055">
L(ei) = (3)
Pp i0=1 s(ei, ei i0)
fei 1, ..., ei p : |NE |&lt; a; p &lt; b}
Pi0=1 s(ei, ei0)
</equation>
<bodyText confidence="0.9572772">
where a E [0, 1] and b E f1,..., |NE|}. L(ei)
gathers the most significant nearest neighbors of ei
by choosing the ones which bring the a most rele-
vant similarities providing that the neighborhood’s
size doesn’t exceed b. This approach can be seen
as a flexible k-nearest neighbor method. In our
experiments we chose a = 20% and b = 10.
Finally, we symmetrize the similarity matrix as
follows and we obtain s:
~ 1 if ei0 E L(ei) or ei E L(ei0)
</bodyText>
<equation confidence="0.884591">
s(ei, ei0) = (4)
0 otherwise
</equation>
<subsubsectionHeader confidence="0.591887">
2.3.3 Cliques computation
</subsubsectionHeader>
<bodyText confidence="0.99639325">
Given s, the adjacency matrix between NEs, we
compute the set of maximal cliques of NEs de-
noted CLI. Then, we construct the matrix T of
general term:
</bodyText>
<equation confidence="0.921786">
~ T cli 1 if ei E clik 5
( k, e Z) = 0 otherwise ( )
</equation>
<bodyText confidence="0.9990658">
where clik is an element of CLI. T will be the
input matrix for the clustering method.
In the following, we also use clik
for denoting the vector represented by
(T(clik, e1), ... ,T(clik, e|NE|)).
Figure 2 shows some cliques which contain Ox-
ford that we can obtain with this method. This fig-
ure also illustrates the over production of cliques
since at least cli8, cli10 and cli12 can be annotated
as &lt;organization&gt;.
</bodyText>
<page confidence="0.998236">
53
</page>
<figureCaption confidence="0.999571">
Figure 2: Examples of cliques containing Oxford
</figureCaption>
<subsectionHeader confidence="0.993467">
2.4 Cliques clustering
</subsectionHeader>
<bodyText confidence="0.999990727272727">
We use a clustering technique in order to group
cliques of NEs which are mutually highly simi-
lar. The clusters of cliques which contain a NE
allow to find the different possible annotations of
this NE.
This clustering technique must be able to con-
struct “pure” clusters in order to have precise an-
notations. In that case, it is desirable to avoid
fixing the number of clusters. That’s the reason
why we propose to use the Relational Analysis ap-
proach described below.
</bodyText>
<subsectionHeader confidence="0.840948">
2.4.1 The Relational Analysis approach
</subsectionHeader>
<bodyText confidence="0.999359923076923">
We propose to apply the Relational Analysis ap-
proach (RA) which is a clustering model that
doesn’t require to fix the number of clusters
(Michaud and Marcotorchino, 1980), (B´ed´ecarrax
and Warnesson, 1989). This approach takes as in-
put a similarity matrix. In our context, since we
want to cluster cliques of NEs, the correspond-
ing similarity matrix S between cliques is given
by the dot products matrix taken from T: S =
T · T0. The general term of this similarity matrix
is: S(clik, clik0) = Skk0 = (clik, clik0). Then, we
want to maximize the following clustering func-
tion:
</bodyText>
<equation confidence="0.9967014">
0(S, X) = (6)
E(k00,k000)∈S+ Sk00k000
Skk0 − |S+|
� N., �
contkk0
</equation>
<bodyText confidence="0.954352">
where S+ = {(clik, clik0) : Skk0 &gt; 0}.
In other words, clik and clik0 have more chances
to be in the same cluster providing that their sim-
ilarity measure, Skk0, is greater or equal to the
mean average of positive similarities.
</bodyText>
<listItem confidence="0.937197818181818">
X is the solution we are looking for. It is a bi-
nary relational matrix with general term: Xkk0 =
1, if clik is in the same cluster as clik0; and Xkk0 =
0, otherwise. X represents an equivalence rela-
tion. Thus, it must respect the following proper-
ties:
• binarity: Xkk0 E {0,1}; bk, k0,
• reflexivity: Xkk = 1; bk,
• symmetry: Xkk0 − Xk0k = 0; bk, k0,
• transitivity: Xkk0 + Xk0k00 − Xkk00 :5
1; bk, k0, k00.
</listItem>
<bodyText confidence="0.999586428571429">
As the objective function is linear with respect
to X and as the constraints that X must respect are
linear equations, we can solve the clustering prob-
lem using an integer linear programming solver.
However, this problem is NP-hard. As a result, in
practice, we use heuristics for dealing with large
data sets.
</bodyText>
<subsubsectionHeader confidence="0.508667">
2.4.2 The Relational Analysis heuristic
</subsubsectionHeader>
<bodyText confidence="0.999592125">
The presented heuristic is quite similar to another
algorithm described in (Hartigan, 1975) known as
the “leader” algorithm. But unlike this last ap-
proach which is based upon euclidean distances
and inertial criteria, the RA heuristic aims at max-
imizing the criterion given in (6). A sketch of this
heuristic is given in Algorithm 1, (see (Marco-
torchino and Michaud, 1981) for further details).
</bodyText>
<equation confidence="0.677718166666667">
Algorithm 1 RA heuristic
Require: nbitr = number of iterations; rcm. = maximal
number of clusters; S the similarity matrix
E
(k k0)∈S+ Skk0
|S+|
</equation>
<bodyText confidence="0.936265375">
Take the first clique clik as the first element of the first
cluster
rc = 1 where rc is the current number of cluster
for q = 1 to nbitr do
for k = 1 to JCLHJ do
for l = 1 to rc do
Compute the contribution of clique clik with clus-
ter clul: contl = Eclik0 ∈clul (Skk0 − m)
</bodyText>
<subsectionHeader confidence="0.910336">
end for
</subsectionHeader>
<bodyText confidence="0.8607275">
clul∗ is the cluster id which has the highest contribu-
tion with clique clik and contl∗ is the corresponding
</bodyText>
<construct confidence="0.520471888888889">
contribution value
if (contl∗ &lt; (Skk − m)) n (rc &lt; rcm.) then
Create a new cluster where clique clik is the first
element and rc +— rc + 1
else
Assign clique clik to cluster clul∗
if the cluster where was taken clik before its new
assignment, is empty then
rc +— rc − 1
</construct>
<tableCaption confidence="0.89721">
end if
end if
end for
end for
</tableCaption>
<bodyText confidence="0.990445">
We have to provide a number of iterations
</bodyText>
<equation confidence="0.7700932">
|CLI|
�
k,k0=1
Xkk0
m +—
</equation>
<page confidence="0.968458">
54
</page>
<bodyText confidence="0.985818666666667">
or/and a delta threshold in order to have an approx-
imate solution in a reasonable processing time.
Besides, it is also required a maximum number of
clusters but since we don’t want to fix this param-
eter, we put by default κmax = |CLI|.
Basically, this heuristic has a O(nbitr xκmax x
|CLI|) computation cost. In general terms, we can
assume that nbitr &lt;&lt; |CLI|, but not κmax &lt;&lt;
|CLI|. Thus, in the worst case, the algorithm has
</bodyText>
<figure confidence="0.833195">
a O(κmax x |CLI|) computation cost.
</figure>
<figureCaption confidence="0.9946465">
Figure 3 gives some examples of clusters of
cliques5 obtained using the RA approach.
Figure 3: Examples of clusters of cliques (only the
NEs are represented) and their associated contexts
</figureCaption>
<subsectionHeader confidence="0.9012435">
2.5 NE resource construction using the CBC
system’s outputs
</subsectionHeader>
<bodyText confidence="0.850889055555556">
Now, we want to exploit the clusters of cliques in
order to annotate NE occurrences. Then, we need
to construct a NE resource where for each pair (NE
x syntactic context) we have an annotation. To this
end, we need first, to assign a cluster to each pair
(NE x syntactic context) (§2.5.1) and second, to
assign each cluster an annotation (§2.5.2).
2.5.1 Cluster assignment to each pair (NE x
syntactic context)
For each cluster clul we provide a score
Fc(cj, clul) for each context cj and a score
5We only represent the NEs and their frequency in the
cluster which corresponds to the number of cliques which
contain the NEs. Furthermore, we represent the most relevant
contexts for this cluster according to equation (7) introduced
in the following.
Fe(ei, clul) for each NE ei. These scores6 are
given by:
</bodyText>
<equation confidence="0.847031833333333">
Fc(cj,clul) = (7)
D(ei, cj)
E|S |D(e• C �) � 1 {D(ei,cj)�0}
i=1 a� 7 eiEclul
where 1{P} equals 1 if P is true and 0 otherwise.
Fe(ei, clul) = #(clul, ei) (8)
</equation>
<bodyText confidence="0.989186428571428">
Given a NE ei and a syntactic context
cj, we now introduce the contextual clus-
ter assignment matrix Actxt(ei, cj) as fol-
lows: Actxt(ei, cj) = clu* where: clu* =
Argmax{clul:clulE)ei;F,(ei,clul)&gt;1}Fc(cj, clul).
In other words, clu* is the cluster for which we
find more than one occurrence of ei and the high-
est score related to the context cj.
Furthermore, we compute a default cluster as-
signment matrix Adef, which does not depend on
the local context: Adef(ei) = clu* where: clu* =
Argmax{clul:clulE){clik:clikE)ei}}|clik|.
In other words, clu* is the cluster containing the
biggest clique clik containing ei.
</bodyText>
<subsectionHeader confidence="0.646549">
2.5.2 Clusters annotation
</subsectionHeader>
<bodyText confidence="0.993618380952381">
So far, the different steps that we have introduced
were unsupervised. In this paragraph, our aim is to
give a correct annotation to each cluster (hence, to
all NEs in this cluster). To this end, we need some
annotation seeds and we propose two different
semi-supervised approaches (regarding the classi-
fication given in (Nadeau and Sekine, 2007)). The
first one is the manual annotation of some clusters.
The second one proposes an automatic cluster an-
notation and assumes that we have some NEs that
are already annotated.
Manual annotation of clusters This method is
fastidious but it is the best way to match the cor-
pus data with a specific guidelines for annotating
NEs. It also allows to identify new types of an-
notation. We used the ACE2007 guidelines for
manually annotating each cluster. However, our
CBC system leads to a high number of clusters of
cliques and we can’t annotate each of them. For-
tunately, it also leads to a distribution of the clus-
ters’ size (number of cliques by cluster) which is
</bodyText>
<footnote confidence="0.9163185">
6For data fusion tasks in information retrieval field, the
scoring method in equation (7) is denoted CombMNZ (Fox
and Shaw, 1994). Other scoring approaches can be used see
for example (Cucchiarelli and Velardi, 2001).
</footnote>
<bodyText confidence="0.445367">
�
ei�clul
</bodyText>
<page confidence="0.99659">
55
</page>
<bodyText confidence="0.983645377777778">
similar to a Zipf distribution. Consequently, in our
experiments, if we annotate the 100 biggest clus-
ters, we annotate around eighty percent of the de-
tected NEs (see §3).
Automatic annotation of clusters We suppose
in this context that many NEs in NE are already
annotated. Thus, under this assumption, we have
in each cluster provided by the CBC system, both
annotated and non-annotated NEs. Our goal is to
exploit the available annotations for refining the
annotation of a cluster by implicitly taking into
account the syntactic contexts and for propagating
the available annotations to NEs which have no
annotation.
Given a cluster clue of cliques, #(clue, ei) is the
weight of the NE ei in this cluster: it is the number
of cliques in clue that contain ei. For all annota-
tions ap in the set of all possible annotations AN,
we compute its associated score in cluster clue: it
is the sum of the weights of NEs in clue that is
annotated ap.
Then, if the maximal annotation score is greater
than a simple majority (half) of the total votes7, we
assign the corresponding annotation to the clus-
ter. We precise that the annotation &lt;none&gt;8 is
processed in the same way as any other annota-
tions. Thus, a cluster can be globally annotated
&lt;none&gt;. The limit of this automatic approach is
that it doesn’t allow to annotate new NE types than
the ones already available.
In the following, we will denote by Aclu(clue)
the annotation of the cluster clue.
The cluster annotation matrix Aclu associated
to the contextual cluster assignment matrix Aetxt
and the default cluster assignment matrix Adef in-
troduced previously will be called the CBC sys-
tem’s NE resource (or shortly the NE resource).
2.6 NEs annotation processes using the NE
resource
In this paragraph, we describe how, given the CBC
system’s NE resource, we annotate occurrences of
NEs in the studied corpus with respect to its local
context. We precise that for an occurrence of a NE
ei its associated local context is the set of syntac-
tical dependencies cj in which ei is involved.
</bodyText>
<footnote confidence="0.995362">
7The //total votes number is given by
EegEclul #lclul, ei).
8The NEs which don’t have any annotation.
</footnote>
<bodyText confidence="0.813294375">
2.6.1 NEs annotation process for the CBC
system
Given a NE occurrence and its local context we
can use Aetxt(ei, cj) and Adef(ei) in order to get
the default annotation Aclu(Adef(ei)) and the list
of contextual annotations {Aclu(Aetxt(ei, cj))}j.
Then for annotating this NE occurrence using
our NE resource, we apply the following rules:
</bodyText>
<listItem confidence="0.988539214285714">
• if the list of contextual annotations
{Aclu(Aetxt(ei, cj))}j is conflictual, we
annotate the NE occurrence as &lt;none&gt;,
• if the list of contextual annotations is non-
conflictual, then we use the corresponding an-
notation to annotate the NE occurrence
• if the list of contextual annotations is empty,
we use the default annotation Aclu(Adef(ei)).
The NE resource plus the annotation process de-
scribed in this paragraph lead to a NER system
based on the CBC system. This NER system will
be called CBC-NER system and it will be tested in
our experiments both alone and as a complemen-
tary resource.
</listItem>
<bodyText confidence="0.903357444444444">
2.6.2 NEs annotation process for an hybrid
system
We place ourselves into an hybrid situation where
we have two NER systems (NER 1 + NER 2)
which provide two different lists of annotated
NEs. We want to combine these two systems when
annotating NEs occurrences.
Therefore, we resolve any conflicts by applying
the following rules:
</bodyText>
<listItem confidence="0.997091333333333">
• If the same NE occurrence has two different an-
notations from the two systems then there are
two cases. If one of the two system is CBC-
NER system then we take its annotation; oth-
erwise we take the annotation provided by the
NER system which gave the best precision.
• If a NE occurrence is included in another one
we only keep the biggest one and its annota-
tion. For example, if Jacques Chirac is anno-
tated &lt;person&gt; by one system and Chirac by
&lt;person&gt; by the other system, then we only
keep the first annotation.
• If two NE occurrences are contiguous and have
the same annotation, we merge the two NEs in
one NE occurrence.
</listItem>
<sectionHeader confidence="0.996366" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99329">
The system described in this paper rather target
corpus-specific NE annotation. Therefore, our ex-
</bodyText>
<page confidence="0.99249">
56
</page>
<bodyText confidence="0.99983805">
periments will deal with a corpus of recent news
articles (see (Shinyama and Sekine, 2004) for
motivations regarding our corpus choice) rather
than well-known annotated corpora. Our corpus
is constituted of news in English published on
the web during two weeks in June 2008. This
corpus is constituted of around 300,000 words
(10Mb) which doesn’t represent a very large cor-
pus. These texts were taken from various press
sources and they involve different themes (sports,
technology, ... ). We extracted randomly a sub-
set of articles and manually annotated 916 NEs (in
our experiments, we deal with three types of an-
notation namely &lt;person&gt;, &lt;organization&gt; and
&lt;location&gt;). This subset constitutes our test set.
In our experiments, first, we applied the XIP
parser (Ait et al., 2002) to the whole corpus in or-
der to construct the frequency matrix D given by
(1). Next, we computed the similarity matrix be-
tween NEs according to (2) in order to obtain s de-
fined by (4). Using the latter, we computed cliques
of NEs that allow us to obtain the assignment ma-
trix T given by (5). Then we applied the clustering
heuristic described in Algorithm 1. At this stage,
we want to build the NE resource using the clus-
ters of cliques. Therefore, as described in §2.5,
we applied two kinds of clusters annotations: the
manual and the automatic processes. For the first
one, we manually annotated the 100 biggest clus-
ters of cliques. For the second one, we exploited
the annotations provided by XIP NER (Brun and
Hag`ege, 2004) and we propagated these annota-
tions to the different clusters (see §2.5.2).
The different materials that we obtained consti-
tute the CBC system’s NE resource. Our aim now
is to exploit this resource and to show that it allows
to improve the performances of different classic
NER systems.
The different NER systems that we tested are
the following ones:
</bodyText>
<listItem confidence="0.997388181818182">
• CBC-NER system M (in short CBC M) based
on the CBC system’s NE resource using the
manual cluster annotation (line 1 in Table 1),
• CBC-NER system A (in short CBC A) based
on the CBC system’s NE resource using the au-
tomatic cluster annotation (line 1 in Table 1),
• XIP NER or in short XIP (Brun and Hag`ege,
2004) (line 2 in Table 1),
• Stanford NER (or in short Stanford) associ-
ated to the following model provided by the
tool and which was trained on different news
</listItem>
<table confidence="0.994655666666667">
Systems Prec. Rec. F-me.
1 CBC-NER system M 71.67 23.47 35.36
CBC-NER system A 70.66 32.86 44.86
2 XIP NER 77.77 56.55 65.48
XIP + CBC M 78.41 60.26 68.15
XIP + CBC A 76.31 60.48 67.48
Stanford NER 67.94 68.01 67.97
3 Stanford + CBC M 69.40 71.07 70.23
Stanford + CBC A 70.09 72.93 71.48
GATE NER 63.30 56.88 59.92
4 GATE + CBC M 66.43 61.79 64.03
GATE + CBC A 66.51 63.10 64.76
5 Stanford + XIP 72.85 75.87 74.33
Stanford + XIP + CBC M 72.94 77.70 75.24
Stanford + XIP + CBC A 73.55 78.93 76.15
GATE + XIP 69.38 66.04 67.67
6 GATE + XIP + CBC M 69.62 67.79 68.69
GATE + XIP + CBC A 69.87 69.10 69.48
GATE + Stanford 63.12 69.32 66.07
7 GATE + Stanford + CBC M 65.09 72.05 68.39
GATE + Stanford + CBC A 65.66 73.25 69.25
</table>
<tableCaption confidence="0.979276">
Table 1: Results given by different hybrid NER
systems and coupled with the CBC-NER system
</tableCaption>
<bodyText confidence="0.915526333333333">
corpora (CoNLL, MUC6, MUC7 and ACE):
ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel
et al., 2005) (line 3 in Table 1),
</bodyText>
<listItem confidence="0.972864666666667">
• GATE NER or in short GATE (Cunningham et
al., 2002) (line 4 in Table 1),
• and several hybrid systems which are given by
</listItem>
<bodyText confidence="0.993924583333333">
the combination of pairs taken among the set
of the three last-mentioned NER systems (lines
5 to 7 in Table 1). Notice that these baseline
hybrid systems use the annotation combination
process described in §2.6.1.
In Table 1 we first reported in each line, the re-
sults given by each system when they are applied
alone (figures in italics). These performances rep-
resent our baselines. Second, we tested for each
baseline system, an extended hybrid system that
integrates the CBC-NER systems (with respect to
the combination process detailed in §2.6.2).
The first two lines of Table 1 show that the
two CBC-NER systems alone lead to rather poor
results. However, our aim is to show that the
CBC-NER system is, despite its low performances
alone, complementary to other basic NER sys-
tems. In other words, we want to show that the
exploitation of the CBC system’s NE resource is
beneficial and non-redundant compared to other
baseline NER systems.
This is actually what we obtained in Table 1 as
for each line from 2 to 7, the extended hybrid sys-
tems that integrate the CBC-NER systems (M or
</bodyText>
<page confidence="0.994671">
57
</page>
<bodyText confidence="0.999888714285714">
A) always perform better than the baseline either
in terms of precision9 or recall. For each line, we
put in bold the best performance according to the
F-measure.
These results allow us to show that the NE re-
source built using the CBC system is complemen-
tary to any baseline NER systems and that it al-
lows to improve the results of the latter.
In order to illustrate why the CBC-NER systems
are beneficial, we give below some examples taken
from the test corpus for which the CBC system A
had allowed to improve the performances by re-
spectively disambiguating or correcting a wrong
annotation or detecting corpus-specific NEs.
First, in the sentence “From the start, his par-
ents, Lourdes and Hemery, were with him.”, the
baseline hybrid system Stanford + XIP anno-
tated the ambiguous NE “Lourdes” as &lt;location&gt;
whereas Stanford + XIP + CBC A gave the correct
annotation &lt;person&gt;.
Second, in the sentence “Got 3 percent chance
of survival, what ya gonna do?” The back read,
”A) Fight Through, b) Stay Strong, c) Overcome
Because I Am a Warrior.”, the baseline hybrid
system Stanford + XIP annotated “Warrior” as
&lt;organization&gt; whereas Stanford + XIP + CBC
A corrected this annotation with &lt;none&gt;.
Finally, in the sentence “Matthew, also a fa-
vorite to win in his fifth and final appearance,
was stunningly eliminated during the semifinal
round Friday when he misspelled “secernent”.”,
the baseline hybrid system Stanford + XIP didn’t
give any annotation to “Matthew” whereas Stan-
ford + XIP + CBC A allowed to give the annota-
tion &lt;person&gt;.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="method">
4 Related works
</sectionHeader>
<bodyText confidence="0.999971833333333">
Many previous works exist in NEs recognition and
classification. However, most of them do not build
a NEs resource but exploit external gazetteers
(Bunescu and Pasca, 2006), (Cucerzan, 2007).
A recent overview of the field is given in
(Nadeau and Sekine, 2007). According to this pa-
per, we can classify our method in the category
of semi-supervised approaches. Our proposal is
close to (Cucchiarelli and Velardi, 2001) as it uses
syntactic relations (§2.2) and as it relies on exist-
ing NER systems (§2.6.2). However, the partic-
ularity of our method concerns the clustering of
</bodyText>
<footnote confidence="0.6638095">
9Except for XIP+CBC A in line 2 where the precision is
slightly lower than XIP’s one.
</footnote>
<bodyText confidence="0.999935486486486">
cliques of NEs that allows both to represent the
different annotations of the NEs and to group the
latter with respect to one precise annotation ac-
cording to a local context.
Regarding this aspect, (Lin and Pantel, 2001)
and (Ngomo, 2008) also use a clique computa-
tion step and a clique merging method. However,
they do not deal with ambiguity of lexical units
nor with NEs. This means that, in their system, a
lexical unit can be in only one merged clique.
From a methodological point of view, our pro-
posal is also close to (Ehrmann and Jacquet, 2007)
as the latter proposes a system for NEs fine-
grained annotation, which is also corpus depen-
dent. However, in the present paper we use all
syntactic relations for measuring the similarity be-
tween NEs whereas in the previous mentioned
work, only specific syntactic relations were ex-
ploited. Moreover, we use clustering techniques
for dealing with the issue related to over produc-
tion of cliques.
In this paper, we construct a NE resource from
the corpus that we want to analyze. In that con-
text, (Pasca, 2004) presents a lightly supervised
method for acquiring NEs in arbitrary categories
from unstructured text of Web documents. How-
ever, Pasca wants to improve web search whereas
we aim at annotating specific NEs of an ana-
lyzed corpus. Besides, as we want to focus on
corpus-specific NEs, our work is also related to
(Shinyama and Sekine, 2004). In this work, the
authors found a significant correlation between the
similarity of the time series distribution of a word
and the likelihood of being a NE. This result mo-
tivated our choice to test our approach on recent
news articles rather than on well-known annotated
corpora.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979166666667">
We propose a system that allows to improve NE
recognition. The core of this system is a clique-
based clustering method based upon a distribu-
tional approach. It allows to extract, analyze and
discover highly relevant information for corpus-
specific NEs annotation. As we have shown in our
experiments, this system combined with another
one can lead to strong improvements. Other appli-
cations are currently addressed in our team using
this approach. For example, we intend to use the
concept of clique-based clustering as a soft clus-
tering method for other issues.
</bodyText>
<page confidence="0.997371">
58
</page>
<sectionHeader confidence="0.993895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866465753425">
S. Ait, J.P. Chanod, and C. Roux. 2002. Robustness
beyond shallowness: incremental dependency pars-
ing. NLE Journal.
C. B´ed´ecarrax and I. Warnesson. 1989. Relational
analysis and dictionnaries. In Proceedings of AS-
MDA 1988, pages 131–151. Wiley, London, New-
York.
C. Brun and C. Hag`ege. 2004. Intertwining deep
syntactic processing and named entity detection. In
Proceedings of ESTAL 2004, Alicante, Spain.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL 2006.
A. Cucchiarelli and P. Velardi. 2001. Unsupervised
Named Entity Recognition using syntactic and se-
mantic contextual evidence. Computational Lin-
guistics, 27(1).
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP/CoNLL 2007, Prague, Czech Republic.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings ofACL 2002, Philadel-
phia.
M. Ehrmann and G. Jacquet. 2007. Vers une dou-
ble annotation des entit´es nomm´ees. TraitementAu-
tomatique des Langues, 47(3).
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL 2005.
E.A. Fox and J.A. Shaw. 1994. Combination of multi-
ple searches. In Proceedings of the 3rd NIST TREC
Conference, pages 105–109.
Z. Harris. 1951. Structural Linguistics. University of
Chicago Press.
J.A. Hartigan. 1975. Clustering Algorithms. John Wi-
ley and Sons.
A. Kilgarriff, P. Rychly, P. Smr, and D. Tugwell. 2004.
The sketch engine. In In Proceedings of EURALEX
2004.
V. Lavrenko and W.B. Croft. 2003. Relevance models
in information retrieval. In W.B. Croft and J. Laf-
ferty (Eds), editors, Language modeling in informa-
tion retrieval. Springer.
D. Lin and P. Pantel. 2001. Induction of semantic
classes from natural language text. In Proceedings
of ACM SIGKDD.
D. Lin. 1998. Using collocation statistics in informa-
tion extraction. In Proceedings of MUC-7.
J.F. Marcotorchino and P. Michaud. 1981. Heuris-
tic approach of the similarity aggregation problem.
Methods of operation research, 43:395–404.
P. Michaud and J.F. Marcotorchino. 1980. Optimisa-
tion en analyse de donn´ees relationnelles. In Data
Analysis and informatics. North Holland Amster-
dam.
D. Nadeau and S. Sekine. 2007. A survey of Named
Entity Recognition and Classification. Lingvisticae
Investigationes, 30(1).
A. C. Ngonga Ngomo. 2008. Signum a graph algo-
rithm for terminology extraction. In Proceedings of
CICLING 2008, Haifa, Israel.
M. Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM
2004, New York, NY, USA.
S. Ploux and B. Victorri. 1998. Construction d’espaces
s´emantiques a` l’aide de dictionnaires de synonymes.
TAL, 39(1).
Y. Shinyama and S. Sekine. 2004. Named Entity Dis-
covery using comparable news articles. In Proceed-
ings of COLING 2004, Geneva.
</reference>
<page confidence="0.999252">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.554156">
<title confidence="0.998206">Clique-Based Clustering for improving Named Entity Recognition systems</title>
<author confidence="0.924815">Julien Ah-Pine</author>
<affiliation confidence="0.886452">Xerox Research Centre Europe</affiliation>
<address confidence="0.883234">6, chemin de Maupertuis 38240 Meylan, France</address>
<email confidence="0.999901">julien.ah-pine@xrce.xerox.com</email>
<author confidence="0.999465">Guillaume Jacquet</author>
<affiliation confidence="0.977465">Xerox Research Centre Europe</affiliation>
<address confidence="0.903804">6, chemin de Maupertuis 38240 Meylan, France</address>
<email confidence="0.999956">guillaume.jacquet@xrce.xerox.com</email>
<abstract confidence="0.99800925">We propose a system which builds, in a semi-supervised manner, a resource that aims at helping a NER system to annotate corpus-specific named entities. This system is based on a distributional approach which uses syntactic dependencies for measuring similarities between named entities. The specificity of the presented method however, is to combine a clique-based approach and a clustering technique that amounts to a soft clustering method. Our experiments show that the resource constructed by using this cliquebased clustering system allows to improve different NER systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ait</author>
<author>J P Chanod</author>
<author>C Roux</author>
</authors>
<title>Robustness beyond shallowness: incremental dependency parsing.</title>
<date>2002</date>
<journal>NLE Journal.</journal>
<contexts>
<context position="5585" citStr="Ait et al., 2002" startWordPosition="909" endWordPosition="912">overnee argument of a modifier syntactic relation with a noun as a governor argument (e.g. modifier company ←−−−− Coca-Cola). The list of potential NEs extracted from the corpus will be denoted NE and the number of NEs |NE|. 2.2 Distributional space of NEs The distributional approach aims at evaluating a distance between words based on their syntactic distribution. This method assumes that words which appear in the same contexts are semantically similar (Harris, 1951). To construct the distributional space associated to a corpus, we use a robust parser (in our experiments, we used XIP parser (Ait et al., 2002)) to extract chunks (i.e. nouns, noun phrases, ... ) and syntactic dependencies between these chunks. Given this parser’s output, we identify triple instances. Each triple has the form w1.R.w2 where w1 and w2 are chunks and R is a syntactic relation (Lin, 1998), (Kilgarriff et al., 2004). One triple gives two contexts (1.w1.R and 2.w2.R) and two chunks (w1 and w2). Then, we only select chunks w which belong to NE. Each point in the distributional space is a NE and each dimension is a syntactic context. CT denotes the set of all syntactic contexts and |CT |represents its cardinal. We illustrate</context>
<context position="23250" citStr="Ait et al., 2002" startWordPosition="4006" endWordPosition="4009">ted corpora. Our corpus is constituted of news in English published on the web during two weeks in June 2008. This corpus is constituted of around 300,000 words (10Mb) which doesn’t represent a very large corpus. These texts were taken from various press sources and they involve different themes (sports, technology, ... ). We extracted randomly a subset of articles and manually annotated 916 NEs (in our experiments, we deal with three types of annotation namely &lt;person&gt;, &lt;organization&gt; and &lt;location&gt;). This subset constitutes our test set. In our experiments, first, we applied the XIP parser (Ait et al., 2002) to the whole corpus in order to construct the frequency matrix D given by (1). Next, we computed the similarity matrix between NEs according to (2) in order to obtain s defined by (4). Using the latter, we computed cliques of NEs that allow us to obtain the assignment matrix T given by (5). Then we applied the clustering heuristic described in Algorithm 1. At this stage, we want to build the NE resource using the clusters of cliques. Therefore, as described in §2.5, we applied two kinds of clusters annotations: the manual and the automatic processes. For the first one, we manually annotated t</context>
</contexts>
<marker>Ait, Chanod, Roux, 2002</marker>
<rawString>S. Ait, J.P. Chanod, and C. Roux. 2002. Robustness beyond shallowness: incremental dependency parsing. NLE Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B´ed´ecarrax</author>
<author>I Warnesson</author>
</authors>
<title>Relational analysis and dictionnaries.</title>
<date>1989</date>
<booktitle>In Proceedings of ASMDA 1988,</booktitle>
<pages>131--151</pages>
<publisher>Wiley,</publisher>
<location>London, NewYork.</location>
<marker>B´ed´ecarrax, Warnesson, 1989</marker>
<rawString>C. B´ed´ecarrax and I. Warnesson. 1989. Relational analysis and dictionnaries. In Proceedings of ASMDA 1988, pages 131–151. Wiley, London, NewYork.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brun</author>
<author>C Hag`ege</author>
</authors>
<title>Intertwining deep syntactic processing and named entity detection.</title>
<date>2004</date>
<booktitle>In Proceedings of ESTAL 2004,</booktitle>
<location>Alicante,</location>
<marker>Brun, Hag`ege, 2004</marker>
<rawString>C. Brun and C. Hag`ege. 2004. Intertwining deep syntactic processing and named entity detection. In Proceedings of ESTAL 2004, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="28672" citStr="Bunescu and Pasca, 2006" startWordPosition="4976" endWordPosition="4979">rrior” as &lt;organization&gt; whereas Stanford + XIP + CBC A corrected this annotation with &lt;none&gt;. Finally, in the sentence “Matthew, also a favorite to win in his fifth and final appearance, was stunningly eliminated during the semifinal round Friday when he misspelled “secernent”.”, the baseline hybrid system Stanford + XIP didn’t give any annotation to “Matthew” whereas Stanford + XIP + CBC A allowed to give the annotation &lt;person&gt;. 4 Related works Many previous works exist in NEs recognition and classification. However, most of them do not build a NEs resource but exploit external gazetteers (Bunescu and Pasca, 2006), (Cucerzan, 2007). A recent overview of the field is given in (Nadeau and Sekine, 2007). According to this paper, we can classify our method in the category of semi-supervised approaches. Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2). However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cucchiarelli</author>
<author>P Velardi</author>
</authors>
<title>Unsupervised Named Entity Recognition using syntactic and semantic contextual evidence.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="18310" citStr="Cucchiarelli and Velardi, 2001" startWordPosition="3164" endWordPosition="3167">best way to match the corpus data with a specific guidelines for annotating NEs. It also allows to identify new types of annotation. We used the ACE2007 guidelines for manually annotating each cluster. However, our CBC system leads to a high number of clusters of cliques and we can’t annotate each of them. Fortunately, it also leads to a distribution of the clusters’ size (number of cliques by cluster) which is 6For data fusion tasks in information retrieval field, the scoring method in equation (7) is denoted CombMNZ (Fox and Shaw, 1994). Other scoring approaches can be used see for example (Cucchiarelli and Velardi, 2001). � ei�clul 55 similar to a Zipf distribution. Consequently, in our experiments, if we annotate the 100 biggest clusters, we annotate around eighty percent of the detected NEs (see §3). Automatic annotation of clusters We suppose in this context that many NEs in NE are already annotated. Thus, under this assumption, we have in each cluster provided by the CBC system, both annotated and non-annotated NEs. Our goal is to exploit the available annotations for refining the annotation of a cluster by implicitly taking into account the syntactic contexts and for propagating the available annotations</context>
<context position="28918" citStr="Cucchiarelli and Velardi, 2001" startWordPosition="5016" endWordPosition="5019">d Friday when he misspelled “secernent”.”, the baseline hybrid system Stanford + XIP didn’t give any annotation to “Matthew” whereas Stanford + XIP + CBC A allowed to give the annotation &lt;person&gt;. 4 Related works Many previous works exist in NEs recognition and classification. However, most of them do not build a NEs resource but exploit external gazetteers (Bunescu and Pasca, 2006), (Cucerzan, 2007). A recent overview of the field is given in (Nadeau and Sekine, 2007). According to this paper, we can classify our method in the category of semi-supervised approaches. Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2). However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context. Regarding this aspect, (Lin and Pantel, 2001) and (Ngomo, 2008) also use a clique computation step and a clique merging method. However, they do not deal with ambiguity of lexical u</context>
</contexts>
<marker>Cucchiarelli, Velardi, 2001</marker>
<rawString>A. Cucchiarelli and P. Velardi. 2001. Unsupervised Named Entity Recognition using syntactic and semantic contextual evidence. Computational Linguistics, 27(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP/CoNLL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="28690" citStr="Cucerzan, 2007" startWordPosition="4980" endWordPosition="4981">ereas Stanford + XIP + CBC A corrected this annotation with &lt;none&gt;. Finally, in the sentence “Matthew, also a favorite to win in his fifth and final appearance, was stunningly eliminated during the semifinal round Friday when he misspelled “secernent”.”, the baseline hybrid system Stanford + XIP didn’t give any annotation to “Matthew” whereas Stanford + XIP + CBC A allowed to give the annotation &lt;person&gt;. 4 Related works Many previous works exist in NEs recognition and classification. However, most of them do not build a NEs resource but exploit external gazetteers (Bunescu and Pasca, 2006), (Cucerzan, 2007). A recent overview of the field is given in (Nadeau and Sekine, 2007). According to this paper, we can classify our method in the category of semi-supervised approaches. Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2). However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one pr</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of EMNLP/CoNLL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="25787" citStr="Cunningham et al., 2002" startWordPosition="4482" endWordPosition="4485">66.51 63.10 64.76 5 Stanford + XIP 72.85 75.87 74.33 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 GATE + XIP 69.38 66.04 67.67 6 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 GATE + Stanford 63.12 69.32 66.07 7 GATE + Stanford + CBC M 65.09 72.05 68.39 GATE + Stanford + CBC A 65.66 73.25 69.25 Table 1: Results given by different hybrid NER systems and coupled with the CBC-NER system corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1), • GATE NER or in short GATE (Cunningham et al., 2002) (line 4 in Table 1), • and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1). Notice that these baseline hybrid systems use the annotation combination process described in §2.6.1. In Table 1 we first reported in each line, the results given by each system when they are applied alone (figures in italics). These performances represent our baselines. Second, we tested for each baseline system, an extended hybrid system that integrates the CBC-NER systems (with respect to the combination process</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings ofACL 2002, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ehrmann</author>
<author>G Jacquet</author>
</authors>
<title>Vers une double annotation des entit´es nomm´ees.</title>
<date>2007</date>
<booktitle>TraitementAutomatique des Langues,</booktitle>
<volume>47</volume>
<issue>3</issue>
<contexts>
<context position="6917" citStr="Ehrmann and Jacquet, 2007" startWordPosition="1126" endWordPosition="1129">(note that aid and food aid are considered as two different chunks): provide VERB•I-OBJ•Albania NOUN provide VERB•PREP WITH•aid NOUN provide VERB•PREP WITH•food aid NP From these triples, we have the following chunks and contexts4: Chunks: Contexts: 2.1 Detection of potential Named Entities Different methods exist for detecting potential NEs. In our system, we used some lexicosyntactic constraints to extract expressions from a corpus because it allows to detect some corpusspecific NEs. In our approach, a potential NE is a noun starting with an upper-case letter or a noun phrase which is (see (Ehrmann and Jacquet, 2007) for similar use): • a governor argument of an attribute syntactic 1.provide VERB.I-OBJ 1.provide VERB.PREP WITH 2.Albania NOUN.I-OBJ 2.aid NOUN.PREP WITH 2.food aid NP.PREP WITH According to the NEs detection method described previously, we only keep the chunks and contexts which are in bold in the above table. 4In the context 1.VERB:provide.I-OBJ, the figure 1 means that the verb provide is the governor argument of the Indirect OBJect relation. provide VERB Albania NOUN aid NOUN food aid NP 52 We also use an heuristic in order to reduce the over production of chunks and contexts: in our expe</context>
<context position="8532" citStr="Ehrmann and Jacquet, 2007" startWordPosition="1412" endWordPosition="1415">e adjacent nodes which is equivalent to a complete subgraph. A maximal clique is a clique that is not a subset of any other clique. Maximal cliques computation was already employed for semantic space representation (Ploux and Victorri, 1998). In this work, cliques of lexical units are used to represent a precise meaning. Similarly, we compute cliques of NEs in order to represent a precise annotation. For example, Oxford is an ambiguous NE but a clique such as &lt;Cambridge, Oxford, Edinburgh University, Edinburgh, Oxford University&gt; allows to focus on the specific annotation &lt;organization&gt; (see (Ehrmann and Jacquet, 2007) for similar use). Given the distributional space described in the previous paragraph, we use a probabilistic framework for computing similarities between NEs. The approach that we propose is inspired from the language modeling framework introduced in the information retrieval field (see for example (Lavrenko and Croft, 2003)). Then, we construct cliques of NEs based on these similarities. 2.3.1 Similarity measures between NEs We first compute the maximum likelihood estimation for a NE ei to be associated with a context cj: Pml(cj|ei) = D(ei,cj) , where |ei |= |ei P|CT| j=1 D(ei, cj) is the to</context>
<context position="29714" citStr="Ehrmann and Jacquet, 2007" startWordPosition="5158" endWordPosition="5161">XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context. Regarding this aspect, (Lin and Pantel, 2001) and (Ngomo, 2008) also use a clique computation step and a clique merging method. However, they do not deal with ambiguity of lexical units nor with NEs. This means that, in their system, a lexical unit can be in only one merged clique. From a methodological point of view, our proposal is also close to (Ehrmann and Jacquet, 2007) as the latter proposes a system for NEs finegrained annotation, which is also corpus dependent. However, in the present paper we use all syntactic relations for measuring the similarity between NEs whereas in the previous mentioned work, only specific syntactic relations were exploited. Moreover, we use clustering techniques for dealing with the issue related to over production of cliques. In this paper, we construct a NE resource from the corpus that we want to analyze. In that context, (Pasca, 2004) presents a lightly supervised method for acquiring NEs in arbitrary categories from unstruct</context>
</contexts>
<marker>Ehrmann, Jacquet, 2007</marker>
<rawString>M. Ehrmann and G. Jacquet. 2007. Vers une double annotation des entit´es nomm´ees. TraitementAutomatique des Langues, 47(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="25712" citStr="Finkel et al., 2005" startWordPosition="4466" endWordPosition="4469">TE NER 63.30 56.88 59.92 4 GATE + CBC M 66.43 61.79 64.03 GATE + CBC A 66.51 63.10 64.76 5 Stanford + XIP 72.85 75.87 74.33 Stanford + XIP + CBC M 72.94 77.70 75.24 Stanford + XIP + CBC A 73.55 78.93 76.15 GATE + XIP 69.38 66.04 67.67 6 GATE + XIP + CBC M 69.62 67.79 68.69 GATE + XIP + CBC A 69.87 69.10 69.48 GATE + Stanford 63.12 69.32 66.07 7 GATE + Stanford + CBC M 65.09 72.05 68.39 GATE + Stanford + CBC A 65.66 73.25 69.25 Table 1: Results given by different hybrid NER systems and coupled with the CBC-NER system corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1), • GATE NER or in short GATE (Cunningham et al., 2002) (line 4 in Table 1), • and several hybrid systems which are given by the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1). Notice that these baseline hybrid systems use the annotation combination process described in §2.6.1. In Table 1 we first reported in each line, the results given by each system when they are applied alone (figures in italics). These performances represent our baselines. Second, we tested for each baseline system, an extended hybrid system t</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J.R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Fox</author>
<author>J A Shaw</author>
</authors>
<title>Combination of multiple searches.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd NIST TREC Conference,</booktitle>
<pages>105--109</pages>
<contexts>
<context position="18223" citStr="Fox and Shaw, 1994" startWordPosition="3151" endWordPosition="3154">ted. Manual annotation of clusters This method is fastidious but it is the best way to match the corpus data with a specific guidelines for annotating NEs. It also allows to identify new types of annotation. We used the ACE2007 guidelines for manually annotating each cluster. However, our CBC system leads to a high number of clusters of cliques and we can’t annotate each of them. Fortunately, it also leads to a distribution of the clusters’ size (number of cliques by cluster) which is 6For data fusion tasks in information retrieval field, the scoring method in equation (7) is denoted CombMNZ (Fox and Shaw, 1994). Other scoring approaches can be used see for example (Cucchiarelli and Velardi, 2001). � ei�clul 55 similar to a Zipf distribution. Consequently, in our experiments, if we annotate the 100 biggest clusters, we annotate around eighty percent of the detected NEs (see §3). Automatic annotation of clusters We suppose in this context that many NEs in NE are already annotated. Thus, under this assumption, we have in each cluster provided by the CBC system, both annotated and non-annotated NEs. Our goal is to exploit the available annotations for refining the annotation of a cluster by implicitly t</context>
</contexts>
<marker>Fox, Shaw, 1994</marker>
<rawString>E.A. Fox and J.A. Shaw. 1994. Combination of multiple searches. In Proceedings of the 3rd NIST TREC Conference, pages 105–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Structural Linguistics.</title>
<date>1951</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="5440" citStr="Harris, 1951" startWordPosition="885" endWordPosition="886">ribe the respective steps mentioned in Figure 1. relation with a noun as governee argument (e.g. attribute president −−−−→ George Bush) • a governee argument of a modifier syntactic relation with a noun as a governor argument (e.g. modifier company ←−−−− Coca-Cola). The list of potential NEs extracted from the corpus will be denoted NE and the number of NEs |NE|. 2.2 Distributional space of NEs The distributional approach aims at evaluating a distance between words based on their syntactic distribution. This method assumes that words which appear in the same contexts are semantically similar (Harris, 1951). To construct the distributional space associated to a corpus, we use a robust parser (in our experiments, we used XIP parser (Ait et al., 2002)) to extract chunks (i.e. nouns, noun phrases, ... ) and syntactic dependencies between these chunks. Given this parser’s output, we identify triple instances. Each triple has the form w1.R.w2 where w1 and w2 are chunks and R is a syntactic relation (Lin, 1998), (Kilgarriff et al., 2004). One triple gives two contexts (1.w1.R and 2.w2.R) and two chunks (w1 and w2). Then, we only select chunks w which belong to NE. Each point in the distributional spac</context>
</contexts>
<marker>Harris, 1951</marker>
<rawString>Z. Harris. 1951. Structural Linguistics. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hartigan</author>
</authors>
<title>Clustering Algorithms.</title>
<date>1975</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="13563" citStr="Hartigan, 1975" startWordPosition="2323" endWordPosition="2324">wing properties: • binarity: Xkk0 E {0,1}; bk, k0, • reflexivity: Xkk = 1; bk, • symmetry: Xkk0 − Xk0k = 0; bk, k0, • transitivity: Xkk0 + Xk0k00 − Xkk00 :5 1; bk, k0, k00. As the objective function is linear with respect to X and as the constraints that X must respect are linear equations, we can solve the clustering problem using an integer linear programming solver. However, this problem is NP-hard. As a result, in practice, we use heuristics for dealing with large data sets. 2.4.2 The Relational Analysis heuristic The presented heuristic is quite similar to another algorithm described in (Hartigan, 1975) known as the “leader” algorithm. But unlike this last approach which is based upon euclidean distances and inertial criteria, the RA heuristic aims at maximizing the criterion given in (6). A sketch of this heuristic is given in Algorithm 1, (see (Marcotorchino and Michaud, 1981) for further details). Algorithm 1 RA heuristic Require: nbitr = number of iterations; rcm. = maximal number of clusters; S the similarity matrix E (k k0)∈S+ Skk0 |S+| Take the first clique clik as the first element of the first cluster rc = 1 where rc is the current number of cluster for q = 1 to nbitr do for k = 1 t</context>
</contexts>
<marker>Hartigan, 1975</marker>
<rawString>J.A. Hartigan. 1975. Clustering Algorithms. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>P Rychly</author>
<author>P Smr</author>
<author>D Tugwell</author>
</authors>
<title>The sketch engine. In</title>
<date>2004</date>
<booktitle>In Proceedings of EURALEX</booktitle>
<contexts>
<context position="5873" citStr="Kilgarriff et al., 2004" startWordPosition="957" endWordPosition="960">ach aims at evaluating a distance between words based on their syntactic distribution. This method assumes that words which appear in the same contexts are semantically similar (Harris, 1951). To construct the distributional space associated to a corpus, we use a robust parser (in our experiments, we used XIP parser (Ait et al., 2002)) to extract chunks (i.e. nouns, noun phrases, ... ) and syntactic dependencies between these chunks. Given this parser’s output, we identify triple instances. Each triple has the form w1.R.w2 where w1 and w2 are chunks and R is a syntactic relation (Lin, 1998), (Kilgarriff et al., 2004). One triple gives two contexts (1.w1.R and 2.w2.R) and two chunks (w1 and w2). Then, we only select chunks w which belong to NE. Each point in the distributional space is a NE and each dimension is a syntactic context. CT denotes the set of all syntactic contexts and |CT |represents its cardinal. We illustrate this construction on the sentence “provide Albania with food aid”. We obtain the three following triples (note that aid and food aid are considered as two different chunks): provide VERB•I-OBJ•Albania NOUN provide VERB•PREP WITH•aid NOUN provide VERB•PREP WITH•food aid NP From these tri</context>
</contexts>
<marker>Kilgarriff, Rychly, Smr, Tugwell, 2004</marker>
<rawString>A. Kilgarriff, P. Rychly, P. Smr, and D. Tugwell. 2004. The sketch engine. In In Proceedings of EURALEX 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>Relevance models in information retrieval.</title>
<date>2003</date>
<editor>In W.B. Croft and J. Lafferty (Eds), editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="8859" citStr="Lavrenko and Croft, 2003" startWordPosition="1460" endWordPosition="1463">arly, we compute cliques of NEs in order to represent a precise annotation. For example, Oxford is an ambiguous NE but a clique such as &lt;Cambridge, Oxford, Edinburgh University, Edinburgh, Oxford University&gt; allows to focus on the specific annotation &lt;organization&gt; (see (Ehrmann and Jacquet, 2007) for similar use). Given the distributional space described in the previous paragraph, we use a probabilistic framework for computing similarities between NEs. The approach that we propose is inspired from the language modeling framework introduced in the information retrieval field (see for example (Lavrenko and Croft, 2003)). Then, we construct cliques of NEs based on these similarities. 2.3.1 Similarity measures between NEs We first compute the maximum likelihood estimation for a NE ei to be associated with a context cj: Pml(cj|ei) = D(ei,cj) , where |ei |= |ei P|CT| j=1 D(ei, cj) is the total occurrences of the NE ei in the corpus. This leads to sparse data which is not suitable for measuring similarities. In order to counter this problem, we use the Jelinek-Mercer smoothing method: D0(ei, cj) = λPml(cj|ei) + (1 − λ)Pml(cj|CORP) where CORP is the corpus and Pml (cj |CORP) = Ei D(ee,cj ). In our experiEi,j i,cj</context>
</contexts>
<marker>Lavrenko, Croft, 2003</marker>
<rawString>V. Lavrenko and W.B. Croft. 2003. Relevance models in information retrieval. In W.B. Croft and J. Lafferty (Eds), editors, Language modeling in information retrieval. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD.</booktitle>
<contexts>
<context position="29382" citStr="Lin and Pantel, 2001" startWordPosition="5097" endWordPosition="5100"> According to this paper, we can classify our method in the category of semi-supervised approaches. Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2). However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context. Regarding this aspect, (Lin and Pantel, 2001) and (Ngomo, 2008) also use a clique computation step and a clique merging method. However, they do not deal with ambiguity of lexical units nor with NEs. This means that, in their system, a lexical unit can be in only one merged clique. From a methodological point of view, our proposal is also close to (Ehrmann and Jacquet, 2007) as the latter proposes a system for NEs finegrained annotation, which is also corpus dependent. However, in the present paper we use all syntactic relations for measuring the similarity between NEs whereas in the previous mentioned work, only specific syntactic relat</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. Induction of semantic classes from natural language text. In Proceedings of ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using collocation statistics in information extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="5846" citStr="Lin, 1998" startWordPosition="955" endWordPosition="956">utional approach aims at evaluating a distance between words based on their syntactic distribution. This method assumes that words which appear in the same contexts are semantically similar (Harris, 1951). To construct the distributional space associated to a corpus, we use a robust parser (in our experiments, we used XIP parser (Ait et al., 2002)) to extract chunks (i.e. nouns, noun phrases, ... ) and syntactic dependencies between these chunks. Given this parser’s output, we identify triple instances. Each triple has the form w1.R.w2 where w1 and w2 are chunks and R is a syntactic relation (Lin, 1998), (Kilgarriff et al., 2004). One triple gives two contexts (1.w1.R and 2.w2.R) and two chunks (w1 and w2). Then, we only select chunks w which belong to NE. Each point in the distributional space is a NE and each dimension is a syntactic context. CT denotes the set of all syntactic contexts and |CT |represents its cardinal. We illustrate this construction on the sentence “provide Albania with food aid”. We obtain the three following triples (note that aid and food aid are considered as two different chunks): provide VERB•I-OBJ•Albania NOUN provide VERB•PREP WITH•aid NOUN provide VERB•PREP WITH</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Using collocation statistics in information extraction. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Marcotorchino</author>
<author>P Michaud</author>
</authors>
<title>Heuristic approach of the similarity aggregation problem. Methods of operation research,</title>
<date>1981</date>
<pages>43--395</pages>
<contexts>
<context position="13844" citStr="Marcotorchino and Michaud, 1981" startWordPosition="2368" endWordPosition="2372">spect are linear equations, we can solve the clustering problem using an integer linear programming solver. However, this problem is NP-hard. As a result, in practice, we use heuristics for dealing with large data sets. 2.4.2 The Relational Analysis heuristic The presented heuristic is quite similar to another algorithm described in (Hartigan, 1975) known as the “leader” algorithm. But unlike this last approach which is based upon euclidean distances and inertial criteria, the RA heuristic aims at maximizing the criterion given in (6). A sketch of this heuristic is given in Algorithm 1, (see (Marcotorchino and Michaud, 1981) for further details). Algorithm 1 RA heuristic Require: nbitr = number of iterations; rcm. = maximal number of clusters; S the similarity matrix E (k k0)∈S+ Skk0 |S+| Take the first clique clik as the first element of the first cluster rc = 1 where rc is the current number of cluster for q = 1 to nbitr do for k = 1 to JCLHJ do for l = 1 to rc do Compute the contribution of clique clik with cluster clul: contl = Eclik0 ∈clul (Skk0 − m) end for clul∗ is the cluster id which has the highest contribution with clique clik and contl∗ is the corresponding contribution value if (contl∗ &lt; (Skk − m)) n</context>
</contexts>
<marker>Marcotorchino, Michaud, 1981</marker>
<rawString>J.F. Marcotorchino and P. Michaud. 1981. Heuristic approach of the similarity aggregation problem. Methods of operation research, 43:395–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Michaud</author>
<author>J F Marcotorchino</author>
</authors>
<title>Optimisation en analyse de donn´ees relationnelles. In Data Analysis and informatics. North</title>
<date>1980</date>
<location>Holland Amsterdam.</location>
<contexts>
<context position="12012" citStr="Michaud and Marcotorchino, 1980" startWordPosition="2040" endWordPosition="2043">up cliques of NEs which are mutually highly similar. The clusters of cliques which contain a NE allow to find the different possible annotations of this NE. This clustering technique must be able to construct “pure” clusters in order to have precise annotations. In that case, it is desirable to avoid fixing the number of clusters. That’s the reason why we propose to use the Relational Analysis approach described below. 2.4.1 The Relational Analysis approach We propose to apply the Relational Analysis approach (RA) which is a clustering model that doesn’t require to fix the number of clusters (Michaud and Marcotorchino, 1980), (B´ed´ecarrax and Warnesson, 1989). This approach takes as input a similarity matrix. In our context, since we want to cluster cliques of NEs, the corresponding similarity matrix S between cliques is given by the dot products matrix taken from T: S = T · T0. The general term of this similarity matrix is: S(clik, clik0) = Skk0 = (clik, clik0). Then, we want to maximize the following clustering function: 0(S, X) = (6) E(k00,k000)∈S+ Sk00k000 Skk0 − |S+| � N., � contkk0 where S+ = {(clik, clik0) : Skk0 &gt; 0}. In other words, clik and clik0 have more chances to be in the same cluster providing th</context>
</contexts>
<marker>Michaud, Marcotorchino, 1980</marker>
<rawString>P. Michaud and J.F. Marcotorchino. 1980. Optimisation en analyse de donn´ees relationnelles. In Data Analysis and informatics. North Holland Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A survey of Named Entity Recognition and Classification. Lingvisticae Investigationes,</title>
<date>2007</date>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="17431" citStr="Nadeau and Sekine, 2007" startWordPosition="3013" endWordPosition="3016">compute a default cluster assignment matrix Adef, which does not depend on the local context: Adef(ei) = clu* where: clu* = Argmax{clul:clulE){clik:clikE)ei}}|clik|. In other words, clu* is the cluster containing the biggest clique clik containing ei. 2.5.2 Clusters annotation So far, the different steps that we have introduced were unsupervised. In this paragraph, our aim is to give a correct annotation to each cluster (hence, to all NEs in this cluster). To this end, we need some annotation seeds and we propose two different semi-supervised approaches (regarding the classification given in (Nadeau and Sekine, 2007)). The first one is the manual annotation of some clusters. The second one proposes an automatic cluster annotation and assumes that we have some NEs that are already annotated. Manual annotation of clusters This method is fastidious but it is the best way to match the corpus data with a specific guidelines for annotating NEs. It also allows to identify new types of annotation. We used the ACE2007 guidelines for manually annotating each cluster. However, our CBC system leads to a high number of clusters of cliques and we can’t annotate each of them. Fortunately, it also leads to a distribution</context>
<context position="28760" citStr="Nadeau and Sekine, 2007" startWordPosition="4991" endWordPosition="4994">none&gt;. Finally, in the sentence “Matthew, also a favorite to win in his fifth and final appearance, was stunningly eliminated during the semifinal round Friday when he misspelled “secernent”.”, the baseline hybrid system Stanford + XIP didn’t give any annotation to “Matthew” whereas Stanford + XIP + CBC A allowed to give the annotation &lt;person&gt;. 4 Related works Many previous works exist in NEs recognition and classification. However, most of them do not build a NEs resource but exploit external gazetteers (Bunescu and Pasca, 2006), (Cucerzan, 2007). A recent overview of the field is given in (Nadeau and Sekine, 2007). According to this paper, we can classify our method in the category of semi-supervised approaches. Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2). However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context. Regarding this aspect, </context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>D. Nadeau and S. Sekine. 2007. A survey of Named Entity Recognition and Classification. Lingvisticae Investigationes, 30(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Ngonga Ngomo</author>
</authors>
<title>Signum a graph algorithm for terminology extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of CICLING 2008,</booktitle>
<location>Haifa,</location>
<contexts>
<context position="29400" citStr="Ngomo, 2008" startWordPosition="5102" endWordPosition="5103">e can classify our method in the category of semi-supervised approaches. Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2). However, the particularity of our method concerns the clustering of 9Except for XIP+CBC A in line 2 where the precision is slightly lower than XIP’s one. cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context. Regarding this aspect, (Lin and Pantel, 2001) and (Ngomo, 2008) also use a clique computation step and a clique merging method. However, they do not deal with ambiguity of lexical units nor with NEs. This means that, in their system, a lexical unit can be in only one merged clique. From a methodological point of view, our proposal is also close to (Ehrmann and Jacquet, 2007) as the latter proposes a system for NEs finegrained annotation, which is also corpus dependent. However, in the present paper we use all syntactic relations for measuring the similarity between NEs whereas in the previous mentioned work, only specific syntactic relations were exploite</context>
</contexts>
<marker>Ngomo, 2008</marker>
<rawString>A. C. Ngonga Ngomo. 2008. Signum a graph algorithm for terminology extraction. In Proceedings of CICLING 2008, Haifa, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of CIKM 2004,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="30221" citStr="Pasca, 2004" startWordPosition="5246" endWordPosition="5247">ed clique. From a methodological point of view, our proposal is also close to (Ehrmann and Jacquet, 2007) as the latter proposes a system for NEs finegrained annotation, which is also corpus dependent. However, in the present paper we use all syntactic relations for measuring the similarity between NEs whereas in the previous mentioned work, only specific syntactic relations were exploited. Moreover, we use clustering techniques for dealing with the issue related to over production of cliques. In this paper, we construct a NE resource from the corpus that we want to analyze. In that context, (Pasca, 2004) presents a lightly supervised method for acquiring NEs in arbitrary categories from unstructured text of Web documents. However, Pasca wants to improve web search whereas we aim at annotating specific NEs of an analyzed corpus. Besides, as we want to focus on corpus-specific NEs, our work is also related to (Shinyama and Sekine, 2004). In this work, the authors found a significant correlation between the similarity of the time series distribution of a word and the likelihood of being a NE. This result motivated our choice to test our approach on recent news articles rather than on well-known </context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>M. Pasca. 2004. Acquisition of categorized named entities for web search. In Proceedings of CIKM 2004, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ploux</author>
<author>B Victorri</author>
</authors>
<title>Construction d’espaces s´emantiques a` l’aide de dictionnaires de synonymes.</title>
<date>1998</date>
<journal>TAL,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="8147" citStr="Ploux and Victorri, 1998" startWordPosition="1350" endWordPosition="1353">ts for example, each NE and each context should appear more than 10 times in the corpus for being considered. D is the resulting (|NE |x |CT|) NE-Context matrix where ei : i = 1, ... , |NE |is a NE and cj : j = 1, ... , |CT |is a syntactic context. Then we have: D(ei, cj) = Nb. of occ. of cj associated to ei (1) 2.3 Cliques of NEs computation A clique in a graph is a set of pairwise adjacent nodes which is equivalent to a complete subgraph. A maximal clique is a clique that is not a subset of any other clique. Maximal cliques computation was already employed for semantic space representation (Ploux and Victorri, 1998). In this work, cliques of lexical units are used to represent a precise meaning. Similarly, we compute cliques of NEs in order to represent a precise annotation. For example, Oxford is an ambiguous NE but a clique such as &lt;Cambridge, Oxford, Edinburgh University, Edinburgh, Oxford University&gt; allows to focus on the specific annotation &lt;organization&gt; (see (Ehrmann and Jacquet, 2007) for similar use). Given the distributional space described in the previous paragraph, we use a probabilistic framework for computing similarities between NEs. The approach that we propose is inspired from the langu</context>
</contexts>
<marker>Ploux, Victorri, 1998</marker>
<rawString>S. Ploux and B. Victorri. 1998. Construction d’espaces s´emantiques a` l’aide de dictionnaires de synonymes. TAL, 39(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
</authors>
<title>Named Entity Discovery using comparable news articles.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<location>Geneva.</location>
<contexts>
<context position="22558" citStr="Shinyama and Sekine, 2004" startWordPosition="3895" endWordPosition="3898">ion provided by the NER system which gave the best precision. • If a NE occurrence is included in another one we only keep the biggest one and its annotation. For example, if Jacques Chirac is annotated &lt;person&gt; by one system and Chirac by &lt;person&gt; by the other system, then we only keep the first annotation. • If two NE occurrences are contiguous and have the same annotation, we merge the two NEs in one NE occurrence. 3 Experiments The system described in this paper rather target corpus-specific NE annotation. Therefore, our ex56 periments will deal with a corpus of recent news articles (see (Shinyama and Sekine, 2004) for motivations regarding our corpus choice) rather than well-known annotated corpora. Our corpus is constituted of news in English published on the web during two weeks in June 2008. This corpus is constituted of around 300,000 words (10Mb) which doesn’t represent a very large corpus. These texts were taken from various press sources and they involve different themes (sports, technology, ... ). We extracted randomly a subset of articles and manually annotated 916 NEs (in our experiments, we deal with three types of annotation namely &lt;person&gt;, &lt;organization&gt; and &lt;location&gt;). This subset const</context>
<context position="30558" citStr="Shinyama and Sekine, 2004" startWordPosition="5300" endWordPosition="5303">ious mentioned work, only specific syntactic relations were exploited. Moreover, we use clustering techniques for dealing with the issue related to over production of cliques. In this paper, we construct a NE resource from the corpus that we want to analyze. In that context, (Pasca, 2004) presents a lightly supervised method for acquiring NEs in arbitrary categories from unstructured text of Web documents. However, Pasca wants to improve web search whereas we aim at annotating specific NEs of an analyzed corpus. Besides, as we want to focus on corpus-specific NEs, our work is also related to (Shinyama and Sekine, 2004). In this work, the authors found a significant correlation between the similarity of the time series distribution of a word and the likelihood of being a NE. This result motivated our choice to test our approach on recent news articles rather than on well-known annotated corpora. 5 Conclusion We propose a system that allows to improve NE recognition. The core of this system is a cliquebased clustering method based upon a distributional approach. It allows to extract, analyze and discover highly relevant information for corpusspecific NEs annotation. As we have shown in our experiments, this s</context>
</contexts>
<marker>Shinyama, Sekine, 2004</marker>
<rawString>Y. Shinyama and S. Sekine. 2004. Named Entity Discovery using comparable news articles. In Proceedings of COLING 2004, Geneva.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>