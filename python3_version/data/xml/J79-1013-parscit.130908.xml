<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.574247">
American Journal of Computational Linguistics Microfiche 13
UNDERSTANDING
BY
</note>
<sectionHeader confidence="0.7946685" genericHeader="method">
CONCEPTUAL INFERENCE
CHUCK RIEGER
</sectionHeader>
<affiliation confidence="0.8444965">
Computer Science Department
University of Maryland
</affiliation>
<address confidence="0.344063333333333">
College Park
Copyright 1975
Association for Computational Linguistics
</address>
<email confidence="0.542373">
ABSTRACT
</email>
<bodyText confidence="0.978460634146342">
Any theory of languaga must also be a theory of inference
and memory. It does not appear to be possible to &amp;quot;understand&amp;quot;
even tae simplest of utterances in a contextually meaningful way
in a system in which language fails to interact with a language-
free memory and belief system, or in a system which, lacks a
spontaneous inference reflex.
People apply a tremendous amount of cognitive effort to
understanding the meaning content of language in context. Most
of this effort is of the form of spontaneous conceptual inferences
which occur in a language-independent meaning environment. I
have developed a theory of how humans process the meaning content
of utterances in context. The theory is called Conceptual Memory,
and has been implemented by a computer program whicn is designed
to accept as input analyzed Cnnceptual Dependency (Schank et a..)
meaning graphs, to generate many conceptual inferences as auto-
matic responses, then to identify points of contact among those
inferences in &amp;quot;infefence space&amp;quot;. Points of contact establish new
patnways through existing memory structures, and hence &amp;quot;knit&amp;quot;
each utterance in witn its surrounding context.
Sixteen classes of conceptual inference have been identified
and implemented, at least at the prototype level. Tnese classes
appear to be essential to all higher-level language comprehension
processes. Among them are causative/resultative (those which
predict cause and effect relations), motivational (those which
predict and describe actors&apos; intentions), enablement (those which
predict the surrounding context of actions), state-duration (those
which predict the fuzzy duration of various states in the world)
normative (those which assess the &amp;quot;normality&amp;quot; of a piece of
information - how unusual it is), and specification (those which
predict and fill .in missing conceptual information in a languagc-
communicated meaning graph).
Interactions of conceptual inference with the language
processes of (1) word sense promotion in context, and (2) ident-
.)
ification of referents to memory tokens are discussed. A theoreti-
cally important inference-reference &amp;quot;relaxation cycle&amp;quot; is identified.
and its solution discussed.
The theory provides the basis of a computationally effective
model of language comprehension at a deep conceptual level, and
should therefore be of interest to computational linguists,
psychologists and computer scientists alike.
</bodyText>
<title confidence="0.362657">
TABLE OF CONTENTS
1. The Need for a Theory
of Conceptual Memory and Inference 5
</title>
<sectionHeader confidence="0.9719704" genericHeader="method">
2. A Simple Example 7
3. Background 10
4. A Brief Overview of the Conceptual
Memory&apos;s Inference Control Structure 15
5. The Sixteen Theoretical Classes
</sectionHeader>
<table confidence="0.291822157894737">
of Conceptual Inference 20
5.1. CLASS 1: Specification Inferences 21
5.2. CLASSES 2 and 3: Resultative
and Causative Inferences 25
5.3. CLASS 4: Motivational Inferences 26
5.4. CLASS 5: Enabling Inferences 27
5.5. CLASS 6: Action Prediction Inferences 28
5.6. CLASS 7: Enablement Prediction
Inferences 30
5.7. CLASS 8: Function Inferences 31
5.8. CLASSES 9 and 10: Missing Enablement
and Intervention Inferences 33
5.9. CLASS 11: Knowledge Propagation
Inferences 34
5.10. CLASS 12: Normative Inferences 35
5.11. CLASS 13: State Duration Inferences 38
5.12. CLASSES 14 and 15: Featmre and
Situation Inferences 41
5.13. CLASS 16: Utterance Intent
</table>
<sectionHeader confidence="0.84417125" genericHeader="method">
Inferences 42
6. Summary of the Inference Component 43
7. The Inference-Reference Relaxation Cycle
in Conceptual Memory 44
8. Word Sense Promotions and Implicit Concept
Activation in the Conceptual Memory 48
9. Conclusion 51
APPENDIX A. Causal Chain Expansion
Computtr Example 52
APPENDIX E. Inference-Reference Relaxation Cycle
Computer Example 57
REFERENCES 62
</sectionHeader>
<page confidence="0.996132">
4
</page>
<sectionHeader confidence="0.299287" genericHeader="method">
1. The Need for a Theory of Conceptual Memory and Inference
</sectionHeader>
<bodyText confidence="0.968877818181818">
Research in natural language over the past twenty years has
been focussed primarily on processes relating to the analysis of
individual sentences (parsing). Most of the early work was devoted
to syntax. Recently, however, there has been a considerable
thrust in the areas of semantic, and importantly, conceptual
analysis (see (R2), (M1), (Si) and (Cl) for example). Whereas a
syntactic analysis elucidates a sentence&apos;s surface syntactic
structure, typically by producing some type of phrase-structure
parse tree, conceptual analysis elucidates a sentence&apos;s meaning
(the &amp;quot;oicture&amp;quot; it produces), typically via production of an
interconnected network of concepts which specifies the interrela-
tionships among the cohcepts referenced by the words of the
sentence. On the one hand, syntactic sentence analysis can more
often than not be performed &amp;quot;locally&amp;quot; that is, on single
sentences, disregarding any sort of global context; and it is
reasonably clear that syntax has generally very little to do
with the meaning of the thoughts it expresses. Hence, although
syntax is an important link in the understanding chain, it is
little more than an abstract system of encoding which does not
for the most part relate in any meaningful way to the information
it encodes. On the other hand, conceptual sentence analysis, by
its very definition, is forced into, the realm of geneLai WOLJU
knowledge; a conceptual analyzer&apos;s &amp;quot;syntax&amp;quot; is the set of rules
which can produce the range of all &amp;quot;reasonable&amp;quot; events that
might occur in the real world. Hence, in order to parse concep-
tually, the conceptual, analyzer must interact with a repository
of world knowledge and world knowledge handlers (inferential
processes). This need for such an analyzer-accessible world
knowledge repository has provided part Df the motivation for
the development of the following theory of conceptual inference
and memory
however, the production of a conceptual network from an
isolated sentence is only the first step in the understanding
</bodyText>
<page confidence="0.818758">
5,
</page>
<bodyText confidence="0.977877952380952">
process. After this first step, the real question is: what
happens to this conceptual network after it has been produced
by the analyzer? That is, if we regard the conceptual analyzer
as a specialized component of a larger memory, then the allocation
of memory resources in reaction to each sentence follows the
pattern: (phase 1) get the sentence into a form which is under-
standable, then (phase 2) understand it It is a desire to
characterize phase 2 which has served as the primary motivation
for developing this theory of memory and inference. In this sense,
he theory is intended to be a charting-out of the kinds of pro-
cesses which must surely occur each time a sentence&apos;s conceptual
network enters the system. Although it is not intended to be an
adequate or verifiable model of how these processes might actually
occur in humans, the theory described in this paper has never-
theless been implemented as a computer model ander PDP-10
Stanford 1.6 LISP. While the implementation follows as best it
can an intuitively correct approach to the various processes
described, the main intent of the underlying theory is to propose
a set of memory processes which, taken together, could behave
in a manner similar to the way a human behaves when he &amp;quot;understands
language&amp;quot;.
</bodyText>
<sectionHeader confidence="0.916645" genericHeader="method">
2. A Simple Example
</sectionHeader>
<bodyText confidence="0.993825722222222">
The attentive human mind is a volatile processor. My conjec-
ture is that information simply cannot be put into it in a passive
way; there are very primitive inference reflexes in its logical
architecture which each input meaning stimulus triggers. I will
call these primitive inference reflexes &amp;quot;conceptual inferences&amp;quot;,
and regard them as one class of subconscious memory process. I
say &amp;quot;subconscious&amp;quot; because the concern is with a relatively low-
level stratum of &amp;quot;higher-level cognition&amp;quot;, particularly insofar
as a human applies it to the understanding of language-communicated
information. The eventual goal is to synthesize in an artificial
system the rOugh flow of information which occurs in any normal
adult response to a meaningfully-connected sequence of natural
language utterances. This of course is a rather ambitious project.
In this paper I will discuss some important classes of conceptual
inference and their relation to a specific formalism I have
developed (R1).
Let me first attempt, by a fairly ludicrous example, to
convince you (1) that your mind is more than a simple receptacle
for data, and (2) that you often have little control over the
thoughts that pop up in response to something you perceive. Read
the following sentence, pretending you were in the midst of an
absorbing novel:
EARLIER THAT EVENING, MARY SAID SHE HAD KILLED HERSELF.
One of two things probably occurred: either you chose as referent
of &amp;quot;herself&amp;quot; -some person other than Mary (in which ease every-
thing works out fine), or (as many people seem to do) you first
identified &amp;quot;herself&amp;quot; as a reference to Mary. In this case,
something undoubtedly seemed awry: you realized either that your
choice of referent was erroneous, that the, sentence was part of
some unspecified &amp;quot;weird&amp;quot; context, or that there was simply an
out-and-out contradiction. Of course, all three interpretations
are unusual in some sense because of a &amp;quot;patently obvious&amp;quot;
contradiction in the picture this utterance elicits. The sentence
is syntactically arid semantically impeccable; only when we &amp;quot;think
about it&amp;quot; does the big fog horn upstairs alert:us to the implicit
contradiction:
</bodyText>
<sectionHeader confidence="0.884021333333333" genericHeader="method">
MARY SPEAK AT TIME T
1 enablement inference
MARY ALIVE AT TIME T
I.3 contradiction
MARY NOT IVE- AT TIME T
Istate-duration inference
MARY CEASES BEING ALIVE AT TIME T-d
1 resultative inference
MARY KILLS HERSELF AT TIME T-d
</sectionHeader>
<bodyText confidence="0.995878">
Here is the argument: before reading the sentence, you
probably had no suspicion that what you were about to read contained
an implicit contradiction. Yet you probably discovered that
contradiction effortlessly: Could there have been any a priori
&amp;quot;goal direction&amp;quot; to the three simple inferences above? My
conclusion is that there could not have been. If we view tne
mind as a multi-dimensional &amp;quot;inference space&amp;quot;, then each incoming
thought produces a spherical burst of activity about the point
where it lands in this space (the place where the conceptual
network representing it is stored). The horizon of this sphere
consists of an advancing wavefront of inferences - spontaneous
probes Which are sent out from the point. Most will
lose momentum and eventually atrophy; but a few will conjoin with
inferences on the horizons of other points&apos; spheres. The sum of
these &amp;quot;points of contact- represents tne integration of the
thought into the existing fabric of the memory in that each point
of contact establishes a new pathway between the new thought and
existing knowledge (or perhaps among several existing pieces of
knowledge). This to me is a pleasing memory paradigm, and there
is a tempting analogy to be drawn with neurons and actual physical
wavefronts as proposed years ago by researchers such as John
Eccles (El). The drawing of this analogy is, however, left for
the pleasure of you, the reader.
This killing example was of course more pedagogical than
serious, since it is a loaded atterance involving rather black
and white, almost trivial interences. But it suggests a powerful
low-level mechanics for general language comprehension. Later,
I will refer you to an example which shows how the implemented
model, called MEMORY and described in (R1), reacts to the more
interesting example MARY KISSED JOHN BECAUSE HE HIT BILL, which
is,perceived in a particular context. It does so in a way that
integrates the thought into the frameWork of that context and
which results in a &amp;quot;causal chain expansion&amp;quot; involving six
probanilistic inferences.
</bodyText>
<page confidence="0.996473">
9
</page>
<sectionHeader confidence="0.947559" genericHeader="method">
3. Background
</sectionHeader>
<bodyText confidence="0.952537827586207">
Central to this theory are sixteen classes of spontaneous
conceptual inferences. These classes are abstract enough to be
divorced from any particular meaning representation formalism.
However, since they were developed concurrently with a larger
model of conceptual memory (R1) which is functionally a part of
a language comprehension system involving a conceptual analyzer
and generator (MARGIE (S3)), it will help make the following
presentation more concrete if we first have a brief look at the
operation and goals of the conceptual memory in the context of
the complete language comprehension system.
The memory adopts Schank et al.&apos;s theory (S1,S2) of Conceptual
Dependency (CD) as its basis for representation. CD is a theory
of meaning representation which posits the existence of a small
number of primitive actions (eleven are used by the conceptual
memory), a number of primitive states, and a small set of
connectives (links) which can join the actions and states
together into conceptual graphs (networks). Typical-of the links
are:
tne ACTOR-ACTION &amp;quot;main&amp;quot; link &lt;
the ACTIONOBJECT link 4111E.I.m...1■•
the CAUSAL link lit
the DIRECTIVE link
and the STATECHANGE link
Each primitive action has a case framework which defines conceptual
slots which must be filled whenever the act appears in a conceptual
graph. There are in addition TIME, LOCation and INSTrumental
links, and these, as are all conceptual cases, are obligatory,
even if they must be inferentially filled in by the conceptual
memory (CM). Figure 1 illustrates the CD representation of the
</bodyText>
<page confidence="0.996998">
10
</page>
<bodyText confidence="0.994396588235294">
sentence MARY XISSED JOHN BECAUSE HE (JOHN) HIT BILL. That
conceptual graph is read as follows: John propelled some unspec-
ified object X from himself toward Bill, causing X to come into
physical contact with Bill, and this entire event cause Mary to
do something which resulted in her lips being in physical contact
with John! Furthermore, the entire event occurred sometime in
the past. Chapter 2 of (R1) contains a fairly complete overview
of the CD representation.
Assuming the conceptual analyzer (see (R2)) has constructed,
in consultation with the CM, a conceptual graph of the sort
typified by &apos;Figure 1, the first step for the CM is to begin
&amp;quot;integrating&amp;quot; it into some internal memory structure which is more
amenable to the kinds of active inference Manipulations the CM
wants to perform. This initial integration occurs in three stages.
First is an initial attempt to replace the symbols (JOHN, MARY,
BILL, X, etc.) by pointers to actual memory concelots and tokens
of concepts. Each concept and token in the CM is represented by
a unique LISP atom (such as C0347) which itself bears no intrinsic
meaning. Instead, the essence of the concept or token is captured
in a set of features associated with the symbol. Thus, for
instance, an internal memory token with no features is simply
&amp;quot;something&amp;quot; if it must be expressed by language, whereas the
token illustrated in Figure 2 would represent part of our
knowledge about Bill&apos;s friend Mary Smith, a female human who
owns, a red Edsel, lives at 222 Avenue St., is 26 years old, and
so forth. This set of features is called 00948&apos;s occurrence set,
and is in the implementation merely a set of pointers to all
other memory structures in which C0948 occurs. The process of
referent identification will attempt to isolate one token, or
second best, a set af candidate tokens for each concept symbol
in the incoming graph by means of a feature-intersedting algorithm
described in (R1).
Reference identification is the first stage of the initial
integration of the graph into internal memory structures. The
</bodyText>
<page confidence="0.995839">
11
</page>
<figure confidence="0.997821333333333">
(c)
JOHN &lt;..=&gt; PROPEL X?
ID 1--4 BILL
JOHN
(a)
MARY &lt;==.&gt; DO
(b)
val
va I
X? &lt;BUM&gt; PHYSCONT 4.-- BILL LrPs &lt;zes&gt; PHYSCONT 4----- JOHN
(d)
part
</figure>
<figureCaption confidence="0.553002">
FIGURE 1
</figureCaption>
<bodyText confidence="0.916600666666667">
Conceptual dependency representation
of the sen-Cence &amp;quot;Mary kissed John
because he hit Bill.&amp;quot;
</bodyText>
<sectionHeader confidence="0.952101571428572" genericHeader="method">
00948: (ISA # #PERSON)
(SEX # #FEMALE)
(NAME # MARY)
(SURNAME # SMITH)
(OWNS # C0713)
(RESIDENCE # C08.46)
(TSTART # C0654)
</sectionHeader>
<listItem confidence="0.447655">
• •
</listItem>
<bodyText confidence="0.996887">
(00718 is the token representing the Edsel which Ilary owns,
C0846 is the token for Mary&apos;s place of residence,. C0654 is a
time token with a numeric :value on the CM&apos;s time scale rep-
resenting Mary&apos;s time of birth in 1948)
</bodyText>
<sectionHeader confidence="0.805759" genericHeader="method">
FIGURE 2
</sectionHeader>
<bodyText confidence="0.988869666666667">
The memory&apos; token and its
Occurrence set which rep-
re&apos;sept Mary ,Smith.
</bodyText>
<sectionHeader confidence="0.27072" genericHeader="method">
MAIL
</sectionHeader>
<page confidence="0.987472">
12
</page>
<bodyText confidence="0.999218677419355">
second and third seages are (2) the isolation of subgraphs which
will form the beginning inference,queue (input to tne spon-
taneous inference component), and (3) the storage of the graph
dependency links themselves as pointers in the memory.Just as for
simple concepts and tokens, composite structures (actiQns and
states) are stored under a unique internal symbol, and this symbol
may also have an occurrence set. In addition, there are several
other properties associated with each composite structure S:
the recency of S&apos;s activation by explicit reference (RECENCY),
the recency of S&apos;s activation by implicit (inferential) reference
(TOUCHED), the degree to which S is held to be true (STRENGTH),
a list of other composite structures fnom which S arose in the
memory - its inferential antecedants-(REASONS), and a list of
other composite structures in whose generation Splayed a role
as antecedant (OFFSPRING). RECENCY and TOUCHED are also prop-
erties of concepts and tokens, and are used in the referent
identification process.
Figure 3 shows the memory structures which result from the
conceptual graph of Figure 1 after the initial integration. The
net result of the initial integration is a set of starting memory
structures,(actuallv, a list of pointers to their symbols, such
as (C2496 C2301 C2207)). Each of these structures references
memory concepts tokens and other composite structures.
Regarding the referent identification process, for those
concepts. and tokens which could not.be uhiquely identified, new
temporary tokens will have beeh created, each having as its
initial occurrence set a list of what is khown about the entity
so far.
After the initial integration, the inference component is
applied simultaneously to each memory structure (&amp;quot;point in
inference space&amp;quot;) on the starting inference queue.
</bodyText>
<page confidence="0.995511">
13
</page>
<figure confidence="0.9901345">
# (ISA # #PERSON)
V&apos;
(SEX # *MALE)
NAME # JOHN)
(CAUSE
(PROPEL * * * *)
# (ISA # #PHYSOBJ)
*)
(PHYSCONT * *
(UNSPECIFIED #) # (ISA # #PMSON)
(4
1 (SEX # *MALE)
----&amp;quot;------_ ,-----.- (NAME # BILL)
(DO
(CAUSE * *)
(ISA #,.*PERSON)
(SEX # #FEMALE)
(NAME-# MARY)
(TIME
(PHYSCONT * *)
c/
# (ISA # *LIPS)
cv (PART # *)
# (ISA # #TIME)
(BEFORE # N)
(N is the numeric &amp;quot;now&amp;quot;
</figure>
<figureCaption confidence="0.491214">
on the CM&apos;s time scale)
The internal memory structurps resulting from the
CD graph of Fig. 1.
Figure 3
</figureCaption>
<page confidence="0.888451">
14
</page>
<sectionHeader confidence="0.4919325" genericHeader="method">
4. A Brief Overview ot the Conce.tual Memor &apos;s Inference
Control Structure.
</sectionHeader>
<bodyText confidence="0.999970814814815">
The control structure which implements the CM inference reflex is a
breadth-first monitor whose queue at any moment is a list of pointers to
dependehcy structures which have arisen by inference from the beginning
structures isolated during the initial integration. It is the inference
monitor&apos;s task to examine each dependency structure on the queue in turn,
isolate its predicate, prepare its arguments in a standard format, collect
several time aspdcts from the structure&apos;s occurrence set, then call the
inference molecule associated with the predicate, passing along the argu-
ments and time information.
All inferential knowledge in the CM is contained in inference molecules,
which lie in one-one correspondence with conceptual predicates. An inference
molecule is a structured LISP program which can perform arbitrary discrimina-
tion testis on a relevant dependency structure&apos;s features and features of all
involved concepts and tokens, and which-can call on specialist programs to
carry out standard test information retrieval functions. (&amp;quot;CAUSER&amp;quot; is an
example of such :4 specialist. It will scan back causal sequences from struc-
ture S until it locates a volitional actiOn, then it returns the actor of
that action as the primary causing agent of S ) Inference molecules are
hence multiple-response discrimination Petworks whose responses are concep-
tual inferences (of the various theoretical types to be described) which can
be made from the dependency structure. Each potential inference within the
inference molecule is called an inference atom.
The contribution of an inference atom which has been found applicable
to the dependency structure reports eight pieces of information to a com-
ponent of the monitor called the structure generator, whose job it is to
embody each new inference in a memory structure. These.eight pieces of in-
formation are the following:
</bodyText>
<listItem confidence="0.990943833333333">
1. a unique mnemonic which indicates to *hich of the 16
theoretical classes the new inference belongs (this
mnemonic is associated with the new structure only
temporarily on the inference queue for subsequent
control purposes)
2. the &amp;quot;reference name&amp;quot; of the generating inference atom
</listItem>
<page confidence="0.978912">
15
</page>
<bodyText confidence="0.939866">
(each atom has a unique name which is associated with
the new memory structure for control purposes)
3, the dependency structure (a predicate which binds to-
gether several pointers to concepts, tokens and other
structures), which is the substance of the new inference
</bodyText>
<listItem confidence="0.8110568">
4, a detault &amp;quot;significance factor&amp;quot; which is a rough, ad hoc
measure of the inference&apos;s probable relative significance
(this is used only if a more sophisticated process, to be
described, fails)
5, a REASONS list, which is a list of all other sttuctures
</listItem>
<bodyText confidence="0.8558124">
in the CM which were tested. by the discrimination net
leading up to this inference atom. tvery dependency
structure has a REASONS list recording how the struc-
ture arose, and the REASONS list plays a vital role in
the generation of certain types of inference
</bodyText>
<listItem confidence="0.833613">
6. a &amp;quot;propagation strength factor&amp;quot; which, when multiplied
</listItem>
<bodyText confidence="0.898312">
by the STRENGTIls (degree of belief) of all structures
on the REASONS list, produces the STRENGTH of the new
inference. Mere is a need for better heuristics here
incidentally -- see (Z1) for instance.)
</bodyText>
<sectionHeader confidence="0.41708" genericHeader="method">
7 a list of modifying structures (typicarly time aspects)
</sectionHeader>
<bodyText confidence="0.982402428571429">
which become the new inferred structure&apos;s initial occur-
rence set
8, propagation and strength factors for each modifying struc
tute Figure 4 illustrates the small implemented.
NEGCHANGL (something undergoes a negative change
on some scale) inference molecule. It is included to
communicate the gestalt rather than correct specifics at
this early stage of development.
The two other main components of the inference monitor are the eval-
uator and the structure merger. It is the function of the evaluator to
detect exact and fuzzy contradictions and confirmations (points of contact)
between eacn new inference as it arises and existing memory dependency struc-
tures. Because &amp;quot;fuzziness&amp;quot; in the matching process iliTplieF, access to a vast
number of heuristics (to illuStrate: would it be more like our friend. the
</bodyText>
<page confidence="0.966469">
16
</page>
<figure confidence="0.871771225">
(1PROG NEGCHANGE (UN PE SC) (X1 X2) (
1COND ( (EVENT UN)
(COMO ( (F1 (15A PE egPERSON))
(IR oNE6CHANGE1
(eWANT PE (GU (ePOSCHANGE PE &apos;SC))) ...PEOPLE OFTEN WANT TO BETTER
(0.95 1.0 (CAR UN).) -.THEMSELVES AFTER SOME NEGCHANGE
(TS e* (11 UN))
(1.0 (CAR UN)))
(COND (AND (SETO X1 (F1 (e*MFEEL* e egNEGEMOT1ON PE)))
(SETO X2 !GLOBALFIND).)
(IR eNEGCHANGE2 ...PERSON GETS HAPPY WHEN ENEMY
(ePOSCHANGE X1 egJOY) NSUFFERS NEGCHANGE
(0.9 1.0.(CAR UN) X2)
(eTIME e* (TI UN))
(1.0 (CAR &apos;UN)))
)
(COND ( (AND (SETO X1 (CAUSER (CAR UN)))
(NOT (EQ (CAR X)1 (a2 (CDR X1)))))
(IR eNEGCHAN,GE3
(e*MFEEN,PE egNEGEMOTIONf(CAR X1)) .PEOPLE (1ON&apos;T LIKE
(0.95 1.0 (CAR UN) (CDR X1)) ...OTHERS WHO HURT THEM
)
)
( (HASPRP PE (eISA PE OPHYSOBJ))
(COND ( (AND (SETO X1 (F1 (e*OWN* PE )))
(SE1Q X2 (CAUSER (CAR UNI))
(NOT (EQ X1 (CAR C2)))
(IR eNEGCHAN6E4
(e*MFEEL* X1 e#NEGEMOTION (CAR X2)) &apos;,IF X DAMAGES Y&apos;S PROPERT
(0.85 1.0 (CAR UN) X1 (CDR X2)) NTHEN X MIGHT FEEL ANGER
(TS e* (11 UN)) ...TOWARD Y
(1.0 (CAR UN)))
)
)
An inference molecule used by the current program.
FIGURE 4
The NEGCHANGE inference molecule.
17
(eTS e* (TI UN))
(1.0 (CAR UN)))
</figure>
<bodyText confidence="0.999700083333333">
lawyer or our friend the carpenter to own a radial arm saw?),
the evaluators delegates most of the matching responsibility
to programs - again organized by conceptual predicates - called
normality molecules (&amp;quot;N-molecules&amp;quot;). N-molecules, which will
be discus-sed more later, can apply detailed heuristics to ferret
out fuzzy confirmations and contradictions. As I will describe,
N-molecules also implement one class of conceptual inference
Confirmations&apos; and contradictions discovered by the evaluator
are noted on special lists which serve as sources for possible
subsequent responses by the CM. In addition, confirmations lead.
to invocation of the structure merger, which physically replaces
the two matching structures by one new aggregate structure, and
thereby knits together two lines of inference. 1s events go, this
is one of the most exciting in the CM.
Inference cutoff occurs when the product of .an inference&apos;s
STRENGTH (likelihood) and its significance factor falls below
a threshold (0.25). This ultimately restricts the raoius of each
sphere in the inference space, and in the current model, the
threshold is set low to allow considerable expansion.
Figure 5 depicts the overall strategy of the inference
monitor. (R1) contains a fuller account of the inference control
structure, whose description will be terminated at this point.
Enter... sixteen theoretical classes of conceptual inference
whichLfuel this inference reflex.
</bodyText>
<page confidence="0.987058">
18
</page>
<figure confidence="0.9898922">
next level becomes
new !NEW1NFS
pcan
!NEWINFS: ( * * * * * * )
INFERENCE
MOLECULE&apos;: ..- predicate
!INFS
STRUCTURE
GENERATOR
====,&gt; ( * * * ,* * * )
iiu
EVALUATOR
* * * * * * * )
. 1 1 1 1 6&amp;quot;
cut off inferences
REORDERER ==&gt; ( * * )
•
i&apos;fITI II TEd7RIFIFs
1 =======F..-=&gt; ( * * * * * )
N-MOLECULES
</figure>
<sectionHeader confidence="0.731084" genericHeader="method">
FIGURE
</sectionHeader>
<bodyText confidence="0.477763">
The inference monitor.
</bodyText>
<sectionHeader confidence="0.531997" genericHeader="method">
5. The Sixteen Theoretical Classes of Cohceptual Inference.
</sectionHeader>
<bodyText confidence="0.999641545454546">
It is phenomenological that most of the human language experience
focuses on actions, their intended and/or resulting states, and the causal-
ity and enabling states which surround them. &apos;There seems &apos;to be an ines-
capable core of notions related to actions, causation and enablement which
almost anyone who introspects long enough&apos; will independently discover. In
his &amp;quot;Cold Warrior&amp;quot; model; Abelson (LA1) Was perhaps the first to attempt a
computationally formal systematization of this fundamental core of Meaning
relations. It is of the utmost primacy in his system,. which models the
political ideologies and behavior patterns of a rabid right-winger, to dis-
cover and relate the underlying purposes, enablement and causality surround-
ing events in some hypothetical international scenerio or crisis. Again, in
Schank t al&apos;s CD theory, the same emphasis arose more or less independently
in a system of meaning representation for everyday utterances: causality,
actions, state-changes and enablement were recurringsthemes. Not surprisingly
the same notions have emerged as central in my analysis of the inference re-
flex: over half of the-16 classes relate to this &amp;quot;action-intention-causality-
enablement-knowledgen complex.
In the following descriptions of these 16 classes, keep in mind that all
types of inference are applicable to every subcomponent of every utterance,
and that the CM is essentially a parallel simulation. Also bear in mind that
the inference evaluator is constantly performing matching operations on each
new inference in order to detect interesting&apos; interactions between inference
spheres. It should also be emphasized. that conceptual inferences are prob-
abilistic and predictive in nature, and that by making them in apparently
wasteful quantities, the CM is not seeking one. result or truth. Rather,
inferential expansion.is an endeavor which broadens&apos;each piece of information
into its surrounding spectrum to fill out the information-rich situation to
which the information-lean utterance might refer. The CM&apos;s.gropings will
resemble more closely the solution of a jigsaw puzzle than the mare goal-
directed solution of a crossword puzzle.
The following discussions can only sketch the main ideas
behind each inference class. See (R1) for a more comprehensive.
treatment.
</bodyText>
<page confidence="0.969265">
20
</page>
<table confidence="0.944770555555556">
5.1 CLASS 1: SPECIFICATION XNFERENCES
PRINCIPLE: The CM must be able to identify and attempt.to fill in
each missing slot of an incoming conceptual graph.
EXAMPLES: &amp;quot;Johmwas driving home from work. He hit Bill&apos;s cat.
(inference) It was a car which John propelled into
the cat.
**John bought a chalk line.
(inference) It was probably from a hardware store
that John bought the chalk line.
</table>
<sectionHeader confidence="0.78576" genericHeader="method">
DISCUSSION:
</sectionHeader>
<bodyText confidence="0.99840295">
Our use of language presupposes a tremendous underlying kffowledge about
the world. Because of this, even in, say, the most explicit techniLal lutt-
ing, certain assumptions are made by the writur (speaker) about the compre-
hender&apos;s knowledge -- that he can fill in the plethora of .detail surrounding
each thought. In the CM, this corresponds to filling in all the missing con-
ceptual slots in a graph.
The utility of such a process is twofold&apos;. first, CM failures to specify
a missing concept can serve as a source of requests, for more information (or
goals to seek out that information by CM actions if. CM is controlling a ro-
bot). Second, by predictively completing the graph by application of genetal
pattern knowledge of the modeled world, novel relations among specific con-
cepts and tokens will arise, and these can lead to potentially significant
discoveries by other inferences.
To illustrate, a very common missing slot is the instrumental case.
We generally leave it to the imaginative powers of the hearer to surmise
the probable instrumental action by which some action occurred:
(husband to wife) I went to SEARS today.
(wife to husband), How? I had the car all day!
Here, wife fills in the instrumental slot as: &amp;quot;Husband drove a car to SEARS&amp;quot;
(clearly relying_on some specific heuristics,,such as the distaAce from their
</bodyText>
<page confidence="0.995872">
21
</page>
<bodyText confidence="0.9984335">
home to SEARS, etc.), and this led to her di&amp;quot;scovery of a contradiction.
That she may have been premature in the specification (and&apos;shad later to
undo it) is of secondary importance to the phenomenon that she did so
spontaneously.
In the CM.specification inferences, as all inferences, are implemented
in the form of structured programs which realize discrimination nets whose
terminal nodes are concepts and tokens rather-than inferences, as in general
inference molecules. These specification procedures are called specifier
molecules (&amp;quot;S-molecules&amp;quot;), and are quite similar to inference molecules.
Fig. 6 shows a small prototype of the PROPEL specifier molecule Which can
predictively fill in the missing object of a PROPEL.attion, as in &amp;quot;John hit
Pete.&amp;quot; That particular &amp;quot;specifier atom&amp;quot; is sensitive to context along one
simple dimension if the actor is known to be grasping an object (this
prototype doesn&apos;t care wnetner it&apos;s a wet noodle or a bludgeon), at the time
of the action, the molecule will infer that it was the grasped object which
was propelled, as in &amp;quot;John picked up the flower pot. He hit Pete.&amp;quot; Other-
wise, the molecule will assume &amp;quot;hand of the actor&amp;quot;. This is ridiculously
oversimplified, tut it repreents a certain philoSophy I will digress a
moment to re-veal.
I, as many other people (see Wl, H1, Cl, for unstance), have come to
believe that passive data structures ,are fundamentally awkward for repre-
senting knowledge in any detail, partitularly for the purposes typified by
</bodyText>
<table confidence="0.959968777777778">
(SPROG *PROPEL* (UN V AC OB OF or) 4X1 X2 X3) (
(COND ( (NULL (CADR V))
(COND ( (AND 4SETO X] (C (4ISA e_ OHAND)
@PART e_ AC)))
(SETO X2 (F1 (*LOC* X1)))
(SETO X3 IGLOSALFIND))—
(SP V 2 X2 . (LIST X3))
)(
(SP V 2 X1 NIL)
)
. • (Other specifier atoms go here)
(RETURN V)
))
IThis is a simplified specifier
molecule containing &apos;just an object
speciflqr atom. (NULL (CADR V)) is
1 test for lack of object sbecificatio
1 If unspecifieo, the atom locates
</table>
<bodyText confidence="0.861425357142857">
&apos; the hand of the actors, assigning
it to Xl. It then checks to
see if anythjng is located in Xl.
If sonething is found, it is bound
to X2, and the LOC structure which
expresses this information is.
bound to X3. If nothin9 4s located
in the actor&apos;s hand, has hand
itself (X1) is inferred. The
(LIST X3) in the first SP call
is the list of REASONS (just.one
here) lustifying the specificatibn
Df the -object the actor was &apos;holding
as the object of the PROPEL.
</bodyText>
<sectionHeader confidence="0.821549" genericHeader="method">
FIGURE 6
</sectionHeader>
<bodyText confidence="0.985663">
The PROPEL specifier molecule.
this simple PROPEL example. The needs for &amp;quot;special.case heuristics&amp;quot; in
even such a inodest pperation as this quickly overtaRe one&apos;s prowess at
devising &amp;quot;declarative&amp;quot; memory, structures. Programs, on the other hand.,
are quick and to the point, quite flexible, and have_as much &amp;quot;aesthetic
potentialnas eyen the most elegant declarative structures.
A life-size procedure for this very narrow process of specifying the
missing object-of a PROPEL action would obviously reqtire-many more tests
for related contexts (&amp;quot;John was racing down. the hill on his bilce. He hit
Bill.&amp;quot;) But independent of the fidelity with which any given g-molecule
executes its task, there is a very important claim buried both here and
in the other inferential procedures in the 01. It is that there are cer-
tain central tasks in which the decision process must seek out the context,
rather than context seeking out the appropriate decision process. In other
words, much of inference capability requires specialists who Rmow a
priori exactly what dimensions of context could possibly affect the gener-
ation of every potential inference, and these specialists carry out active
probes to search, for those dimensions before any inference is generated.
I can imagine no &amp;quot;uniform context mechanism&amp;quot; which accounts for the human&apos;s
diverse ability to attend to the relevant and ignore the superfluous. &gt;-
conjecture i5 that the mechanism for contextual guidance of inference is
highly distributed throughout the memory rather than.centralized as a com-
ponent of the, memory&apos;s control structure.
</bodyText>
<page confidence="0.994287">
24
</page>
<subsectionHeader confidence="0.532024">
5.2 CLASSES 2 and 3: RESULTATIVE and CAUSATIVE INFERENCES
</subsectionHeader>
<bodyText confidence="0.2637145">
PRINCIPLE! If an action is perceived, its probable resulting states
should be inferred (RESULTATIVE). If u state is perceived,
the general nature of its probable causing action (or a
specific action, if possible) should be inferred (CAUSATIVE).
EXAMPLES: **11.Jary hit Pete with a rock.
(inference) Pete probably became hurt. (RESULTATIVE)
**Bill was angry at Mary.
(inference) Mary&apos;may have done something to Bill. (CAUSATIVE)
</bodyText>
<sectionHeader confidence="0.360795" genericHeader="method">
DISCUSSION.:
</sectionHeader>
<bodyText confidence="0.951329125">
These two classes Of inference embody the CH&apos;s ability to relate ac-
tions arid states in causal sequences relative to the Ws models of
causality. In addition to serving as the basis for MOTIVATIONAI, in-
ferences and contributing to the general expansjon process, CAUSATIVE
and RESULTATIVE inferences often achieve the rather exotic form of under-
standing I have termed &amp;quot;causal chain expansion.&amp;quot; It is this process which
makes explicit the oft-abbreviated statements of causality: language com-
municated predications ot causality must always (if only subconsciously)
be explained in terts of the comprehender&apos;s models of causality, and fail-
ures to do so signal a lack of understanding and form another source of CM
queries for more information. CaUsal expansion successes on the other hand
result in important intervening actions and states which draw out (&amp;quot;touch&amp;quot;)
surtounding context and serve as the basis for inferences in other cate-
gories. Appendix A contains the computer printout from MEMOM , tracing a
causal expansion for &amp;quot;Mary kissed John because he hit Bill&amp;quot; in a particular
context M-lidh makes the explanation plausible.
</bodyText>
<page confidence="0.983325">
25
</page>
<figure confidence="0.5132865">
5 . 3 CLASS 4: MOTIVATIONAL .INF ERENC ES
1RINCIPLE The desires (intentions) of an actor can frequently be
</figure>
<bodyText confidence="0.91917025">
inferred by analyzing the states (AESULTATIVE inferences)
which result from an action he executes. These WANT-STATE
patterns are essential to understanding and should be made
in abundance.
</bodyText>
<listItem confidence="0.908015285714286">
EXAMPLES: **John pointed out to Mary that she hadn-t done her chores.
(inference) Mary may have felt guilty. (RESULTATIVE)
(inference) John may have wanted Mary to feel guilty.
(MOTIVATIONAL)
**Andy blew on the hot &apos;meat.
(inference) Andy may have wanted the meat to .decrease
in temperature.
</listItem>
<sectionHeader confidence="0.600022" genericHeader="method">
DISCUSSION:
</sectionHeader>
<bodyText confidence="0.9947814">
Language is a dual system of communication in that it usually com-
municates both the actual, and, either explicitly or by inference, the
intentional; Where the intentions of actors (the set of stat?s.they de-
sire) are not explicitly communicated, they must be inferred as the
immediate causality of thea action. In the CM candidates for MOTIVATIONAL
1:
infereneesareneRESULTATIVEinfer&apos;encesMthe CM can produce from
i
anactionA:foreachRESULTATIVEinferenceR.which the CM could make
from A , it conjectures that perhaps the actor of A desired R.
Since the generation of MOTIVATIONAL inference is dependent upon the
results of another class of inference (in general, the actor could have
desired things causally removed by several inferences from the immediate
resultg of his action), the MOTIVATIONAL inference process is implemented
by a special proceuu-re POSTSCAN which is invok.ed between &amp;quot;passes&amp;quot; of the
main breadth-first monitor. These passes will be discussed more later.
Once generated, each MOTIVATIONAL inference will generally lead back-
ward, via CAUSATIVE inferences, into an entire causal chain which lead up
to the action. This chain will frequently connect in interesting ways with
chains working forward from other actions.
</bodyText>
<page confidence="0.984716">
26
</page>
<figure confidence="0.8786693">
5 . 4 CLASS ENABLING. IN4ERENCES
PRINCIPLE: Every action has a set of enabling conditions -- conditions
which must be met for the action to begin or proceed. The
CM needs a rich knowledge of these conditions (state), and
should infer suitable ones to surround each perceived action.
EXANPLES: **John saw Mary yesterday.
(inference) John and -Iary were in the same general location
sometime &apos;yesterday.
**Mary told Pete that John was at the store
(inference) Mary knew that John was at the store.
</figure>
<sectionHeader confidence="0.662892" genericHeader="method">
DISCUSSION:
</sectionHeader>
<bodyText confidence="0.998366625">
The example at the beginning of the paper contained a contradiction
which could be discovered only.by.making a very simple enabling inference
about the action of speaking (any action for that matter), namely that the
actor was alive at the time! Enabling inferences can fruitfully lead from
the known action through the enabling states to predications about other
actions the actor Might have performed in order to set up the enabling states
for the primary action. This Idea is closely related to the next class of
inference.
</bodyText>
<page confidence="0.995866">
27
</page>
<bodyText confidence="0.980645085714286">
5. 5 MAU 6: ACTION PREDICTION INFERENCES
Whenever some WANT STATE of a potential actor is known,
predictions about possible actions the actor might perform
to achieve the state should be attempted. These predic-
tions will provide potent potential points of contact
for subsequently perceived actions.
EXAMPLES: **John&apos;Wants some nails.
(inference) John might attempt to acquire some nails.
**Mary is furious at Rita.
(inference) Mary might do something to hurt Rita.
DISCUSSION:
Action prediction inferences serve the inverse role of MOTIVATIONAL
inferences, in that they work forward from a known WANT STATE pattern into
predictions about future actions which could produce the desired state.
Just as a MOTIVATIONAL inference relies upon RESULTATIVE inferences, an
ACTION PREDICTION inference relies upon CAUSATIVE Inferences which can be
generated from the state the potential actor desires. Because it is often
impossible to anticipate the specific causing action, ACTION PREDICTION
inferences typically will be &apos;more general expectancies for a class of pos-
sible actions. In the nails example above, the general expentandy is, sim-
ply that John may do something which normally causes a PTRANS-(in CD ter-
minology, a change of location of some object) of some nails from somewhere
to himself. Often the nature of the desired state is such that some specific
action can be predicted (&amp;quot;John is hungry... John will ingest food.&amp;quot;) By mak-
ing specific action predictions, a new crop of enabling inferences can be pre-
dicted (&amp;quot;John must be near food.&amp;quot;, etc.),.and those conditions which cannot be
assumed to be already satisfied can.serve as new WANT-STATEs of the actor.
Thus it is through MOTIVATIONAL, ACTION PREDICTION and ENABLING inferences
that the CM can model (predict) the problem-solving behavior of each actor.
Predicted actions which match up with subsequently perceived conceptual
input serve as a very real measure of the CM&apos;s success at piecing together con-
nected discourse and stories. I suspect in addition that ACTION PREDICTION
inferences will play a key role in the eventual solutions of the &amp;quot;contextual
guidance of inference&amp;quot; problem. Levy DI) ha S some interesting beginning
thoughts on this topic.
</bodyText>
<figureCaption confidence="0.931188">
Fig. 7 illustrates the ACTION PREDICTION inference cycle.
</figureCaption>
<page confidence="0.967853">
28
</page>
<figure confidence="0.976711675">
(WANT P rklY
(WANT P A2)
(WANT P A3)
1411
f •
(WANT ID *(4) -179
cancause relations
(causative fnferences
from S)
Inkri
&gt;
\f /
(WANT P *I A1E1
1:1 1:1 A1E2
■
(WANT P *).
(WANT P *)
%-
A3tr 1-A I T- I
_1 I A2
• I
1 A-3Z
A2E1
■1=1
A
ufi &amp;quot;•A 2
•
A2E3
infer
(WANT P AiEj)
for each AiEj
which cannot
be explicitly
found. or
assumed to be
true
3
What the action prediction inference process tries to doe&apos;
A.E. is enabling state j
1 3 for action.i.
</figure>
<figureCaption confidence="0.530842">
FIGURE 7&apos;
</figureCaption>
<bodyText confidence="0.835252">
The action synalaiction&apos;inferefire pr.ocess.
</bodyText>
<page confidence="0.938218">
2.9
</page>
<figure confidence="0.9151047">
5 . 6 CLASS 7: ENABLEMENT PREDICTION II‘tERENC ES .
PRINCIPLE: If ,a potential actor desires-a state which is a commbn
enabling condition for some specific action, then it
can be inferred that the %tor migiIt wish to execute
that action.
EXAMPLES: **Mary asked John to tarn on the light.
(inference) Mary probably wants to see something..
**Andr wants the meat to be cool.
(inference) Nndy migfit want to eat the meat.
)ISCUSSION:
</figure>
<bodyText confidence="0.960728388888889">
Inferences in this class arc, in a sense, the inverse of ENABLING
inferences, because they attempt to predict an action from an enabling
state known to be desired by a would-be actor. Whereas an ACTION PREDIC-
TION infereRce.predicts.a.possible future action_to fulfill the desired
state, enablement prediction draws out thn motivation of the desire for the
state by identifying a probable action the state 1%ould enable. Although
(as with ACTION PREDICTION inference) it will frequently happen that no
specific action can be anticipated (since most states could enable infinite-
ly many specific actions), it i5 nevertheless possible to&apos; form general pre-
dictions about the nature of (-restrictions on) the enabled &apos;action. If,
for example, John walks over to Mary, then a RESULTATIVE inference is that
he is near Mary, and a MOTIVATIONAL inference is that he wants to be. near
qARY At this point an ENABLEMENT PREDICTION inference can be made to repre-
sent the general class of interactions John might have in mind. This will
be of&apos;particular significance if, for instance, the CM knows already that
John had something to tell her, since then the inferred action pattern would
match quite well the action of verbal communication in which.the state of
spatial proximity plays a key enabling role.
</bodyText>
<page confidence="0.985352">
30
</page>
<figure confidence="0.973015642857143">
5 . 7 CLASS 8: FUNCTION INFERENCES
PRINCIPLL: Control over some physical obj.ect P is usually desired by
a potential actor becauSe jle is engaged. in an algorithm in
which P plays a role. The CM should attempt to infer
a probable action from its knowledge of P&apos;s normal func-
tion.
EXAMPLES: **Mary wants the book.
(inference) Mary probably wftnts to read the book.
**John wants a knife.,
(inferencej- John -prObably wants. to cut something with
the knife.
**Bill Tikes to pour sundaes down girls&apos; dresses.
Bill asked Pete to .hand him the sundae...
DISCUSSION:
</figure>
<bodyText confidence="0.920471785714286">
Function inferences form a very diverse, rather colorful subclass
of ENABLEMENT PREDICTION inference. The underlyihg principle is that
desire of`immediate control over an object is usually tantamount to a
desire .to use that objept in the norma,1 function of objects of that type,
or in some function vitith is peculiar to the object-and/or actor (third
example above). In the CM, normal functions of objects are stored as
(NKT X Y) patterns, as ip Fig. 8 for things that are printed matter.
Before applying NFCT patterns, the el firSt checks for unu-sual relations
involving the specific actor and specific object excluding paths which
include. the normal ISA relations between, say sundae and food). Thus, that
Bill is known to require sundaes for slightly different algorithms from most
people will be discovered and used&apos;in the prediction. The result of a
FUNCTION inference is always some predicted action, assumed to be part of
some.al2orithvin which the actor is engaged,
</bodyText>
<page confidence="0.999811">
31
</page>
<figure confidence="0.926026230769231">
(NFCT #PRINTEDMATTER.*)
17 d
.( I SA # #CONCEPTS)
(MLOC # #PR I NTEQMAT TER);
(MTRANS * * #PRINTEDMATTER *)
#. 3 &apos;0
(ISA # #PERSON) ( I SA. # #CP)
(PART # )K)
The memory structure which stores
the normal function of printed matter.
FIGURE 8
A &amp;quot;normal—function—of&amp;quot;
memory structure.
</figure>
<page confidence="0.914229">
3&apos;2
</page>
<note confidence="0.460833">
5.8 CLASSES 9 and 10: MISSING ENABLEMENT and
</note>
<sectionHeader confidence="0.947109" genericHeader="method">
INTERVENTION INFERENCES
</sectionHeader>
<bodyText confidence="0.941926303030303">
PRINCIPLE: If a would-be actor is known to have been unsuccessful
in achieving some action, it is often possible to infer
the absence of one of the action&apos;s enabling states CMISS-
ING BIABLEMENT). If a potential actor is known to desire
that some action cease, it&apos;can be predicted that he will
attempt VD remove one or more enabling states of the
action (INTERVENTION).
EXAMPLES: **Mary couldn&apos;t see the horses finish.
(inference) Something bloc_ked Mary&apos;s view. (MISSING
ENABLEMENT)
She. cursed the man in front of her...
**Mary saw that Baby Billy was running out into the street.
(inference) Mary will pick Billy off the ground (INTER-
VENTION)
She ran after him...
DISCUSSION:
Closely related to the other enabling inferences, these forms attempt
to apply lmowledge about enablement relations to infer the cause of an
action&apos;s failure (in the case of MUSSING ENABLEMENT), or to predict a WANT
NOT-STATE which can lead by action prediction inference to possible actions
of intervention on the part of the WANTer. In the second example above;
Mary (and the CM&apos; first Must realize (via RESULTATIVE inferences) the
potentially undesirable consequences‘ of Billy&apos;s running attion (i.e.,
possible NEGCHANGE for Billy) From this, the CM can retrace, loaate the
running action which could lead to such a NEGCHANGE, collect its enabling
states-, then coniecture that Mary might desire to annul one or more of them.
Among them tor instance would be that Billy&apos;s feet be jn intermittent PINS-
CONT with the ground. From the (WANT (NOT myscomr FEET GROUND))) struc-
ture, a subsequent ACTION PREDICTION inference can arise, predicting that
Mary &apos;might put an end to (PHYSCONT FEET GROUND). This _pill in turn requilea
her to be located near Billy, and that prediction will matCh the RESULTATIVE
inference made from her directed running (the next utterance input), knifing
the two thoughts together.
</bodyText>
<page confidence="0.997759">
33
</page>
<subsectionHeader confidence="0.468303">
5.9 CLASS 11: KNOWLEDGE PROPAGATION INFERENCES
</subsectionHeader>
<bodyText confidence="0.997941970588235">
PRINCIPLE: Based on what the CM knows an actor to know, it can often
infer other knowledge which must also be available to the
actor. Since most conceptual inferences involve the in-
tentions .of actors, this modeling of ,knowledge is crucial.
EXAMPLES: **John saw Mary beating Pete with a baseball bat.
(inherence) Joh4 probably knew that Pete was getting&apos;hUrt.
**Betty asked Bill for the aspitin.
(inference) Bill probably surmised that Betty wasnit feel-
ing well.
DISCUSSION:
Modeling the knowledge of potential actors is fundamentally difficult.
Yet it is essential, since most all intention/prediction-related inferences
must be based in part on guesses about what knowledge each actor has avail-
able to him at various times. The CM currently models others&apos; knowledge
by &amp;quot;introspecting&amp;quot; on its on assuming another person P has access to_
the same kinds of information as the CM, P might be expected, to make
some of the same inferences the CM does. Since the CM presertcs a logical
conneetlyity among all its inferred structures (by the REASONS and OFFSPRIY(
properties of each structure), after inferences of othef types have arisen
from some unit of information U, the CM can return, determine who knew the
original fact U, locate U&apos;s OFFSPRING (those other memory structures which
arose by inference from U)., then infer that p may also be aware of each
of the offspring. As with MOTIVATIONAL inferences (Which rely on the
RESULTATIVE.inferonces from a structure), KNOWLEDGE PROPAGATION inferences
are implemented in the procedure POSTSCAN- which runs after the initial
breadth-first inference expansion by the monitor.
Modeling others&apos; knowledge demands a rich knowledge of what is normal
in the world.(&amp;quot;dOes John Smith know that kissing is a sign.of affection?&amp;quot;).
In fact, all inferences must rely upon default assumptiont;. about normality,
since most of the EMPs .knowledge&apos; (and presumably a. Luman&apos;s) exists in the
form of general patterns, rather than specific relations among specific con-
cepts and tokens. The next olass of .inference implements my belief that
patterns, just as inferences, should be realized in &apos;the CM by active programs
rathbr than by passive declarative data structures.
</bodyText>
<page confidence="0.995588">
34
</page>
<note confidence="0.915385">
5,10 CLASS 12 : NORMATIVE INFERENCES
</note>
<tableCaption confidence="0.549657333333333">
PRINCIPLE: The QM must make heavy reliance, upon programs whIcaencoue
commonsense pattern informatiOn about the modeled world.
When the retrieval of a sought-after unit of inforfttion
fails, the relevant normality program should be executed
on (pattern applied to) that information to assess its
likelihood in the absence of explicit inforMation..
LXAMPLES:&apos; **Does John Smith own a book?
(inference) Probably so; middle&apos;-class business executives
normally own books.
</tableCaption>
<subsectionHeader confidence="0.682503">
**Was John Likely to have been asleep at 3 pm yesterdav2
</subsectionHeader>
<bodyText confidence="0.91520544">
(inference) Most likely.not, since he has a normal day-
time job, and yesterday was avorkday.
DISCUSSION:
There are several low-level information retrieval procedures in the
CM which search for explicit information units as directed by specific
inference molecules. Such searches are on the basis of form alone, and
successes-result in precise matches, while failures are total. If there
were no recourse for such failures, the CM would quickly grind to a halt,
being unable to make intelligent assumptions, There must be some more
Positive and flexible mechanism to.ameliorate&amp;quot;syntactio&amp;quot;,lookup failures.
the CM, this ability To Rake intelligent assumptions is implement
ed by having the low-level lookup procedures defer control to the appro-
priate normality molecule (N-molecule) which will perform systemmatic tests
organized in single-response discrimination nets, to the unlocatable in-
formation. The goal is to arrive at-a terminal node in the. net where a
real number between C and 1 is located. ii some sequence of tests
leads to spch a number, the N-molecule return.it as the assessed likeli-
hood (&amp;quot;compatibility&amp;quot;&apos; in fuzzy logic tetminology (Z1)] ®f X being true.
Although the test in the N-molecules are themselves discrete, they
result in the fuzzy. compatibility. The point of course is that the tests
can encode quite diverse and very specific heuristics peculiar to each small
domain of patterns; For instance, based on known (or N-molecule inferrable.--
one N-molecule can Gall upon&apos; others in its testing process!) features of
either Jobn or the hammer, we would suspect the compatibllity of each of the
following four Conjectures. to form a decreasing sequence:
</bodyText>
<listItem confidence="0.960966333333333">
1. John Smith owns something. (very likely, but &apos;dependent
on his age, society in which he lives, etc.)
2 John Smith owns a hammer. (probably, &apos;nit potentially
related to featuressof John, such as his profession)
3. John Smith owns a claw hammer with a wooden handle.
(maybe, but again dependent on features of John ahd
models of hammers in general -- i.e., how likely is
any given,hammer to hare a claw and wooden handle?)
4. John Smith owns a 16 oz. Stanley claw hammer with a
steel-reinforced wooden handle and a. tack puller on
the claw. (likelihood is quite low unless the N-mole-
cuie can locate some specific hints, such as that
</listItem>
<bodyText confidence="0.983155230769231">
John usuall—buys good e&apos;quipment&apos;, etc.)
A successful N-molecule assessment results in the. creation of the
assessed information .as a permanent, explicit memory struture whose STRENGT1i
is the assessed compatibility.. This structure is the normative inference.
One is quickly awed by his own ability to rate (wsually quite accurately)
commonsense conjecture such as these, and.thc-process seems usually to be
quite sensitive to features of the entities involved the conjecture. It
is my feeling that important insight&amp; can be gained via a ffiore thorough
investigation of the &amp;quot;normative inference&amp;quot; process in humans.
Another role of N-molecules is&apos; mentioned in (R1) with respect to
the inference-reference cycle I will describe shortly. Fig. 9 shows the
substance of a prototype N-molecule for assessihg dependency structures of
the form (0*; P X). (perl-Ion P owns object X ).
</bodyText>
<page confidence="0.998715">
36
</page>
<tableCaption confidence="0.764685666666667">
s P a member. of a pure communal society, or is it an infant?
if so, very unlikely that P 9wns,X
otherwise, does X have any distinctlYe conceptual features?
</tableCaption>
<construct confidence="0.848964882352941">
if so).assess each one, form the product.of likelihoods, arid call it
M. M will be used at the end to mitigate the likelihood which would
normally.be assigned.
is.X living?
if so, is X person?
is Pa slave owner: and does X possess characteristics
of a slave? if .so, likelihood is low but non-zero
otherwise likelihood is zero
otherwise, is X a non-human animal Or a plant?
if so, is X domestic-in P&apos;s culture?
if so, does P have a fear of X&apos;s or is
P allergic to X&apos;s of this type?
if so, likelihood i$ low
otherwise, liketihood is-moderate
otherwise, is X related to actions P. does in any special
way?
if so, likeliho,od is 1&apos;ow, but non-zero
otherwise, likelihood is near-zero
otherwise, does X have a normal function?
if so, does P do actions like this normal function? (Note here
that we nould want t ok.at P&apos;s profession, and actions commonlu
associated with that profession.)
if so, IWelihood is poderately high
otherwise, is X a common personal item?-
if so, is it s value within P&apos;megns
if so, likelihood is high
if not, likelihood is low, but non zero
otherwise, is X a common household .item?
if so, is P a homeowner?
if so, is X within P&apos;s means?
if so, likelihood&apos;is high
otherwise, likelihood is moderate
otherwise,. likelihood is-low, but non-zero
and so on ...
</construct>
<bodyText confidence="0.878439">
Row we tnigh,t„gy.aZoiding
whether person P i;405-11
</bodyText>
<sectionHeader confidence="0.538903" genericHeader="method">
FIGURE 9
</sectionHeader>
<subsectionHeader confidence="0.310804">
The normality-molecule
</subsectionHeader>
<bodyText confidence="0.894427">
disctimination&apos;network for the
pattern (OWNS&apos;P X).
</bodyText>
<page confidence="0.998453">
37
</page>
<note confidence="0.703481">
5.11 CLASS 13: STATE DURATION INFERENCES
</note>
<table confidence="0.582413454545454">
PRINCIPLE: Most interesting states in the world.are transient. The
CM must have the ability to make specific predictions
about the-expected (fuzzy) duration of an arbitrary state
so that information- in the CM can be kept up to date.
EXAMPLES: **John handed Mary the orange peel.
ttomorroW I Mary still holding the orange peel?
(inference) Almost certainly not.
**Rita ate lunch a half hour ago
Is she hungry yet?
(inference) Unlikely.
DISCUSSION:
</table>
<subsectionHeader confidence="0.6865">
Time features of states relate in critical ways to the likelihood
</subsectionHeader>
<bodyText confidence="0.97931364516129">
those states will be true at some given time. The thought of a scenario
wherein the CM is informed that Mary is holding an orange peel, then 50
years -later, uses that information in the generation of some other infer-
ence is a bit unsettling! The CM must Simply posess a low-level function
whose job it is to predictmOrmal durations of states based on the particulars
of the states, and to use that information in marking as &amp;quot;terminated&amp;quot; those
states whose likelihood has diminished below some threshold.
My conjecture is that a human notices and updates the temporal truth
of a ;tate only when he is about to use it in some cognitive activity --
that most of the transient knowledge in our—heads is out of date Until we
again attempt to use it in; say,.some inference. Accordingly, before using
any state information, the CM first filters it.through the STATE DURATION
inference process to arrive at In updated estimate of the state-&apos;s likeli-
hood as a function of its known starting time (its TS feature,.in CD
notation-).
iti
The implementation of this process in the CM is as follows: an (OUR S ?)
structure is constructed felr the state S whose duration is to be pre-
dicted, and this is passed to the NDUR specifier molecule: The NDUR S-
molecule applies discrimination tests on features of the objects involved
in S. Terminal nodes in we net are duration concepts (typically fuzzy
ones), such as #ORDERHOUR, #ORDERYEAR. If a terminal node can be success
fully reached, thus locating such a concept D, the property CHARF (Character-
istic time .function) is retrieved from D&apos;s property list. CHARF is
a step function of STRENGTH vs. the amount of time some state
has been i existence (Fig. 10). From this function a STRENGTH is computed
for S and btacomes S&apos;s&apos;predicted likelihood. If the STRENGTH turns out
to be mfficiently low, a (TF S now) structure is predictively generated
to make S&apos;s low likelihood elicit. The STATE,DURATJON inference thus
acts as a cleansing filter on state information which is fed to various
other inference processes.
</bodyText>
<figure confidence="0.873063076923077">
1.0.+
1.
STRENGTH
0 1hr
(T-T&apos;)
A typical STRENGTH function for fuzzy duration uORDERHOUR.
(W1MAX Si) (W2MAX 52) (MAX 53) (WnMAX Sn) )
0 &lt; W1MAX has strength Si
W1MAX 5 r-T &lt; W2MAX has strength S2
. • •
WriMAX. has strength 0
The format of a fuzzy duration concept&apos;s step function.
FIGURE ao
</figure>
<sectionHeader confidence="0.896291" genericHeader="method">
A,typical cftaracteristie
STRENGTH function for the
state-duration inference-
</sectionHeader>
<bodyText confidence="0.655766">
process.
</bodyText>
<subsectionHeader confidence="0.346349">
5.12 CLASSES 14 and 154.: .FEATURE and SITUATION INFMIENCES
</subsectionHeader>
<bodyText confidence="0.725385">
PRINCIPLE: Many inferences can be based solely on commonly observed
or learned associations, rather than upon-&amp;quot;logical&amp;quot; re-
lations such as causation., motivation, and so forth. In
a rough way, we can compare these inferences to the phe-
nomenon of visual imagery which constructs a &amp;quot;picture&amp;quot;
EXMPLES,:.
of a thdught&apos;s surrounding environment.
should be made in abundance.
</bodyText>
<figure confidence="0.9175918">
**Andy&apos;s diaper is wet.
(inference) Andy is a youngster. (FEATURE)
Such inferences
John was on his way to a masquerade.
(inference) John was ptdbably wearing a costume. (SITUATION)
</figure>
<sectionHeader confidence="0.761163" genericHeader="method">
DISCUSSION:
</sectionHeader>
<bodyText confidence="0.994771333333333">
Many &amp;quot;associative&amp;quot; inferences can be made to preduce new features of
an object (or aspects of a situation) from known features. If something
wags-its tail, it is probably an animal of some sort, if it bites the mail-
man&apos;s leg, itis probably a dog., if it has a gray beard and speaks, it is
probably an old mans if it honks in a distinctive way, it is probably some
sort of vehicle, etc. These classes are inherently unstructured, so I will
say no.more about them here, except that they frequently contribute fea-•
tures which help clear up reference ambiguities and initial reference fail—
ures.
</bodyText>
<page confidence="0.998478">
41
</page>
<table confidence="0.760365375">
5.13 CLASS 16- UTTERANCE INTENT INFERENCES
PRINCIPLE: Base4 on the way a thought is communicated (especially the
often telling presence or absence of information), infer-
ences can be made about the speaker&apos;s reasons for speaking.
EXAMPLES: **Don&apos;t eat green gronks.
(inference) Other kinds of gronks are probably edible
**Mary threw out the rotten part of the fig.
(inference) She threw it out because it was rotten.
**John was unable to get an aspirin.
(inference) John wanted to get. an aspirin.
**Rita like the,chair, but it was green.
(Inference) The clasis color is a negative .feature to
Rita (or the speaker).
DISCUSSION:
I have included this class only to represent the largely unexplored
domain ot interences dtawnefrom the way a thought is phrased. The CM will
</table>
<bodyText confidence="0.86322">
eventually need an explicit model of conversation, and this model will ,in-
corporate inferences from this class. Typical of such inferences are those,
which translate the inclusion of referentially superfluous features of an
object into an implied causality relation (the fig example), those which
infer desire froM failure (the aspirin example) those which infer features
of an oxdinary X from features of special kinds of X. (the gronk example),
and so forth. These issues will lead to a more goal directed-model than I
am currently exploring.
</bodyText>
<page confidence="0.996597">
42
</page>
<note confidence="0.47977">
G. Summary of the inference Component
</note>
<bodyText confidence="0.981008444444444">
I have now sketched 16 inference classes which, I conjecture, lie
at the core of the human interence reflex. The central hypothesis is
tnat a Inmuul language comprehender performs more subGonscious computation.
on ileaning structures than any other theory of language compmhension has
yet acknowledged. When the current CM is turned loose, it will often gen-
erate ui-wards of 100 inferences from a fairly banal stimulus such as &amp;quot;John
avc&apos;4ar? the book.&amp;quot; While most are irrefutible, they. are for the most part
mundane and &amp;quot;uninterestinr to a.criti&apos;cal human observer, and are, after
the fact, &amp;quot;I‘asteful.&amp;quot; But change the. context and the hula&apos; becomes salient --
even crucial --while the crucial can become irrelevant! I. can sec no other
Liecnanisn. for exilaining contextual.interhction of information than this
sl-ontaneous, -subtionscious groping.
I .sLoul..1 perhaps briefly address the adequacy and applicability of the
inference classes in the current model. There is undouhtedly a number
.1.3, 32, 04?) of eklually interesting inforence crasscs I have ignored or
ovei-locked. but I feel the number is not large, and that other classes will
sul‘:-It to the sain.‘ sorts of systematization as described nere. While the
;iecerk.il examples I have used to illustrate the various inferences were not
dTawn from any coherent domain such as a &amp;quot;blocks harld&amp;quot; (1&apos;1) -- and this is
a :ezikness -- I believe the net result (these inference .classes and their
control structure) fdll prove central to any restricted domain which involves
volitional actors.. It is a current challenge to find such a restricted, yet
interestir, domain to which these ideas can he transrlanted and applied in
nore 1.-,oal-c!irbcted envirornents.
I .,‘ant to describe now how two other _important language cok,onents
reference and inplicit concent activation -- aid an0 abet the inference
reflex.
</bodyText>
<page confidence="0.999532">
43
</page>
<sectionHeader confidence="0.497503" genericHeader="method">
7. The Inferenee-Reference Relaxation cycle in Conceptual Memory.
</sectionHeader>
<bodyText confidence="0.99098690625">
A %.A.A1111.C.allaP.AA;1 1 liv4uLlmly inLopoulu ul 111:-,LanLanvu11sly lucilLIumg
the referent (concept or token in memory) of a langua0.construction (noun
group, rronoun, etc.). Yet an attentive listener seldom fmil..; eventually
to identify tile intended &apos;referent, and he will seldom lose information
because of the reference delay. Furthermore, incorrect reference decisions
are em:‘irically few and fill- between. I believe that these .phenomena&apos; are
intimately related to the inference reflex.
In the Cl, initial reference attempts are made for concepts and toens
from descriptive sets.---coilections of &apos;conceptual features Ocancd From an
utterance by RiesUeck s conceptual analy.zer (R2). lig. 11 illustrates
the descriptive set for the &amp;quot;th&apos;e big red Jog who ate the bird.&amp;quot; Potential
memory concepts and token referents arc identified by an intersection scarCh
procedure which locates memory objects Whose features satisfy nil the
features-of the descriptive set. Such a searcb hill result in either (a) a
unique identification of some memory entity,. (h) a failure to locate any
satisfactory entitle, dr (c) a set of&apos;candidates,,one of which is the
probable referent. Case (a) requires no-decision, but (b) end (c) do
In either case, a nol%, possibly temporary token T is created aid, for
case (b), T receives as its initial occurrenCe set the descriptive set
identic:ally. In case (c), where a set of candidates can be locattd, T
receives the set of features lying in the intersection of all candidates&apos;
occurfenCe sets &apos;(this will be at least the descriptive set). In either
case, the CM then has an internal token, to ,work hith, allowing the conccp-
tuai graph in hhi.ch referenCes to it occur to be tentatively integrated into
memory.s.-tructures.
Tbe inference reflex I have described then.generate,s all the vdrious
inferences, and eventualliy returns to its quiescent state. One byprOduct
of the inferencing is that-the occurrence set of each memory object involv-
e:2 in the original- structures hill emerge hith a possible enhanced-occur-
ren66 set which may contain inferred information sufficient either (1) to
identify the temporary token of category (b) above, or (2) to narroh the
set df candidate associated with the temporary token of category (c) (hope-
</bodyText>
<page confidence="0.993819">
44
</page>
<figure confidence="0.908296727272727">
X: I (VSA &apos;X #006)
(COLOR X gREID)
(RELS I ZE #LAIJLI
: (INGEST X
(ISA g gBIRO) (REF if *THE*) I
(ISA g *MOUTH) I
( (ISA g gSTOMACH) I
(TIME Y I &apos;ISA g gT1ME) (BEFORE # #NIOW) I ).
) 4
(REF X *THE*)
Descriptive set for &amp;quot;The big red dog who ale the bird&amp;quot;
</figure>
<figureCaption confidence="0.272618">
FIGURE 11
</figureCaption>
<bodyText confidence="0.981781285714286">
41.1 example of a desoriptive set.
fully to exactly one). Thus, when the inference reflex has ceased, the eM
re-applies the refer&apos;ence intersection algorithms to each unidentified mleri.
to seek out any inference-tlarUied references. Successful identification&amp;
at this point result in the merging (by the samo structure merger mentioned
earlier)&apos; of the temporary token&apos;s occurrence set with the identified tokPn&apos;s
occurrence set, thus preserving all information collected to that point about
the temporary token. (Implicit in the merge operation is the substitution of
of all references to the, temporary token by references to.the identified
one.) If, on the other land, the results of inferencing serve only to
nacrow the candidate set of case (c.), the occurrence sets of the remaiRing
candidates are re-intersected, and if this increases the size of the set)
the set is re-attached to the temporary toRnv In either case progress
has been made.
Now .comes a key point. It any referents were in fact identified on
this second attempt Ouaking.their entire occurrence sets accessible), or
if any candidate set decreases. caused new features to be associated Aith
the temporary token, then there is the possibiiity that more inferences
(which can make use.of the riewly-accessible.features) cah.be made. lne (72!
thus re-applies the inference reflex&apos; to all memory structures which were
produced on the first pass. (The monitor is conditioned not to duplicate
work already done on the first pass.) But a potential bnwoduct of the-
second pass is further feature generation wnicn can again restrict candi-
date sets or produce positive identifications. This inference-reference
interaction can proceed tintil no new narrowings or Identifications&apos; occur;
hence.the&apos;term &amp;quot;relaxation cycle.&amp;quot; fig. 12 illustrates two examples&apos;of
this phenomenon which are handled by the current CM, arid Appendix E con-
tains the computer trace of the second example.
</bodyText>
<page confidence="0.999256">
46
</page>
<reference confidence="0.9766675">
EXAMPLE .1 Andy Rieger is a youngster.
Andy Mooter is an adult.
Andy&apos;s diaper is wet.
INFERENCE—REFERENCE: Andy Riecler&apos;s diaper is wet.
EXAMPLE 2 John was in Palo Alto yesterday.
Jenny Jones was in Palo Alto yesterday.
Jenny Smith was in France yesterday.
Bill loves Jenny Jones.
Bill saw John kiss Jenny yesterday.
INFERENCE-REFERENCE, FIRST PASS: It Was Jenny. Jones
that John kissed.
INFEIZENCE-REFERENCE, SECOND PASS: Bill felt anger
toward John
FIGURE 12a
Two examples of inference-reference
interaction.
</reference>
<figure confidence="0.988393096774193">
I UTTERANCE I
starting
inference queue
&apos;SUBPROP
EXTRACTOR
= = &gt;
(
more new information, but
this time a6out #PETE17
-.■••■•■■■■■■•■••
*.)
IREFERENCER1
.•••
#PETE1:74&apos;
1. *De Arr I
R EFeetuur
temporary token
for unestablished
reference
■■•
#
4- (ISA # #PERSON)
.(NAME # PETE)
fir6t pass
( * * * *
second
pass
HE-REFERENCER
(new inferred feature)4--5
4- (new, inferred feature)
INFERENCER
</figure>
<reference confidence="0.581034666666667">
Multiple reference-inference interactiont passes.
FIGURE 12b
ThP. inference-reference relaxation cycle.
</reference>
<page confidence="0.7673">
47
</page>
<bodyText confidence="0.975711032258065">
8. Word Sense. Promotion end Implicit Concept Activation
in the Conceatual Memory
Another Jyproduct of the generation of an abundance of probabilistic
conceptual patterns from each input is that many related concepts and
tokens implicitly inVolved.in the situation are activated, or &amp;quot;touched.&amp;quot;
This can be put .to use in two ways.
First, implicitly touched concepts can clarify what might otherwise
be an utterly opaque Subsequent teference. If, for instance, someone says
(outside of a particular context): &amp;quot;The nurses were nice&amp;quot;, you will prob-
ably inquire &amp;quot;What nurses?&amp;quot; If, on the other hana, someone says: &amp;quot;John
was run over by a milk truck. When he woke up. the nurses were nice&amp;quot; you
will experience neither doubt about the referents of &amp;quot;the nurses&amp;quot;, nor
surprise at their mention. I presume that a-subconscious filling-out of
the situation &amp;quot;John was run over by.a milk trucW implicitly activates an
entire set of coneptually relevant concepts, &amp;quot;precharging&amp;quot; ideas of hos-
pitals and their relation to patients.
Other theories founded more on concept associationism than conceptual
inference have suggested that such activation occurs through word-word or
concept-concept free associations (see (A2) and (1Q1) for instance). While
these more direct associations play an undoubted role in many language
functions, it is my belief that these straight associative phenomena are
not fundamentally powerful enough to explain the Rind of language behavio/
underlying the nurse example. It is more often than not the &amp;quot;gestalt&amp;quot;
meaning context of an utterance which restricts the kinds of meaningful
associations a human makes. In contrast to the nurse example above,
most people would agree that the reference to &amp;quot;the nurses&amp;quot; in the follow-
ing situation is a bit peculiar:
In the dark of the night, John had wallowed
through the knee-deep mud to the north wall
of the deserted animal hospital. The nurses
were nice.
</bodyText>
<page confidence="0.996795">
48
</page>
<bodyText confidence="0.999946972222222">
A simple hospital-nurses association model cannot account tor this. on
the other hand, those concepts tduched by the more. restrictive conceptual
inference patterns would presumably be quite distant from the medical
staff of a hospital in this example, thus explaining the incongruity.
Related to this idea of concept activation through conceptual infer-
ence Structures is another mechanism which, I presume, underlies a campre-
henders&apos; ability to select (almost unerringly) the proper senses of words
in context dpring the linguistic analysis of each utterance. This
mechanism is frequently called word sense prombtion, and its
exact nature is one of the major conundrums of language analysis. It
underlies our ability to avoid -- almost totally -- backing up to reinter-
pret words. It is as though at each moment during our comprehension we
possess a dynamically shifting predisposition toward a unique sense of
just about any word we are likely to hear next. Fig. 13 cdntaans some
illustrations of this phenomenon.
I have only a thought (which I plan to develop) on this issue. At
each instant in the CM, there is a powerful inference momentum which is
the product of conceptual inferences. -Obviously, these concepts which
the inference patterns touch will. correspond to senses of words. These
senses can be &amp;quot;promoted&amp;quot; in the same way implicit activation promotes
certain referents. This is a partial explanation or word sense promotion.
Suppose, however.; that in additiop the CM had an independent parallel pro-
cess whirl, took each inference as it arose and. mapped it back into a near-
language &amp;quot;proto-sentence&amp;quot;, a linear sequence of concepts which is almost
a sentence of the language, except that the actual word realizates of each
concept have not yet been chosen. In other words, a generation process
(see (Cl) for example) would be applied to each inference, but would be
stopped short of the final lexical substitutions of word senses. By pre-
cnarging all the senses of the various words which could be substituted in
such a proto-sentence, the CM would, have a word sense &amp;quot;set&amp;quot; which would be
a function of the kind of restrictive inferential context which I feel is
so vital to the process of analysis. This. .idea is obviously computationally
exorbitant, but it might model a very real mechanism. We often catch our-
selves subvocalizing what we expect to hear next (especially while listening
to an annoyingly slow speaker), and this is tantalizing evidence that some-
thing like a proto-sentence generator is thrashing about upstairs.
</bodyText>
<page confidence="0.99879">
49
</page>
<figure confidence="0.924403227272727">
EXAMPLE&apos; 1: (CONTEXT) John asked Maty which piece of fruit
she wanted:
(SENSE) Mary Licked the apple.
versus (CONTEXT) Mary climbed the apple tree.
(SENSE) Mary picked the apple.
EXAMPLE 2: (CONTEXT) John was in a meadow.
(SENSE) The Esaa.s. smelled good.
versus (CONTEXT) John was looking forward to getting high
(SENSE) The grass smellea good.
EXAMPLE 3: (Riesbeck&apos;s example (R2))
John went on a hunting trip. shot two bucks.
It was all he had:
FIGURE 13a
Examples of word sense promotion.
I ncoming
utterances
====-=Z&gt;
======&gt;
======&gt;
analyzed
graphs
CONCEPTUAL
ANALYZER
OPEOMIlmim,
.16%
activated T T T
worA &apos;senses
are prefered
by ttle
analyzer
:ND
conceptual
in ferencep
from analyzed
graphs
the partial generator
) &lt; PABTIAL runs independently
) &lt; CONCEPTUAL from the memorw
) &lt; GENERATOR
concep tua I
structures
back into
prbto-sentences
pro;to-sentences
</figure>
<reference confidence="0.8437186">
which are the various
Ways each inference
might be expressed
by languageA These
involve many alternative
word senses-.
Mapping inferences back into proto-sentences, activating many word senses.
FIGURE 13b
Mapping inferences back into
proto-sentences, activating many word senses.
</reference>
<page confidence="0.997131">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.938883">Journal of Computational Linguistics 13</note>
<title confidence="0.857761666666667">UNDERSTANDING BY CONCEPTUAL INFERENCE</title>
<author confidence="0.996842">CHUCK RIEGER</author>
<affiliation confidence="0.999964">Computer Science Department University of Maryland</affiliation>
<address confidence="0.931277">College Park</address>
<note confidence="0.890231">Copyright 1975 Association for Computational Linguistics</note>
<abstract confidence="0.999427357142857">Any theory of languaga must also be a theory of inference and memory. It does not appear to be possible to &amp;quot;understand&amp;quot; even tae simplest of utterances in a contextually meaningful way in a system in which language fails to interact with a languagefree memory and belief system, or in a system which, lacks a spontaneous inference reflex. People apply a tremendous amount of cognitive effort to understanding the meaning content of language in context. Most of this effort is of the form of spontaneous conceptual inferences which occur in a language-independent meaning environment. I have developed a theory of how humans process the meaning content of utterances in context. The theory is called Conceptual Memory, and has been implemented by a computer program whicn is designed to accept as input analyzed Cnnceptual Dependency (Schank et a..) meaning graphs, to generate many conceptual inferences as automatic responses, then to identify points of contact among those inferences in &amp;quot;infefence space&amp;quot;. Points of contact establish new patnways through existing memory structures, and hence &amp;quot;knit&amp;quot; each utterance in witn its surrounding context. Sixteen classes of conceptual inference have been identified and implemented, at least at the prototype level. Tnese classes appear to be essential to all higher-level language comprehension processes. Among them are causative/resultative (those which predict cause and effect relations), motivational (those which actors&apos; intentions), enablement (those which predict the surrounding context of actions), state-duration (those which predict the fuzzy duration of various states in the world) normative (those which assess the &amp;quot;normality&amp;quot; of a piece of information how unusual it is), and specification (those which predict and fill .in missing conceptual information in a languagccommunicated meaning graph). Interactions of conceptual inference with the language of (1) word sense promotion in context, and (2) ident- .) ification of referents to memory tokens are discussed. A theoretically important inference-reference &amp;quot;relaxation cycle&amp;quot; is identified. and its solution discussed. The theory provides the basis of a computationally effective model of language comprehension at a deep conceptual level, and should therefore be of interest to computational linguists, psychologists and computer scientists alike.</abstract>
<note confidence="0.9444665">TABLE OF CONTENTS 1. The Need for a Theory of Conceptual Memory and Inference 5 2. A Simple Example 7 3. Background 10 4. A Brief Overview of the Conceptual Memory&apos;s Inference Control Structure 15 5. The Sixteen Theoretical Classes of Conceptual Inference 20 5.1. CLASS 1: Specification Inferences 21 5.2. CLASSES 2 and 3: Resultative and Causative Inferences 25 5.3. CLASS 4: Motivational Inferences 26 5.4. CLASS 5: Enabling Inferences 27 5.5. CLASS 6: Action Prediction Inferences 28 5.6. CLASS 7: Enablement Prediction Inferences 30 5.7. CLASS 8: Function Inferences 31 5.8. CLASSES 9 and 10: Missing Enablement and Intervention Inferences 33 5.9. CLASS 11: Knowledge Propagation Inferences 34 5.10. CLASS 12: Normative Inferences 35 5.11. CLASS 13: State Duration Inferences 38 CLASSES 14 and 15: Featmre Situation Inferences 41 5.13. CLASS 16: Utterance Intent Inferences 42 6. Summary of the Inference Component 43 7. The Inference-Reference Relaxation Cycle in Conceptual Memory 44 8. Word Sense Promotions and Implicit Concept Activation in the Conceptual Memory 48 9. Conclusion 51 APPENDIX A. Causal Chain Expansion Computtr Example 52 APPENDIX E. Inference-Reference Relaxation Cycle Computer Example 57 REFERENCES 62 4 Need for a Theory of Conceptual Memory and Inference Research in natural language over the past twenty years has</note>
<abstract confidence="0.958682238764045">been focussed primarily on processes relating to the analysis of individual sentences (parsing). Most of the early work was devoted to syntax. Recently, however, there has been a considerable thrust in the areas of semantic, and importantly, conceptual analysis (see (R2), (M1), (Si) and (Cl) for example). Whereas a syntactic analysis elucidates a sentence&apos;s surface syntactic structure, typically by producing some type of phrase-structure parse tree, conceptual analysis elucidates a sentence&apos;s meaning (the &amp;quot;oicture&amp;quot; it produces), typically via production of an interconnected network of concepts which specifies the interrelationships among the cohcepts referenced by the words of the sentence. On the one hand, syntactic sentence analysis can more often than not be performed &amp;quot;locally&amp;quot; that is, on single sentences, disregarding any sort of global context; and it is reasonably clear that syntax has generally very little to do with the meaning of the thoughts it expresses. Hence, although syntax is an important link in the understanding chain, it is little more than an abstract system of encoding which does not for the most part relate in any meaningful way to the information it encodes. On the other hand, conceptual sentence analysis, by very definition, is forced into, the realm of geneLai knowledge; a conceptual analyzer&apos;s &amp;quot;syntax&amp;quot; is the set of rules which can produce the range of all &amp;quot;reasonable&amp;quot; events that might occur in the real world. Hence, in order to parse conceptually, the conceptual, analyzer must interact with a repository of world knowledge and world knowledge handlers (inferential processes). This need for such an analyzer-accessible world repository part Df the motivation for the development of the following theory of conceptual inference and memory however, the production of a conceptual network from an isolated sentence is only the first step in the understanding 5, process. After this first step, the real question is: what happens to this conceptual network after it has been produced by the analyzer? That is, if we regard the conceptual analyzer as a specialized component of a larger memory, then the allocation of memory resources in reaction to each sentence follows the pattern: (phase 1) get the sentence into a form which is understandable, then (phase 2) understand it It is a desire to characterize phase 2 which has served as the primary motivation for developing this theory of memory and inference. In this sense, theory is intended to be a charting-out of the kinds of processes which must surely occur each time a sentence&apos;s conceptual network enters the system. Although it is not intended to be an adequate or verifiable model of how these processes might actually in humans, the theory described in this paper has neveras a computer model ander PDP-10 Stanford 1.6 LISP. While the implementation follows as best it can an intuitively correct approach to the various processes described, the main intent of the underlying theory is to propose a set of memory processes which, taken together, could behave in a manner similar to the way a human behaves when he &amp;quot;understands language&amp;quot;. Simple Example attentive human mind is a volatile processor. My conjecture is that information simply cannot be put into it in a passive way; there are very primitive inference reflexes in its logical architecture which each input meaning stimulus triggers. I will call these primitive inference reflexes &amp;quot;conceptual inferences&amp;quot;, and regard them as one class of subconscious memory process. I say &amp;quot;subconscious&amp;quot; because the concern is with a relatively lowlevel stratum of &amp;quot;higher-level cognition&amp;quot;, particularly insofar as a human applies it to the understanding of language-communicated information. The eventual goal is to synthesize in an artificial system the rOugh flow of information which occurs in any normal adult response to a meaningfully-connected sequence of natural language utterances. This of course is a rather ambitious project. In this paper I will discuss some important classes of conceptual inference and their relation to a specific formalism I have developed (R1). Let me first attempt, by a fairly ludicrous example, to convince you (1) that your mind is more than a simple receptacle for data, and (2) that you often have little control over the thoughts that pop up in response to something you perceive. Read the following sentence, pretending you were in the midst of an absorbing novel: EARLIER THAT EVENING, MARY SAID SHE HAD KILLED HERSELF. One of two things probably occurred: either you chose as referent &amp;quot;herself&amp;quot; person other than Mary (in which ease everything works out fine), or (as many people seem to do) you first identified &amp;quot;herself&amp;quot; as a reference to Mary. In this case, something undoubtedly seemed awry: you realized either that your choice of referent was erroneous, that the, sentence was part of some unspecified &amp;quot;weird&amp;quot; context, or that there was simply an out-and-out contradiction. Of course, all three interpretations are unusual in some sense because of a &amp;quot;patently obvious&amp;quot; contradiction in the picture this utterance elicits. The sentence is syntactically arid semantically impeccable; only when we &amp;quot;think about it&amp;quot; does the big fog horn upstairs alert:us to the implicit contradiction: MARY SPEAK AT TIME T inference MARY ALIVE AT TIME T I.3 contradiction MARY NOT IVE- AT TIME T inference MARY CEASES BEING ALIVE AT TIME T-d inference MARY KILLS HERSELF AT TIME T-d Here is the argument: before reading the sentence, you probably had no suspicion that what you were about to read contained an implicit contradiction. Yet you probably discovered that contradiction effortlessly: Could there have been any a priori &amp;quot;goal direction&amp;quot; to the three simple inferences above? My conclusion is that there could not have been. If we view tne mind as a multi-dimensional &amp;quot;inference space&amp;quot;, then each incoming thought produces a spherical burst of activity about the point where it lands in this space (the place where the conceptual network representing it is stored). The horizon of this sphere consists of an advancing wavefront of inferences spontaneous probes Which are sent out from the point. Most will lose momentum and eventually atrophy; but a few will conjoin with inferences on the horizons of other points&apos; spheres. The sum of &amp;quot;points of represents tne integration of the thought into the existing fabric of the memory in that each point of contact establishes a new pathway between the new thought and existing knowledge (or perhaps among several existing pieces of knowledge). This to me is a pleasing memory paradigm, and there is a tempting analogy to be drawn with neurons and actual physical wavefronts as proposed years ago by researchers such as John drawing of this analogy is, however, left for the pleasure of you, the reader. This killing example was of course more pedagogical than serious, since it is a loaded atterance involving rather black and white, almost trivial interences. But it suggests a powerful low-level mechanics for general language comprehension. Later, I will refer you to an example which shows how the implemented model, called MEMORY and described in (R1), reacts to the more interesting example MARY KISSED JOHN BECAUSE HE HIT BILL, which is,perceived in a particular context. It does so in a way that integrates the thought into the frameWork of that context and which results in a &amp;quot;causal chain expansion&amp;quot; involving six probanilistic inferences. 9 3. Background Central to this theory are sixteen classes of spontaneous conceptual inferences. These classes are abstract enough to be divorced from any particular meaning representation formalism. However, since they were developed concurrently with a larger model of conceptual memory (R1) which is functionally a part of a language comprehension system involving a conceptual analyzer generator (MARGIE will help make the following presentation more concrete if we first have a brief look at the operation and goals of the conceptual memory in the context of the complete language comprehension system. The memory adopts Schank et al.&apos;s theory (S1,S2) of Conceptual Dependency (CD) as its basis for representation. CD is a theory of meaning representation which posits the existence of a small number of primitive actions (eleven are used by the conceptual memory), a number of primitive states, and a small set of connectives (links) which can join the actions and states together into conceptual graphs (networks). Typical-of the links are: ACTOR-ACTION &amp;quot;main&amp;quot; link ACTIONOBJECT link CAUSAL link the DIRECTIVE link and the STATECHANGE link Each primitive action has a case framework which defines conceptual slots which must be filled whenever the act appears in a conceptual graph. There are in addition TIME, LOCation and INSTrumental links, and these, as are all conceptual cases, are obligatory, even if they must be inferentially filled in by the conceptual memory (CM). Figure 1 illustrates the CD representation of the sentence MARY XISSED JOHN BECAUSE HE (JOHN) HIT BILL. That conceptual graph is read as follows: John propelled some unspecified object X from himself toward Bill, causing X to come into physical contact with Bill, and this entire event cause Mary to do something which resulted in her lips being in physical contact John! Furthermore, the entire event in the past. Chapter 2 of (R1) contains a fairly complete overview of the CD representation. Assuming the conceptual analyzer (see (R2)) has constructed, in consultation with the CM, a conceptual graph of the sort typified by &apos;Figure 1, the first step for the CM is to begin &amp;quot;integrating&amp;quot; it into some internal memory structure which is more amenable to the kinds of active inference Manipulations the CM wants to perform. This initial integration occurs in three stages. First is an initial attempt to replace the symbols (JOHN, MARY, BILL, X, etc.) by pointers to actual memory concelots and tokens concepts. Each concept and token in the CM is represented LISP atom (such as C0347) which itself bears no intrinsic meaning. Instead, the essence of the concept or token is captured set of features associated with the symbol. Thus, for instance, an internal memory token with no features is simply &amp;quot;something&amp;quot; if it must be expressed by language, whereas the token illustrated in Figure 2 would represent part of our knowledge about Bill&apos;s friend Mary Smith, a female human who owns, a red Edsel, lives at 222 Avenue St., is 26 years old, and forth. This set of features is called 00948&apos;s set, and is in the implementation merely a set of pointers to all other memory structures in which C0948 occurs. The process of identificationwill attempt to isolate one token, or second best, a set af candidate tokens for each concept symbol in the incoming graph by means of a feature-intersedting algorithm described in (R1). Reference identification is the first stage of the initial integration of the graph into internal memory structures. The (c) JOHN &lt;..=&gt; PROPEL X? ID 1--4 BILL JOHN (a) MARY &lt;==.&gt; DO (b) val &lt;BUM&gt; PHYSCONT 4.-- BILL PHYSCONT 4----- JOHN (d) part FIGURE 1 Conceptual dependency representation the &amp;quot;Mary kissed John because he hit Bill.&amp;quot; 00948: (ISA # #PERSON) (SEX # #FEMALE) (NAME # MARY) (SURNAME # SMITH) (OWNS # C0713) (TSTART # C0654) • • (00718 is the token representing the Edsel which Ilary owns, C0846 is the token for Mary&apos;s place of residence,. C0654 is a time token with a numeric :value on the CM&apos;s time scale representing Mary&apos;s time of birth in 1948) FIGURE 2 The memory&apos; token and its Occurrence set which repre&apos;sept Mary ,Smith. 12 second and third seages are (2) the isolation of subgraphs which form the beginning inference,queue(input to tne spontaneous inference component), and (3) the storage of the graph dependency links themselves as pointers in the memory.Just as for simple concepts and tokens, composite structures (actiQns and states) are stored under a unique internal symbol, and this symbol may also have an occurrence set. In addition, there are several other properties associated with each composite structure S: the recency of S&apos;s activation by explicit reference (RECENCY), recency S&apos;s activation by implicit (inferential) reference (TOUCHED), the degree to which S is held to be true (STRENGTH), a list of other composite structures fnom which S arose in the memory its inferential antecedants-(REASONS), and a list of other composite structures in whose generation Splayed a role as antecedant (OFFSPRING). RECENCY and TOUCHED are also properties of concepts and tokens, and are used in the referent identification process. Figure 3 shows the memory structures which result from the conceptual graph of Figure 1 after the initial integration. The net result of the initial integration is a set of starting memory structures,(actuallv, a list of pointers to their symbols, such as (C2496 C2301 C2207)). Each of these structures references memory concepts tokens and other composite structures. Regarding the referent identification process, for those concepts. and tokens which could not.be uhiquely identified, new temporary tokens will have beeh created, each having as its initial occurrence set a list of what is khown about the entity so far. After the initial integration, the inference component is applied simultaneously to each memory structure (&amp;quot;point in inference space&amp;quot;) on the starting inference queue. 13 (CAUSE (PROPEL * * * *) *) (PHYSCONT * * (UNSPECIFIED #) # (ISA # #PMSON) (SEX # ----&amp;quot;------_ ,-----.- (NAME # BILL) (DO (CAUSE * *) (ISA #,.*PERSON) (SEX # #FEMALE) (NAME-# MARY) (TIME (PHYSCONT * *) c/ (PART # *) (BEFORE # N) (N is the numeric &amp;quot;now&amp;quot; on the CM&apos;s time scale) The internal memory structurps resulting from the CD graph of Fig. 1. Figure 3 14 A Overview ot the Conce.tual Memor &apos;s Inference Control Structure. The control structure which implements the CM inference reflex is a breadth-first monitor whose queue at any moment is a list of pointers to dependehcy structures which have arisen by inference from the beginning structures isolated during the initial integration. It is the inference monitor&apos;s task to examine each dependency structure on the queue in turn, isolate its predicate, prepare its arguments in a standard format, collect several time aspdcts from the structure&apos;s occurrence set, then call the moleculeassociated with the predicate, passing along the arguments and time information. inferential knowledge in the CM is contained in molecules, which lie in one-one correspondence with conceptual predicates. An inference is a LISP which can discrimination testis on a relevant dependency structure&apos;s features and features of all tokens, and which-can call specialist programs to standard test information retrieval functions. (&amp;quot;CAUSER&amp;quot; is an example of such :4 specialist. It will scan back causal sequences from structure S until it locates a volitional actiOn, then it returns the actor of that action as the primary causing agent of S ) Inference molecules are hence multiple-response discrimination Petworks whose responses are conceptual inferences (of the various theoretical types to be described) which can be made from the dependency structure. Each potential inference within the molecule is called an atom. The contribution of an inference atom which has been found applicable the dependency structure reports eight pieces of information to a the monitor called the generator,whose job it embody each new inference in a memory structure. These.eight pieces of information are the following: a unique mnemonic which indicates to *hich of the theoretical classes the new inference belongs (this mnemonic is associated with the new structure only temporarily on the inference queue for subsequent control purposes) 2. the &amp;quot;reference name&amp;quot; of the generating inference atom 15 (each atom has a unique name which is associated with the new memory structure for control purposes) 3, the dependency structure (a predicate which binds together several pointers to concepts, tokens and other structures), which is the substance of the new inference a factor&amp;quot; which is a rough, ad hoc measure of the inference&apos;s probable relative significance (this is used only if a more sophisticated process, to be described, fails) 5, a REASONS list, which is a list of all other sttuctures which tested. by the discrimination net leading up to this inference atom. tvery dependency has a REASONS list recording how the structure arose, and the REASONS list plays a vital role in the generation of certain types of inference 6. a &amp;quot;propagation strength factor&amp;quot; which, when multiplied by the STRENGTIls (degree of belief) of all structures on the REASONS list, produces the STRENGTH of the new inference. Mere is a need for better heuristics here incidentally -see (Z1) for instance.) 7 a list of modifying structures (typicarly time aspects) become the new inferred structure&apos;s initial occurrence set 8, propagation and strength factors for each modifying struc Figure 4 illustrates small implemented. NEGCHANGL (something undergoes a negative change on some scale) inference molecule. It is included to communicate the gestalt rather than correct specifics at this early stage of development. two other main components of the inference monitor are the evaluatorand the merger.It is the function of the evaluator to exact and fuzzy contradictionsand confirmations(points of contact) eacn new inference as it arises and existing memory dependency structures. Because &amp;quot;fuzziness&amp;quot; in the matching process iliTplieF, access to a vast of heuristics (to illuStrate: would it be more like our the</abstract>
<note confidence="0.915338810810811">16 (1PROG NEGCHANGE (UN PE SC) (X1 X2) ( 1COND ( (EVENT UN) (COMO ( (F1 (15A PE egPERSON)) (IR oNE6CHANGE1 (eWANT PE (GU (ePOSCHANGE PE &apos;SC))) ...PEOPLE OFTEN WANT TO BETTER (0.95 1.0 (CAR UN).) -.THEMSELVES AFTER SOME (TS e* (11 UN)) (1.0 (CAR UN))) (COND (AND (SETO X1 (F1 (e*MFEEL* e egNEGEMOT1ON PE))) (SETO X2 !GLOBALFIND).) (IR eNEGCHANGE2 (ePOSCHANGE X1 egJOY) (0.9 1.0.(CAR UN) X2) (eTIME e* (TI UN)) (1.0 (CAR &apos;UN))) ...PERSON GETS HAPPY WHEN ENEMY NSUFFERS NEGCHANGE ) (COND ( (AND (SETO X1 (CAUSER (CAR UN))) (NOT (EQ (CAR X)1 (a2 (CDR X1))))) (IR eNEGCHAN,GE3 egNEGEMOTIONf(CAR X1)) .PEOPLE (1ON&apos;T LIKE (0.95 1.0 (CAR UN) (CDR X1)) ...OTHERS WHO HURT THEM ) ) ( (HASPRP PE (eISA PE OPHYSOBJ)) (COND ( (AND (SETO X1 (F1 (e*OWN* PE ))) (SE1Q X2 (CAUSER (CAR UNI)) (NOT (EQ X1 (CAR C2))) (IR eNEGCHAN6E4 X1 e#NEGEMOTION (CAR X2)) X DAMAGES Y&apos;S PROPERT (0.85 1.0 (CAR UN) X1 (CDR X2)) NTHEN X MIGHT FEEL ANGER (TS e* (11 UN)) ...TOWARD Y (1.0 (CAR UN))) ) ) An inference molecule used by the current program. FIGURE4 The NEGCHANGE inference molecule. 17 (eTS e* (TI UN)) (1.0 (CAR UN</note>
<abstract confidence="0.994829888059701">lawyer or our friend the carpenter to own a radial arm saw?), the evaluators delegates most of the matching responsibility to programs again organized by conceptual predicates called molecules(&amp;quot;N-molecules&amp;quot;). N-molecules, which will be discus-sed more later, can apply detailed heuristics to ferret out fuzzy confirmations and contradictions. As I will describe, N-molecules also implement one class of conceptual inference Confirmations&apos; and contradictions discovered by the evaluator are noted on special lists which serve as sources for possible subsequent responses by the CM. In addition, confirmations lead. to invocation of the structure merger, which physically replaces two by one new aggregate structure, and thereby knits together two lines of inference. 1s events go, this is one of the most exciting in the CM. cutoffoccurs when the product of .an inference&apos;s STRENGTH (likelihood) and its significance factor falls below a threshold (0.25). This ultimately restricts the raoius of each sphere in the inference space, and in the current model, the threshold is set low to allow considerable expansion. Figure 5 depicts the overall strategy of the inference monitor. (R1) contains a fuller account of the inference control structure, whose description will be terminated at this point. sixteen classes of inference whichLfuel this inference reflex. 18 next level becomes new !NEW1NFS pcan !NEWINFS: ( * * * * * * ) INFERENCE MOLECULE&apos;: ..predicate !INFS GENERATOR ( * ,* * * ) iiu EVALUATOR * * * * * * * ) 1 1 1 16&amp;quot; cut off inferences REORDERER ==&gt; ( * * ) • II ( * * * * * ) N-MOLECULES FIGURE The inference monitor. The Theoretical Classesof Inference. It is phenomenological that most of the human language experience focuses on actions, their intended and/or resulting states, and the causality and enabling states which surround them. &apos;There seems &apos;to be an inescapable core of notions related to actions, causation and enablement which almost anyone who introspects long enough&apos; will independently discover. In &amp;quot;Cold Warrior&amp;quot; model; Abelson Was perhaps the first to attempt a computationally formal systematization of this fundamental core of Meaning relations. It is of the utmost primacy in his system,. which models the ideologies and behavior patterns of a rabid right-winger, to discover and relate the underlying purposes, enablement and causality surrounding events in some hypothetical international scenerio or crisis. Again, in Schank t al&apos;s CD theory, the same emphasis arose more or less independently in a system of meaning representation for everyday utterances: causality, actions, state-changes and enablement were recurringsthemes. Not surprisingly the same notions have emerged as central in my analysis of the inference reover half of classes relate to this &amp;quot;action-intention-causalityenablement-knowledgen complex. In the following descriptions of these 16 classes, keep in mind that all types of inference are applicable to every subcomponent of every utterance, and that the CM is essentially a parallel simulation. Also bear in mind that the inference evaluator is constantly performing matching operations on each new inference in order to detect interesting&apos; interactions between inference spheres. It should also be emphasized. that conceptual inferences are probabilistic and predictive in nature, and that by making them in apparently wasteful quantities, the CM is not seeking one. result or truth. Rather, inferential expansion.is an endeavor which broadens&apos;each piece of information into its surrounding spectrum to fill out the information-rich situation to which the information-lean utterance might refer. The CM&apos;s.gropings will resemble more closely the solution of a jigsaw puzzle than the mare goaldirected solution of a crossword puzzle. The following discussions can only sketch the main ideas behind each inference class. See (R1) for a more comprehensive. treatment. 20 5.1 CLASS 1: SPECIFICATION XNFERENCES PRINCIPLE:The CM must be able to identify and attempt.to fill in each missing slot of an incoming conceptual graph. EXAMPLES:&amp;quot;Johmwas driving home from work. He hit Bill&apos;s (inference) It was a car which John propelled into the cat. **John bought a chalk line. (inference) It was probably from a hardware store that John bought the chalk line. DISCUSSION: Our use of language presupposes a tremendous underlying kffowledge about the world. Because of this, even in, say, the most explicit techniLal lutting, certain assumptions are made by the writur (speaker) about the comprehender&apos;s knowledge -that he can fill in the plethora of .detail surrounding each thought. In the CM, this corresponds to filling in all the missing conceptual slots in a graph. The utility of such a process is twofold&apos;. first, CM failures to specify a missing concept can serve as a source of requests, for more information (or goals to seek out that information by CM actions if. CM is controlling a ro- Second, by completing graph by application of knowledge of the modeled world, novel relations among specificconcepts and tokens will arise, and these can lead to potentially significant discoveries by other inferences. To illustrate, a very common missing slot is the instrumental case. generally leave it the imaginative powers of the hearer to surmise the probable instrumental action by which some action occurred: (husband to wife) I went to SEARS today. to How? had the car all day! Here, wife fills in the instrumental slot as: &amp;quot;Husband drove a car to relying_on some specific heuristics,,such as the distaAce from 21 home to SEARS, etc.), and this led to her di&amp;quot;scovery of a contradiction. That she may have been premature in the specification (and&apos;shad later to undo it) is of secondary importance to the phenomenon that she did so spontaneously. the CM.specification inferences, as are implemented in the form of structured programs which realize discrimination nets whose terminal nodes are concepts and tokens rather-than inferences, as in general molecules. These specification procedures are called molecules(&amp;quot;S-molecules&amp;quot;), and are quite similar to inference molecules. Fig. 6 shows a small prototype of the PROPEL specifier molecule Which can fill in the missing a PROPEL.attion, as in &amp;quot;John hit Pete.&amp;quot; That particular &amp;quot;specifier atom&amp;quot; is sensitive to context along one simple dimension if the actor is known to be grasping an object (this prototype doesn&apos;t care wnetner it&apos;s a wet noodle or a bludgeon), at the time of the action, the molecule will infer that it was the grasped object which was propelled, as in &amp;quot;John picked up the flower pot. He hit Pete.&amp;quot; Otherwise, the molecule will assume &amp;quot;hand of the actor&amp;quot;. This is ridiculously oversimplified, tut it repreents a certain philoSophy I will digress a to I, as many other people (see Wl, H1, Cl, for unstance), have come to that passive data structures fundamentally awkward for representing knowledge in any detail, partitularly for the purposes typified by</abstract>
<note confidence="0.918115666666667">PROPEL* (UN V AC OB OF X2 X3) ( (COND ( (NULL (CADR V)) (COND ( (AND 4SETO X] (C (4ISA e_ OHAND) @PART e_ AC))) (SETO X2 (F1 (*LOC* X1))) X3 (SP V 2 X2 . (LIST X3)) )( (SP V 2 X1 NIL)</note>
<abstract confidence="0.972112095679015">specifier atoms go here) (RETURN V) )) is a simplified specifier molecule containing &apos;just an object speciflqr atom. (NULL (CADR V)) is 1 test for lack of object sbecificatio 1 If unspecifieo, the atom locates &apos; the hand of the actors, assigning it to Xl. It then checks to see if anythjng is located in Xl. If sonething is found, it is bound X2, and LOC which expresses this information is. to X3. If nothin9 4s actor&apos;s hand, has hand (X1) is inferred. in the call the list of REASONS here) lustifying the specificatibn the the was &apos;holding as the object of the PROPEL. FIGURE 6 PROPEL molecule. this simple PROPEL example. The needs for &amp;quot;special.case heuristics&amp;quot; in even such a inodest pperation as this quickly overtaRe one&apos;s prowess at devising &amp;quot;declarative&amp;quot; memory, structures. Programs, on the other hand., are quick and to the point, quite flexible, and have_as much &amp;quot;aesthetic eyen the most elegant declarative structures. A life-size procedure for this very narrow process of specifying the a PROPEL action would obviously more tests for related contexts (&amp;quot;John was racing down. the hill on his bilce. He hit But of the fidelity which any given executes its task, there is a very important claim buried both here and the other inferential procedures in the 01. It is that are cercentral tasks in which the decisionprocess must seek out the context, than context seeking appropriate decision process. In other words, much of inference capability requires specialists who Rmow priori exactly what dimensions of context could possibly affect the generation of every potential inference, and these specialists carry out active to search, for those any inference is generated. can imagine no &amp;quot;uniform context mechanism&amp;quot; which accounts for diverse ability to attend to the relevant and ignore the superfluous. the mechanism for contextual guidance of inference is highly distributed throughout the memory rather than.centralized as a component of the, memory&apos;s control structure. 24 2 and 3: RESULTATIVE and CAUSATIVE INFERENCES PRINCIPLE!If an action is perceived, its probable be (RESULTATIVE). If u state is perceived, the general nature of its probable causing action (or a specific action, if possible) should be inferred (CAUSATIVE). EXAMPLES:**11.Jary Pete with a rock. (inference) Pete probably became hurt. (RESULTATIVE) **Bill was angry at Mary. (inference) Mary&apos;may have done something to Bill. (CAUSATIVE) DISCUSSION.: two classes Of inference embody the CH&apos;s ability to relate acarid causal sequences relative to Ws models of addition to serving as the basis for MOTIVATIONAI, inferences and contributing to the general expansjon process, CAUSATIVE and RESULTATIVE inferences often achieve the rather exotic form of under- I have termed chain expansion.&amp;quot; It this which makes explicit the oft-abbreviated statements of causality: language compredications ot causality must always (if only be explained in terts of the comprehender&apos;s models of causality, and failures to do so signal a lack of understanding and form another source of CM queries for more information. CaUsal expansion successes on the other hand result in important intervening actions and states which draw out (&amp;quot;touch&amp;quot;) context and serve as basis for in other cate- Appendix the computer printout from MEMOM , tracing a expansion for &amp;quot;Mary kissed John because he hit Bill&amp;quot; in makes the explanation plausible. 25 . 3 4: MOTIVATIONAL .INF ERENC ES 1RINCIPLEThe desires (intentions) of an actor can frequently be inferred by analyzing the states (AESULTATIVE inferences) which result from an action he executes. These WANT-STATE patterns are essential to understanding and should be made in abundance. EXAMPLES:**John pointed out to Mary that she hadn-t done her (inference) Mary may have felt guilty. (RESULTATIVE) (inference) John may have wanted Mary to feel guilty. (MOTIVATIONAL) **Andy blew on the hot &apos;meat. (inference) Andy may have wanted the meat to .decrease in temperature. DISCUSSION: Language is a dual system of communication in that it usually communicates both the actual, and, either explicitly or by inference, the intentional;Where the intentions of actors (the set of stat?s.they desire) are not explicitly communicated, they must be inferred as the causality of thea action. In the for MOTIVATIONAL 1: CM from i CMcould make A , it conjectures that perhaps actorof A desired R. Since the generation of MOTIVATIONAL inference is dependent upon the results of another class of inference (in general, the actor could have desired things causally removed by several inferences from the immediate resultg of his action), the MOTIVATIONAL inference process is implemented a special proceuu-re POSTSCAN is invok.ed of the main breadth-first monitor. These passes will be discussed more later. each MOTIVATIONAL inference will generally lead backward, via CAUSATIVE inferences, into an entire causal chain which lead up to the action. This chain will frequently connect in interesting ways with chains working forward from other actions. 26 . 4 CLASS IN4ERENCES PRINCIPLE:Every action has a set enabling conditions -conditions which must be met for the action to begin or proceed. The CM needs a rich knowledge of these conditions (state), and infer to EXANPLES:**John saw Mary yesterday. John and were same general location sometime &apos;yesterday. **Mary told Pete that John was at the store (inference) Mary knew that John was at the store. DISCUSSION: The example at the beginning of the paper contained a contradiction which could be discovered only.by.making a very simple enabling inference about the action of speaking (any action for that matter), namely that the actor was alive at the time! Enabling inferences can fruitfully lead from known action through the enabling states to predications actions the actor Might have performed in order to set up the enabling states for the primary action. This Idea is closely related to the next class of inference. 27 5 6: ACTION PREDICTION INFERENCES Whenever some WANT STATE of a potential actor is known, predictions about possible actions the actor might perform to achieve the state should be attempted. These predictions will provide potent potential points of contact for subsequently perceived actions. EXAMPLES:**John&apos;Wants some nails. (inference) John might attempt to acquire some nails. **Mary is furious at Rita. (inference) Mary might do something to hurt Rita. DISCUSSION: Action prediction inferences serve the inverse role of MOTIVATIONAL inferences, in that they work forward from a known WANT STATE pattern into predictions about future actions which could produce the desired state. Just as a MOTIVATIONAL inference relies upon RESULTATIVE inferences, an ACTION PREDICTION inference relies upon CAUSATIVE Inferences which can be generated from the state the potential actor desires. Because it is often to anticipate the specificcausing action, ACTION PREDICTION inferences typically will be &apos;more general expectancies for a class of possible actions. In the nails example above, the general expentandy is, simply that John may do something which normally causes a PTRANS-(in CD terminology, a change of location of some object) of some nails from somewhere himself. Often the nature of the desired state is such that some specific action can be predicted (&amp;quot;John is hungry... John will ingest food.&amp;quot;) By making specific action predictions, a new crop of enabling inferences can be predicted (&amp;quot;John must be near food.&amp;quot;, etc.),.and those conditions which cannot be assumed to be already satisfied can.serve as new WANT-STATEs of the actor. Thus it is through MOTIVATIONAL, ACTION PREDICTION and ENABLING inferences that the CM can model (predict) the problem-solving behavior of each actor. Predicted actions which match up with subsequently perceived conceptual input serve as a very real measure of the CM&apos;s success at piecing together connected discourse and stories. I suspect in addition that ACTION PREDICTION inferences will play a key role in the eventual solutions of the &amp;quot;contextual guidance of inference&amp;quot; problem. Levy DI) ha S some interesting beginning thoughts on this topic. Fig. 7 illustrates the ACTION PREDICTION inference cycle. 28 (WANT P (WANT P (WANT P A3) &gt; \f / (WANT P *I A1E1 ■ (WANT P *). (WANT P *) %- I _1 • I A2E1 ■1=1 A • A2E3 infer (WANT P AiEj) for each AiEj which cannot be explicitly found. or assumed to be true 3 the action prediction inference process tries to enabling state j 13for FIGURE 7&apos; The action synalaiction&apos;inferefire pr.ocess. 2.9 5 . 6 CLASS 7: ENABLEMENT PREDICTION II‘tERENC ES . PRINCIPLE:If ,a potential actor desires-a state which is commbn enabling condition for some specific action, then it be inferred that the %tor wish to execute that action. EXAMPLES:asked John to tarn the (inference) Mary probably wants to see something.. wants meat to be cool. eat meat. )ISCUSSION: in this class arc, in a the inverse of ENABLING because they attempt to predict an action an enabling known to desired by a would-be actor. an ACTION PREDICfuture fulfill desired enablement prediction draws out motivation of the by a probable the state 1%ould Although with ACTION PREDICTION inference) it will frequently happen specific action can be anticipated (since most states could enable infinitemany specific actions), i5 nevertheless possible to&apos; form general predictions about the nature of (-restrictions on) the enabled &apos;action. If, example, walks to Mary, then a is that is and a is that he wants to be. near At this point an inference can be to represent the general class of interactions John might have in mind. This will be of&apos;particular significance if, for instance, the CM knows already that John had something to tell her, since then the inferred action pattern would quite well the action of verbal communication in state of proximity key enabling role. 30 . 7 8: FUNCTION INFERENCES PRINCIPLL:Control over some physical obj.ect P is usually desired by a potential actor becauSe jle is engaged. in an algorithm in which P plays a role. The CM should attempt to infer probable action from its knowledge of P&apos;s normal function. EXAMPLES:**Mary wants the book. (inference) Mary probably wftnts to read the book. wants a John wants. to cut something with the knife. **Bill Tikes to pour sundaes down girls&apos; dresses. asked Pete to .hand him DISCUSSION: Function inferences form a very diverse, rather colorful subclass of ENABLEMENT PREDICTION inference. The underlyihg principle is that desire of`immediate control over an object is usually tantamount to a desire .to use that objept in the norma,1 function of objects of that type, or in some function vitith is peculiar to the object-and/or actor (third example above). In the CM, normal functions of objects are stored as (NKT X Y) patterns, as ip Fig. 8 for things that are printed matter. applying NFCT patterns, the el firSt checks for relations involving the specific actor and specific object excluding paths which the normal ISA between, say sundae and food). Thus, is known to require sundaes for different algorithms from most will be and used&apos;in the The result of a FUNCTION inference is always some predicted action, assumed to be part of which the actor is engaged, 31 (NFCT #PRINTEDMATTER.*) 17 d .( I SA # #CONCEPTS) I NTEQMAT TER); (MTRANS * * #PRINTEDMATTER *) (ISA # #PERSON) ( I SA. # #CP) (PART # )K) structure which stores the normal function of printed matter. FIGURE 8 A &amp;quot;normal—function—of&amp;quot; memory structure. 3&apos;2 9 and 10: MISSING ENABLEMENT and INTERVENTION INFERENCES PRINCIPLE:If a would-be actor is known to have unsuccessful in achieving some action, it is often possible to infer absence of one the action&apos;s enabling states CMISS- BIABLEMENT). If a potential is known to desire that some action cease, it&apos;can be predicted that he will one or more enabling states of the action (INTERVENTION). EXAMPLES:**Mary couldn&apos;t see the finish. (inference) Something bloc_ked Mary&apos;s view. (MISSING ENABLEMENT) cursed the man of her... **Mary saw that Baby Billy was running out into the street. (inference) Mary will pick Billy off the ground (INTER- VENTION) She ran after him... DISCUSSION: Closely related to the other enabling inferences, these forms attempt apply lmowledge about relations to infer the cause of an action&apos;s failure (in the case of MUSSING ENABLEMENT), or to predict a WANT NOT-STATE which can lead by action prediction inference to possible actions intervention on the part of the WANTer. In the second example Mary (and the CM&apos; first Must realize (via RESULTATIVE inferences) the potentially undesirable consequences‘ of Billy&apos;s running attion (i.e., possible NEGCHANGE for Billy) From this, the CM can retrace, loaate the running action which could lead to such a NEGCHANGE, collect its enabling then coniecture that Mary might desire to annul more of them. tor be that Billy&apos;s feet be jn intermittent PINSwith the ground. From the (WANT (NOT GROUND))) structure, a subsequent ACTION PREDICTION inference can arise, predicting that Mary &apos;might put an end to (PHYSCONT FEET GROUND). This _pill in turn requilea her to be located near Billy, and that prediction will matCh the RESULTATIVE made from her directed utterance input), knifing the two thoughts together. 33 CLASS 11: PROPAGATION INFERENCES PRINCIPLE:Based on what the CM knows an actor to know, it can often otherknowledge which must also be available to the actor. Since most conceptual inferences involve the intentions .of actors, this modeling of ,knowledge is crucial. EXAMPLES:**John saw Mary beating Pete with a baseball bat. (inherence) Joh4 probably knew that Pete was getting&apos;hUrt. **Betty asked Bill for the aspitin. (inference) Bill probably surmised that Betty wasnit feeling well. DISCUSSION: Modeling the knowledge of potential actors is fundamentally difficult. Yet it is essential, since most all intention/prediction-related inferences must be based in part on guesses about what knowledge each actor has availto him at various times. The models others&apos; knowledge by &amp;quot;introspecting&amp;quot; on its on assuming another person P has access to_ same kinds of information as P might be expected, to make some of the same inferences the CM does. Since the CM presertcs a logical conneetlyity among all its inferred structures (by the REASONS and OFFSPRIY( properties of each structure), after inferences of othef types have arisen from some unit of information U, the CM can return, determine who knew the original fact U, locate U&apos;s OFFSPRING (those other memory structures which by inference from U)., then infer that also be aware of each the offspring. MOTIVATIONAL inferences (Which rely on the RESULTATIVE.inferonces from a structure), KNOWLEDGE PROPAGATION inferences implemented in the procedure after the initial breadth-first inference expansion by the monitor. Modeling others&apos; knowledge demands a rich knowledge of what is normal in the world.(&amp;quot;dOes John Smith know that kissing is a sign.of affection?&amp;quot;). fact, all inferences must rely upon default about since most of the EMPs .knowledge&apos; (and presumably a. Luman&apos;s) exists in the form of general patterns, rather than specific relations among specific concepts and tokens. The next olass of .inference implements my belief that patterns,just as inferences, should be realized in &apos;the CM by activeprograms rathbr than by passive declarative data structures. 34 12: INFERENCES PRINCIPLE:The QM must make heavy reliance, upon programs whIcaencoue commonsense pattern informatiOn about the modeled world. When the retrieval of a sought-after unit of inforfttion fails, the relevant normality program should be executed on (pattern applied to) that information to assess its likelihood in the absence of explicit inforMation.. LXAMPLES:&apos;**Does John Smith own a book? (inference) Probably so; middle&apos;-class business executives normally own books. John Likely to have been asleep at 3 pm Most likely.not, since he normal daytime job, and yesterday was avorkday. DISCUSSION: There are several low-level information retrieval procedures in the CM which search for explicit information units as directed by specific inference molecules. Such searches are on the basis of form alone, and in precise matches, while failures are total. were no recourse for such failures, the CM would quickly grind to a halt, being unable to make intelligent assumptions, There must be some more and flexible mechanism failures. the CM, this ability To Rake intelligent assumptions is implement by having procedures defer control to the appromolecule(N-molecule) will perform systemmatic tests in single-response discrimination nets, to the unlocatable information. The goal is to arrive at-a terminal node in the. net where a real number between C and 1 is located. ii some sequence of tests to spch a number, the N-molecule return.it as the assessed likelihood (&amp;quot;compatibility&amp;quot;&apos; in fuzzy logic tetminology (Z1)] ®f X being true. Although the test in the N-molecules are themselves discrete, they fuzzy. compatibility. The point of course is that the tests diverse and very specific peculiar each small domain of patterns; For instance, based on known (or N-molecule inferrable.-one N-molecule can Gall upon&apos; others in its testing process!) features of Jobn or we would suspect of each of the following four Conjectures. to form a decreasing sequence: 1. John Smith owns something. (very likely, but &apos;dependent his age, in which he lives, etc.) 2 John Smith owns a hammer. (probably, &apos;nit potentially to featuressof John, such as his 3. John Smith owns a claw hammer with a wooden handle. again dependent on features of John ahd models of hammers in general -i.e., how likely is any given,hammer to hare a claw and wooden handle?) 4. John Smith owns a 16 oz. Stanley claw hammer with a steel-reinforced wooden handle and a. tack puller on the claw. (likelihood is quite low unless the N-molecuie can locate some specific hints, such as that good e&apos;quipment&apos;, etc.) A successful N-molecule assessment results in the. creation of the assessed information .as a permanent, explicit memory struture whose STRENGT1i is the assessed compatibility.. This structure is the normative inference. One is quickly awed by his own ability to rate (wsually quite accurately) commonsense conjecture such as these, and.thc-process seems usually to be quite sensitive to features of the entities involved the conjecture. It is my feeling that important insight&amp; can be gained via a ffiore thorough investigation of the &amp;quot;normative inference&amp;quot; process in humans. Another role of N-molecules is&apos; mentioned in (R1) with respect to the inference-reference cycle I will describe shortly. Fig. 9 shows the of prototype N-molecule for assessihg dependency structures of (0*; P X). (perl-Ion P owns object X ). 36 s P a member. of a pure communal society, or is it an infant? if so, very unlikely that P 9wns,X does X any conceptual features? so).assess each one, form product.of likelihoods, arid call it M will be used at the end to mitigate which would is.X living? if so, is X person? is Pa slave owner: and does X possess characteristics of a slave? if .so, likelihood is low but non-zero otherwise likelihood is zero otherwise, is X a non-human animal Or a plant? so, is X P&apos;s culture? if so, does P have a fear of X&apos;s or is allergic to X&apos;s of type? if so, likelihood i$ low otherwise, liketihood is-moderate otherwise, is X related to actions P. does in any special way? if so, likeliho,od is 1&apos;ow, but non-zero otherwise, likelihood is near-zero otherwise, does X have a normal function? if so, does P do actions like this normal function? (Note here we want t ok.at P&apos;s profession, and actions commonlu with profession.) so, IWelihood is high is X a common personal so, is it s within P&apos;megns if so, likelihood is high not, is low, non is X common household if so, is P a homeowner? so, is X P&apos;s if so, likelihood&apos;is high likelihood is likelihood but non-zero and so on ... Row we tnigh,t„gy.aZoiding whether person P i;405-11 FIGURE 9 The normality-molecule disctimination&apos;network for the pattern (OWNS&apos;P X). 37 CLASS STATE DURATION INFERENCES PRINCIPLE: Most interesting states in the world.are transient. The have the ability to specific predictions about the-expected (fuzzy) duration of an arbitrary state so that informationin the CM can be kept up to date. EXAMPLES:handed Mary the orange peel. I Mary holding the orange peel? (inference) Almost certainly not. ate a half hour ago Is she hungry yet? (inference) Unlikely. DISCUSSION: Time features of states relate in critical ways to the likelihood those states will be true at some given time. The thought of a scenario wherein the CM is informed that Mary is holding an orange peel, then 50 uses that information in of some other inference is a bit unsettling! The CM must Simply posess a low-level function whose job it is to predictmOrmal durations of states based on the particulars of the states, and to use that information in marking as &amp;quot;terminated&amp;quot; those states whose likelihood has diminished below some threshold. is that a human notices and updates the temporal truth ;tate only when he is about to use it in some cognitive activity -that most of the transient knowledge in our—heads is out of date Until we attempt to use it say,.some inference. Accordingly, before using state information, the CM first it.through the STATE DURATION inference process to arrive at In updated estimate of the state-&apos;s likelihood as a function of its known starting time (its TS feature,.in CD iti of this process in the CM is as follows: an (OUR S ?) structure is constructed felr the state S whose duration is to be preand this is passed to the specifier molecule: NDUR Sapplies discrimination tests on of the involved in S. Terminal nodes in we net are duration concepts (typically fuzzy such as #ORDERHOUR, #ORDERYEAR. a terminal node can be success reached, thus such a concept D, the property CHARF (Charactertime .function) retrieved D&apos;s property list. CHARF a step function of STRENGTH vs. the amount of time some state been i (Fig. 10). From this a STRENGTH is computed for S and btacomes S&apos;s&apos;predicted likelihood. If the STRENGTH turns out to be mfficiently low, a (TF S now) structure is predictively generated make S&apos;s low likelihood elicit. STATE,DURATJON inference thus acts as a cleansing filter on state information which is fed to various STRENGTH 0 1hr (T-T&apos;) typical STRENGTH function duration uORDERHOUR. (W1MAX Si) (W2MAX 52) (MAX 53) (WnMAX Sn) ) has strength Si 5 &lt; has strength S2 . • • WriMAX. has strength 0 The format of a fuzzy duration concept&apos;s step function. cftaracteristie STRENGTH function for the inferenceprocess. 14 and .FEATURE and SITUATION INFMIENCES PRINCIPLE:Many inferences can be based solely on commonly observed or learned associations, rather than upon-&amp;quot;logical&amp;quot; relations such as causation., motivation, and so forth. In a rough way, we can compare these inferences to the phenomenon of visual imagery which constructs a &amp;quot;picture&amp;quot; EXMPLES,:. of a thdught&apos;s surrounding environment. should be made in abundance. **Andy&apos;s diaper is wet. (inference) Andy is a youngster. (FEATURE) Such inferences John was on his way to a masquerade. (inference) John was ptdbably wearing a costume. (SITUATION) DISCUSSION: Many &amp;quot;associative&amp;quot; inferences can be made to preduce new features of an object (or aspects of a situation) from known features. If something wags-its tail, it is probably an animal of some sort, if it bites the mailleg, itis probably a if it has a gray beard and speaks, it is an old if it honks in a distinctive way, it is probably some sort of vehicle, etc. These classes are inherently unstructured, so I will say no.more about them here, except that they frequently contribute fea-• which help clear up and reference fail— ures. 41 CLASS 16- UTTERANCEINTENT INFERENCES PRINCIPLE:Base4 on the way a thought is communicated (especially the often telling presence or absence of information), inferences can be made about the speaker&apos;s reasons for speaking. EXAMPLES:**Don&apos;t eat green gronks. (inference) Other kinds of gronks are probably edible threw out the rotten part fig. (inference) She threw it out because it was rotten. **John was unable to get an aspirin. (inference) John wanted to get. an aspirin. **Rita like the,chair, but it was green. The clasis color is a .feature Rita (or the speaker). DISCUSSION: I have included this class only to represent the largely unexplored ot interences dtawnefrom the way a thought is phrased. The will need an explicit model of conversation, and this model will ,infrom this class. Typical of such inferences are those, which translate the inclusion of referentially superfluous features of an object into an implied causality relation (the fig example), those which infer desire froM failure (the aspirin example) those which infer features an oxdinaryX from features of kindsof X. (the gronk example), These issues will lead to a more goal than I am currently exploring. 42 of the inferenceComponent now sketched 16 inference classes which, I conjecture, lie at the core of the human interence reflex. The central hypothesis is tnat a Inmuul language comprehender performs more subGonscious computation. ileaning than any other theory of language compmhension has yet acknowledged. When the current CM is turned loose, it will often generate ui-wards of 100 inferences from a fairly banal stimulus such as &amp;quot;John the book.&amp;quot; While most are irrefutible, they. are for the most part mundane and &amp;quot;uninterestinr to a.criti&apos;cal human observer, and are, after the fact, &amp;quot;I‘asteful.&amp;quot; But change the. context and the hula&apos; becomes salient -crucial --while the crucial can I. sec no other Liecnanisn. for exilaining contextual.interhction of information than this sl-ontaneous, -subtionscious groping. I .sLoul..1 perhaps briefly address the adequacy and applicability of the classes the model. There is a number 32, 04?) of eklually interesting inforence crasscs I have or but the number is large, and that other classes will sul‘:-It to the sain.‘ sorts of systematization as described nere. While the examples I have used to the inferences not dTawn from any coherent domain such as a &amp;quot;blocks harld&amp;quot; (1&apos;1) -and this is a :ezikness -- I believe the net result (these inference .classes and their structure) fdll prove central to restricted domain which involves actors.. It is a current to find such a yet domain to which these ideas can and applied in .,‘ant to describe now other _important language referenceand inplicitconcent activation-aid the inference reflex. 43 TheInferenee-Reference Relaxation cycle in 1 inLopoulu lucilLIumg referent (concept or token in memory) of a (noun group, rronoun, etc.). Yet an attentive listener seldom fmil..; eventually tile intended and he will seldom lose information of the reference Furthermore, incorrect reference em:‘irically few and between. I that these .phenomena&apos; are intimately related to the inference reflex. In the Cl, initial reference attempts are made for concepts and toens of &apos;conceptual features Ocancd From an by RiesUeck s conceptual (R2). lig. 11 illustrates the descriptive set for the &amp;quot;th&apos;e big red Jog who ate the bird.&amp;quot; Potential concepts and arc by an scarCh procedure which locates memory objects Whose features satisfy nil the the set. Such a searcb result in either (a) a identification of some memory entity,. (h) failure to any satisfactory entitle, dr (c) a set of&apos;candidates,,one of which is the probable referent. Case (a) requires no-decision, but (b) end (c) do either case, nol%, temporary token T is created aid, for case (b), T receives as its initial occurrenCe set the descriptive set identic:ally. In case (c), where a set of candidates can be locattd, T the set of features lying in the intersectionof all candidates&apos; occurfenCe sets &apos;(this will be at least the descriptive set). In either case, the CM then has an internal token, to ,work hith, allowing the conccptuai graph in hhi.ch referenCes to it occur to be tentatively integrated into inference reflex I have described then.generate,s all the returns to its quiescent state. One byprOduct the inferencing is that-the occurrence set of each memory object involvstructures hill emerge hith possible enhanced-occurren66 set which may contain inferred information sufficient either (1) to identify the temporary token of category (b) above, or (2) to narroh the df candidate associated with the temporary token of category (c) (hope- 44 I (VSA &apos;X (COLOR X gREID) I ZE X (ISA g gBIRO) (REF if *THE*) I I ( (ISA g gSTOMACH) I Y I &apos;ISA g gT1ME) (BEFORE #NIOW) I ). 4 (REF X *THE*) Descriptive set for &amp;quot;The big red dog who ale the bird&amp;quot; FIGURE 11 41.1 example of a desoriptive set. to exactly one). when the inference reflex has ceased, the eM re-applies the refer&apos;ence intersection algorithms to each unidentified mleri. seek out any references. Successful this point result in the merging(by samo structure mentioned of the temporarytoken&apos;s occurrence set with the identifiedtokPn&apos;s set, preserving all collected to that point about token. in the merge operation is the substitution all references to the, token by references to.the If, on land, the of inferencing only to candidate set of case (c.), the occurrence sets of the remaiRing are re-intersected, and if this the size of the set) set is to the temporary toRnv In either case progress has been made. Now .comes a key point. It any referents were in fact identified on this second attempt Ouaking.their entire occurrence sets accessible), or any candidate set decreases. caused new features to be associated temporary token, then there is the that inferences (which can make use.of the riewly-accessible.features) cah.be made. lne (72! the reflex&apos; to all memory structures which on monitor is not to duplicate on the first pass.) But a potential bnwoduct of thesecond pass is further feature generation wnicn can again restrict candi- This inference-reference interaction can proceed tintil no new narrowings or Identifications&apos; occur; hence.the&apos;term &amp;quot;relaxation cycle.&amp;quot; fig. 12 illustrates two examples&apos;of handled by the arid Appendix E contains the computer trace of the second example.</abstract>
<note confidence="0.893008583333333">46 EXAMPLE .1 Andy Rieger is a youngster. Andy Mooter is an adult. Andy&apos;s diaper is wet. Riecler&apos;sdiaper is wet. EXAMPLE 2 John was in Palo Alto yesterday. Jenny Jones was in Palo Alto yesterday. Jenny Smith was in France yesterday. Bill loves Jenny Jones. John kiss Jenny yesterday. INFERENCE-REFERENCE, FIRST PASS: It Was Jenny. Jones that John kissed.</note>
<author confidence="0.536238">Bill felt anger</author>
<abstract confidence="0.991286350993377">toward John FIGURE 12a Two examples of inference-reference interaction. I UTTERANCE I starting inference queue &apos;SUBPROP EXTRACTOR = = &gt; ( new but this time a6out #PETE17 -.■••■•■■■■■■•■•• *.) .••• Arr R EFeetuur temporary token reference ■■• .(NAME # PETE) fir6t pass ( * * * * second pass HE-REFERENCER inferred inferred INFERENCER Multiple reference-inference interactiont passes. FIGURE 12b inference-reference relaxation cycle. 47 Sense. Promotion end Implicit Concept Activation the ConceatualMemory Another Jyproduct of the generation of an abundance of probabilistic conceptual patterns from each input is that many related concepts and tokens implicitly inVolved.in the situation are activated, or &amp;quot;touched.&amp;quot; This can be put .to use in two ways. First, implicitly touched concepts can clarify what might otherwise be an utterly opaque Subsequent teference. If, for instance, someone says (outside of a particular context): &amp;quot;The nurses were nice&amp;quot;, you will probably inquire &amp;quot;What nurses?&amp;quot; If, on the other hana, someone says: &amp;quot;John was run over by a milk truck. When he woke up. the nurses were nice&amp;quot; you will experience neither doubt about the referents of &amp;quot;the nurses&amp;quot;, nor at their mention. I presume that filling-out of the situation &amp;quot;John was run over by.a milk trucW implicitly activates an set of relevantconcepts, &amp;quot;precharging&amp;quot; ideas of hospitals and their relation to patients. Other theories founded more on concept associationism than conceptual inference have suggested that such activation occurs through word-word or concept-concept free associations (see (A2) and (1Q1) for instance). While more direct an undoubted role in many language functions, it is my belief that these straight associative phenomena are not fundamentally powerful enough to explain the Rind of language behavio/ the nurse example. It is more often than not the of an utterance which restricts the kinds of meaningful associations a human makes. In contrast to the nurse example above, most people would agree that the reference to &amp;quot;the nurses&amp;quot; in the following situation is a bit peculiar: In the dark of the night, John had wallowed through the knee-deep mud to the north wall of the deserted animal hospital. The nurses were nice. 48 A simple hospital-nurses association model cannot account tor this. on other hand, those concepts tduched more. restrictive conceptual inference patterns would presumably be quite distant from the medical staff of a hospital in this example, thus explaining the incongruity. Related to this idea of concept activation through conceptual inference Structures is another mechanism which, I presume, underlies a camprehenders&apos; ability to select (almost unerringly) the proper senses of words in context dpring the linguistic analysis of each utterance. This is frequently called word sense prombtion,and its exact nature is one of the major conundrums of language analysis. It underlies our ability to avoid -almost totally -backing up to reinterpret words. It is as though at each moment during our comprehension we possess a dynamically shifting predisposition toward a unique sense of just about any word we are likely to hear next. Fig. 13 cdntaans some illustrations of this phenomenon. I have only a thought (which I plan to develop) on this issue. At each instant in the CM, there is a powerful inference momentum which is the product of conceptual inferences. -Obviously, these concepts which the inference patterns touch will. correspond to senses of words. These senses can be &amp;quot;promoted&amp;quot; in the same way implicit activation promotes referents.This is a partial explanation or word sense promotion. however.; that in additiop the CM had an independentparallel protook each inference as it and. mapped back into a nearlanguage &amp;quot;proto-sentence&amp;quot;, a linear sequence of concepts which is almost a sentence of the language, except that the actual word realizates of each concept have not yet been chosen. In other words, a generation process (Cl) for example) would be to inference, would be of the final lexical substitutions of word senses. By preall the of the words which could be a the CM would, have a word sense &amp;quot;set&amp;quot; which would be a function of the kind of restrictive inferential context which I feel is vital to the process of analysis. This. .idea is obviously but it might model a very real We catch ourselves subvocalizing what we expect to hear next (especially while listening to an annoyingly slow speaker), and this is tantalizing evidence that somelike a generator is upstairs. 49 1: (CONTEXT) asked Maty which piece of fruit she wanted: Lickedthe apple. versus (CONTEXT) Mary climbed the apple tree. pickedthe apple. 2: (CONTEXT) was in a meadow. The smelled good. versus (CONTEXT) John was looking forward to getting high The grass smellea EXAMPLE 3: (Riesbeck&apos;s example (R2)) John went on a hunting trip. shot two bucks. It was all he had: FIGURE 13a Examples of word sense promotion. I ncoming utterances ======&gt; analyzed graphs CONCEPTUAL ANALYZER .16% activated T T T worA &apos;senses are prefered by ttle analyzer :ND conceptual in ferencep from analyzed graphs the partial generator PABTIAL runs independently &lt; CONCEPTUAL from the memorw &lt; GENERATOR concep tua I structures pro;to-sentences which are the various Ways each inference might be expressed by languageA These involve many alternative word senses-. Mapping inferences back into proto-sentences, activating many word senses. Mapping inferences back into proto-sentences, activating many word senses.</abstract>
<intro confidence="0.892504">50</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Andy Rieger is a youngster. Andy Mooter is an adult. Andy&apos;s diaper is wet. INFERENCE—REFERENCE: Andy Riecler&apos;s diaper is wet. EXAMPLE 2 John was in Palo Alto yesterday. Jenny Jones was in Palo Alto yesterday.</title>
<journal>EXAMPLE</journal>
<volume>1</volume>
<marker></marker>
<rawString>EXAMPLE .1 Andy Rieger is a youngster. Andy Mooter is an adult. Andy&apos;s diaper is wet. INFERENCE—REFERENCE: Andy Riecler&apos;s diaper is wet. EXAMPLE 2 John was in Palo Alto yesterday. Jenny Jones was in Palo Alto yesterday.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jenny Smith</author>
</authors>
<title>was in France yesterday. Bill loves Jenny Jones. Bill saw John kiss Jenny yesterday.</title>
<marker>Smith, </marker>
<rawString>Jenny Smith was in France yesterday. Bill loves Jenny Jones. Bill saw John kiss Jenny yesterday.</rawString>
</citation>
<citation valid="false">
<authors>
<author>FIRST INFERENCE-REFERENCE</author>
</authors>
<title>PASS: It Was Jenny. Jones that John kissed.</title>
<marker>INFERENCE-REFERENCE, </marker>
<rawString>INFERENCE-REFERENCE, FIRST PASS: It Was Jenny. Jones that John kissed.</rawString>
</citation>
<citation valid="false">
<authors>
<author>SECOND INFEIZENCE-REFERENCE</author>
</authors>
<title>PASS: Bill felt anger toward</title>
<booktitle>John FIGURE 12a</booktitle>
<marker>INFEIZENCE-REFERENCE, </marker>
<rawString>INFEIZENCE-REFERENCE, SECOND PASS: Bill felt anger toward John FIGURE 12a</rawString>
</citation>
<citation valid="false">
<title>Two examples of inference-reference interaction.</title>
<marker></marker>
<rawString>Two examples of inference-reference interaction.</rawString>
</citation>
<citation valid="false">
<title>Multiple reference-inference interactiont passes. FIGURE 12b ThP. inference-reference relaxation cycle. which are the various Ways each inference might be expressed by languageA These involve many alternative word senses-. Mapping inferences back into proto-sentences, activating many word senses. FIGURE 13b Mapping inferences back into proto-sentences, activating many word senses.</title>
<marker></marker>
<rawString>Multiple reference-inference interactiont passes. FIGURE 12b ThP. inference-reference relaxation cycle. which are the various Ways each inference might be expressed by languageA These involve many alternative word senses-. Mapping inferences back into proto-sentences, activating many word senses. FIGURE 13b Mapping inferences back into proto-sentences, activating many word senses.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>