<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000857">
<note confidence="0.686908">
Proceedings of EACL &apos;99
</note>
<title confidence="0.89147">
Representing Text Chunks
</title>
<author confidence="0.876478">
Erik F. Tjong Kim Sang
</author>
<affiliation confidence="0.989126">
Center for Dutch Language and Speech
University of Antwerp
</affiliation>
<address confidence="0.966354">
Universiteitsplein 1
B-2610 Wilrijk, Belgium
</address>
<email confidence="0.994478">
erikt@uia.ac.be
</email>
<author confidence="0.920603">
Jorn Veenstra
</author>
<affiliation confidence="0.8451765">
Computational Linguistics
Tilburg University
</affiliation>
<address confidence="0.9452065">
P.O. Box 90153
5000 LE Tilburg, The Netherlands
</address>
<email confidence="0.99786">
veenstra@kub.nl
</email>
<sectionHeader confidence="0.99026" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835705882353">
Dividing sentences in chunks of words is
a useful preprocessing step for parsing,
information extraction and information
retrieval. (Ramshaw and Marcus, 1995)
have introduced a &amp;quot;convenient&amp;quot; data rep-
resentation for chunking by converting
it to a tagging task. In this paper we
will examine seven different data repre-
sentations for the problem of recogniz-
ing noun phrase chunks. We will show
that the the data representation choice
has a minor influence on chunking per-
formance. However, equipped with the
most suitable data representation, our
memory-based learning chunker was able
to improve the best published chunking
results for a standard data set.
</bodyText>
<sectionHeader confidence="0.995597" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986284">
The text corpus tasks parsing, information extrac-
tion and information retrieval can benefit from di-
viding sentences in chunks of words. (Ramshaw
and Marcus, 1995) describe an error-driven
transformation-based learning (TBL) method for
finding NP chunks in texts. NP chunks (or
baseNPs) are non-overlapping, non-recursive noun
phrases. In their experiments they have modeled
chunk recognition as a tagging task: words that
are inside a baseNP were marked I, words outside
a baseNP received an 0 tag and a special tag B was
used for the first word inside a baseNP immedi-
ately following another baseNP. A text example:
original:
In [N early trading NI in [N Hong Kong
N] [N Monday NI , [N gold N] was quoted
at [N $ 366.50 NI [N an ounce N] •
tagged:
</bodyText>
<equation confidence="0.97349875">
In/0 early/I trading/I in/0 Hong/I
Kong/I Monday/B gold/I was/0
quoted/0 at/0 $/1 366.50/1 an/B
ounce/I 10
</equation>
<bodyText confidence="0.999953409090909">
Other representations for NP chunking can be
used as well. An example is the representation
used in (Ratnaparkhi, 1998) where all the chunk-
initial words receive the same start tag (analo-
gous to the B tag) while the remainder of the
words in the chunk are paired with a different tag.
This removes tagging ambiguities. In the Ratna-
parkhi representation equal noun phrases receive
the same tag sequence regardless of the context in
which they appear.
The data representation choice might influence
the performance of chunking systems. In this pa-
per we discuss how large this influence is. There-
fore we will compare seven different data rep-
resentation formats for the baseNP recognition
task. We are particularly interested in finding out
whether with one of the representation formats
the best reported results for this task can be im-
proved. The second section of this paper presents
the general setup of the experiments. The results
ean be found in the third section. In the fourth
section we will describe some related work.
</bodyText>
<sectionHeader confidence="0.787105" genericHeader="introduction">
2 Methods and experiments
</sectionHeader>
<bodyText confidence="0.9995552">
In this section we present and explain the data
representation formats and the machine learning
algorithm that we have used. In the final part
we describe the feature representation used in our
experiments.
</bodyText>
<subsectionHeader confidence="0.990841">
2.1 Data representation
</subsectionHeader>
<bodyText confidence="0.999938">
We have compared four complete and three partial
data representation formats for the baseNP recog-
nition task presented in (Ramshaw and Marcus,
1995). The four complete formats all use an I tag
for words that are inside a baseNP and an 0 tag
for words that are outside a baseNP. They differ
</bodyText>
<page confidence="0.97785">
173
</page>
<equation confidence="0.600276">
Proceedings of EACL &apos;99
IOB1
I0B2
IOE1
10E2
I0
</equation>
<tableCaption confidence="0.912949">
Table 1: The chunk tag sequences for the example sentence In early trading in Hong Kong Monday ,
</tableCaption>
<bodyText confidence="0.947633777777777">
gold was quoted at S 366.50 an ounce . for seven different tagging formats. The I tag has been used
for words inside a baseNP, 0 for words outside a baseNP, B and E for baseNP-initial words and E and]
for baseNP-final words.
JOB&apos; The first word inside a baseNP
immediately following an-
other baseNP receives a B
tag (Ramshaw and Marcus,
1995).
I0B2 All baseNP-initial words receive a
B tag (Ratnaparkhi, 1998).
IOE1 The final word inside a baseNP
immediately preceding another
baseNP receives an E tag.
10E2 All baseNP-final words receive an
E tag.
We wanted to compare these data representa-
tion formats with a standard bracket representa-
tion. We have chosen to divide bracketing exper-
iments in two parts: one for recognizing opening
brackets and one for recognizing closing brackets.
Additionally we have worked with another partial
representation which seemed promising: a tag-
ging representation which disregards boundaries
between adjacent chunks. These boundaries can
be recovered by combining this format with one
of the bracketing formats. Our three partial rep-
resentations are:
All baseNP-initial words receive an
[ tag, other words receive a . tag.
All baseNP-final words receive a ]
tag, other words receive a . tag.
I0 Words inside a baseNP receive an I
tag, others receive an 0 tag.
These partial representations can be combined
in three pairs which encode the complete baseNP
structure of the data:
A word sequence is regarded as a
baseNP if the first word has re-
ceived an [ tag, the final word has
received a ] tag and these are the
only brackets that have been as-
signed to words in the sequence.
[ + JO In the 10 format, tags of words
that have received an I tag and an
[ tag are changed into B tags. The
result is interpreted as the 10B2
format.
10 + ] In the JO format, tags of words
that have received an I tag and a
] tag are changed into E tags. The
result is interpreted as the 10E2
format.
Examples of the four complete formats and the
three partial formats can be found in table 1.
</bodyText>
<subsectionHeader confidence="0.995698">
2.2 Memory-Based Learning
</subsectionHeader>
<bodyText confidence="0.989070608695652">
We have build a baseNP recognizer by training
a machine learning algorithm with correct tagged
data and testing it with unseen data. The ma-
chine learning algorithm we used was a Memory-
Based Learning algorithm (MBL). During train-
ing it stores a symbolic feature representation of
a word in the training data together with its classi-
fication (chunk tag). In the testing phase the algo-
rithm compares a feature representation of a test
word with every training data item and chooses
the classification of the training item which is clos-
est to the test item.
In the version of the algorithm that we have
used, is 1-IG, the distances between feature rep-
resentations are computed as the weighted sum
of distances between individual features (Daele-
mans et al., 1998). Equal features are defined to
have distance 0, while the distance between other
pairs is some feature-dependent value. This value
is equal to the information gain of the feature, an
information theoretic measure which contains the
in their treatment of chunk-initial and chunk-final [+1
words:
</bodyText>
<page confidence="0.980553">
174
</page>
<table confidence="0.992318777777778">
Proceedings of EACL &apos;99
word/POS context F0.1
IOB1 L=2 /R=1 89.17
10B2 L=2 /R= 1 88.76
10E1 L=1/R=2 88.67
10E2 L=2/R=2 89.01
[ + ] L=2/R=1 + L=0/R=2 89.32
[ + IO L=2/R=0 + L=1/R=1 89.43
IO +] L=1/R=1 + L=0/R=2 89.42
</table>
<tableCaption confidence="0.890088333333333">
Table 2: Results first experiment series: the best F0=1 scores for different left (L) and right (R)
word/POS tag pair context sizes for the seven representation formats using 5-fold cross-validation on
section 15 of the WSJ corpus.
</tableCaption>
<bodyText confidence="0.99732875">
normalized entropy decrease of the classification
set caused by the presence of the feature. Details
of the algorithm can be found in (Daelemans et
al., 1998)1.
</bodyText>
<subsectionHeader confidence="0.879918">
2.3 Representing words with features
</subsectionHeader>
<bodyText confidence="0.99996134375">
An important decision in an MBL experiment is
the choice of the features that will be used for
representing the data. is 1-IG is thought to be
less sensitive to redundant features because of the
data-dependent feature weighting that is included
in the algorithm. We have found that the presence
of redundant features has a negative influence on
the performance of the baseNP recognizer.
In (Ramshaw and Marcus, 1995) a set of trans-
formational rules is used for modifying the clas-
sification of words. The rules use context infor-
mation of the words, the part-of-speech tags that
have been assigned to them and the chunk tags
that are associated with them. We will use the
same information as in our feature representation
for words.
In TBL, rules with different context information
are used successively for solving different prob-
lems. We will use the same context information
for all data. The optimal context size will be
determined by comparing the results of different
context sizes on the training data. Here we will
perform four steps. We will start with testing dif-
ferent context sizes of words with their part-of-
speech tag. After this, we will use the classifica-
tion results of the best context size for determining
the optimal context size for the classification tags.
As a third step, we will evaluate combinations of
classification results and find the best combina-
tion. Finally we will examine the influence of an
MBL algorithm parameter: the number of exam-
ined nearest neighbors.
</bodyText>
<footnote confidence="0.9144435">
is a part of the TiMBL software package
which is available from http://ilk.kub.n1
</footnote>
<sectionHeader confidence="0.998332" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999975351351351">
We have used the baseNP data presented in
(Ramshaw and Marcus, 1995)2. This data was
divided in two parts. The first part was training
data and consisted of 211727 words taken from
sections 15, 16, 17 and 18 from the Wall Street
Journal corpus (WSJ). The second part was test
data and consisted of 47377 words taken from
section 20 of the same corpus. The words were
part-of-speech (POS) tagged with the Brill tagger
and each word was classified as being inside or
outside a baseNP with the IOB1 representation
scheme. The chunking classification was made by
(Ramshaw and Marcus, 1995) based on the pars-
ing information in the WSJ corpus.
The performance of the baseNP recognizer can
be measured in different ways: by computing
the percentage of correct classification tags (ac-
curacy), the percentage of recognized baseNPs
that are correct (precision) and the percentage of
baseNPs in the corpus that are found (recall). We
will follow (Argamon et al., 1998) and use a com-
bination of the precision and recall rates: Fo.--1 =
(2*precision*recall)/(precision+recall).
In our first experiment series we have tried to
discover the best word/part-of-speech tag context
for each representation format. For computational
reasons we have limited ourselves to working with
section 15 of the WSJ corpus. This section con-
tains 50442 words. We have run 5-fold cross-
validation experiments with all combinations of
left and right contexts of word/POS tag pairs in
the size range 0 to 4. A summary of the results
can be found in table 2.
The baseNP recognizer performed best with rel-
atively small word/POS tag pair contexts. Differ-
ent representation formats required different con-
text sizes for optimal performance. All formats
</bodyText>
<footnote confidence="0.986582">
2The data described in (Ramshaw
and Marcus, 1995) is available from
ftp://ftp.cis.upenn.edu/pub/chunker/
</footnote>
<page confidence="0.982773">
175
</page>
<table confidence="0.996797666666667">
Proceedings of EACL &apos;99
word/POS context chunk tag context P3=1
IOB1 L=2/R,=1 1/2 90.12
I0B2 L=2/R=1 1/0 89.30
10E1 L=1/R=2 1/2 89.55
10E2 L=1/R=2 0/1 89.73
[ +] L=2/R=1 + L=0/R=2 0/0 + 0/0 89.32
[ + IO L=2/R=0 + L=1/R=1 0/0 + 1/1 89.78
IO + ] L=1/R=1 + L=0/R=2 1/1 + 0/0 89.86
</table>
<tableCaption confidence="0.808076666666667">
Table 3: Results second experiment series: the best p3=1 scores for different left (L) and right (R)
chunk tag context sizes for the seven representation formats using 5-fold cross-validation on section 15
of the WSJ corpus.
</tableCaption>
<table confidence="0.9997045">
word/POS chunk tag combinations P3.1
IOB1 2/1 1/1 0/0 1/1 2/2 3/3 90.53
I0B2 2/1 1/0 2/1 89.30
10E1 1/2 1/2 0/0 1/1 2/2 3/3 90.03
10E2 1/2 0/1 1/2 89.73
[ + 1 2/1 + 0/2 0/0 + 0/0 - + - 89.32
[ + 10 2/0 + 1/1 0/0 + 1/1 - + 0/1 1/2 2/3 3/4 89.91
IO +] 1/1 + 0/2 1/1 + 0/0 0/1 1/2 2/3 3/4 + - 90.03
</table>
<tableCaption confidence="0.992954">
Table 4: Results third experiment series: the best P3=1 scores for different combinations of chunk tag
</tableCaption>
<bodyText confidence="0.983633694915254">
context sizes for the seven representation formats using 5-fold cross-validation on section 15 of the WSJ
corpus.
with explicit open bracket information preferred
larger left context and most formats with explicit
closing bracket information preferred larger right
context size. The three combinations of partial
representations systematically outperformed the
four complete representations. This is probably
caused by the fact that they are able to use two
different context sizes for solving two different
parts of the recognition problem.
In a second series of experiments we used a &amp;quot;cas-
caded&amp;quot; classifier. This classifier has two stages
(cascades). The first cascade is similar to the clas-
sifier described in the first experiment. For the
second cascade we added the classifications of the
first cascade as extra features. The extra features
consisted of the left and the right context of the
classification tags. The focus chunk tag (the clas-
sification of the current word) accounts for the cor-
rect classification in about 95% of the cases. The
MBL algorithm assigns a large weight to this in-
put feature and this makes it harder for the other
features to contribute to a good result. To avoid
this we have refrained from using this tag. Our
goal was to find out the optimal number of ex-
tra classification tags in the input. We performed
5-fold cross-validation experiments with all com-
binations of left and right classification tag con-
texts in the range 0 tags to 3 tags. A summary of
the results can be found in table 33. We achieved
higher P3=1 for all representations except for the
bracket pair representation.
The third experiment series was similar to the
second but instead of adding output of one ex-
periment we added classification results of three,
four or five experiments of the first series. By do-
ing this we supplied the learning algorithm with
information about different context sizes. This in-
formation is available to TBL in the rules which
use different contexts. We have limited ourselves
to examining all successive combinations of three,
four and five experiments of the lists (L=0/R=0,
1/1, 2/2, 3/3, 4/4), (0/1, 1/2, 2/3, 3/4) and (1/0,
2/1, 3/2, 4/3). A summary of the results can be
found in table 4. The results for four representa-
tion formats improved.
In the fourth experiment series we have exper-
imented with a different value for the number of
nearest neighbors examined by the iBl-IG algo-
rithm (parameter k). This algorithm standardly
uses the single training item closest to the test
3In a number of cases a different base configuration
in one experiment series outperformed the best base
configuration found in the previous series. In the sec-
ond series L/R=1/2 outperformed 2/2 for 10E2 when
chunk tags were added and in the third series chunk
tag context 1/1 outperformed 1/2 for IOB1 when dif-
ferent combinations were tested.
</bodyText>
<page confidence="0.996602">
176
</page>
<table confidence="0.994544222222222">
Proceedings of EACL &apos;99
word/POS chunk tag combinations F0=1
IOB1 3/3(k=3) 1/1 0/0(1) 1/1(1) 2/2(3) 3/3(3) 90.89 ± 0.63
I0B2 3/3(k=3) 1/0 3/3(3) 89.72 ± 0.79
IOE1 2/3(k=3) 1/2 0/0(1) 1/1(1) 2/2(3) 3/3(3) 90.12 ± 0.27
10E2 2/3(k=3) 0/1 2/3(3) 90.02 ± 0.48
[+ j 4/3(3) + 4/4(3) 0/0 + 0/0 - + - 90.08 ± 0.57
[ + IO 4/3(3) + 3/3(3) 0/0 + 1/1 - + 0/1(1) 1/2(3) 2/3(3) 3/4(3) 90.35 ± 0.75
10 + ] 3/3(3) + 2/3(3) 1/1 + 0/0 0/1(1) 1/2(3) 2/3(3) 3/4(3) + - 90.23 ± 0.73
</table>
<tableCaption confidence="0.995377">
Table 5: Results fourth experiment series: the best F0=1 scores for different combinations of left and
</tableCaption>
<bodyText confidence="0.979539840909091">
right classification tag context sizes for the seven representation formats using 5-fold cross-validation
on section 15 of the WSJ corpus obtained with iBl-IG parameter k=3. IOB1 is the best representation
format but the differences with the results of the other formats are not significant.
item. However (Daelemans et al., 1999) report
that for baseNP recognition better results can be
obtained by making the algorithm consider the
classification values of the three closest training
items. We have tested this by repeating the first
experiment series and part of the third experiment
series for k=3. In this revised version we have
repeated the best experiment of the third series
with the results for k=1 replaced by the k=3 re-
sults whenever the latter outperformed the first
in the revised first experiment series. The results
can be found in table 5. All formats benefited
from this step. In this final experiment series the
best results were obtained with IOB1 but the dif-
ferences with the results of the other formats are
not significant.
We have used the optimal experiment configura-
tions that we had obtained from the fourth experi-
ment series for processing the complete (Ramshaw
and Marcus, 1995) data set. The results can be
found in table 6. They are better than the results
for section 15 because more training data was used
in these experiments. Again the best result was
obtained with IOB1 (F0=1=92.37) which is an im-
provement of the best reported Fi3=1 rate for this
data set ((Rainshaw and Marcus, 1995): 92.03).
We would like to apply our learning approach
to the large data set mentioned in (Ramshaw and
Marcus, 1995): Wall Street Journal corpus sec-
tions 2-21 as training material and section 0 as
test material. With our present hardware apply-
ing our optimal experiment, configuration to this
data would require several months of computer
time. Therefore we have only used the best stage
1 approach with IOB1 tags: a left and right con-
text of three words and three POS tags combined
with k=3. This time the chunker achieved a p3=1
score of 93.81 which is half a point better than the
results obtained by (Ramshaw and Marcus, 1995):
93.3 (other chunker rates for this data: accuracy:
98.04%; precision: 93.71%; recall: 93.90%).
</bodyText>
<sectionHeader confidence="0.999155" genericHeader="related work">
4 Related work
</sectionHeader>
<bodyText confidence="0.999779358974359">
The concept of chunking was introduced by Ab-
ney in (Abney, 1991). He suggested to develop
a chunking parser which uses a two-part syntac-
tic analysis: creating word chunks (partial trees)
and attaching the chunks to create complete syn-
tactic trees. Abney obtained support for such a
chunking stage from psycholinguistic literature.
Ramshaw and Marcus used transformation-
based learning (TBL) for developing two chunkers
(Ramshaw and Marcus, 1995). One was trained
to recognize baseNPs and the other was trained
to recognize both NP chunks and VP chunks.
Ramshaw and Marcus approached the chunking
task as a tagging problem. Their baseNP training
and test data from the Wall Street Journal corpus
are still being used as benchmark data for current
chunking experiments. (Ramshaw and Marcus,
1995) shows that baseNP recognition (F0=1=92.0)
is easier than finding both NP and VP chunks
(F0=1=88.1) and that increasing the size of the
training data increases the performance on the
test set.
The work by Ramshaw and Marcus has inspired
three other groups to build chunking algorithms.
(Argamon et al., 1998) introduce Memory-Based
Sequence Learning and use it for different chunk-
ing experiments. Their algorithm stores sequences
of POS tags with chunk brackets and uses this in-
formation for recognizing chunks in unseen data.
It performed slightly worse on baseNP recognition
than the (Ramshaw and Marcus, 1995) experi-
ments (F0=1=91.6). (Cardie and Pierce, 1998)
uses a related method but they only store POS
tag sequences forming complete baseNPs. These
sequences were applied to unseen tagged data af-
ter which post-processing repair rules were used
for fixing some frequent errors. This approach
performs worse than other reported approaches
(F0=1=90.9).
</bodyText>
<page confidence="0.993235">
177
</page>
<table confidence="0.995463538461539">
Proceedings of EACL &apos;99
accuracy precision recall Fo=1
IOB1 97.58% 92.50% 92.25% 92.37
10B2 96.50% 91.24% 92.32% 91.78
IOE1 97.58% 92.41% 92.04% 92.23
10E2 96.77% 91.93% 92.46% 92.20
[ ± ] - 93.66% 90.81% 92.22
[ + JO - 91.47% 92.61% 92.04
10 + ] .. 91.25% 92.54% 91.89
(Ramshaw and Marcus, 1995) 97.37% 91.80% 92.27% 92.03
(Veenstra, 1998) 97.2% 89.0% 94.3% 91.6
(Argamon et al., 1998) - 91.6 % 91.6% 91.6
(Cardie and Pierce, 1998) - 90.7% 91.1% 90.9
</table>
<tableCaption confidence="0.963956">
Table 6: The Fo=1 scores for the (Ramshaw and Marcus, 1995) test set after training with their
</tableCaption>
<bodyText confidence="0.982238238095238">
training data set. The data was processed with the optimal input feature combinations found in the
fourth experiment series. The accuracy rate contains the fraction of chunk tags that was correct. The
other three rates regard baseNP recognition. The bottom part of the table shows some other reported
results with this data set. With all but two formats isl-IG achieves better Fo=1 rates than the best
published result in (Ramshaw and Marcus, 1995).
(Veenstra, 1998) uses cascaded decision tree
learning (IGTree) for baseNP recognition. This al-
gorithm stores context information of words, POS
tags and chunking tags in a decision tree and clas-
sifies new items by comparing them to the training
items. The algorithm is very fast and it reaches
the same performance as (Argamon et al., 1998)
(F0=1=91.6). (Daelemans et al., 1999) uses cas-
caded MBL (rB1-IG) in a similar way for several
tasks among which baseNP recognition. They do
not report Fo=i rates but their tag accuracy rates
are a lot better than accuracy rates reported by
others. However, they use the (R,amshaw and
Marcus, 1995) data set in a different, training-test
division (10-fold cross validation) which makes it
difficult to compare their results with others.
</bodyText>
<sectionHeader confidence="0.99069" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999950037037037">
We have compared seven different data formats
for the recognition of baseNPs with memory-based
learning (lB1-10). The I0B1 format, introduced
in (Ramshaw and Marcus, 1995), consistently
came out as the best format. However, the dif-
ferences with other formats were not significant.
Some representation formats achieved better pre-
cision rates, others better recall rates. This infor-
mation is useful for tasks that require chunking
structures because some tasks might, be more in-
terested in high precision rates while others might
be more interested in high recall rates.
The 031-1G algorithm has been able to im-
prove the best reported Fo=1 rates for a stan-
dard data set, (92.37 versus (Ramshaw and Mar-
cus, 1995)&apos;s 92.03). This result was aided by us-
ing non-standard parameter values (k=3) and the
algorithm was sensitive for redundant input fea-
tures. This means that finding an optimal per-
formance or this task requires searching a large
parameter/feature configuration space. An inter-
esting topic for future research would be to embed
Isl-IG in a standard search algorithm, like hill-
climbing, and explore this parameter space. Some
more room for improved performance lies in com-
puting the POS tags in the data with a better
tagger than presently used.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9986373">
Steven Abney. 1991. Parsing by chunks.
In Principle-Based Parsing. Kluwer Academic
Publishers,.
Shlomo Argamon, Ido Dagan, and Yuval Kry-
molowski. 1998. A memory-based approach to
learning shallow natural language patterns. In
Proceedings of the 17th International Confer-
ence on Computational Linguistics (COLING-
ACL &apos;98).
Claire Cardie and David Pierce. 1998. Error-
driven pruning of treebank grammars for base
noun phrase identification. In Proceedings of
the 17th International Conference on Compu-
tational Linguistics (COLING-ACL &apos;98).
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 1998.
TiMBL: Tilburg Memory Based Learner
- version 1.0 - Reference Guide. ILK,
Tilburg University, The Netherlands.
http: / / ilk .kub .n1 /- ilk /papers / ilk9803 .ps.gz
</reference>
<page confidence="0.982061">
178
</page>
<reference confidence="0.992789">
Proceedings-of EACL &apos;99
Walter Daelemans, Antal van den Bosch, and
Jakub Zavrel. 1999. Forgetting exceptions is
harmful in language learning. Machine Learn-
ing, 11.
Lance A. R,amshaw and Mitchell P. Marcus.
1995. Text chunking using transformation-
based learning. In Proceedings of the Third
ACL Workshop on Very Large Corpora.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Reso-
lution. PhD thesis Computer and Information
Science, University of Pennsylvania.
Jorn Veenstra. 1998. Fast np chunking us-
ing memory-based learning techniques. In
BENELEARN-98: Proceedings of the Eigth
Belgian-Dutch Conference on Machine Learn-
ing. ATO-DLO, Wageningen, report 352.
</reference>
<page confidence="0.998803">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495554">
<note confidence="0.815696">Proceedings of EACL &apos;99</note>
<title confidence="0.998836">Representing Text Chunks</title>
<author confidence="0.999971">Erik F Tjong Kim Sang</author>
<affiliation confidence="0.9994055">Center for Dutch Language and Speech University of Antwerp</affiliation>
<address confidence="0.8151895">Universiteitsplein 1 B-2610 Wilrijk, Belgium</address>
<email confidence="0.97603">erikt@uia.ac.be</email>
<author confidence="0.999519">Jorn Veenstra</author>
<affiliation confidence="0.9986625">Computational Linguistics Tilburg University</affiliation>
<address confidence="0.9994605">P.O. Box 90153 5000 LE Tilburg, The Netherlands</address>
<email confidence="0.995681">veenstra@kub.nl</email>
<abstract confidence="0.999570944444444">Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a &amp;quot;convenient&amp;quot; data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks. In Principle-Based Parsing.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers,.</publisher>
<contexts>
<context position="17365" citStr="Abney, 1991" startWordPosition="2938" endWordPosition="2939">as test material. With our present hardware applying our optimal experiment, configuration to this data would require several months of computer time. Therefore we have only used the best stage 1 approach with IOB1 tags: a left and right context of three words and three POS tags combined with k=3. This time the chunker achieved a p3=1 score of 93.81 which is half a point better than the results obtained by (Ramshaw and Marcus, 1995): 93.3 (other chunker rates for this data: accuracy: 98.04%; precision: 93.71%; recall: 93.90%). 4 Related work The concept of chunking was introduced by Abney in (Abney, 1991). He suggested to develop a chunking parser which uses a two-part syntactic analysis: creating word chunks (partial trees) and attaching the chunks to create complete syntactic trees. Abney obtained support for such a chunking stage from psycholinguistic literature. Ramshaw and Marcus used transformationbased learning (TBL) for developing two chunkers (Ramshaw and Marcus, 1995). One was trained to recognize baseNPs and the other was trained to recognize both NP chunks and VP chunks. Ramshaw and Marcus approached the chunking task as a tagging problem. Their baseNP training and test data from t</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In Principle-Based Parsing. Kluwer Academic Publishers,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Ido Dagan</author>
<author>Yuval Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics (COLINGACL &apos;98).</booktitle>
<contexts>
<context position="9841" citStr="Argamon et al., 1998" startWordPosition="1649" endWordPosition="1652">he same corpus. The words were part-of-speech (POS) tagged with the Brill tagger and each word was classified as being inside or outside a baseNP with the IOB1 representation scheme. The chunking classification was made by (Ramshaw and Marcus, 1995) based on the parsing information in the WSJ corpus. The performance of the baseNP recognizer can be measured in different ways: by computing the percentage of correct classification tags (accuracy), the percentage of recognized baseNPs that are correct (precision) and the percentage of baseNPs in the corpus that are found (recall). We will follow (Argamon et al., 1998) and use a combination of the precision and recall rates: Fo.--1 = (2*precision*recall)/(precision+recall). In our first experiment series we have tried to discover the best word/part-of-speech tag context for each representation format. For computational reasons we have limited ourselves to working with section 15 of the WSJ corpus. This section contains 50442 words. We have run 5-fold crossvalidation experiments with all combinations of left and right contexts of word/POS tag pairs in the size range 0 to 4. A summary of the results can be found in table 2. The baseNP recognizer performed bes</context>
<context position="18402" citStr="Argamon et al., 1998" startWordPosition="3099" endWordPosition="3102">Ps and the other was trained to recognize both NP chunks and VP chunks. Ramshaw and Marcus approached the chunking task as a tagging problem. Their baseNP training and test data from the Wall Street Journal corpus are still being used as benchmark data for current chunking experiments. (Ramshaw and Marcus, 1995) shows that baseNP recognition (F0=1=92.0) is easier than finding both NP and VP chunks (F0=1=88.1) and that increasing the size of the training data increases the performance on the test set. The work by Ramshaw and Marcus has inspired three other groups to build chunking algorithms. (Argamon et al., 1998) introduce Memory-Based Sequence Learning and use it for different chunking experiments. Their algorithm stores sequences of POS tags with chunk brackets and uses this information for recognizing chunks in unseen data. It performed slightly worse on baseNP recognition than the (Ramshaw and Marcus, 1995) experiments (F0=1=91.6). (Cardie and Pierce, 1998) uses a related method but they only store POS tag sequences forming complete baseNPs. These sequences were applied to unseen tagged data after which post-processing repair rules were used for fixing some frequent errors. This approach performs </context>
<context position="20392" citStr="Argamon et al., 1998" startWordPosition="3423" endWordPosition="3426">chunk tags that was correct. The other three rates regard baseNP recognition. The bottom part of the table shows some other reported results with this data set. With all but two formats isl-IG achieves better Fo=1 rates than the best published result in (Ramshaw and Marcus, 1995). (Veenstra, 1998) uses cascaded decision tree learning (IGTree) for baseNP recognition. This algorithm stores context information of words, POS tags and chunking tags in a decision tree and classifies new items by comparing them to the training items. The algorithm is very fast and it reaches the same performance as (Argamon et al., 1998) (F0=1=91.6). (Daelemans et al., 1999) uses cascaded MBL (rB1-IG) in a similar way for several tasks among which baseNP recognition. They do not report Fo=i rates but their tag accuracy rates are a lot better than accuracy rates reported by others. However, they use the (R,amshaw and Marcus, 1995) data set in a different, training-test division (10-fold cross validation) which makes it difficult to compare their results with others. 5 Concluding remarks We have compared seven different data formats for the recognition of baseNPs with memory-based learning (lB1-10). The I0B1 format, introduced </context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>Shlomo Argamon, Ido Dagan, and Yuval Krymolowski. 1998. A memory-based approach to learning shallow natural language patterns. In Proceedings of the 17th International Conference on Computational Linguistics (COLINGACL &apos;98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>David Pierce</author>
</authors>
<title>Errordriven pruning of treebank grammars for base noun phrase identification.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics (COLING-ACL &apos;98).</booktitle>
<contexts>
<context position="18757" citStr="Cardie and Pierce, 1998" startWordPosition="3152" endWordPosition="3155">.0) is easier than finding both NP and VP chunks (F0=1=88.1) and that increasing the size of the training data increases the performance on the test set. The work by Ramshaw and Marcus has inspired three other groups to build chunking algorithms. (Argamon et al., 1998) introduce Memory-Based Sequence Learning and use it for different chunking experiments. Their algorithm stores sequences of POS tags with chunk brackets and uses this information for recognizing chunks in unseen data. It performed slightly worse on baseNP recognition than the (Ramshaw and Marcus, 1995) experiments (F0=1=91.6). (Cardie and Pierce, 1998) uses a related method but they only store POS tag sequences forming complete baseNPs. These sequences were applied to unseen tagged data after which post-processing repair rules were used for fixing some frequent errors. This approach performs worse than other reported approaches (F0=1=90.9). 177 Proceedings of EACL &apos;99 accuracy precision recall Fo=1 IOB1 97.58% 92.50% 92.25% 92.37 10B2 96.50% 91.24% 92.32% 91.78 IOE1 97.58% 92.41% 92.04% 92.23 10E2 96.77% 91.93% 92.46% 92.20 [ ± ] - 93.66% 90.81% 92.22 [ + JO - 91.47% 92.61% 92.04 10 + ] .. 91.25% 92.54% 91.89 (Ramshaw and Marcus, 1995) 97.3</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>Claire Cardie and David Pierce. 1998. Errordriven pruning of treebank grammars for base noun phrase identification. In Proceedings of the 17th International Conference on Computational Linguistics (COLING-ACL &apos;98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner - version 1.0 - Reference Guide. ILK,</title>
<date>1998</date>
<booktitle>The Netherlands. http: / / ilk .kub .n1 /- ilk /papers / ilk9803 .ps.gz Proceedings-of EACL &apos;99</booktitle>
<institution>Tilburg University,</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 1998</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1998. TiMBL: Tilburg Memory Based Learner - version 1.0 - Reference Guide. ILK, Tilburg University, The Netherlands. http: / / ilk .kub .n1 /- ilk /papers / ilk9803 .ps.gz Proceedings-of EACL &apos;99</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
<author>Jakub Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>11</volume>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>Walter Daelemans, Antal van den Bosch, and Jakub Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Lance</author>
<author>amshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformationbased learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora.</booktitle>
<marker>Lance, amshaw, Marcus, 1995</marker>
<rawString>Lance A. R,amshaw and Mitchell P. Marcus. 1995. Text chunking using transformationbased learning. In Proceedings of the Third ACL Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>PhD thesis</tech>
<institution>Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="1967" citStr="Ratnaparkhi, 1998" startWordPosition="308" endWordPosition="309">they have modeled chunk recognition as a tagging task: words that are inside a baseNP were marked I, words outside a baseNP received an 0 tag and a special tag B was used for the first word inside a baseNP immediately following another baseNP. A text example: original: In [N early trading NI in [N Hong Kong N] [N Monday NI , [N gold N] was quoted at [N $ 366.50 NI [N an ounce N] • tagged: In/0 early/I trading/I in/0 Hong/I Kong/I Monday/B gold/I was/0 quoted/0 at/0 $/1 366.50/1 an/B ounce/I 10 Other representations for NP chunking can be used as well. An example is the representation used in (Ratnaparkhi, 1998) where all the chunkinitial words receive the same start tag (analogous to the B tag) while the remainder of the words in the chunk are paired with a different tag. This removes tagging ambiguities. In the Ratnaparkhi representation equal noun phrases receive the same tag sequence regardless of the context in which they appear. The data representation choice might influence the performance of chunking systems. In this paper we discuss how large this influence is. Therefore we will compare seven different data representation formats for the baseNP recognition task. We are particularly intereste</context>
<context position="3976" citStr="Ratnaparkhi, 1998" startWordPosition="653" endWordPosition="654">and an 0 tag for words that are outside a baseNP. They differ 173 Proceedings of EACL &apos;99 IOB1 I0B2 IOE1 10E2 I0 Table 1: The chunk tag sequences for the example sentence In early trading in Hong Kong Monday , gold was quoted at S 366.50 an ounce . for seven different tagging formats. The I tag has been used for words inside a baseNP, 0 for words outside a baseNP, B and E for baseNP-initial words and E and] for baseNP-final words. JOB&apos; The first word inside a baseNP immediately following another baseNP receives a B tag (Ramshaw and Marcus, 1995). I0B2 All baseNP-initial words receive a B tag (Ratnaparkhi, 1998). IOE1 The final word inside a baseNP immediately preceding another baseNP receives an E tag. 10E2 All baseNP-final words receive an E tag. We wanted to compare these data representation formats with a standard bracket representation. We have chosen to divide bracketing experiments in two parts: one for recognizing opening brackets and one for recognizing closing brackets. Additionally we have worked with another partial representation which seemed promising: a tagging representation which disregards boundaries between adjacent chunks. These boundaries can be recovered by combining this format</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. PhD thesis Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorn Veenstra</author>
</authors>
<title>Fast np chunking using memory-based learning techniques.</title>
<date>1998</date>
<booktitle>In BENELEARN-98: Proceedings of the Eigth Belgian-Dutch Conference on Machine Learning. ATO-DLO,</booktitle>
<pages>352</pages>
<location>Wageningen, report</location>
<contexts>
<context position="19396" citStr="Veenstra, 1998" startWordPosition="3258" endWordPosition="3259">ut they only store POS tag sequences forming complete baseNPs. These sequences were applied to unseen tagged data after which post-processing repair rules were used for fixing some frequent errors. This approach performs worse than other reported approaches (F0=1=90.9). 177 Proceedings of EACL &apos;99 accuracy precision recall Fo=1 IOB1 97.58% 92.50% 92.25% 92.37 10B2 96.50% 91.24% 92.32% 91.78 IOE1 97.58% 92.41% 92.04% 92.23 10E2 96.77% 91.93% 92.46% 92.20 [ ± ] - 93.66% 90.81% 92.22 [ + JO - 91.47% 92.61% 92.04 10 + ] .. 91.25% 92.54% 91.89 (Ramshaw and Marcus, 1995) 97.37% 91.80% 92.27% 92.03 (Veenstra, 1998) 97.2% 89.0% 94.3% 91.6 (Argamon et al., 1998) - 91.6 % 91.6% 91.6 (Cardie and Pierce, 1998) - 90.7% 91.1% 90.9 Table 6: The Fo=1 scores for the (Ramshaw and Marcus, 1995) test set after training with their training data set. The data was processed with the optimal input feature combinations found in the fourth experiment series. The accuracy rate contains the fraction of chunk tags that was correct. The other three rates regard baseNP recognition. The bottom part of the table shows some other reported results with this data set. With all but two formats isl-IG achieves better Fo=1 rates than </context>
</contexts>
<marker>Veenstra, 1998</marker>
<rawString>Jorn Veenstra. 1998. Fast np chunking using memory-based learning techniques. In BENELEARN-98: Proceedings of the Eigth Belgian-Dutch Conference on Machine Learning. ATO-DLO, Wageningen, report 352.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>