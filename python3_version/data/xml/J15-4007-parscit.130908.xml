<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<title confidence="0.9471545">
Lifetime Achievement Award
Translating Today into Tomorrow
</title>
<author confidence="0.996158">
Sheng Li
</author>
<affiliation confidence="0.913258">
Harbin Institute of Technology
</affiliation>
<bodyText confidence="0.999671307692308">
Good afternoon, ladies and gentlemen. I am standing here, grateful, excited, and proud.
I see so many friends, my colleagues, students, and many more researchers in this room.
I see that the work we started 50 years ago is now flourishing and is embedded in
people’s everyday lives. I see for the first time that the ACL conference is held here
in Beijing, China. And I am deeply honored to be awarded the Lifetime Achievement
Award of 2015.
I want to thank the ACL for giving me the Lifetime Achievement Award of 2015. It is
the appreciation of not only my work, but also of the work that my fellow researchers,
my colleagues, and my students have done through all these years. It is an honor for
all of us. As a veteran of NLP research, I am fortunate to witness and be a part of its
long yet inspiring journey in China. So today, to everyone here, my friends, colleagues,
and students, either well-known scientists or young researchers: I’d like to share my
experience and thoughts with you.
</bodyText>
<sectionHeader confidence="0.810469" genericHeader="method">
1. Early Machine Translation in China
</sectionHeader>
<bodyText confidence="0.999962235294118">
The history of machine translation (MT) in China dates back to 1956. At that time the
new country had immense construction projects to recover what had been ruined in
the war. However, the government precisely recognized the significance of machine
translation, and started to explore this area, as the fourth country following the United
States, the United Kingdom, and the Soviet Union. In 1959, Russian–Chinese machine
translation was demonstrated on a Type-104 general-purpose computer made in China.
This first MT system had a dictionary of 2,030 entries, and 29 groups of rules for
lexical analysis. Programmed by machine instructions, the system was able to translate
nine different types of sentences. It used punched tape as the input, and the output
was a special kind of code for Chinese characters, since there was no Chinese char-
acter output device at the time. As the pioneer in Chinese MT, the system touched
the issues of word sense disambiguation, word reordering, and proposed the idea of
predicate-focused sentence analysis and pivot language for multilingual translation.
In the same year, machine translation research at the Harbin Institute of Technol-
ogy (HIT) was started by Prof. Zhen Wang (and later Prof. Kaizhu Wang), focusing
on the Russian–Chinese MT group. The pursuit for MT has never halted after these
forerunners.
</bodyText>
<note confidence="0.811877666666667">
doi:10.1162/COLI a 00240
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 4
</note>
<sectionHeader confidence="0.980629" genericHeader="method">
2. The CEMT Series
</sectionHeader>
<bodyText confidence="0.998804393939394">
In 1960, I was admitted to HIT. Five years later, I graduated and became a faculty
member in the computer department of HIT, which was probably the first computer
discipline among Chinese universities. I started my research, however, not from ma-
chine translation but from information retrieval (IR). I was fully occupied by how to
effectively store books and documents on computers, and then retrieve them quickly
and accurately. The start of my research in MT was incidentally caused by IR problems.
At that time, Ming Zhou was my Ph.D. student. He is now the principal researcher
of Natural Language Computing at Microsoft Research Asia (MSRA), and many of
you may be acquainted with him. In 1985, at the beginning of his graduate study, he
was aiming to address the topic of word extraction for Chinese documents to boost IR
performance. For an exhaustive survey, Ming went to Beijing from Harbin alone, and
buried himself at the National Library for over a month. He came back disappointed,
finding that the related work was some language-dependent solutions for English.
Actually, many research directions encountered this problem at that time. That’s why
Ming and I decided to develop an MT system through which we could first translate
Chinese materials into English, so as to take advantage of the solutions proposed for
English, and finally translate the results back into Chinese, if necessary.
In those years, the translation from Chinese to other foreign languages was less
studied in China. Everything was hard in the beginning. We had to build everything
from scratch, such as collecting and inputting each entry of the translation dictionary.
Fortunately, we were not alone. I came to know many peer scholars, including Prof.
Weitian Wu, Zhiwei Feng, Prof. Zhendong Dong, Prof. Shiwen Yu, and Prof. Changning
Huang, as well as Dr. Zhaoxiong Chen. Although we didn’t work together, we could
always learn from each other and inspire each other in MT research.
After three years’ effort, we accomplished a rule-based MT system named CEMT-I
(Li et al. 1988). It ran on an IBM PC XT1 and was capable of translating eight kinds
of Chinese sentence patterns with fewer than one thousand rules. It had a dictionary
of 30,000 Chinese-English entries. Simple or even crude as it now seems, it really
encouraged every member of our team. After that, we developed CEMT-II (Zhou et al.
1990) and CEMT-III (Zhao, Li, and Zhang 1995) successively. The CEMT series seemed
to have a special kind of magic. Almost all the students who participated in these
projects devoted themselves to machine translation in their following careers, including
Ming Zhou, Min Zhang, and Tiejun Zhao.
</bodyText>
<listItem confidence="0.549969">
3. DEAR and BT863
</listItem>
<bodyText confidence="0.990011777777778">
Inspired by the success of the CEMT series, we also developed a computer-aided
translation system called “DEAR.” DEAR was put to market via a software chain store.
Although it did not sell much, it was our first effort to commercialize the MT technology.
I still remember how excited I was when I saw DEAR placed on the shelves for the first
time. Today, it still reminds me that research work cannot just stay in the lab.
Also in the 1980s, China’s NLP field was marked by a milestone event: the establish-
ment of the Chinese Information Processing Society of China (CIPS). From then on, NLP
researchers throughout the country have been connected and the academic exchange
has been facilitated at the national scale. It was far beyond my imagination then that,
</bodyText>
<footnote confidence="0.96268">
1 https://en.wikipedia.org/wiki/IBM_Personal_Computer_XT.
</footnote>
<page confidence="0.987829">
710
</page>
<bodyText confidence="0.9714914">
Li Translating Today into Tomorrow
thirty years later, I would have the honor to be the president of this society, leading
it to keep on contributing to the development of world-level NLP technology.
I usually regard the series of MT systems that we developed as a large family. In
1994, BT863 joined this family with some new features (Zhao, Li, and Wang 1995; Wang
et al. 1997). First, BT863 was distinguished as a bi-direction translation between Chinese
and English under a uniform architecture. Second, in addition to the rules, it was
augmented with examples and templates learned from a corpus. Finally, this system
is remembered for its top performance in the early national MT evaluation organized
by the 863 High Tech Program of China.
</bodyText>
<sectionHeader confidence="0.809623" genericHeader="method">
4. Syntactic and Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999959605263158">
Time passed quickly. The rising of the Internet made communication more convenient,
and our research was gradually connected with international peers. We concentrated
on the mining and accumulation of bilingual and multilingual corpora. We explored
how to integrate rule-based and example-based MT models under a unifying statistical
framework. However, as more and more work was conducted, I found it more difficult
to go deeper. I began to realize that translation problems cannot rely only on translation
methods.
From word segmentation, morphology, word meaning, to named entity, syntax, and
semantics, every step in this procedure affects the quality of translation. I remember an
interesting story. One day, my student Wanxiang Che input his name into our machine
translation system. The system literally translated his name into ‘thousands of cars
flying in the sky’. This was rated as the joke of the year in my lab, but the underlying
problem is worth pondering.
Traditional Chinese medicine advocates the treatment of both symptoms and root
causes. The same principle applies to MT research, in which models for word alignment,
decoding, reordering, and so forth, can solve the surface problems of machine transla-
tion, whereas understanding the word sense, sentence structure, and semantics is the
solution to the fundamental problems. We therefore carried out research on syntactic
analysis, including phrase-structure parsing and dependency parsing.
In those days, dependency parsing on Chinese was not widely studied. There was
no well-accepted annotation or transformed standard. Therefore, we referred to a large
number of linguistic studies, developed a Chinese syntactic dependency annotation
standard, and annotated a 50,000-sentence Chinese syntactic dependency treebank on
this basis. This is the largest Chinese dependency treebank available. Differently from
those transformed from phrase-structure treebanks, our dependency structure uses
native dependency structure, which can handle a large number of specific grammatical
phenomena in dependency structures. This treebank has been released by the Linguistic
Data Consortium (LDC) (Che, Li, and Liu 2012). We hope that more researchers can
benefit from it.
Based on syntactic parsing, we hoped to further explore the semantic structure
and the relationship of sentences. Therefore, we carried out research on semantic role
labeling, and worked on the semantic role labeling methods based on the tree kernel,
including the hybrid convolution tree kernel (Che et al. 2008) and the grammar-driven
tree kernel (Zhang et al. 2007). In addition, we further broadened our mind and tried to
analyze the semantics of Chinese directly. We proposed semantic dependency parsing
tasks that directly establish semantic-level dependencies between content words, ignor-
ing auxiliaries and prepositions. Meanwhile, we violated the tree structure constraints,
allowing one word to depend on more than one parent node, so as to form semantic
</bodyText>
<page confidence="0.971719">
711
</page>
<figure confidence="0.861751">
Computational Linguistics Volume 41, Number 4
</figure>
<figureCaption confidence="0.993755">
Figure 1
</figureCaption>
<bodyText confidence="0.988059428571428">
Example of syntactic dependency parsing, semantic role labeling, and semantic dependency
parsing in LTP-Cloud.
dependency graph structures. At this point, the semantic dependency treebank that we
have already labeled has reached more than 30,000 sentences. Much ongoing research
is based on these data. Figure 1 shows an example of syntactic dependency parsing,
semantic role labeling, and semantic dependency parsing for an input sentence “现在 /
她 / 脸色 / 难看 / , / 好像 / 病了 / 。 [Now she looks terrible, seems to be sick]” .
</bodyText>
<sectionHeader confidence="0.936191" genericHeader="method">
5. ITP and ITP-Cloud
</sectionHeader>
<bodyText confidence="0.999945722222222">
Every summer, HIT and MSRA would jointly organize a summer school for NLP
research students. We invited domestic and foreign experts to give lectures to Chinese
students engaged in this field. Because the summer school was free, students from
all over the country came together every year, listening to lectures and conducting
experiments. When I communicated with these students, I found that many of them
came from labs that lacked fundamental NLP tools, such as word segmentors, part-
of-speech taggers, and syntactic parsers. It would have been very difficult for them to
implement their research ideas without these tools. I felt bad when I saw that. They are
all students with dreams and innovative ideas. We must create a level playing field for
all of them, I thought.
After coming back from the summer school, I met Ting Liu. He is a strong supporter
of the idea of sharing. We decided to release an open-source NLP system: Language
Technology Platform (LTP). This platform integrates several Chinese NLP basic tech-
nologies, including Chinese word segmentation, part-of-speech tagging, named entity
recognition, dependency parsing, and semantic role labeling, which has made great
contributions to the development of further applications.
In recent years, we realized that cloud computing and the mobile Internet have
brought great opportunities and challenges to the NLP field. Therefore, we developed
</bodyText>
<page confidence="0.921669">
712
</page>
<bodyText confidence="0.956779">
Li Translating Today into Tomorrow
LTP-cloud2 in 2013, which provides accurate and fast NLP service via the Internet.
Currently, the number of LTP-cloud registered users has exceeded 3,000, and most of
them are NLP beginners. As I had wished, they no longer need to build an NLP basic
processing system from scratch for their research. Every time I see the thank-you notes
to the LTP and LTP-cloud in the acknowledgments of their papers, I am proud and
grateful.
</bodyText>
<sectionHeader confidence="0.924424" genericHeader="method">
6. Machine Translation on the Internet
</sectionHeader>
<bodyText confidence="0.999975081081081">
As more and more papers were published in top conferences and journals, our lab
made a name in the academic world. Many people in the lab were satisfied, but I felt
differently, since publishing papers should not be the major objective for research. New
models and techniques should be applied to solve real-world problems and improve
people’s daily lives. Particularly since we have moved into the era of the Internet. Many
new concepts and ideas have come into being, such as big data and cloud computing. In
such a new era, machine translation research should no longer be restricted to the labs,
running experiments on a small parallel corpus. Instead, it should embrace the Internet,
and embrace big data. We paid great attention to the cooperation with IT and Internet
companies. We established a joint lab with MSRA right after it was founded. After
that, we have also established joint labs with other companies, like IBM, Baidu, and
Tencent.
My student Haifeng Wang is the vice president of Baidu. He is in charge of NLP
research and development, as well as Web search. We decided to collaborate in MT
shortly after he joined Baidu, since Baidu can provide a huge platform for us to verify
our ideas. Together with Tsinghua University, Zhejiang University, the Institute of Com-
puting Technology, and the Institute of Automation of the Chinese Academy of Science,
we successfully applied for an 863 project titled “Machine Translation on the Internet.”
All the members participating in this project have great passion for MT technologies
and products.
Chinese people accept the principle that “取ZT民,用ZT民” [what is taken
from the people should be used for the interests of the people]. Internet-based machine
translation also follows this principle, which mines a large volume of translation data
from the Internet, trains the translation model, and then provides high-quality services
for Internet users. In our online translation system, taking Chinese–English translation,
for example, there are hundreds of millions of parallel data for this language pair, which
were filtered from billions of raw parallel data. We collected a large amount of data from
hundreds of billions of Web pages. So I should say our MT service is actually built upon
the whole Internet.
We have designed various mining models for these heterogeneous Internet data
sources, including bilingual parallel pages, bilingual comparable pages, Web pages
containing aligned sentence pairs, as well as plain texts containing entity and termi-
nology translations. The mined translation data are filtered and refined. We set different
updating frequencies for different Web sites, so as to guarantee that the latest data can be
included. I often observe the mined translation data by myself, and I can find plenty of
wonderful translations generated by ordinary Internet users. Their wisdom is perfectly
integrated into the translation system. However, how to make use of such a big corpus?
</bodyText>
<footnote confidence="0.894009">
2 http://www.ltp-cloud.com/demo/.
</footnote>
<page confidence="0.996293">
713
</page>
<figure confidence="0.78506">
Computational Linguistics Volume 41, Number 4
</figure>
<figureCaption confidence="0.990679">
Figure 2
</figureCaption>
<subsectionHeader confidence="0.662128">
Examples of the Baidu online machine translation service.
</subsectionHeader>
<bodyText confidence="0.999901785714286">
This is a sweet annoyance. To handle big data, we have developed fast training and
parallel decoding techniques in our project.
With such big data and frequent updates, even Internet buzzwords can be correctly
translated. My students often post the so-called “magic translation” on microblogs.
After the machine translation service came online, I began to realize that it would not
only influence those Ph.D. students who are reading and writing research papers, or
those businessmen who are studying materials from foreign countries. It also makes a
huge difference to ordinary people’s lives. Figure 2 shows some examples of Chinese–
English machine translation from the Baidu online translation service,3 which integrates
the research work of the 863 project “Machine Translation on the Internet.”
I once met a 50-year-old Chinese lady on a flight to Japan. She could not speak
Japanese, but she had finally decided to marry her Japanese husband, with whom she
had chatted online using machine translation. Another story comes from my neighbors,
who are a couple my age. Their children have lived in Germany for years. The first
</bodyText>
<footnote confidence="0.694891">
3 http://fanyi.baidu.com/.
</footnote>
<page confidence="0.989371">
714
</page>
<figure confidence="0.547942">
Li Translating Today into Tomorrow
</figure>
<figureCaption confidence="0.795686">
Figure 3
</figureCaption>
<bodyText confidence="0.969733088235294">
Integration of MT models.
time the old couple met their grandson when their family came back to China, they
were thrilled. However, on meeting their grandson, who can only speak German, they
had no way to express their love, which made them sad. The grandma blamed herself
and even wept when she was alone. With my recommendation, they started to use the
online speech translation app in their smart phone. Now, they can finally talk to their
grandson.
7. Integration of MT Models
I have been working in machine translation for several decades, going through almost
all the streams of technologies, from rule-based MT (RBMT) models at the very begin-
ning, example-based MT (EBMT) methods, to statistical MT (SMT) methods, as well as
the research hotspot today—neural network machine translation (NMT). Actually, we
tried the neural network–based models in NLP tasks, such as dialogue act analysis and
word sense disambiguation, more than 15 years ago (Wang, Gao, and Li 1999a, 1999b).
It is big data and computing power today that help neural network–based models
significantly outperform traditional ones. I know that every method has its advantages
and disadvantages. Although the new model and its methodology surpasses the old
ones overall, it does not mean that the old methods are useless. There’s an old saying
in Chinese, “the silly bear keeps picking corn,” which describes that when a bear is
stealing corn from a peasant’s field, it would always throw away the old one in its hand
when it picked a new one; the silly bear would always end up with only one corn in
his hand. I hoped that my team and I wouldn’t become the “silly bears.” Therefore,
when we decided to develop an Internet MT system, we all agreed on the idea that we
needed a hybrid approach, with which we could integrate all translation models and
subsystems, on each of which we have all spent great effort. It is just like an orchestra,
in which all instruments, such as piano, violin, cello, trumpet, and so on, are arranged
perfectly together. Only in this way can the orchestra present a wonderful perfor-
mance. As shown in Figure 3, in our MT system today, different models work together
perfectly.
The rule-based method is used to translate expressions like date, time, and num-
ber. The example-based method is applied to translate buzzwords, especially the new
emerging Internet expressions. On the other hand, those complicated long sentences
are translated using the syntax-based statistical model, while those sentences that can
be covered by a predefined vocabulary are translated with an NMT model. Finally, the
</bodyText>
<page confidence="0.993454">
715
</page>
<note confidence="0.65671">
Computational Linguistics Volume 41, Number 4
</note>
<bodyText confidence="0.9989415">
sentences left are all translated with a classical SMT model. The conductor of such an
orchestra is a discriminative distributing module, which decides what subsystem an
input sentence should be distributed to, based on a variety of statistical and linguistic
features.
</bodyText>
<sectionHeader confidence="0.519087" genericHeader="method">
8. Translation for Resource-Poor Languages
</sectionHeader>
<bodyText confidence="0.9996135">
Shortly after the release of Chinese–English and English–Chinese translation services,
we also released translation services between Chinese and Japanese, Korean, and other
daily-used foreign languages. However, with translation directions expanded, users’
expectations for the translation between the resource-poor languages became higher
and higher. Especially in recent years, China has been doing business more frequently
with many countries, such as Thailand and Portugal, among others, and the destina-
tions for Chinese tourists have become more diverse. One of my friends told me a story
after he came back from a tour in Southeast Asia. He ordered three kinds of salads in
a restaurant, since he did not understand or speak the local language. He could not
communicate with the waiters or even read the menu. These incidents told us that
solving translation problems for these resource-poor languages is urgent. Therefore, we
have successively released translation services between Chinese and over 20 foreign
languages. Now, we have covered languages in eight of the top ten destinations for
Chinese tourists, and all the top ten foreign cities where Chinese tourists spend the
most money.
On this basis, we took a further step. We built translation systems between any two
languages using the pivot approach (Wang, Wu, and Liu 2006; Wu and Wang 2007). For
resource-poor language pairs, we use English or Chinese as the pivot language. Trans-
lation models are trained for source-pivot and pivot-target, respectively, which are then
combined to form the translation model from the source to the target language. Using
this model, Baidu online translation services successfully realized pairwise translation
between any two of 27 languages; in total, 702 translation directions.
</bodyText>
<sectionHeader confidence="0.617731" genericHeader="method">
9. MT Methodology for Other Areas
</sectionHeader>
<bodyText confidence="0.998351944444444">
“他山之石,可以攻玉” [Stones from other hills may serve to polish the jade at hand].
This is a Chinese old saying from “《诗经》” [The Book of Songs], which was written
2,500 years ago. It suggests that one may benefit from other people’s opinions and
methods for their task. Machine translation technology is now a “stone from another
hill,” which has been used in many other areas. For instance, some researchers recast
paraphrasing as a monolingual translation problem and use MT models to generate
paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers
who regard query reformulation as the translation from the original query to the rewrit-
ten one (Riezler and Liu 2010). However, what interests me the most is the encounter
between translation technology and Chinese traditional culture. For example, MSRA
uses the translation model to automatically generate couplets (Jiang and Zhou 2008),
which are posted on the doors of every house during Chinese New Year. Baidu applies
translation methods to compose poems. Given a picture and the first line of a poem,
the system can generate another three lines of the poem that describe the content of the
picture. In addition, I have heard recently that both Microsoft and Baidu have released
their chatting robots, which are named Microsoft XiaoIce and Baidu Xiaodu, respec-
tively. They both use translation techniques in the searching and generation of chatting
responses. It is fair to say that machine translation has become more than a specific
</bodyText>
<page confidence="0.975151">
716
</page>
<bodyText confidence="0.890090666666667">
Li Translating Today into Tomorrow
method. Instead, it has evolved into a methodology and could make a contribution to
other similar or related areas.
</bodyText>
<sectionHeader confidence="0.878407" genericHeader="conclusions">
10. Conclusion
</sectionHeader>
<bodyText confidence="0.999990866666667">
There is an ancient story in China called “愚公移山” [Yugong moves the mountain]. In
the story, an old man called Yugong—meaning an unwise man—lived in a mountain
area. He decided to build a road to the outside world by moving two huge mountains
away. Other people all thought it was impossible and laughed at him. However, Yugong
said to the people calmly: “Even if I die, I have children; and my children would have
children in the future. As the mountain wouldn’t grow, we would move the mountain
away eventually.” Today, when facing the ambitious goal of automatic high-quality
machine translation, and even the whole NLP field, I cannot help thinking of Yugong’s
spirit. I have been, and I still am, trying to solve the questions and obstacles along the
way. Even if one day I will no longer be able to keep exploring MT, I believe that the
younger generations will keep on going until the dream of making a computer truly
understand languages eventually comes true.
My friends, especially the young ones, to share what I have learned from my career,
I’d like to say: Make yourself a good translation system: Input diligence today, and it
will definitely translate into an amazing tomorrow!
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999758925373134">
Che, Wanxiang, Zhenghua Li, and Ting Liu.
2012. Chinese dependency treebank 1.0
LDC2012T05. Web Download. Philadelphia:
Linguistic Data Consortium, 2012.
Che, Wanxiang, Min Zhang, AiTi Aw,
ChewLim Tan, Ting Liu, and Sheng Li.
2008. Using a hybrid convolution tree
kernel for semantic role labeling. ACM
Transactions on Asian Language Information
Processing (TALIP), 7(4):13.
Jiang, Long and Ming Zhou. 2008.
Generating Chinese couplets using a
statistical MT approach. In Proceedings of
COLING, pages 377–384, Manchester.
Li, Sheng, Ming Zhou, Miao Shi, and Weitian
Wu. 1988. A Chinese–English machine
translation system: CEMT-I. Journal of the
China Society for Scientific and Technical
Information, 7(6):409–416.
Riezler, Stefan and Yi Liu. 2010. Query
rewriting using monolingual statistical
machine translation. Computational
Linguistics, 36(3):569–582.
Wang, Haifeng, Wen Gao, and Sheng Li.
1999a. Dialog act analysis of spoken
Chinese based on neural networks. Chinese
Journal of Computers, 22(10):1014–1018.
Wang, Haifeng, Wen Gao, and Sheng Li.
1999b. Word sense disambiguation of
spoken Chinese using neural network.
Journal of Software, 10(12):1279–1283.
Wang, Haifeng, Sheng Li, Tiejun Zhao, Yan
Yang, Endong Xun, and Min Zhang. 1997.
The research and implementation of a
bilingual machine translation system
(BT863) for Chinese and English. Journal of
the China Society for Scientific and Technical
Information, 16(5):360–369.
Wang, Haifeng, Hua Wu, and Zhanyi Liu.
2006. Word alignment for languages with
scarce resources using bilingual corpora of
other language pairs. In Proceedings of
COLING/ACL, pages 874–881,
Sydney.
Wu, Hua and Haifeng Wang. 2007. Pivot
language approach for phrase-based
statistical machine translation. In
Proceedings of the ACL, pages 856–863,
Prague.
Zhang, Min, Wanxiang Che, Aiti Aw,
Chew Lim Tan, Guodong Zhou, Ting Liu,
and Sheng Li. 2007. A grammar-driven
convolution tree kernel for semantic role
classification. In Proceedings of the ACL,
pages 200–207, Prague.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng
Li. 2009. Application-driven statistical
paraphrase generation. In Proceedings of the
ACL/IJCNLP, pages 834–842, Suntec.
Zhao, Shiqi, Haifeng Wang, Xiang Lan, and
Ting Liu. 2010. Leveraging multiple MT
engines for paraphrase generation. In
Proceedings of COLING, pages 1326–1334,
Beijing.
Zhao, Tiejun, Sheng Li, and Haifeng Wang.
1995. Pattern-based machine translation:
Accomplishment of BT863 system. In
</reference>
<page confidence="0.929423">
717
</page>
<reference confidence="0.9500306">
Computational Linguistics Volume 41, Number 4
Proceedings of the NLPPRS, pages 371–376,
Seoul.
Zhao, Tiejun, Sheng Li, and Min Zhang. 1995.
CEMT-III: A fully automatic
Chinese–English MT system. In Proceedings
of International Conference for Chinese
Computing, pages 8–14, Singapore.
Zhou, Ming, Sheng Li, Mingzeng Hu,
and Shi Miao. 1990. An interactive
Chinese–English machine translation
system: CEMT-II. Journal of the
China Society for Scientific and
Technical Information,
9(2):151–154.
</reference>
<page confidence="0.994222">
718
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.987325">Lifetime Achievement Award Translating Today into Tomorrow</title>
<author confidence="0.999111">Sheng Li</author>
<affiliation confidence="0.999742">Harbin Institute of Technology</affiliation>
<abstract confidence="0.99156970967742">Good afternoon, ladies and gentlemen. I am standing here, grateful, excited, and proud. I see so many friends, my colleagues, students, and many more researchers in this room. I see that the work we started 50 years ago is now flourishing and is embedded in people’s everyday lives. I see for the first time that the ACL conference is held here in Beijing, China. And I am deeply honored to be awarded the Lifetime Achievement Award of 2015. I want to thank the ACL for giving me the Lifetime Achievement Award of 2015. It is the appreciation of not only my work, but also of the work that my fellow researchers, my colleagues, and my students have done through all these years. It is an honor for all of us. As a veteran of NLP research, I am fortunate to witness and be a part of its long yet inspiring journey in China. So today, to everyone here, my friends, colleagues, and students, either well-known scientists or young researchers: I’d like to share my experience and thoughts with you. 1. Early Machine Translation in China The history of machine translation (MT) in China dates back to 1956. At that time the new country had immense construction projects to recover what had been ruined in the war. However, the government precisely recognized the significance of machine translation, and started to explore this area, as the fourth country following the United States, the United Kingdom, and the Soviet Union. In 1959, Russian–Chinese machine translation was demonstrated on a Type-104 general-purpose computer made in China. This first MT system had a dictionary of 2,030 entries, and 29 groups of rules for lexical analysis. Programmed by machine instructions, the system was able to translate nine different types of sentences. It used punched tape as the input, and the output was a special kind of code for Chinese characters, since there was no Chinese character output device at the time. As the pioneer in Chinese MT, the system touched the issues of word sense disambiguation, word reordering, and proposed the idea of predicate-focused sentence analysis and pivot language for multilingual translation. In the same year, machine translation research at the Harbin Institute of Technology (HIT) was started by Prof. Zhen Wang (and later Prof. Kaizhu Wang), focusing on the Russian–Chinese MT group. The pursuit for MT has never halted after these forerunners.</abstract>
<note confidence="0.814748666666667">doi:10.1162/COLI a 00240 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4</note>
<abstract confidence="0.994601369863014">2. The CEMT Series In 1960, I was admitted to HIT. Five years later, I graduated and became a faculty member in the computer department of HIT, which was probably the first computer discipline among Chinese universities. I started my research, however, not from machine translation but from information retrieval (IR). I was fully occupied by how to effectively store books and documents on computers, and then retrieve them quickly and accurately. The start of my research in MT was incidentally caused by IR problems. At that time, Ming Zhou was my Ph.D. student. He is now the principal researcher of Natural Language Computing at Microsoft Research Asia (MSRA), and many of you may be acquainted with him. In 1985, at the beginning of his graduate study, he was aiming to address the topic of word extraction for Chinese documents to boost IR performance. For an exhaustive survey, Ming went to Beijing from Harbin alone, and buried himself at the National Library for over a month. He came back disappointed, finding that the related work was some language-dependent solutions for English. Actually, many research directions encountered this problem at that time. That’s why Ming and I decided to develop an MT system through which we could first translate Chinese materials into English, so as to take advantage of the solutions proposed for English, and finally translate the results back into Chinese, if necessary. In those years, the translation from Chinese to other foreign languages was less studied in China. Everything was hard in the beginning. We had to build everything from scratch, such as collecting and inputting each entry of the translation dictionary. Fortunately, we were not alone. I came to know many peer scholars, including Prof. Weitian Wu, Zhiwei Feng, Prof. Zhendong Dong, Prof. Shiwen Yu, and Prof. Changning Huang, as well as Dr. Zhaoxiong Chen. Although we didn’t work together, we could always learn from each other and inspire each other in MT research. After three years’ effort, we accomplished a rule-based MT system named CEMT-I et al. 1988). It ran on an IBM PC and was capable of translating eight kinds of Chinese sentence patterns with fewer than one thousand rules. It had a dictionary of 30,000 Chinese-English entries. Simple or even crude as it now seems, it really encouraged every member of our team. After that, we developed CEMT-II (Zhou et al. 1990) and CEMT-III (Zhao, Li, and Zhang 1995) successively. The CEMT series seemed to have a special kind of magic. Almost all the students who participated in these projects devoted themselves to machine translation in their following careers, including Ming Zhou, Min Zhang, and Tiejun Zhao. 3. DEAR and BT863 Inspired by the success of the CEMT series, we also developed a computer-aided translation system called “DEAR.” DEAR was put to market via a software chain store. Although it did not sell much, it was our first effort to commercialize the MT technology. I still remember how excited I was when I saw DEAR placed on the shelves for the first time. Today, it still reminds me that research work cannot just stay in the lab. Also in the 1980s, China’s NLP field was marked by a milestone event: the establishment of the Chinese Information Processing Society of China (CIPS). From then on, NLP researchers throughout the country have been connected and the academic exchange has been facilitated at the national scale. It was far beyond my imagination then that, 710 Li Translating Today into Tomorrow thirty years later, I would have the honor to be the president of this society, leading it to keep on contributing to the development of world-level NLP technology. I usually regard the series of MT systems that we developed as a large family. In 1994, BT863 joined this family with some new features (Zhao, Li, and Wang 1995; Wang et al. 1997). First, BT863 was distinguished as a bi-direction translation between Chinese and English under a uniform architecture. Second, in addition to the rules, it was augmented with examples and templates learned from a corpus. Finally, this system is remembered for its top performance in the early national MT evaluation organized by the 863 High Tech Program of China. 4. Syntactic and Semantic Parsing Time passed quickly. The rising of the Internet made communication more convenient, and our research was gradually connected with international peers. We concentrated on the mining and accumulation of bilingual and multilingual corpora. We explored how to integrate rule-based and example-based MT models under a unifying statistical framework. However, as more and more work was conducted, I found it more difficult to go deeper. I began to realize that translation problems cannot rely only on translation methods. From word segmentation, morphology, word meaning, to named entity, syntax, and semantics, every step in this procedure affects the quality of translation. I remember an interesting story. One day, my student Wanxiang Che input his name into our machine translation system. The system literally translated his name into ‘thousands of cars flying in the sky’. This was rated as the joke of the year in my lab, but the underlying problem is worth pondering. Traditional Chinese medicine advocates the treatment of both symptoms and root causes. The same principle applies to MT research, in which models for word alignment, decoding, reordering, and so forth, can solve the surface problems of machine translation, whereas understanding the word sense, sentence structure, and semantics is the solution to the fundamental problems. We therefore carried out research on syntactic analysis, including phrase-structure parsing and dependency parsing. In those days, dependency parsing on Chinese was not widely studied. There was no well-accepted annotation or transformed standard. Therefore, we referred to a large number of linguistic studies, developed a Chinese syntactic dependency annotation standard, and annotated a 50,000-sentence Chinese syntactic dependency treebank on this basis. This is the largest Chinese dependency treebank available. Differently from those transformed from phrase-structure treebanks, our dependency structure uses native dependency structure, which can handle a large number of specific grammatical phenomena in dependency structures. This treebank has been released by the Linguistic Data Consortium (LDC) (Che, Li, and Liu 2012). We hope that more researchers can benefit from it. Based on syntactic parsing, we hoped to further explore the semantic structure and the relationship of sentences. Therefore, we carried out research on semantic role labeling, and worked on the semantic role labeling methods based on the tree kernel, including the hybrid convolution tree kernel (Che et al. 2008) and the grammar-driven tree kernel (Zhang et al. 2007). In addition, we further broadened our mind and tried to analyze the semantics of Chinese directly. We proposed semantic dependency parsing tasks that directly establish semantic-level dependencies between content words, ignoring auxiliaries and prepositions. Meanwhile, we violated the tree structure constraints, allowing one word to depend on more than one parent node, so as to form semantic 711 Computational Linguistics Volume 41, Number 4 Figure 1 Example of syntactic dependency parsing, semantic role labeling, and semantic dependency parsing in LTP-Cloud. dependency graph structures. At this point, the semantic dependency treebank that we have already labeled has reached more than 30,000 sentences. Much ongoing research is based on these data. Figure 1 shows an example of syntactic dependency parsing, role labeling, and semantic dependency parsing for an input sentence she looks terrible, seems to be sick]” . 5. ITP and ITP-Cloud Every summer, HIT and MSRA would jointly organize a summer school for NLP research students. We invited domestic and foreign experts to give lectures to Chinese students engaged in this field. Because the summer school was free, students from all over the country came together every year, listening to lectures and conducting experiments. When I communicated with these students, I found that many of them came from labs that lacked fundamental NLP tools, such as word segmentors, partof-speech taggers, and syntactic parsers. It would have been very difficult for them to implement their research ideas without these tools. I felt bad when I saw that. They are all students with dreams and innovative ideas. We must create a level playing field for all of them, I thought. After coming back from the summer school, I met Ting Liu. He is a strong supporter of the idea of sharing. We decided to release an open-source NLP system: Language Technology Platform (LTP). This platform integrates several Chinese NLP basic technologies, including Chinese word segmentation, part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labeling, which has made great contributions to the development of further applications. In recent years, we realized that cloud computing and the mobile Internet have brought great opportunities and challenges to the NLP field. Therefore, we developed 712 Li Translating Today into Tomorrow in 2013, which provides accurate and fast NLP service via the Internet. Currently, the number of LTP-cloud registered users has exceeded 3,000, and most of them are NLP beginners. As I had wished, they no longer need to build an NLP basic processing system from scratch for their research. Every time I see the thank-you notes to the LTP and LTP-cloud in the acknowledgments of their papers, I am proud and grateful. 6. Machine Translation on the Internet As more and more papers were published in top conferences and journals, our lab made a name in the academic world. Many people in the lab were satisfied, but I felt differently, since publishing papers should not be the major objective for research. New models and techniques should be applied to solve real-world problems and improve people’s daily lives. Particularly since we have moved into the era of the Internet. Many new concepts and ideas have come into being, such as big data and cloud computing. In such a new era, machine translation research should no longer be restricted to the labs, running experiments on a small parallel corpus. Instead, it should embrace the Internet, and embrace big data. We paid great attention to the cooperation with IT and Internet companies. We established a joint lab with MSRA right after it was founded. After that, we have also established joint labs with other companies, like IBM, Baidu, and Tencent. My student Haifeng Wang is the vice president of Baidu. He is in charge of NLP research and development, as well as Web search. We decided to collaborate in MT shortly after he joined Baidu, since Baidu can provide a huge platform for us to verify our ideas. Together with Tsinghua University, Zhejiang University, the Institute of Computing Technology, and the Institute of Automation of the Chinese Academy of Science, we successfully applied for an 863 project titled “Machine Translation on the Internet.” All the members participating in this project have great passion for MT technologies and products. people accept the principle that [what is taken from the people should be used for the interests of the people]. Internet-based machine translation also follows this principle, which mines a large volume of translation data from the Internet, trains the translation model, and then provides high-quality services for Internet users. In our online translation system, taking Chinese–English translation, for example, there are hundreds of millions of parallel data for this language pair, which were filtered from billions of raw parallel data. We collected a large amount of data from hundreds of billions of Web pages. So I should say our MT service is actually built upon the whole Internet. We have designed various mining models for these heterogeneous Internet data sources, including bilingual parallel pages, bilingual comparable pages, Web pages containing aligned sentence pairs, as well as plain texts containing entity and terminology translations. The mined translation data are filtered and refined. We set different updating frequencies for different Web sites, so as to guarantee that the latest data can be included. I often observe the mined translation data by myself, and I can find plenty of wonderful translations generated by ordinary Internet users. Their wisdom is perfectly integrated into the translation system. However, how to make use of such a big corpus? 713 Computational Linguistics Volume 41, Number 4 Figure 2 Examples of the Baidu online machine translation service. This is a sweet annoyance. To handle big data, we have developed fast training and parallel decoding techniques in our project. With such big data and frequent updates, even Internet buzzwords can be correctly translated. My students often post the so-called “magic translation” on microblogs. After the machine translation service came online, I began to realize that it would not only influence those Ph.D. students who are reading and writing research papers, or those businessmen who are studying materials from foreign countries. It also makes a huge difference to ordinary people’s lives. Figure 2 shows some examples of Chinese– machine translation from the Baidu online translation which integrates the research work of the 863 project “Machine Translation on the Internet.” I once met a 50-year-old Chinese lady on a flight to Japan. She could not speak Japanese, but she had finally decided to marry her Japanese husband, with whom she had chatted online using machine translation. Another story comes from my neighbors, who are a couple my age. Their children have lived in Germany for years. The first 714 Li Translating Today into Tomorrow Figure 3 Integration of MT models. time the old couple met their grandson when their family came back to China, they were thrilled. However, on meeting their grandson, who can only speak German, they had no way to express their love, which made them sad. The grandma blamed herself and even wept when she was alone. With my recommendation, they started to use the online speech translation app in their smart phone. Now, they can finally talk to their grandson. 7. Integration of MT Models I have been working in machine translation for several decades, going through almost all the streams of technologies, from rule-based MT (RBMT) models at the very beginning, example-based MT (EBMT) methods, to statistical MT (SMT) methods, as well as the research hotspot today—neural network machine translation (NMT). Actually, we tried the neural network–based models in NLP tasks, such as dialogue act analysis and word sense disambiguation, more than 15 years ago (Wang, Gao, and Li 1999a, 1999b). It is big data and computing power today that help neural network–based models significantly outperform traditional ones. I know that every method has its advantages and disadvantages. Although the new model and its methodology surpasses the old ones overall, it does not mean that the old methods are useless. There’s an old saying in Chinese, “the silly bear keeps picking corn,” which describes that when a bear is stealing corn from a peasant’s field, it would always throw away the old one in its hand when it picked a new one; the silly bear would always end up with only one corn in his hand. I hoped that my team and I wouldn’t become the “silly bears.” Therefore, when we decided to develop an Internet MT system, we all agreed on the idea that we needed a hybrid approach, with which we could integrate all translation models and subsystems, on each of which we have all spent great effort. It is just like an orchestra, in which all instruments, such as piano, violin, cello, trumpet, and so on, are arranged perfectly together. Only in this way can the orchestra present a wonderful performance. As shown in Figure 3, in our MT system today, different models work together perfectly. The rule-based method is used to translate expressions like date, time, and number. The example-based method is applied to translate buzzwords, especially the new emerging Internet expressions. On the other hand, those complicated long sentences are translated using the syntax-based statistical model, while those sentences that can be covered by a predefined vocabulary are translated with an NMT model. Finally, the 715 Computational Linguistics Volume 41, Number 4 sentences left are all translated with a classical SMT model. The conductor of such an orchestra is a discriminative distributing module, which decides what subsystem an input sentence should be distributed to, based on a variety of statistical and linguistic features. 8. Translation for Resource-Poor Languages Shortly after the release of Chinese–English and English–Chinese translation services, we also released translation services between Chinese and Japanese, Korean, and other daily-used foreign languages. However, with translation directions expanded, users’ expectations for the translation between the resource-poor languages became higher and higher. Especially in recent years, China has been doing business more frequently with many countries, such as Thailand and Portugal, among others, and the destinations for Chinese tourists have become more diverse. One of my friends told me a story after he came back from a tour in Southeast Asia. He ordered three kinds of salads in a restaurant, since he did not understand or speak the local language. He could not communicate with the waiters or even read the menu. These incidents told us that solving translation problems for these resource-poor languages is urgent. Therefore, we have successively released translation services between Chinese and over 20 foreign languages. Now, we have covered languages in eight of the top ten destinations for Chinese tourists, and all the top ten foreign cities where Chinese tourists spend the most money. On this basis, we took a further step. We built translation systems between any two languages using the pivot approach (Wang, Wu, and Liu 2006; Wu and Wang 2007). For resource-poor language pairs, we use English or Chinese as the pivot language. Translation models are trained for source-pivot and pivot-target, respectively, which are then combined to form the translation model from the source to the target language. Using this model, Baidu online translation services successfully realized pairwise translation between any two of 27 languages; in total, 702 translation directions. 9. MT Methodology for Other Areas [Stones from other hills may serve to polish the jade at hand]. is a Chinese old saying from [The Book of Songs], which was written 2,500 years ago. It suggests that one may benefit from other people’s opinions and methods for their task. Machine translation technology is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem that describe the content of the picture. In addition, I have heard recently that both Microsoft and Baidu have released their chatting robots, which are named Microsoft XiaoIce and Baidu Xiaodu, respectively. They both use translation techniques in the searching and generation of chatting responses. It is fair to say that machine translation has become more than a specific 716 Li Translating Today into Tomorrow method. Instead, it has evolved into a methodology and could make a contribution to other similar or related areas. 10. Conclusion is an ancient story in China called [Yugong moves the mountain]. In the story, an old man called Yugong—meaning an unwise man—lived in a mountain area. He decided to build a road to the outside world by moving two huge mountains away. Other people all thought it was impossible and laughed at him. However, Yugong said to the people calmly: “Even if I die, I have children; and my children would have children in the future. As the mountain wouldn’t grow, we would move the mountain away eventually.” Today, when facing the ambitious goal of automatic high-quality machine translation, and even the whole NLP field, I cannot help thinking of Yugong’s spirit. I have been, and I still am, trying to solve the questions and obstacles along the way. Even if one day I will no longer be able to keep exploring MT, I believe that the younger generations will keep on going until the dream of making a computer truly understand languages eventually comes true. My friends, especially the young ones, to share what I have learned from my career, I’d like to say: Make yourself a good translation system: Input diligence today, and it will definitely translate into an amazing tomorrow!</abstract>
<note confidence="0.841135">References Che, Wanxiang, Zhenghua Li, and Ting Liu. 2012. Chinese dependency treebank 1.0 Download. Philadelphia: Linguistic Data Consortium, 2012. Che, Wanxiang, Min Zhang, AiTi Aw, ChewLim Tan, Ting Liu, and Sheng Li.</note>
<abstract confidence="0.879685046511628">2008. Using a hybrid convolution tree for semantic role labeling. Transactions on Asian Language Information 7(4):13. Jiang, Long and Ming Zhou. 2008. Generating Chinese couplets using a MT approach. In of pages 377–384, Manchester. Li, Sheng, Ming Zhou, Miao Shi, and Weitian Wu. 1988. A Chinese–English machine system: CEMT-I. of the China Society for Scientific and Technical 7(6):409–416. Riezler, Stefan and Yi Liu. 2010. Query rewriting using monolingual statistical translation. 36(3):569–582. Wang, Haifeng, Wen Gao, and Sheng Li. 1999a. Dialog act analysis of spoken based on neural networks. of 22(10):1014–1018. Wang, Haifeng, Wen Gao, and Sheng Li. 1999b. Word sense disambiguation of spoken Chinese using neural network. of 10(12):1279–1283. Wang, Haifeng, Sheng Li, Tiejun Zhao, Yan Yang, Endong Xun, and Min Zhang. 1997. The research and implementation of a bilingual machine translation system for Chinese and English. of the China Society for Scientific and Technical 16(5):360–369. Wang, Haifeng, Hua Wu, and Zhanyi Liu. 2006. Word alignment for languages with scarce resources using bilingual corpora of language pairs. In of pages 874–881, Sydney. Wu, Hua and Haifeng Wang. 2007. Pivot language approach for phrase-based statistical machine translation. In of the pages 856–863, Prague.</abstract>
<author confidence="0.701915666666667">A grammar-driven</author>
<abstract confidence="0.7967255">convolution tree kernel for semantic role In of the pages 200–207, Prague. Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-driven statistical generation. In of the pages 834–842, Suntec. Zhao, Shiqi, Haifeng Wang, Xiang Lan, and Ting Liu. 2010. Leveraging multiple MT engines for paraphrase generation. In</abstract>
<note confidence="0.857182181818182">of pages 1326–1334, Beijing. Zhao, Tiejun, Sheng Li, and Haifeng Wang. 1995. Pattern-based machine translation: Accomplishment of BT863 system. In 717 Computational Linguistics Volume 41, Number 4 of the pages 371–376, Seoul. Zhao, Tiejun, Sheng Li, and Min Zhang. 1995. CEMT-III: A fully automatic</note>
<abstract confidence="0.446593333333333">MT system. In of International Conference for Chinese pages 8–14, Singapore. Zhou, Ming, Sheng Li, Mingzeng Hu, and Shi Miao. 1990. An interactive Chinese–English machine translation CEMT-II. of the China Society for Scientific and 9(2):151–154.</abstract>
<intro confidence="0.568333">718</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Chinese dependency treebank 1.0 LDC2012T05. Web Download. Philadelphia: Linguistic Data Consortium,</title>
<date>2012</date>
<marker>Che, Li, Liu, 2012</marker>
<rawString>Che, Wanxiang, Zhenghua Li, and Ting Liu. 2012. Chinese dependency treebank 1.0 LDC2012T05. Web Download. Philadelphia: Linguistic Data Consortium, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>AiTi Aw</author>
<author>ChewLim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Using a hybrid convolution tree kernel for semantic role labeling.</title>
<date>2008</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="9466" citStr="Che et al. 2008" startWordPosition="1511" endWordPosition="1514">ks, our dependency structure uses native dependency structure, which can handle a large number of specific grammatical phenomena in dependency structures. This treebank has been released by the Linguistic Data Consortium (LDC) (Che, Li, and Liu 2012). We hope that more researchers can benefit from it. Based on syntactic parsing, we hoped to further explore the semantic structure and the relationship of sentences. Therefore, we carried out research on semantic role labeling, and worked on the semantic role labeling methods based on the tree kernel, including the hybrid convolution tree kernel (Che et al. 2008) and the grammar-driven tree kernel (Zhang et al. 2007). In addition, we further broadened our mind and tried to analyze the semantics of Chinese directly. We proposed semantic dependency parsing tasks that directly establish semantic-level dependencies between content words, ignoring auxiliaries and prepositions. Meanwhile, we violated the tree structure constraints, allowing one word to depend on more than one parent node, so as to form semantic 711 Computational Linguistics Volume 41, Number 4 Figure 1 Example of syntactic dependency parsing, semantic role labeling, and semantic dependency </context>
</contexts>
<marker>Che, Zhang, Aw, Tan, Liu, Li, 2008</marker>
<rawString>Che, Wanxiang, Min Zhang, AiTi Aw, ChewLim Tan, Ting Liu, and Sheng Li. 2008. Using a hybrid convolution tree kernel for semantic role labeling. ACM Transactions on Asian Language Information Processing (TALIP), 7(4):13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Ming Zhou</author>
</authors>
<title>Generating Chinese couplets using a statistical MT approach.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>377--384</pages>
<location>Manchester.</location>
<contexts>
<context position="22454" citStr="Jiang and Zhou 2008" startWordPosition="3575" endWordPosition="3578">is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem that describe the content of the picture. In addition, I have heard recently that both Microsoft and Baidu have released their chatting robots, which are named Microsoft XiaoIce and Baidu Xiaodu, respectively. They both use translation techniques in the searching and generation of chatting responses. It is fair to say that machine translation has become more than a specif</context>
</contexts>
<marker>Jiang, Zhou, 2008</marker>
<rawString>Jiang, Long and Ming Zhou. 2008. Generating Chinese couplets using a statistical MT approach. In Proceedings of COLING, pages 377–384, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheng Li</author>
<author>Ming Zhou</author>
<author>Miao Shi</author>
<author>Weitian Wu</author>
</authors>
<title>A Chinese–English machine translation system:</title>
<date>1988</date>
<journal>CEMT-I. Journal of the China Society for Scientific and Technical Information,</journal>
<volume>7</volume>
<issue>6</issue>
<contexts>
<context position="4687" citStr="Li et al. 1988" startWordPosition="766" endWordPosition="769">other foreign languages was less studied in China. Everything was hard in the beginning. We had to build everything from scratch, such as collecting and inputting each entry of the translation dictionary. Fortunately, we were not alone. I came to know many peer scholars, including Prof. Weitian Wu, Zhiwei Feng, Prof. Zhendong Dong, Prof. Shiwen Yu, and Prof. Changning Huang, as well as Dr. Zhaoxiong Chen. Although we didn’t work together, we could always learn from each other and inspire each other in MT research. After three years’ effort, we accomplished a rule-based MT system named CEMT-I (Li et al. 1988). It ran on an IBM PC XT1 and was capable of translating eight kinds of Chinese sentence patterns with fewer than one thousand rules. It had a dictionary of 30,000 Chinese-English entries. Simple or even crude as it now seems, it really encouraged every member of our team. After that, we developed CEMT-II (Zhou et al. 1990) and CEMT-III (Zhao, Li, and Zhang 1995) successively. The CEMT series seemed to have a special kind of magic. Almost all the students who participated in these projects devoted themselves to machine translation in their following careers, including Ming Zhou, Min Zhang, and</context>
</contexts>
<marker>Li, Zhou, Shi, Wu, 1988</marker>
<rawString>Li, Sheng, Ming Zhou, Miao Shi, and Weitian Wu. 1988. A Chinese–English machine translation system: CEMT-I. Journal of the China Society for Scientific and Technical Information, 7(6):409–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Yi Liu</author>
</authors>
<title>Query rewriting using monolingual statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="22234" citStr="Riezler and Liu 2010" startWordPosition="3544" endWordPosition="3547">]. This is a Chinese old saying from “《诗经》” [The Book of Songs], which was written 2,500 years ago. It suggests that one may benefit from other people’s opinions and methods for their task. Machine translation technology is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem that describe the content of the picture. In addition, I have heard recently that both Microsoft and Baidu have released their chatting robots, which are </context>
</contexts>
<marker>Riezler, Liu, 2010</marker>
<rawString>Riezler, Stefan and Yi Liu. 2010. Query rewriting using monolingual statistical machine translation. Computational Linguistics, 36(3):569–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Wen Gao</author>
<author>Sheng Li</author>
</authors>
<title>Dialog act analysis of spoken Chinese based on neural networks.</title>
<date>1999</date>
<journal>Chinese Journal of Computers,</journal>
<volume>22</volume>
<issue>10</issue>
<marker>Wang, Gao, Li, 1999</marker>
<rawString>Wang, Haifeng, Wen Gao, and Sheng Li. 1999a. Dialog act analysis of spoken Chinese based on neural networks. Chinese Journal of Computers, 22(10):1014–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Wen Gao</author>
<author>Sheng Li</author>
</authors>
<title>Word sense disambiguation of spoken Chinese using neural network.</title>
<date>1999</date>
<journal>Journal of Software,</journal>
<volume>10</volume>
<issue>12</issue>
<marker>Wang, Gao, Li, 1999</marker>
<rawString>Wang, Haifeng, Wen Gao, and Sheng Li. 1999b. Word sense disambiguation of spoken Chinese using neural network. Journal of Software, 10(12):1279–1283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
</authors>
<title>Sheng Li, Tiejun Zhao,</title>
<date>1997</date>
<location>Yan Yang, Endong</location>
<marker>Wang, 1997</marker>
<rawString>Wang, Haifeng, Sheng Li, Tiejun Zhao, Yan Yang, Endong Xun, and Min Zhang. 1997.</rawString>
</citation>
<citation valid="false">
<title>The research and implementation of a bilingual machine translation system (BT863) for Chinese and English.</title>
<journal>Journal of the China Society for Scientific and Technical Information,</journal>
<volume>16</volume>
<issue>5</issue>
<marker></marker>
<rawString>The research and implementation of a bilingual machine translation system (BT863) for Chinese and English. Journal of the China Society for Scientific and Technical Information, 16(5):360–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Zhanyi Liu</author>
</authors>
<title>Word alignment for languages with scarce resources using bilingual corpora of other language pairs.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>874--881</pages>
<location>Sydney.</location>
<marker>Wang, Wu, Liu, 2006</marker>
<rawString>Wang, Haifeng, Hua Wu, and Zhanyi Liu. 2006. Word alignment for languages with scarce resources using bilingual corpora of other language pairs. In Proceedings of COLING/ACL, pages 874–881, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot language approach for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>856--863</pages>
<location>Prague.</location>
<contexts>
<context position="21081" citStr="Wu and Wang 2007" startWordPosition="3362" endWordPosition="3365">anguage. He could not communicate with the waiters or even read the menu. These incidents told us that solving translation problems for these resource-poor languages is urgent. Therefore, we have successively released translation services between Chinese and over 20 foreign languages. Now, we have covered languages in eight of the top ten destinations for Chinese tourists, and all the top ten foreign cities where Chinese tourists spend the most money. On this basis, we took a further step. We built translation systems between any two languages using the pivot approach (Wang, Wu, and Liu 2006; Wu and Wang 2007). For resource-poor language pairs, we use English or Chinese as the pivot language. Translation models are trained for source-pivot and pivot-target, respectively, which are then combined to form the translation model from the source to the target language. Using this model, Baidu online translation services successfully realized pairwise translation between any two of 27 languages; in total, 702 translation directions. 9. MT Methodology for Other Areas “他山之石,可以攻玉” [Stones from other hills may serve to polish the jade at hand]. This is a Chinese old saying from “《诗经》” [The Book of Songs], whi</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Wu, Hua and Haifeng Wang. 2007. Pivot language approach for phrase-based statistical machine translation. In Proceedings of the ACL, pages 856–863, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
<author>Guodong Zhou</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A grammar-driven convolution tree kernel for semantic role classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>200--207</pages>
<location>Prague.</location>
<contexts>
<context position="9521" citStr="Zhang et al. 2007" startWordPosition="1520" endWordPosition="1523">tructure, which can handle a large number of specific grammatical phenomena in dependency structures. This treebank has been released by the Linguistic Data Consortium (LDC) (Che, Li, and Liu 2012). We hope that more researchers can benefit from it. Based on syntactic parsing, we hoped to further explore the semantic structure and the relationship of sentences. Therefore, we carried out research on semantic role labeling, and worked on the semantic role labeling methods based on the tree kernel, including the hybrid convolution tree kernel (Che et al. 2008) and the grammar-driven tree kernel (Zhang et al. 2007). In addition, we further broadened our mind and tried to analyze the semantics of Chinese directly. We proposed semantic dependency parsing tasks that directly establish semantic-level dependencies between content words, ignoring auxiliaries and prepositions. Meanwhile, we violated the tree structure constraints, allowing one word to depend on more than one parent node, so as to form semantic 711 Computational Linguistics Volume 41, Number 4 Figure 1 Example of syntactic dependency parsing, semantic role labeling, and semantic dependency parsing in LTP-Cloud. dependency graph structures. At t</context>
</contexts>
<marker>Zhang, Che, Aw, Tan, Zhou, Liu, Li, 2007</marker>
<rawString>Zhang, Min, Wanxiang Che, Aiti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu, and Sheng Li. 2007. A grammar-driven convolution tree kernel for semantic role classification. In Proceedings of the ACL, pages 200–207, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Xiang Lan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Application-driven statistical paraphrase generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL/IJCNLP,</booktitle>
<pages>834--842</pages>
<contexts>
<context position="22081" citStr="Zhao et al. 2009" startWordPosition="3519" endWordPosition="3522">s; in total, 702 translation directions. 9. MT Methodology for Other Areas “他山之石,可以攻玉” [Stones from other hills may serve to polish the jade at hand]. This is a Chinese old saying from “《诗经》” [The Book of Songs], which was written 2,500 years ago. It suggests that one may benefit from other people’s opinions and methods for their task. Machine translation technology is now a “stone from another hill,” which has been used in many other areas. For instance, some researchers recast paraphrasing as a monolingual translation problem and use MT models to generate paraphrases of the input sentences (Zhao et al. 2009, 2010). There are also researchers who regard query reformulation as the translation from the original query to the rewritten one (Riezler and Liu 2010). However, what interests me the most is the encounter between translation technology and Chinese traditional culture. For example, MSRA uses the translation model to automatically generate couplets (Jiang and Zhou 2008), which are posted on the doors of every house during Chinese New Year. Baidu applies translation methods to compose poems. Given a picture and the first line of a poem, the system can generate another three lines of the poem t</context>
</contexts>
<marker>Zhao, Lan, Liu, Li, 2009</marker>
<rawString>Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-driven statistical paraphrase generation. In Proceedings of the ACL/IJCNLP, pages 834–842, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Xiang Lan</author>
<author>Ting Liu</author>
</authors>
<title>Leveraging multiple MT engines for paraphrase generation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1326--1334</pages>
<location>Beijing.</location>
<marker>Zhao, Wang, Lan, Liu, 2010</marker>
<rawString>Zhao, Shiqi, Haifeng Wang, Xiang Lan, and Ting Liu. 2010. Leveraging multiple MT engines for paraphrase generation. In Proceedings of COLING, pages 1326–1334, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiejun Zhao</author>
<author>Sheng Li</author>
<author>Haifeng Wang</author>
</authors>
<title>Pattern-based machine translation: Accomplishment of BT863 system.</title>
<date>1995</date>
<booktitle>In Computational Linguistics Volume 41, Number 4 Proceedings of the NLPPRS,</booktitle>
<pages>371--376</pages>
<marker>Zhao, Li, Wang, 1995</marker>
<rawString>Zhao, Tiejun, Sheng Li, and Haifeng Wang. 1995. Pattern-based machine translation: Accomplishment of BT863 system. In Computational Linguistics Volume 41, Number 4 Proceedings of the NLPPRS, pages 371–376, Seoul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiejun Zhao</author>
<author>Sheng Li</author>
<author>Min Zhang</author>
</authors>
<title>CEMT-III: A fully automatic Chinese–English MT system.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference for Chinese Computing,</booktitle>
<pages>8--14</pages>
<marker>Zhao, Li, Zhang, 1995</marker>
<rawString>Zhao, Tiejun, Sheng Li, and Min Zhang. 1995. CEMT-III: A fully automatic Chinese–English MT system. In Proceedings of International Conference for Chinese Computing, pages 8–14, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Zhou</author>
<author>Sheng Li</author>
<author>Mingzeng Hu</author>
<author>Shi Miao</author>
</authors>
<title>An interactive Chinese–English machine translation system:</title>
<date>1990</date>
<journal>CEMT-II. Journal of the China Society for Scientific and Technical Information,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="5012" citStr="Zhou et al. 1990" startWordPosition="823" endWordPosition="826">dong Dong, Prof. Shiwen Yu, and Prof. Changning Huang, as well as Dr. Zhaoxiong Chen. Although we didn’t work together, we could always learn from each other and inspire each other in MT research. After three years’ effort, we accomplished a rule-based MT system named CEMT-I (Li et al. 1988). It ran on an IBM PC XT1 and was capable of translating eight kinds of Chinese sentence patterns with fewer than one thousand rules. It had a dictionary of 30,000 Chinese-English entries. Simple or even crude as it now seems, it really encouraged every member of our team. After that, we developed CEMT-II (Zhou et al. 1990) and CEMT-III (Zhao, Li, and Zhang 1995) successively. The CEMT series seemed to have a special kind of magic. Almost all the students who participated in these projects devoted themselves to machine translation in their following careers, including Ming Zhou, Min Zhang, and Tiejun Zhao. 3. DEAR and BT863 Inspired by the success of the CEMT series, we also developed a computer-aided translation system called “DEAR.” DEAR was put to market via a software chain store. Although it did not sell much, it was our first effort to commercialize the MT technology. I still remember how excited I was whe</context>
</contexts>
<marker>Zhou, Li, Hu, Miao, 1990</marker>
<rawString>Zhou, Ming, Sheng Li, Mingzeng Hu, and Shi Miao. 1990. An interactive Chinese–English machine translation system: CEMT-II. Journal of the China Society for Scientific and Technical Information, 9(2):151–154.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>