<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009407">
<title confidence="0.995375">
Rating Computer-Generated Questions with Mechanical Turk
</title>
<author confidence="0.996045">
Michael Heilman
</author>
<affiliation confidence="0.884523333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.996515">
mheilman@cs.cmu.edu
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999619166666667">
We use Amazon Mechanical Turk to rate
computer-generated reading comprehension
questions about Wikipedia articles. Such
application-specific ratings can be used to
train statistical rankers to improve systems’
final output, or to evaluate technologies that
generate natural language. We discuss the
question rating scheme we developed, assess
the quality of the ratings that we gathered
through Amazon Mechanical Turk, and show
evidence that these ratings can be used to im-
prove question generation.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961625">
This paper discusses the use of Amazon Mechani-
cal Turk (MTurk) to rate computer-generated read-
ing comprehension questions about Wikipedia arti-
cles.
We have developed a question generation sys-
tem (Heilman and Smith, 2009; Heilman and
Smith, 2010) that uses the overgenerate-and-rank
paradigm (Langkilde and Knight, 1998). In the
the overgenerate-and-rank approach, many system-
generated outputs are ranked in order to select higher
quality outputs. While the approach has had con-
siderable success in natural language generation
(Langkilde and Knight, 1998; Walker et al., 2001),
it often requires human labels on system output for
the purpose of learning to rank. We employ MTurk
to reduce the time and cost of acquiring these labels.
For many problems, large labeled datasets do not
exist. One alternative is to build rule-based sys-
tems, but it is often difficult and time-consuming
to accurately encode relevant linguistic knowledge
in rules. Another alternative, unsupervised or semi-
supervised learning, usually requires clever formu-
lations of bias that guide the learning process (Car-
roll and Charniak, 1992; Yarowsky, 1995); such
</bodyText>
<page confidence="0.995433">
35
</page>
<author confidence="0.866092">
Noah A. Smith
</author>
<affiliation confidence="0.885364666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.957307">
nasmith@cs.cmu.edu
</email>
<bodyText confidence="0.999813027777778">
intuitions are not always available. Thus, small,
application-specific labeled datasets, which can be
cheaply constructed using MTurk, may provide con-
siderable benefits by enabling the use of supervised
learning.
In addition to using MTurk ratings to train a
learned ranking component, we could also use
MTurk ratings to evaluate the final top-ranked out-
put of our system. More generally, MTurk can be a
useful evaluation tool for systems that output natu-
ral language (e.g., systems for natural language gen-
eration, summarization, translation). For example,
Callison-Burch (2009) used MTurk to evaluate ma-
chine translations. MTurk facilitates the efficient
measurement and understanding of errors made by
such technologies, and could be used to complement
automatic evaluation metrics such as BLEU (Pap-
ineni et al., 2002) and ROUGE (Lin, 2004).
It is true that, for our task, MTurk workers
annotate computer-generated rather than human-
generated natural language. Thus, the data will
not be as generally useful as other types of anno-
tations, such as parse trees, which could be used to
build general purpose syntactic parsers. However,
for the reasons described above, we believe the use
of MTurk to rate computer-generated output can be
useful for the training, development, and evaluation
of language technologies.
The remainder of the paper is organized as fol-
lows: §2 and §3 briefly describe the question gener-
ation system and corpora used in our experiments.
§4 provides the details of our rating scheme. §5 dis-
cusses the quantity, cost, speed, and quality of the
ratings we gathered. §6 presents preliminary experi-
ments showing that the MTurk ratings improve ques-
tion ranking. Finally, in §7, we conclude.
</bodyText>
<note confidence="0.763162">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 35–40,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.772516" genericHeader="method">
2 Question Generation System
</sectionHeader>
<bodyText confidence="0.999952363636364">
We use MTurk to improve and evaluate a system
for automatic question generation (QG). In our QG
approach, hand-crafted rules transform declarative
sentences from an input text into a large set of ques-
tions (i.e., hundreds per page). This rule system is
complemented by a statistical ranker, which ranks
questions according to their quality. Currently, we
focus on basic linguistic issues and the goal of pro-
ducing acceptable questions—that is, questions that
are grammatical, make sense, and are not vague. We
believe an educator could select and revise output
from the system in order to produce a final set of
high-quality, challenging questions.
Our system is described by Heilman and Smith
(2010). In that work, we employed a differ-
ent scheme involving binary judgments of question
quality according to various factors such as gram-
maticality, vagueness, and others. We also employed
university students as novice annotators. For the
training dataset, only one human rated each ques-
tion. See Heilman and Smith (2009) for more de-
tails.1
</bodyText>
<sectionHeader confidence="0.996367" genericHeader="method">
3 Corpora
</sectionHeader>
<bodyText confidence="0.990584555555556">
In our experiments, we generated questions from
60 articles sampled from the “featured” articles in
the English Wikipedia2 that have between 250 and
2,000 word tokens. This collection provides expos-
itory texts written at an adult reading level from a
variety of domains, which roughly approximates the
prose that a secondary or post-secondary level stu-
dent would encounter. By choosing from the fea-
tured articles, we intended to select well-edited ar-
ticles about topics of general interest. We then ran-
domly selected 20 questions from each of 60 articles
for labeling with MTurk.3
1We also generated some questions using a technique that
replaces pronouns and underspecified noun phrases with an-
tecedent mentions identified by a coreference resolver. We will
not provide details about this component here because they are
not relevant to our use of MTurk to rate questions. A forthcom-
ing paper will describe these additions.
</bodyText>
<footnote confidence="0.97763">
2The English Wikipedia data were downloaded on Decem-
ber 16, 2008 from http://en.wikipedia.org
3Five questions were later eliminated from this set due to
minor implementation changes, the details of which are unin-
teresting. The final set contained 1,195 questions.
</footnote>
<table confidence="0.797282333333333">
Rating Details
1 Bad The question has major prob-
lems.
2 Unacceptable The question definitely has a
minor problem.
3 Borderline The question might have a
problem, but I’m not sure.
4 Acceptable The question does not have
problems.
5 Good The question is as good as one
that a human teacher might
write for a reading quiz.
</table>
<tableCaption confidence="0.999827">
Table 1: The five-point question rating scale.
</tableCaption>
<sectionHeader confidence="0.953258" genericHeader="method">
4 Rating Scheme
</sectionHeader>
<bodyText confidence="0.99996912">
This section describes the rating scheme we de-
veloped for evaluating the quality of computer-
generated questions on MTurk.
Questions were presented independently as sin-
gle human intelligence tasks (HITs). At the top of
the page, raters were given the instructions shown
in Figure 1 along with 7 examples of good and bad
questions with their appropriate ratings. Below the
instructions and examples was an excerpt from the
source text consisting of up to 5 sentences of con-
text, ending with the primary sentence that the ques-
tion was generated from. The question to be rated
then followed.
Below each question was the five-point rating
scale shown in Table 1. Workers were required to
select a single rating by clicking a radio button. At
the bottom of the page, the entire source article text
was given, in case the worker felt it was necessary
to refer back to more context.
We paid 5 cents per rating,4 and each question was
rated by five workers. With the 10% commission
charge by Amazon, each question cost 27.5 cents.
The final rating value was computed by taking
the arithmetic mean of the ratings. Table 2 provides
some examples of questions and their mean ratings.
</bodyText>
<subsectionHeader confidence="0.999229">
4.1 Monitoring Turker Ratings
</subsectionHeader>
<bodyText confidence="0.997651">
During some pilot tests, we found that it was par-
ticularly important to set some qualification criteria
for workers. Specifically, we only allowed workers
</bodyText>
<footnote confidence="0.989051">
4Given the average time spent per HIT, the pay rate can be
extrapolated to $5–10 per hour.
</footnote>
<page confidence="0.995512">
36
</page>
<figureCaption confidence="0.999904">
Figure 1: A screenshot of the instructions given to workers.
</figureCaption>
<bodyText confidence="0.999466590909091">
who had completed at least 50 previously accepted
HITs. We also required that at least 95% of workers’
previous submissions had been accepted.
We also submitted HITs in batches of 100 to 500
so that we could more closely monitor the process.
In addition, we performed a limited amount of
semi-automated monitoring of the ratings, and re-
jected work from workers who were clearly ran-
domly clicking on answers or not following the rat-
ing scheme properly. We tried to err on the side of
accepting bad work. After all ratings for a batch
of questions were received, we calculated for each
worker the number of ratings submitted, the aver-
age time spent on each question, the average rating,
and the correlation of the worker’s rating with the
mean of the other 4 ratings. We used a combination
of these statistics to identify extremely bad workers
(e.g., ones who had negative correlations with other
workers and spent less than 10 seconds per ques-
tion). If some of the ratings for a question were
rejected, then the HIT was “extended” in order to
receive 5 ratings.
</bodyText>
<sectionHeader confidence="0.966997" genericHeader="method">
5 Quantity, Cost, Speed, and Quality
</sectionHeader>
<bodyText confidence="0.999959">
This section discusses the quantity and quality of the
question ratings we received from MTurk.
</bodyText>
<subsectionHeader confidence="0.995341">
5.1 Quantity and Cost of Ratings
</subsectionHeader>
<bodyText confidence="0.999992555555556">
We received 5 ratings each for 1,200 questions, cost-
ing a total of $330. 178 workers participated. Work-
ers submitted 33.9 ratings on average (s.d. = 58.0).
The distribution of ratings per worker was highly
skewed, such that a handful of workers submitted
100 or more ratings (max = 395). The ratings from
these who submitted more than 100 ratings seemed
to be slightly lower in quality but still acceptable.
The median number of ratings per worker was 11.
</bodyText>
<subsectionHeader confidence="0.999512">
5.2 Speed of Ratings
</subsectionHeader>
<bodyText confidence="0.999717666666667">
Ratings were received very quickly once the HITs
were submitted. Figure 2 shows the cumulative
number of ratings received for a batch of questions,
</bodyText>
<page confidence="0.996756">
37
</page>
<figureCaption confidence="0.5382601875">
Source Text Excerpt Question Rating
MD 36 serves as the main road through the Georges Creek Which part has MD 36 been desig- 1.4
Valley, a region which is historically known for coal mining, nated by MDSHA as?
and has been designated by MDSHA as part of the Coal Her-
itage Scenic Byway.
He worked further on the story with the Soviet author Isaac What did the production of Bezhin 2.0
Babel, but no material was ever published or released from Meadow come to?
their collaboration, and the production of Bezhin Meadow
came to an end.
The design was lethal, successful and much imitated, and Does the design remain one of the 2.8
remains one of the definitive weapons of World War II. definitive weapons of World War II?
Francium was discovered by Marguerite Perey in France Where was Francium discovered by 3.8
(from which the element takes its name) in 1939. Marguerite Perey in 1939?
Lazare Ponticelli was the longest-surviving officially recog- Did Lazare Ponticelli attempt to re- 4.4
nized veteran... Although he attempted to remain with his main with his French regiment?
French regiment, he eventually enlisted in...
</figureCaption>
<tableCaption confidence="0.988255">
Table 2: Example computer-generated questions, along with their mean ratings from Mechanical Turk.
</tableCaption>
<figure confidence="0.9712465">
0 10 20 30 40 50 60 70
Minutes Elapsed
</figure>
<figureCaption confidence="0.996417666666667">
Figure 2: The cumulative number of ratings submitted by
MTurk workers over time, for a batch of 497 questions
posted simultaneously (there are 5 ratings per question).
</figureCaption>
<bodyText confidence="0.998917">
indicating that more than 1,000 ratings were re-
ceived per hour.
</bodyText>
<subsectionHeader confidence="0.999812">
5.3 Quality of Ratings
</subsectionHeader>
<bodyText confidence="0.9998718">
We evaluated inter-rater agreement by having the
first author and an independent judge rate a random
sample of 40 questions from 4 articles. The indepen-
dent judge was a computational linguist. The Pear-
son correlation coefficient between the first author’s
ratings and the mean ratings from MTurk work-
ers was r = 0.79, which is fairly strong though
not ideal. The correlation between the independent
judge’s ratings and the MTurk workers was r =
0.74.tiThese fairly strong positive correlations be-
tween the MTurk ratings and the two human judges
provide evidence that the rating scheme is consis-
tent and well-defined. The results also agree with
Snow et al. (2008), who found that aggregating la-
bels from 3 to 7 workers often provides expert lev-
els of agreement. Interestingly, the agreement be-
tween the two human raters was somewhat lower
(r = 0.65), suggesting that aggregated labels from a
crowd of MTurk workers can be more reliable than
individual humans.5
</bodyText>
<equation confidence="0.7666575">
t
o
</equation>
<sectionHeader confidence="0.7933345" genericHeader="method">
6 Using Labeled Data to Improve Question
Ranking
</sectionHeader>
<bodyText confidence="0.999961083333333">
In this section, we provide some preliminary results
to demonstrate that MTurk ratings can be used for
learning to rank QG output.
First, we briefly characterize the quality of un-
ranked output. Figure 3 shows a histogram of the
mean MTurk ratings for the 1,195 questions, show-
ing that only a relatively small fraction of the ques-
tions created by the overgenerating steps of our sys-
tem are acceptable: 12.9% when using 3.5 as the
threshold for acceptability.
However, ranking can lead to substantially higher
levels of quality in the top-ranked questions, which
</bodyText>
<footnote confidence="0.9622345">
5We also converted the ratings into binary values based on
whether they exceeded a threshold of 3.5. After this conversion
to a nominal scale, we computed a Cohen’s r. of 0.54, which
indicates “moderate” agreement (Landis and Koch, 1977).
</footnote>
<figure confidence="0.977067714285714">
2500
2000
1500
1000
500
0
R
v
um
38
25%
20%
15%
10%
5%
0%
0.
0.
0.
0.
0.
</figure>
<figureCaption confidence="0.999986">
Figure 3: The distribution of the 1,195 question ratings.
</figureCaption>
<bodyText confidence="0.999969655172414">
might be presented first in a user interface. There-
fore, we investigated how many MTurk-rated ques-
tions are needed to train an effective statistical ques-
tion ranker. Our ranking model is essentially the
same as the one used by Heilman and Smith (2010).
Rather than logistic regression, which we used pre-
viously, here we use a linear regression with f2 reg-
ularization to account for the ordinal scale of the av-
eraged question ratings. We set the regularization
parameter through cross-validation with the training
data.
The regression includes all of the features de-
scribed by Heilman and Smith (2010). It includes
features for sentence lengths, whether the question
includes various WH words, whether certain syntac-
tic transformations performed during QG, whether
negation words are present in questions, how many
times various parts of speech appeared, and others.
It also includes some additional coreference features
for parts of speech and lengths of noun phrase men-
tions and their antecedents.6 In all, the ranker in-
cludes 326 features.
For our experiments, we set aside a randomly cho-
sen 200 of the 1,195 rated questions as a test set.
We then trained statistical rankers on randomly sam-
pled subsets of the remaining questions, from size
N = 50 up to N = 995. For each value of N,
we used the ranker trained on that amount of data
to rank the 200 test questions. We then computed
</bodyText>
<footnote confidence="0.950009666666667">
6Since these additional coreference features are not immedi-
ately relevant to this work, we will not describe them fully here.
A forthcoming paper will describe them in more detail.
</footnote>
<figureCaption confidence="0.9955996">
Figure 4: A graph of the acceptability of top-ranked ques-
tions when datasets of increasing size are used to train a
statistical question ranker. Error bars show 95% confi-
dence intervals computed from the 10 runs of the sam-
pling process.
</figureCaption>
<bodyText confidence="0.999787">
the percentage of the top fifth of the ranked test set
questions with a mean rating above 3.5. For each
N less than 995, we repeated the entire sampling,
training, and ranking process 10 times and averaged
the results. (We used the same 200 question test set
throughout the process.)
Figure 4 presents the results, with the acceptabil-
ity of unranked questions (23%) included at N = 0
for comparison. We see that ranking more than dou-
bles the acceptability of the top-ranked questions,
consistent with findings from Heilman and Smith
(2010). It appears that ranking performance im-
proves as more training data are used. When 650 ex-
amples were used, 49% of the top-ranked questions
were acceptable. Ranking performance appears to
level off somewhat when more than 650 training ex-
amples are used. However, we speculate that if the
model included more fine-grained features, the value
of additional labeled data might increase.7
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.636070555555555">
In this paper, we used MTurk to gather quality rat-
ings for computer-generated questions. We pre-
7To directly compare the ranker’s predictions to the correla-
tions presented in §5.3, we computed a correlation coefficient
between the test set ratings from MTurk and the ratings pre-
dicted by the ranker when it was trained on all 995 training ex-
amples. The coefficient was r = 0.36, which is statistically sig-
nificant (p &lt; .001) but suggests that there is substantial room
for improvement in the ranking model.
</bodyText>
<page confidence="0.998445">
39
</page>
<bodyText confidence="0.999946375">
sented a question rating scheme, and found high lev-
els of inter-rater agreement (r &gt; 0.74) between rat-
ings from reliable humans and ratings from MTurk.
We also showed that ratings can be gathered from
MTurk quickly (more than 1,000 per hour) and
cheaply (less than 30 cents per question).
While ratings of computer-generated language are
not as generally useful as, for example, annotations
of the syntactic structure of human-generated lan-
guage, many research paradigms involving the auto-
matic generation of language may be able to benefit
from using MTurk to quickly and cheaply evaluate
ongoing work. Also, we demonstrated that such rat-
ings can be used in an overgenerate-and-rank strat-
egy to greatly improve the quality of a system’s top-
ranked output.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977405882353">
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In Proc. of EMNLP.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
M. Heilman and N. A. Smith. 2009. Question gener-
ation via overgenerating transformations and ranking.
Technical Report CMU-LTI-09-013, Language Tech-
nologies Institute, Carnegie Mellon University.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL-HLT.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33.
I. Langkilde and Kevin Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Proc.
of ACL.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In Proc. of Workshop on Text
Summarization.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast – but is it good? evaluating non-expert
annotations for natural language tasks. In Proc. of
EMNLP.
M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proc. of NAACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL.
</reference>
<page confidence="0.998622">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960512">
<title confidence="0.998967">Rating Computer-Generated Questions with Mechanical Turk</title>
<author confidence="0.999997">Michael Heilman</author>
<affiliation confidence="0.99874">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.991561">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999821">mheilman@cs.cmu.edu</email>
<abstract confidence="0.997866538461538">We use Amazon Mechanical Turk to rate computer-generated reading comprehension questions about Wikipedia articles. Such application-specific ratings can be used to train statistical rankers to improve systems’ final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2535" citStr="Callison-Burch (2009)" startWordPosition="365" endWordPosition="366">tsburgh, PA 15213, USA nasmith@cs.cmu.edu intuitions are not always available. Thus, small, application-specific labeled datasets, which can be cheaply constructed using MTurk, may provide considerable benefits by enabling the use of supervised learning. In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation). For example, Callison-Burch (2009) used MTurk to evaluate machine translations. MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). It is true that, for our task, MTurk workers annotate computer-generated rather than humangenerated natural language. Thus, the data will not be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers. However, for the reasons described above, we believe </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>C. Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>E Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<tech>Technical report,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="1811" citStr="Carroll and Charniak, 1992" startWordPosition="258" endWordPosition="262">ble success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules. Another alternative, unsupervised or semisupervised learning, usually requires clever formulations of bias that guide the learning process (Carroll and Charniak, 1992; Yarowsky, 1995); such 35 Noah A. Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu intuitions are not always available. Thus, small, application-specific labeled datasets, which can be cheaply constructed using MTurk, may provide considerable benefits by enabling the use of supervised learning. In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natu</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>G. Carroll and E. Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Technical report, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Question generation via overgenerating transformations and ranking.</title>
<date>2009</date>
<tech>Technical Report CMU-LTI-09-013,</tech>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="921" citStr="Heilman and Smith, 2009" startWordPosition="124" endWordPosition="127">es. Such application-specific ratings can be used to train statistical rankers to improve systems’ final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build ru</context>
<context position="4924" citStr="Heilman and Smith (2009)" startWordPosition="740" endWordPosition="743"> goal of producing acceptable questions—that is, questions that are grammatical, make sense, and are not vague. We believe an educator could select and revise output from the system in order to produce a final set of high-quality, challenging questions. Our system is described by Heilman and Smith (2010). In that work, we employed a different scheme involving binary judgments of question quality according to various factors such as grammaticality, vagueness, and others. We also employed university students as novice annotators. For the training dataset, only one human rated each question. See Heilman and Smith (2009) for more details.1 3 Corpora In our experiments, we generated questions from 60 articles sampled from the “featured” articles in the English Wikipedia2 that have between 250 and 2,000 word tokens. This collection provides expository texts written at an adult reading level from a variety of domains, which roughly approximates the prose that a secondary or post-secondary level student would encounter. By choosing from the featured articles, we intended to select well-edited articles about topics of general interest. We then randomly selected 20 questions from each of 60 articles for labeling wi</context>
</contexts>
<marker>Heilman, Smith, 2009</marker>
<rawString>M. Heilman and N. A. Smith. 2009. Question generation via overgenerating transformations and ranking. Technical Report CMU-LTI-09-013, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Good question! statistical ranking for question generation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="947" citStr="Heilman and Smith, 2010" startWordPosition="128" endWordPosition="131">ific ratings can be used to train statistical rankers to improve systems’ final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it i</context>
<context position="4605" citStr="Heilman and Smith (2010)" startWordPosition="690" endWordPosition="693">ion (QG). In our QG approach, hand-crafted rules transform declarative sentences from an input text into a large set of questions (i.e., hundreds per page). This rule system is complemented by a statistical ranker, which ranks questions according to their quality. Currently, we focus on basic linguistic issues and the goal of producing acceptable questions—that is, questions that are grammatical, make sense, and are not vague. We believe an educator could select and revise output from the system in order to produce a final set of high-quality, challenging questions. Our system is described by Heilman and Smith (2010). In that work, we employed a different scheme involving binary judgments of question quality according to various factors such as grammaticality, vagueness, and others. We also employed university students as novice annotators. For the training dataset, only one human rated each question. See Heilman and Smith (2009) for more details.1 3 Corpora In our experiments, we generated questions from 60 articles sampled from the “featured” articles in the English Wikipedia2 that have between 250 and 2,000 word tokens. This collection provides expository texts written at an adult reading level from a </context>
<context position="13605" citStr="Heilman and Smith (2010)" startWordPosition="2206" endWordPosition="2209">ch 5We also converted the ratings into binary values based on whether they exceeded a threshold of 3.5. After this conversion to a nominal scale, we computed a Cohen’s r. of 0.54, which indicates “moderate” agreement (Landis and Koch, 1977). 2500 2000 1500 1000 500 0 R v um 38 25% 20% 15% 10% 5% 0% 0. 0. 0. 0. 0. Figure 3: The distribution of the 1,195 question ratings. might be presented first in a user interface. Therefore, we investigated how many MTurk-rated questions are needed to train an effective statistical question ranker. Our ranking model is essentially the same as the one used by Heilman and Smith (2010). Rather than logistic regression, which we used previously, here we use a linear regression with f2 regularization to account for the ordinal scale of the averaged question ratings. We set the regularization parameter through cross-validation with the training data. The regression includes all of the features described by Heilman and Smith (2010). It includes features for sentence lengths, whether the question includes various WH words, whether certain syntactic transformations performed during QG, whether negation words are present in questions, how many times various parts of speech appeare</context>
<context position="15697" citStr="Heilman and Smith (2010)" startWordPosition="2559" endWordPosition="2562">ars show 95% confidence intervals computed from the 10 runs of the sampling process. the percentage of the top fifth of the ranked test set questions with a mean rating above 3.5. For each N less than 995, we repeated the entire sampling, training, and ranking process 10 times and averaged the results. (We used the same 200 question test set throughout the process.) Figure 4 presents the results, with the acceptability of unranked questions (23%) included at N = 0 for comparison. We see that ranking more than doubles the acceptability of the top-ranked questions, consistent with findings from Heilman and Smith (2010). It appears that ranking performance improves as more training data are used. When 650 examples were used, 49% of the top-ranked questions were acceptable. Ranking performance appears to level off somewhat when more than 650 training examples are used. However, we speculate that if the model included more fine-grained features, the value of additional labeled data might increase.7 7 Conclusion In this paper, we used MTurk to gather quality ratings for computer-generated questions. We pre7To directly compare the ranker’s predictions to the correlations presented in §5.3, we computed a correlat</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>M. Heilman and N. A. Smith. 2010. Good question! statistical ranking for question generation. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<contexts>
<context position="13221" citStr="Landis and Koch, 1977" startWordPosition="2133" endWordPosition="2136"> unranked output. Figure 3 shows a histogram of the mean MTurk ratings for the 1,195 questions, showing that only a relatively small fraction of the questions created by the overgenerating steps of our system are acceptable: 12.9% when using 3.5 as the threshold for acceptability. However, ranking can lead to substantially higher levels of quality in the top-ranked questions, which 5We also converted the ratings into binary values based on whether they exceeded a threshold of 3.5. After this conversion to a nominal scale, we computed a Cohen’s r. of 0.54, which indicates “moderate” agreement (Landis and Koch, 1977). 2500 2000 1500 1000 500 0 R v um 38 25% 20% 15% 10% 5% 0% 0. 0. 0. 0. 0. Figure 3: The distribution of the 1,195 question ratings. might be presented first in a user interface. Therefore, we investigated how many MTurk-rated questions are needed to train an effective statistical question ranker. Our ranking model is essentially the same as the one used by Heilman and Smith (2010). Rather than logistic regression, which we used previously, here we use a linear regression with f2 regularization to account for the ordinal scale of the averaged question ratings. We set the regularization paramet</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1021" citStr="Langkilde and Knight, 1998" startWordPosition="137" endWordPosition="140">s’ final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguis</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. of Workshop on Text Summarization.</booktitle>
<contexts>
<context position="2799" citStr="Lin, 2004" startWordPosition="406" endWordPosition="407"> MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation). For example, Callison-Burch (2009) used MTurk to evaluate machine translations. MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). It is true that, for our task, MTurk workers annotate computer-generated rather than humangenerated natural language. Thus, the data will not be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers. However, for the reasons described above, we believe the use of MTurk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies. The remainder of the paper is organized as follows: §2 and §3 briefly describe the question generation system and corpora used </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proc. of Workshop on Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2777" citStr="Papineni et al., 2002" startWordPosition="399" endWordPosition="403">sed learning. In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation). For example, Callison-Burch (2009) used MTurk to evaluate machine translations. MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). It is true that, for our task, MTurk workers annotate computer-generated rather than humangenerated natural language. Thus, the data will not be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers. However, for the reasons described above, we believe the use of MTurk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies. The remainder of the paper is organized as follows: §2 and §3 briefly describe the question generation sy</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>O Rambow</author>
<author>M Rogati</author>
</authors>
<title>Spot: a trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1277" citStr="Walker et al., 2001" startWordPosition="175" endWordPosition="178"> improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules. Another alternative, unsupervised or semisupervised learning, usually requires clever formulations of bias that guide the learning process (Carroll and Charniak, 1992; Yarowsky, 1995); such 35 Noah A. Smith Language Technologies Ins</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot: a trainable sentence planner. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1828" citStr="Yarowsky, 1995" startWordPosition="263" endWordPosition="264">age generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules. Another alternative, unsupervised or semisupervised learning, usually requires clever formulations of bias that guide the learning process (Carroll and Charniak, 1992; Yarowsky, 1995); such 35 Noah A. Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu intuitions are not always available. Thus, small, application-specific labeled datasets, which can be cheaply constructed using MTurk, may provide considerable benefits by enabling the use of supervised learning. In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>