<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000103">
<title confidence="0.99864">
A Fully Unsupervised Word Sense Disambiguation Method Using
Dependency Knowledge
</title>
<author confidence="0.997094">
Ping Chen
</author>
<affiliation confidence="0.9978505">
Dept. of Computer and Math. Sciences
University of Houston-Downtown
</affiliation>
<email confidence="0.991091">
chenp@uhd.edu
</email>
<author confidence="0.99679">
Chris Bowes
</author>
<affiliation confidence="0.9983625">
Dept. of Computer and Math. Sciences
University of Houston-Downtown
</affiliation>
<email confidence="0.998476">
bowesc@uhd.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99950224137931">
Word sense disambiguation is the process of
determining which sense of a word is used
in a given context. Due to its importance in
understanding semantics of natural languages,
word sense disambiguation has been exten-
sively studied in Computational Linguistics.
However, existing methods either are brit-
tle and narrowly focus on specific topics or
words, or provide only mediocre performance
in real-world settings. Broad coverage and
disambiguation quality are critical for a word
sense disambiguation system. In this paper we
present a fully unsupervised word sense dis-
ambiguation method that requires only a dic-
tionary and unannotated text as input. Such an
automatic approach overcomes the problem of
brittleness suffered in many existing methods
and makes broad-coverage word sense dis-
ambiguation feasible in practice. We evalu-
ated our approach using SemEval 2007 Task
7 (Coarse-grained English All-words Task),
and our system significantly outperformed the
best unsupervised system participating in Se-
mEval 2007 and achieved the performance ap-
proaching top-performing supervised systems.
Although our method was only tested with
coarse-grained sense disambiguation, it can be
directly applied to fine-grained sense disam-
biguation.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982034666666667">
In many natural languages, a word can represent
multiple meanings/senses, and such a word is called
a homograph. Word sense disambiguation(WSD)
</bodyText>
<author confidence="0.905666">
Wei Ding
</author>
<affiliation confidence="0.9983615">
Department of Computer Science
University of Massachusetts-Boston
</affiliation>
<email confidence="0.971693">
ding@cs.umb.edu
</email>
<author confidence="0.989698">
David Brown
</author>
<affiliation confidence="0.9987795">
Dept. of Computer and Math. Sciences
University of Houston-Downtown
</affiliation>
<email confidence="0.992195">
brownd@uhd.edu
</email>
<bodyText confidence="0.999293888888889">
is the process of determining which sense of a ho-
mograph is used in a given context. WSD is a
long-standing problem in Computational Linguis-
tics, and has significant impact in many real-world
applications including machine translation, informa-
tion extraction, and information retrieval. Gener-
ally, WSD methods use the context of a word for
its sense disambiguation, and the context informa-
tion can come from either annotated/unannotated
text or other knowledge resources, such as Word-
Net (Fellbaum, 1998), SemCor (SemCor, 2008),
Open Mind Word Expert (Chklovski and Mihal-
cea, 2002), eXtended WordNet (Moldovan and Rus,
2001), Wikipedia (Mihalcea, 2007), parallel corpora
(Ng, Wang, and Chan, 2003). In (Ide and V´eronis,
1998) many different WSD approaches were de-
scribed. Usually, WSD techniques can be divided
into four categories (Agirre and Edmonds, 2006),
</bodyText>
<listItem confidence="0.564577">
• Dictionary and knowledge based methods.
</listItem>
<bodyText confidence="0.970288571428571">
These methods use lexical knowledge bases
such as dictionaries and thesauri, and hypoth-
esize that context knowledge can be extracted
from definitions of words. For example, Lesk
disambiguated two words by finding the pair of
senses with the greatest word overlap in their
dictionary definitions (Lesk, 1986).
</bodyText>
<listItem confidence="0.861077">
• Supervised methods. Supervised methods
</listItem>
<bodyText confidence="0.911907666666667">
mainly adopt context to disambiguate words.
A supervised method includes a training phase
and a testing phase. In the training phase,
a sense-annotated training corpus is required,
from which syntactic and semantic features are
extracted to create a classifier using machine
</bodyText>
<page confidence="0.982819">
28
</page>
<note confidence="0.8902245">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28–36,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.996478333333333">
learning techniques, such as Support Vector
Machine (Novischi et al., 2007). In the fol-
lowing testing phase, a word is classified into
senses (Mihalcea, 2002) (Ng and Lee, 1996).
Currently supervised methods achieve the best
disambiguation quality (about 80% precision
and recall for coarse-grained WSD in the most
recent WSD evaluation conference SemEval
2007 (Navigli et al., 2007)). Nevertheless,
since training corpora are manually annotated
and expensive, supervised methods are often
brittle due to data scarcity, and it is hard to an-
notate and acquire sufficient contextual infor-
mation for every sense of a large number of
words existing in natural languages.
</bodyText>
<listItem confidence="0.999862777777778">
• Semi-supervised methods. To overcome the
knowledge acquisition bottleneck problem suf-
fered by supervised methods, these methods
make use of a small annotated corpus as seed
data in a bootstrapping process (Hearst, 1991)
(Yarowsky, 1995). A word-aligned bilingual
corpus can also serve as seed data (Ng, Wang,
and Chan, 2003).
• Unsupervised methods. These methods acquire
</listItem>
<bodyText confidence="0.886822346153846">
contextual information directly from unanno-
tated raw text, and senses can be induced from
text using some similarity measure (Lin, 1997).
However, automatically acquired information
is often noisy or even erroneous. In the most
recent SemEval 2007 (Navigli et al., 2007), the
best unsupervised systems only achieved about
70% precision and 50% recall.
Disambiguation of a limited number of words is
not hard, and necessary context information can be
carefully collected and hand-crafted to achieve high
disambiguation accuracy as shown in (Yarowsky,
1995). However, such approaches suffer a signifi-
cant performance drop in practice when domain or
vocabulary is not limited. Such a “cliff-style” per-
formance collapse is called brittleness, which is due
to insufficient knowledge and shared by many tech-
niques in Artificial Intelligence. The main challenge
of a WSD system is how to overcome the knowl-
edge acquisition bottleneck and efficiently collect
the huge amount of context knowledge. More pre-
cisely, a practical WSD need figure out how to create
and maintain a comprehensive, dynamic, and up-to-
date context knowledge base in a highly automatic
manner. The context knowledge required in WSD
has the following properties:
</bodyText>
<listItem confidence="0.949413904761905">
1. The context knowledge need cover a large
number of words and their usage. Such a
requirement of broad coverage is not trivial
because a natural language usually contains
thousands of words, and some popular words
can have dozens of senses. For example, the
Oxford English Dictionary has approximately
301,100 main entries (Oxford, 2003), and the
average polysemy of the WordNet inventory is
6.18 (Fellbaum, 1998). Clearly acquisition of
such a huge amount of knowledge can only be
achieved with automatic techniques.
2. Natural language is not a static phenomenon.
New usage of existing words emerges, which
creates new senses. New words are created,
and some words may “die” over time. It is esti-
mated that every year around 2,500 new words
appear in English (Kister, 1992). Such dynam-
ics requires a timely maintenance and updating
of context knowledge base, which makes man-
ual collection even more impractical.
</listItem>
<bodyText confidence="0.999816904761905">
Taking into consideration the large amount and
dynamic nature of context knowledge, we only have
limited options when choosing knowledge sources
for WSD. WSD is often an unconscious process to
human beings. With a dictionary and sample sen-
tences/phrases an average educated person can cor-
rectly disambiguate most polysemous words. In-
spired by human WSD process, we choose an elec-
tronic dictionary and unannotated text samples of
word instances as context knowledge sources for
our WSD system. Both sources can be automat-
ically accessed, provide an excellent coverage of
word meanings and usage, and are actively updated
to reflect the current state of languages. In this pa-
per we present a fully unsupervised WSD system,
which only requires WordNet sense inventory and
unannotated text. In the rest of this paper, section
2 describes how to acquire and represent the con-
text knowledge for WSD. We present our WSD al-
gorithm in section 3. Our WSD system is evaluated
with SemEval-2007 Task 7 (Coarse-grained English
</bodyText>
<page confidence="0.998248">
29
</page>
<figureCaption confidence="0.9889355">
Figure 1: Context Knowledge Acquisition and Represen-
tation Process
</figureCaption>
<bodyText confidence="0.932874">
All-words Task) data set, and the experiment results
are discussed in section 4. We conclude in section 5.
</bodyText>
<sectionHeader confidence="0.985805" genericHeader="method">
2 Context Knowledge Acquisition and
Representation
</sectionHeader>
<bodyText confidence="0.999427">
Figure 1 shows an overview of our context knowl-
edge acquisition process, and collected knowledge
is saved in a local knowledge base. Here are some
details about each step.
</bodyText>
<subsectionHeader confidence="0.987785">
2.1 Corpus building through Web search
</subsectionHeader>
<bodyText confidence="0.9999758">
The goal of this step is to collect as many as possi-
ble valid sample sentences containing the instances
of to-be-disambiguated words. Preferably these in-
stances are also diverse and cover many senses of a
word. We have considered two possible text sources,
</bodyText>
<listItem confidence="0.978577">
1. Electronic text collection, e.g., Gutenberg
project (Gutenberg, 1971). Such collections of-
ten include thousands of books, which are often
written by professionals and can provide many
valid and accurate usage of a large number of
words. Nevertheless, books in these collections
are usually copyright-free and old, hence are
lack of new words or new senses of words used
in modern English.
2. Web documents. Billions of documents exist
in the World Wide Web, and millions of Web
pages are created and updated everyday. Such a
huge dynamic text collection is an ideal source
</listItem>
<bodyText confidence="0.999772071428571">
to provide broad and up-to-date context knowl-
edge for WSD. The major concern about Web
documents is inconsistency of their quality, and
many Web pages are spam or contain erroneous
information. However, factual errors in Web
pages will not hurt the performance of WSD.
Nevertheless, the quality of context knowledge
is affected by broken sentences of poor linguis-
tic quality and invalid word usage, e.g., sen-
tences like “Colorless green ideas sleep furi-
ously” that violate commonsense knowledge.
Based on our experience these kind of errors
are negligible when using popular Web search
engines to retrieve relevant Web pages.
To start the acquisition process, words that need
to be disambiguated are compiled and saved in a
text file. Each single word is submitted to a Web
search engine as a query. Several search engines
provide API’s for research communities to auto-
matically retrieve large number of Web pages. In
our experiments we used both Google and Yahoo!
API’s to retrieve up to 1,000 Web pages for each to-
be-disambiguated word. Collected Web pages are
cleaned first, e.g., control characters and HTML tags
are removed. Then sentences are segmented simply
based on punctuation (e.g., ?, !, .). Sentences that
contain the instances of a specific word are extracted
and saved into a local repository.
</bodyText>
<subsectionHeader confidence="0.998877">
2.2 Parsing
</subsectionHeader>
<bodyText confidence="0.999995705882353">
Sentences organized according to each word are
sent to a dependency parser, Minipar. Dependency
parsers have been widely used in Computational
Linguistics and natural language processing. An
evaluation with the SUSANNE corpus shows that
Minipar achieves 89% precision with respect to de-
pendency relations (Lin, 1998). After parsing sen-
tences are converted to parsing trees and saved in
files. Neither our simple sentence segmentation ap-
proach nor Minipar parsing is 100% accurate, so a
small number of invalid dependency relations may
exist in parsing trees. The impact of these erroneous
relations will be minimized in our WSD algorithm.
Comparing with tagging or chunking, parsing is rel-
atively expensive and time-consuming. However, in
our method parsing is not performed in real time
when we disambiguate words. Instead, sentences
</bodyText>
<page confidence="0.995766">
30
</page>
<figureCaption confidence="0.99526325">
Figure 3: WSD Procedure
Figure 2: Merging two parsing trees. The number beside
each edge is the number of occurrences of this depen-
dency relation existing in the context knowledge base.
</figureCaption>
<bodyText confidence="0.9980096">
are parsed only once to extract dependency relations,
then these relations are merged and saved in a local
knowledge base for the following disambiguation.
Hence, parsing will not affect the speed of disam-
biguation at all.
</bodyText>
<subsectionHeader confidence="0.998417">
2.3 Merging dependency relations
</subsectionHeader>
<bodyText confidence="0.998629714285714">
After parsing, dependency relations from different
sentences are merged and saved in a context knowl-
edge base. The merging process is straightforward.
A dependency relation includes one head word/node
and one dependent word/node. Nodes from different
dependency relations are merged into one as long as
they represent the same word. An example is shown
in Figure 2, which merges the following two sen-
tences:
“Computer programmers write software.”
“Many companies hire computer programmers.”
In a dependency relation “word1 —* word2”,
word1 is the head word, and word2 is the depen-
dent word. After merging dependency relations, we
will obtain a weighted directed graph with a word
as a node, a dependency relation as an edge, and
the number of occurrences of dependency relation as
weight of an edge. This weight indicates the strength
of semantic relevancy of head word and dependent
word. This graph will be used in the following WSD
process as our context knowledge base. As a fully
automatic knowledge acquisition process, it is in-
evitable to include erroneous dependency relations
in the knowledge base. However, since in a large text
collection valid dependency relations tend to repeat
far more times than invalid ones, these erroneous
edges only have minimal impact on the disambigua-
tion quality as shown in our evaluation results.
</bodyText>
<sectionHeader confidence="0.998141" genericHeader="method">
3 WSD Algorithm
</sectionHeader>
<bodyText confidence="0.976232095238095">
Our WSD approach is based on the following in-
sight:
If a word is semantically coherent with its context,
then at least one sense of this word is semantically
coherent with its context.
Assume that the text to be disambiguated is se-
mantically valid, if we replace a word with its
glosses one by one, the correct sense should be
the one that will maximize the semantic coherence
within this word’s context. Based on this idea we
set up our WSD procedure as shown in Figure 3.
First both the original sentence that contains the
to-be-disambiguated word and the glosses of to-be-
disambiguated word are parsed. Then the parsing
tree generated from each gloss is matched with the
parsing tree of original sentence one by one. The
gloss most semantically coherent with the original
sentence will be chosen as the correct sense. How
to measure the semantic coherence is critical. Our
idea is based on the following hypotheses (assume
word1 is the to-be-disambiguated word):
</bodyText>
<listItem confidence="0.814555">
• In a sentence if word1 is dependent on word2,
</listItem>
<bodyText confidence="0.831435666666667">
and we denote the gloss of the correct sense of
word1 as g1z, then g1z contains the most se-
mantically coherent words that are dependent
</bodyText>
<page confidence="0.997746">
31
</page>
<bodyText confidence="0.590738">
on word2;
</bodyText>
<listItem confidence="0.6584512">
• In a sentence if a set of words DEP1 are de-
pendent on word1, and we denote the gloss of
the correct sense of word1 as g1i, then g1i con-
tains the most semantically coherent words that
DEP1 are dependent on.
</listItem>
<bodyText confidence="0.99981204">
For example, we try to disambiguate “company”
in “A large company hires many computer program-
mers”, after parsing we obtain the dependency rela-
tions “hire —* company” and “company —* large”.
The correct sense for the word “company” should
be “an institution created to conduct business”. If
in the context knowledge base there exist the depen-
dency relations “hire —* institution” or “institution
—* large”, then we believe that the gloss “an institu-
tion created to conduct business” is semantically co-
herent with its context - the original sentence. The
gloss with the highest semantic coherence will be
chosen as the correct sense. Obviously, the size of
context knowledge base has a positive impact on the
disambiguation quality, which is also verified in our
experiments (see Section 4.2). Figure 4 shows our
detailed WSD algorithm. Semantic coherence score
is generated by the function TreeMatching, and
we adopt a sentence as the context of a word.
We illustrate our WSD algorithm through an ex-
ample. Assume we try to disambiguate “company”
in the sentence “A large software company hires
many computer programmers”. “company” has 9
senses as a noun in WordNet 2.1. Let’s pick the fol-
lowing two glosses to go through our WSD process.
</bodyText>
<listItem confidence="0.9996215">
• an institution created to conduct business
• small military unit
</listItem>
<bodyText confidence="0.996358333333333">
First we parse the original sentence and two
glosses, and get three weighted parsing trees as
shown in Figure 5. All weights are assigned to
nodes/words in these parsing trees. In the parsing
tree of the original sentence the weight of a node is
reciprocal of the distance between this node and to-
be-disambiguated node “company” (line 12 in Fig-
ure 4). In the parsing tree of a gloss the weight
of a node is reciprocal of the level of this node in
the parsing tree (line 16 in Figure 4). Assume that
our context knowledge base contains relevant depen-
dency relations shown in Figure 6.
Input: Glosses from WordNet;
S: the sentence to be disambiguated;
G: the knowledge base generated in Section 2;
</bodyText>
<listItem confidence="0.999009071428571">
1. Input a sentence S, W = {w |w’s part of speech
is noun, verb, adjective, or adverb, w E S};
2. Parse S with a dependency parser, generate
parsing tree TS;
3. For each w E W {
4. Input all w’s glosses from WordNet;
5. For each gloss wi {
6. Parse wi, get a parsing tree Twi;
7. score = TreeMatching(TS, Twi);
}
8. If the highest score is larger than a preset
threshold, choose the sense with the
highest score as the correct sense;
9. Otherwise, choose the first sense.
</listItem>
<figure confidence="0.7098665">
10. }
TreeMatching(TS, Twi)
11. For each node nSi E TS {
12. Assign weight wSi = 1
���, lSi is the
length between nSi and wi in TS;
13. }
14. For each node nwi E Twi {
15. Load its dependent words Dwi from G;
16. Assign weight wwi = 1
</figure>
<bodyText confidence="0.928473">
iw., lwi is the
level number of nwi in Twi;
</bodyText>
<listItem confidence="0.8872788">
17. For each nSj {
18. If nSj E Dwi
19. calculate connection strength sji
between nSj and nwi;
20. score = score + wSi x wwi x sji;
</listItem>
<figure confidence="0.861769666666667">
21. }
22. }
23. Return score;
</figure>
<figureCaption confidence="0.999863">
Figure 4: WSD Algorithm
</figureCaption>
<bodyText confidence="0.999710285714286">
The weights in the context knowledge base are as-
signed to dependency relation edges. These weights
are normalized to [0, 1] based on the number of de-
pendency relation instances obtained in the acquisi-
tion and merging process. A large number of occur-
rences will be normalized to a high value (close to
1), and a small number of occurrences will be nor-
</bodyText>
<page confidence="0.998718">
32
</page>
<figureCaption confidence="0.999956666666667">
Figure 5: Weighted parsing trees of the original sentence
and two glosses of “company”
Figure 6: A fragment of context knowledge base
</figureCaption>
<bodyText confidence="0.9939675">
malized to a low value (close to 0).
Now we load the dependent words of each word
in gloss 1 from the knowledge base (line 14, 15 in
Figure 4), and we get {small, large} for “institu-
tion” and {large, software} for “business”. In the
dependent words of “company”, “large” belongs to
the dependent word sets of “institution” and “busi-
ness”, and “software” belongs to the dependent word
set of “business”, so the coherence score of gloss 1
is calculated as (line 19, 20 in Figure 4):
</bodyText>
<equation confidence="0.9196865">
1.0x1.0x0.7 + 1.0x0.25x0.8 + 1.0x0.25x0.9
= 1.125
</equation>
<bodyText confidence="0.9999562">
We go through the same process with the second
gloss “small military unit”. “Large” is the only de-
pendent word of “company” appearing in the depen-
dent word set of “unit” in gloss 2, so the coherence
score of gloss 2 in the current context is:
</bodyText>
<equation confidence="0.852418">
1.0 x 1.0 x 0.8 = 0.8
</equation>
<bodyText confidence="0.999977675675676">
After comparing the coherence scores of two
glosses, we choose sense 1 of “company” as the cor-
rect sense (line 9 in Figure 4). This example illus-
trates that a strong dependency relation between a
head word and a dependent word has a powerful dis-
ambiguation capability, and disambiguation quality
is also significantly affected by the quality of dictio-
nary definitions.
In Figure 4 the TreeMatching function matches
the dependent words of to-be-disambiguated word
(line 15 in Figure 4), and we call this matching strat-
egy as dependency matching. This strategy will not
work if a to-be-disambiguated word has no depen-
dent words at all, for example, when the word “com-
pany” in “Companies hire computer programmers”
has no dependent words. In this case, we developed
the second matching strategy, which is to match the
head words that the to-be-disambiguated word is de-
pendent on, such as matching “hire” (the head word
of “company”) in Figure 5(a). Using the dependency
relation “hire —* company”, we can correctly choose
sense 1 since there is no such relation as “hire —*
unit” in the knowledge base. This strategy is also
helpful when disambiguating adjectives and adverbs
since they usually only depend on other words, and
rarely any other words are dependent on them. The
third matching strategy is to consider synonyms as a
match besides the exact matching words. Synonyms
can be obtained through the synsets in WordNet.
For example, when we disambiguate “company” in
“Big companies hire many computer programmers”,
“big” can be considered as a match for “large”. We
call this matching strategy as synonym matching.
The three matching strategies can be combined and
applied together, and in Section 4.1 we show the
experiment results of 5 different matching strategy
combinations.
</bodyText>
<page confidence="0.99912">
33
</page>
<sectionHeader confidence="0.99853" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999693178571429">
We have evaluated our method using SemEval-2007
Task 07 (Coarse-grained English All-words Task)
test set (Navigli et al., 2007). The task organiz-
ers provide a coarse-grained sense inventory cre-
ated with SSI algorithm (Navigli and Velardi, 2005),
training data, and test data. Since our method
does not need any training or special tuning, neither
coarse-grained sense inventory nor training data was
used. The test data includes: a news article about
“homeless” (including totally 951 words, 368 words
are annotated and need to be disambiguated), a re-
view of the book “Feeding Frenzy” (including to-
tally 987 words, 379 words are annotated and need
to be disambiguated), an article about some trav-
eling experience in France (including totally 1311
words, 500 words are annotated and need to be dis-
ambiguated), computer programming(including to-
tally 1326 words, 677 words are annotated and need
to be disambiguated), and a biography of the painter
Masaccio (including totally 802 words, 345 words
are annotated and need to be disambiguated). Two
authors of (Navigli et al., 2007) independently and
manually annotated part of the test set (710 word
instances), and the pairwise agreement was 93.80%.
This inter-annotator agreement is usually considered
an upper-bound for WSD systems.
We followed the WSD process described in Sec-
tion 2 and 3 using the WordNet 2.1 sense repository
that is adopted by SemEval-2007 Task 07. All exper-
iments were performed on a Pentium 2.33GHz dual
core PC with 3GB memory. Among the 2269 to-
be-disambiguated words in the five test documents,
1112 words are unique and submitted to Google
API as queries. The retrieved Web pages were
cleaned, and 1945189 relevant sentences were ex-
tracted. On average 1749 sentences were obtained
for each word. The Web page retrieval step took 3
days, and the cleaning step took 2 days. Parsing was
very time-consuming and took 11 days. The merg-
ing step took 3 days. Disambiguation of 2269 words
in the 5 test articles took 4 hours. All these steps can
be parallelized and run on multiple computers, and
the whole process will be shortened accordingly.
The overall disambiguation results are shown in
Table 1. For comparison we also listed the re-
sults of the top three systems and three unsuper-
vised systems participating in SemEval-2007 Task
07. All of the top three systems (UoR-SSI, NUS-
PT, NUS-ML) are supervised systems, which used
annotated resources (e.g., SemCor, Defense Science
Organization Corpus) during the training phase. Our
fully unsupervised WSD system significantly out-
performs the three unsupervised systems (SUSSZ-
FR, SUSSX-C-WD, SUSSX-CR) and achieves per-
formance approaching the top-performing super-
vised WSD systems.
</bodyText>
<subsectionHeader confidence="0.9943965">
4.1 Impact of different matching strategies to
disambiguation quality
</subsectionHeader>
<bodyText confidence="0.9999964">
To test the effectiveness of different matching strate-
gies discussed in Section 3, we performed some ad-
ditional experiments. Table 2 shows the disambigua-
tion results by each individual document with the
following 5 matching strategies:
</bodyText>
<listItem confidence="0.984118571428571">
1. Dependency matching only.
2. Dependency and backward matching.
3. Dependency and synonym backward matching.
4. Dependency and synonym dependency match-
ing.
5. Dependency, backward, synonym backward,
and synonym dependency matching.
</listItem>
<bodyText confidence="0.9999054">
As expected combination of more matching
strategies results in higher disambiguation quality.
By analyzing the scoring details, we verified that
backward matching is especially useful to disam-
biguate adjectives and adverbs. Adjectives and ad-
verbs are often dependent words, so dependency
matching itself rarely finds any matched words.
Since synonyms are semantically equivalent, it is
reasonable that synonym matching can also improve
disambiguation performance.
</bodyText>
<subsectionHeader confidence="0.951347">
4.2 Impact of knowledge base size to
disambiguation quality
</subsectionHeader>
<bodyText confidence="0.999705666666667">
To test the impact of knowledge base size to dis-
ambiguation quality we randomly selected 1339264
sentences (about two thirds of all sentences) from
our text collection and built a smaller knowledge
base. Table 3 shows the experiment results. Overall
disambiguation quality has dropped slightly, which
</bodyText>
<page confidence="0.995384">
34
</page>
<table confidence="0.9996165">
System Attempted Precision Recall F1
UoR-SSI 100.0 83.21 83.21 83.21
NUS-PT 100.0 82.50 82.50 82.50
NUS-ML 100.0 81.58 81.58 81.58
TreeMatch 100.0 73.65 73.65 73.65
SUSSZ-FR 72.8 71.73 52.23 60.44
SUSSX-C-WD 72.8 54.54 39.71 45.96
SUSSX-CR 72.8 54.30 39.53 45.75
</table>
<tableCaption confidence="0.987189">
Table 1: Overall disambiguation scores (Our system “TreeMatch” is marked in bold)
</tableCaption>
<table confidence="0.999875428571429">
Matching d001 d002 d003 d004 d005 Overall
strategy P R P R P R P R P R P R
1 72.28 72.28 66.23 66.23 63.20 63.20 66.47 66.47 56.52 56.52 65.14 65.14
2 70.65 70.65 70.98 70.98 65.20 65.20 72.23 72.23 58.84 58.84 68.18 68.18
3 79.89 79.89 75.20 75.20 69.00 69.00 71.94 71.94 64.64 64.64 72.01 72.01
4 80.71 80.71 78.10 78.10 72.80 72.80 71.05 71.05 67.54 67.54 73.65 73.65
5 80.16 80.16 78.10 78.10 69.40 69.40 72.82 72.82 66.09 66.09 73.12 73.12
</table>
<tableCaption confidence="0.998069">
Table 2: Disambiguation scores by article with 5 matching strategies
</tableCaption>
<bodyText confidence="0.9977396">
shows a positive correlation between the amount of
context knowledge and disambiguation quality. It is
reasonable to assume that our disambiguation per-
formance can be improved further by collecting and
incorporating more context knowledge.
</bodyText>
<table confidence="0.998569">
Matching Overall
strategy P R
1 65.36 65.36
2 67.78 67.78
3 68.09 68.09
4 70.69 70.69
5 67.78 67.78
</table>
<tableCaption confidence="0.973428">
Table 3: Disambiguation scores by article with a smaller
knowledge base
</tableCaption>
<sectionHeader confidence="0.991377" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998438636363636">
Broad coverage and disambiguation quality are crit-
ical for WSD techniques to be adopted in prac-
tice. This paper proposed a fully unsupervised
WSD method. We have evaluated our approach with
SemEval-2007 Task 7 (Coarse-grained English All-
words Task) data set, and we achieved F-scores ap-
proaching the top performing supervised WSD sys-
tems. By using widely available unannotated text
and a fully unsupervised disambiguation approach,
our method may provide a viable solution to the
problem of WSD. The future work includes:
</bodyText>
<listItem confidence="0.707199352941176">
1. Continue to build the knowledge base, enlarge
the coverage and improve the system perfor-
mance. The experiment results in Section 4.2
clearly show that more word instances can im-
prove the disambiguation accuracy and recall
scores;
2. WSD is often an unconscious process for hu-
man beings. It is unlikely that a reader exam-
ines all surrounding words when determining
the sense of a word, which calls for a smarter
and more selective matching strategy than what
we have tried in Section 4.1;
3. Test our WSD system on fine-grained SemEval
2007 WSD task 17. Although we only evalu-
ated our approach with coarse-grained senses,
our method can be directly applied to fine-
grained WSD without any modifications.
</listItem>
<sectionHeader confidence="0.998188" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996347">
This work is partially funded by NSF grant 0737408
and Scholar Academy at the University of Houston
Downtown. This paper contains proprietary infor-
mation protected under a pending U.S. patent.
</bodyText>
<page confidence="0.998996">
35
</page>
<sectionHeader confidence="0.995882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860397590361">
Agirre, Eneko, Philip Edmonds (eds.). 2006. Word
Sense Disambiguation: Algorithms and Applications,
Springer.
Chklovski, T. and Mihalcea, R. 2002. Building a sense
tagged corpus with open mind word expert. In Pro-
ceedings of the Acl-02 Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Directions,
Morristown, NJ, 116-122.
C. Fellbaum, WordNet: An Electronic Lexical Database,
MIT press, 1998
Project Gutenberg, available at www.gutenberg.org
Hearst, M. (1991) Noun Homograph Disambiguation Us-
ing Local Context in Large Text Corpora, Proc. 7th
Annual Conference of the University of Waterloo Cen-
ter for the New OED and Text Research, Oxford.
Nancy Ide and Jean V´eronis. 1998. Introduction to the
special issue on word sense disambiguation: the state
of the art. Comput. Linguist., 24(1):2–40.
Kister, Ken. “Dictionaries defined”, Library Journal, Vol.
117 Issue 11, p43, 4p, 2bw
Lesk, M. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of the 5th An-
nual international Conference on Systems Documenta-
tion (Toronto, Ontario, Canada). V. DeBuys, Ed. SIG-
DOC ’86.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the LREC Workshop on
the Evaluation of Parsing Systems, pages 234–241,
Granada, Spain.
Lin, D. 1997. Using syntactic dependency as local con-
text to resolve word sense ambiguity. In Proceedings of
the 35th Annual Meeting of the Association For Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association For Computa-
tional Linguistics (Madrid, Spain, July 07 - 12, 1997).
Rada Mihalcea, Using Wikipedia for Automatic Word
Sense Disambiguation, in Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2007), Rochester, April
2007.
Rada Mihalcea. 2002. Instance based learning with au-
tomatic feature selection applied to word sense disam-
biguation. In Proceedings of the 19th international
conference on Computational linguistics, pages 1–7,
Morristown, NJ.
Dan Moldovan and Vasile Rus, Explaining Answers with
Extended WordNet, ACL 2001.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30–35, Prague, Czech
Republic.
Roberto Navigli and Paola Velardi. 2005. Structural se-
mantic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI),
27(7):10631074.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. Exploit-
ing Parallel Texts for Word Sense Disambiguation: An
Empirical Study. ACL, 2003.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings of
the 34th annual meeting on Association for Computa-
tional Linguistics, pages 40–47, Morristown, NJ.
Adrian Novischi, Muirathnam Srikanth, and Andrew
Bennett. 2007. Lcc-wsd: System description for En-
glish coarse grained all words task at semeval 2007.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 223–
226, Prague, Czech Republic.
Catherine Soanes and Angus Stevenson, editors. 2003.
Oxford Dictionary of English. Oxford University
Press.
Rada Mihalcea, available at
http://www.cs.unt.edu/ rada/downloads.html
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of the 33rd Annual Meeting on Association For Com-
putational Linguistics (Cambridge, Massachusetts,
June 26 - 30, 1995).
</reference>
<page confidence="0.99892">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.465274">
<title confidence="0.986489">A Fully Unsupervised Word Sense Disambiguation Method Dependency Knowledge</title>
<author confidence="0.739809">Ping</author>
<affiliation confidence="0.999066">Dept. of Computer and Math. University of</affiliation>
<email confidence="0.995618">chenp@uhd.edu</email>
<author confidence="0.907133">Chris</author>
<affiliation confidence="0.9986615">Dept. of Computer and Math. University of</affiliation>
<email confidence="0.998878">bowesc@uhd.edu</email>
<abstract confidence="0.990405366666666">Word sense disambiguation is the process of determining which sense of a word is used in a given context. Due to its importance in understanding semantics of natural languages, word sense disambiguation has been extensively studied in Computational Linguistics. However, existing methods either are brittle and narrowly focus on specific topics or words, or provide only mediocre performance in real-world settings. Broad coverage and disambiguation quality are critical for a word sense disambiguation system. In this paper we present a fully unsupervised word sense disambiguation method that requires only a dictionary and unannotated text as input. Such an automatic approach overcomes the problem of brittleness suffered in many existing methods and makes broad-coverage word sense disambiguation feasible in practice. We evaluated our approach using SemEval 2007 Task 7 (Coarse-grained English All-words Task), and our system significantly outperformed the best unsupervised system participating in SemEval 2007 and achieved the performance approaching top-performing supervised systems. Although our method was only tested with coarse-grained sense disambiguation, it can be directly applied to fine-grained sense disambiguation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2006</date>
<booktitle>Word Sense Disambiguation: Algorithms and Applications,</booktitle>
<editor>Agirre, Eneko, Philip Edmonds (eds.).</editor>
<publisher>Springer.</publisher>
<marker>2006</marker>
<rawString>Agirre, Eneko, Philip Edmonds (eds.). 2006. Word Sense Disambiguation: Algorithms and Applications, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>R Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with open mind word expert.</title>
<date>2002</date>
<booktitle>In Proceedings of the Acl-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>116--122</pages>
<location>Morristown, NJ,</location>
<contexts>
<context position="2440" citStr="Chklovski and Mihalcea, 2002" startWordPosition="346" endWordPosition="350">s University of Houston-Downtown brownd@uhd.edu is the process of determining which sense of a homograph is used in a given context. WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval. Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictio</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Chklovski, T. and Mihalcea, R. 2002. Building a sense tagged corpus with open mind word expert. In Proceedings of the Acl-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Morristown, NJ, 116-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT press,</publisher>
<note>Project Gutenberg, available at www.gutenberg.org</note>
<contexts>
<context position="2363" citStr="Fellbaum, 1998" startWordPosition="337" endWordPosition="338">ding@cs.umb.edu David Brown Dept. of Computer and Math. Sciences University of Houston-Downtown brownd@uhd.edu is the process of determining which sense of a homograph is used in a given context. WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval. Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words</context>
<context position="6261" citStr="Fellbaum, 1998" startWordPosition="928" endWordPosition="929"> figure out how to create and maintain a comprehensive, dynamic, and up-todate context knowledge base in a highly automatic manner. The context knowledge required in WSD has the following properties: 1. The context knowledge need cover a large number of words and their usage. Such a requirement of broad coverage is not trivial because a natural language usually contains thousands of words, and some popular words can have dozens of senses. For example, the Oxford English Dictionary has approximately 301,100 main entries (Oxford, 2003), and the average polysemy of the WordNet inventory is 6.18 (Fellbaum, 1998). Clearly acquisition of such a huge amount of knowledge can only be achieved with automatic techniques. 2. Natural language is not a static phenomenon. New usage of existing words emerges, which creates new senses. New words are created, and some words may “die” over time. It is estimated that every year around 2,500 new words appear in English (Kister, 1992). Such dynamics requires a timely maintenance and updating of context knowledge base, which makes manual collection even more impractical. Taking into consideration the large amount and dynamic nature of context knowledge, we only have li</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum, WordNet: An Electronic Lexical Database, MIT press, 1998 Project Gutenberg, available at www.gutenberg.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Noun Homograph Disambiguation Using Local Context in Large Text Corpora,</title>
<date>1991</date>
<booktitle>Proc. 7th Annual Conference of the University of Waterloo Center for the New OED and Text Research,</booktitle>
<location>Oxford.</location>
<contexts>
<context position="4468" citStr="Hearst, 1991" startWordPosition="651" endWordPosition="652">call for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A word-aligned bilingual corpus can also serve as seed data (Ng, Wang, and Chan, 2003). • Unsupervised methods. These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure (Lin, 1997). However, automatically acquired information is often noisy or even erroneous. In the most recent SemEval 2007 (Navigli et al., 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. Disambiguation of a limited number of words is not hard, and necessary context information can </context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, M. (1991) Noun Homograph Disambiguation Using Local Context in Large Text Corpora, Proc. 7th Annual Conference of the University of Waterloo Center for the New OED and Text Research, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean V´eronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: the state of the art.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Ide, V´eronis, 1998</marker>
<rawString>Nancy Ide and Jean V´eronis. 1998. Introduction to the special issue on word sense disambiguation: the state of the art. Comput. Linguist., 24(1):2–40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ken Kister</author>
</authors>
<title>Dictionaries defined”,</title>
<journal>Library Journal,</journal>
<volume>117</volume>
<pages>43--4</pages>
<marker>Kister, </marker>
<rawString>Kister, Ken. “Dictionaries defined”, Library Journal, Vol. 117 Issue 11, p43, 4p, 2bw</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<journal>SIGDOC</journal>
<booktitle>In Proceedings of the 5th Annual international Conference on Systems Documentation</booktitle>
<volume>86</volume>
<location>(Toronto, Ontario, Canada).</location>
<contexts>
<context position="3069" citStr="Lesk, 1986" startWordPosition="443" endWordPosition="444"> (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictionary definitions (Lesk, 1986). • Supervised methods. Supervised methods mainly adopt context to disambiguate words. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to create a classifier using machine 28 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics learning techniques, such as Support Vector Machine (Novischi et al., 2007). In the follo</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, M. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual international Conference on Systems Documentation (Toronto, Ontario, Canada). V. DeBuys, Ed. SIGDOC ’86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC Workshop on the Evaluation of Parsing Systems,</booktitle>
<pages>234--241</pages>
<location>Granada,</location>
<contexts>
<context position="10685" citStr="Lin, 1998" startWordPosition="1641" endWordPosition="1642">uated word. Collected Web pages are cleaned first, e.g., control characters and HTML tags are removed. Then sentences are segmented simply based on punctuation (e.g., ?, !, .). Sentences that contain the instances of a specific word are extracted and saved into a local repository. 2.2 Parsing Sentences organized according to each word are sent to a dependency parser, Minipar. Dependency parsers have been widely used in Computational Linguistics and natural language processing. An evaluation with the SUSANNE corpus shows that Minipar achieves 89% precision with respect to dependency relations (Lin, 1998). After parsing sentences are converted to parsing trees and saved in files. Neither our simple sentence segmentation approach nor Minipar parsing is 100% accurate, so a small number of invalid dependency relations may exist in parsing trees. The impact of these erroneous relations will be minimized in our WSD algorithm. Comparing with tagging or chunking, parsing is relatively expensive and time-consuming. However, in our method parsing is not performed in real time when we disambiguate words. Instead, sentences 30 Figure 3: WSD Procedure Figure 2: Merging two parsing trees. The number beside</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the LREC Workshop on the Evaluation of Parsing Systems, pages 234–241, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association For Computational Linguistics and Eighth Conference of the European Chapter of the Association For Computational Linguistics</booktitle>
<volume>12</volume>
<location>Madrid, Spain,</location>
<contexts>
<context position="4757" citStr="Lin, 1997" startWordPosition="695" endWordPosition="696">nt contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A word-aligned bilingual corpus can also serve as seed data (Ng, Wang, and Chan, 2003). • Unsupervised methods. These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure (Lin, 1997). However, automatically acquired information is often noisy or even erroneous. In the most recent SemEval 2007 (Navigli et al., 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in (Yarowsky, 1995). However, such approaches suffer a significant performance drop in practice when domain or vocabulary is not limited. Such a “cliff-style” performance collapse is called brittlenes</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Lin, D. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of the Association For Computational Linguistics and Eighth Conference of the European Chapter of the Association For Computational Linguistics (Madrid, Spain, July 07 - 12, 1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for Automatic Word Sense Disambiguation,</title>
<date>2007</date>
<booktitle>in Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL 2007),</booktitle>
<location>Rochester,</location>
<contexts>
<context position="2511" citStr="Mihalcea, 2007" startWordPosition="358" endWordPosition="359">ense of a homograph is used in a given context. WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval. Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictionary definitions (Lesk, 1986). • Supervised methods. Supervised methods</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea, Using Wikipedia for Automatic Word Sense Disambiguation, in Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL 2007), Rochester, April 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="2440" citStr="Mihalcea, 2002" startWordPosition="348" endWordPosition="350">f Houston-Downtown brownd@uhd.edu is the process of determining which sense of a homograph is used in a given context. WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval. Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictio</context>
<context position="3738" citStr="Mihalcea, 2002" startWordPosition="541" endWordPosition="542">t context to disambiguate words. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to create a classifier using machine 28 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics learning techniques, such as Support Vector Machine (Novischi et al., 2007). In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffere</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada Mihalcea. 2002. Instance based learning with automatic feature selection applied to word sense disambiguation. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Vasile Rus</author>
</authors>
<title>Explaining Answers with Extended WordNet,</title>
<date>2001</date>
<location>ACL</location>
<contexts>
<context position="2483" citStr="Moldovan and Rus, 2001" startWordPosition="353" endWordPosition="356">s the process of determining which sense of a homograph is used in a given context. WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval. Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictionary definitions (Lesk, 1986). • Supervised</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Dan Moldovan and Vasile Rus, Explaining Answers with Extended WordNet, ACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 07: Coarsegrained english all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>30--35</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3963" citStr="Navigli et al., 2007" startWordPosition="572" endWordPosition="575">tracted to create a classifier using machine 28 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics learning techniques, such as Support Vector Machine (Novischi et al., 2007). In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A word-aligned bilingual corpus can also serve as seed data (Ng, Wang, and C</context>
<context position="20524" citStr="Navigli et al., 2007" startWordPosition="3335" endWordPosition="3338">sider synonyms as a match besides the exact matching words. Synonyms can be obtained through the synsets in WordNet. For example, when we disambiguate “company” in “Big companies hire many computer programmers”, “big” can be considered as a match for “large”. We call this matching strategy as synonym matching. The three matching strategies can be combined and applied together, and in Section 4.1 we show the experiment results of 5 different matching strategy combinations. 33 4 Experiments We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al., 2007). The task organizers provide a coarse-grained sense inventory created with SSI algorithm (Navigli and Velardi, 2005), training data, and test data. Since our method does not need any training or special tuning, neither coarse-grained sense inventory nor training data was used. The test data includes: a news article about “homeless” (including totally 951 words, 368 words are annotated and need to be disambiguated), a review of the book “Feeding Frenzy” (including totally 987 words, 379 words are annotated and need to be disambiguated), an article about some traveling experience in France (inc</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. Semeval-2007 task 07: Coarsegrained english all-words task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 30–35, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="20641" citStr="Navigli and Velardi, 2005" startWordPosition="3353" endWordPosition="3356">Net. For example, when we disambiguate “company” in “Big companies hire many computer programmers”, “big” can be considered as a match for “large”. We call this matching strategy as synonym matching. The three matching strategies can be combined and applied together, and in Section 4.1 we show the experiment results of 5 different matching strategy combinations. 33 4 Experiments We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al., 2007). The task organizers provide a coarse-grained sense inventory created with SSI algorithm (Navigli and Velardi, 2005), training data, and test data. Since our method does not need any training or special tuning, neither coarse-grained sense inventory nor training data was used. The test data includes: a news article about “homeless” (including totally 951 words, 368 words are annotated and need to be disambiguated), a review of the book “Feeding Frenzy” (including totally 987 words, 379 words are annotated and need to be disambiguated), an article about some traveling experience in France (including totally 1311 words, 500 words are annotated and need to be disambiguated), computer programming(including tota</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(7):10631074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study.</title>
<date>2003</date>
<publisher>ACL,</publisher>
<contexts>
<context position="2555" citStr="Ng, Wang, and Chan, 2003" startWordPosition="362" endWordPosition="366">ven context. WSD is a long-standing problem in Computational Linguistics, and has significant impact in many real-world applications including machine translation, information extraction, and information retrieval. Generally, WSD methods use the context of a word for its sense disambiguation, and the context information can come from either annotated/unannotated text or other knowledge resources, such as WordNet (Fellbaum, 1998), SemCor (SemCor, 2008), Open Mind Word Expert (Chklovski and Mihalcea, 2002), eXtended WordNet (Moldovan and Rus, 2001), Wikipedia (Mihalcea, 2007), parallel corpora (Ng, Wang, and Chan, 2003). In (Ide and V´eronis, 1998) many different WSD approaches were described. Usually, WSD techniques can be divided into four categories (Agirre and Edmonds, 2006), • Dictionary and knowledge based methods. These methods use lexical knowledge bases such as dictionaries and thesauri, and hypothesize that context knowledge can be extracted from definitions of words. For example, Lesk disambiguated two words by finding the pair of senses with the greatest word overlap in their dictionary definitions (Lesk, 1986). • Supervised methods. Supervised methods mainly adopt context to disambiguate words.</context>
<context position="4572" citStr="Ng, Wang, and Chan, 2003" startWordPosition="665" endWordPosition="669">i et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A word-aligned bilingual corpus can also serve as seed data (Ng, Wang, and Chan, 2003). • Unsupervised methods. These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure (Lin, 1997). However, automatically acquired information is often noisy or even erroneous. In the most recent SemEval 2007 (Navigli et al., 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in (Yarowsky, 1</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study. ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="3757" citStr="Ng and Lee, 1996" startWordPosition="543" endWordPosition="546">mbiguate words. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to create a classifier using machine 28 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics learning techniques, such as Support Vector Machine (Novischi et al., 2007). In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffered by supervised met</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 40–47, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Novischi</author>
<author>Muirathnam Srikanth</author>
<author>Andrew Bennett</author>
</authors>
<title>Lcc-wsd: System description for English coarse grained all words task at semeval</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>223--226</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3655" citStr="Novischi et al., 2007" startWordPosition="525" endWordPosition="528"> dictionary definitions (Lesk, 1986). • Supervised methods. Supervised methods mainly adopt context to disambiguate words. A supervised method includes a training phase and a testing phase. In the training phase, a sense-annotated training corpus is required, from which syntactic and semantic features are extracted to create a classifier using machine 28 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28–36, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics learning techniques, such as Support Vector Machine (Novischi et al., 2007). In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-s</context>
</contexts>
<marker>Novischi, Srikanth, Bennett, 2007</marker>
<rawString>Adrian Novischi, Muirathnam Srikanth, and Andrew Bennett. 2007. Lcc-wsd: System description for English coarse grained all words task at semeval 2007. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 223– 226, Prague, Czech Republic.</rawString>
</citation>
<citation valid="false">
<booktitle>2003. Oxford Dictionary of English.</booktitle>
<editor>Catherine Soanes and Angus Stevenson, editors.</editor>
<publisher>Oxford University Press.</publisher>
<marker></marker>
<rawString>Catherine Soanes and Angus Stevenson, editors. 2003. Oxford Dictionary of English. Oxford University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rada Mihalcea</author>
</authors>
<note>available at http://www.cs.unt.edu/ rada/downloads.html</note>
<marker>Mihalcea, </marker>
<rawString>Rada Mihalcea, available at http://www.cs.unt.edu/ rada/downloads.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting on Association For Computational Linguistics</booktitle>
<volume>26</volume>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="4485" citStr="Yarowsky, 1995" startWordPosition="653" endWordPosition="654">-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al., 2007)). Nevertheless, since training corpora are manually annotated and expensive, supervised methods are often brittle due to data scarcity, and it is hard to annotate and acquire sufficient contextual information for every sense of a large number of words existing in natural languages. • Semi-supervised methods. To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). A word-aligned bilingual corpus can also serve as seed data (Ng, Wang, and Chan, 2003). • Unsupervised methods. These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure (Lin, 1997). However, automatically acquired information is often noisy or even erroneous. In the most recent SemEval 2007 (Navigli et al., 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. Disambiguation of a limited number of words is not hard, and necessary context information can be carefully coll</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting on Association For Computational Linguistics (Cambridge, Massachusetts, June 26 - 30, 1995).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>