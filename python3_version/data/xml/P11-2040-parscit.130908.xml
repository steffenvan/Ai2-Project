<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000808">
<title confidence="0.988235">
Discrete vs. Continuous Rating Scales for Language Evaluation in NLP
</title>
<author confidence="0.996228">
Anja Belz Eric Kow
</author>
<affiliation confidence="0.997758">
School of Computing, Engineering and Mathematics
University of Brighton
</affiliation>
<address confidence="0.995877">
Brighton BN2 4GJ, UK
</address>
<email confidence="0.999762">
{A.S.Belz,E.Y.Kow}@brighton.ac.uk
</email>
<sectionHeader confidence="0.998648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996386833333333">
Studies assessing rating scales are very com-
mon in psychology and related fields, but
are rare in NLP. In this paper we as-
sess discrete and continuous scales used for
measuring quality assessments of computer-
generated language. We conducted six sep-
arate experiments designed to investigate the
validity, reliability, stability, interchangeabil-
ity and sensitivity of discrete vs. continuous
scales. We show that continuous scales are vi-
able for use in language evaluation, and offer
distinct advantages over discrete scales.
</bodyText>
<sectionHeader confidence="0.945606" genericHeader="categories and subject descriptors">
1 Background and Introduction
</sectionHeader>
<bodyText confidence="0.999948462962963">
Rating scales have been used for measuring hu-
man perception of various stimuli for a long time,
at least since the early 20th century (Freyd, 1923).
First used in psychology and psychophysics, they
are now also common in a variety of other disci-
plines, including NLP. Discrete scales are the only
type of scale commonly used for qualitative assess-
ments of computer-generated language in NLP (e.g.
in the DUC/TAC evaluation competitions). Contin-
uous scales are commonly used in psychology and
related fields, but are virtually unknown in NLP.
While studies assessing the quality of individual
scales and comparing different types of rating scales
are common in psychology and related fields, such
studies hardly exist in NLP, and so at present little is
known about whether discrete scales are a suitable
rating tool for NLP evaluation tasks, or whether con-
tinuous scales might provide a better alternative.
A range of studies from sociology, psychophys-
iology, biometrics and other fields have compared
discrete and continuous scales. Results tend to dif-
fer for different types of data. E.g., results from pain
measurement show a continuous scale to outperform
a discrete scale (ten Klooster et al., 2006). Other
results (Svensson, 2000) from measuring students’
ease of following lectures show a discrete scale to
outperform a continuous scale. When measuring
dyspnea, Lansing et al. (2003) found a hybrid scale
to perform on a par with a discrete scale.
Another consideration is the types of data pro-
duced by discrete and continuous scales. Parametric
methods of statistical analysis, which are far more
sensitive than non-parametric ones, are commonly
applied to both discrete and continuous data. How-
ever, parametric methods make very strong assump-
tions about data, including that it is numerical and
normally distributed (Siegel, 1957). If these as-
sumptions are violated, then the significance of re-
sults is overestimated. Clearly, the numerical as-
sumption does not hold for the categorial data pro-
duced by discrete scales, and it is unlikely to be nor-
mally distributed. Many researchers are happier to
apply parametric methods to data from continuous
scales, and some simply take it as read that such data
is normally distributed (Lansing et al., 2003).
Our aim in the present study was to system-
atically assess and compare discrete and continu-
ous scales when used for the qualitative assess-
ment of computer-generated language. We start with
an overview of assessment scale types (Section 2).
We describe the experiments we conducted (Sec-
tion 4), the data we used in them (Section 3), and
the properties we examined in our inter-scale com-
parisons (Section 5), before presenting our results
</bodyText>
<page confidence="0.933987">
230
</page>
<note confidence="0.564975">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 230–235,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<listItem confidence="0.969309">
Q1: Grammaticality The summary should have no date-
lines, system-internal formatting, capitalization errors or obvi-
ously ungrammatical sentences (e.g., fragments, missing com-
ponents) that make the text difficult to read.
1. Very Poor
2. Poor
3. Barely Acceptable
4. Good
5. Very Good
</listItem>
<figureCaption confidence="0.9394856">
Figure 1: Evaluation of Readability in DUC’06, compris-
ing 5 evaluation criteria, including Grammaticality. Eval-
uation task for each summary text: evaluator selects one
of the options (1–5) to represent quality of the summary
in terms of the criterion.
</figureCaption>
<bodyText confidence="0.733118">
(Section 6), and some conclusions (Section 7).
</bodyText>
<sectionHeader confidence="0.977667" genericHeader="method">
2 Rating Scales
</sectionHeader>
<bodyText confidence="0.998466657142857">
With Verbal Descriptor Scales (VDSs), partici-
pants give responses on ordered lists of verbally de-
scribed and/or numerically labelled response cate-
gories, typically varying in number from 2 to 11
(Svensson, 2000). An example of a VDS used in NLP
is shown in Figure 1. VDSs are used very widely in
contexts where computationally generated language
is evaluated, including in dialogue, summarisation,
MT and data-to-text generation.
Visual analogue scales (VASs) are far less com-
mon outside psychology and related areas than
VDSs. Responses are given by selecting a point on
a typically horizontal line (although vertical lines
have also been used (Scott and Huskisson, 2003)),
on which the two end points represent the extreme
values of the variable to be measured. Such lines
can be mono-polar or bi-polar, and the end points
are labelled with an image (smiling/frowning face),
or a brief verbal descriptor, to indicate which end
of the line corresponds to which extreme of the vari-
able. The labels are commonly chosen to represent a
point beyond any response actually likely to be cho-
sen by raters. There is only one examples of a VAS
in NLP system evaluation that we are aware of (Gatt
et al., 2009).
Hybrid scales, known as a graphic rating scales,
combine the features of VDSs and VASs, and are also
used in psychology. Here, the verbal descriptors are
aligned along the line of a VAS and the endpoints are
typically unmarked (Svensson, 2000). We are aware
of one example in NLP (Williams and Reiter, 2008);
Q1: Grammaticality The summary should have no datelines,
system-internal formatting, capitalization errors or obviously
ungrammatical sentences (e.g., fragments, missing compo-
nents) that make the text difficult to read.
</bodyText>
<figure confidence="0.751155">
excellent
bad
</figure>
<figureCaption confidence="0.99821525">
Figure 2: Evaluation of Grammaticality with alternative
VAS scale (cf. Figure 1). Evaluation task for each sum-
mary text: evaluator selects a place on the line to repre-
sent quality of the summary in terms of the criterion.
</figureCaption>
<bodyText confidence="0.999600952380952">
we did not investigate this scale in our study.
We used the following two specific scale designs
in our experiments:
VDS-7: 7 response categories, numbered (7 =
best) and verbally described (e.g. 7 = “perfectly flu-
ent” for Fluency, and 7 = “perfectly clear” for Clar-
ity). Response categories were presented in a verti-
cal list, with the best category at the bottom. Each
category had a tick-box placed next to it; the rater’s
task was to tick the box by their chosen rating.
VAS: a horizontal, bi-polar line, with no ticks on
it, mapping to 0–100. In the image description tests,
statements identified the left end as negative, the
right end as positive; in the weather forecast tests,
the positive end had a smiling face and the label
“statement couldn’t be clearer/read better”; the neg-
ative end had a frowning face and the label “state-
ment couldn’t be more unclear/read worse”. The
raters’ task was to move a pointer (initially in the
middle of the line) to the place corresponding to
their rating.
</bodyText>
<sectionHeader confidence="0.996666" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.998714727272727">
Weather forecast texts: In one half of our evalua-
tion experiments we used human-written and auto-
matically generated weather forecasts for the same
weather data. The data in our evaluations was for 22
different forecast dates and included outputs from 10
generator systems and one set of human forecasts.
This data has also been used for comparative sys-
tem evaluation in previous research (Langner, 2010;
Angeli et al., 2010; Belz and Kow, 2009). The fol-
lowing are examples of weather forecast texts from
the data:
</bodyText>
<listItem confidence="0.54930475">
1: SSE 28-32 INCREASING 36-40 BY MID AF-
TERNOON
2: S’LY 26-32 BACKING SSE 30-35 BY AFTER-
extremely
</listItem>
<page confidence="0.929915">
231
</page>
<bodyText confidence="0.907146642857143">
NOON INCREASING 35-40 GUSTS 50 BY MID
EVENING
Image descriptions: In the other half of our eval-
uations, we used human-written and automatically
generated image descriptions for the same images.
The data in our evaluations was for 112 different
image sets and included outputs from 6 generator
systems and 2 sets of human-authored descriptions.
This data was originally created in the TUNA Project
(van Deemter et al., 2006). The following is an ex-
ample of an item from the corpus, consisting of a set
of images and a description for the entity in the red
frame:
the small blue fan
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<subsectionHeader confidence="0.979626">
4.1 Evaluation criteria
</subsectionHeader>
<bodyText confidence="0.999984416666667">
Fluency/Readability: Both the weather forecast and
image description evaluation experiments used a
quality criterion intended to capture ‘how well a
piece of text reads’, called Fluency in the latter,
Readability in the former.
Adequacy/Clarity: In the image description ex-
periments, the second quality criterion was Ade-
quacy, explained as “how clear the description is”,
and “how easy it would be to identify the image from
the description”. This criterion was called Clarity in
the weather forecast experiments, explained as “how
easy is it to understand what is being described”.
</bodyText>
<subsectionHeader confidence="0.952073">
4.2 Raters
</subsectionHeader>
<bodyText confidence="0.999992555555556">
In the image experiments we used 8 raters (native
speakers) in each experiment, from cohorts of 3rd-
year undergraduate and postgraduate students doing
a degree in a linguistics-related subject. They were
paid and spent about 1 hour doing the experiment.
In the weather forecast experiments, we used 22
raters in each experiment, from among academic
staff at our own university. They were not paid and
spent about 15 minutes doing the experiment.
</bodyText>
<subsectionHeader confidence="0.99747">
4.3 Summary overview of experiments
</subsectionHeader>
<bodyText confidence="0.997807">
Weather VDS-7 (A): VDS-7 scale; weather forecast
data; criteria: Readability and Clarity; 22 raters (uni-
versity staff) each assessing 22 forecasts.
Weather VDS-7 (B): exact repeat of Weather
VDS-7 (A), including same raters.
Weather VAS: VAS scale; 22 raters (university
staff), no overlap with raters in Weather VDS-7 ex-
periments; other details same as in Weather VDS-7.
Image VDS-7: VDS-7 scale; image description
data; 8 raters (linguistics students) each rating 112
descriptions; criteria: Fluency and Adequacy.
Image VAS (A): VAS scale; 8 raters (linguistics
students), no overlap with raters in Image VAS-7;
other details same as in Image VDS-7 experiment.
Image VAS (B): exact repeat of Image VAS (A),
including same raters.
</bodyText>
<subsectionHeader confidence="0.982475">
4.4 Design features common to all experiments
</subsectionHeader>
<bodyText confidence="0.999995454545455">
In all our experiments we used a Repeated Latin
Squares design to ensure that each rater sees the
same number of outputs from each system and for
each text type (forecast date/image set). Following
detailed instructions, raters first did a small number
of practice examples, followed by the texts to be
rated, in an order randomised for each rater. Eval-
uations were carried out via a web interface. They
were allowed to interrupt the experiment, and in the
case of the 1 hour long image description evaluation
they were encouraged to take breaks.
</bodyText>
<sectionHeader confidence="0.992871" genericHeader="method">
5 Comparison and Assessment of Scales
</sectionHeader>
<bodyText confidence="0.999963066666667">
Validity is to the extent to which an assessment
method measures what it is intended to measure
(Svensson, 2000). Validity is often impossible to as-
sess objectively, as is the case of all our criteria ex-
cept Adequacy, the validity of which we can directly
test by looking at correlations with the accuracy with
which participants in a separate experiment identify
the intended images given their descriptions.
A standard method for assessing Reliability is
Kendall’s W, a coefficient of concordance, measur-
ing the degree to which different raters agree in their
ratings. We report W for all 6 experiments.
Stability refers to the extent to which the results
of an experiment run on one occasion agree with
the results of the same experiment (with the same
</bodyText>
<page confidence="0.986524">
232
</page>
<bodyText confidence="0.999802115384615">
raters) run on a different occasion. In the present
study, we assess stability in an intra-rater, test-retest
design, assessing the agreement between the same
participant’s responses in the first and second runs
of the test with Pearson’s product-moment correla-
tion coefficient. We report these measures between
ratings given in Image VAS (A) vs. those given in Im-
age VAS (B), and between ratings given in Weather
VDS-7 (A) vs. those given in Weather VDS-7 (B).
We assess Interchangeability, that is, the extent
to which our VDS and VAS scales agree, by comput-
ing Pearson’s and Spearman’s coefficients between
results. We report these measures for all pairs of
weather forecast/image description evaluations.
We assess the Sensitivity of our scales by de-
termining the number of significant differences be-
tween different systems and human authors detected
by each scale.
We also look at the relative effect of the differ-
ent experimental factors by computing the F-Ratio
for System (the main factor under investigation, so
its relative effect should be high), Rater and Text
Type (their effect should be low). F-ratios were de-
termined by a one-way ANOVA with the evaluation
criterion in question as the dependent variable and
System, Rater or Text Type as grouping factors.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.838851">
6.1 Interchangeability and Reliability for
</subsectionHeader>
<bodyText confidence="0.8832958">
system/human authored image descriptions
Interchangeability: Pearson’s r between the means
per system/human in the three image description
evaluation experiments were as follows (Spearman’s
p shown in brackets):
</bodyText>
<table confidence="0.748811714285714">
Adeq. VAS (A) VAS (B)
VDS-7 .957**(.958**) .819* (.755*)
VAS (A) — .874** (.810*)
Flue. VDS-7 .948**(.922**) .864** (.850**)
VAS (A) — .937** (.929**)
For both Adequacy and Fluency, correlations be-
tween Image VDS-7 and Image VAS (A) (the main
VAS experiment) are extremely high, meaning that
they could substitute for each other here.
Reliability: Inter-rater agreement in terms of
Kendall’s W in each of the experiments:
VDS-7 VAS (A) VAS (B)
K’s W Adequacy .598** .471** .595*
K’s W Fluency .640** .676** .729**
</table>
<bodyText confidence="0.9972615">
W was higher in the VAS data in the case of Fluency,
whereas for Adequacy, W was the same for the VDS
data and VAS (B), and higher in the VDS data than
in the VAS (A) data.
</bodyText>
<subsectionHeader confidence="0.8591665">
6.2 Interchangeability and Reliability for
system/human authored weather forecasts
</subsectionHeader>
<bodyText confidence="0.81398275">
Interchangeability: The correlation coefficients
(Pearson’s r with Spearman’s p in brackets) between
the means per system/human in the image descrip-
tion experiments were as follows:
</bodyText>
<table confidence="0.989560857142857">
Clar. VDS-7 (B) VAS
VDS-7 (A) .995** (.989**) .942** (.832**)
VDS-7 (B) — .939**( .836**)
Read. VDS-7 (A) .981** (.870**) .947** (.709*)
VDS-7 (B) — .951** (.656*)
For both Adequacy and Fluency, correlations be-
tween Weather VDS-7 (A) (the main VDS-7 experi-
ment) and Weather VAS (A) are again very high, al-
though rank-correlation is somewhat lower.
Reliability: Inter-rater agreement in terms of
Kendall’s W was as follows:
VDS-7 (A) VDS-7 (B) VAS
W Clarity .497** .453** .485**
W Read. .533** .488** .480**
</table>
<bodyText confidence="0.994493">
This time the highest agreement for both Clarity and
Readability was in the VDS-7 data.
</bodyText>
<subsectionHeader confidence="0.99901">
6.3 Stability tests for image and weather data
</subsectionHeader>
<bodyText confidence="0.999974111111111">
Pearson’s r between ratings given by the same raters
first in Image VAS (A) and then in Image VAS (B)
was .666 for Adequacy, .593 for Fluency. Between
ratings given by the same raters first in Weather
VDS-7 (A) and then in Weather VDS-7 (B), Pearson’s
r was .656 for Clarity, .704 for Readability. (All sig-
nificant at p &lt; .01.) Note that these are computed
on individual scores (rather than means as in the cor-
relation figures given in previous sections).
</bodyText>
<subsectionHeader confidence="0.960035">
6.4 F-ratios and post-hoc analysis for image
data
</subsectionHeader>
<bodyText confidence="0.9997805">
The table below shows F-ratios determined by a one-
way ANOVA with the evaluation criterion in question
(Adequacy/Fluency) as the dependent variable and
System/Rater/Text Type as the grouping factor. Note
</bodyText>
<page confidence="0.996598">
233
</page>
<bodyText confidence="0.6255985">
that for System a high F-ratio is desirable, but a low
F-ratio is desirable for other factors.
</bodyText>
<table confidence="0.9986095">
Image descriptions
VDS-7 VAS (A)
System 8.822** 6.371**
Adequacy Rater 12.623** 13.136**
Text Type 1.193 1.519**
System 13.312** 17.207**
Fluency Rater 27.401** 17.479**
Text Type .894 1.091
</table>
<bodyText confidence="0.94634925">
Out of a possible 28 significant differences for Sys-
tem, the main factor under investigation, VDS-7
found 8 for Adequacy and 14 for Fluency; VAS (A)
found 7 for Adequacy and 15 for Fluency.
</bodyText>
<subsectionHeader confidence="0.888216">
6.5 F-ratios and post-hoc analysis for weather
data
</subsectionHeader>
<bodyText confidence="0.9962995">
The table below shows F-ratios analogous to the pre-
vious section (for Clarity/Readability).
</bodyText>
<table confidence="0.998418875">
Weather forecasts
VDS-7 (A) VAS
System 23.507** 23.468**
Clarity Rater 4.832** 6.857**
Text Type 1.467 1.632*
System 24.351** 22.538**
Read. Rater 4.824** 5.560**
Text Type 1.961** 2.906**
</table>
<bodyText confidence="0.996343">
Out of a possible 55 significant differences for Sys-
tem, VDS-7 (A) found 24 for Clarity, 23 for Read-
ability; VAS found 25 for Adequacy, 26 for Fluency.
</bodyText>
<subsectionHeader confidence="0.988217">
6.6 Scale validity test for image data
</subsectionHeader>
<bodyText confidence="0.999150875">
Our final table of results shows Pearson’s correla-
tion coefficients (calculated on means per system)
between the Adequacy data from the three image
description evaluation experiments on the one hand,
and the data from an extrinsic experiment in which
we measured the accuracy with which participants
identified the intended image described by a descrip-
tion:
</bodyText>
<table confidence="0.78347425">
ID Acc.
Image VAS (A) Adequacy .870 **
Image VAS (B) Adequacy .927 **
Image VDS-7 Adequacy .906 **
</table>
<bodyText confidence="0.999814111111111">
The correlation between Adequacy and ID Accuracy
was strong and highly significant in all three image
description evaluation experiments, but strongest in
VAS (B), and weakest in VAS (A). For comparison,
Pearson’s between Fluency and ID Accuracy ranged
between .3 and .5, whereas Pearson’s between Ade-
quacy and ID Speed (also measured in the same im-
age identfication experiment) ranged between -.35
and -.29.
</bodyText>
<sectionHeader confidence="0.993767" genericHeader="conclusions">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.99995341025641">
Our interchangeability results (Sections 6.1 and 6.2)
indicate that the VAS and VDS-7 scales we have
tested can substitute for each other in our present
evaluation tasks in terms of the mean system scores
they produce. Where we were able to measure va-
lidity (Section 6.6), both scales were shown to be
similarly valid, predicting image identification ac-
curacy figures from a separate experiment equally
well. Stability (Section 6.3) was marginally better
for VDS-7 data, and Reliability (Sections 6.1 and
6.2) was better for VAS data in the image descrip-
tion evaluations, but (mostly) better for VDS-7 data
in the weather forecast evaluations. Finally, the VAS
experiments found greater numbers of statistically
significant differences between systems in 3 out of 4
cases (Section 6.5).
Our own raters strongly prefer working with VAS
scales over VDSs. This has also long been clear from
the psychology literature (Svensson, 2000)), where
raters are typically found to prefer VAS scales over
VDSs which can be a “constant source of vexation
to the conscientious rater when he finds his judg-
ments falling between the defined points” (Champ-
ney, 1941). Moreover, if a rater’s judgment falls be-
tween two points on a VDS then they must make the
false choice between the two points just above and
just below their actual judgment. In this case we
know that the point they end up selecting is not an
accurate measure of their judgment but rather just
one of two equally accurate ones (one of which goes
unrecorded).
Our results establish (for our evaluation tasks) that
VAS scales, so far unproven for use in NLP, are at
least as good as VDSs, currently virtually the only
scale in use in NLP. Combined with the fact that
raters strongly prefer VASs and that they are regarded
as more amenable to parametric means of statisti-
cal analysis, this indicates that VAS scales should be
used more widely for NLP evaluation tasks.
</bodyText>
<page confidence="0.996575">
234
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987116">
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 15th Conference on
Empirical Methods in Natural Language Processing
(EMNLP’10).
Anja Belz and Eric Kow. 2009. System building cost
vs. output quality in data-to-text generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 16–24.
H. Champney. 1941. The measurement of parent behav-
ior. Child Development, 12(2):131.
M. Freyd. 1923. The graphic rating scale. Biometrical
Journal, 42:83–102.
A. Gatt, A. Belz, and E. Kow. 2009. The TUNA Chal-
lenge 2009: Overview and evaluation results. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG’09), pages 198–206.
Brian Langner. 2010. Data-driven Natural Language
Generation: Making Machines Talk Like Humans
Using Natural Corpora. Ph.D. thesis, Language
Technologies Institute, School of Computer Science,
Carnegie Mellon University.
Robert W. Lansing, Shakeeb H. Moosavi, and Robert B.
Banzett. 2003. Measurement of dyspnea: word la-
beled visual analog scale vs. verbal ordinal scale. Res-
piratory Physiology &amp; Neurobiology, 134(2):77 –83.
J. Scott and E. C. Huskisson. 2003. Vertical or hori-
zontal visual analogue scales. Annals of the rheumatic
diseases, (38):560.
Sidney Siegel. 1957. Non-parametric statistics. The
American Statistician, 11(3):13–19.
Elisabeth Svensson. 2000. Comparison of the quality
of assessments using continuous and discrete ordinal
rating scales. Biometrical Journal, 42(4):417–434.
P. M. ten Klooster, A. P. Klaar, E. Taal, R. E. Gheith,
J. J. Rasker, A. K. El-Garf, and M. A. van de Laar.
2006. The validity and reliability of the graphic rating
scale and verbal rating scale for measuing pain across
cultures: A study in egyptian and dutch women with
rheumatoid arthritis. The Clinical Journal of Pain,
22(9):827–30.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Conference on Natural
Language Generation, pages 130–132, Sydney, Aus-
tralia, July.
S. Williams and E. Reiter. 2008. Generating basic skills
reports for low-skilled readers. Natural Language En-
gineering, 14(4):495–525.
</reference>
<page confidence="0.998531">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.522313">
<title confidence="0.994716">Discrete vs. Continuous Rating Scales for Language Evaluation in NLP</title>
<author confidence="0.999621">Anja Belz Eric Kow</author>
<affiliation confidence="0.986843">School of Computing, Engineering and University of</affiliation>
<address confidence="0.527726">Brighton BN2 4GJ,</address>
<abstract confidence="0.999636538461539">Studies assessing rating scales are very common in psychology and related fields, but rare in In this paper we assess discrete and continuous scales used for measuring quality assessments of computergenerated language. We conducted six separate experiments designed to investigate the validity, reliability, stability, interchangeability and sensitivity of discrete vs. continuous scales. We show that continuous scales are viable for use in language evaluation, and offer distinct advantages over discrete scales.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 15th Conference on Empirical Methods in Natural Language Processing (EMNLP’10).</booktitle>
<contexts>
<context position="7674" citStr="Angeli et al., 2010" startWordPosition="1223" endWordPosition="1226">face and the label “statement couldn’t be more unclear/read worse”. The raters’ task was to move a pointer (initially in the middle of the line) to the place corresponding to their rating. 3 Data Weather forecast texts: In one half of our evaluation experiments we used human-written and automatically generated weather forecasts for the same weather data. The data in our evaluations was for 22 different forecast dates and included outputs from 10 generator systems and one set of human forecasts. This data has also been used for comparative system evaluation in previous research (Langner, 2010; Angeli et al., 2010; Belz and Kow, 2009). The following are examples of weather forecast texts from the data: 1: SSE 28-32 INCREASING 36-40 BY MID AFTERNOON 2: S’LY 26-32 BACKING SSE 30-35 BY AFTERextremely 231 NOON INCREASING 35-40 GUSTS 50 BY MID EVENING Image descriptions: In the other half of our evaluations, we used human-written and automatically generated image descriptions for the same images. The data in our evaluations was for 112 different image sets and included outputs from 6 generator systems and 2 sets of human-authored descriptions. This data was originally created in the TUNA Project (van Deemte</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 15th Conference on Empirical Methods in Natural Language Processing (EMNLP’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>System building cost vs. output quality in data-to-text generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation,</booktitle>
<pages>16--24</pages>
<contexts>
<context position="7695" citStr="Belz and Kow, 2009" startWordPosition="1227" endWordPosition="1230">tatement couldn’t be more unclear/read worse”. The raters’ task was to move a pointer (initially in the middle of the line) to the place corresponding to their rating. 3 Data Weather forecast texts: In one half of our evaluation experiments we used human-written and automatically generated weather forecasts for the same weather data. The data in our evaluations was for 22 different forecast dates and included outputs from 10 generator systems and one set of human forecasts. This data has also been used for comparative system evaluation in previous research (Langner, 2010; Angeli et al., 2010; Belz and Kow, 2009). The following are examples of weather forecast texts from the data: 1: SSE 28-32 INCREASING 36-40 BY MID AFTERNOON 2: S’LY 26-32 BACKING SSE 30-35 BY AFTERextremely 231 NOON INCREASING 35-40 GUSTS 50 BY MID EVENING Image descriptions: In the other half of our evaluations, we used human-written and automatically generated image descriptions for the same images. The data in our evaluations was for 112 different image sets and included outputs from 6 generator systems and 2 sets of human-authored descriptions. This data was originally created in the TUNA Project (van Deemter et al., 2006). The </context>
</contexts>
<marker>Belz, Kow, 2009</marker>
<rawString>Anja Belz and Eric Kow. 2009. System building cost vs. output quality in data-to-text generation. In Proceedings of the 12th European Workshop on Natural Language Generation, pages 16–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Champney</author>
</authors>
<title>The measurement of parent behavior.</title>
<date>1941</date>
<journal>Child Development,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="18578" citStr="Champney, 1941" startWordPosition="2986" endWordPosition="2988"> in the image description evaluations, but (mostly) better for VDS-7 data in the weather forecast evaluations. Finally, the VAS experiments found greater numbers of statistically significant differences between systems in 3 out of 4 cases (Section 6.5). Our own raters strongly prefer working with VAS scales over VDSs. This has also long been clear from the psychology literature (Svensson, 2000)), where raters are typically found to prefer VAS scales over VDSs which can be a “constant source of vexation to the conscientious rater when he finds his judgments falling between the defined points” (Champney, 1941). Moreover, if a rater’s judgment falls between two points on a VDS then they must make the false choice between the two points just above and just below their actual judgment. In this case we know that the point they end up selecting is not an accurate measure of their judgment but rather just one of two equally accurate ones (one of which goes unrecorded). Our results establish (for our evaluation tasks) that VAS scales, so far unproven for use in NLP, are at least as good as VDSs, currently virtually the only scale in use in NLP. Combined with the fact that raters strongly prefer VASs and t</context>
</contexts>
<marker>Champney, 1941</marker>
<rawString>H. Champney. 1941. The measurement of parent behavior. Child Development, 12(2):131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Freyd</author>
</authors>
<title>The graphic rating scale.</title>
<date>1923</date>
<journal>Biometrical Journal,</journal>
<pages>42--83</pages>
<contexts>
<context position="925" citStr="Freyd, 1923" startWordPosition="136" endWordPosition="137"> rare in NLP. In this paper we assess discrete and continuous scales used for measuring quality assessments of computergenerated language. We conducted six separate experiments designed to investigate the validity, reliability, stability, interchangeability and sensitivity of discrete vs. continuous scales. We show that continuous scales are viable for use in language evaluation, and offer distinct advantages over discrete scales. 1 Background and Introduction Rating scales have been used for measuring human perception of various stimuli for a long time, at least since the early 20th century (Freyd, 1923). First used in psychology and psychophysics, they are now also common in a variety of other disciplines, including NLP. Discrete scales are the only type of scale commonly used for qualitative assessments of computer-generated language in NLP (e.g. in the DUC/TAC evaluation competitions). Continuous scales are commonly used in psychology and related fields, but are virtually unknown in NLP. While studies assessing the quality of individual scales and comparing different types of rating scales are common in psychology and related fields, such studies hardly exist in NLP, and so at present litt</context>
</contexts>
<marker>Freyd, 1923</marker>
<rawString>M. Freyd. 1923. The graphic rating scale. Biometrical Journal, 42:83–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>A Belz</author>
<author>E Kow</author>
</authors>
<title>The TUNA Challenge 2009: Overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG’09),</booktitle>
<pages>198--206</pages>
<contexts>
<context position="5476" citStr="Gatt et al., 2009" startWordPosition="857" endWordPosition="860">y horizontal line (although vertical lines have also been used (Scott and Huskisson, 2003)), on which the two end points represent the extreme values of the variable to be measured. Such lines can be mono-polar or bi-polar, and the end points are labelled with an image (smiling/frowning face), or a brief verbal descriptor, to indicate which end of the line corresponds to which extreme of the variable. The labels are commonly chosen to represent a point beyond any response actually likely to be chosen by raters. There is only one examples of a VAS in NLP system evaluation that we are aware of (Gatt et al., 2009). Hybrid scales, known as a graphic rating scales, combine the features of VDSs and VASs, and are also used in psychology. Here, the verbal descriptors are aligned along the line of a VAS and the endpoints are typically unmarked (Svensson, 2000). We are aware of one example in NLP (Williams and Reiter, 2008); Q1: Grammaticality The summary should have no datelines, system-internal formatting, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read. excellent bad Figure 2: Evaluation of Grammaticality with alternative</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>A. Gatt, A. Belz, and E. Kow. 2009. The TUNA Challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG’09), pages 198–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Langner</author>
</authors>
<title>Data-driven Natural Language Generation: Making Machines Talk Like Humans Using Natural Corpora.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Language Technologies Institute, School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="7653" citStr="Langner, 2010" startWordPosition="1221" endWordPosition="1222">had a frowning face and the label “statement couldn’t be more unclear/read worse”. The raters’ task was to move a pointer (initially in the middle of the line) to the place corresponding to their rating. 3 Data Weather forecast texts: In one half of our evaluation experiments we used human-written and automatically generated weather forecasts for the same weather data. The data in our evaluations was for 22 different forecast dates and included outputs from 10 generator systems and one set of human forecasts. This data has also been used for comparative system evaluation in previous research (Langner, 2010; Angeli et al., 2010; Belz and Kow, 2009). The following are examples of weather forecast texts from the data: 1: SSE 28-32 INCREASING 36-40 BY MID AFTERNOON 2: S’LY 26-32 BACKING SSE 30-35 BY AFTERextremely 231 NOON INCREASING 35-40 GUSTS 50 BY MID EVENING Image descriptions: In the other half of our evaluations, we used human-written and automatically generated image descriptions for the same images. The data in our evaluations was for 112 different image sets and included outputs from 6 generator systems and 2 sets of human-authored descriptions. This data was originally created in the TUN</context>
</contexts>
<marker>Langner, 2010</marker>
<rawString>Brian Langner. 2010. Data-driven Natural Language Generation: Making Machines Talk Like Humans Using Natural Corpora. Ph.D. thesis, Language Technologies Institute, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert W Lansing</author>
<author>Shakeeb H Moosavi</author>
<author>Robert B Banzett</author>
</authors>
<title>Measurement of dyspnea: word labeled visual analog scale vs. verbal ordinal scale.</title>
<date>2003</date>
<journal>Respiratory Physiology &amp; Neurobiology,</journal>
<volume>134</volume>
<issue>2</issue>
<pages>83</pages>
<contexts>
<context position="2168" citStr="Lansing et al. (2003)" startWordPosition="328" endWordPosition="331">her discrete scales are a suitable rating tool for NLP evaluation tasks, or whether continuous scales might provide a better alternative. A range of studies from sociology, psychophysiology, biometrics and other fields have compared discrete and continuous scales. Results tend to differ for different types of data. E.g., results from pain measurement show a continuous scale to outperform a discrete scale (ten Klooster et al., 2006). Other results (Svensson, 2000) from measuring students’ ease of following lectures show a discrete scale to outperform a continuous scale. When measuring dyspnea, Lansing et al. (2003) found a hybrid scale to perform on a par with a discrete scale. Another consideration is the types of data produced by discrete and continuous scales. Parametric methods of statistical analysis, which are far more sensitive than non-parametric ones, are commonly applied to both discrete and continuous data. However, parametric methods make very strong assumptions about data, including that it is numerical and normally distributed (Siegel, 1957). If these assumptions are violated, then the significance of results is overestimated. Clearly, the numerical assumption does not hold for the categor</context>
</contexts>
<marker>Lansing, Moosavi, Banzett, 2003</marker>
<rawString>Robert W. Lansing, Shakeeb H. Moosavi, and Robert B. Banzett. 2003. Measurement of dyspnea: word labeled visual analog scale vs. verbal ordinal scale. Respiratory Physiology &amp; Neurobiology, 134(2):77 –83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Scott</author>
<author>E C Huskisson</author>
</authors>
<title>Vertical or horizontal visual analogue scales. Annals of the rheumatic diseases,</title>
<date>2003</date>
<pages>38--560</pages>
<contexts>
<context position="4948" citStr="Scott and Huskisson, 2003" startWordPosition="762" endWordPosition="765">ants give responses on ordered lists of verbally described and/or numerically labelled response categories, typically varying in number from 2 to 11 (Svensson, 2000). An example of a VDS used in NLP is shown in Figure 1. VDSs are used very widely in contexts where computationally generated language is evaluated, including in dialogue, summarisation, MT and data-to-text generation. Visual analogue scales (VASs) are far less common outside psychology and related areas than VDSs. Responses are given by selecting a point on a typically horizontal line (although vertical lines have also been used (Scott and Huskisson, 2003)), on which the two end points represent the extreme values of the variable to be measured. Such lines can be mono-polar or bi-polar, and the end points are labelled with an image (smiling/frowning face), or a brief verbal descriptor, to indicate which end of the line corresponds to which extreme of the variable. The labels are commonly chosen to represent a point beyond any response actually likely to be chosen by raters. There is only one examples of a VAS in NLP system evaluation that we are aware of (Gatt et al., 2009). Hybrid scales, known as a graphic rating scales, combine the features </context>
</contexts>
<marker>Scott, Huskisson, 2003</marker>
<rawString>J. Scott and E. C. Huskisson. 2003. Vertical or horizontal visual analogue scales. Annals of the rheumatic diseases, (38):560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
</authors>
<title>Non-parametric statistics.</title>
<date>1957</date>
<journal>The American Statistician,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="2617" citStr="Siegel, 1957" startWordPosition="400" endWordPosition="401">son, 2000) from measuring students’ ease of following lectures show a discrete scale to outperform a continuous scale. When measuring dyspnea, Lansing et al. (2003) found a hybrid scale to perform on a par with a discrete scale. Another consideration is the types of data produced by discrete and continuous scales. Parametric methods of statistical analysis, which are far more sensitive than non-parametric ones, are commonly applied to both discrete and continuous data. However, parametric methods make very strong assumptions about data, including that it is numerical and normally distributed (Siegel, 1957). If these assumptions are violated, then the significance of results is overestimated. Clearly, the numerical assumption does not hold for the categorial data produced by discrete scales, and it is unlikely to be normally distributed. Many researchers are happier to apply parametric methods to data from continuous scales, and some simply take it as read that such data is normally distributed (Lansing et al., 2003). Our aim in the present study was to systematically assess and compare discrete and continuous scales when used for the qualitative assessment of computer-generated language. We sta</context>
</contexts>
<marker>Siegel, 1957</marker>
<rawString>Sidney Siegel. 1957. Non-parametric statistics. The American Statistician, 11(3):13–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Svensson</author>
</authors>
<title>Comparison of the quality of assessments using continuous and discrete ordinal rating scales.</title>
<date>2000</date>
<journal>Biometrical Journal,</journal>
<volume>42</volume>
<issue>4</issue>
<contexts>
<context position="2014" citStr="Svensson, 2000" startWordPosition="307" endWordPosition="308">t types of rating scales are common in psychology and related fields, such studies hardly exist in NLP, and so at present little is known about whether discrete scales are a suitable rating tool for NLP evaluation tasks, or whether continuous scales might provide a better alternative. A range of studies from sociology, psychophysiology, biometrics and other fields have compared discrete and continuous scales. Results tend to differ for different types of data. E.g., results from pain measurement show a continuous scale to outperform a discrete scale (ten Klooster et al., 2006). Other results (Svensson, 2000) from measuring students’ ease of following lectures show a discrete scale to outperform a continuous scale. When measuring dyspnea, Lansing et al. (2003) found a hybrid scale to perform on a par with a discrete scale. Another consideration is the types of data produced by discrete and continuous scales. Parametric methods of statistical analysis, which are far more sensitive than non-parametric ones, are commonly applied to both discrete and continuous data. However, parametric methods make very strong assumptions about data, including that it is numerical and normally distributed (Siegel, 19</context>
<context position="4487" citStr="Svensson, 2000" startWordPosition="691" endWordPosition="692">e text difficult to read. 1. Very Poor 2. Poor 3. Barely Acceptable 4. Good 5. Very Good Figure 1: Evaluation of Readability in DUC’06, comprising 5 evaluation criteria, including Grammaticality. Evaluation task for each summary text: evaluator selects one of the options (1–5) to represent quality of the summary in terms of the criterion. (Section 6), and some conclusions (Section 7). 2 Rating Scales With Verbal Descriptor Scales (VDSs), participants give responses on ordered lists of verbally described and/or numerically labelled response categories, typically varying in number from 2 to 11 (Svensson, 2000). An example of a VDS used in NLP is shown in Figure 1. VDSs are used very widely in contexts where computationally generated language is evaluated, including in dialogue, summarisation, MT and data-to-text generation. Visual analogue scales (VASs) are far less common outside psychology and related areas than VDSs. Responses are given by selecting a point on a typically horizontal line (although vertical lines have also been used (Scott and Huskisson, 2003)), on which the two end points represent the extreme values of the variable to be measured. Such lines can be mono-polar or bi-polar, and t</context>
<context position="5721" citStr="Svensson, 2000" startWordPosition="900" endWordPosition="901">abelled with an image (smiling/frowning face), or a brief verbal descriptor, to indicate which end of the line corresponds to which extreme of the variable. The labels are commonly chosen to represent a point beyond any response actually likely to be chosen by raters. There is only one examples of a VAS in NLP system evaluation that we are aware of (Gatt et al., 2009). Hybrid scales, known as a graphic rating scales, combine the features of VDSs and VASs, and are also used in psychology. Here, the verbal descriptors are aligned along the line of a VAS and the endpoints are typically unmarked (Svensson, 2000). We are aware of one example in NLP (Williams and Reiter, 2008); Q1: Grammaticality The summary should have no datelines, system-internal formatting, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read. excellent bad Figure 2: Evaluation of Grammaticality with alternative VAS scale (cf. Figure 1). Evaluation task for each summary text: evaluator selects a place on the line to represent quality of the summary in terms of the criterion. we did not investigate this scale in our study. We used the following two spec</context>
<context position="11043" citStr="Svensson, 2000" startWordPosition="1771" endWordPosition="1772"> rater sees the same number of outputs from each system and for each text type (forecast date/image set). Following detailed instructions, raters first did a small number of practice examples, followed by the texts to be rated, in an order randomised for each rater. Evaluations were carried out via a web interface. They were allowed to interrupt the experiment, and in the case of the 1 hour long image description evaluation they were encouraged to take breaks. 5 Comparison and Assessment of Scales Validity is to the extent to which an assessment method measures what it is intended to measure (Svensson, 2000). Validity is often impossible to assess objectively, as is the case of all our criteria except Adequacy, the validity of which we can directly test by looking at correlations with the accuracy with which participants in a separate experiment identify the intended images given their descriptions. A standard method for assessing Reliability is Kendall’s W, a coefficient of concordance, measuring the degree to which different raters agree in their ratings. We report W for all 6 experiments. Stability refers to the extent to which the results of an experiment run on one occasion agree with the re</context>
<context position="18360" citStr="Svensson, 2000" startWordPosition="2950" endWordPosition="2951">id, predicting image identification accuracy figures from a separate experiment equally well. Stability (Section 6.3) was marginally better for VDS-7 data, and Reliability (Sections 6.1 and 6.2) was better for VAS data in the image description evaluations, but (mostly) better for VDS-7 data in the weather forecast evaluations. Finally, the VAS experiments found greater numbers of statistically significant differences between systems in 3 out of 4 cases (Section 6.5). Our own raters strongly prefer working with VAS scales over VDSs. This has also long been clear from the psychology literature (Svensson, 2000)), where raters are typically found to prefer VAS scales over VDSs which can be a “constant source of vexation to the conscientious rater when he finds his judgments falling between the defined points” (Champney, 1941). Moreover, if a rater’s judgment falls between two points on a VDS then they must make the false choice between the two points just above and just below their actual judgment. In this case we know that the point they end up selecting is not an accurate measure of their judgment but rather just one of two equally accurate ones (one of which goes unrecorded). Our results establish</context>
</contexts>
<marker>Svensson, 2000</marker>
<rawString>Elisabeth Svensson. 2000. Comparison of the quality of assessments using continuous and discrete ordinal rating scales. Biometrical Journal, 42(4):417–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M ten Klooster</author>
<author>A P Klaar</author>
<author>E Taal</author>
<author>R E Gheith</author>
<author>J J Rasker</author>
<author>A K El-Garf</author>
<author>M A van de Laar</author>
</authors>
<title>The validity and reliability of the graphic rating scale and verbal rating scale for measuing pain across cultures: A study in egyptian and dutch women with rheumatoid arthritis.</title>
<date>2006</date>
<journal>The Clinical Journal of Pain,</journal>
<volume>22</volume>
<issue>9</issue>
<marker>Klooster, Klaar, Taal, Gheith, Rasker, El-Garf, van de Laar, 2006</marker>
<rawString>P. M. ten Klooster, A. P. Klaar, E. Taal, R. E. Gheith, J. J. Rasker, A. K. El-Garf, and M. A. van de Laar. 2006. The validity and reliability of the graphic rating scale and verbal rating scale for measuing pain across cultures: A study in egyptian and dutch women with rheumatoid arthritis. The Clinical Journal of Pain, 22(9):827–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Ielka van der Sluis</author>
<author>Albert Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Conference on Natural Language Generation,</booktitle>
<pages>130--132</pages>
<location>Sydney, Australia,</location>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>Kees van Deemter, Ielka van der Sluis, and Albert Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In Proceedings of the 4th International Conference on Natural Language Generation, pages 130–132, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Generating basic skills reports for low-skilled readers.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="5785" citStr="Williams and Reiter, 2008" startWordPosition="910" endWordPosition="913">rief verbal descriptor, to indicate which end of the line corresponds to which extreme of the variable. The labels are commonly chosen to represent a point beyond any response actually likely to be chosen by raters. There is only one examples of a VAS in NLP system evaluation that we are aware of (Gatt et al., 2009). Hybrid scales, known as a graphic rating scales, combine the features of VDSs and VASs, and are also used in psychology. Here, the verbal descriptors are aligned along the line of a VAS and the endpoints are typically unmarked (Svensson, 2000). We are aware of one example in NLP (Williams and Reiter, 2008); Q1: Grammaticality The summary should have no datelines, system-internal formatting, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read. excellent bad Figure 2: Evaluation of Grammaticality with alternative VAS scale (cf. Figure 1). Evaluation task for each summary text: evaluator selects a place on the line to represent quality of the summary in terms of the criterion. we did not investigate this scale in our study. We used the following two specific scale designs in our experiments: VDS-7: 7 response categor</context>
</contexts>
<marker>Williams, Reiter, 2008</marker>
<rawString>S. Williams and E. Reiter. 2008. Generating basic skills reports for low-skilled readers. Natural Language Engineering, 14(4):495–525.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>