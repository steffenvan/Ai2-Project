<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.997007">
Distributional Representations for Handling Sparsity in Supervised
Sequence-Labeling
</title>
<author confidence="0.996768">
Fei Huang
</author>
<affiliation confidence="0.989796">
Temple University
</affiliation>
<address confidence="0.43468">
1805 N. Broad St.
Wachman Hall 324
</address>
<email confidence="0.998146">
tub58431@temple.edu
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977276">
Supervised sequence-labeling systems in
natural language processing often suffer
from data sparsity because they use word
types as features in their prediction tasks.
Consequently, they have difficulty estimat-
ing parameters for types which appear in
the test set, but seldom (or never) ap-
pear in the training set. We demonstrate
that distributional representations of word
types, trained on unannotated text, can
be used to improve performance on rare
words. We incorporate aspects of these
representations into the feature space of
our sequence-labeling systems. In an ex-
periment on a standard chunking dataset,
our best technique improves a chunker
from 0.76 F1 to 0.86 F1 on chunks begin-
ning with rare words. On the same dataset,
it improves our part-of-speech tagger from
74% to 80% accuracy on rare words. Fur-
thermore, our system improves signifi-
cantly over a baseline system when ap-
plied to text from a different domain, and
it reduces the sample complexity of se-
quence labeling.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999805769230769">
Data sparsity and high dimensionality are the twin
curses of statistical natural language processing
(NLP). In many traditional supervised NLP sys-
tems, the feature space includes dimensions for
each word type in the data, or perhaps even combi-
nations of word types. Since vocabularies can be
extremely large, this leads to an explosion in the
number of parameters. To make matters worse,
language is Zipf-distributed, so that a large frac-
tion of any training data set will be hapax legom-
ena, very many word types will appear only a few
times, and many word types will be left out of
the training set altogether. As a consequence, for
</bodyText>
<note confidence="0.8690375">
Alexander Yates
Temple University
1805 N. Broad St.
Wachman Hall 324
</note>
<email confidence="0.984726">
yates@temple.edu
</email>
<bodyText confidence="0.999882864864865">
many word types supervised NLP systems have
very few, or even zero, labeled examples from
which to estimate parameters.
The negative effects of data sparsity have been
well-documented in the NLP literature. The per-
formance of state-of-the-art, supervised NLP sys-
tems like part-of-speech (POS) taggers degrades
significantly on words that do not appear in the
training data, or out-of-vocabulary (OOV) words
(Lafferty et al., 2001). Performance also degrades
when the domain of the test set differs from the do-
main of the training set, in part because the test set
includes more OOV words and words that appear
only a few times in the training set (henceforth,
rare words) (Blitzer et al., 2006; Daum´e III and
Marcu, 2006; Chelba and Acero, 2004).
We investigate the use of distributional repre-
sentations, which model the probability distribu-
tion of a word’s context, as techniques for find-
ing smoothed representations of word sequences.
That is, we use the distributional representations
to share information across unannotated examples
of the same word type. We then compute features
of the distributional representations, and provide
them as input to our supervised sequence label-
ers. Our technique is particularly well-suited to
handling data sparsity because it is possible to im-
prove performance on rare words by supplement-
ing the training data with additional unannotated
text containing more examples of the rare words.
We provide empirical evidence that shows how
distributional representations improve sequence-
labeling in the face of data sparsity.
Specifically, we investigate empirically the
effects of our smoothing techniques on two
sequence-labeling tasks, POS tagging and chunk-
ing, to answer the following:
</bodyText>
<footnote confidence="0.7418678">
1. What is the effect of smoothing on sequence-
labeling accuracy for rare word types? Our best
smoothing technique improves a POS tagger by
11% on OOV words, and a chunker by an impres-
sive 21% on OOV words.
</footnote>
<page confidence="0.964442">
495
</page>
<note confidence="0.999564">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495–503,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<listItem confidence="0.86355">
2. Can smoothing improve adaptability to new do-
mains? After training our chunker on newswire
</listItem>
<bodyText confidence="0.97454055">
text, we apply it to biomedical texts. Remark-
ably, we find that the smoothed chunker achieves
a higher F1 on the new domain than the baseline
chunker achieves on a test set from the original
newswire domain.
3. How does our smoothing technique affect sam-
ple complexity? We show that smoothing drasti-
cally reduces sample complexity: our smoothed
chunker requires under 100 labeled samples to
reach 85% accuracy, whereas the unsmoothed
chunker requires 3500 samples to reach the same
level of performance.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the smoothing problem
for word sequences, and introduces three smooth-
ing techniques. Section 3 presents our empirical
study of the effects of smoothing on two sequence-
labeling tasks. Section 4 describes related work,
and Section 5 concludes and suggests items for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.6970735" genericHeader="method">
2 Smoothing Natural Language
Sequences
</sectionHeader>
<bodyText confidence="0.999980854545454">
To smooth a dataset is to find an approximation of
it that retains the important patterns of the origi-
nal data while hiding the noise or other compli-
cating factors. Formally, we define the smoothing
task as follows: let D = {(x, z)|x is a word se-
quence, z is a label sequence} be a labeled dataset
of word sequences, and let M be a machine learn-
ing algorithm that will learn a function f to pre-
dict the correct labels. The smoothing task is to
find a function g such that when M is applied to
D&apos; = {(g(x), z)|(x, z) E D}, it produces a func-
tion f� that is more accurate than f.
For supervised sequence-labeling problems in
NLP, the most important “complicating factor”
that we seek to avoid through smoothing is the
data sparsity associated with word-based represen-
tations. Thus, the task is to find g such that for
every word x, g(x) is much less sparse, but still
retains the essential features of x that are useful
for predicting its label.
As an example, consider the string “Researchers
test reformulated gasolines on newer engines.” In
a common dataset for NP chunking, the word “re-
formulated” never appears in the training data, but
appears four times in the test set as part of the
NP “reformulated gasolines.” Thus, a learning al-
gorithm supplied with word-level features would
have a difficult time determining that “reformu-
lated” is the start of a NP. Character-level features
are of little help as well, since the “-ed” suffix is
more commonly associated with verb phrases. Fi-
nally, context may be of some help, but “test” is
ambiguous between a noun and verb, and “gaso-
lines” is only seen once in the training data, so
there is no guarantee that context is sufficient to
make a correct judgment.
On the other hand, some of the other contexts
in which “reformulated” appears in the test set,
such as “testing of reformulated gasolines,” pro-
vide strong evidence that it can start a NP, since
“of” is a highly reliable indicator that a NP is to
follow. This example provides the intuition for our
approach to smoothing: we seek to share informa-
tion about the contexts of a word across multiple
instances of the word, in order to provide more in-
formation about words that are rarely or never seen
in training. In particular, we seek to represent each
word by a distribution over its contexts, and then
provide the learning algorithm with features com-
puted from this distribution. Importantly, we seek
distributional representations that will provide fea-
tures that are common in both training and test
data, to avoid data sparsity. In the next three sec-
tions, we develop three techniques for smoothing
text using distributional representations.
</bodyText>
<subsectionHeader confidence="0.98948">
2.1 Multinomial Representation
</subsectionHeader>
<bodyText confidence="0.998269521739131">
In its simplest form, the context of a word may be
represented as a multinomial distribution over the
terms that appear on either side of the word. If V is
the vocabulary, or the set of word types, and X is a
sequence of random variables over V, the left and
right context of XZ = v may each be represented
as aprobability distribution over V: P(XZ_1|XZ =
v) and P(XZ+1|X = v) respectively.
We learn these distributions from unlabeled
texts in two different ways. The first method com-
putes word count vectors for the left and right con-
texts of each word type in the vocabulary of the
training and test texts. We also use a large col-
lection of additional text to determine the vectors.
We then normalize each vector to form a proba-
bility distribution. The second technique first ap-
plies TF-IDF weighting to each vector, where the
context words of each word type constitute a doc-
ument, before applying normalization. This gives
greater weight to words with more idiosyncratic
distributions and may improve the informativeness
of a distributional representation. We refer to these
techniques as TF and TF-IDF.
</bodyText>
<page confidence="0.998632">
496
</page>
<bodyText confidence="0.999957">
To supply a sequence-labeling algorithm with
information from these distributional representa-
tions, we compute real-valued features of the con-
text distributions. In particular, for every word
xi in a sequence, we provide the sequence labeler
with a set of features of the left and right contexts
</bodyText>
<equation confidence="0.50463575">
indexed by v E V: F left
� (xi) = P(Xi−1 = v|xi)
and Fright
� (xi) = P(Xi+1 = v|xi). For exam-
</equation>
<bodyText confidence="0.999922222222222">
ple, the left context for “reformulated” in our ex-
ample above would contain a nonzero probability
for the word “of.” Using the features F(xi), a se-
quence labeler can learn patterns such as, if xi has
a high probability of following “of,” it is a good
candidate for the start of a noun phrase. These
features provide smoothing by aggregating infor-
mation across multiple unannotated examples of
the same word.
</bodyText>
<subsectionHeader confidence="0.941242">
2.2 LSA Model
</subsectionHeader>
<bodyText confidence="0.999807368421053">
One drawback of the multinomial representation
is that it does not handle sparsity well enough,
because the multinomial distributions themselves
are so high-dimensional. For example, the two
phrases “red lamp” and “magenta tablecloth”
share no words in common. If “magenta” is never
observed in training, the fact that “tablecloth” ap-
pears in its right context is of no help in connecting
it with the phrase “red lamp.” But if we can group
similar context words together, putting “lamp” and
“tablecloth” into a category for household items,
say, then these two adjectives will share that cat-
egory in their context distributions. Any pat-
terns learned for the more common “red lamp”
will then also apply to the less common “magenta
tablecloth.” Our second distributional represen-
tation aggregates information from multiple con-
text words by grouping together the distributions
P(xi−1 = v|xi = w) and P(xi−1 = v0|xi = w)
if v and v0 appear together with many of the same
words w. Aggregating counts in this way smooths
our representations even further, by supplying bet-
ter estimates when the data is too sparse to esti-
mate P(xi−1|xi) accurately.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is a widely-used technique for comput-
ing dimensionality-reduced representations from a
bag-of-words model. We apply LSA to the set of
right context vectors and the set of left context vec-
tors separately, to find compact versions of each
vector, where each dimension represents a com-
bination of several context word types. We nor-
malize each vector, and then calculate features as
above. After experimenting with different choices
for the number of dimensions to reduce our vec-
tors to, we choose a value of 10 dimensions as the
one that maximizes the performance of our super-
vised sequence labelers on held-out data.
</bodyText>
<subsectionHeader confidence="0.9319825">
2.3 Latent Variable Language Model
Representation
</subsectionHeader>
<bodyText confidence="0.9925412">
To take smoothing one step further, we present
a technique that aggregates context distributions
both for similar context words xi−1 = v and v0,
and for similar words xi = w and w0. Latent
variable language models (LVLMs) can be used to
produce just such a distributional representation.
We use Hidden Markov Models (HMMs) as the
main example in the discussion and as the LVLMs
in our experiments, but the smoothing technique
can be generalized to other forms of LVLMs, such
as factorial HMMs and latent variable maximum
entropy models (Ghahramani and Jordan, 1997;
Smith and Eisner, 2005).
An HMM is a generative probabilistic model
that generates each word xi in the corpus con-
ditioned on a latent variable Yi. Each Yi in the
model takes on integral values from 1 to S, and
each one is generated by the latent variable for the
preceding word, Yi−1. The distribution for a cor-
pus x = (x1, ... , xN) given a set of state vectors
</bodyText>
<equation confidence="0.917870666666667">
Y = (y1, ... , yN) is given by:
P (x|Y) = � P(xi|yi)P(yi|yi−1)
i
</equation>
<bodyText confidence="0.9999026">
Using Expectation-Maximization (Dempster et
al., 1977), it is possible to estimate the distribu-
tions for P(xi|yi) and P(yi|yi−1) from unlabeled
data. We use a trained HMM to determine the op-
timal sequence of latent states yi using the well-
known Viterbi algorithm (Rabiner, 1989). The
output of this process is an integer (ranging from 1
to S) for every word xi in the corpus; we include a
new boolean feature for each possible value of yi
in our sequence labelers.
To compare our models, note that in the multi-
nomial representation we directly model the prob-
ability that a word v appears before a word w:
P(xi−1 = v|xi = w)). In our LSA model, we find
latent categories of context words z, and model the
probability that a category appears before the cur-
rent word w: P(xi−1 = z|xi = w). The HMM
finds (probabilistic) categories Y for both the cur-
rent word xi and the context word xi−1, and mod-
els the probability that one category follows the
</bodyText>
<page confidence="0.992091">
497
</page>
<bodyText confidence="0.999300428571429">
other: P(YijYi−1). Thus the HMM is our most
extreme smoothing model, as it aggregates infor-
mation over the greatest number of examples: for
a given consecutive pair of words xi−1, xi in the
test set, it aggregates over all pairs of consecutive
words x0i−1, x0 i where x0i−1 is similar to xi−1 and
x0i is similar to xi.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.995886">
We tested the following hypotheses in our experi-
ments:
</bodyText>
<listItem confidence="0.998502428571428">
1. Smoothing can improve the performance of
a supervised sequence labeling system on words
that are rare or nonexistent in the training data.
2. A supervised sequence labeler achieves greater
accuracy on new domains with smoothing.
3. A supervised sequence labeler has a better sam-
ple complexity with smoothing.
</listItem>
<subsectionHeader confidence="0.993892">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999926032258064">
We investigate the use of smoothing in two test
systems, conditional random field (CRF) models
for POS tagging and chunking. To incorporate
smoothing into our models, we follow the follow-
ing general procedure: first, we collect a set of
unannotated text from the same domain as the test
data set. Second, we train a smoothing model on
the text of the training data, the test data, and the
additional collection. We then automatically an-
notate both the training and test data with features
calculated from the distributional representation.
Finally, we train the CRF model on the annotated
training set and apply it to the test set.
We use an open source CRF software package
designed by Sunita Sajarwal and William W. Co-
hen to implement our CRF models.1 We use a set
of boolean features listed in Table 1.
Our baseline CRF system for POS tagging fol-
lows the model described by Lafferty et al.(2001).
We include transition features between pairs of
consecutive tag variables, features between tag
variables and words, and a set of orthographic fea-
tures that Lafferty et al. found helpful for perfor-
mance on OOV words. Our smoothed models add
features computed from the distributional repre-
sentations, as discussed above.
Our chunker follows the system described by
Sha and Pereira (2003). In addition to the tran-
sition, word-level, and orthographic features, we
include features relating automatically-generated
POS tags and the chunk labels. Unlike Sha and
</bodyText>
<footnote confidence="0.982896">
1Available from http://sourceforge.net/projects/crf/
</footnote>
<table confidence="0.8725532">
CRF Feature Set
Transition zi=z
zi=z and zi−1=z0
Word xi=w and zi=z
POS ti=t and zi=z
</table>
<bodyText confidence="0.780764166666667">
Orthography for every s E {-ing, -ogy, -
ed, -s, -ly, -ion, -tion, -ity},
suffix(xi)= s and zi=z
xi is capitalized and zi = z
xi has a digit and zi = z
for every context type v,
</bodyText>
<sectionHeader confidence="0.57487" genericHeader="evaluation">
Fleft(xi) and Fvight(xi)
</sectionHeader>
<bodyText confidence="0.381298">
HMM features yi=y and zi = z
</bodyText>
<tableCaption confidence="0.88085225">
Table 1: Features used in our CRF systems. zi vari-
ables represent labels to be predicted, ti represent tags (for
the chunker), and xi represent word tokens. All features are
boolean except for the TF, TF-IDF, and LSA features.
</tableCaption>
<bodyText confidence="0.999975428571429">
Pereira, we exclude features relating consecutive
pairs of words and a chunk label, or features re-
lating consecutive tag labels and a chunk label,
in order to expedite our experiments. We found
that including such features does improve chunk-
ing F1 by approximately 2%, but it also signifi-
cantly slows down CRF training.
</bodyText>
<subsectionHeader confidence="0.996884">
3.2 Rare Word Accuracy
</subsectionHeader>
<bodyText confidence="0.956193608695652">
For these experiments, we use the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993). Following the CoNLL shared task from
2000, we use sections 15-18 of the Penn Treebank
for our labeled training data for the supervised
sequence labeler in all experiments (Tjong et al.,
2000). For the tagging experiments, we train and
test using the gold standard POS tags contained in
the Penn Treebank. For the chunking experiments,
we train and test with POS tags that are automati-
cally generated by a standard tagger (Brill, 1994).
We tested the accuracy of our models for chunking
and POS tagging on section 20 of the Penn Tree-
bank, which corresponds to the test set from the
CoNLL 2000 task.
Our distributional representations are trained on
sections 2-22 of the Penn Treebank. Because we
include the text from the train and test sets in our
training data for the distributional representations,
we do not need to worry about smoothing them
— when they are decoded on the test set, they
TF, TF-IDF, and
LSA features
</bodyText>
<page confidence="0.623019">
498
</page>
<table confidence="0.999746714285714">
Freq: 0 1 2 0-2 all
#Samples 438 508 588 1534 46661
Baseline .62 .77 .81 .74 .93
TF .76 .72 .77 .75 .92
TF-IDF .82 .75 .76 .78 .94
LSA .78 .80 .77 .78 .94
HMM .73 .81 .86 .80 .94
</table>
<tableCaption confidence="0.9332795">
Table 2: POS tagging accuracy: our HMM-smoothed
tagger outperforms the baseline tagger by 6% on rare
</tableCaption>
<bodyText confidence="0.975155853658536">
words. Differences between the baseline and the HMM are
statistically significant at P &lt; 0.01 for the OOV, 0-2, and all
cases using the two-tailed Chi-squared test with 1 degree of
freedom.
will not encounter any previously unseen words.
However, to speed up training during our exper-
iments and, in some cases, to avoid running out
of memory, we replaced words appearing twice or
fewer times in the data with the special symbol
*UNKNOWN*. In addition, all numbers were re-
placed with another special symbol. For the LSA
model, we had to use a more drastic cutoff to fit
the singular value decomposition computation into
memory: we replaced words appearing 10 times or
fewer with the *UNKNOWN* symbol. We initial-
ize our HMMs randomly. We run EM ten times
and take the model with the best cross-entropy on
a held-out set. After experimenting with differ-
ent variations of HMM models, we settled on a
model with 80 latent states as a good compromise
between accuracy and efficiency.
For our POS tagging experiments, we measured
the accuracy of the tagger on “rare” words, or
words that appear at most twice in the training
data. For our chunking experiments, we focus on
chunks that begin with rare words, as we found
that those were the most difficult for the chunker
to identify correctly. So we define “rare” chunks
as those that begin with words appearing at most
twice in training data. To ensure that our smooth-
ing models have enough training data for our test
set, we further narrow our focus to those words
that appear rarely in the labeled training data, but
appear at least ten times in sections 2-22. Tables 2
and 3 show the accuracy of our smoothed models
and the baseline model on tagging and chunking,
respectively. The line for “all” in both tables indi-
cates results on the complete test set.
Both our baseline tagger and chunker achieve
respectable results on their respective tasks for
all words, and the results were good enough for
</bodyText>
<table confidence="0.993754142857143">
Freq: 0 1 2 0-2 all
#Samples 133 199 231 563 21900
Baseline .69 .75 .81 .76 .90
TF .70 .82 .79 .77 .89
TF-IDF .77 .77 .80 .78 .90
LSA .84 .82 .83 .84 .90
HMM .90 .85 .85 .86 .93
</table>
<tableCaption confidence="0.9890945">
Table 3: Chunking F1: our HMM-smoothed chunker
outperforms the baseline CRF chunker by 0.21 on chunks
that begin with OOV words, and 0.10 on chunks that be-
gin with rare words.
</tableCaption>
<bodyText confidence="0.999889657894737">
us to be satisfied that performance on rare words
closely follows how a state-of-the-art supervised
sequence-labeler behaves. The chunker’s accuracy
is roughly in the middle of the range of results for
the original CoNLL 2000 shared task (Tjong et
al., 2000) . While several systems have achieved
slightly higher accuracy on supervised POS tag-
ging, they are usually trained on larger training
sets.
As expected, the drop-off in the baseline sys-
tem’s performance from all words to rare words
is impressive for both tasks. Comparing perfor-
mance on all terms and OOV terms, the baseline
tagger’s accuracy drops by 0.31, and the baseline
chunker’s F1 drops by 0.21. Comparing perfor-
mance on all terms and rare terms, the drop is less
severe but still dramatic: 0.19 for tagging and 0.15
for chunking.
Our hypothesis that smoothing would improve
performance on rare terms is validated by these ex-
periments. In fact, the more aggregation a smooth-
ing model performs, the better it appears to be at
smoothing. The HMM-smoothed system outper-
forms all other systems in all categories except
tagging on OOV words, where TF-IDF performs
best. And in most cases, the clear trend is for
HMM smoothing to outperform LSA, which in
turn outperforms TF and TF-IDF. HMM tagging
performance on OOV terms improves by 11%, and
chunking performance by 21%. Tagging perfor-
mance on all of the rare terms improves by 6%,
and chunking by 10%. In chunking, there is a
clear trend toward larger increases in performance
as words become rarer in the labeled data set, from
a 0.02 improvement on words of frequency 2, to an
improvement of 0.21 on OOV words.
Because the test data for this experiment is
drawn from the same domain (newswire) as the
</bodyText>
<page confidence="0.998462">
499
</page>
<bodyText confidence="0.9999613">
training data, the rare terms make up a relatively
small portion of the overall dataset (approximately
4% of both the tagged words and the chunks).
Still, the increased performance by the HMM-
smoothed model on the rare-word subset con-
tributes in part to an increase in performance on
the overall dataset of 1% for tagging and 3% for
chunking. In our next experiment, we consider
a common scenario where rare terms make up a
much larger fraction of the test data.
</bodyText>
<subsectionHeader confidence="0.997811">
3.3 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999896073170731">
For our experiment on domain adaptation, we fo-
cus on NP chunking and POS tagging, and we
use the labeled training data from the CoNLL
2000 shared task as before. For NP chunking, we
use 198 sentences from the biochemistry domain
in the Open American National Corpus (OANC)
(Reppen et al., 2005) as or our test set. We man-
ually tagged the test set with POS tags and NP
chunk boundaries. The test set contains 5330
words and a total of 1258 NP chunks. We used
sections 15-18 of the Penn Treebank as our labeled
training set, including the gold standard POS tags.
We use our best-performing smoothing model, the
HMM, and train it on sections 13 through 19 of
the Penn Treebank, plus the written portion of
the OANC that contains journal articles from bio-
chemistry (40,727 sentences). We focus on chunks
that begin with words appearing 0-2 times in the
labeled training data, and appearing at least ten
times in the HMM’s training data. Table 4 con-
tains our results. For our POS tagging experi-
ments, we use 561 MEDLINE sentences (9576
words) from the Penn BioIE project (PennBioIE,
2005), a test set previously used by Blitzer et
al.(2006). We use the same experimental setup as
Blitzer et al.: 40,000 manually tagged sentences
from the Penn Treebank for our labeled training
data, and all of the unlabeled text from the Penn
Treebank plus their MEDLINE corpus of 71,306
sentences to train our HMM. We report on tagging
accuracy for all words and OOV words in Table
5. This table also includes results for two previous
systems as reported by Blitzer et al. (2006): the
semi-supervised Alternating Structural Optimiza-
tion (ASO) technique and the Structural Corre-
spondence Learning (SCL) technique for domain
adaptation.
Note that this test set for NP chunking con-
tains a much higher proportion of rare and OOV
words: 23% of chunks begin with an OOV word,
and 29% begin with a rare word, as compared with
</bodyText>
<table confidence="0.998587857142857">
Freq. # Baseline HMM
R P F1 R P F1
0 284 .74 .70 .72 .80 .89 .84
1 39 .85 .87 .86 .92 .88 .90
2 39 .79 .86 .83 .92 .90 .91
0-2 362 .75 .73 .74 .82 .89 .85
all 1258 .86 .87 .86 .91 .90 .91
</table>
<tableCaption confidence="0.992150857142857">
Table 4: On biochemistry journal data from the OANC,
our HMM-smoothed NP chunker outperforms the base-
line CRF chunker by 0.12 (F1) on chunks that begin with
OOV words, and by 0.05 (F1) on all chunks. Results in
bold are statistically significantly different from the baseline
results at P &lt; 0.05 using the two-tailed Fisher’s exact test.
We did not perform significance tests for F1.
</tableCaption>
<table confidence="0.969430166666667">
All Unknown
Model words words
Baseline 88.3 67.3
ASO 88.4 70.9
SCL 88.9 72.0
HMM 90.5 75.2
</table>
<tableCaption confidence="0.818014714285714">
Table 5: On biomedical data from the Penn BioIE
project, our HMM-smoothed tagger outperforms the
SCL tagger by 3% (accuracy) on OOV words, and by
1.6% (accuracy) on all words. Differences between the
smoothed tagger and the SCL tagger are significant at P &lt;
.001 for all words and for OOV words, using the Chi-squared
test with 1 degree of freedom.
</tableCaption>
<bodyText confidence="0.99985765">
1% and 4%, respectively, for NP chunks in the test
set from the original domain. The test set for tag-
ging also contains a much higher proportion: 23%
OOV words, as compared with 1% in the original
domain. Because of the increase in the number of
rare words, the baseline chunker’s overall perfor-
mance drops by 4% compared with performance
on WSJ data, and the baseline tagger’s overall per-
formance drops by 5% in the new domain.
The performance improvements for both the
smoothed NP chunker and tagger are again im-
pressive: there is a 12% improvement on OOV
words, and a 10% overall improvement on rare
words for chunking; the tagger shows an 8% im-
provement on OOV words compared to out base-
line and a 3% improvement on OOV words com-
pared to the SCL model. The resulting perfor-
mance of the smoothed NP chunker is almost iden-
tical to its performance on the WSJ data. Through
smoothing, the chunker not only improves by 5%
</bodyText>
<page confidence="0.981584">
500
</page>
<bodyText confidence="0.999987863636364">
in F1 over the baseline system on all words, it in
fact outperforms our baseline NP chunker on the
WSJ data. 60% of this improvement comes from
improved accuracy on rare words.
The performance of our HMM-smoothed chun-
ker caused us to wonder how well the chunker
could work without some of its other features. We
removed all tag features and all features for word
types that appear fewer than 20 times in training.
This chunker achieves 0.91 F1 on OANC data, and
0.93 F1 on WSJ data, outperforming the baseline
system in both cases. It has only 20% as many fea-
tures as the baseline chunker, greatly improving
its training time. Thus our smoothing features are
more valuable to the chunker than features from
POS tags and features for all but the most common
words. Our results point to the exciting possibil-
ity that with smoothing, we may be able to train a
sequence-labeling system on a small labeled sam-
ple, and have it apply generally to other domains.
Exactly what size training set we need is a ques-
tion that we address next.
</bodyText>
<subsectionHeader confidence="0.998495">
3.4 Sample Complexity
</subsectionHeader>
<bodyText confidence="0.999939724137931">
Our complete system consists of two learned com-
ponents, a supervised CRF system and an unsu-
pervised smoothing model. We measure the sam-
ple complexity of each component separately. To
measure the sample complexity of the supervised
CRF, we use the same experimental setup as in
the chunking experiment on WSJ text, but we vary
the amount of labeled data available to the CRF.
We take ten random samples of a fixed size from
the labeled training set, train a chunking model on
each subset, and graph the F1 on the labeled test
set, averaged over the ten runs, in Figure 1. To
measure the sample complexity of our HMM with
respect to unlabeled text, we use the full labeled
training set and vary the amount of unlabeled text
available to the HMM. At minimum, we use the
text available in the labeled training and test sets,
and then add random subsets of the Penn Tree-
bank, sections 2-22. For each subset size, we take
ten random samples of the unlabeled text, train an
HMM and then a chunking model, and graph the
F1 on the labeled test set averaged over the ten
runs in Figure 2.
The results from our labeled sample complex-
ity experiment indicate that sample complexity is
drastically reduced by HMM smoothing. On rare
chunks, the smoothed system reaches 0.78 F1 us-
ing only 87 labeled training sentences, a level that
the baseline system never reaches, even with 6933
</bodyText>
<figureCaption confidence="0.9979722">
Figure 1: The smoothed NP chunker requires less than
10% of the samples needed by the baseline chunker to
achieve .83 F1, and the same for .88 F1.
Figure 2: By leveraging plentiful unannotated text, the
smoothed chunker soon outperforms the baseline.
</figureCaption>
<bodyText confidence="0.999863210526316">
labeled sentences. On the overall data set, the
smoothed system reaches 0.83 F1 with 50 labeled
sentences, which the baseline does not reach un-
til it has 867 labeled sentences. With 434 labeled
sentences, the smoothed system reaches 0.88 F1,
which the baseline system does not reach until it
has 5200 labeled samples.
Our unlabeled sample complexity results show
that even with access to a small amount of unla-
beled text, 6000 sentences more than what appears
in the training and test sets, smoothing using the
HMM yields 0.78 F1 on rare chunks. However, the
smoothed system requires 25,000 more sentences
before it outperforms the baseline system on all
chunks. No peak in performance is reached, so
further improvements are possible with more unla-
beled data. Thus smoothing is optimizing perfor-
mance for the case where unlabeled data is plenti-
ful and labeled data is scarce, as we would hope.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.995664">
To our knowledge, only one previous system —
the REALM system for sparse information extrac-
</bodyText>
<figure confidence="0.999097580645161">
F1 (Chunking)
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
1 10 100 1000 10000
Number of Labeled Sentences (log scale)
HMM (rare)
Labeled Sample Complexity
HMM (all)
baseline (all)
baseline (rare)
F1 (Chunking)
0.95
0.90
0.85
.
0.80
0.75
0.70
0 10000 20000 30000 40000
Unlabeled Sample Complexity
Baseline (all)
Number of Unannotated Sentences
HMM (all) HMM (rare)
Baseline (rare)
</figure>
<page confidence="0.991295">
501
</page>
<bodyText confidence="0.99998521686747">
tion — has used HMMs as a feature represen-
tation for other applications. REALM uses an
HMM trained on a large corpus to help determine
whether the arguments of a candidate relation are
of the appropriate type (Downey et al., 2007). We
extend and generalize this smoothing technique
and apply it to common NLP applications involv-
ing supervised sequence-labeling, and we provide
an in-depth empirical analysis of its performance.
Several researchers have previously studied
methods for using unlabeled data for tagging and
chunking, either alone or as a supplement to la-
beled data. Ando and Zhang develop a semi-
supervised chunker that outperforms purely su-
pervised approaches on the CoNLL 2000 dataset
(Ando and Zhang, 2005). Recent projects in semi-
supervised (Toutanova and Johnson, 2007) and un-
supervised (Biemann et al., 2007; Smith and Eis-
ner, 2005) tagging also show significant progress.
Unlike these systems, our efforts are aimed at us-
ing unlabeled data to find distributional represen-
tations that work well on rare terms, making the
supervised systems more applicable to other do-
mains and decreasing their sample complexity.
HMMs have been used many times for POS
tagging and chunking, in supervised, semi-
supervised, and in unsupervised settings (Banko
and Moore, 2004; Goldwater and Griffiths, 2007;
Johnson, 2007; Zhou, 2004). We take a novel per-
spective on the use of HMMs by using them to
compute features of each token in the data that
represent the distribution over that token’s con-
texts. Our technique lets the HMM find param-
eters that maximize cross-entropy, and then uses
labeled data to learn the best mapping from the
HMM categories to the POS categories.
Smoothing in NLP usually refers to the prob-
lem of smoothing n-gram models. Sophisticated
smoothing techniques like modified Kneser-Ney
and Katz smoothing (Chen and Goodman, 1996)
smooth together the predictions of unigram, bi-
gram, trigram, and potentially higher n-gram se-
quences to obtain accurate probability estimates in
the face of data sparsity. Our task differs in that we
are primarily concerned with the case where even
the unigram model (single word) is rarely or never
observed in the labeled training data.
Sparsity for low-order contexts has recently
spurred interest in using latent variables to repre-
sent distributions over contexts in language mod-
els. While n-gram models have traditionally dom-
inated in language modeling, two recent efforts de-
velop latent-variable probabilistic models that ri-
val and even surpass n-gram models in accuracy
(Blitzer et al., 2005; Mnih and Hinton, 2007).
Several authors investigate neural network mod-
els that learn not just one latent state, but rather a
vector of latent variables, to represent each word
in a language model (Bengio et al., 2003; Emami
et al., 2003; Morin and Bengio, 2005).
One of the benefits of our smoothing technique
is that it allows for domain adaptation, a topic
that has received a great deal of attention from
the NLP community recently. Unlike our tech-
nique, in most cases researchers have focused on
the scenario where labeled training data is avail-
able in both the source and the target domain
(e.g., (Daum´e III, 2007; Chelba and Acero, 2004;
Daum´e III and Marcu, 2006)). Our technique uses
unlabeled training data from the target domain,
and is thus applicable more generally, including
in web processing, where the domain and vocab-
ulary is highly variable, and it is extremely diffi-
cult to obtain labeled data that is representative of
the test distribution. When labeled target-domain
data is available, instance weighting and similar
techniques can be used in combination with our
smoothing technique to improve our results fur-
ther, although this has not yet been demonstrated
empirically. HMM-smoothing improves on the
most closely related work, the Structural Corre-
spondence Learning technique for domain adap-
tation (Blitzer et al., 2006), in experiments.
</bodyText>
<sectionHeader confidence="0.990527" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999828777777778">
Our study of smoothing techniques demonstrates
that by aggregating information across many
unannotated examples, it is possible to find ac-
curate distributional representations that can pro-
vide highly informative features to supervised se-
quence labelers. These features help improve se-
quence labeling performance on rare word types,
on domains that differ from the training set, and
on smaller training sets.
Further experiments are of course necessary
to investigate distributional representations as
smoothing techniques. One particularly promis-
ing area for further study is the combination of
smoothing and instance weighting techniques for
domain adaptation. Whether the current tech-
niques are applicable to structured prediction
tasks, like parsing and relation extraction, also de-
serves future attention.
</bodyText>
<page confidence="0.995886">
502
</page>
<sectionHeader confidence="0.995754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907877358491">
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In ACL.
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
C. Biemann, C. Giuliano, and A. Gliozzo. 2007. Un-
supervised pos tagging supporting supervised meth-
ods. Proceeding of RANLP-07.
J. Blitzer, A. Globerson, and F. Pereira. 2005. Dis-
tributed latent variable models of lexical cooccur-
rences. In Proceedings of the Tenth International
Workshop on Artificial Intelligence and Statistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
E. Brill. 1994. Some Advances in Rule-Based Part of
Speech Tagging. In AAAI, pages 722–727, Seattle,
Washington.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In EMNLP.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310–318, Morristown, NJ, USA. Association
for Computational Linguistics.
Hal Daum´e III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal ofArtificial
Intelligence Research, 26.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society ofInformation Science, 41(6):391–407.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
A. Emami, P. Xu, and F. Jelinek. 2003. Using a
connectionist model in a syntactical based language
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 372–
375.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2-3):245–273.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers. In EMNLP.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313–330.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641–648, New York,
NY, USA. ACM.
F. Morin and Y. Bengio. 2005. Hierarchical probabilis-
tic neural network language model. In Proceedings
of the International Workshop on Artificial Intelli-
gence and Statistics, pages 246–252.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–
285.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
American national corpus (ANC) second release.
Linguistic Data Consortium.
F. Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
Human Language Technology - NAACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 354–362, Ann Arbor, Michigan, June.
Erik F. Tjong, Kim Sang, and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the 4th Conference on
Computational Natural Language Learning, pages
127–132.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian LDA-based model for semi-supervised
part-of-speech tagging. In NIPS.
GuoDong Zhou. 2004. Discriminative hidden Markov
modeling with long state dependence using a kNN
ensemble. In COLING.
</reference>
<page confidence="0.998859">
503
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916598">
<title confidence="0.998083">Distributional Representations for Handling Sparsity in Supervised Sequence-Labeling</title>
<author confidence="0.999465">Fei Huang</author>
<affiliation confidence="0.999255">Temple University</affiliation>
<address confidence="0.9921575">1805 N. Broad St. Wachman Hall 324</address>
<email confidence="0.996223">tub58431@temple.edu</email>
<abstract confidence="0.997587346153846">Supervised sequence-labeling systems in natural language processing often suffer from data sparsity because they use word types as features in their prediction tasks. Consequently, they have difficulty estimating parameters for types which appear in the test set, but seldom (or never) appear in the training set. We demonstrate that distributional representations of word types, trained on unannotated text, can be used to improve performance on rare words. We incorporate aspects of these representations into the feature space of our sequence-labeling systems. In an experiment on a standard chunking dataset, our best technique improves a chunker from 0.76 F1 to 0.86 F1 on chunks beginning with rare words. On the same dataset, it improves our part-of-speech tagger from 74% to 80% accuracy on rare words. Furthermore, our system improves significantly over a baseline system when applied to text from a different domain, and it reduces the sample complexity of sequence labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30923" citStr="Ando and Zhang, 2005" startWordPosition="5299" endWordPosition="5302"> a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007;</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A highperformance semi-supervised learning method for text chunking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part of speech tagging in context.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="31476" citStr="Banko and Moore, 2004" startWordPosition="5386" endWordPosition="5389">pervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the p</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>Michele Banko and Robert C. Moore. 2004. Part of speech tagging in context. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="32971" citStr="Bengio et al., 2003" startWordPosition="5627" endWordPosition="5630">er observed in the labeled training data. Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007). Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005). One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently. Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daum´e III, 2007; Chelba and Acero, 2004; Daum´e III and Marcu, 2006)). Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
<author>C Giuliano</author>
<author>A Gliozzo</author>
</authors>
<title>Unsupervised pos tagging supporting supervised methods. Proceeding of RANLP-07.</title>
<date>2007</date>
<contexts>
<context position="31027" citStr="Biemann et al., 2007" startWordPosition="5315" endWordPosition="5318">pe (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each t</context>
</contexts>
<marker>Biemann, Giuliano, Gliozzo, 2007</marker>
<rawString>C. Biemann, C. Giuliano, and A. Gliozzo. 2007. Unsupervised pos tagging supporting supervised methods. Proceeding of RANLP-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>A Globerson</author>
<author>F Pereira</author>
</authors>
<title>Distributed latent variable models of lexical cooccurrences.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="32753" citStr="Blitzer et al., 2005" startWordPosition="5590" endWordPosition="5593">igher n-gram sequences to obtain accurate probability estimates in the face of data sparsity. Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data. Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007). Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005). One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently. Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.</context>
</contexts>
<marker>Blitzer, Globerson, Pereira, 2005</marker>
<rawString>J. Blitzer, A. Globerson, and F. Pereira. 2005. Distributed latent variable models of lexical cooccurrences. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2591" citStr="Blitzer et al., 2006" startWordPosition="410" endWordPosition="413">ed examples from which to estimate parameters. The negative effects of data sparsity have been well-documented in the NLP literature. The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001). Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daum´e III and Marcu, 2006; Chelba and Acero, 2004). We investigate the use of distributional representations, which model the probability distribution of a word’s context, as techniques for finding smoothed representations of word sequences. That is, we use the distributional representations to share information across unannotated examples of the same word type. We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers. Our technique is particularly well-suited to handling data sparsity because it is possible to improve per</context>
<context position="23879" citStr="Blitzer et al. (2006)" startWordPosition="4062" endWordPosition="4065"> 4 contains our results. For our POS tagging experiments, we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE, 2005), a test set previously used by Blitzer et al.(2006). We use the same experimental setup as Blitzer et al.: 40,000 manually tagged sentences from the Penn Treebank for our labeled training data, and all of the unlabeled text from the Penn Treebank plus their MEDLINE corpus of 71,306 sentences to train our HMM. We report on tagging accuracy for all words and OOV words in Table 5. This table also includes results for two previous systems as reported by Blitzer et al. (2006): the semi-supervised Alternating Structural Optimization (ASO) technique and the Structural Correspondence Learning (SCL) technique for domain adaptation. Note that this test set for NP chunking contains a much higher proportion of rare and OOV words: 23% of chunks begin with an OOV word, and 29% begin with a rare word, as compared with Freq. # Baseline HMM R P F1 R P F1 0 284 .74 .70 .72 .80 .89 .84 1 39 .85 .87 .86 .92 .88 .90 2 39 .79 .86 .83 .92 .90 .91 0-2 362 .75 .73 .74 .82 .89 .85 all 1258 .86 .87 .86 .91 .90 .91 Table 4: On biochemistry journal data from the OANC, our HMM-smoothed NP</context>
<context position="34102" citStr="Blitzer et al., 2006" startWordPosition="5808" endWordPosition="5811"> target domain, and is thus applicable more generally, including in web processing, where the domain and vocabulary is highly variable, and it is extremely difficult to obtain labeled data that is representative of the test distribution. When labeled target-domain data is available, instance weighting and similar techniques can be used in combination with our smoothing technique to improve our results further, although this has not yet been demonstrated empirically. HMM-smoothing improves on the most closely related work, the Structural Correspondence Learning technique for domain adaptation (Blitzer et al., 2006), in experiments. 5 Conclusion and Future Work Our study of smoothing techniques demonstrates that by aggregating information across many unannotated examples, it is possible to find accurate distributional representations that can provide highly informative features to supervised sequence labelers. These features help improve sequence labeling performance on rare word types, on domains that differ from the training set, and on smaller training sets. Further experiments are of course necessary to investigate distributional representations as smoothing techniques. One particularly promising are</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in Rule-Based Part of Speech Tagging.</title>
<date>1994</date>
<booktitle>In AAAI,</booktitle>
<pages>722--727</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="17043" citStr="Brill, 1994" startWordPosition="2858" endWordPosition="2859">t it also significantly slows down CRF training. 3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). Following the CoNLL shared task from 2000, we use sections 15-18 of the Penn Treebank for our labeled training data for the supervised sequence labeler in all experiments (Tjong et al., 2000). For the tagging experiments, we train and test using the gold standard POS tags contained in the Penn Treebank. For the chunking experiments, we train and test with POS tags that are automatically generated by a standard tagger (Brill, 1994). We tested the accuracy of our models for chunking and POS tagging on section 20 of the Penn Treebank, which corresponds to the test set from the CoNLL 2000 task. Our distributional representations are trained on sections 2-22 of the Penn Treebank. Because we include the text from the train and test sets in our training data for the distributional representations, we do not need to worry about smoothing them — when they are decoded on the test set, they TF, TF-IDF, and LSA features 498 Freq: 0 1 2 0-2 all #Samples 438 508 588 1534 46661 Baseline .62 .77 .81 .74 .93 TF .76 .72 .77 .75 .92 TF-I</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some Advances in Rule-Based Part of Speech Tagging. In AAAI, pages 722–727, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy classifier: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2644" citStr="Chelba and Acero, 2004" startWordPosition="419" endWordPosition="422"> negative effects of data sparsity have been well-documented in the NLP literature. The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001). Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daum´e III and Marcu, 2006; Chelba and Acero, 2004). We investigate the use of distributional representations, which model the probability distribution of a word’s context, as techniques for finding smoothed representations of word sequences. That is, we use the distributional representations to share information across unannotated examples of the same word type. We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers. Our technique is particularly well-suited to handling data sparsity because it is possible to improve performance on rare words by supplementing the training </context>
<context position="33398" citStr="Chelba and Acero, 2004" startWordPosition="5701" endWordPosition="5704">). Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005). One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently. Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daum´e III, 2007; Chelba and Acero, 2004; Daum´e III and Marcu, 2006)). Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where the domain and vocabulary is highly variable, and it is extremely difficult to obtain labeled data that is representative of the test distribution. When labeled target-domain data is available, instance weighting and similar techniques can be used in combination with our smoothing technique to improve our results further, although this has not yet been demonstrated empirically. HMM-smoothing improves on the most closely rel</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy classifier: Little data can help a lot. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="32054" citStr="Chen and Goodman, 1996" startWordPosition="5481" endWordPosition="5484"> unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity. Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data. Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variab</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>26</volume>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal ofArtificial Intelligence Research, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society ofInformation Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="10775" citStr="Deerwester et al., 1990" startWordPosition="1784" endWordPosition="1787">ll share that category in their context distributions. Any patterns learned for the more common “red lamp” will then also apply to the less common “magenta tablecloth.” Our second distributional representation aggregates information from multiple context words by grouping together the distributions P(xi−1 = v|xi = w) and P(xi−1 = v0|xi = w) if v and v0 appear together with many of the same words w. Aggregating counts in this way smooths our representations even further, by supplying better estimates when the data is too sparse to estimate P(xi−1|xi) accurately. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely-used technique for computing dimensionality-reduced representations from a bag-of-words model. We apply LSA to the set of right context vectors and the set of left context vectors separately, to find compact versions of each vector, where each dimension represents a combination of several context word types. We normalize each vector, and then calculate features as above. After experimenting with different choices for the number of dimensions to reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes the performance of our supervised sequence labelers on</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society ofInformation Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="12489" citStr="Dempster et al., 1977" startWordPosition="2077" endWordPosition="2080">can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005). An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi. Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi−1. The distribution for a corpus x = (x1, ... , xN) given a set of state vectors Y = (y1, ... , yN) is given by: P (x|Y) = � P(xi|yi)P(yi|yi−1) i Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi−1) from unlabeled data. We use a trained HMM to determine the optimal sequence of latent states yi using the wellknown Viterbi algorithm (Rabiner, 1989). The output of this process is an integer (ranging from 1 to S) for every word xi in the corpus; we include a new boolean feature for each possible value of yi in our sequence labelers. To compare our models, note that in the multinomial representation we directly model the probability that a word v appears before a word w: P(xi−1 = v|xi = w)). In our LSA model, we find la</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
</authors>
<title>Sparse information extraction: Unsupervised language models to the rescue.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30431" citStr="Downey et al., 2007" startWordPosition="5224" endWordPosition="5227">formation extracF1 (Chunking) 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 1 1 10 100 1000 10000 Number of Labeled Sentences (log scale) HMM (rare) Labeled Sample Complexity HMM (all) baseline (all) baseline (rare) F1 (Chunking) 0.95 0.90 0.85 . 0.80 0.75 0.70 0 10000 20000 30000 40000 Unlabeled Sample Complexity Baseline (all) Number of Unannotated Sentences HMM (all) HMM (rare) Baseline (rare) 501 tion — has used HMMs as a feature representation for other applications. REALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Sm</context>
</contexts>
<marker>Downey, Schoenmackers, Etzioni, 2007</marker>
<rawString>Doug Downey, Stefan Schoenmackers, and Oren Etzioni. 2007. Sparse information extraction: Unsupervised language models to the rescue. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Emami</author>
<author>P Xu</author>
<author>F Jelinek</author>
</authors>
<title>Using a connectionist model in a syntactical based language model.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>372--375</pages>
<contexts>
<context position="32991" citStr="Emami et al., 2003" startWordPosition="5631" endWordPosition="5634">beled training data. Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007). Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005). One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently. Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daum´e III, 2007; Chelba and Acero, 2004; Daum´e III and Marcu, 2006)). Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where the domain and vocab</context>
</contexts>
<marker>Emami, Xu, Jelinek, 2003</marker>
<rawString>A. Emami, P. Xu, and F. Jelinek. 2003. Using a connectionist model in a syntactical based language model. In Proceedings of the International Conference on Spoken Language Processing, pages 372– 375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Factorial hidden markov models.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="12005" citStr="Ghahramani and Jordan, 1997" startWordPosition="1985" endWordPosition="1988">ld-out data. 2.3 Latent Variable Language Model Representation To take smoothing one step further, we present a technique that aggregates context distributions both for similar context words xi−1 = v and v0, and for similar words xi = w and w0. Latent variable language models (LVLMs) can be used to produce just such a distributional representation. We use Hidden Markov Models (HMMs) as the main example in the discussion and as the LVLMs in our experiments, but the smoothing technique can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005). An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi. Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi−1. The distribution for a corpus x = (x1, ... , xN) given a set of state vectors Y = (y1, ... , yN) is given by: P (x|Y) = � P(xi|yi)P(yi|yi−1) i Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi−1) from unlabeled data. We use a trained HMM</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I. Jordan. 1997. Factorial hidden markov models. Machine Learning, 29(2-3):245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31507" citStr="Goldwater and Griffiths, 2007" startWordPosition="5390" endWordPosition="5393">the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully bayesian approach to unsupervised part-ofspeech tagging. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="30988" citStr="Johnson, 2007" startWordPosition="5310" endWordPosition="5311">elation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by u</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2328" citStr="Lafferty et al., 2001" startWordPosition="362" endWordPosition="365">y a few times, and many word types will be left out of the training set altogether. As a consequence, for Alexander Yates Temple University 1805 N. Broad St. Wachman Hall 324 yates@temple.edu many word types supervised NLP systems have very few, or even zero, labeled examples from which to estimate parameters. The negative effects of data sparsity have been well-documented in the NLP literature. The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001). Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daum´e III and Marcu, 2006; Chelba and Acero, 2004). We investigate the use of distributional representations, which model the probability distribution of a word’s context, as techniques for finding smoothed representations of word sequences. That is, we use the distributional representations to share information across unannotated e</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="16607" citStr="Marcus et al., 1993" startWordPosition="2783" endWordPosition="2786">s represent labels to be predicted, ti represent tags (for the chunker), and xi represent word tokens. All features are boolean except for the TF, TF-IDF, and LSA features. Pereira, we exclude features relating consecutive pairs of words and a chunk label, or features relating consecutive tag labels and a chunk label, in order to expedite our experiments. We found that including such features does improve chunking F1 by approximately 2%, but it also significantly slows down CRF training. 3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). Following the CoNLL shared task from 2000, we use sections 15-18 of the Penn Treebank for our labeled training data for the supervised sequence labeler in all experiments (Tjong et al., 2000). For the tagging experiments, we train and test using the gold standard POS tags contained in the Penn Treebank. For the chunking experiments, we train and test with POS tags that are automatically generated by a standard tagger (Brill, 1994). We tested the accuracy of our models for chunking and POS tagging on section 20 of the Penn Treebank, which corresponds to the test set from the CoNLL 2000 task. </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="32777" citStr="Mnih and Hinton, 2007" startWordPosition="5594" endWordPosition="5597"> to obtain accurate probability estimates in the face of data sparsity. Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data. Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007). Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005). One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently. Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daum´e III, 2007; C</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine Learning, pages 641–648, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morin</author>
<author>Y Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="33016" citStr="Morin and Bengio, 2005" startWordPosition="5635" endWordPosition="5638"> Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variable probabilistic models that rival and even surpass n-gram models in accuracy (Blitzer et al., 2005; Mnih and Hinton, 2007). Several authors investigate neural network models that learn not just one latent state, but rather a vector of latent variables, to represent each word in a language model (Bengio et al., 2003; Emami et al., 2003; Morin and Bengio, 2005). One of the benefits of our smoothing technique is that it allows for domain adaptation, a topic that has received a great deal of attention from the NLP community recently. Unlike our technique, in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain (e.g., (Daum´e III, 2007; Chelba and Acero, 2004; Daum´e III and Marcu, 2006)). Our technique uses unlabeled training data from the target domain, and is thus applicable more generally, including in web processing, where the domain and vocabulary is highly variable,</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the International Workshop on Artificial Intelligence and Statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PennBioIE</author>
</authors>
<title>Mining the bibliome project.</title>
<date>2005</date>
<note>http://bioie.ldc.upenn.edu/.</note>
<contexts>
<context position="23403" citStr="PennBioIE, 2005" startWordPosition="3981" endWordPosition="3982">15-18 of the Penn Treebank as our labeled training set, including the gold standard POS tags. We use our best-performing smoothing model, the HMM, and train it on sections 13 through 19 of the Penn Treebank, plus the written portion of the OANC that contains journal articles from biochemistry (40,727 sentences). We focus on chunks that begin with words appearing 0-2 times in the labeled training data, and appearing at least ten times in the HMM’s training data. Table 4 contains our results. For our POS tagging experiments, we use 561 MEDLINE sentences (9576 words) from the Penn BioIE project (PennBioIE, 2005), a test set previously used by Blitzer et al.(2006). We use the same experimental setup as Blitzer et al.: 40,000 manually tagged sentences from the Penn Treebank for our labeled training data, and all of the unlabeled text from the Penn Treebank plus their MEDLINE corpus of 71,306 sentences to train our HMM. We report on tagging accuracy for all words and OOV words in Table 5. This table also includes results for two previous systems as reported by Blitzer et al. (2006): the semi-supervised Alternating Structural Optimization (ASO) technique and the Structural Correspondence Learning (SCL) t</context>
</contexts>
<marker>PennBioIE, 2005</marker>
<rawString>PennBioIE. 2005. Mining the bibliome project. http://bioie.ldc.upenn.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>285</pages>
<contexts>
<context position="12713" citStr="Rabiner, 1989" startWordPosition="2117" endWordPosition="2118"> xi in the corpus conditioned on a latent variable Yi. Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi−1. The distribution for a corpus x = (x1, ... , xN) given a set of state vectors Y = (y1, ... , yN) is given by: P (x|Y) = � P(xi|yi)P(yi|yi−1) i Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi−1) from unlabeled data. We use a trained HMM to determine the optimal sequence of latent states yi using the wellknown Viterbi algorithm (Rabiner, 1989). The output of this process is an integer (ranging from 1 to S) for every word xi in the corpus; we include a new boolean feature for each possible value of yi in our sequence labelers. To compare our models, note that in the multinomial representation we directly model the probability that a word v appears before a word w: P(xi−1 = v|xi = w)). In our LSA model, we find latent categories of context words z, and model the probability that a category appears before the current word w: P(xi−1 = z|xi = w). The HMM finds (probabilistic) categories Y for both the current word xi and the context wor</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randi Reppen</author>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>American national corpus (ANC) second release. Linguistic Data Consortium.</title>
<date>2005</date>
<contexts>
<context position="22614" citStr="Reppen et al., 2005" startWordPosition="3839" endWordPosition="3842"> increased performance by the HMMsmoothed model on the rare-word subset contributes in part to an increase in performance on the overall dataset of 1% for tagging and 3% for chunking. In our next experiment, we consider a common scenario where rare terms make up a much larger fraction of the test data. 3.3 Domain Adaptation For our experiment on domain adaptation, we focus on NP chunking and POS tagging, and we use the labeled training data from the CoNLL 2000 shared task as before. For NP chunking, we use 198 sentences from the biochemistry domain in the Open American National Corpus (OANC) (Reppen et al., 2005) as or our test set. We manually tagged the test set with POS tags and NP chunk boundaries. The test set contains 5330 words and a total of 1258 NP chunks. We used sections 15-18 of the Penn Treebank as our labeled training set, including the gold standard POS tags. We use our best-performing smoothing model, the HMM, and train it on sections 13 through 19 of the Penn Treebank, plus the written portion of the OANC that contains journal articles from biochemistry (40,727 sentences). We focus on chunks that begin with words appearing 0-2 times in the labeled training data, and appearing at least</context>
</contexts>
<marker>Reppen, Ide, Suderman, 2005</marker>
<rawString>Randi Reppen, Nancy Ide, and Keith Suderman. 2005. American national corpus (ANC) second release. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology - NAACL.</booktitle>
<contexts>
<context position="15391" citStr="Sha and Pereira (2003)" startWordPosition="2578" endWordPosition="2581">re package designed by Sunita Sajarwal and William W. Cohen to implement our CRF models.1 We use a set of boolean features listed in Table 1. Our baseline CRF system for POS tagging follows the model described by Lafferty et al.(2001). We include transition features between pairs of consecutive tag variables, features between tag variables and words, and a set of orthographic features that Lafferty et al. found helpful for performance on OOV words. Our smoothed models add features computed from the distributional representations, as discussed above. Our chunker follows the system described by Sha and Pereira (2003). In addition to the transition, word-level, and orthographic features, we include features relating automatically-generated POS tags and the chunk labels. Unlike Sha and 1Available from http://sourceforge.net/projects/crf/ CRF Feature Set Transition zi=z zi=z and zi−1=z0 Word xi=w and zi=z POS ti=t and zi=z Orthography for every s E {-ing, -ogy, - ed, -s, -ly, -ion, -tion, -ity}, suffix(xi)= s and zi=z xi is capitalized and zi = z xi has a digit and zi = z for every context type v, Fleft(xi) and Fvight(xi) HMM features yi=y and zi = z Table 1: Features used in our CRF systems. zi variables re</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of Human Language Technology - NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>354--362</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="12030" citStr="Smith and Eisner, 2005" startWordPosition="1989" endWordPosition="1992">ble Language Model Representation To take smoothing one step further, we present a technique that aggregates context distributions both for similar context words xi−1 = v and v0, and for similar words xi = w and w0. Latent variable language models (LVLMs) can be used to produce just such a distributional representation. We use Hidden Markov Models (HMMs) as the main example in the discussion and as the LVLMs in our experiments, but the smoothing technique can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005). An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi. Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi−1. The distribution for a corpus x = (x1, ... , xN) given a set of state vectors Y = (y1, ... , yN) is given by: P (x|Y) = � P(xi|yi)P(yi|yi−1) i Using Expectation-Maximization (Dempster et al., 1977), it is possible to estimate the distributions for P(xi|yi) and P(yi|yi−1) from unlabeled data. We use a trained HMM to determine the optimal</context>
<context position="31052" citStr="Smith and Eisner, 2005" startWordPosition="5319" endWordPosition="5323">7). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that rep</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 354–362, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
<author>Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Conference on Computational Natural Language Learning,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="16800" citStr="Tjong et al., 2000" startWordPosition="2815" endWordPosition="2818">eatures relating consecutive pairs of words and a chunk label, or features relating consecutive tag labels and a chunk label, in order to expedite our experiments. We found that including such features does improve chunking F1 by approximately 2%, but it also significantly slows down CRF training. 3.2 Rare Word Accuracy For these experiments, we use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). Following the CoNLL shared task from 2000, we use sections 15-18 of the Penn Treebank for our labeled training data for the supervised sequence labeler in all experiments (Tjong et al., 2000). For the tagging experiments, we train and test using the gold standard POS tags contained in the Penn Treebank. For the chunking experiments, we train and test with POS tags that are automatically generated by a standard tagger (Brill, 1994). We tested the accuracy of our models for chunking and POS tagging on section 20 of the Penn Treebank, which corresponds to the test set from the CoNLL 2000 task. Our distributional representations are trained on sections 2-22 of the Penn Treebank. Because we include the text from the train and test sets in our training data for the distributional repres</context>
<context position="20373" citStr="Tjong et al., 2000" startWordPosition="3452" endWordPosition="3455">gh for Freq: 0 1 2 0-2 all #Samples 133 199 231 563 21900 Baseline .69 .75 .81 .76 .90 TF .70 .82 .79 .77 .89 TF-IDF .77 .77 .80 .78 .90 LSA .84 .82 .83 .84 .90 HMM .90 .85 .85 .86 .93 Table 3: Chunking F1: our HMM-smoothed chunker outperforms the baseline CRF chunker by 0.21 on chunks that begin with OOV words, and 0.10 on chunks that begin with rare words. us to be satisfied that performance on rare words closely follows how a state-of-the-art supervised sequence-labeler behaves. The chunker’s accuracy is roughly in the middle of the range of results for the original CoNLL 2000 shared task (Tjong et al., 2000) . While several systems have achieved slightly higher accuracy on supervised POS tagging, they are usually trained on larger training sets. As expected, the drop-off in the baseline system’s performance from all words to rare words is impressive for both tasks. Comparing performance on all terms and OOV terms, the baseline tagger’s accuracy drops by 0.31, and the baseline chunker’s F1 drops by 0.21. Comparing performance on all terms and rare terms, the drop is less severe but still dramatic: 0.19 for tagging and 0.15 for chunking. Our hypothesis that smoothing would improve performance on ra</context>
</contexts>
<marker>Tjong, Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong, Kim Sang, and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the 4th Conference on Computational Natural Language Learning, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian LDA-based model for semi-supervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="30988" citStr="Toutanova and Johnson, 2007" startWordPosition="5308" endWordPosition="5311"> a candidate relation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by u</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2007. A bayesian LDA-based model for semi-supervised part-of-speech tagging. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
</authors>
<title>Discriminative hidden Markov modeling with long state dependence using a kNN ensemble.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="31535" citStr="Zhou, 2004" startWordPosition="5396" endWordPosition="5397"> Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially hig</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>GuoDong Zhou. 2004. Discriminative hidden Markov modeling with long state dependence using a kNN ensemble. In COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>