<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001392">
<title confidence="0.982625">
Softmax-Margin CRFs: Training Log-Linear Models with Cost Functions
</title>
<author confidence="0.998736">
Kevin Gimpel Noah A. Smith
</author>
<affiliation confidence="0.900215333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.995937">
{kgimpel,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996366875">
We describe a method of incorporating task-
specific cost functions into standard condi-
tional log-likelihood (CLL) training of linear
structured prediction models. Recently intro-
duced in the speech recognition community,
we describe the method generally for struc-
tured models, highlight connections to CLL
and max-margin learning for structured pre-
diction (Taskar et al., 2003), and show that
the method optimizes a bound on risk. The
approach is simple, efficient, and easy to im-
plement, requiring very little change to an
existing CLL implementation. We present
experimental results comparing with several
commonly-used methods for training struc-
tured predictors for named-entity recognition.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999700571428572">
Conditional random fields (CRFs; Lafferty et al,
2001) and other conditional log-linear models
(Berger et al., 1996) achieve strong performance
for many NLP problems, but the conditional log-
likelihood (CLL) criterion optimized when training
these models cannot take a task-specific cost func-
tion into account.
In this paper, we describe a simple approach
for training conditional log-linear models with cost
functions. We show how the method relates to other
methods and how it provides a bound on risk. We
apply the method to train a discriminative model
for named-entity recognition, showing a statistically
significant improvement over CLL.
</bodyText>
<sectionHeader confidence="0.982062" genericHeader="method">
2 Structured Log-Linear Models
</sectionHeader>
<bodyText confidence="0.999665111111111">
Let X denote a structured input space and, for a par-
ticular x E X, let �(x) denote a structured output
space for x. The size of �(x) is often exponential
in x, which differentiates structured prediction from
multiclass classification. For named-entity recogni-
tion, for example, x might be a sentence and �(x)
the set of all possible named-entity labelings for the
sentence. Given an x E X and a y E �(x), we use a
conditional log-linear model for pe(y|x):
</bodyText>
<equation confidence="0.9703455">
pe exp{0Tf (x, y)} (1)
(y�x ) _ EY/E%x) exp{0Tf (x, y&apos;)}
</equation>
<bodyText confidence="0.993162666666667">
where f(x, y) is a feature vector representation of
x and y and 0 is a parameter vector containing one
component for each feature.
</bodyText>
<subsectionHeader confidence="0.995719">
2.1 Training Criteria
</subsectionHeader>
<bodyText confidence="0.999940857142857">
Many criteria exist for training the weights 0. We
next review three choices in detail. For the follow-
ing, we assume a training set consisting of n exam-
ples {(x(z), y(z))}Z_1. Some criteria will make use of
a task-specific cost function that measures the extent
to which a structure y differs from the true structure
y(z), denoted by cost(y(z), y).
</bodyText>
<subsectionHeader confidence="0.791259">
2.1.1 Conditional Log-Likelihood
</subsectionHeader>
<bodyText confidence="0.999964">
The learning problem for maximizing conditional
log-likelihood is shown in Eq. 3 in Fig. 1 (we trans-
form it into a minimization problem for easier com-
parison). This criterion is commonly used when a
probabilistic interpretation of the model is desired.
</bodyText>
<subsectionHeader confidence="0.979986">
2.1.2 Max-Margin
</subsectionHeader>
<bodyText confidence="0.997949666666667">
An alternative approach to training structured lin-
ear classifiers is based on maximum-margin Markov
networks (Taskar et al., 2003). The basic idea is
to choose weights such that the linear score of each
(x(z), y(z)) is better than (x(z), y) for all alternatives
y E �(x(z)) \ {y(z)}, with a larger margin for those
y with higher cost. The “margin rescaling” form of
this training criterion is shown in Eq. 4. Note that
the cost function is incorporated into the criterion.
</bodyText>
<subsectionHeader confidence="0.619491">
2.1.3 Risk
</subsectionHeader>
<bodyText confidence="0.746421">
Risk is defined as the expected value of the cost
with respect to the conditional distribution pe(y|x);
</bodyText>
<page confidence="0.96375">
733
</page>
<subsubsectionHeader confidence="0.553896">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736,
</subsubsectionHeader>
<subsectionHeader confidence="0.241319">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.870556">
on training data:
</bodyText>
<equation confidence="0.9990525">
En EyE�(x(i)) p�(y|x(i))cost(y(i), y) (2)
i=1
</equation>
<bodyText confidence="0.999948">
With a log-linear model, learning then requires solv-
ing the problem shown in Eq. 5. Unlike the previous
two criteria, risk is typically non-convex.
Risk minimization first appeared in the speech
recognition community (Kaiser et al., 2000; Povey
and Woodland, 2002). In NLP, Smith and Eis-
ner (2006) minimized risk using k-best lists to de-
fine the distribution over output structures. Li and
Eisner (2009) introduced a novel semiring for min-
imizing risk using dynamic programming; Xiong et
al. (2009) minimized risk in a CRF.
</bodyText>
<subsubsectionHeader confidence="0.669292">
2.1.4 Other Criteria
</subsubsectionHeader>
<bodyText confidence="0.999950333333333">
Many other criteria have been proposed to at-
tempt to tailor training conditions to match task-
specific evaluation metrics. These include the aver-
age per-label marginal likelihood for sequence label-
ing (Kakade et al., 2002), minimum error-rate train-
ing for machine translation (Och, 2003), F1 for lo-
gistic regression classifiers (Jansche, 2005), and a
wide range of possible metrics for sequence label-
ing and segmentation tasks (Suzuki et al., 2006).
</bodyText>
<sectionHeader confidence="0.967195" genericHeader="method">
3 Softmax-Margin
</sectionHeader>
<bodyText confidence="0.999982071428571">
The softmax-margin objective is shown as Eq. 6 and
is a generalization of that used by Povey et al. (2008)
and similar to that used by Sha and Saul (2006).
The simple intuition is the same as the intuition
in max-margin learning: high-cost outputs for x(i)
should be penalized more heavily. Another view
says that we replace the probabilistic score inside
the exp function of CLL with the “cost-augmented”
score from max-margin. A third view says that we
replace the “hard” maximum of max-margin with
the “softmax” (log E exp) from CLL; hence we use
the name “softmax-margin.” Like CLL and max-
margin, the objective is convex; a proof is provided
in Gimpel and Smith (2010).
</bodyText>
<subsectionHeader confidence="0.999499">
3.1 Relation to Other Objectives
</subsectionHeader>
<bodyText confidence="0.999773333333333">
We next show how the softmax-margin criterion
(Eq. 6) bounds the risk criterion (Eq. 5). We first
define some additional notation:
</bodyText>
<equation confidence="0.851205">
E(i)[F] = EyE%x(i)) p0(y  |x(i))F(y)
</equation>
<bodyText confidence="0.995562">
for some function F : �(x(i)) —* R. First note that
the softmax-margin objective (Eq. 6) is equal to:
</bodyText>
<equation confidence="0.979302">
(Eq. 3) + Eni=1 log E(i)[exp cost(y(i), ·)] (7)
</equation>
<bodyText confidence="0.9997175">
The first term must be nonnegative. Taking each part
of the second term, and using Jensen’s inequality,
</bodyText>
<equation confidence="0.676398">
log E(i)[ecost(y(i),·)] &gt; E(i)[log ecost(y(i),·)]
= E(i)[cost(y(i), ·)]
</equation>
<bodyText confidence="0.999825153846154">
which is exactly Eq. 5. Softmax-margin is also an
upper bound on the CLL criterion because, assum-
ing cost is nonnegative, log E[exp cost] &gt; 0. Fur-
thermore, softmax-margin is a differentiable upper
bound on max-margin, because the softmax function
is a differentiable upper bound on the max function.
We note that it may also be interest-
ing to consider minimizing the function
Eni=1 log E(i)[exp cost(y(i), ·)], since it is an
upper bound on risk but requires less computation
for computing the gradient.1 We call this objec-
tive the Jensen risk bound and include it in our
experimental comparison below.
</bodyText>
<subsectionHeader confidence="0.997216">
3.2 Implementation
</subsectionHeader>
<bodyText confidence="0.99995725">
Most methods for training structured models with
cost functions require the cost function to decom-
pose across the pieces of the structure in the same
way as the features, such as the standard methods
for maximizing margin and minimizing risk (Taskar
et al., 2003; Li and Eisner, 2009). If the same con-
ditions hold, softmax-margin training can be im-
plemented atop standard CRF training simply by
adding additional “features” to encode the local
cost components, only when computing the partition
function during training.2 The weights of these “cost
features” are not learned.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99862675">
We consider the problem of named-entity recog-
nition (NER) and use the English data from the
CoNLL 2003 shared task (Tjong Kim Sang and De
Meulder, 2003). The data consist of news articles
</bodyText>
<footnote confidence="0.98994225">
1Space does not permit a full discussion; see Gimpel and
Smith (2010) for details.
2Since cost(y(i), y(i)) = 0 by definition, these “features”
will never fire for the numerator and can be ignored.
</footnote>
<page confidence="0.977475">
734
</page>
<figure confidence="0.56902675">
CLL: min n � exp{θTf(x(i), y)} (3)
e i=1 −θTf(x(i), y(i)) + log
yE&apos;d(x(&amp;quot;))
Max-Margin: min n ( )
</figure>
<equation confidence="0.689688375">
e i=1 −θTf(x(i), y(i)) + max θTf(x(i), y) + cost(y(i), y) (4)
yE&apos;d(x(&amp;quot;))
exp{θT f (x(i), y)}
cost(y(i), y)
� (5)
y�E&apos;d(x(&amp;quot;)) exp{θTf(x(i), y�)}
�n E
i=1 yE&apos;d(x(&amp;quot;))
</equation>
<figure confidence="0.9420754">
Risk: min
e
Softmax-Margin: min n � exp{θTf(x(i), y) + cost(y(i), y)} (6)
e i=1 −θTf(x(i), y(i)) + log
yE&apos;d(x(&amp;quot;))
</figure>
<figureCaption confidence="0.999889">
Figure 1: Objective functions for training linear models. Regularization terms (e.g., C Edj=1 θ2j) are not shown here.
</figureCaption>
<bodyText confidence="0.999733210526316">
annotated with four entity types: person, location,
organization, and miscellaneous. Our experiments
focus on comparing training objectives for struc-
tured sequential models for this task. For all objec-
tives, we use the same standard set of feature tem-
plates, following Kazama and Torisawa (2007) with
additional token shape like those in Collins (2002b)
and simple gazetteer features. A feature was in-
cluded if it occurred at least once in training data
(total 1,312,255 features).
The task is evaluated using the F1 score, which
is the harmonic mean of precision and recall (com-
puted at the level of entire entities). Since this metric
is computed from corpus-level precision and recall,
it is not easily decomposable into features used in
standard chain CRFs. For simplicity, we only con-
sider Hamming cost in this paper; experiments with
other cost functions more targeted to NER are pre-
sented in Gimpel and Smith (2010).
</bodyText>
<subsectionHeader confidence="0.996504">
4.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999882742857143">
We compared softmax-margin to several baselines:
the structured perceptron (Collins, 2002a), 1-best
MIRA with cost-augmented inference (Crammer et
al., 2006), CLL, max-margin, risk, and our Jensen
risk bound (JRB) introduced above.
We used L2 regularization, experimenting with
several coefficients for each method. For CLL,
softmax-margin, max-margin, and MIRA, we used
regularization coefficients C ∈ {0.01, 0.1,1}. Risk
has not always been used with regularization, as reg-
ularization does not have as clear a probabilistic in-
terpretation with risk as it does with CLL; so, for
risk and JRB we only used C ∈ {0.0, 0.01}. In
addition, since these two objectives are non-convex,
we initialized with the output of the best-performing
CLL model on dev data (which was the CLL model
with C = 0.01).3 All methods except CLL and the
perceptron make use of a cost function, for which
we used Hamming cost. We experimented with dif-
ferent fixed multipliers m for the cost function, for
m ∈ {1, 5, 10, 20}.
The hyperparameters C and m were tuned on the
development data and the best-performing combina-
tion was used to label the test data. We also tuned
the decision to average parameters across all train-
ing iterations; this has generally been found to help
the perceptron and MIRA, but in our experiments
had mixed results for the other methods.
We ran 100 iterations through the training data for
each method. For CLL, softmax-margin, risk, and
JRB, we used stochastic gradient ascent with a fixed
step size of 0.01. For max-margin, we used stochas-
tic subgradient ascent (Ratliff et al., 2006) also with
a fixed step size of 0.01.4 For the perceptron and
MIRA, we used their built-in step size formulas.
</bodyText>
<subsectionHeader confidence="0.824896">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.966433888888889">
Table 1 shows our results. On test data, softmax-
margin is statistically indistinguishable from MIRA,
risk, and JRB, but performs significantly better
than CLL, max-margin, and the perceptron (p &lt;
0.03, paired bootstrap with 10,000 samples; Koehn,
3When using initialization of all ones for risk and JRB, re-
sults were several points below the results here, and with all
zeroes, learning failed, resulting in 0.0 F-measure on dev data.
Thus, risk and JRB appear sensitive to model initialization.
</bodyText>
<footnote confidence="0.952252">
4In preliminary experiments, we tried other fixed and de-
creasing step sizes for (sub)gradient ascent and found that a
fixed step of 0.01 consistently performed well across training
objectives, so we used it for all settings for simplicity.
</footnote>
<page confidence="0.989879">
735
</page>
<table confidence="0.99701975">
Method Dev. Test (C, m, avg.?)
Perceptron 90.48 83.98 (Y)
MIRA 91.13 85.72 (0.01, 20, Y)
CLL 90.79 85.46 (0.01, N)
Max-Margin 91.17 85.28 (0.01, 1, Y)
Risk 91.14 85.59 (0.01, 10, N)
JRB 91.05 85.65 (0.01, 1, N)
Softmax-Margin 91.30 85.84 (0.01, 5, N)
</table>
<tableCaption confidence="0.9977155">
Table 1: Results on development and test sets, along with
hyperparameter values chosen using development set.
</tableCaption>
<bodyText confidence="0.999826857142857">
2004). It may be surprising that an improvement
of 0.38 in Fi could be significant, but this indicates
that the improvements are not limited to certain cate-
gories of phenomena in a small number of sentences
but rather appear throughout the majority of the test
set. The Jensen risk bound performs comparably to
risk, and takes roughly half as long to train.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999994210526316">
The softmax-margin approach offers (1) a convex
objective, (2) the ability to incorporate task-specific
cost functions, and (3) a probabilistic interpretation
(which supports, e.g., hidden-variable learning and
computation of posteriors). In contrast, max-margin
training and MIRA do not provide (3); risk and
JRB do not provide (1); and CLL does not support
(2). Furthermore, softmax-margin training improves
over standard CLL training of CRFs, is straightfor-
ward to implement, and requires the same amount of
computation as CLL.
We have also presented the Jensen risk bound,
which is easier to implement and faster to train than
risk, yet gives comparable performance. The pri-
mary limitation of all these approaches, including
softmax-margin, is that they only support cost func-
tions that factor in the same way as the features of
the model. Future work might exploit approximate
inference for more expressive cost functions.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999765666666667">
We thank the reviewers, John Lafferty, and Andr´e Martins
for helpful comments and feedback on this work. This
research was supported by NSF grant IIS-0844507.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999809483333334">
A. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39–71.
M. Collins. 2002a. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
M. Collins. 2002b. Ranking algorithms for named-entity ex-
traction: Boosting and the voted perceptron. In Proc. of
ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
Journal of Machine Learning Research, 7:551–585.
K. Gimpel and N. A. Smith. 2010. Softmax-margin training
for structured log-linear models. Technical report, Carnegie
Mellon University.
M. Jansche. 2005. Maximum expected F-measure training of
logistic regression models. In Proc. of HLT-EMNLP.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss function
for the overall risk criterion based discriminative training of
HMM models. In Proc. of ICSLP.
S. Kakade, Y. W. Teh, and S. Roweis. 2002. An alternate ob-
jective function for Markovian fields. In Proc. of ICML.
J. Kazama and K. Torisawa. 2007. A new perceptron algorithm
for sequence labeling with non-local features. In Proc. of
EMNLP-CoNLL.
P. Koehn. 2004. Statistical significance tests for machine trans-
lation evaluation. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
Z. Li and J. Eisner. 2009. First- and second-order expecta-
tion semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In Proc. of ACL.
D. Povey and P. C. Woodland. 2002. Minimum phone error and
I-smoothing for improved discrimative training. In Proc. of
ICASSP.
D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran,
G. Saon, and K. Visweswariah. 2008. Boosted MMI for
model and feature space discriminative training. In Proc. of
ICASSP.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006. Subgradient
methods for maximum margin structured learning. In ICML
Workshop on Learning in Structured Output Spaces.
F. Sha and L. K. Saul. 2006. Large margin hidden Markov
models for automatic speech recognition. In Proc. of NIPS.
D. A. Smith and J. Eisner. 2006. Minimum risk annealing for
training log-linear models. In Proc. of COLING-ACL.
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Training con-
ditional random fields with multivariate evaluation measures.
In Proc. of COLING-ACL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in NIPS 16.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proc. of CoNLL.
Y. Xiong, J. Zhu, H. Huang, and H. Xu. 2009. Minimum tag
error for discriminative training of conditional random fields.
Information Sciences, 179(1-2):169–179.
</reference>
<page confidence="0.998562">
736
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920323">
<title confidence="0.999745">Softmax-Margin CRFs: Training Log-Linear Models with Cost Functions</title>
<author confidence="0.999992">Kevin Gimpel Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.995336176470588">We describe a method of incorporating taskspecific cost functions into standard conditional log-likelihood (CLL) training of linear structured prediction models. Recently introduced in the speech recognition community, we describe the method generally for structured models, highlight connections to CLL and max-margin learning for structured prediction (Taskar et al., 2003), and show that the method optimizes a bound on risk. The approach is simple, efficient, and easy to implement, requiring very little change to an existing CLL implementation. We present experimental results comparing with several commonly-used methods for training structured predictors for named-entity recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1037" citStr="Berger et al., 1996" startWordPosition="141" endWordPosition="144">ion community, we describe the method generally for structured models, highlight connections to CLL and max-margin learning for structured prediction (Taskar et al., 2003), and show that the method optimizes a bound on risk. The approach is simple, efficient, and easy to implement, requiring very little change to an existing CLL implementation. We present experimental results comparing with several commonly-used methods for training structured predictors for named-entity recognition. 1 Introduction Conditional random fields (CRFs; Lafferty et al, 2001) and other conditional log-linear models (Berger et al., 1996) achieve strong performance for many NLP problems, but the conditional loglikelihood (CLL) criterion optimized when training these models cannot take a task-specific cost function into account. In this paper, we describe a simple approach for training conditional log-linear models with cost functions. We show how the method relates to other methods and how it provides a bound on risk. We apply the method to train a discriminative model for named-entity recognition, showing a statistically significant improvement over CLL. 2 Structured Log-Linear Models Let X denote a structured input space and</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8425" citStr="Collins (2002" startWordPosition="1352" endWordPosition="1353">i), y�)} �n E i=1 yE&apos;d(x(&amp;quot;)) Risk: min e Softmax-Margin: min n � exp{θTf(x(i), y) + cost(y(i), y)} (6) e i=1 −θTf(x(i), y(i)) + log yE&apos;d(x(&amp;quot;)) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C Edj=1 θ2j) are not shown here. annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented in Gimpel and Smith (2010). 4.1 Baselines We compared s</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002a. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="8425" citStr="Collins (2002" startWordPosition="1352" endWordPosition="1353">i), y�)} �n E i=1 yE&apos;d(x(&amp;quot;)) Risk: min e Softmax-Margin: min n � exp{θTf(x(i), y) + cost(y(i), y)} (6) e i=1 −θTf(x(i), y(i)) + log yE&apos;d(x(&amp;quot;)) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C Edj=1 θ2j) are not shown here. annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented in Gimpel and Smith (2010). 4.1 Baselines We compared s</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002b. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="9169" citStr="Crammer et al., 2006" startWordPosition="1468" endWordPosition="1471">tures). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented in Gimpel and Smith (2010). 4.1 Baselines We compared softmax-margin to several baselines: the structured perceptron (Collins, 2002a), 1-best MIRA with cost-augmented inference (Crammer et al., 2006), CLL, max-margin, risk, and our Jensen risk bound (JRB) introduced above. We used L2 regularization, experimenting with several coefficients for each method. For CLL, softmax-margin, max-margin, and MIRA, we used regularization coefficients C ∈ {0.01, 0.1,1}. Risk has not always been used with regularization, as regularization does not have as clear a probabilistic interpretation with risk as it does with CLL; so, for risk and JRB we only used C ∈ {0.0, 0.01}. In addition, since these two objectives are non-convex, we initialized with the output of the best-performing CLL model on dev data (w</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Softmax-margin training for structured log-linear models.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="5451" citStr="Gimpel and Smith (2010)" startWordPosition="864" endWordPosition="867">d is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard” maximum of max-margin with the “softmax” (log E exp) from CLL; hence we use the name “softmax-margin.” Like CLL and maxmargin, the objective is convex; a proof is provided in Gimpel and Smith (2010). 3.1 Relation to Other Objectives We next show how the softmax-margin criterion (Eq. 6) bounds the risk criterion (Eq. 5). We first define some additional notation: E(i)[F] = EyE%x(i)) p0(y |x(i))F(y) for some function F : �(x(i)) —* R. First note that the softmax-margin objective (Eq. 6) is equal to: (Eq. 3) + Eni=1 log E(i)[exp cost(y(i), ·)] (7) The first term must be nonnegative. Taking each part of the second term, and using Jensen’s inequality, log E(i)[ecost(y(i),·)] &gt; E(i)[log ecost(y(i),·)] = E(i)[cost(y(i), ·)] which is exactly Eq. 5. Softmax-margin is also an upper bound on the CLL</context>
<context position="7448" citStr="Gimpel and Smith (2010)" startWordPosition="1190" endWordPosition="1193">d minimizing risk (Taskar et al., 2003; Li and Eisner, 2009). If the same conditions hold, softmax-margin training can be implemented atop standard CRF training simply by adding additional “features” to encode the local cost components, only when computing the partition function during training.2 The weights of these “cost features” are not learned. 4 Experiments We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). The data consist of news articles 1Space does not permit a full discussion; see Gimpel and Smith (2010) for details. 2Since cost(y(i), y(i)) = 0 by definition, these “features” will never fire for the numerator and can be ignored. 734 CLL: min n � exp{θTf(x(i), y)} (3) e i=1 −θTf(x(i), y(i)) + log yE&apos;d(x(&amp;quot;)) Max-Margin: min n ( ) e i=1 −θTf(x(i), y(i)) + max θTf(x(i), y) + cost(y(i), y) (4) yE&apos;d(x(&amp;quot;)) exp{θT f (x(i), y)} cost(y(i), y) � (5) y�E&apos;d(x(&amp;quot;)) exp{θTf(x(i), y�)} �n E i=1 yE&apos;d(x(&amp;quot;)) Risk: min e Softmax-Margin: min n � exp{θTf(x(i), y) + cost(y(i), y)} (6) e i=1 −θTf(x(i), y(i)) + log yE&apos;d(x(&amp;quot;)) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C Edj=1</context>
<context position="8996" citStr="Gimpel and Smith (2010)" startWordPosition="1446" endWordPosition="1449">h additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented in Gimpel and Smith (2010). 4.1 Baselines We compared softmax-margin to several baselines: the structured perceptron (Collins, 2002a), 1-best MIRA with cost-augmented inference (Crammer et al., 2006), CLL, max-margin, risk, and our Jensen risk bound (JRB) introduced above. We used L2 regularization, experimenting with several coefficients for each method. For CLL, softmax-margin, max-margin, and MIRA, we used regularization coefficients C ∈ {0.01, 0.1,1}. Risk has not always been used with regularization, as regularization does not have as clear a probabilistic interpretation with risk as it does with CLL; so, for risk</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>K. Gimpel and N. A. Smith. 2010. Softmax-margin training for structured log-linear models. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jansche</author>
</authors>
<title>Maximum expected F-measure training of logistic regression models.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="4655" citStr="Jansche, 2005" startWordPosition="730" endWordPosition="731"> Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard” ma</context>
</contexts>
<marker>Jansche, 2005</marker>
<rawString>M. Jansche. 2005. Maximum expected F-measure training of logistic regression models. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kaiser</author>
<author>B Horvat</author>
<author>Z Kacic</author>
</authors>
<title>A novel loss function for the overall risk criterion based discriminative training of HMM models.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="4005" citStr="Kaiser et al., 2000" startWordPosition="625" endWordPosition="628"> 2.1.3 Risk Risk is defined as the expected value of the cost with respect to the conditional distribution pe(y|x); 733 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics on training data: En EyE�(x(i)) p�(y|x(i))cost(y(i), y) (2) i=1 With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 f</context>
</contexts>
<marker>Kaiser, Horvat, Kacic, 2000</marker>
<rawString>J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss function for the overall risk criterion based discriminative training of HMM models. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kakade</author>
<author>Y W Teh</author>
<author>S Roweis</author>
</authors>
<title>An alternate objective function for Markovian fields.</title>
<date>2002</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="4534" citStr="Kakade et al., 2002" startWordPosition="711" endWordPosition="714">. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside</context>
</contexts>
<marker>Kakade, Teh, Roweis, 2002</marker>
<rawString>S. Kakade, Y. W. Teh, and S. Roweis. 2002. An alternate objective function for Markovian fields. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>A new perceptron algorithm for sequence labeling with non-local features.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="8369" citStr="Kazama and Torisawa (2007)" startWordPosition="1341" endWordPosition="1344">(x(&amp;quot;)) exp{θT f (x(i), y)} cost(y(i), y) � (5) y�E&apos;d(x(&amp;quot;)) exp{θTf(x(i), y�)} �n E i=1 yE&apos;d(x(&amp;quot;)) Risk: min e Softmax-Margin: min n � exp{θTf(x(i), y) + cost(y(i), y)} (6) e i=1 −θTf(x(i), y(i)) + log yE&apos;d(x(&amp;quot;)) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C Edj=1 θ2j) are not shown here. annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>J. Kazama and K. Torisawa. 2007. A new perceptron algorithm for sequence labeling with non-local features. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="975" citStr="Lafferty et al, 2001" startWordPosition="132" endWordPosition="135">d prediction models. Recently introduced in the speech recognition community, we describe the method generally for structured models, highlight connections to CLL and max-margin learning for structured prediction (Taskar et al., 2003), and show that the method optimizes a bound on risk. The approach is simple, efficient, and easy to implement, requiring very little change to an existing CLL implementation. We present experimental results comparing with several commonly-used methods for training structured predictors for named-entity recognition. 1 Introduction Conditional random fields (CRFs; Lafferty et al, 2001) and other conditional log-linear models (Berger et al., 1996) achieve strong performance for many NLP problems, but the conditional loglikelihood (CLL) criterion optimized when training these models cannot take a task-specific cost function into account. In this paper, we describe a simple approach for training conditional log-linear models with cost functions. We show how the method relates to other methods and how it provides a bound on risk. We apply the method to train a discriminative model for named-entity recognition, showing a statistically significant improvement over CLL. 2 Structur</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4171" citStr="Li and Eisner (2009)" startWordPosition="653" endWordPosition="656">Conference of the North American Chapter of the ACL, pages 733–736, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics on training data: En EyE�(x(i)) p�(y|x(i))cost(y(i), y) (2) i=1 With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax</context>
<context position="6885" citStr="Li and Eisner, 2009" startWordPosition="1099" endWordPosition="1102">max function. We note that it may also be interesting to consider minimizing the function Eni=1 log E(i)[exp cost(y(i), ·)], since it is an upper bound on risk but requires less computation for computing the gradient.1 We call this objective the Jensen risk bound and include it in our experimental comparison below. 3.2 Implementation Most methods for training structured models with cost functions require the cost function to decompose across the pieces of the structure in the same way as the features, such as the standard methods for maximizing margin and minimizing risk (Taskar et al., 2003; Li and Eisner, 2009). If the same conditions hold, softmax-margin training can be implemented atop standard CRF training simply by adding additional “features” to encode the local cost components, only when computing the partition function during training.2 The weights of these “cost features” are not learned. 4 Experiments We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). The data consist of news articles 1Space does not permit a full discussion; see Gimpel and Smith (2010) for details. 2Since cost(y(i), y(i))</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Z. Li and J. Eisner. 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4599" citStr="Och, 2003" startWordPosition="722" endWordPosition="723">ser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>P C Woodland</author>
</authors>
<title>Minimum phone error and I-smoothing for improved discrimative training.</title>
<date>2002</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="4032" citStr="Povey and Woodland, 2002" startWordPosition="629" endWordPosition="632">efined as the expected value of the cost with respect to the conditional distribution pe(y|x); 733 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics on training data: En EyE�(x(i)) p�(y|x(i))cost(y(i), y) (2) i=1 With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression clas</context>
</contexts>
<marker>Povey, Woodland, 2002</marker>
<rawString>D. Povey and P. C. Woodland. 2002. Minimum phone error and I-smoothing for improved discrimative training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>D Kanevsky</author>
<author>B Kingsbury</author>
<author>B Ramabhadran</author>
<author>G Saon</author>
<author>K Visweswariah</author>
</authors>
<title>Boosted MMI for model and feature space discriminative training.</title>
<date>2008</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="4885" citStr="Povey et al. (2008)" startWordPosition="768" endWordPosition="771"> minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard” maximum of max-margin with the “softmax” (log E exp) from CLL; hence we use the name “softmax-margin.” Like CLL and maxmargin, the objective is convex; a proof is provided in Gimpel and Smith (2010). 3.1 Relation to Other Objectives</context>
</contexts>
<marker>Povey, Kanevsky, Kingsbury, Ramabhadran, Saon, Visweswariah, 2008</marker>
<rawString>D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah. 2008. Boosted MMI for model and feature space discriminative training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ratliff</author>
<author>J A Bagnell</author>
<author>M Zinkevich</author>
</authors>
<title>Subgradient methods for maximum margin structured learning.</title>
<date>2006</date>
<booktitle>In ICML Workshop on Learning in Structured Output Spaces.</booktitle>
<contexts>
<context position="10598" citStr="Ratliff et al., 2006" startWordPosition="1710" endWordPosition="1713">unction, for m ∈ {1, 5, 10, 20}. The hyperparameters C and m were tuned on the development data and the best-performing combination was used to label the test data. We also tuned the decision to average parameters across all training iterations; this has generally been found to help the perceptron and MIRA, but in our experiments had mixed results for the other methods. We ran 100 iterations through the training data for each method. For CLL, softmax-margin, risk, and JRB, we used stochastic gradient ascent with a fixed step size of 0.01. For max-margin, we used stochastic subgradient ascent (Ratliff et al., 2006) also with a fixed step size of 0.01.4 For the perceptron and MIRA, we used their built-in step size formulas. 4.2 Results Table 1 shows our results. On test data, softmaxmargin is statistically indistinguishable from MIRA, risk, and JRB, but performs significantly better than CLL, max-margin, and the perceptron (p &lt; 0.03, paired bootstrap with 10,000 samples; Koehn, 3When using initialization of all ones for risk and JRB, results were several points below the results here, and with all zeroes, learning failed, resulting in 0.0 F-measure on dev data. Thus, risk and JRB appear sensitive to mode</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2006</marker>
<rawString>N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006. Subgradient methods for maximum margin structured learning. In ICML Workshop on Learning in Structured Output Spaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>L K Saul</author>
</authors>
<title>Large margin hidden Markov models for automatic speech recognition.</title>
<date>2006</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="4933" citStr="Sha and Saul (2006)" startWordPosition="778" endWordPosition="781">any other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard” maximum of max-margin with the “softmax” (log E exp) from CLL; hence we use the name “softmax-margin.” Like CLL and maxmargin, the objective is convex; a proof is provided in Gimpel and Smith (2010). 3.1 Relation to Other Objectives We next show how the softmax-margin criterion (</context>
</contexts>
<marker>Sha, Saul, 2006</marker>
<rawString>F. Sha and L. K. Saul. 2006. Large margin hidden Markov models for automatic speech recognition. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="4065" citStr="Smith and Eisner (2006)" startWordPosition="635" endWordPosition="639"> cost with respect to the conditional distribution pe(y|x); 733 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics on training data: En EyE�(x(i)) p�(y|x(i))cost(y(i), y) (2) i=1 With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wi</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. A. Smith and J. Eisner. 2006. Minimum risk annealing for training log-linear models. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>E McDermott</author>
<author>H Isozaki</author>
</authors>
<title>Training conditional random fields with multivariate evaluation measures.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="4760" citStr="Suzuki et al., 2006" startWordPosition="746" endWordPosition="749">tructures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard” maximum of max-margin with the “softmax” (log E exp) from CLL; hence we use the name “softmax-margin.” Like</context>
</contexts>
<marker>Suzuki, McDermott, Isozaki, 2006</marker>
<rawString>J. Suzuki, E. McDermott, and H. Isozaki. 2006. Training conditional random fields with multivariate evaluation measures. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in NIPS 16.</booktitle>
<contexts>
<context position="3044" citStr="Taskar et al., 2003" startWordPosition="470" endWordPosition="473">), y(z))}Z_1. Some criteria will make use of a task-specific cost function that measures the extent to which a structure y differs from the true structure y(z), denoted by cost(y(z), y). 2.1.1 Conditional Log-Likelihood The learning problem for maximizing conditional log-likelihood is shown in Eq. 3 in Fig. 1 (we transform it into a minimization problem for easier comparison). This criterion is commonly used when a probabilistic interpretation of the model is desired. 2.1.2 Max-Margin An alternative approach to training structured linear classifiers is based on maximum-margin Markov networks (Taskar et al., 2003). The basic idea is to choose weights such that the linear score of each (x(z), y(z)) is better than (x(z), y) for all alternatives y E �(x(z)) \ {y(z)}, with a larger margin for those y with higher cost. The “margin rescaling” form of this training criterion is shown in Eq. 4. Note that the cost function is incorporated into the criterion. 2.1.3 Risk Risk is defined as the expected value of the cost with respect to the conditional distribution pe(y|x); 733 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736, Los Angeles, California, </context>
<context position="6863" citStr="Taskar et al., 2003" startWordPosition="1095" endWordPosition="1098">e upper bound on the max function. We note that it may also be interesting to consider minimizing the function Eni=1 log E(i)[exp cost(y(i), ·)], since it is an upper bound on risk but requires less computation for computing the gradient.1 We call this objective the Jensen risk bound and include it in our experimental comparison below. 3.2 Implementation Most methods for training structured models with cost functions require the cost function to decompose across the pieces of the structure in the same way as the features, such as the standard methods for maximizing margin and minimizing risk (Taskar et al., 2003; Li and Eisner, 2009). If the same conditions hold, softmax-margin training can be implemented atop standard CRF training simply by adding additional “features” to encode the local cost components, only when computing the partition function during training.2 The weights of these “cost features” are not learned. 4 Experiments We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). The data consist of news articles 1Space does not permit a full discussion; see Gimpel and Smith (2010) for details. 2</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In Advances in NIPS 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Xiong</author>
<author>J Zhu</author>
<author>H Huang</author>
<author>H Xu</author>
</authors>
<title>Minimum tag error for discriminative training of conditional random fields.</title>
<date>2009</date>
<journal>Information Sciences,</journal>
<pages>179--1</pages>
<contexts>
<context position="4266" citStr="Xiong et al. (2009)" startWordPosition="668" endWordPosition="671">e 2010. c�2010 Association for Computational Linguistics on training data: En EyE�(x(i)) p�(y|x(i))cost(y(i), y) (2) i=1 With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by </context>
</contexts>
<marker>Xiong, Zhu, Huang, Xu, 2009</marker>
<rawString>Y. Xiong, J. Zhu, H. Huang, and H. Xu. 2009. Minimum tag error for discriminative training of conditional random fields. Information Sciences, 179(1-2):169–179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>