<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015792">
<title confidence="0.9994525">
A Pronoun Anaphora Resolution System based on
Factorial Hidden Markov Models
</title>
<author confidence="0.999813">
Dingcheng Li Tim Miller William Schuler
</author>
<affiliation confidence="0.99995">
University of Minnesota, University of Wisconsin The Ohio State University
</affiliation>
<address confidence="0.36217">
Twin Cities, Minnesosta Milwaukee, Wisconsin Columbus, Ohio
</address>
<email confidence="0.998333">
lixxx345@umn.edu tmill@cs.umn.edu schuler@ling.osu.edu
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999429076923077">
This paper presents a supervised pronoun
anaphora resolution system based on factorial
hidden Markov models (FHMMs). The ba-
sic idea is that the hidden states of FHMMs
are an explicit short-term memory with an an-
tecedent buffer containing recently described
referents. Thus an observed pronoun can find
its antecedent from the hidden buffer, or in
terms of a generative model, the entries in the
hidden buffer generate the corresponding pro-
nouns. A system implementing this model is
evaluated on the ACE corpus with promising
performance.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925452830189">
Pronoun anaphora resolution is the task of find-
ing the correct antecedent for a given pronominal
anaphor in a document. It is a subtask of corefer-
ence resolution, which is the process of determin-
ing whether two or more linguistic expressions in
a document refer to the same entity. Adopting ter-
minology used in the Automatic Context Extraction
(ACE) program (NIST, 2003), these expressions
are called mentions. Each mention is a reference
to some entity in the domain of discourse. Men-
tions usually fall into three categories – proper men-
tions (proper names), nominal mentions (descrip-
tions), and pronominal mentions (pronouns). There
is a great deal of related work on this subject, so
the descriptions of other systems below are those
which are most related or which the current model
has drawn insight from.
Pairwise models (Yang et al., 2004; Qiu et al.,
2004) and graph-partitioning methods (McCallum
and Wellner, 2003) decompose the task into a col-
lection of pairwise or mention set coreference de-
cisions. Decisions for each pair or each group
of mentions are based on probabilities of features
extracted by discriminative learning models. The
aforementioned approaches have proven to be fruit-
ful; however, there are some notable problems. Pair-
wise modeling may fail to produce coherent parti-
tions. That is, if we link results of pairwise deci-
sions to each other, there may be conflicting corefer-
ences. Graph-partitioning methods attempt to recon-
cile pairwise scores into a final coherent clustering,
but they are combinatorially harder to work with in
discriminative approaches.
One line of research aiming at overcoming the
limitation of pairwise models is to learn a mention-
ranking model to rank preceding mentions for a
given anaphor (Denis and Baldridge, 2007) This ap-
proach results in more coherent coreference chains.
Recent years have also seen the revival of in-
terest in generative models in both machine learn-
ing and natural language processing. Haghighi
and Klein (2007), proposed an unsupervised non-
parametric Bayesian model for coreference resolu-
tion. In contrast to pairwise models, this fully gener-
ative model produces each mention from a combina-
tion of global entity properties and local attentional
state. Ng (2008) did similar work using the same un-
supervised generative model, but relaxed head gen-
eration as head-index generation, enforced agree-
ment constraints at the global level, and assigned
salience only to pronouns.
Another unsupervised generative model was re-
cently presented to tackle only pronoun anaphora
</bodyText>
<page confidence="0.969437">
1169
</page>
<note confidence="0.979498">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998441795918367">
resolution (Charniak and Elsner, 2009). The While the framework described here can be ex-
expectation-maximization algorithm (EM) was ap- tended to deeper structural information, POS tags
plied to learn parameters automatically from the alone are valuable as they can be used to incorpo-
parsed version of the North American News Cor- rate the binding features (described below).
pus (McClosky et al., 2008). This model generates a Although the system described here is evaluated
pronoun’s person, number and gender features along for pronoun resolution, the framework we describe
with the governor of the pronoun and the syntactic can be extended to more general coreference resolu-
relation between the pronoun and the governor. This tion in a fairly straightforward manner. Further, as
inference process allows the system to keep track of in other HMM-based systems, the system can be ei-
multiple hypotheses through time, including multi- ther supervised or unsupervised. But extensions to
ple different possible histories of the discourse. unsupervised learning are left for future work.
Haghighi and Klein (2010) improved their non- The final results are compared with a few super-
parametric model by sharing lexical statistics at the vised systems as the mention-ranking model (De-
level of abstract entity types. Consequently, their nis and Baldridge, 2007) and systems compared in
model substantially reduces semantic compatibility their paper, and Charniak and Elsner’s (2009) unsu-
errors. They report the best results to date on the pervised system, emPronouns. The FHMM-based
complete end-to-end coreference task. Further, this pronoun resolution system does a better job than the
model functions in an online setting at mention level. global ranking technique and other approaches. This
Namely, the system identifies mentions from a parse is a promising start for this novel FHMM-based pro-
tree and resolves resolution with a left-to-right se- noun resolution system.
quential beam search. This is similar to Luo (2005) 2 Model Description
where a Bell tree is used to score and store the This work is based on a graphical model framework
searching path. called Factorial Hidden Markov Models (FHMMs).
In this paper, we present a supervised pro- Unlike the more commonly known Hidden Markov
noun resolution system based on Factorial Hidden Model (HMM), in an FHMM the hidden state at
Markov Models (FHMMs). This system is moti- each time step is expanded to contain more than one
vated by human processing concerns, by operating random variable (as shown in Figure 1). This al-
incrementally and maintaining a limited short term lows for the use of more complex hidden states by
memory for holding recently mentioned referents. taking advantage of conditional independence be-
According to Clark and Sengul (1979), anaphoric tween substates. This conditional independence al-
definite NPs are much faster retrieved if the an- lows complex hidden states to be learned with lim-
tecedent of a pronoun is in immediately previous ited training data.
sentence. Therefore, a limited short term memory 2.1 Factorial Hidden Markov Model
should be good enough for resolving the majority of Factorial Hidden Markov Models are an extension
pronouns. In order to construct an operable model, of HMMs (Ghahramani and Jordan, 1997). HMMs
we also measured the average distance between pro- represent sequential data as a sequence of hidden
nouns and their antecedents as discussed in next sec- states generating observation states (words in this
tions and used distances as important salience fea- case) at corresponding time steps t. A most likely
tures in the model. sequence of hidden states can then be hypothesized
Second, like Morton (2000), the current sys- given any sequence of observed states, using Bayes
tem essentially uses prior information as a dis- Law (Equation 2) and Markov independence as-
course model with a time-series manner, using a sumptions (Equation 3) to define a full probability as
dynamic programming inference algorithm. Third, the product of a Transition Model (OT) prior prob-
the FHMM described here is an integrated system, ability and an Observation Model (Op) likelihood
in contrast with (Haghighi and Klein, 2010). The
model generates part of speech tags as simple struc-
tural information, as well as related semantic in-
formation at each time step or word-by-word step.
1170
</bodyText>
<equation confidence="0.9709225">
probability.
P(h1..T |o1..T) (1)
P(h1..T) · P(o1..T  |h1..T) (2)
T PΘT (ht  |ht−1) · PΘO(ot  |ht)
H (3)
t=1
</equation>
<bodyText confidence="0.99992175">
For a simple HMM, the hidden state corresponding
to each observation state only involves one variable.
An FHMM contains more than one hidden variable
in the hidden state. These hidden substates are usu-
ally layered processes that jointly generate the ev-
idence. In the model described here, the substates
are also coupled to allow interaction between the
separate processes. As Figure 1 shows, the hidden
states include three sub-states, op, cr and pos which
are short forms of operation, coreference feature and
part-of-speech. Then, the transition model expands
the left term in (3) to (4).
</bodyText>
<equation confidence="0.997754">
PΘT (ht  |ht−1)def = P(opt  |opt−1, post−1)
·P(crt  |crt−1, opt−1) (4)
·P(post  |opt, post−1)
</equation>
<bodyText confidence="0.963966">
The observation model expands from the right
term in (3) to (5).
</bodyText>
<equation confidence="0.946209">
PΘO(ot  |ht)def = P(ot  |post, crt) (5)
</equation>
<bodyText confidence="0.999624166666667">
The observation state depends on more than one hid-
den state at each time step in an FHMM. Each hid-
den variable can be further split into smaller vari-
ables. What these terms stand for and the motiva-
tions behind the above equations will be explained
in the next section.
</bodyText>
<subsectionHeader confidence="0.9912085">
2.2 Modeling a Coreference Resolver with
FHMMs
</subsectionHeader>
<bodyText confidence="0.999772">
FHMMs in our model, like standard HMMs, can-
not represent the hierarchical structure of a syntac-
tic phrase. In order to partially represent this in-
formation, the head word is used to represent the
whole noun phrase. After coreference is resolved,
the coreferring chain can then be expanded to the
whole phrase with NP chunker tools.
In this system, hidden states are composed of
three main variables: a referent operation (OP),
coreference features (CR) and part of speech tags
(POS) as displayed in Figure 1. The transition model
is defined as Equation 4.
</bodyText>
<figureCaption confidence="0.998664">
Figure 1: Factorial HMM CR Model
</figureCaption>
<bodyText confidence="0.99994668">
The starting point for the hidden state at each time
step is the OP variable, which determines which
kind of referent operations will occur at the current
word. Its domain has three possible states: none,
new and old.
The none state indicates that the present state will
not generate a mention. All previous hidden state
values (the list of previous mentions) will be passed
deterministically (with probability 1) to the current
time step without any changes. The new state signi-
fies that there is a new mention in the present time
step. In this event, a new mention will be added to
the entity set, as represented by its set of feature val-
ues and position in the coreference table. The old
state indicates that there is a mention in the present
time state and that this mention refers back to some
antecedent mention. In such a case, the list of enti-
ties in the buffer will be reordered deterministically,
moving the currently mentioned entity to the top of
the list.
Notice that opt is defined to depend on opt−1
and post−1. This is sometimes called a switching
FHMM (Duh, 2005). This dependency can be use-
ful, for example, if opt−1 is new, in which case opt
has a higher probability of being none or old. If
</bodyText>
<figure confidence="0.989629837837838">
h!
h!&amp;quot;# op!&amp;quot;#=
copY
cr!&amp;quot;#
neu,fem
n!&amp;quot;#=
plu,sing
e!&amp;quot;#=
per,org
g!&amp;quot;#=
i!&amp;quot;#=
-,2
o!&amp;quot;#=loves
pos!&amp;quot;#=
VBZ
cr!
sing,plu
fem,neu
e!=
org,per
i!=
0,2
n!=
g!=
o!=them
op!=
old
pos!=
PRP
�h1..T def = argmax
hi..T
def
= argmax
hi..T
def
= argmax
hi..T
</figure>
<page confidence="0.981023">
1171
</page>
<bodyText confidence="0.999943384615385">
post−1 is a verb or preposition, opt has more proba-
bility of being old or new.
One may wonder why opt generates post, and
not the other way around. This model only roughly
models the process of (new and old) entity genera-
tion, and either direction of causality might be con-
sistent with a model of human entity generation,
but this direction of causality is chosen to represent
the effect of semantics (referents) generating syn-
tax (POS tags). In addition, this is a joint model in
which POS tagging and coreference resolution are
integrated together, so the best combination of those
hidden states will be computed in either case.
</bodyText>
<subsectionHeader confidence="0.999157">
2.3 Coreference Features
</subsectionHeader>
<bodyText confidence="0.999932375">
Coreference features for this model refer to features
that may help to identify co-referring entities.
In this paper, they mainly include index (I),
named entity type (E), number (N) and gender (G).
The index feature represents the order that a men-
tion was encountered relative to the other mentions
in the buffer. The latter three features are well
known and described elsewhere, and are not them-
selves intended as the contribution of this work. The
novel aspect of this part of the model is the fact that
the features are carried forward, updated after ev-
ery word, and essentially act as a discourse model.
The features are just a shorthand way of represent-
ing some well known essential aspects of a referent
(as pertains to anaphora resolution) in a discourse
model.
</bodyText>
<table confidence="0.993308">
Features Values
I positive integers from 1...n
G male, female, neutral, unknown
N singular, plural, unknown
E person, location, organization,
GPE, vehicle,
company, facility
</table>
<tableCaption confidence="0.999892">
Table 1: Coreference features stored with each mention.
</tableCaption>
<bodyText confidence="0.999948327272727">
Unlike discriminative approaches, generative
models like the FHMM described here do not have
access to all observations at once. This model must
then have a mechanism for jointly considering pro-
nouns in tandem with previous mentions, as well as
the features of those mentions that might be used to
find matches between pronouns and antecedents.
Further, higher order HMMs may contain more
accurate information about observation states. This
is especially true for coreference resolution because
pronouns often refer back to mentions that are far
away from the present state. In this case, we would
need to know information about mentions which are
at least two mentions before the present one. In
this sense, a higher order HMM may seem ideal
for coreference resolution. However, higher order
HMMs will quickly become intractable as the order
increases.
In order to overcome these limitations, two strate-
gies which have been discussed in the last section
are taken: First, a switching variable called OP is
designed (as discussed in last section); second, a
memory of recently mentioned entities is maintained
to store features of mentions and pass them forward
incrementally.
OP is intended to model the decision to use the
current word to introduce a new referent (new), refer
to an antecedent (old), or neither (none). The entity
buffer is intended to model the set of ‘activated’ en-
tities in the discourse – those which could plausibly
be referred to with a pronoun. These designs allow
similar benefits as longer dependencies of higher-
order HMMs but avoid the problem of intractability.
The number of mentions maintained must be limited
in order for the model to be tractable. Fortunately,
human short term memory faces effectively similar
limitations and thus pronouns usually refer back to
mentions not very far away.
Even so, the impact of the size of the buffer on
decoding time may be a concern. Since the buffer of
our system will carry forward a few previous groups
of coreference features plus op and pos, the compu-
tational complexity will be exorbitantly high if we
keep high beam size and meanwhile if each feature
interacts with others. Luckily, we have successfully
reduced the intractability to a workable system in
both speed and space with following methods. First,
we estimate the size of buffer with a simple count
of average distances between pronouns and their an-
tecedents in the corpus. It is found that about six is
enough for covering 99.2% of all pronouns.
Secondly, the coreference features we have used
have the nice property of being independent from
one another. One might expect English non-person
entities to almost always have neutral gender, and
</bodyText>
<page confidence="0.97625">
1172
</page>
<bodyText confidence="0.989104045454545">
thus be modeled as follows:
P(et, gt  |et−1, gt−1) = P(gt  |gt−1, et) &apos; P(et  |et−1)
However, a few considerations made us reconsider.
First, exceptions are found in the corpus. Personal
pronouns such as she or he are used to refer to coun-
try, regions, states or organizations. Second, existing
model files made by Bergsma (2005) include a large
number of non-neutral gender information for non-
person words. We employ these files for acquiring
gender information of unknown words. If we use
Equation 6, sparsity and complexity will increase.
Further, preliminary experiments have shown mod-
els using an independence assumption between gen-
der and personhood work better. Thus, we treat each
coreference feature as an independent event. Hence,
we can safely split coreference features into sepa-
rate parts. This way dramatically reduces the model
complexity. Thirdly, our HMM decoding uses the
Viterbi algorithm with A-star beam search.
The probability of the new state of the coreference
table P(crt  |crt−1, opt) is defined to be the product
of probabilities of the individual feature transitions.
</bodyText>
<equation confidence="0.81359425">
P(crt  |crt−1, opt) = P(it  |it−1, opt)&apos;
P(et  |et−1, it, opt)&apos;
P(gt  |gt−1, it, opt)&apos;
P(nt  |nt−1, it, opt)
</equation>
<bodyText confidence="0.999812333333333">
This supposes that the features are conditionally in-
dependent of each other given the index variable, the
operator and previous instance. Each feature only
depends on the operator and the corresponding fea-
ture at the previous state, with that set of features
re-ordered as specified by the index model.
</bodyText>
<subsectionHeader confidence="0.994388">
2.4 Feature Passing
</subsectionHeader>
<bodyText confidence="0.999916972972973">
Equation 7 is correct and complete, but in fact the
switching variable for operation type results in three
different cases which simplifies the calculation of
the transition probabilities for the coreference fea-
ture table.
Note the following observations about corefer-
ence features: it only needs a probabilistic model
when opt is old – in other words, only when the
model must choose between several antecedents to
re-refer to. gt, et and nt are deterministic except
when opt is new, when gender, entity type, and num-
ber information must be generated for the new entity
being introduced.
When opt is none, all coreference variables (en-
tity features) will be copied over from the previous
time step to the current time step, and the probabil-
ity of this transition is 1.0. When opt is new, it is
changed deterministically by adding the new entity
to the first position in the list and moving every other
entity down one position. If the list of entities is
full, the least recently mentioned entity will be dis-
carded. The values for the top of the feature lists
gt, et, and nt will then be generated from feature-
specific probability distributions estimated from the
training data. When opt is old, it will probabilisti-
cally select a value 1... n, for an entity list contain-
ing n items. The selected value will deterministi-
cally order the gt, nt and et lists. This distribution
is also estimated from training data, and takes into
account recency of mention. The shape of this dis-
tribution varies slightly depending on list size and
noise in the training data, but in general the probabil-
ity of a mention being selected is directly correlated
to how recently it was mentioned.
With this understanding, coreference table tran-
sition probabilities can be written in terms of only
their non-deterministic substate distributions:
</bodyText>
<equation confidence="0.99922975">
P(crt  |crt−1,old) = Pold(it  |it−1)&apos;
Preorder(et  |et−1, it)&apos;
Preorder(gt  |gt−1, it)&apos;
Preorder(nt  |nt−1, it)
</equation>
<bodyText confidence="0.9499542">
where the old model probabilistically selects the an-
tecedent and moves it to the top of the list as de-
scribed above, thus deciding how the reordering will
take place. The reorder model actually implements
the list reordering for each independent feature by
moving the feature value corresponding to the se-
lected entity in the index model to the top of that
feature’s list. The overall effect is simply the prob-
abilistic reordering of entities in a list, where each
entity is defined as a label and a set of features.
</bodyText>
<equation confidence="0.9997195">
P(crt  |crt−1, new) = Pnew(it  |it−1)&apos;
Pnew(gt  |gt−1)&apos;
Pnew(nt  |nt−1)&apos;
Pnew(et  |et−1)
</equation>
<bodyText confidence="0.999589">
where the new model probabilistically generates a
</bodyText>
<page confidence="0.979994">
1173
</page>
<bodyText confidence="0.999974366666667">
feature value based on the training data and puts it
at the top of the list, moves every other entity down
one position in the list, and removes the final item if
the list is already full. Each entity in i takes a value
from 1 to n for a list of size n. Each g can be one of
four values – male, female, neuter and unknown; n
one of three values – plural, singular and unknown
and e around eight values.
Note that post is used in both hidden states and
observation states. While it is not considered a
coreference feature as such, it can still play an im-
portant role in the resolving process. Basically, the
system tags parts of speech incrementally while si-
multaneously resolving pronoun anaphora. Mean-
while, post−1 and opt−1 will jointly generate opt.
This point has been discussed in Section 2.2.
Importantly, the pos model can help to imple-
ment binding principles (Chomsky, 1981). It is
applied when opt is old. In training, pronouns
are sub-categorised into personal pronouns, reflex-
ive and other-pronoun. We then define a vari-
able loct whose value is how far back in the list
of antecedents the current hypothesis must have
gone to arrive at the current value of it. If we
have the syntax annotations or parsed trees, then,
the part of speech model can be defined when
opt is old as Pbinding(post  |loct, sloct). For ex-
ample, if post ∈ reflexive, P(post  |loct, sloct)
where loct has smaller values (implying closer men-
tions to post) and sloct = subject should have
higher values since reflexive pronouns always re-
fer back to subjects within its governing domains.
This was what (Haghighi and Klein, 2009) did and
we did this in training with the REUTERS cor-
pus (Hasler et al., 2006) in which syntactic roles
are annotated. We finally switched to the ACE
corpus for the purpose of comparison with other
work. In the ACE corpus, no syntactic roles are
annotated. We did use the Stanford parser to ex-
tract syntactic roles from the ACE corpus. But
the result is largely affected by the parsing accu-
racy. Again, for a fair comparison, we extract simi-
lar features to Denis and Baldridge (2007), which is
the model we mainly compare with. They approx-
imate syntactic contexts with POS tags surround-
ing the pronoun. Inspired by this idea, we success-
fully represent binding features with POS tags be-
fore anaphors. Instead of using P(post  |loct, sloct),
we train P(post  |loct, posloct) which can play
the role of binding. For example, suppose the
buffer size is 6 and loct = 5, posloct = noun.
Then, P(post = reflexive  |loct, posloct) is usu-
ally higher than P(post =pronoun  |loct, posloct),
since the reflexive has a higher probability of refer-
ring back to the noun located in position 5 than the
pronoun.
In future work expanding to coreference resolu-
tion between any noun phrases we intend to inte-
grate syntax into this framework as a joint model of
coreference resolution and parsing.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="method">
3 Observation Model
</sectionHeader>
<bodyText confidence="0.9999695">
The observation model that generates an observed
state is defined as Equation 5. To expand that equa-
tion in detail, the observation state, the word, de-
pends on its part of speech and its coreference fea-
tures as well. Since FHMMs are generative, we can
say part of speech and coreference features generate
the word.
In actual implementation, the observed model will
be very sparse, since crt will be split into more vari-
ables according to how many coreference features it
is composed of. In order to avoid the sparsity, we
transform the equation with Bayes’ law as follows.
We define pos and cr to be independent of each
other, so we can further split the above equation as:
</bodyText>
<equation confidence="0.996567">
PiO(ot  |ht)def = P(ot) �P(post  |ot) �)P( (t  |ot) ′)
Eo′ P(o P(post o P crt |o
(12)
</equation>
<bodyText confidence="0.975907888888889">
where P(crt  |ot) = P(gt  |ot)P(nt  |ot)P(et  |ot) and
P(crt  |o′) = P(gt  |o′)P(nt  |o′)P(et  |o′).
This change transforms the FHMM to a hybrid
FHMM since the observation model no longer gen-
erates the data. Instead, the observation model gen-
erates hidden states, which is more a combination
of discriminative and generative approaches. This
way facilitates building likelihood model files of fea-
tures for given mentions from the training data. The
</bodyText>
<equation confidence="0.996898333333333">
P PiO (ot  |ht) = (p�o′)P(hl )) (10)
P(ot) · P(post, crt  |ot)
Eo′ P(o′)P(post, crt  |o′) (11)
</equation>
<page confidence="0.945631">
1174
</page>
<bodyText confidence="0.999926">
hidden state transition model represents prior proba-
bilities of coreference features associated with each
while this observation model factors in the probabil-
ity given a pronoun.
</bodyText>
<subsectionHeader confidence="0.997733">
3.1 Unknown Words Processing
</subsectionHeader>
<bodyText confidence="0.999949730769231">
If an observed word was not seen in training, the
distribution of its part of speech, gender, number and
entity type will be unknown. In this case, a special
unknown words model is used.
The part of speech of unknown words
P(post I wt = unkword) is estimated using a
decision tree model. This decision tree is built
by splitting letters in words from the end of the
word backward to its beginning. A POS tag is
assigned to the word after comparisons between
the morphological features of words trained from
the corpus and the strings concatenated from the
tree leaves are made. This method is about as
accurate as the approach described by Klein and
Manning (2003).
Next, a similar model is set up for estimating
P(nt I wt = unkword). Most English words have
regular plural forms, and even irregular words have
their patterns. Therefore, the morphological features
of English words can often be used to determine
whether a word is singular or plural.
Gender is irregular in English, so model-based
predictions are problematic. Instead, we follow
Bergsma and Lin (2005) to get the distribution of
gender from their gender/number data and then pre-
dict the gender for unknown words.
</bodyText>
<sectionHeader confidence="0.976936" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<subsectionHeader confidence="0.986364">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999562">
In this research, we used the ACE corpus (Phase 2) 1
for evaluation. The development of this corpus in-
volved two stages. The first stage is called EDT (en-
tity detection and tracking) while the second stage
is called RDC (relation detection and characteriza-
tion). All markables have named entity types such
as FACILITY, GPE (geopolitical entity), PERSON,
LOCATION, ORGANIZATION, PERSON, VEHI-
CLE and WEAPONS, which were annotated in the
first stage. In the second stage, relations between
</bodyText>
<footnote confidence="0.9916035">
1See http://projects.ldc.upenn.edu/ace/
annotation/previous/ for details on the corpus.
</footnote>
<bodyText confidence="0.999549875">
named entities were annotated. This corpus include
three parts, composed of different genres: newspa-
per texts (NPAPER), newswire texts (NWIRE) and
broadcasted news (BNEWS). Each of these is split
into a train part and a devtest part. For the train
part, there are 76, 130 and 217 articles in NPA-
PER, NWIRE and BNEWS respectively while for
the test part, there are 17, 29 and 51 articles respec-
tively. Though the number of articles are quite dif-
ferent for three genres, the total number of words are
almost the same. Namely, the length of NPAPER
is much longer than BNEWS (about 1200 words,
800 word and 500 words respectively for three gen-
res). The longer articles involve longer coreference
chains. Following the common practice, we used
the devtest material only for testing. Progress during
the development phase was estimated only by using
cross-validation on the training set for the BNEWS
section. In order to make comparisons with publica-
tions which used the same corpus, we make efforts
to set up identical conditions for our experiments.
The main point of comparison is Denis and
Baldridge (2007), which was similar in that it de-
scribed a new type of coreference resolver using
simple features.
Therefore, similar to their practice, we use all
forms of personal and possessive pronouns that were
annotated as ACE ”markables”. Namely, pronouns
associated with named entity types could be used in
this system. In experiments, we also used true ACE
mentions as they did. This means that pleonastics
and references to eventualities or to non-ACE enti-
ties are not included in our experiments either. In
all, 7263 referential pronouns in training data set
and 1866 in testing data set are found in all three
genres. They have results of three different systems:
SCC (single candidate classifier), TCC (twin candi-
date classifier) and RK (ranking). Besides the three
and our own system, we also report results of em-
Pronouns, which is an unsupervised system based
on a recently published paper (Charniak and Elsner,
2009). We select this unsupervised system for two
reasons. Firstly, emPronouns is a publicly available
system with high accuracy in pronoun resolution.
Secondly, it is necessary for us to demonstrate our
system has strong empirical superiority over unsu-
pervised ones. In testing, we also used the OPNLP
Named Entity Recognizer to tag the test corpus.
</bodyText>
<page confidence="0.980293">
1175
</page>
<bodyText confidence="0.9999031875">
During training, besides coreference annotation
itself, the part of speech, dependencies between
words and named entities, gender, number and index
are extracted using relative frequency estimation to
train models for the coreference resolution system.
Inputs for testing are the plain text and the trained
model files. The entity buffer used in these exper-
iments kept track of only the six most recent men-
tions. The result of this process is an annotation
of the headword of every noun phrase denoting it
as a mention. In addition, this system does not
do anaphoricity detection, so the antecedent oper-
ation for non-anaphora pronoun it is set to be none.
Finally, the system does not yet model cataphora,
about 10 cataphoric pronouns in the testing data
which are all counted as wrong.
</bodyText>
<sectionHeader confidence="0.849932" genericHeader="conclusions">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999767071428572">
The performance was evaluated using the ratio of
the number of correctly resolved anaphors over the
number of all anaphors as a success metrics. All the
standards are consistent with those defined in Char-
niak and Elsner (2009).
During development, several preliminary experi-
ments explored the effects of starting from a simple
baseline and adding more features. The BNEWS
corpus was employed in these development exper-
iments. The baseline only includes part of speech
tags, the index feature and and syntactic roles. Syn-
tactic roles are extracted from the parsing results
with Stanford parser. The success rate of this base-
line configuration is 0.48. This low accuracy is par-
tially due to the errors of automatic parsing. With
gender and number features added, the performance
jumped to 0.65. This shows that number and gen-
der agreements play an important role in pronoun
anaphora resolution. For a more standard compari-
son to other work, subsequent tests were performed
on the gold standard ACE corpus (using the model
as described with named entity features instead of
syntactic role features). As shown in Denis and
Baldridge (2007), they employ all features we use
except syntactic roles. In these experiments, the sys-
tem got better results as shown in Table 2.
The result of the first one is obtained by running
the publicly available system emPronouns2. It is a
</bodyText>
<footnote confidence="0.495261">
2the available system in fact only includes the testing part.
Thus, it may be unfair to compare emPronouns this way with
</footnote>
<table confidence="0.999765">
System BNEWS NPAPER NWIRE
emPronouns 58.5 64.5 60.6
SCC 62.2 70.7 68.3
TCC 68.6 74.7 71.1
RK 72.9 76.4 72.4
FHMM 74.9 79.4 74.5
</table>
<tableCaption confidence="0.894820666666667">
Table 2: Accuracy scores for emPronouns, the single-
candidate classifier (SCC), the twin-candidate classifier
(TCC), the ranker and FHMM
</tableCaption>
<bodyText confidence="0.993611675675675">
high-accuracy unsupervised system which reported
the best result in Charniak and Elsner (2009).
The results of the other three systems are those
reported by Denis and Baldridge (2007). As Table 2
shows, the FHMM system gets the highest average
results.
The emPronouns system got the lowest results
partially due to the reason that we only directly
run the existing system with its existing model files
without retraining. But the gap between its results
and results of our system is large. Thus, we may
still say that our system probably can do a better job
even if we train new models files for emPronouns
with ACE corpus.
With almost exactly identical settings, why does
our FHMM system get the highest average results?
The convincing reason is that FHMM is strongly in-
fluenced by the sequential dependencies. The rank-
ing approach ranks a set of mentions using a set of
features, and it also maintains the discourse model,
but it is not processing sequentially. The FHMM
system always maintain a set of mentions as well
as a first-order dependencies between part of speech
and operator. Therefore, context can be more fully
taken into consideration. This is the main reason that
the FHMM approach achieved better results than the
ranking approach.
From the result, one point we may notice is that
NPAPER usually obtains higher results than both
BNEWS and NWIRE for all systems while BNEWS
lower than other two genres. In last section, we
mention that articles in NPAPER are longer than
other genres and also have denser coreference chains
while articles in BENEWS are shorter and have
sparer chains. Then, it is not hard to understand
why results of NPAPER are better while those of
other systems.
</bodyText>
<page confidence="0.839778">
1176
</page>
<bodyText confidence="0.9993383125">
BNEWS are poorer. they, demands will be selected as the antecedent.
In Denis and Baldridge (2007), they also reported This may lead to the wrong choice if they in fact
new results with a window of 10 sentences for RK refers to Israelis. This may require better measures
model. All three genres obtained higher results than of referent salience than the “least recently used”
those when with shorter ones. They are 73.0, 77.6 heuristic currently implemented.
and 75.0 for BNEWS, NPAPER and NWIRE respec- Third, these results also show difficulty resolv-
tively. We can see that except the one for NWIRE, ing coordinate noun phrases due to the simplistic
the results are still poorer than our system. For representation of noun phrases in the input. Con-
NWIRE, the RK model got 0.5 higher. The average sider this sentence: President Barack Obama and
of the RK is 75.2 while that of the FHMM system is his wife Michelle Obama visited China last week.
76.3, which is still the best. They had a meeting with President Hu in Beijing.
Since the emPronoun system can output sample- In this example, the pronoun they corefers with the
level results, it is possible to do a paired Student’s noun phrase President Barack Obama and his wife
t-test. That test shows that the improvement of our Michelle Obama. The present model cannot repre-
system on all three genres is statistically significant sent both the larger noun phrase and its contained
(p &lt; 0.001). Unfortunately, the other systems only noun phrases. Since the noun phrase is a coordinate
report overall results so the same comparison was one that includes both noun phrases, the model can-
not so straightforward. not find a head word to represent it.
4.3 Error Analysis Finally, while the coreference feature annotations
After running the system on these documents, we of the ACE are valuable for learning feature mod-
checked which pronouns fail to catch their an- els, the model training may still give some mislead-
tecedents. There are a few general reasons for er- ing results. This is brought about by missing fea-
rors. tures in the training corpus and by the data sparsity.
First, pronouns which have antecedents very far We solved the problem with add-one smoothing and
away cannot be caught. Long-distance anaphora res- deleted interpolation in training models besides the
olution may pose a problem since the buffer size transformation in the generation order of the obser-
cannot be too long considering the complexity of vation model.
tracking a large number of mentions through time. 5 Conclusion and Future Work
During development, estimation of an acceptable This paper has presented a pronoun anaphora resolu-
size was attempted using the training data. It was tion system based on FHMMs. This generative sys-
found that a mention distance of fourteen would ac- tem incrementally resolves pronoun anaphora with
count for every case found in this corpus, though an entity buffer carrying forward mention features.
most cases fall well short of that distance. Future The system performs well and outperforms other
work will explore optimizations that will allow for available models. This shows that FHMMs and
larger or variable buffer sizes so that longer distance other time-series models may be a valuable model
anaphora can be detected. to resolve anaphora.
A second source of error is simple misjudgments Acknowledgments
when more than one candidate is waiting for selec- We would like to thank the authors and maintainers
tion. A simple case is that the system fails to distin- of ranker models and emPronouns. We also would
guish plural personal nouns and non-personal nouns like to thank the three anonymous reviewers. The
if both candidates are plural. This is not a problem final version is revised based on their valuable com-
for singular pronouns since gender features can tell ments. Thanks are extended to Shane Bergsma, who
whether pronouns are personal or not. Plural nouns provided us the gender and number data distribution.
in English do not have such distinctions, however. In addition, Professor Jeanette Gundel and our lab-
Consequently, demands and Israelis have the same mate Stephen Wu also gave us support in paper edit-
probability of being selected as the antecedents for ing and in theoretical discussion.
they, all else being equal. If demands is closer to
1177
</bodyText>
<sectionHeader confidence="0.996397" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904792207792">
S Bergsma. 2005. Automatic acquisition of gender
information for anaphora resolution. page 342353.
Springer.
Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Noam Chomsky. 1981. Lectures on government and
binding. Foris, Dordercht.
H.H. Clark and CJ Sengul. 1979. In search of refer-
ents for nouns and pronouns. Memory &amp; Cognition,
7(1):35–41.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In Proc. IJCAI.
Kevin Duh. 2005. Jointly labeling multiple sequences:
a factorial HMM approach. In ACL ’05: Proceedings
of the ACL Student Research Workshop, pages 19–24,
Ann Arbor, Michigan.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29:1–
31.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th annual meeting on Associ-
ation for Computational Linguistics, page 848.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152–1161. Association for Compu-
tational Linguistics.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385–393. Associa-
tion for Computational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167–1172. Citeseer.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan.
X Luo. 2005. On coreference resolution performance
metrics. pages 25–32. Association for Computational
Linguistics Morristown, NJ, USA.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web. Citeseer.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. BLLIP North American News Text, Complete.
Linguistic Data Consortium. LDC2008T13.
T.S. Morton. 2000. Coreference for NLP applications.
In Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 173–180.
Association for Computational Linguistics.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 640–
649. Association for Computational Linguistics.
US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-
tional Institute for Standards and Technology (NIST),
Gaithersburg, MDjonline, pages 2003–08.
L. Qiu, M.Y. Kan, and T.S. Chua. 2004. A public ref-
erence implementation of the rap anaphora resolution
algorithm. Arxiv preprint cs/0406031.
X. Yang, J. Su, G. Zhou, and C.L. Tan. 2004. Im-
proving pronoun resolution by incorporating corefer-
ential information of candidates. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 127. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.99529">
1178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871522">
<title confidence="0.9808895">A Pronoun Anaphora Resolution System based Factorial Hidden Markov Models</title>
<author confidence="0.999817">Dingcheng Li Tim Miller William Schuler</author>
<affiliation confidence="0.970873">University of Minnesota, University of Wisconsin The Ohio State University Twin Cities, Minnesosta Milwaukee, Wisconsin Columbus,</affiliation>
<email confidence="0.999188">lixxx345@umn.edutmill@cs.umn.eduschuler@ling.osu.edu</email>
<abstract confidence="0.997290142857143">This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bergsma</author>
</authors>
<title>Automatic acquisition of gender information for anaphora resolution.</title>
<date>2005</date>
<pages>342353</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="15914" citStr="Bergsma (2005)" startWordPosition="2573" endWordPosition="2574"> the corpus. It is found that about six is enough for covering 99.2% of all pronouns. Secondly, the coreference features we have used have the nice property of being independent from one another. One might expect English non-person entities to almost always have neutral gender, and 1172 thus be modeled as follows: P(et, gt |et−1, gt−1) = P(gt |gt−1, et) &apos; P(et |et−1) However, a few considerations made us reconsider. First, exceptions are found in the corpus. Personal pronouns such as she or he are used to refer to country, regions, states or organizations. Second, existing model files made by Bergsma (2005) include a large number of non-neutral gender information for nonperson words. We employ these files for acquiring gender information of unknown words. If we use Equation 6, sparsity and complexity will increase. Further, preliminary experiments have shown models using an independence assumption between gender and personhood work better. Thus, we treat each coreference feature as an independent event. Hence, we can safely split coreference features into separate parts. This way dramatically reduces the model complexity. Thirdly, our HMM decoding uses the Viterbi algorithm with A-star beam sear</context>
</contexts>
<marker>Bergsma, 2005</marker>
<rawString>S Bergsma. 2005. Automatic acquisition of gender information for anaphora resolution. page 342353. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>Em works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL-09),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="3644" citStr="Charniak and Elsner, 2009" startWordPosition="554" endWordPosition="557">ion from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreement constraints at the global level, and assigned salience only to pronouns. Another unsupervised generative model was recently presented to tackle only pronoun anaphora 1169 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics resolution (Charniak and Elsner, 2009). The While the framework described here can be exexpectation-maximization algorithm (EM) was ap- tended to deeper structural information, POS tags plied to learn parameters automatically from the alone are valuable as they can be used to incorpoparsed version of the North American News Cor- rate the binding features (described below). pus (McClosky et al., 2008). This model generates a Although the system described here is evaluated pronoun’s person, number and gender features along for pronoun resolution, the framework we describe with the governor of the pronoun and the syntactic can be ext</context>
<context position="27924" citStr="Charniak and Elsner, 2009" startWordPosition="4589" endWordPosition="4592"> used in this system. In experiments, we also used true ACE mentions as they did. This means that pleonastics and references to eventualities or to non-ACE entities are not included in our experiments either. In all, 7263 referential pronouns in training data set and 1866 in testing data set are found in all three genres. They have results of three different systems: SCC (single candidate classifier), TCC (twin candidate classifier) and RK (ranking). Besides the three and our own system, we also report results of emPronouns, which is an unsupervised system based on a recently published paper (Charniak and Elsner, 2009). We select this unsupervised system for two reasons. Firstly, emPronouns is a publicly available system with high accuracy in pronoun resolution. Secondly, it is necessary for us to demonstrate our system has strong empirical superiority over unsupervised ones. In testing, we also used the OPNLP Named Entity Recognizer to tag the test corpus. 1175 During training, besides coreference annotation itself, the part of speech, dependencies between words and named entities, gender, number and index are extracted using relative frequency estimation to train models for the coreference resolution syst</context>
<context position="29300" citStr="Charniak and Elsner (2009)" startWordPosition="4812" endWordPosition="4816">ent mentions. The result of this process is an annotation of the headword of every noun phrase denoting it as a mention. In addition, this system does not do anaphoricity detection, so the antecedent operation for non-anaphora pronoun it is set to be none. Finally, the system does not yet model cataphora, about 10 cataphoric pronouns in the testing data which are all counted as wrong. 4.2 Results The performance was evaluated using the ratio of the number of correctly resolved anaphors over the number of all anaphors as a success metrics. All the standards are consistent with those defined in Charniak and Elsner (2009). During development, several preliminary experiments explored the effects of starting from a simple baseline and adding more features. The BNEWS corpus was employed in these development experiments. The baseline only includes part of speech tags, the index feature and and syntactic roles. Syntactic roles are extracted from the parsing results with Stanford parser. The success rate of this baseline configuration is 0.48. This low accuracy is partially due to the errors of automatic parsing. With gender and number features added, the performance jumped to 0.65. This shows that number and gender</context>
<context position="30921" citStr="Charniak and Elsner (2009)" startWordPosition="5077" endWordPosition="5080">stem got better results as shown in Table 2. The result of the first one is obtained by running the publicly available system emPronouns2. It is a 2the available system in fact only includes the testing part. Thus, it may be unfair to compare emPronouns this way with System BNEWS NPAPER NWIRE emPronouns 58.5 64.5 60.6 SCC 62.2 70.7 68.3 TCC 68.6 74.7 71.1 RK 72.9 76.4 72.4 FHMM 74.9 79.4 74.5 Table 2: Accuracy scores for emPronouns, the singlecandidate classifier (SCC), the twin-candidate classifier (TCC), the ranker and FHMM high-accuracy unsupervised system which reported the best result in Charniak and Elsner (2009). The results of the other three systems are those reported by Denis and Baldridge (2007). As Table 2 shows, the FHMM system gets the highest average results. The emPronouns system got the lowest results partially due to the reason that we only directly run the existing system with its existing model files without retraining. But the gap between its results and results of our system is large. Thus, we may still say that our system probably can do a better job even if we train new models files for emPronouns with ACE corpus. With almost exactly identical settings, why does our FHMM system get t</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. Em works for pronoun anaphora resolution. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL-09), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Lectures on government and binding.</title>
<date>1981</date>
<location>Foris, Dordercht.</location>
<contexts>
<context position="20575" citStr="Chomsky, 1981" startWordPosition="3353" endWordPosition="3354">one of four values – male, female, neuter and unknown; n one of three values – plural, singular and unknown and e around eight values. Note that post is used in both hidden states and observation states. While it is not considered a coreference feature as such, it can still play an important role in the resolving process. Basically, the system tags parts of speech incrementally while simultaneously resolving pronoun anaphora. Meanwhile, post−1 and opt−1 will jointly generate opt. This point has been discussed in Section 2.2. Importantly, the pos model can help to implement binding principles (Chomsky, 1981). It is applied when opt is old. In training, pronouns are sub-categorised into personal pronouns, reflexive and other-pronoun. We then define a variable loct whose value is how far back in the list of antecedents the current hypothesis must have gone to arrive at the current value of it. If we have the syntax annotations or parsed trees, then, the part of speech model can be defined when opt is old as Pbinding(post |loct, sloct). For example, if post ∈ reflexive, P(post |loct, sloct) where loct has smaller values (implying closer mentions to post) and sloct = subject should have higher values</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Noam Chomsky. 1981. Lectures on government and binding. Foris, Dordercht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>CJ Sengul</author>
</authors>
<title>In search of referents for nouns and pronouns.</title>
<date>1979</date>
<journal>Memory &amp; Cognition,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="6415" citStr="Clark and Sengul (1979)" startWordPosition="986" endWordPosition="989">Models (FHMMs). In this paper, we present a supervised pro- Unlike the more commonly known Hidden Markov noun resolution system based on Factorial Hidden Model (HMM), in an FHMM the hidden state at Markov Models (FHMMs). This system is moti- each time step is expanded to contain more than one vated by human processing concerns, by operating random variable (as shown in Figure 1). This alincrementally and maintaining a limited short term lows for the use of more complex hidden states by memory for holding recently mentioned referents. taking advantage of conditional independence beAccording to Clark and Sengul (1979), anaphoric tween substates. This conditional independence aldefinite NPs are much faster retrieved if the an- lows complex hidden states to be learned with limtecedent of a pronoun is in immediately previous ited training data. sentence. Therefore, a limited short term memory 2.1 Factorial Hidden Markov Model should be good enough for resolving the majority of Factorial Hidden Markov Models are an extension pronouns. In order to construct an operable model, of HMMs (Ghahramani and Jordan, 1997). HMMs we also measured the average distance between pro- represent sequential data as a sequence of</context>
</contexts>
<marker>Clark, Sengul, 1979</marker>
<rawString>H.H. Clark and CJ Sengul. 1979. In search of referents for nouns and pronouns. Memory &amp; Cognition, 7(1):35–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>A ranking approach to pronoun resolution.</title>
<date>2007</date>
<booktitle>In Proc. IJCAI.</booktitle>
<contexts>
<context position="2640" citStr="Denis and Baldridge, 2007" startWordPosition="406" endWordPosition="409"> The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each other, there may be conflicting coreferences. Graph-partitioning methods attempt to reconcile pairwise scores into a final coherent clustering, but they are combinatorially harder to work with in discriminative approaches. One line of research aiming at overcoming the limitation of pairwise models is to learn a mentionranking model to rank preceding mentions for a given anaphor (Denis and Baldridge, 2007) This approach results in more coherent coreference chains. Recent years have also seen the revival of interest in generative models in both machine learning and natural language processing. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. In contrast to pairwise models, this fully generative model produces each mention from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreem</context>
<context position="21784" citStr="Denis and Baldridge (2007)" startWordPosition="3564" endWordPosition="3567">ave higher values since reflexive pronouns always refer back to subjects within its governing domains. This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al., 2006) in which syntactic roles are annotated. We finally switched to the ACE corpus for the purpose of comparison with other work. In the ACE corpus, no syntactic roles are annotated. We did use the Stanford parser to extract syntactic roles from the ACE corpus. But the result is largely affected by the parsing accuracy. Again, for a fair comparison, we extract similar features to Denis and Baldridge (2007), which is the model we mainly compare with. They approximate syntactic contexts with POS tags surrounding the pronoun. Inspired by this idea, we successfully represent binding features with POS tags before anaphors. Instead of using P(post |loct, sloct), we train P(post |loct, posloct) which can play the role of binding. For example, suppose the buffer size is 6 and loct = 5, posloct = noun. Then, P(post = reflexive |loct, posloct) is usually higher than P(post =pronoun |loct, posloct), since the reflexive has a higher probability of referring back to the noun located in position 5 than the p</context>
<context position="27008" citStr="Denis and Baldridge (2007)" startWordPosition="4439" endWordPosition="4442">number of words are almost the same. Namely, the length of NPAPER is much longer than BNEWS (about 1200 words, 800 word and 500 words respectively for three genres). The longer articles involve longer coreference chains. Following the common practice, we used the devtest material only for testing. Progress during the development phase was estimated only by using cross-validation on the training set for the BNEWS section. In order to make comparisons with publications which used the same corpus, we make efforts to set up identical conditions for our experiments. The main point of comparison is Denis and Baldridge (2007), which was similar in that it described a new type of coreference resolver using simple features. Therefore, similar to their practice, we use all forms of personal and possessive pronouns that were annotated as ACE ”markables”. Namely, pronouns associated with named entity types could be used in this system. In experiments, we also used true ACE mentions as they did. This means that pleonastics and references to eventualities or to non-ACE entities are not included in our experiments either. In all, 7263 referential pronouns in training data set and 1866 in testing data set are found in all </context>
<context position="30209" citStr="Denis and Baldridge (2007)" startWordPosition="4959" endWordPosition="4962"> roles. Syntactic roles are extracted from the parsing results with Stanford parser. The success rate of this baseline configuration is 0.48. This low accuracy is partially due to the errors of automatic parsing. With gender and number features added, the performance jumped to 0.65. This shows that number and gender agreements play an important role in pronoun anaphora resolution. For a more standard comparison to other work, subsequent tests were performed on the gold standard ACE corpus (using the model as described with named entity features instead of syntactic role features). As shown in Denis and Baldridge (2007), they employ all features we use except syntactic roles. In these experiments, the system got better results as shown in Table 2. The result of the first one is obtained by running the publicly available system emPronouns2. It is a 2the available system in fact only includes the testing part. Thus, it may be unfair to compare emPronouns this way with System BNEWS NPAPER NWIRE emPronouns 58.5 64.5 60.6 SCC 62.2 70.7 68.3 TCC 68.6 74.7 71.1 RK 72.9 76.4 72.4 FHMM 74.9 79.4 74.5 Table 2: Accuracy scores for emPronouns, the singlecandidate classifier (SCC), the twin-candidate classifier (TCC), th</context>
<context position="32629" citStr="Denis and Baldridge (2007)" startWordPosition="5367" endWordPosition="5370">that the FHMM approach achieved better results than the ranking approach. From the result, one point we may notice is that NPAPER usually obtains higher results than both BNEWS and NWIRE for all systems while BNEWS lower than other two genres. In last section, we mention that articles in NPAPER are longer than other genres and also have denser coreference chains while articles in BENEWS are shorter and have sparer chains. Then, it is not hard to understand why results of NPAPER are better while those of other systems. 1176 BNEWS are poorer. they, demands will be selected as the antecedent. In Denis and Baldridge (2007), they also reported This may lead to the wrong choice if they in fact new results with a window of 10 sentences for RK refers to Israelis. This may require better measures model. All three genres obtained higher results than of referent salience than the “least recently used” those when with shorter ones. They are 73.0, 77.6 heuristic currently implemented. and 75.0 for BNEWS, NPAPER and NWIRE respec- Third, these results also show difficulty resolvtively. We can see that except the one for NWIRE, ing coordinate noun phrases due to the simplistic the results are still poorer than our system. </context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. A ranking approach to pronoun resolution. In Proc. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Jointly labeling multiple sequences: a factorial HMM approach.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>pages</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="10867" citStr="Duh, 2005" startWordPosition="1732" endWordPosition="1733">ate signifies that there is a new mention in the present time step. In this event, a new mention will be added to the entity set, as represented by its set of feature values and position in the coreference table. The old state indicates that there is a mention in the present time state and that this mention refers back to some antecedent mention. In such a case, the list of entities in the buffer will be reordered deterministically, moving the currently mentioned entity to the top of the list. Notice that opt is defined to depend on opt−1 and post−1. This is sometimes called a switching FHMM (Duh, 2005). This dependency can be useful, for example, if opt−1 is new, in which case opt has a higher probability of being none or old. If h! h!&amp;quot;# op!&amp;quot;#= copY cr!&amp;quot;# neu,fem n!&amp;quot;#= plu,sing e!&amp;quot;#= per,org g!&amp;quot;#= i!&amp;quot;#= -,2 o!&amp;quot;#=loves pos!&amp;quot;#= VBZ cr! sing,plu fem,neu e!= org,per i!= 0,2 n!= g!= o!=them op!= old pos!= PRP �h1..T def = argmax hi..T def = argmax hi..T def = argmax hi..T 1171 post−1 is a verb or preposition, opt has more probability of being old or new. One may wonder why opt generates post, and not the other way around. This model only roughly models the process of (new and old) entity generat</context>
</contexts>
<marker>Duh, 2005</marker>
<rawString>Kevin Duh. 2005. Jointly labeling multiple sequences: a factorial HMM approach. In ACL ’05: Proceedings of the ACL Student Research Workshop, pages 19–24, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Factorial hidden markov models.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<volume>29</volume>
<pages>31</pages>
<contexts>
<context position="6915" citStr="Ghahramani and Jordan, 1997" startWordPosition="1064" endWordPosition="1067">ry for holding recently mentioned referents. taking advantage of conditional independence beAccording to Clark and Sengul (1979), anaphoric tween substates. This conditional independence aldefinite NPs are much faster retrieved if the an- lows complex hidden states to be learned with limtecedent of a pronoun is in immediately previous ited training data. sentence. Therefore, a limited short term memory 2.1 Factorial Hidden Markov Model should be good enough for resolving the majority of Factorial Hidden Markov Models are an extension pronouns. In order to construct an operable model, of HMMs (Ghahramani and Jordan, 1997). HMMs we also measured the average distance between pro- represent sequential data as a sequence of hidden nouns and their antecedents as discussed in next sec- states generating observation states (words in this tions and used distances as important salience fea- case) at corresponding time steps t. A most likely tures in the model. sequence of hidden states can then be hypothesized Second, like Morton (2000), the current sys- given any sequence of observed states, using Bayes tem essentially uses prior information as a dis- Law (Equation 2) and Markov independence ascourse model with a time</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I. Jordan. 1997. Factorial hidden markov models. Machine Learning, 29:1– 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>848</pages>
<contexts>
<context position="2856" citStr="Haghighi and Klein (2007)" startWordPosition="441" endWordPosition="444"> other, there may be conflicting coreferences. Graph-partitioning methods attempt to reconcile pairwise scores into a final coherent clustering, but they are combinatorially harder to work with in discriminative approaches. One line of research aiming at overcoming the limitation of pairwise models is to learn a mentionranking model to rank preceding mentions for a given anaphor (Denis and Baldridge, 2007) This approach results in more coherent coreference chains. Recent years have also seen the revival of interest in generative models in both machine learning and natural language processing. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. In contrast to pairwise models, this fully generative model produces each mention from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreement constraints at the global level, and assigned salience only to pronouns. Another unsupervised generative model was recently presented to tackle only pronoun anaphora 1169 Proceedings of the 49th Annual Meeting of</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>A. Haghighi and D. Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In Proceedings of the 45th annual meeting on Association for Computational Linguistics, page 848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21301" citStr="Haghighi and Klein, 2009" startWordPosition="3477" endWordPosition="3480">flexive and other-pronoun. We then define a variable loct whose value is how far back in the list of antecedents the current hypothesis must have gone to arrive at the current value of it. If we have the syntax annotations or parsed trees, then, the part of speech model can be defined when opt is old as Pbinding(post |loct, sloct). For example, if post ∈ reflexive, P(post |loct, sloct) where loct has smaller values (implying closer mentions to post) and sloct = subject should have higher values since reflexive pronouns always refer back to subjects within its governing domains. This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al., 2006) in which syntactic roles are annotated. We finally switched to the ACE corpus for the purpose of comparison with other work. In the ACE corpus, no syntactic roles are annotated. We did use the Stanford parser to extract syntactic roles from the ACE corpus. But the result is largely affected by the parsing accuracy. Again, for a fair comparison, we extract similar features to Denis and Baldridge (2007), which is the model we mainly compare with. They approximate syntactic contexts with POS tags surrounding the pronou</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>A. Haghighi and D. Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1152–1161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4716" citStr="Haghighi and Klein (2010)" startWordPosition="718" endWordPosition="721"> person, number and gender features along for pronoun resolution, the framework we describe with the governor of the pronoun and the syntactic can be extended to more general coreference resolurelation between the pronoun and the governor. This tion in a fairly straightforward manner. Further, as inference process allows the system to keep track of in other HMM-based systems, the system can be eimultiple hypotheses through time, including multi- ther supervised or unsupervised. But extensions to ple different possible histories of the discourse. unsupervised learning are left for future work. Haghighi and Klein (2010) improved their non- The final results are compared with a few superparametric model by sharing lexical statistics at the vised systems as the mention-ranking model (Delevel of abstract entity types. Consequently, their nis and Baldridge, 2007) and systems compared in model substantially reduces semantic compatibility their paper, and Charniak and Elsner’s (2009) unsuerrors. They report the best results to date on the pervised system, emPronouns. The FHMM-based complete end-to-end coreference task. Further, this pronoun resolution system does a better job than the model functions in an online </context>
<context position="7832" citStr="Haghighi and Klein, 2010" startWordPosition="1209" endWordPosition="1212">e steps t. A most likely tures in the model. sequence of hidden states can then be hypothesized Second, like Morton (2000), the current sys- given any sequence of observed states, using Bayes tem essentially uses prior information as a dis- Law (Equation 2) and Markov independence ascourse model with a time-series manner, using a sumptions (Equation 3) to define a full probability as dynamic programming inference algorithm. Third, the product of a Transition Model (OT) prior probthe FHMM described here is an integrated system, ability and an Observation Model (Op) likelihood in contrast with (Haghighi and Klein, 2010). The model generates part of speech tags as simple structural information, as well as related semantic information at each time step or word-by-word step. 1170 probability. P(h1..T |o1..T) (1) P(h1..T) · P(o1..T |h1..T) (2) T PΘT (ht |ht−1) · PΘO(ot |ht) H (3) t=1 For a simple HMM, the hidden state corresponding to each observation state only involves one variable. An FHMM contains more than one hidden variable in the hidden state. These hidden substates are usually layered processes that jointly generate the evidence. In the model described here, the substates are also coupled to allow inter</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>A. Haghighi and D. Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hasler</author>
<author>C Orasan</author>
<author>K Naumann</author>
</authors>
<title>NPs for events: Experiments in coreference annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th edition of the International Conference on Language Resources and Evaluation (LREC2006),</booktitle>
<pages>1167--1172</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="21379" citStr="Hasler et al., 2006" startWordPosition="3493" endWordPosition="3496">ck in the list of antecedents the current hypothesis must have gone to arrive at the current value of it. If we have the syntax annotations or parsed trees, then, the part of speech model can be defined when opt is old as Pbinding(post |loct, sloct). For example, if post ∈ reflexive, P(post |loct, sloct) where loct has smaller values (implying closer mentions to post) and sloct = subject should have higher values since reflexive pronouns always refer back to subjects within its governing domains. This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al., 2006) in which syntactic roles are annotated. We finally switched to the ACE corpus for the purpose of comparison with other work. In the ACE corpus, no syntactic roles are annotated. We did use the Stanford parser to extract syntactic roles from the ACE corpus. But the result is largely affected by the parsing accuracy. Again, for a fair comparison, we extract similar features to Denis and Baldridge (2007), which is the model we mainly compare with. They approximate syntactic contexts with POS tags surrounding the pronoun. Inspired by this idea, we successfully represent binding features with POS </context>
</contexts>
<marker>Hasler, Orasan, Naumann, 2006</marker>
<rawString>L. Hasler, C. Orasan, and K. Naumann. 2006. NPs for events: Experiments in coreference annotation. In Proceedings of the 5th edition of the International Conference on Language Resources and Evaluation (LREC2006), pages 1167–1172. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="24761" citStr="Klein and Manning (2003)" startWordPosition="4077" endWordPosition="4080">ning, the distribution of its part of speech, gender, number and entity type will be unknown. In this case, a special unknown words model is used. The part of speech of unknown words P(post I wt = unkword) is estimated using a decision tree model. This decision tree is built by splitting letters in words from the end of the word backward to its beginning. A POS tag is assigned to the word after comparisons between the morphological features of words trained from the corpus and the strings concatenated from the tree leaves are made. This method is about as accurate as the approach described by Klein and Manning (2003). Next, a similar model is set up for estimating P(nt I wt = unkword). Most English words have regular plural forms, and even irregular words have their patterns. Therefore, the morphological features of English words can often be used to determine whether a word is singular or plural. Gender is irregular in English, so model-based predictions are problematic. Instead, we follow Bergsma and Lin (2005) to get the distribution of gender from their gender/number data and then predict the gender for unknown words. 4 Evaluation and Discussion 4.1 Experimental Setup In this research, we used the ACE</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>Association for Computational Linguistics</booktitle>
<pages>25--32</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5625" citStr="Luo (2005)" startWordPosition="858" endWordPosition="859">mantic compatibility their paper, and Charniak and Elsner’s (2009) unsuerrors. They report the best results to date on the pervised system, emPronouns. The FHMM-based complete end-to-end coreference task. Further, this pronoun resolution system does a better job than the model functions in an online setting at mention level. global ranking technique and other approaches. This Namely, the system identifies mentions from a parse is a promising start for this novel FHMM-based protree and resolves resolution with a left-to-right se- noun resolution system. quential beam search. This is similar to Luo (2005) 2 Model Description where a Bell tree is used to score and store the This work is based on a graphical model framework searching path. called Factorial Hidden Markov Models (FHMMs). In this paper, we present a supervised pro- Unlike the more commonly known Hidden Markov noun resolution system based on Factorial Hidden Model (HMM), in an FHMM the hidden state at Markov Models (FHMMs). This system is moti- each time step is expanded to contain more than one vated by human processing concerns, by operating random variable (as shown in Figure 1). This alincrementally and maintaining a limited sho</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X Luo. 2005. On coreference resolution performance metrics. pages 25–32. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In IJCAI Workshop on Information Integration on the Web. Citeseer.</booktitle>
<contexts>
<context position="1793" citStr="McCallum and Wellner, 2003" startWordPosition="272" endWordPosition="275">ology used in the Automatic Context Extraction (ACE) program (NIST, 2003), these expressions are called mentions. Each mention is a reference to some entity in the domain of discourse. Mentions usually fall into three categories – proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). There is a great deal of related work on this subject, so the descriptions of other systems below are those which are most related or which the current model has drawn insight from. Pairwise models (Yang et al., 2004; Qiu et al., 2004) and graph-partitioning methods (McCallum and Wellner, 2003) decompose the task into a collection of pairwise or mention set coreference decisions. Decisions for each pair or each group of mentions are based on probabilities of features extracted by discriminative learning models. The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each other, there may be conflicting coreferences. Graph-partitioning methods attempt to reconcile pairwise scores into a final coherent clustering, but they are comb</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In IJCAI Workshop on Information Integration on the Web. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<date>2008</date>
<journal>BLLIP North American News Text, Complete. Linguistic Data Consortium. LDC2008T13.</journal>
<contexts>
<context position="4009" citStr="McClosky et al., 2008" startWordPosition="611" endWordPosition="614"> only pronoun anaphora 1169 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics resolution (Charniak and Elsner, 2009). The While the framework described here can be exexpectation-maximization algorithm (EM) was ap- tended to deeper structural information, POS tags plied to learn parameters automatically from the alone are valuable as they can be used to incorpoparsed version of the North American News Cor- rate the binding features (described below). pus (McClosky et al., 2008). This model generates a Although the system described here is evaluated pronoun’s person, number and gender features along for pronoun resolution, the framework we describe with the governor of the pronoun and the syntactic can be extended to more general coreference resolurelation between the pronoun and the governor. This tion in a fairly straightforward manner. Further, as inference process allows the system to keep track of in other HMM-based systems, the system can be eimultiple hypotheses through time, including multi- ther supervised or unsupervised. But extensions to ple different pos</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2008</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2008. BLLIP North American News Text, Complete. Linguistic Data Consortium. LDC2008T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Morton</author>
</authors>
<title>Coreference for NLP applications.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7329" citStr="Morton (2000)" startWordPosition="1132" endWordPosition="1133">rkov Model should be good enough for resolving the majority of Factorial Hidden Markov Models are an extension pronouns. In order to construct an operable model, of HMMs (Ghahramani and Jordan, 1997). HMMs we also measured the average distance between pro- represent sequential data as a sequence of hidden nouns and their antecedents as discussed in next sec- states generating observation states (words in this tions and used distances as important salience fea- case) at corresponding time steps t. A most likely tures in the model. sequence of hidden states can then be hypothesized Second, like Morton (2000), the current sys- given any sequence of observed states, using Bayes tem essentially uses prior information as a dis- Law (Equation 2) and Markov independence ascourse model with a time-series manner, using a sumptions (Equation 3) to define a full probability as dynamic programming inference algorithm. Third, the product of a Transition Model (OT) prior probthe FHMM described here is an integrated system, ability and an Observation Model (Op) likelihood in contrast with (Haghighi and Klein, 2010). The model generates part of speech tags as simple structural information, as well as related se</context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>T.S. Morton. 2000. Coreference for NLP applications. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>640--649</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3107" citStr="Ng (2008)" startWordPosition="481" endWordPosition="482">he limitation of pairwise models is to learn a mentionranking model to rank preceding mentions for a given anaphor (Denis and Baldridge, 2007) This approach results in more coherent coreference chains. Recent years have also seen the revival of interest in generative models in both machine learning and natural language processing. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. In contrast to pairwise models, this fully generative model produces each mention from a combination of global entity properties and local attentional state. Ng (2008) did similar work using the same unsupervised generative model, but relaxed head generation as head-index generation, enforced agreement constraints at the global level, and assigned salience only to pronouns. Another unsupervised generative model was recently presented to tackle only pronoun anaphora 1169 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics resolution (Charniak and Elsner, 2009). The While the framework described here can be exexpectation-m</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>V. Ng. 2008. Unsupervised models for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 640– 649. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>US NIST</author>
</authors>
<date>2003</date>
<booktitle>The ACE 2003 Evaluation Plan. US National Institute for Standards and Technology (NIST),</booktitle>
<pages>2003--08</pages>
<location>Gaithersburg, MDjonline,</location>
<contexts>
<context position="1239" citStr="NIST, 2003" startWordPosition="185" endWordPosition="186">dent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance. 1 Introduction Pronoun anaphora resolution is the task of finding the correct antecedent for a given pronominal anaphor in a document. It is a subtask of coreference resolution, which is the process of determining whether two or more linguistic expressions in a document refer to the same entity. Adopting terminology used in the Automatic Context Extraction (ACE) program (NIST, 2003), these expressions are called mentions. Each mention is a reference to some entity in the domain of discourse. Mentions usually fall into three categories – proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). There is a great deal of related work on this subject, so the descriptions of other systems below are those which are most related or which the current model has drawn insight from. Pairwise models (Yang et al., 2004; Qiu et al., 2004) and graph-partitioning methods (McCallum and Wellner, 2003) decompose the task into a collection of pairw</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>US NIST. 2003. The ACE 2003 Evaluation Plan. US National Institute for Standards and Technology (NIST), Gaithersburg, MDjonline, pages 2003–08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>A public reference implementation of the rap anaphora resolution algorithm. Arxiv preprint cs/0406031.</title>
<date>2004</date>
<contexts>
<context position="1733" citStr="Qiu et al., 2004" startWordPosition="265" endWordPosition="268">document refer to the same entity. Adopting terminology used in the Automatic Context Extraction (ACE) program (NIST, 2003), these expressions are called mentions. Each mention is a reference to some entity in the domain of discourse. Mentions usually fall into three categories – proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). There is a great deal of related work on this subject, so the descriptions of other systems below are those which are most related or which the current model has drawn insight from. Pairwise models (Yang et al., 2004; Qiu et al., 2004) and graph-partitioning methods (McCallum and Wellner, 2003) decompose the task into a collection of pairwise or mention set coreference decisions. Decisions for each pair or each group of mentions are based on probabilities of features extracted by discriminative learning models. The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each other, there may be conflicting coreferences. Graph-partitioning methods attempt to reconcile pairwis</context>
</contexts>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>L. Qiu, M.Y. Kan, and T.S. Chua. 2004. A public reference implementation of the rap anaphora resolution algorithm. Arxiv preprint cs/0406031.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C L Tan</author>
</authors>
<title>Improving pronoun resolution by incorporating coreferential information of candidates.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1714" citStr="Yang et al., 2004" startWordPosition="261" endWordPosition="264">c expressions in a document refer to the same entity. Adopting terminology used in the Automatic Context Extraction (ACE) program (NIST, 2003), these expressions are called mentions. Each mention is a reference to some entity in the domain of discourse. Mentions usually fall into three categories – proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). There is a great deal of related work on this subject, so the descriptions of other systems below are those which are most related or which the current model has drawn insight from. Pairwise models (Yang et al., 2004; Qiu et al., 2004) and graph-partitioning methods (McCallum and Wellner, 2003) decompose the task into a collection of pairwise or mention set coreference decisions. Decisions for each pair or each group of mentions are based on probabilities of features extracted by discriminative learning models. The aforementioned approaches have proven to be fruitful; however, there are some notable problems. Pairwise modeling may fail to produce coherent partitions. That is, if we link results of pairwise decisions to each other, there may be conflicting coreferences. Graph-partitioning methods attempt t</context>
</contexts>
<marker>Yang, Su, Zhou, Tan, 2004</marker>
<rawString>X. Yang, J. Su, G. Zhou, and C.L. Tan. 2004. Improving pronoun resolution by incorporating coreferential information of candidates. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 127. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>