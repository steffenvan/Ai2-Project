<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985977">
Incremental Syntactic Language Models for Phrase-based Translation
</title>
<author confidence="0.988218">
Lane Schwartz
</author>
<affiliation confidence="0.84736">
Air Force Research Laboratory
Wright-Patterson AFB, OH USA
</affiliation>
<email confidence="0.955878">
lane.schwartz@wpafb.af.mil
</email>
<author confidence="0.997236">
William Schuler
</author>
<affiliation confidence="0.999032">
Ohio State University
</affiliation>
<address confidence="0.69184">
Columbus, OH USA
</address>
<email confidence="0.998454">
schuler@ling.ohio-state.edu
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820333333333">
This paper describes a novel technique for in-
corporating syntactic knowledge into phrase-
based machine translation through incremen-
tal syntactic parsing. Bottom-up and top-
down parsers typically require a completed
string as input. This requirement makes it dif-
ficult to incorporate them into phrase-based
translation, which generates partial hypothe-
sized translations from left-to-right. Incre-
mental syntactic language models score sen-
tences in a similar left-to-right fashion, and are
therefore a good mechanism for incorporat-
ing syntax into phrase-based translation. We
give a formal definition of one such linear-
time syntactic language model, detail its re-
lation to phrase-based decoding, and integrate
the model with the Moses phrase-based trans-
lation system. We present empirical results
on a constrained Urdu-English translation task
that demonstrate a significant BLEU score im-
provement and a large decrease in perplexity.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994375">
Early work in statistical machine translation viewed
translation as a noisy channel process comprised of
a translation model, which functioned to posit ad-
equate translations of source language words, and
a target language model, which guided the fluency
of generated target language strings (Brown et al.,
</bodyText>
<footnote confidence="0.949307571428571">
This research was supported by NSF CAREER/PECASE
award 0447685, NSF grant IIS-0713448, and the European
Commission through the EuroMatrixPlus project. Opinions, in-
terpretations, conclusions, and recommendations are those of
the authors and are not necessarily endorsed by the sponsors or
the United States Air Force. Cleared for public release (Case
Number 88ABW-2010-6489) on 10 Dec 2010.
</footnote>
<note confidence="0.73516">
Chris Callison-Burch
Johns Hopkins University
Baltimore, MD USA
</note>
<email confidence="0.855988">
ccb@cs.jhu.edu
</email>
<author confidence="0.816949">
Stephen Wu
</author>
<sectionHeader confidence="0.6933655" genericHeader="introduction">
Mayo Clinic
Rochester, MN USA
</sectionHeader>
<email confidence="0.868825">
wu.stephen@mayo.edu
</email>
<bodyText confidence="0.999448137931034">
1990). Drawing on earlier successes in speech
recognition, research in statistical machine trans-
lation has effectively used n-gram word sequence
models as language models.
Modern phrase-based translation using large scale
n-gram language models generally performs well
in terms of lexical choice, but still often produces
ungrammatical output. Syntactic parsing may help
produce more grammatical output by better model-
ing structural relationships and long-distance depen-
dencies. Bottom-up and top-down parsers typically
require a completed string as input; this requirement
makes it difficult to incorporate these parsers into
phrase-based translation, which generates hypothe-
sized translations incrementally, from left-to-right.1
As a workaround, parsers can rerank the translated
output of translation systems (Och et al., 2004).
On the other hand, incremental parsers (Roark,
2001; Henderson, 2004; Schuler et al., 2010; Huang
and Sagae, 2010) process input in a straightforward
left-to-right manner. We observe that incremental
parsers, used as structured language models, pro-
vide an appropriate algorithmic match to incremen-
tal phrase-based decoding. We directly integrate in-
cremental syntactic parsing into phrase-based trans-
lation. This approach re-exerts the role of the lan-
guage model as a mechanism for encouraging syn-
tactically fluent translations.
The contributions of this work are as follows:
</bodyText>
<listItem confidence="0.999011666666667">
• A novel method for integrating syntactic LMs
into phrase-based translation (§3)
• A formal definition of an incremental parser for
</listItem>
<footnote confidence="0.986369333333333">
1While not all languages are written left-to-right, we will
refer to incremental processing which proceeds from the begin-
ning of a sentence as left-to-right.
</footnote>
<page confidence="0.89486">
620
</page>
<note confidence="0.985299">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.8109258">
statistical MT that can run in linear-time (§4)
• Integration with Moses (§5) along with empiri-
cal results for perplexity and significant transla-
tion score improvement on a constrained Urdu-
English task (§6)
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="method">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999983548780488">
Neither phrase-based (Koehn et al., 2003) nor hierar-
chical phrase-based translation (Chiang, 2005) take
explicit advantage of the syntactic structure of either
source or target language. The translation models in
these techniques define phrases as contiguous word
sequences (with gaps allowed in the case of hierar-
chical phrases) which may or may not correspond
to any linguistic constituent. Early work in statisti-
cal phrase-based translation considered whether re-
stricting translation models to use only syntactically
well-formed constituents might improve translation
quality (Koehn et al., 2003) but found such restric-
tions failed to improve translation quality.
Significant research has examined the extent to
which syntax can be usefully incorporated into sta-
tistical tree-based translation models: string-to-tree
(Yamada and Knight, 2001; Gildea, 2003; Imamura
et al., 2004; Galley et al., 2004; Graehl and Knight,
2004; Melamed, 2004; Galley et al., 2006; Huang
et al., 2006; Shen et al., 2008), tree-to-string (Liu
et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi
and Huang, 2008; Huang and Mi, 2010), tree-to-tree
(Abeill´e et al., 1990; Shieber and Schabes, 1990;
Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan
et al., 2006; Nesson et al., 2006; Zhang et al., 2007;
DeNeefe et al., 2007; DeNeefe and Knight, 2009;
Liu et al., 2009; Chiang, 2010), and treelet (Ding
and Palmer, 2005; Quirk et al., 2005) techniques
use syntactic information to inform the translation
model. Recent work has shown that parsing-based
machine translation using syntax-augmented (Zoll-
mann and Venugopal, 2006) hierarchical translation
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al., 2009). In
contrast to the above tree-based translation models,
our approach maintains a standard (non-syntactic)
phrase-based translation model. Instead, we incor-
porate syntax into the language model.
Traditional approaches to language models in
speech recognition and statistical machine transla-
tion focus on the use of n-grams, which provide a
simple finite-state model approximation of the tar-
get language. Chelba and Jelinek (1998) proposed
that syntactic structure could be used as an alterna-
tive technique in language modeling. This insight
has been explored in the context of speech recogni-
tion (Chelba and Jelinek, 2000; Collins et al., 2005).
Hassan et al. (2007) and Birch et al. (2007) use
supertag n-gram LMs. Syntactic language models
have also been explored with tree-based translation
models. Charniak et al. (2003) use syntactic lan-
guage models to rescore the output of a tree-based
translation system. Post and Gildea (2008) investi-
gate the integration of parsers as syntactic language
models during binary bracketing transduction trans-
lation (Wu, 1997); under these conditions, both syn-
tactic phrase-structure and dependency parsing lan-
guage models were found to improve oracle-best
translations, but did not improve actual translation
results. Post and Gildea (2009) use tree substitution
grammar parsing for language modeling, but do not
use this language model in a translation system. Our
work, in contrast to the above approaches, explores
the use of incremental syntactic language models in
conjunction with phrase-based translation models.
Our syntactic language model fits into the fam-
ily of linear-time dynamic programming parsers de-
scribed in (Huang and Sagae, 2010). Like (Galley
and Manning, 2009) our work implements an in-
cremental syntactic language model; our approach
differs by calculating syntactic LM scores over all
available phrase-structure parses at each hypothesis
instead of the 1-best dependency parse.
The syntax-driven reordering model of Ge (2010)
uses syntax-driven features to influence word order
within standard phrase-based translation. The syn-
tactic cohesion features of Cherry (2008) encour-
ages the use of syntactically well-formed translation
phrases. These approaches are fully orthogonal to
our proposed incremental syntactic language model,
and could be applied in concert with our work.
</bodyText>
<sectionHeader confidence="0.8765585" genericHeader="method">
3 Parser as Syntactic Language Model in
Phrase-Based Translation
</sectionHeader>
<bodyText confidence="0.9588905">
Parsing is the task of selecting the representation &apos;r
(typically a tree) that best models the structure of
</bodyText>
<page confidence="0.990908">
621
</page>
<figure confidence="0.9997436">
➀➁➂➃➄➅➆
(s)
�τ0
(s) president
➀➋➂➃➄➅➆
➊➁➂➃➄➅➆
➊➁➂➃➄➅➆
(s) that
(s) the
�τ13
�τ1a
�τ11
. . .
president Friday
that president
the president
➀➋➂➍➄➅➆
➊➋➂➃➄➅➆
➊➋➂➃➄➅➆
�τ23
�τ2a
�τ21
. . .
president meets
Obama met
➊➋➌➃➄➅➆
➊➋➌➃➄➅➆
�τ3a
f31
. . .
</figure>
<figureCaption confidence="0.999526">
Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
</figureCaption>
<bodyText confidence="0.909276222222222">
sentence Der Pr¨asident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-
gram state, and syntactic language model state fth. Hypothesis combination is also shown, indicating
where lattice paths with identical n-gram histories converge. We use the English translation The president
meets the board on Friday as a running example throughout all Figures.
sentence e, out of all such possible representations
τ. This set of representations may be all phrase
structure trees or all dependency trees allowed by
the parsing model. Typically, tree T� is taken to be:
</bodyText>
<equation confidence="0.998859">
T� = argmax P(T  |e) (1)
T
</equation>
<bodyText confidence="0.99925725">
We define a syntactic language model P(e) based
on the total probability mass over all possible trees
for string e. This is shown in Equation 2 and decom-
posed in Equation 3.
</bodyText>
<equation confidence="0.903561">
P(e) = � P(T, e) (2)
TEτ
P(e) = � P(e  |T)P(T) (3)
TEτ
</equation>
<subsectionHeader confidence="0.994109">
3.1 Incremental syntactic language model
</subsectionHeader>
<bodyText confidence="0.9986852">
An incremental parser processes each token of in-
put sequentially from the beginning of a sentence to
the end, rather than processing input in a top-down
(Earley, 1968) or bottom-up (Cocke and Schwartz,
1970; Kasami, 1965; Younger, 1967) fashion. After
processing the tth token in string e, an incremen-
tal parser has some internal representation of possi-
ble hypothesized (incomplete) trees, τt. The syntac-
tic language model probability of a partial sentence
e1...et is defined:
</bodyText>
<equation confidence="0.9645685">
�P(e1...et) = P(e1...et |T)P(T) (4)
TEτt
</equation>
<bodyText confidence="0.999946666666667">
In practice, a parser may constrain the set of trees
under consideration to ft, that subset of analyses or
partial analyses that remains after any pruning is per-
formed. An incremental syntactic language model
can then be defined by a probability mass function
(Equation 5) and a transition function 6 (Equation
6). The role of 6 is explained in §3.3 below. Any
parser which implements these two functions can
serve as a syntactic language model.
</bodyText>
<equation confidence="0.99937">
�P(e1...et) ~ P(Tt) = P(e1...et  |T)P(T) (5)
TE-Ft
6(et,�τt−1) — ft (6)
</equation>
<page confidence="0.991839">
622
</page>
<subsectionHeader confidence="0.99906">
3.2 Decoding in phrase-based translation
</subsectionHeader>
<bodyText confidence="0.999415666666667">
Given a source language input sentence f, a trained
source-to-target translation model, and a target lan-
guage model, the task of translation is to find the
maximally probable translation eˆ using a linear
combination of j feature functions h weighted ac-
cording to tuned parameters A (Och and Ney, 2002).
</bodyText>
<equation confidence="0.854119230769231">
NN
president
S
VP
DT
The
VB
meets DT
VP
NP
NP
eˆ = argmax �exp( Ajhj(e,f)) (7)
e j
</equation>
<bodyText confidence="0.986631769230769">
Phrase-based translation constructs a set of trans-
lation options — hypothesized translations for con-
tiguous portions of the source sentence — from a
trained phrase table, then incrementally constructs a
lattice of partial target translations (Koehn, 2010).
To prune the search space, lattice nodes are orga-
nized into beam stacks (Jelinek, 1969) according to
the number of source words translated. An n-gram
language model history is also maintained at each
node in the translation lattice. The search space
is further trimmed with hypothesis recombination,
which collapses lattice nodes that share a common
coverage vector and n-gram state.
</bodyText>
<subsectionHeader confidence="0.999764">
3.3 Incorporating a Syntactic Language Model
</subsectionHeader>
<bodyText confidence="0.998365434782609">
Phrase-based translation produces target language
words in an incremental left-to-right fashion, gen-
erating words at the beginning of a translation first
and words at the end of a translation last. Similarly,
incremental parsers process sentences in an incre-
mental fashion, analyzing words at the beginning of
a sentence first and words at the end of a sentence
last. As such, an incremental parser with transition
function 6 can be incorporated into the phrase-based
decoding process in a straightforward manner. Each
node in the translation lattice is augmented with a
syntactic language model state ˜�t.
The hypothesis at the root of the translation lattice
is initialized with ˜�0, representing the internal state
of the incremental parser before any input words are
processed. The phrase-based translation decoding
process adds nodes to the lattice; each new node
contains one or more target language words. Each
node contains a backpointer to its parent node, in
which ˜�t−1 is stored. Given a new target language
word et and ˜�t−1, the incremental parser’s transi-
tion function 6 calculates ˜�t. Figure 1 illustrates
the
</bodyText>
<page confidence="0.996975">
623
</page>
<figureCaption confidence="0.532909">
Figure 4: Graphical representation of the depen-
</figureCaption>
<bodyText confidence="0.999470382352941">
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that can
be used to parse syntax. Circles denote random vari-
ables, and edges denote conditional dependencies.
Shaded circles denote variables with observed val-
ues.
sive phrase structure trees using the tree transforms
in Schuler et al. (2010). Constituent nontermi-
nals in right-corner transformed trees take the form
of incomplete constituents cη/cηι consisting of an
‘active’ constituent cη lacking an ‘awaited’ con-
stituent cηι yet to come, similar to non-constituent
categories in a Combinatory Categorial Grammar
(Ades and Steedman, 1982; Steedman, 2000). As
an example, the parser might consider VP/NN as a
possible category for input “meets the”.
A sample phrase structure tree is shown before
and after the right-corner transform in Figures 2
and 3. Our parser operates over a right-corner trans-
formed probabilistic context-free grammar (PCFG).
Parsing runs in linear time on the length of the input.
This model of incremental parsing is implemented
as a Hierarchical Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001), and is equivalent to a
probabilistic pushdown automaton with a bounded
pushdown store. The parser runs in O(n) time,
where n is the number of words in the input. This
model is shown graphically in Figure 4 and formally
defined in §4.1 below.
The incremental parser assigns a probability
(Eq. 5) for a partial target language hypothesis, using
a bounded store of incomplete constituents cη/cηι.
The phrase-based decoder uses this probability value
as the syntactic language model feature score.
</bodyText>
<subsectionHeader confidence="0.971677">
4.1 Formal Parsing Model: Scoring Partial
Translation Hypotheses
</subsectionHeader>
<bodyText confidence="0.9995785">
This model is essentially an extension of an HHMM,
which obtains a most likely sequence of hidden store
states, s1..D
1..T , of some length T and some maxi-
mum depth D, given a sequence of observed tokens
(e.g. generated target language words), e1..T, using
HHMM state transition model θA and observation
symbol model θB (Rabiner, 1990):
</bodyText>
<equation confidence="0.998091333333333">
PθA(s1..D
t |s1..D
t−1 )·PθB(et  |s1..D
t )
s1..D
1..T
</equation>
<bodyText confidence="0.95303">
(8)
The HHMM parser is equivalent to a probabilis-
tic pushdown automaton with a bounded push-
down store. The model generates each successive
store (using store model θS) only after considering
whether each nested sequence of incomplete con-
stituents has completed and reduced (using reduc-
tion model θR):
</bodyText>
<equation confidence="0.989878333333333">
PθA(s1..D
t  |s1..D
t−1 )def =
PθR(rdt  |rd+1
t sd t−1sd−1
t−1 )
· PθS(sdt  |rd+1
t rd t sd t−1sd−1
t ) (9)
</equation>
<bodyText confidence="0.918009090909091">
Store elements are defined to contain only the
active (cη) and awaited (cηι) constituent categories
necessary to compute an incomplete constituent
probability:
sd def = hcη, cηιi (10)
t
Reduction states are defined to contain only the
complete constituent category crd t necessary to com-
pute an inside likelihood probability, as well as a
flag frd t indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):
</bodyText>
<equation confidence="0.5873105">
rd def = hcrd t , frd (11)
t t i
</equation>
<bodyText confidence="0.999885833333333">
The model probabilities for these store elements
and reduction states can then be defined (from Mur-
phy and Paskin 2001) to expand a new incomplete
constituent after a reduction has taken place (frdt =
1; using depth-specific store state expansion model
θS-E,d), transition along a sequence of store elements
</bodyText>
<figure confidence="0.993103642276423">
. . .
. . .
. . .
. . .
1
rt−1
2
rt−1
3
rt−1
1
st−1
2
st−1
3
st−1
et−1 et
r1
t
r2
t
r3
t
s1
t
s2t
s3t
s1..D def ar ax
1 T = �
T
H
t=1
E D
1 D H
rt ..rt d=1
624
s1
1
s2
1
s3
1
e1
=The e2 =president e3 =meets e4 =the e5 =board e6 =on e7 =Friday
r1
2
r2
2
r3
2
s1
2
s2
2
s3
2
r1
3
r2
3
r3
3
s1
3
s2
3
s3
3
r1
4
r2
4
r3
4
s1
4
s2
4
s3
4
r1
5
r2
5
r3
5
s1
5
s2
5
s3
5
r1
6
r2
6
r3
6
s1
6
s2
6
s3
6
r1
7
r2
7
r3
7
s1
7
s2
7
s3
7
r1
8
r2
8
r3
8
t=1 t=2 t=3 t=4 t=5 t=6 t=7
</figure>
<figureCaption confidence="0.998859">
Figure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The
president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized
right-corner tree structure of Figure 3.
</figureCaption>
<bodyText confidence="0.8730375">
if no reduction has taken place (frd t =0; using depth-
specific store state transition model BS-T,d): 2
</bodyText>
<equation confidence="0.997455615384615">
PθS(sd t  |rd+1
t rd t sd t−1sd−1
t )def =
if frd+1
t =1, frdt =1 : PθS-E,d(sdt  |sd−1
t )
if frd+1
t =1, frdt =0 : PθS-T,d(sdt  |rd+1
t rdt sdt−1sd−1
t )
if frd+1
t =0, frdt =0 : Jsd t =sdt−1K
(12)
</equation>
<bodyText confidence="0.953423">
and possibly reduce a store element (terminate
</bodyText>
<equation confidence="0.946346727272727">
a sequence) if the store state below it has re-
duced (frd+1 t= 1; using depth-specific reduction
model BR,d):
PθR(rdt  |rd+1
t sd t−1sd−1
t−1 )def =
~if frd+1
t =0 : Jrd t = r⊥K t−1 ) (13)
if frd+1
t =1 : PθR,d(rd t  |rd+1
t sd t−1 sd−1
</equation>
<bodyText confidence="0.9257964">
where r⊥ is a null state resulting from the failure of
an incomplete constituent to complete, and constants
are defined for the edge conditions of st and rD+1
t .
Figure 5 illustrates this model in action.
These pushdown automaton operations are then
refined for right-corner parsing (Schuler, 2009),
distinguishing active transitions (model BS-T-A,d, in
which an incomplete constituent is completed, but
not reduced, and then immediately expanded to a
2An indicator function [I is used to denote deterministic
probabilities: � 1 if φ is true, 0 otherwise.
new incomplete constituent in the same store el-
ement) from awaited transitions (model BS-T-W,d,
which involve no completion):
</bodyText>
<equation confidence="0.997094333333333">
PθS-T,d(sdt  |rd+1
t rd t sd t−1sd−1
t )def =
~if rdt 6=r⊥: PθS-T-A,d(sd t |sd−1
t rd t ) 14
if rd = r⊥: PθS-T-W,d (sdt |sdt− 1 rt ( )
PθR,d(rd t |rd+1 tsd t−1sd−1
t−1)def =
1I
</equation>
<bodyText confidence="0.87612375">
if crd+1 6= xt: Jrdt = r⊥11
�if crd+1=xt: PθR-R,d(rdt  |sdt−1std−−1 (15)
These HHMM right-corner parsing operations are
then defined in terms of branch- and depth-specific
PCFG probabilities BG-R,d and BG-L,d: 3
3Model probabilities are also defined in terms of left-
progeny probability distribution EθG-RL∗,d which is itself defined
in terms of PCFG probabilities:
</bodyText>
<equation confidence="0.997451076923077">
0 def E
EθG-RL∗,d(cη → cη0 ...) �
cη1
E
k) def
EθG-RL∗,d(cη cη0k0 �
cη0k
E· PθG-L,d(cη0k →cη0k0 cη0k1) (17)
cη0k1
* ) def
EθG-RL∗,d (cη cηι �
+de * )
EθG-RL∗,d(cη cηι e
...) e EθG-RL∗,d(cη cηι
− EθG-RL∗,d(cη → cηι ...) (19)
0
⎧
⎨
⎩
PθG-R,d(cη → cη0 cη1) (16)
/ k� 1
EθG-RL∗,d(Cη → cη0k ...)
EθG-RL∗,d(cη → cηι ...) (18)
k
E
k=0
</equation>
<page confidence="0.992923">
625
</page>
<figure confidence="0.998271">
president meets
➊➋➌➃➄➅➆
1 1
s3 s4
2 2
s3 s4
r4 r5
s3 s4
f31
=meets
e3
. . .
r1
4
r2
4
e4
r1
5
r2
5
the board
➊➋➌➃➄➏➐
s15
s2
5
s3
5
e5
�T51
. . .
=board
</figure>
<figureCaption confidence="0.997964">
Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-
tion the board of source phrase den Vorstand. Syntactic language model state T31 contains random variables
</figureCaption>
<figure confidence="0.948213333333333">
s1..3
3 ; likewise T51 contains s1..3
5 . The intervening random variables r1..3
4 , s1..3
4 , and r1..3
5 are calculated by
</figure>
<bodyText confidence="0.960116">
transition function S (Eq. 6, as defined by §4.1), but are not stored. Observed random variables (e3..e5) are
shown for clarity, but are not explicitly stored in any syntactic language model state.
</bodyText>
<listItem confidence="0.979642">
• for expansions:
</listItem>
<equation confidence="0.997636">
PθS-E,d(hcηι, c0 ηιi  |h−, cηi)def =
EθG-RL*,d(cη → cηι ...) · Jxηι =c0
∗ ηι =cηιK (20)
</equation>
<listItem confidence="0.631235">
• for awaited transitions:
</listItem>
<equation confidence="0.998473">
PθS-T-W,d(hcη, cηι1i  |hc0η, cηιi cηι0) def =
PθG-R,d(cηι → cηι0 cηι1)
Jcη = c0 ηK · (21) 0
EθG-RL*,d(cηι → cηι0 ...)
</equation>
<listItem confidence="0.899212">
• for active transitions:
</listItem>
<equation confidence="0.9944391">
PθS-T-A,d(hcηι, cηι1i  |h−, cηi cηι0) def =
∗
EθG-RL*,d(cη → cηι ...) · PθG-L,d(cηι → cηι0 cηι1)
+
EθG-RL*,d(cη → cηι0 ...)
(22)
E
G-RL*,
(
...) (23)
θ
d
cη→cηι
∗
EθG-RL*,d(cη→cηι ...)
(24)
EθG-RL*,d(cη→ cηι ...)
+
∗
EθG-RL*,d(cη→cηι ...)
</equation>
<bodyText confidence="0.9977755">
We use the parser implementation of (Schuler,
2009; Schuler et al., 2010).
</bodyText>
<sectionHeader confidence="0.9508005" genericHeader="method">
5 Phrase Based Translation with an
Incremental Syntactic Language Model
</sectionHeader>
<bodyText confidence="0.880093">
=the
</bodyText>
<page confidence="0.986269">
626
</page>
<listItem confidence="0.9947072">
• for cross-element reductions:
PθR-R,d(cηι, 1  |h−, cηi hc0ηι,−i
) def
0
• for in-element reductions:
</listItem>
<bodyText confidence="0.952525448275862">
PθR-R,d(cηι, 0  |h−, cηi hc0ηι,−i
) def
The phrase-based decoder is augmented by adding
additional state data to each hypothesis in the de-
η
·
η
·
coder’s hypothesis stacks. Figure 1 illustrates an ex-
cerpt from a standard phrase-based translation lat-
tice. Within each decoder stack t, each hypothe-
sis h is augmented with a syntactic language model
state fth. Each syntactic language model state is
a random variable store, containing a slice of ran-
dom variables from the HHMM. Specifically, -�th
contains those random variables s1..D
t that maintain
distributions over syntactic elements.
By maintaining these syntactic random variable
stores, each hypothesis has access to the current
language model probability for the partial transla-
tion ending at that hypothesis, as calculated by an
incremental syntactic language model defined by
the HHMM. Specifically, the random variable store
at hypothesis h provides P(Tth) = P(eh1..t, s1..D
1..t ),
where eh1..t is the sequence of words in a partial hy-
pothesis ending at h which contains t target words,
and where there are D syntactic random variables in
each random variable store (Eq. 5).
During stack decoding, the phrase-based decoder
progressively constructs new hypotheses by extend-
ing existing hypotheses. New hypotheses are placed
in appropriate hypothesis stacks. In the simplest
case, a new hypothesis extends an existing hypothe-
sis by exactly one target word. As the new hypothe-
sis is constructed by extending an existing stack ele-
ment, the store and reduction state random variables
are processed, along with the newly hypothesized
word. This results in a new store of syntactic ran-
dom variables (Eq. 6) that are associated with the
new stack element.
When a new hypothesis extends an existing hy-
pothesis by more than one word, this process is first
carried out for the first new word in the hypothe-
sis. It is then repeated for the remaining words in
the hypothesis extension. Once the final word in
the hypothesis has been processed, the resulting ran-
dom variable store is associated with that hypoth-
esis. The random variable stores created for the
non-final words in the extending hypothesis are dis-
carded, and need not be explicitly retained.
Figure 6 illustrates this process, showing how a
syntactic language model state &apos;T51 in a phrase-based
decoding lattice is obtained from a previous syn-
tactic language model state f31 (from Figure 1) by
parsing the target language words from a phrase-
based translation option.
</bodyText>
<table confidence="0.994390733333333">
LM In-domain Out-of-domain
WSJ 23 ppl ur-en dev ppl
WSJ 1-gram 1973.57 3581.72
WSJ 2-gram 349.18 1312.61
WSJ 3-gram 262.04 1264.47
WSJ 4-gram 244.12 1261.37
WSJ 5-gram 232.08 1261.90
WSJ HHMM 384.66 529.41
Interpolated WSJ 209.13 225.48
5-gram + HHMM
Giga 5-gram 258.35 312.28
Interp. Giga 5-gr
+ WSJ HHMM 222.39 123.10
Interp. Giga 5-gr
+ WSJ 5-gram 174.88 321.05
</table>
<figureCaption confidence="0.62764275">
Figure 7: Average per-word perplexity values.
HHMM was run with beam size of 2000. Bold in-
dicates best single-model results for LMs trained on
WSJ sections 2-21. Best overall in italics.
</figureCaption>
<bodyText confidence="0.9974805">
Our syntactic language model is integrated into
the current version of Moses (Koehn et al., 2007).
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99998">
As an initial measure to compare language models,
average per-word perplexity, ppl, reports how sur-
prised a model is by test data. Equation 25 calculates
ppl using log base b for a test set of T tokens.
</bodyText>
<equation confidence="0.989381">
−logbP(e1...eT )
ppl = b T (25)
</equation>
<bodyText confidence="0.9999568">
We trained the syntactic language model from
§4 (HHMM) and an interpolated n-gram language
model with modified Kneser-Ney smoothing (Chen
and Goodman, 1998); models were trained on sec-
tions 2-21 of the Wall Street Journal (WSJ) tree-
bank (Marcus et al., 1993). The HHMM outper-
forms the n-gram model in terms of out-of-domain
test set perplexity when trained on the same WSJ
data; the best perplexity results for in-domain and
out-of-domain test sets4 are found by interpolating
</bodyText>
<footnote confidence="0.99628675">
4In-domain is WSJ Section 23. Out-of-domain are the En-
glish reference translations of the dev section , set aside in
(Baker et al., 2009) for parameter tuning, of the NIST Open
MT 2008 Urdu-English task.
</footnote>
<page confidence="0.988382">
627
</page>
<table confidence="0.996334333333333">
Sentence Moses +HHMM +HHMM
length beam=50 beam=2000
10 0.21 533 1143
20 0.53 1193 2562
30 0.85 1746 3749
40 1.13 2095 4588
</table>
<figureCaption confidence="0.961441">
Figure 8: Mean per-sentence decoding time (in sec-
</figureCaption>
<bodyText confidence="0.996809757575758">
onds) for dev set using Moses with and without syn-
tactic language model. HHMM parser beam sizes
are indicated for the syntactic LM.
HHMM and n-gram LMs (Figure 7). To show the
effects of training an LM on more data, we also re-
port perplexity results on the 5-gram LM trained for
the GALE Arabic-English task using the English Gi-
gaword corpus. In all cases, including the HHMM
significantly reduces perplexity.
We trained a phrase-based translation model on
the full NIST Open MT08 Urdu-English translation
model using the full training data. We trained the
HHMM and n-gram LMs on the WSJ data in order
to make them as similar as possible. During tuning,
Moses was first configured to use just the n-gram
LM, then configured to use both the n-gram LM and
the syntactic HHMM LM. MERT consistently as-
signed positive weight to the syntactic LM feature,
typically slightly less than the n-gram LM weight.
In our integration with Moses, incorporating a
syntactic language model dramatically slows the de-
coding process. Figure 8 illustrates a slowdown
around three orders of magnitude. Although speed
remains roughly linear to the size of the source sen-
tence (ruling out exponential behavior), it is with an
extremely large constant time factor. Due to this
slowdown, we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words),
and tested using a constrained devtest set (only sen-
tences with 1-20 words). Figure 9 shows a statis-
tically significant improvement to the BLEU score
when using the HHMM and the n-gram LMs to-
gether on this reduced test set.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.98929">
This paper argues that incremental syntactic lan-
guages models are a straightforward and appro-
</bodyText>
<note confidence="0.396860333333333">
Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78
</note>
<figureCaption confidence="0.86069">
Figure 9: Results for Ur-En devtest (only sentences
</figureCaption>
<bodyText confidence="0.971837075">
with 1-20 words) with HHMM beam size of 2000
and Moses settings of distortion limit 10, stack size
200, and ttable limit 20.
priate algorithmic fit for incorporating syntax into
phrase-based statistical machine translation, since
both process sentences in an incremental left-to-
right fashion. This means incremental syntactic LM
scores can be calculated during the decoding pro-
cess, rather than waiting until a complete sentence is
posited, which is typically necessary in top-down or
bottom-up parsing.
We provided a rigorous formal definition of in-
cremental syntactic languages models, and detailed
what steps are necessary to incorporate such LMs
into phrase-based decoding. We integrated an incre-
mental syntactic language model into Moses. The
translation quality significantly improved on a con-
strained task, and the perplexity improvements sug-
gest that interpolating between n-gram and syntactic
LMs may hold promise on larger data sets.
The use of very large n-gram language models is
typically a key ingredient in the best-performing ma-
chine translation systems (Brants et al., 2007). Our
n-gram model trained only on WSJ is admittedly
small. Our future work seeks to incorporate large-
scale n-gram language models in conjunction with
incremental syntactic language models.
The added decoding time cost of our syntactic
language model is very high. By increasing the
beam size and distortion limit of the baseline sys-
tem, future work may examine whether a baseline
system with comparable runtimes can achieve com-
parable translation quality.
A more efficient implementation of the HHMM
parser would speed decoding and make more exten-
sive and conclusive translation experiments possi-
ble. Various additional improvements could include
caching the HHMM LM calculations, and exploiting
properties of the right-corner transform that limit the
number of decisions between successive time steps.
</bodyText>
<page confidence="0.998474">
628
</page>
<sectionHeader confidence="0.98463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999251301886793">
Anne Abeill´e, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized tree adjoining grammars for
machine translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics.
Anthony E. Ades and Mark Steedman. 1982. On the
order of words. Linguistics and Philosophy, 4:517–
558.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). SCALE summer workshop final report, Hu-
man Language Technology Center Of Excellence.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9–16.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, John Lafferty,
Robert Mercer, and Paul Roossin. 1990. A statisti-
cal approach to machine translation. Computational
Linguistics, 16(2):79–85.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of the Ninth Ma-
chine Translation Summit of the International Associ-
ation for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 225–
231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283–332.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 72–80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263–270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443–1452.
John Cocke and Jacob Schwartz. 1970. Program-
ming languages and their compilers. Technical report,
Courant Institute of Mathematical Sciences, New York
University.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 507–514.
Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232–241.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727–736.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755–763.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 541–548.
Jay Earley. 1968. An efficient context-free parsing algo-
rithm. Ph.D. thesis, Department of Computer Science,
Carnegie Mellon University.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205–208.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773–781.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In
</reference>
<page confidence="0.994836">
629
</page>
<reference confidence="0.998353747663551">
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 273–
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961–
968.
Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 849–857.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 80–87.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 105–112.
Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 288–295.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and
Cognition Together, pages 26–33.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 273–283.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077–1086.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
conference of the Association for Machine Translation
in the Americas.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical models.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 99–105.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, pages 675–685.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177–180.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609–616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 704–711.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558–566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
653–660.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206–214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 192–199.
</reference>
<page confidence="0.970308">
630
</page>
<reference confidence="0.999873462365591">
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proceedings of
Neural Information Processing Systems, pages 833–
840.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Biennial conference of the Associ-
ation for Machine Translation in the Americas, pages
128–137.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295–302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
161–168.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172–181.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning.
Arjen Poutsma. 1998. Data-oriented translation. In
Ninth Conference of Computational Linguistics in the
Netherlands.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 271–279.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267–296.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1–30.
William Schuler. 2009. Positive results for parsing with a
bounded stack using a model-based right-corner trans-
form. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 344–352.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 577–585.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523–530.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n cubed. Information
and Control, 10(2):189–208.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of the 11th Machine Translation Summit
of the International Association for Machine Transla-
tion, pages 535–542.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138–141.
</reference>
<page confidence="0.998301">
631
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.394455">
<title confidence="0.945285666666667">Incremental Syntactic Language Models for Phrase-based Translation Lane Air Force Research</title>
<author confidence="0.933651">Wright-Patterson AFB</author>
<author confidence="0.933651">OH</author>
<email confidence="0.980377">lane.schwartz@wpafb.af.mil</email>
<affiliation confidence="0.7471225">William Ohio State</affiliation>
<address confidence="0.987518">Columbus, OH</address>
<email confidence="0.999483">schuler@ling.ohio-state.edu</email>
<abstract confidence="0.997983863636364">This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using lexicalized tree adjoining grammars for machine translation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics.</booktitle>
<marker>Abeill´e, Schabes, Joshi, 1990</marker>
<rawString>Anne Abeill´e, Yves Schabes, and Aravind K. Joshi. 1990. Using lexicalized tree adjoining grammars for machine translation. In Proceedings of the 13th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony E Ades</author>
<author>Mark Steedman</author>
</authors>
<title>On the order of words. Linguistics and Philosophy,</title>
<date>1982</date>
<pages>4--517</pages>
<contexts>
<context position="13738" citStr="Ades and Steedman, 1982" startWordPosition="2070" endWordPosition="2073">cy structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values. sive phrase structure trees using the tree transforms in Schuler et al. (2010). Constituent nonterminals in right-corner transformed trees take the form of incomplete constituents cη/cηι consisting of an ‘active’ constituent cη lacking an ‘awaited’ constituent cηι yet to come, similar to non-constituent categories in a Combinatory Categorial Grammar (Ades and Steedman, 1982; Steedman, 2000). As an example, the parser might consider VP/NN as a possible category for input “meets the”. A sample phrase structure tree is shown before and after the right-corner transform in Figures 2 and 3. Our parser operates over a right-corner transformed probabilistic context-free grammar (PCFG). Parsing runs in linear time on the length of the input. This model of incremental parsing is implemented as a Hierarchical Hidden Markov Model (HHMM) (Murphy and Paskin, 2001), and is equivalent to a probabilistic pushdown automaton with a bounded pushdown store. The parser runs in O(n) t</context>
</contexts>
<marker>Ades, Steedman, 1982</marker>
<rawString>Anthony E. Ades and Mark Steedman. 1982. On the order of words. Linguistics and Philosophy, 4:517– 558.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kathy Baker</author>
<author>Steven Bethard</author>
<author>Michael Bloodgood</author>
<author>Ralf Brown</author>
<author>Chris Callison-Burch</author>
<author>Glen Coppersmith</author>
<author>Bonnie Dorr</author>
<author>Wes Filardo</author>
<author>Kendall Giles</author>
</authors>
<title>Semantically informed machine translation (SIMT). SCALE summer workshop final report,</title>
<date>2009</date>
<institution>Human Language Technology Center Of Excellence.</institution>
<location>Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane</location>
<contexts>
<context position="5926" citStr="Baker et al., 2009" startWordPosition="847" endWordPosition="850">ber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Ch</context>
<context position="24668" citStr="Baker et al., 2009" startWordPosition="4017" endWordPosition="4020">(25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets4 are found by interpolating 4In-domain is WSJ Section 23. Out-of-domain are the English reference translations of the dev section , set aside in (Baker et al., 2009) for parameter tuning, of the NIST Open MT 2008 Urdu-English task. 627 Sentence Moses +HHMM +HHMM length beam=50 beam=2000 10 0.21 533 1143 20 0.53 1193 2562 30 0.85 1746 3749 40 1.13 2095 4588 Figure 8: Mean per-sentence decoding time (in seconds) for dev set using Moses with and without syntactic language model. HHMM parser beam sizes are indicated for the syntactic LM. HHMM and n-gram LMs (Figure 7). To show the effects of training an LM on more data, we also report perplexity results on the 5-gram LM trained for the GALE Arabic-English task using the English Gigaword corpus. In all cases, </context>
</contexts>
<marker>Baker, Bethard, Bloodgood, Brown, Callison-Burch, Coppersmith, Dorr, Filardo, Giles, 2009</marker>
<rawString>Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz, and David Zajic. 2009. Semantically informed machine translation (SIMT). SCALE summer workshop final report, Human Language Technology Center Of Excellence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="6617" citStr="Birch et al. (2007)" startWordPosition="952" endWordPosition="955">aintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree s</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="27786" citStr="Brants et al., 2007" startWordPosition="4525" endWordPosition="4528">n top-down or bottom-up parsing. We provided a rigorous formal definition of incremental syntactic languages models, and detailed what steps are necessary to incorporate such LMs into phrase-based decoding. We integrated an incremental syntactic language model into Moses. The translation quality significantly improved on a constrained task, and the perplexity improvements suggest that interpolating between n-gram and syntactic LMs may hold promise on larger data sets. The use of very large n-gram language models is typically a key ingredient in the best-performing machine translation systems (Brants et al., 2007). Our n-gram model trained only on WSJ is admittedly small. Our future work seeks to incorporate largescale n-gram language models in conjunction with incremental syntactic language models. The added decoding time cost of our syntactic language model is very high. By increasing the beam size and distortion limit of the baseline system, future work may examine whether a baseline system with comparable runtimes can achieve comparable translation quality. A more efficient implementation of the HHMM parser would speed decoding and make more extensive and conclusive translation experiments possible</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>John Cocke</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>Robert Mercer</author>
<author>Paul Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter Brown, John Cocke, Stephen Della Pietra, Vincent Della Pietra, Frederick Jelinek, John Lafferty, Robert Mercer, and Paul Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Translation Summit of the International Association for Machine Translation.</booktitle>
<contexts>
<context position="6751" citStr="Charniak et al. (2003)" startWordPosition="971" endWordPosition="974">onal approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast t</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. In Proceedings of the Ninth Machine Translation Summit of the International Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>225--231</pages>
<contexts>
<context position="6356" citStr="Chelba and Jelinek (1998)" startWordPosition="908" endWordPosition="911">mann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing </context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 225– 231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="6548" citStr="Chelba and Jelinek, 2000" startWordPosition="939" endWordPosition="942">9). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not </context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="24210" citStr="Chen and Goodman, 1998" startWordPosition="3939" endWordPosition="3942">beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. −logbP(e1...eT ) ppl = b T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets4 are found by interpolating 4In-domain is WSJ Section 23. Out-of-domain are the English reference translations of the dev section , set aside in (Baker et al., 2009) for parameter tuning, of the NIST Open MT 2008 Urdu-English task. 627 Sentence Moses +HHMM +HHMM length beam=50 beam=2000 10 0.21 533 1143 20</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="8058" citStr="Cherry (2008)" startWordPosition="1163" endWordPosition="1164">on with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. 3 Parser as Syntactic Language Model in Phrase-Based Translation Parsing is the task of selecting the representation &apos;r (typically a tree) that best models the structure of 621  (s) �τ0 (s) president    (s) that (s) the �τ13 �τ1a �τ11 . . . president Friday that president the president    �τ23 �τ2a �τ21 . . . president meets Obama met </context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Colin Cherry. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="4250" citStr="Chiang, 2005" startWordPosition="596" endWordPosition="597">l refer to incremental processing which proceeds from the beginning of a sentence as left-to-right. 620 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics statistical MT that can run in linear-time (§4) • Integration with Moses (§5) along with empirical results for perplexity and significant translation score improvement on a constrained UrduEnglish task (§6) 2 Related Work Neither phrase-based (Koehn et al., 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examin</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="5514" citStr="Chiang, 2010" startWordPosition="792" endWordPosition="793">porated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
<author>Jacob Schwartz</author>
</authors>
<title>Programming languages and their compilers.</title>
<date>1970</date>
<tech>Technical report,</tech>
<institution>Courant Institute of Mathematical Sciences, New York University.</institution>
<contexts>
<context position="9987" citStr="Cocke and Schwartz, 1970" startWordPosition="1477" endWordPosition="1480">may be all phrase structure trees or all dependency trees allowed by the parsing model. Typically, tree T� is taken to be: T� = argmax P(T |e) (1) T We define a syntactic language model P(e) based on the total probability mass over all possible trees for string e. This is shown in Equation 2 and decomposed in Equation 3. P(e) = � P(T, e) (2) TEτ P(e) = � P(e |T)P(T) (3) TEτ 3.1 Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After processing the tth token in string e, an incremental parser has some internal representation of possible hypothesized (incomplete) trees, τt. The syntactic language model probability of a partial sentence e1...et is defined: �P(e1...et) = P(e1...et |T)P(T) (4) TEτt In practice, a parser may constrain the set of trees under consideration to ft, that subset of analyses or partial analyses that remains after any pruning is performed. An incremental syntactic language model can then be defined by a probability mass function (Equation 5) and a transitio</context>
</contexts>
<marker>Cocke, Schwartz, 1970</marker>
<rawString>John Cocke and Jacob Schwartz. 1970. Programming languages and their compilers. Technical report, Courant Institute of Mathematical Sciences, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
</authors>
<title>Discriminative syntactic language modeling for speech recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>507--514</pages>
<contexts>
<context position="6571" citStr="Collins et al., 2005" startWordPosition="943" endWordPosition="946">ve tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translat</context>
</contexts>
<marker>Collins, Roark, Saraclar, 2005</marker>
<rawString>Michael Collins, Brian Roark, and Murat Saraclar. 2005. Discriminative syntactic language modeling for speech recognition. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 507–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Ku˘cerov´a</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>232--241</pages>
<marker>Cowan, Ku˘cerov´a, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous tree adjoining machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>727--736</pages>
<contexts>
<context position="5481" citStr="DeNeefe and Knight, 2009" startWordPosition="784" endWordPosition="787">extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we inc</context>
</contexts>
<marker>DeNeefe, Knight, 2009</marker>
<rawString>Steve DeNeefe and Kevin Knight. 2009. Synchronous tree adjoining machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>755--763</pages>
<contexts>
<context position="5455" citStr="DeNeefe et al., 2007" startWordPosition="780" endWordPosition="783">arch has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translat</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 755–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="5550" citStr="Ding and Palmer, 2005" startWordPosition="796" endWordPosition="799">e-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to la</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1968</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="9948" citStr="Earley, 1968" startWordPosition="1473" endWordPosition="1474">This set of representations may be all phrase structure trees or all dependency trees allowed by the parsing model. Typically, tree T� is taken to be: T� = argmax P(T |e) (1) T We define a syntactic language model P(e) based on the total probability mass over all possible trees for string e. This is shown in Equation 2 and decomposed in Equation 3. P(e) = � P(T, e) (2) TEτ P(e) = � P(e |T)P(T) (3) TEτ 3.1 Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After processing the tth token in string e, an incremental parser has some internal representation of possible hypothesized (incomplete) trees, τt. The syntactic language model probability of a partial sentence e1...et is defined: �P(e1...et) = P(e1...et |T)P(T) (4) TEτt In practice, a parser may constrain the set of trees under consideration to ft, that subset of analyses or partial analyses that remains after any pruning is performed. An incremental syntactic language model can then be defined by a probability mas</context>
</contexts>
<marker>Earley, 1968</marker>
<rawString>Jay Earley. 1968. An efficient context-free parsing algorithm. Ph.D. thesis, Department of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>205--208</pages>
<contexts>
<context position="5357" citStr="Eisner, 2003" startWordPosition="764" endWordPosition="765"> 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Quadratic-time dependency parsing for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>773--781</pages>
<contexts>
<context position="7648" citStr="Galley and Manning, 2009" startWordPosition="1104" endWordPosition="1107">yntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in conc</context>
</contexts>
<marker>Galley, Manning, 2009</marker>
<rawString>Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 773–781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="5053" citStr="Galley et al., 2004" startWordPosition="710" endWordPosition="713">ps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 273– 280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="5114" citStr="Galley et al., 2006" startWordPosition="720" endWordPosition="723">may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using synta</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961– 968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>A direct syntax-driven reordering model for phrase-based machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>849--857</pages>
<contexts>
<context position="7915" citStr="Ge (2010)" startWordPosition="1144" endWordPosition="1145">a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. 3 Parser as Syntactic Language Model in Phrase-Based Translation Parsing is the task of selecting the representation &apos;r (typically a tree) that best models the structure of 621  (s) �τ0 (s) president    (s) that (s) the </context>
</contexts>
<marker>Ge, 2010</marker>
<rawString>Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 849–857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="5010" citStr="Gildea, 2003" startWordPosition="704" endWordPosition="705">s contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information t</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="5078" citStr="Graehl and Knight, 2004" startWordPosition="714" endWordPosition="717">e of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-b</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Khalil Sima’an</author>
<author>Andy Way</author>
</authors>
<title>Supertagged phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>288--295</pages>
<marker>Hassan, Sima’an, Way, 2007</marker>
<rawString>Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Supertagged phrase-based statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Lookahead in deterministic left-corner parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="2939" citStr="Henderson, 2004" startWordPosition="403" endWordPosition="404">cal choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal </context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Lookahead in deterministic left-corner parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="5264" citStr="Huang and Mi, 2010" startWordPosition="749" endWordPosition="752"> to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarc</context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="2985" citStr="Huang and Sagae, 2010" startWordPosition="409" endWordPosition="412">grammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal definition of an incremental parser for 1While</context>
<context position="7615" citStr="Huang and Sagae, 2010" startWordPosition="1099" endWordPosition="1102">under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language mo</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="5134" citStr="Huang et al., 2006" startWordPosition="724" endWordPosition="727"> any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollman</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
</authors>
<title>Hideo Okuma, Taro Watanabe, and Eiichiro Sumita.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>99--105</pages>
<marker>Imamura, 2004</marker>
<rawString>Kenji Imamura, Hideo Okuma, Taro Watanabe, and Eiichiro Sumita. 2004. Example-based machine translation based on syntactic transfer with statistical models. In Proceedings of the 20th International Conference on Computational Linguistics, pages 99–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Fast sequential decoding algorithm using a stack.</title>
<date>1969</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>675--685</pages>
<contexts>
<context position="11592" citStr="Jelinek, 1969" startWordPosition="1741" endWordPosition="1742">the task of translation is to find the maximally probable translation eˆ using a linear combination of j feature functions h weighted according to tuned parameters A (Och and Ney, 2002). NN president S VP DT The VB meets DT VP NP NP eˆ = argmax �exp( Ajhj(e,f)) (7) e j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is also maintained at each node in the translation lattice. The search space is further trimmed with hypothesis recombination, which collapses lattice nodes that share a common coverage vector and n-gram state. 3.3 Incorporating a Syntactic Language Model Phrase-based translation produces target language words in an incremental left-to-right fashion, generating words at the beginning of a translation first and words at the end of a translation last. Similarly, incremental parsers process sentences in an incre</context>
</contexts>
<marker>Jelinek, 1969</marker>
<rawString>Frederick Jelinek. 1969. Fast sequential decoding algorithm using a stack. IBM Journal of Research and Development, pages 675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory.</institution>
<contexts>
<context position="10001" citStr="Kasami, 1965" startWordPosition="1481" endWordPosition="1482">e trees or all dependency trees allowed by the parsing model. Typically, tree T� is taken to be: T� = argmax P(T |e) (1) T We define a syntactic language model P(e) based on the total probability mass over all possible trees for string e. This is shown in Equation 2 and decomposed in Equation 3. P(e) = � P(T, e) (2) TEτ P(e) = � P(e |T)P(T) (3) TEτ 3.1 Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After processing the tth token in string e, an incremental parser has some internal representation of possible hypothesized (incomplete) trees, τt. The syntactic language model probability of a partial sentence e1...et is defined: �P(e1...et) = P(e1...et |T)P(T) (4) TEτt In practice, a parser may constrain the set of trees under consideration to ft, that subset of analyses or partial analyses that remains after any pruning is performed. An incremental syntactic language model can then be defined by a probability mass function (Equation 5) and a transition function 6 (</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntax analysis algorithm for context free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="4193" citStr="Koehn et al., 2003" startWordPosition="587" endWordPosition="590"> for 1While not all languages are written left-to-right, we will refer to incremental processing which proceeds from the beginning of a sentence as left-to-right. 620 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics statistical MT that can run in linear-time (§4) • Integration with Moses (§5) along with empirical results for perplexity and significant translation score improvement on a constrained UrduEnglish task (§6) 2 Related Work Neither phrase-based (Koehn et al., 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to imp</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="23807" citStr="Koehn et al., 2007" startWordPosition="3870" endWordPosition="3873"> 23 ppl ur-en dev ppl WSJ 1-gram 1973.57 3581.72 WSJ 2-gram 349.18 1312.61 WSJ 3-gram 262.04 1264.47 WSJ 4-gram 244.12 1261.37 WSJ 5-gram 232.08 1261.90 WSJ HHMM 384.66 529.41 Interpolated WSJ 209.13 225.48 5-gram + HHMM Giga 5-gram 258.35 312.28 Interp. Giga 5-gr + WSJ HHMM 222.39 123.10 Interp. Giga 5-gr + WSJ 5-gram 174.88 321.05 Figure 7: Average per-word perplexity values. HHMM was run with beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. −logbP(e1...eT ) ppl = b T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trai</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11503" citStr="Koehn, 2010" startWordPosition="1726" endWordPosition="1727">sentence f, a trained source-to-target translation model, and a target language model, the task of translation is to find the maximally probable translation eˆ using a linear combination of j feature functions h weighted according to tuned parameters A (Och and Ney, 2002). NN president S VP DT The VB meets DT VP NP NP eˆ = argmax �exp( Ajhj(e,f)) (7) e j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is also maintained at each node in the translation lattice. The search space is further trimmed with hypothesis recombination, which collapses lattice nodes that share a common coverage vector and n-gram state. 3.3 Incorporating a Syntactic Language Model Phrase-based translation produces target language words in an incremental left-to-right fashion, generating words at the beginning of a translation first and words at th</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="5188" citStr="Liu et al., 2006" startWordPosition="733" endWordPosition="736">hrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation gramma</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>704--711</pages>
<contexts>
<context position="5206" citStr="Liu et al., 2007" startWordPosition="737" endWordPosition="740">ation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonte</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 558–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="24312" citStr="Marcus et al., 1993" startWordPosition="3958" endWordPosition="3961">erall in italics. Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). 6 Results As an initial measure to compare language models, average per-word perplexity, ppl, reports how surprised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens. −logbP(e1...eT ) ppl = b T (25) We trained the syntactic language model from §4 (HHMM) and an interpolated n-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sections 2-21 of the Wall Street Journal (WSJ) treebank (Marcus et al., 1993). The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets4 are found by interpolating 4In-domain is WSJ Section 23. Out-of-domain are the English reference translations of the dev section , set aside in (Baker et al., 2009) for parameter tuning, of the NIST Open MT 2008 Urdu-English task. 627 Sentence Moses +HHMM +HHMM length beam=50 beam=2000 10 0.21 533 1143 20 0.53 1193 2562 30 0.85 1746 3749 40 1.13 2095 4588 Figure 8: Mean per-sentence decoding time (in seco</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>653--660</pages>
<contexts>
<context position="5093" citStr="Melamed, 2004" startWordPosition="718" endWordPosition="719">) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine tr</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. Dan Melamed. 2004. Statistical machine translation by parsing. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, pages 653–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="5243" citStr="Mi and Huang, 2008" startWordPosition="745" endWordPosition="748">g translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substanti</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="5223" citStr="Mi et al., 2008" startWordPosition="741" endWordPosition="744">hether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can d</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Mark A Paskin</author>
</authors>
<title>Linear time inference in hierarchical HMMs.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<pages>833--840</pages>
<contexts>
<context position="14224" citStr="Murphy and Paskin, 2001" startWordPosition="2147" endWordPosition="2150"> ‘awaited’ constituent cηι yet to come, similar to non-constituent categories in a Combinatory Categorial Grammar (Ades and Steedman, 1982; Steedman, 2000). As an example, the parser might consider VP/NN as a possible category for input “meets the”. A sample phrase structure tree is shown before and after the right-corner transform in Figures 2 and 3. Our parser operates over a right-corner transformed probabilistic context-free grammar (PCFG). Parsing runs in linear time on the length of the input. This model of incremental parsing is implemented as a Hierarchical Hidden Markov Model (HHMM) (Murphy and Paskin, 2001), and is equivalent to a probabilistic pushdown automaton with a bounded pushdown store. The parser runs in O(n) time, where n is the number of words in the input. This model is shown graphically in Figure 4 and formally defined in §4.1 below. The incremental parser assigns a probability (Eq. 5) for a partial target language hypothesis, using a bounded store of incomplete constituents cη/cηι. The phrase-based decoder uses this probability value as the syntactic language model feature score. 4.1 Formal Parsing Model: Scoring Partial Translation Hypotheses This model is essentially an extension </context>
<context position="16179" citStr="Murphy and Paskin 2001" startWordPosition="2474" endWordPosition="2478">t rd t sd t−1sd−1 t ) (9) Store elements are defined to contain only the active (cη) and awaited (cηι) constituent categories necessary to compute an incomplete constituent probability: sd def = hcη, cηιi (10) t Reduction states are defined to contain only the complete constituent category crd t necessary to compute an inside likelihood probability, as well as a flag frd t indicating whether a reduction has taken place (to end a sequence of incomplete constituents): rd def = hcrd t , frd (11) t t i The model probabilities for these store elements and reduction states can then be defined (from Murphy and Paskin 2001) to expand a new incomplete constituent after a reduction has taken place (frdt = 1; using depth-specific store state expansion model θS-E,d), transition along a sequence of store elements . . . . . . . . . . . . 1 rt−1 2 rt−1 3 rt−1 1 st−1 2 st−1 3 st−1 et−1 et r1 t r2 t r3 t s1 t s2t s3t s1..D def ar ax 1 T = � T H t=1 E D 1 D H rt ..rt d=1 624 s1 1 s2 1 s3 1 e1 =The e2 =president e3 =meets e4 =the e5 =board e6 =on e7 =Friday r1 2 r2 2 r3 2 s1 2 s2 2 s3 2 r1 3 r2 3 r3 3 s1 3 s2 3 s3 3 r1 4 r2 4 r3 4 s1 4 s2 4 s3 4 r1 5 r2 5 r3 5 s1 5 s2 5 s3 5 r1 6 r2 6 r3 6 s1 6 s2 6 s3 6 r1 7 r2 7 r3 7 s1 </context>
</contexts>
<marker>Murphy, Paskin, 2001</marker>
<rawString>Kevin P. Murphy and Mark A. Paskin. 2001. Linear time inference in hierarchical HMMs. In Proceedings of Neural Information Processing Systems, pages 833– 840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Nesson</author>
<author>Stuart Shieber</author>
<author>Alexander Rush</author>
</authors>
<title>Induction of probabilistic synchronous treeinsertion grammars for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>128--137</pages>
<contexts>
<context position="5413" citStr="Nesson et al., 2006" startWordPosition="772" endWordPosition="775">ove translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a stan</context>
</contexts>
<marker>Nesson, Shieber, Rush, 2006</marker>
<rawString>Rebecca Nesson, Stuart Shieber, and Alexander Rush. 2006. Induction of probabilistic synchronous treeinsertion grammars for machine translation. In Proceedings of the 7th Biennial conference of the Association for Machine Translation in the Americas, pages 128–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="11163" citStr="Och and Ney, 2002" startWordPosition="1667" endWordPosition="1670">mass function (Equation 5) and a transition function 6 (Equation 6). The role of 6 is explained in §3.3 below. Any parser which implements these two functions can serve as a syntactic language model. �P(e1...et) ~ P(Tt) = P(e1...et |T)P(T) (5) TE-Ft 6(et,�τt−1) — ft (6) 622 3.2 Decoding in phrase-based translation Given a source language input sentence f, a trained source-to-target translation model, and a target language model, the task of translation is to find the maximally probable translation eˆ using a linear combination of j feature functions h weighted according to tuned parameters A (Och and Ney, 2002). NN president S VP DT The VB meets DT VP NP NP eˆ = argmax �exp( Ajhj(e,f)) (7) e j Phrase-based translation constructs a set of translation options — hypothesized translations for contiguous portions of the source sentence — from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n-gram language model history is also maintained at each node in the translation lattice. The search space is furth</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>161--168</pages>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Parsers as language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>172--181</pages>
<contexts>
<context position="6862" citStr="Post and Gildea (2008)" startWordPosition="989" endWordPosition="992">f n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-ba</context>
</contexts>
<marker>Post, Gildea, 2008</marker>
<rawString>Matt Post and Daniel Gildea. 2008. Parsers as language models for statistical machine translation. In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, pages 172–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Language modeling with tree substitution grammars.</title>
<date>2009</date>
<booktitle>In NIPS workshop on Grammar Induction, Representation of Language, and Language Learning.</booktitle>
<contexts>
<context position="7206" citStr="Post and Gildea (2009)" startWordPosition="1037" endWordPosition="1040">l. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure pars</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. In NIPS workshop on Grammar Induction, Representation of Language, and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Data-oriented translation.</title>
<date>1998</date>
<booktitle>In Ninth Conference of Computational Linguistics in the Netherlands.</booktitle>
<contexts>
<context position="5343" citStr="Poutsma, 1998" startWordPosition="762" endWordPosition="763"> (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to </context>
</contexts>
<marker>Poutsma, 1998</marker>
<rawString>Arjen Poutsma. 1998. Data-oriented translation. In Ninth Conference of Computational Linguistics in the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="5571" citStr="Quirk et al., 2005" startWordPosition="800" endWordPosition="803">els: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in spee</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1990</date>
<booktitle>Readings in speech recognition,</booktitle>
<pages>53--3</pages>
<contexts>
<context position="15120" citStr="Rabiner, 1990" startWordPosition="2293" endWordPosition="2294">bility (Eq. 5) for a partial target language hypothesis, using a bounded store of incomplete constituents cη/cηι. The phrase-based decoder uses this probability value as the syntactic language model feature score. 4.1 Formal Parsing Model: Scoring Partial Translation Hypotheses This model is essentially an extension of an HHMM, which obtains a most likely sequence of hidden store states, s1..D 1..T , of some length T and some maximum depth D, given a sequence of observed tokens (e.g. generated target language words), e1..T, using HHMM state transition model θA and observation symbol model θB (Rabiner, 1990): PθA(s1..D t |s1..D t−1 )·PθB(et |s1..D t ) s1..D 1..T (8) The HHMM parser is equivalent to a probabilistic pushdown automaton with a bounded pushdown store. The model generates each successive store (using store model θS) only after considering whether each nested sequence of incomplete constituents has completed and reduced (using reduction model θR): PθA(s1..D t |s1..D t−1 )def = PθR(rdt |rd+1 t sd t−1sd−1 t−1 ) · PθS(sdt |rd+1 t rd t sd t−1sd−1 t ) (9) Store elements are defined to contain only the active (cη) and awaited (cηι) constituent categories necessary to compute an incomplete con</context>
</contexts>
<marker>Rabiner, 1990</marker>
<rawString>Lawrence R. Rabiner. 1990. A tutorial on hidden Markov models and selected applications in speech recognition. Readings in speech recognition, 53(3):267–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2922" citStr="Roark, 2001" startWordPosition="401" endWordPosition="402">terms of lexical choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Broad-coverage incremental parsing using human-like memory constraints.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="2961" citStr="Schuler et al., 2010" startWordPosition="405" endWordPosition="408">till often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better modeling structural relationships and long-distance dependencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothesized translations incrementally, from left-to-right.1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004). On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. We directly integrate incremental syntactic parsing into phrase-based translation. This approach re-exerts the role of the language model as a mechanism for encouraging syntactically fluent translations. The contributions of this work are as follows: • A novel method for integrating syntactic LMs into phrase-based translation (§3) • A formal definition of an incre</context>
<context position="13440" citStr="Schuler et al. (2010)" startWordPosition="2028" endWordPosition="2031">more target language words. Each node contains a backpointer to its parent node, in which ˜�t−1 is stored. Given a new target language word et and ˜�t−1, the incremental parser’s transition function 6 calculates ˜�t. Figure 1 illustrates the 623 Figure 4: Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values. sive phrase structure trees using the tree transforms in Schuler et al. (2010). Constituent nonterminals in right-corner transformed trees take the form of incomplete constituents cη/cηι consisting of an ‘active’ constituent cη lacking an ‘awaited’ constituent cηι yet to come, similar to non-constituent categories in a Combinatory Categorial Grammar (Ades and Steedman, 1982; Steedman, 2000). As an example, the parser might consider VP/NN as a possible category for input “meets the”. A sample phrase structure tree is shown before and after the right-corner transform in Figures 2 and 3. Our parser operates over a right-corner transformed probabilistic context-free grammar</context>
<context position="20496" citStr="Schuler et al., 2010" startWordPosition="3333" endWordPosition="3336">ny syntactic language model state. • for expansions: PθS-E,d(hcηι, c0 ηιi |h−, cηi)def = EθG-RL*,d(cη → cηι ...) · Jxηι =c0 ∗ ηι =cηιK (20) • for awaited transitions: PθS-T-W,d(hcη, cηι1i |hc0η, cηιi cηι0) def = PθG-R,d(cηι → cηι0 cηι1) Jcη = c0 ηK · (21) 0 EθG-RL*,d(cηι → cηι0 ...) • for active transitions: PθS-T-A,d(hcηι, cηι1i |h−, cηi cηι0) def = ∗ EθG-RL*,d(cη → cηι ...) · PθG-L,d(cηι → cηι0 cηι1) + EθG-RL*,d(cη → cηι0 ...) (22) E G-RL*, ( ...) (23) θ d cη→cηι ∗ EθG-RL*,d(cη→cηι ...) (24) EθG-RL*,d(cη→ cηι ...) + ∗ EθG-RL*,d(cη→cηι ...) We use the parser implementation of (Schuler, 2009; Schuler et al., 2010). 5 Phrase Based Translation with an Incremental Syntactic Language Model =the 626 • for cross-element reductions: PθR-R,d(cηι, 1 |h−, cηi hc0ηι,−i ) def 0 • for in-element reductions: PθR-R,d(cηι, 0 |h−, cηi hc0ηι,−i ) def The phrase-based decoder is augmented by adding additional state data to each hypothesis in the deη · η · coder’s hypothesis stacks. Figure 1 illustrates an excerpt from a standard phrase-based translation lattice. Within each decoder stack t, each hypothesis h is augmented with a syntactic language model state fth. Each syntactic language model state is a random variable s</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-coverage incremental parsing using human-like memory constraints. Computational Linguistics, 36(1):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Positive results for parsing with a bounded stack using a model-based right-corner transform.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="17966" citStr="Schuler, 2009" startWordPosition="2876" endWordPosition="2877"> =0, frdt =0 : Jsd t =sdt−1K (12) and possibly reduce a store element (terminate a sequence) if the store state below it has reduced (frd+1 t= 1; using depth-specific reduction model BR,d): PθR(rdt |rd+1 t sd t−1sd−1 t−1 )def = ~if frd+1 t =0 : Jrd t = r⊥K t−1 ) (13) if frd+1 t =1 : PθR,d(rd t |rd+1 t sd t−1 sd−1 where r⊥ is a null state resulting from the failure of an incomplete constituent to complete, and constants are defined for the edge conditions of st and rD+1 t . Figure 5 illustrates this model in action. These pushdown automaton operations are then refined for right-corner parsing (Schuler, 2009), distinguishing active transitions (model BS-T-A,d, in which an incomplete constituent is completed, but not reduced, and then immediately expanded to a 2An indicator function [I is used to denote deterministic probabilities: � 1 if φ is true, 0 otherwise. new incomplete constituent in the same store element) from awaited transitions (model BS-T-W,d, which involve no completion): PθS-T,d(sdt |rd+1 t rd t sd t−1sd−1 t )def = ~if rdt 6=r⊥: PθS-T-A,d(sd t |sd−1 t rd t ) 14 if rd = r⊥: PθS-T-W,d (sdt |sdt− 1 rt ( ) PθR,d(rd t |rd+1 tsd t−1sd−1 t−1)def = 1I if crd+1 6= xt: Jrdt = r⊥11 �if crd+1=xt</context>
<context position="20473" citStr="Schuler, 2009" startWordPosition="3331" endWordPosition="3332">tly stored in any syntactic language model state. • for expansions: PθS-E,d(hcηι, c0 ηιi |h−, cηi)def = EθG-RL*,d(cη → cηι ...) · Jxηι =c0 ∗ ηι =cηιK (20) • for awaited transitions: PθS-T-W,d(hcη, cηι1i |hc0η, cηιi cηι0) def = PθG-R,d(cηι → cηι0 cηι1) Jcη = c0 ηK · (21) 0 EθG-RL*,d(cηι → cηι0 ...) • for active transitions: PθS-T-A,d(hcηι, cηι1i |h−, cηi cηι0) def = ∗ EθG-RL*,d(cη → cηι ...) · PθG-L,d(cηι → cηι0 cηι1) + EθG-RL*,d(cη → cηι0 ...) (22) E G-RL*, ( ...) (23) θ d cη→cηι ∗ EθG-RL*,d(cη→cηι ...) (24) EθG-RL*,d(cη→ cηι ...) + ∗ EθG-RL*,d(cη→cηι ...) We use the parser implementation of (Schuler, 2009; Schuler et al., 2010). 5 Phrase Based Translation with an Incremental Syntactic Language Model =the 626 • for cross-element reductions: PθR-R,d(cηι, 1 |h−, cηi hc0ηι,−i ) def 0 • for in-element reductions: PθR-R,d(cηι, 0 |h−, cηi hc0ηι,−i ) def The phrase-based decoder is augmented by adding additional state data to each hypothesis in the deη · η · coder’s hypothesis stacks. Figure 1 illustrates an excerpt from a standard phrase-based translation lattice. Within each decoder stack t, each hypothesis h is augmented with a syntactic language model state fth. Each syntactic language model state</context>
</contexts>
<marker>Schuler, 2009</marker>
<rawString>William Schuler. 2009. Positive results for parsing with a bounded stack using a model-based right-corner transform. In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 344–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="5154" citStr="Shen et al., 2008" startWordPosition="728" endWordPosition="731">tituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 200</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="5328" citStr="Shieber and Schabes, 1990" startWordPosition="758" endWordPosition="761">improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). </context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart M. Shieber and Yves Schabes. 1990. Synchronous tree adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Synchronous grammars as tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms.</booktitle>
<contexts>
<context position="5372" citStr="Shieber, 2004" startWordPosition="766" endWordPosition="767">nd such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translat</context>
</contexts>
<marker>Shieber, 2004</marker>
<rawString>Stuart M. Shieber. 2004. Synchronous grammars as tree transducers. In Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13755" citStr="Steedman, 2000" startWordPosition="2074" endWordPosition="2075">d Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values. sive phrase structure trees using the tree transforms in Schuler et al. (2010). Constituent nonterminals in right-corner transformed trees take the form of incomplete constituents cη/cηι consisting of an ‘active’ constituent cη lacking an ‘awaited’ constituent cηι yet to come, similar to non-constituent categories in a Combinatory Categorial Grammar (Ades and Steedman, 1982; Steedman, 2000). As an example, the parser might consider VP/NN as a possible category for input “meets the”. A sample phrase structure tree is shown before and after the right-corner transform in Figures 2 and 3. Our parser operates over a right-corner transformed probabilistic context-free grammar (PCFG). Parsing runs in linear time on the length of the input. This model of incremental parsing is implemented as a Hierarchical Hidden Markov Model (HHMM) (Murphy and Paskin, 2001), and is equivalent to a probabilistic pushdown automaton with a bounded pushdown store. The parser runs in O(n) time, where n is t</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press/Bradford Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="6991" citStr="Wu, 1997" startWordPosition="1009" endWordPosition="1010">tructure could be used as an alternative technique in language modeling. This insight has been explored in the context of speech recognition (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n-gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic language models to rescore the output of a tree-based translation system. Post and Gildea (2008) investigate the integration of parsers as syntactic language models during binary bracketing transduction translation (Wu, 1997); under these conditions, both syntactic phrase-structure and dependency parsing language models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. Our syntactic language model fits into the family of linear-time dynamic programming parsers described in</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="4996" citStr="Yamada and Knight, 2001" startWordPosition="700" endWordPosition="703">chniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restrictions failed to improve translation quality. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n cubed.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="10017" citStr="Younger, 1967" startWordPosition="1483" endWordPosition="1484"> dependency trees allowed by the parsing model. Typically, tree T� is taken to be: T� = argmax P(T |e) (1) T We define a syntactic language model P(e) based on the total probability mass over all possible trees for string e. This is shown in Equation 2 and decomposed in Equation 3. P(e) = � P(T, e) (2) TEτ P(e) = � P(e |T)P(T) (3) TEτ 3.1 Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After processing the tth token in string e, an incremental parser has some internal representation of possible hypothesized (incomplete) trees, τt. The syntactic language model probability of a partial sentence e1...et is defined: �P(e1...et) = P(e1...et |T)P(T) (4) TEτt In practice, a parser may constrain the set of trees under consideration to ft, that subset of analyses or partial analyses that remains after any pruning is performed. An incremental syntactic language model can then be defined by a probability mass function (Equation 5) and a transition function 6 (Equation 6). The</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D.H. Younger. 1967. Recognition and parsing of context-free languages in time n cubed. Information and Control, 10(2):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Ai Ti Aw</author>
<author>Jun Sun</author>
<author>Seng Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>A tree-to-tree alignmentbased model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Machine Translation Summit of the International Association for Machine Translation,</booktitle>
<pages>535--542</pages>
<contexts>
<context position="5433" citStr="Zhang et al., 2007" startWordPosition="776" endWordPosition="779">ty. Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic)</context>
</contexts>
<marker>Zhang, Jiang, Aw, Sun, Li, Tan, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li, and Chew Lim Tan. 2007. A tree-to-tree alignmentbased model for statistical machine translation. In Proceedings of the 11th Machine Translation Summit of the International Association for Machine Translation, pages 535–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="5756" citStr="Zollmann and Venugopal, 2006" startWordPosition="823" endWordPosition="827">., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill´e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incorporate syntax into the language model. Traditional approaches to language models in speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. Chelba and Jelinek (1998)</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>