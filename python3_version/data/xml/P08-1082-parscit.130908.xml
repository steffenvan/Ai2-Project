<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.951249">
Learning to Rank Answers on Large Online QA Collections
</title>
<author confidence="0.992921">
Mihai Surdeanu, Massimiliano Ciaramita, Hugo Zaragoza
</author>
<affiliation confidence="0.984489">
Barcelona Media Innovation Center, Yahoo! Research Barcelona
</affiliation>
<email confidence="0.9986">
mihai.surdeanu@barcelonamedia.org,{massi,hugo}@yahoo-inc.com
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901545454546">
This work describes an answer ranking engine
for non-factoid questions built using a large
online community-generated question-answer
collection (Yahoo! Answers). We show how
such collections may be used to effectively
set up large supervised learning experiments.
Furthermore we investigate a wide range of
feature types, some exploiting NLP proces-
sors, and demonstrate that using them in com-
bination leads to considerable improvements
in accuracy.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820611111111">
The problem of Question Answering (QA) has re-
ceived considerable attention in the past few years.
Nevertheless, most of the work has focused on the
task of factoid QA, where questions match short an-
swers, usually in the form of named or numerical en-
tities. Thanks to international evaluations organized
by conferences such as the Text REtrieval Confer-
ence (TREC)1 or the Cross Language Evaluation Fo-
rum (CLEF) Workshop2, annotated corpora of ques-
tions and answers have become available for several
languages, which has facilitated the development of
robust machine learning models for the task.
The situation is different once one moves beyond
the task of factoid QA. Comparatively little research
has focused on QA models for non-factoid ques-
tions such as causation, manner, or reason questions.
Because virtually no training data is available for
this problem, most automated systems train either
</bodyText>
<footnote confidence="0.999631">
1http://trec.nist.gov
2http://www.clef-campaign.org
</footnote>
<note confidence="0.525377">
Q: How do you quiet a squeaky door?
</note>
<figureCaption confidence="0.6968029">
A: Spray WD-40 directly onto the hinges
of the door. Open and close the door
several times. Remove hinges if the
door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to
removed hinges. Put the hinges back,
open and close door several times again.
Q: How to extract html tags from an html
documents with c++?
A: very carefully
</figureCaption>
<tableCaption confidence="0.993688">
Table 1: Sample content from Yahoo! Answers.
</tableCaption>
<bodyText confidence="0.9995703">
on small hand-annotated corpora built in house (Hi-
gashinaka and Isozaki, 2008) or on question-answer
pairs harvested from Frequently Asked Questions
(FAQ) lists or similar resources (Soricut and Brill,
2006). None of these situations is ideal: the cost
of building the training corpus in the former setup
is high; in the latter scenario the data tends to be
domain-specific, hence unsuitable for the learning of
open-domain models.
On the other hand, recent years have seen an ex-
plosion of user-generated content (or social media).
Of particular interest in our context are community-
driven question-answering sites, such as Yahoo! An-
swers3, where users answer questions posed by other
users and best answers are selected manually either
by the asker or by all the participants in the thread.
The data generated by these sites has significant ad-
vantages over other web resources: (a) it has a high
growth rate and it is already abundant; (b) it cov-
ers a large number of topics, hence it offers a better
</bodyText>
<footnote confidence="0.811958">
3http://answers.yahoo.com
</footnote>
<figure confidence="0.93435175">
High
Quality
Low
Quality
</figure>
<page confidence="0.972219">
719
</page>
<note confidence="0.706702">
Proceedings of ACL-08: HLT, pages 719–727,
</note>
<page confidence="0.537615">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999745166666667">
approximation of open-domain content; and (c) it is
available for many languages. Community QA sites,
similar to FAQs, provide large number of question-
answer pairs. Nevertheless, this data has a signifi-
cant drawback: it has high variance of quality, i.e.,
answers range from very informative to completely
irrelevant or even abusive. Table 1 shows some ex-
amples of both high and low quality content.
In this paper we address the problem of answer
ranking for non-factoid questions from social media
content. Our research objectives focus on answering
the following two questions:
</bodyText>
<listItem confidence="0.6000601">
1. Is it possible to learn an answer ranking model
for complex questions from such noisy data?
This is an interesting question because a posi-
tive answer indicates that a plethora of training
data is readily available to QA researchers and
system developers.
2. Which features are most useful in this sce-
nario? Are similarity models as effective as
models that learn question-to-answer transfor-
mations? Does syntactic and semantic infor-
</listItem>
<bodyText confidence="0.98430048">
mation help? For generality, we focus only on
textual features extracted from the answer text
and we ignore all meta data information that is
not generally available.
Notice that we concentrate on one component of a
possible social-media QA system. In addition to
answer ranking, a complete system would have to
search for similar questions already answered (Jeon
et al., 2005), and rank content quality using ”social”
features such as the authority of users (Jeon et al.,
2006; Agichtein et al., 2008). This is not the focus of
our work: here we investigate the problem of learn-
ing an answer ranking model capable of dealing with
complex questions, using a large number of, possi-
ble noisy, question-answer pairs. By focusing exclu-
sively on textual content we increase the portability
of our approach to other collections where “social”
features might not available, e.g., Web search.
The paper is organized as follows. We describe
our approach, including all the features explored for
answer modeling, in Section 2. We introduce the
corpus used in our empirical analysis in Section 3.
We detail our experiments and analyze the results in
Section 4. We overview related work in Section 5
and conclude the paper in Section 6.
</bodyText>
<figure confidence="0.363867">
(class−conditional learning)
</figure>
<figureCaption confidence="0.999331">
Figure 1: System architecture.
</figureCaption>
<sectionHeader confidence="0.987971" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999860189189189">
The architecture of the QA system analyzed in the
paper, summarized in Figure 1, follows that of the
most successful TREC systems. The first com-
ponent, answer retrieval, extracts a set of candi-
date answers A for question Q from a large col-
lection of answers, C, provided by a community-
generated question-answering site. The retrieval
component uses a state-of-the-art information re-
trieval (IR) model to extract A given Q. Since
our focus is on exploring the usability of the an-
swer content, we do not perform retrieval by find-
ing similar questions already answered (Jeon et al.,
2005), i.e., our answer collection C contains only
the site’s answers without the corresponding ques-
tions answered.
The second component, answer ranking, assigns
to each answer AZ E A a score that represents
the likelihood that AZ is a correct answer for Q,
and ranks all answers in descending order of these
scores. The scoring function is a linear combina-
tion of four different classes of features (detailed in
Section 2.2). This function is the focus of the pa-
per. To answer our first research objective we will
compare the quality of the rankings provided by this
component against the rankings generated by the IR
model used for answer retrieval. To answer the sec-
ond research objective we will analyze the contri-
bution of the proposed feature set to this function.
Again, since our interest is in investigating the util-
ity of the answer textual content, we use only infor-
mation extracted from the answer text when learn-
ing the scoring function. We do not use any meta
information (e.g., answerer credibility, click counts,
etc.) (Agichtein et al., 2008; Jeon et al., 2006).
Our QA approach combines three types of ma-
chine learning methodologies (as highlighted in Fig-
ure 1): the answer retrieval component uses un-
</bodyText>
<figure confidence="0.998594722222222">
Similarity
Features
Density/Frequency
Features
Web Correlation
Features
Translation
Features
Q
(unsupervised)
Answer
Retrieval
Answer
Ranking
(discriminative learning)
Answer
Collection
Answers
</figure>
<page confidence="0.991796">
720
</page>
<bodyText confidence="0.9996052">
supervised IR models, the answer ranking is im-
plemented using discriminative learning, and fi-
nally, some of the ranking features are produced
by question-to-answer translation models, which use
class-conditional learning.
</bodyText>
<subsectionHeader confidence="0.96941">
2.1 Ranking Model
</subsectionHeader>
<bodyText confidence="0.999805785714286">
Learning with user-generated content can involve
arbitrarily large amounts of data. For this reason
we choose as a ranking algorithm the Perceptron
which is both accurate and efficient and can be
trained with online protocols. Specifically, we im-
plement the ranking Perceptron proposed by Shen
and Joshi (2005), which reduces the ranking prob-
lem to a binary classification problem. The general
intuition is to exploit the pairwise preferences in-
duced from the data by training on pairs of patterns,
rather than independently on each pattern. Given a
weight vector α, the score for a pattern x (a candi-
date answer) is simply the inner product between the
pattern and the weight vector:
</bodyText>
<equation confidence="0.999814">
fα(x) = (x, α) (1)
</equation>
<bodyText confidence="0.999660166666667">
However, the error function depends on pairwise
scores. In training, for each pair (xi, xj) E A,
the score fα(xi − xj) is computed; note that if f
is an inner product fα(xi − xj) = fα(xi) − fα(xj).
Given a margin function g(i, j) and a positive rate T,
if fα(xi − xj) G g(i, j)T, an update is performed:
</bodyText>
<equation confidence="0.997852">
αt+1 = αt + (xi − xj)Tg(i,j) (2)
</equation>
<bodyText confidence="0.9999685">
By default we use g(i, j) = (1i− j 1), as a mar-
gin function, as suggested in (Shen and Joshi, 2005),
and find T empirically on development data. Given
that there are only two possible ranks in our set-
ting, this function only generates two possible val-
ues. For regularization purposes, we use as a final
model the average of all Perceptron models posited
during training (Freund and Schapire, 1999).
</bodyText>
<subsectionHeader confidence="0.960057">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999998">
In the scoring model we explore a rich set of features
inspired by several state-of-the-art QA systems. We
investigate how such features can be adapted and
combined for non-factoid answer ranking, and per-
form a comparative feature analysis using a signif-
icant amount of real-world data. For clarity, we
group the features into four sets: features that model
the similarity between questions and answers (FG1),
features that encode question-to-answer transfor-
mations using a translation model (FG2), features
that measure keyword density and frequency (FG3),
and features that measure the correlation between
question-answer pairs and other collections (FG4).
Wherever applicable, we explore different syntactic
and semantic representations of the textual content,
e.g., extracting the dependency-based representation
of the text or generalizing words to their WordNet
supersenses (WNSS) (Ciaramita and Altun, 2006).
We detail each of these feature groups next.
</bodyText>
<sectionHeader confidence="0.558193" genericHeader="method">
FG1: Similarity Features
</sectionHeader>
<bodyText confidence="0.999056032258064">
We measure the similarity between a question
Q and an answer A using the length-normalized
BM25 formula (Robertson and Walker, 1997). We
chose this similarity formula because, out of all the
IR models we tried, it provided the best ranking at
the output of the answer retrieval component. For
completeness we also include in the feature set the
value of the tf ·idf similarity measure. For both for-
mulas we use the implementations available in the
Terrier IR platform4 with the default parameters.
To understand the contribution of our syntactic
and semantic processors we compute the above sim-
ilarity features for five different representations of
the question and answer content:
Words (W) - this is the traditional IR view where the
text is seen as a bag of words.
Dependencies (D) - the text is represented as a bag
of binary syntactic dependencies. The relative syn-
tactic processor is detailed in Section 3. Dependen-
cies are fully lexicalized but unlabeled and we cur-
rently extract dependency paths of length 1, i.e., di-
rect head-modifier relations (this setup achieved the
best performance).
Generalized dependencies (Dg) - same as above, but
the words in dependencies are generalized to their
WNSS, if detected.
Bigrams (B) - the text is represented as a bag of bi-
grams (larger n-grams did not help). We added this
view for a fair analysis of the above syntactic views.
Generalized bigrams (Bg) - same as above, but the
words are generalized to their WNSS.
</bodyText>
<footnote confidence="0.978331">
4http://ir.dcs.gla.ac.uk/terrier
</footnote>
<page confidence="0.995489">
721
</page>
<bodyText confidence="0.9851525">
In all these representations we skip stop words
and normalize all words to their WordNet lemmas.
</bodyText>
<sectionHeader confidence="0.357083" genericHeader="method">
FG2: Translation Features
</sectionHeader>
<bodyText confidence="0.9999155">
Berger et al. (2000) showed that similarity-based
models are doomed to perform poorly for QA be-
cause they fail to “bridge the lexical chasm” be-
tween questions and answers. One way to address
this problem is to learn question-to-answer trans-
formations using a translation model (Berger et al.,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al., 2007). In our model, we in-
corporate this approach by adding the probability
that the question Q is a translation of the answer A,
P(Q|A), as a feature. This probability is computed
using IBM’s Model 1 (Brown et al., 1993):
</bodyText>
<equation confidence="0.9960808">
P(Q|A) = 11 P(q|A) (3)
qEQ
P(q|A) = (1 − A)P.l(q|A) + AP.l(q|C) (4)
P.l(q|A) = � (T(q|a)P.l(a|A)) (5)
aEA
</equation>
<bodyText confidence="0.999951684210526">
where the probability that the question term q is
generated from answer A, P(q|A), is smoothed us-
ing the prior probability that the term q is gen-
erated from the entire collection of answers C,
P,,tl(q|C). A is the smoothing parameter. P,,tl(q|C)
is computed using the maximum likelihood estima-
tor. P,,tl(q|A) is computed as the sum of the proba-
bilities that the question term q is a translation of an
answer term a, T(q|a), weighted by the probability
that a is generated from A. The translation table for
T(q|a) is computed using the EM-based algorithm
implemented in the GIZA++ toolkit5.
Similarly with the previous feature group, we
add translation-based features for the five differ-
ent text representations introduced above. By
moving beyond the bag-of-word representation we
hope to learn relevant transformations of structures,
e.g., from the “squeaky” —* “door” dependency to
“spray” &lt;-- “WD-40” in the Table 1 example.
</bodyText>
<listItem confidence="0.378132">
FG3: Density and Frequency Features
</listItem>
<bodyText confidence="0.9999028">
These features measure the density and frequency
of question terms in the answer text. Variants of
these features were used previously for either an-
swer or passage ranking in factoid QA (Moldovan
et al., 1999; Harabagiu et al., 2000).
</bodyText>
<footnote confidence="0.750753">
5http://www.fjoch.com/GIZA++.html
</footnote>
<bodyText confidence="0.99990552">
Same word sequence - computes the number of non-
stop question words that are recognized in the same
order in the answer.
Answer span - the largest distance (in words) be-
tween two non-stop question words in the answer.
Same sentence match - number of non-stop question
terms matched in a single sentence in the answer.
Overall match - number of non-stop question terms
matched in the complete answer.
These last two features are computed also for the
other four text representations previously introduced
(B, Bg, D, and Dg). Counting the number of
matched dependencies is essentially a simplified
tree kernel for QA (e.g., see (Moschitti et al.,
2007)) matching only trees of depth 2. Experiments
with full dependency tree kernels based on several
variants of the convolution kernels of Collins and
Duffy (2001) did not yield improvements. We con-
jecture that the mistakes of the syntactic parser may
be amplified in tree kernels, which consider an ex-
ponential number of sub-trees.
Informativeness - we model the amount of informa-
tion contained in the answer by counting the num-
ber of non-stop nouns, verbs, and adjectives in the
answer text that do not appear in the question.
</bodyText>
<sectionHeader confidence="0.425538" genericHeader="method">
FG4: Web Correlation Features
</sectionHeader>
<bodyText confidence="0.9983463">
Previous work has shown that the redundancy of
a large collection (e.g., the web) can be used for an-
swer validation (Brill et al., 2001; Magnini et al.,
2002). In the same spirit, we add features that mea-
sure the correlation between question-answer pairs
and large external collections:
Web correlation - we measure the correlation be-
tween the question-answer pair and the web using
the Corrected Conditional Probability (CCP) for-
mula of Magnini et al. (2002): CCP(Q, A) _
hits(Q + A)/(hits(Q) hits(A)2/3) where hits re-
turns the number of page hits from a search engine.
When a query returns zero hits we iteratively relax it
by dropping the keyword with the smallest priority.
Keyword priorities are assigned using the heuristics
of Moldovan et al. (1999).
Query-log correlation - as in (Ciaramita et al., 2008)
we also compute the correlation between question-
answer pairs and a search-engine query-log cor-
pus of more than 7.5 million queries, which shares
</bodyText>
<page confidence="0.976928">
722
</page>
<bodyText confidence="0.703212333333333">
roughly the same time stamp with the community-
generated question-answer corpus. We compute the
Pointwise Mutual Information (PMI) and Chi square
(x2) association measures between each question-
answer word pair in the query-log corpus. The
largest and the average values are included as fea-
tures, as well as the number of QA word pairs which
appear in the top 10, 5, and 1 percentile of the PMI
and x2 word pair rankings.
</bodyText>
<sectionHeader confidence="0.997574" genericHeader="method">
3 The Corpus
</sectionHeader>
<bodyText confidence="0.99307925">
The corpus is extracted from a sample of the U.S.
Yahoo! Answers logs. In this paper we focus on
the subset of advice or “how to” questions due to
their frequency and importance in social communi-
ties.6 To construct our corpus, we implemented the
following successive filtering steps:
Step 1: from the full corpus we keep only questions
that match the regular expression:
</bodyText>
<equation confidence="0.583071">
how (to|do|did|does|can|would|could|should)
</equation>
<bodyText confidence="0.970349038461539">
and have an answer selected as best either by
the asker or by the participants in the thread.
The outcome of this step is a set of 364,419
question-answer pairs.
Step 2: from the above corpus we remove the questions
and answers of obvious low quality. We im-
plement this filter with a simple heuristic by
keeping only questions and answers that have
at least 4 words each, out of which at least 1 is
a noun and at least 1 is a verb. This step filters
out questions like “How to be excellent?” and
answers such as “I don’t know”. The outcome
of this step forms our answer collection C. C
contains 142,627 question-answer pairs.7.
Arguably, all these filters could be improved. For
example, the first step can be replaced by a question
classifier (Li and Roth, 2005). Similarly, the second
step can be implemented with a statistical classifier
that ranks the quality of the content using both the
textual and non-textual information available in the
database (Jeon et al., 2006; Agichtein et al., 2008).
We plan to further investigate these issues which are
not the main object of this work.
6Nevertheless, the approach proposed here is independent
of the question type. We will explore answer ranking for other
non-factoid question types in future work.
</bodyText>
<footnote confidence="0.8964705">
7The data will be available through the Yahoo! Webscope
program (research-data-requests@yahoo-inc.com).
</footnote>
<bodyText confidence="0.999971444444444">
The data was processed as follows. The text was
split at the sentence level, tokenized and PoS tagged,
in the style of the Wall Street Journal Penn Tree-
Bank (Marcus et al., 1993). Each word was morpho-
logically simplified using the morphological func-
tions of the WordNet library8. Sentences were an-
notated with WNSS categories, using the tagger of
Ciaramita and Altun (2006)9, which annotates text
with a 46-label tagset. These tags, defined by Word-
Net lexicographers, provide a broad semantic cat-
egorization for nouns and verbs and include labels
for nouns such as food, animal, body and feeling,
and for verbs labels such as communication, con-
tact, and possession. Next, we parsed all sentences
with the dependency parser of Attardi et al. (2007)10.
It is important to realize that the output of all men-
tioned processing steps is noisy and contains plenty
of mistakes, since the data has huge variability in
terms of quality, style, genres, domains etc., and do-
main adaptation for the NLP tasks involved is still
an open problem (Dredze et al., 2007).
We used 60% of the questions for training, 20%
for development, and 20% for test. The candidate
answer set for a given question is composed by one
positive example, i.e., its corresponding best answer,
and as negative examples all the other answers re-
trieved in the top N by the retrieval component.
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999905">
We evaluate our results using two measures: mean
Precision at rank=1 (P@1) – i.e., the percentage of
questions with the correct answer on the first posi-
tion – and Mean Reciprocal Rank (MRR) – i.e., the
score of a question is 1/k, where k is the position
of the correct answer. We use as baseline the output
of our answer retrieval component (Figure 1). This
component uses the BM25 criterion, the highest per-
forming IR model in our experiments.
Table 2 lists the results obtained using this base-
line and our best model (“Ranking” in the table) on
the testing partition. Since we are interested in the
performance of the ranking model, we evaluate on
the subset of questions where the correct answer is
retrieved by answer retrieval in the top N answers
(similar to Ko et al. (2007)). In the table we report
</bodyText>
<footnote confidence="0.999621666666667">
8http://wordnet.princeton.edu
9sourceforge.net/projects/supersensetag
10http://sourceforge.net/projects/desr
</footnote>
<page confidence="0.994577">
723
</page>
<table confidence="0.998340166666667">
MRR P@1
N = 10 N = 15 N = 25 N = 50 N = 10 N = 15 N = 25 N = 50
recall@N 26.25% 29.04% 32.81% 38.09% 26.25% 29.04% 32.81% 38.09%
Baseline 61.33 56.12 50.31 43.74 45.94 41.48 36.74 31.66
Ranking 68.72f0.01 63.84f0.01 57.76f0.07 50.72f0.01 54.22f0.01 49.59f0.03 43.98f0.09 37.99f0.01
Improvement +12.04% +13.75% +14.80% +15.95% +18.02% +19.55% +19.70% +19.99%
</table>
<tableCaption confidence="0.997933">
Table 2: Overall results for the test partition.
</tableCaption>
<bodyText confidence="0.999905333333333">
results for several N values. For completeness, we
show the percentage of questions that match this cri-
terion in the “recall@N” row.
Our ranking model was tuned strictly on the de-
velopment set (i.e., feature selection and parame-
ters of the translation models). During training, the
presentation of the training instances is randomized,
which generates a randomized ranking algorithm.
We exploit this property to estimate the variance in
the results produced by each model and report the
average result over 10 trials together with an esti-
mate of the standard deviation.
The baseline result shows that, for N = 15,
BM25 alone can retrieve in first rank 41% of the
correct answers, and MRR tells us that the correct
answer is often found within the first three answers
(this is not so surprising if we remember that in this
configuration only questions with the correct answer
in the first 15 were kept for the experiment). The
baseline results are interesting because they indicate
that the problem is not hopelessly hard, but it is far
from trivial. In principle, we see much room for im-
provement over bag-of-word methods.
Next we see that learning a weighted combina-
tion of features yields consistently marked improve-
ments: for example, for N = 15, the best model
yields a 19% relative improvement in P@1 and 14%
in MRR. More importantly, the results indicate that
the model learned is stable: even though for the
model analyzed in Table 2 we used N = 15 in train-
ing, we measure approximately the same relative im-
provement as N increases during evaluation.
These results provide robust evidence that: (a) we
can use publicly available online QA collections to
investigate features for answer ranking without the
need for costly human evaluation, (b) we can exploit
large and noisy online QA collections to improve the
accuracy of answer ranking systems and (c) readily
available and scalable NLP technology can be used
</bodyText>
<figure confidence="0.731045217391304">
Iter. Feature Set MRR P@1
0 BM25(W) 56.06 41.12%
1 + translation(B9) 61.13 46.24%
2 + overall match(D) 62.50 48.34%
3 + translation(W) 63.00 49.08%
4 + query-log avg(x2) 63.50 49.63%
5 + answer span
normalized by A size 63.71 49.84%
6 + query-log max(PMI) 63.87 50.09%
7 + same word sequence 63.99 50.23%
8 + translation(B) 64.03 50.30%
9 + tfidf(W) 64.08 50.42%
10 + same sentence match(W) 64.10 50.42%
11 + informativeness:
verb count 64.18 50.36%
12 + tfidf(B) 64.22 50.36%
13 + same word sequence
normalized by Q size 64.33 50.54%
14 + query-log max(x2) 64.46 50.66%
15 + same sentence match(W)
normalized by Q size 64.55 50.78%
16 + query-log avg(PMI) 64.60 50.88%
17 + overall match(W) 64.65 50.91%
</figure>
<tableCaption confidence="0.995103">
Table 3: Summary of the model selection process.
</tableCaption>
<bodyText confidence="0.999863117647059">
to improve lexical matching and translation models.
In the remaining of this section we analyze the per-
formance of the different features.
Table 3 summarizes the outcome of our automatic
greedy feature selection process on the development
set. Where applicable, we show within parentheses
the text representation for the corresponding feature.
The process is initialized with a single feature that
replicates the baseline model (BM25 applied to the
bag-of-words (W) representation). The algorithm
incrementally adds to the feature set the feature that
provides the highest MRR improvement in the de-
velopment partition. The process stops when no fea-
tures yield any improvement. The table shows that,
while the features selected span all the four feature
groups introduced, the lion’s share is taken by the
translation features: approximately 60% of the MRR
</bodyText>
<page confidence="0.994291">
724
</page>
<table confidence="0.9963402">
W B Bg D Dg W + W + W + B + W+B+Bg
B B+Bg Bg + D D+Dg
FG1 (Similarity) 0 +1.06 -2.01 +0.84 -1.75 +1.06 +1.06 +1.06 +1.06
FG2 (Translation) +4.95 +4.73 +5.06 +4.63 +4.66 +5.80 +6.01 +6.36 +6.36
FG3 (Frequency) +2.24 +2.33 +2.39 +2.27 +2.41 +3.56 +3.56 +3.62 +3.62
</table>
<tableCaption confidence="0.999913">
Table 4: Contribution of NLP processors. Scores are MRR improvements on the development set.
</tableCaption>
<bodyText confidence="0.999977986666667">
improvement is achieved by these features. The fre-
quency/density features are responsible for approx-
imately 23% of the improvement. The rest is due
to the query-log correlation features. This indicates
that, even though translation models are the most
useful, it is worth exploring approaches that com-
bine several strategies for answer ranking.
Note that if some features do not appear in Table 3
it does not necessarily mean that they are useless.
In some cases such features are highly correlated
with features previously selected, which already ex-
ploited their signal. For example, most similarity
features (FG1) are correlated. Because BM25(W)
is part of the baseline model, the selection process
chooses another FG1 feature only much later (iter-
ation 9) when the model is significantly changed.
On the other hand, some features do not provide a
useful signal at all. A notable example in this class
is the web-based CCP feature, which was designed
originally for factoid answer validation and does not
adapt well to our problem. Because the length of
non-factoid answers is typically significantly larger
than in the factoid QA task, we have to discard a
large part of the query when computing hits(Q+A)
to reach non-zero counts. This means that the final
hit counts, hence the CCP value, are generally un-
correlated with the original (Q,A) tuple.
One interesting observation is that the first two
features chosen by our model selection process use
information from the NLP processors. The first cho-
sen feature is the translation probability computed
between the Bg question and answer representations
(bigrams with words generalized to their WNSS
tags). The second feature selected measures the
number of syntactic dependencies from the question
that are matched in the answer. These results pro-
vide empirical evidence that coarse semantic disam-
biguation and syntactic parsing have a positive con-
tribution to non-factoid QA, even in broad-coverage
noisy settings based on Web data.
The above observation deserves a more detailed
analysis. Table 4 shows the performance of our first
three feature groups when they are applied to each
of the five text representations or incremental com-
binations of representations. For each model cor-
responding to a table cell we use only the features
from the corresponding feature group and represen-
tation to avoid the correlation with features from
other groups. We generate each best model using
the same feature selection process described above.
The left part of Table 4 shows that, generally, the
models using representations that include the output
of our NLP processors (Bg, D and Dg) improve over
the baseline (FG1 and W).11 However, comparable
improvements can be obtained with the simpler bi-
gram representation (B). This indicates that, in terms
of individual contributions, our NLP processors can
be approximated with simpler n-gram models in this
task. Hence, is it fair to say that syntactic and se-
mantic analysis is useful for such Web QA tasks?
While the above analysis seems to suggest a neg-
ative answer, the right-hand side of Table 4 tells a
more interesting story. It shows that the NLP anal-
ysis provides complementary information to the n-
gram-based models. The best models for the FG2
and FG3 feature groups are obtained when combin-
ing the n-gram representations with the representa-
tions that use the output of the NLP processors (W +
B + Bg + D). The improvements are relatively small,
but remarkable (e.g., see FG2) if we take into ac-
count the significant scale of the evaluation. This
observation correlates well with the analysis shown
in Table 3, which shows that features using semantic
(Bg) and syntactic (D) representations contribute the
most on top of the IR model (BM25(W)).
</bodyText>
<footnote confidence="0.9964812">
11The exception to this rule are the models FG1(B9) and
FG1(D9). This is caused by the fact that the BM25 formula
is less forgiving with errors of the NLP processors (due to the
high idf scores assigned to bigrams and dependencies), and the
WNSS tagger is the least robust component in our pipeline.
</footnote>
<page confidence="0.996452">
725
</page>
<sectionHeader confidence="0.999907" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999963509433962">
Content from community-built question-answer
sites can be retrieved by searching for similar ques-
tions already answered (Jeon et al., 2005) and
ranked using meta-data information like answerer
authority (Jeon et al., 2006; Agichtein et al., 2008).
Here we show that the answer text can be success-
fully used to improve answer ranking quality. Our
method is complementary to the above approaches.
In fact, it is likely that an optimal retrieval engine
from social media should combine all these three
methodologies. Moreover, our approach might have
applications outside of social media (e.g., for open-
domain web-based QA), because the ranking model
built is based only on open-domain knowledge and
the analysis of textual content.
In the QA literature, answer ranking for non-
factoid questions has typically been performed by
learning question-to-answer transformations, either
using translation models (Berger et al., 2000; Sori-
cut and Brill, 2006) or by exploiting the redundancy
of the Web (Agichtein et al., 2001). Girju (2003) ex-
tracts non-factoid answers by searching for certain
semantic structures, e.g., causation relations as an-
swers to causation questions. In this paper we com-
bine several methodologies, including the above,
into a single model. This approach allowed us to per-
form a systematic feature analysis on a large-scale
real-world corpus and a comprehensive feature set.
Recent work has showed that structured retrieval
improves answer ranking for factoid questions:
Bilotti et al. (2007) showed that matching predicate-
argument frames constructed from the question and
the expected answer types improves answer ranking.
Cui et al. (2005) learned transformations of depen-
dency paths from questions to answers to improve
passage ranking. However, both approaches use
similarity models at their core because they require
the matching of the lexical elements in the search
structures. On the other hand, our approach al-
lows the learning of full transformations from ques-
tion structures to answer structures using translation
models applied to different text representations.
Our answer ranking framework is closest in spirit
to the system of Ko et al. (2007) or Higashinaka et
al. (2008). However, the former was applied only
to factoid QA and both are limited to similarity, re-
dundancy and gazetteer-based features. Our model
uses a larger feature set that includes correlation and
transformation-based features and five different con-
tent representations. Our evaluation is also carried
out on a larger scale. Our work is also related to that
of Riezler et al. (2007) where SMT-based query ex-
pansion methods are used on data from FAQ pages.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999979973684211">
In this work we described an answer ranking en-
gine for non-factoid questions built using a large
community-generated question-answer collection.
On one hand, this study shows that we can effec-
tively exploit large amounts of available Web data
to do research on NLP for non-factoid QA systems,
without any annotation or evaluation cost. This pro-
vides an excellent framework for large-scale experi-
mentation with various models that otherwise might
be hard to understand or evaluate. On the other hand,
we expect the outcome of this process to help sev-
eral applications, such as open-domain QA on the
Web and retrieval from social media. For example,
on the Web our ranking system could be combined
with a passage retrieval system to form a QA system
for complex questions. On social media, our system
should be combined with a component that searches
for similar questions already answered; this output
can possibly be filtered further by a content-quality
module that explores “social” features such as the
authority of users, etc.
We show that the best ranking performance
is obtained when several strategies are combined
into a single model. We obtain the best results
when similarity models are aggregated with features
that model question-to-answer transformations, fre-
quency and density of content, and correlation of
QA pairs with external collections. While the fea-
tures that model question-to-answer transformations
provide most benefits, we show that the combination
is crucial for improvement.
Lastly, we show that syntactic dependency pars-
ing and coarse semantic disambiguation yield a
small, yet statistically significant performance in-
crease on top of the traditional bag-of-words and
n-gram representation. We obtain these results us-
ing only off-the-shelf NLP processors that were not
adapted in any way for our task.
</bodyText>
<page confidence="0.997279">
726
</page>
<sectionHeader confidence="0.995832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948895833333">
G. Attardi, F. Dell’Orletta, M. Simi, A. Chanev and
M. Ciaramita. 2007. Multilingual Dependency Pars-
ing and Domain Adaptation using DeSR. Proc. of
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G.
Mishne. 2008. Finding High-Quality Content in So-
cial Media, with an Application to Community-based
Question Answering. Proc. of WSDM.
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning Search Engine Specific Query Transforma-
tions for Question Answering. Proc. of WWW.
A. Berger, R. Caruana, D. Cohn, D. Freytag, and V. Mit-
tal. 2000. Bridging the Lexical Chasm: Statistical
Approaches to Answer Finding. Proc. of SIGIR.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured Retrieval for Question Answering. Proc. of
SIGIR.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001.
Data-Intensive Question Answering. Proc. of TREC.
P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2).
M. Ciaramita and Y. Altun. 2006. Broad Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. Proc. of EMNLP.
M. Ciaramita, V. Murdock and V. Plachouras. 2008. Se-
mantic Associations for Contextual Advertising. 2008.
Journal ofElectronic Commerce Research - Special Is-
sue on Online Advertising and Sponsored Search, 9(1),
pp.1-15.
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proc. of NIPS 2001.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion Answering Passage Retrieval Using Dependency
Relations. Proc. of SIGIR.
M. Dredze, J. Blitzer, P. Pratim Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly Hard
Domain Adaptation for Parsing. In Proc. of EMNLP-
CoNLL 2007 Shared Task.
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. Proc. of ACL.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37, pp. 277-296.
R. Girju. 2003. Automatic Detection of Causal Relations
for Question Answering. Proc. of ACL, Workshop on
Multilingual Summarization and Question Answering.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P.
Morarescu. 2000. Falcon: Boosting Knowledge for
Answer Engines. Proc. of TREC.
R. Higashinaka and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. Proc. of IJC-
NLP.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding Simi-
lar Questions in Large Question and Answer Archives.
Proc. of CIKM.
J. Jeon, W. B. Croft, J. H. Lee, and S. Park. 2006. A
Framework to Predict the Quality of Answers with
Non-Textual Features. Proc. of SIGIR.
J. Ko, T. Mitamura, and E. Nyberg. 2007. Language-
independent Probabilistic Answer Ranking. for Ques-
tion Answering. Proc. of ACL.
X. Li and D. Roth. 2005. Learning Question Classifiers:
The Role of Semantic Information. Natural Language
Engineering.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Comparing Statistical and Content-Based Techniques
for Answer Validation on the Web. Proc. of the VIII
Convegno AI*IA.
M.P. Marcus, B. Santorini and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn TreeBank. Computational Linguis-
tics, 19(2), pp. 313-330.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R.
Goodrum, R. Girju, and V. Rus. 1999. LASSO - A
Tool for Surfing the Answer Net. Proc. of TREC.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. Proc. of
ACL.
S. Robertson and S. Walker. 1997. On relevance Weights
with Little Relevance Information. Proc. of SIGIR.
R. Soricut and E. Brill. 2006. Automatic Question An-
swering Using the Web: Beyond the Factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9(2).
L. Shen and A. Joshi. 2005. Ranking and Reranking
with Perceptron, Machine Learning. Special Issue on
Learning in Speech and Language Technologies, 60(1-
3), pp. 73-96.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal
and Y. Liu. 2007. Statistical Machine Translation
for Query Expansion in Answer Retrieval. In Proc.
of ACL.
</reference>
<page confidence="0.997168">
727
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867367">
<title confidence="0.99968">Learning to Rank Answers on Large Online QA Collections</title>
<author confidence="0.997893">Mihai Surdeanu</author>
<author confidence="0.997893">Massimiliano Ciaramita</author>
<author confidence="0.997893">Hugo Zaragoza</author>
<affiliation confidence="0.896301">Barcelona Media Innovation Center, Yahoo! Research Barcelona</affiliation>
<abstract confidence="0.997383333333334">This work describes an answer ranking engine for non-factoid questions built using a large online community-generated question-answer collection (Yahoo! Answers). We show how such collections may be used to effectively set up large supervised learning experiments. Furthermore we investigate a wide range of feature types, some exploiting NLP processors, and demonstrate that using them in combination leads to considerable improvements in accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
<author>M Simi</author>
<author>A Chanev</author>
<author>M Ciaramita</author>
</authors>
<title>Multilingual Dependency Parsing and Domain Adaptation using DeSR.</title>
<date>2007</date>
<booktitle>Proc. of CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<marker>Attardi, Dell’Orletta, Simi, Chanev, Ciaramita, 2007</marker>
<rawString>G. Attardi, F. Dell’Orletta, M. Simi, A. Chanev and M. Ciaramita. 2007. Multilingual Dependency Parsing and Domain Adaptation using DeSR. Proc. of CoNLL Shared Task Session of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>C Castillo</author>
<author>D Donato</author>
<author>A Gionis</author>
<author>G Mishne</author>
</authors>
<title>Finding High-Quality Content in Social Media, with an Application to Community-based Question Answering.</title>
<date>2008</date>
<booktitle>Proc. of WSDM.</booktitle>
<contexts>
<context position="4779" citStr="Agichtein et al., 2008" startWordPosition="738" endWordPosition="741">similarity models as effective as models that learn question-to-answer transformations? Does syntactic and semantic information help? For generality, we focus only on textual features extracted from the answer text and we ignore all meta data information that is not generally available. Notice that we concentrate on one component of a possible social-media QA system. In addition to answer ranking, a complete system would have to search for similar questions already answered (Jeon et al., 2005), and rank content quality using ”social” features such as the authority of users (Jeon et al., 2006; Agichtein et al., 2008). This is not the focus of our work: here we investigate the problem of learning an answer ranking model capable of dealing with complex questions, using a large number of, possible noisy, question-answer pairs. By focusing exclusively on textual content we increase the portability of our approach to other collections where “social” features might not available, e.g., Web search. The paper is organized as follows. We describe our approach, including all the features explored for answer modeling, in Section 2. We introduce the corpus used in our empirical analysis in Section 3. We detail our ex</context>
<context position="7211" citStr="Agichtein et al., 2008" startWordPosition="1143" endWordPosition="1146">tion is the focus of the paper. To answer our first research objective we will compare the quality of the rankings provided by this component against the rankings generated by the IR model used for answer retrieval. To answer the second research objective we will analyze the contribution of the proposed feature set to this function. Again, since our interest is in investigating the utility of the answer textual content, we use only information extracted from the answer text when learning the scoring function. We do not use any meta information (e.g., answerer credibility, click counts, etc.) (Agichtein et al., 2008; Jeon et al., 2006). Our QA approach combines three types of machine learning methodologies (as highlighted in Figure 1): the answer retrieval component uses unSimilarity Features Density/Frequency Features Web Correlation Features Translation Features Q (unsupervised) Answer Retrieval Answer Ranking (discriminative learning) Answer Collection Answers 720 supervised IR models, the answer ranking is implemented using discriminative learning, and finally, some of the ranking features are produced by question-to-answer translation models, which use class-conditional learning. 2.1 Ranking Model L</context>
<context position="17814" citStr="Agichtein et al., 2008" startWordPosition="2881" endWordPosition="2884">of which at least 1 is a noun and at least 1 is a verb. This step filters out questions like “How to be excellent?” and answers such as “I don’t know”. The outcome of this step forms our answer collection C. C contains 142,627 question-answer pairs.7. Arguably, all these filters could be improved. For example, the first step can be replaced by a question classifier (Li and Roth, 2005). Similarly, the second step can be implemented with a statistical classifier that ranks the quality of the content using both the textual and non-textual information available in the database (Jeon et al., 2006; Agichtein et al., 2008). We plan to further investigate these issues which are not the main object of this work. 6Nevertheless, the approach proposed here is independent of the question type. We will explore answer ranking for other non-factoid question types in future work. 7The data will be available through the Yahoo! Webscope program (research-data-requests@yahoo-inc.com). The data was processed as follows. The text was split at the sentence level, tokenized and PoS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus et al., 1993). Each word was morphologically simplified using the morphologica</context>
<context position="29065" citStr="Agichtein et al., 2008" startWordPosition="4734" endWordPosition="4737">ons contribute the most on top of the IR model (BM25(W)). 11The exception to this rule are the models FG1(B9) and FG1(D9). This is caused by the fact that the BM25 formula is less forgiving with errors of the NLP processors (due to the high idf scores assigned to bigrams and dependencies), and the WNSS tagger is the least robust component in our pipeline. 725 5 Related Work Content from community-built question-answer sites can be retrieved by searching for similar questions already answered (Jeon et al., 2005) and ranked using meta-data information like answerer authority (Jeon et al., 2006; Agichtein et al., 2008). Here we show that the answer text can be successfully used to improve answer ranking quality. Our method is complementary to the above approaches. In fact, it is likely that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed by learning question-to-a</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. 2008. Finding High-Quality Content in Social Media, with an Application to Community-based Question Answering. Proc. of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>S Lawrence</author>
<author>L Gravano</author>
</authors>
<title>Learning Search Engine Specific Query Transformations for Question Answering.</title>
<date>2001</date>
<booktitle>Proc. of WWW.</booktitle>
<contexts>
<context position="29834" citStr="Agichtein et al., 2001" startWordPosition="4854" endWordPosition="4857">s. In fact, it is likely that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed by learning question-to-answer transformations, either using translation models (Berger et al., 2000; Soricut and Brill, 2006) or by exploiting the redundancy of the Web (Agichtein et al., 2001). Girju (2003) extracts non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that matching predicateargument frames constructed from the question and the expected answer types i</context>
</contexts>
<marker>Agichtein, Lawrence, Gravano, 2001</marker>
<rawString>E. Agichtein, S. Lawrence, and L. Gravano. 2001. Learning Search Engine Specific Query Transformations for Question Answering. Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>R Caruana</author>
<author>D Cohn</author>
<author>D Freytag</author>
<author>V Mittal</author>
</authors>
<title>Bridging the Lexical Chasm: Statistical Approaches to Answer Finding.</title>
<date>2000</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="11893" citStr="Berger et al. (2000)" startWordPosition="1895" endWordPosition="1898">irect head-modifier relations (this setup achieved the best performance). Generalized dependencies (Dg) - same as above, but the words in dependencies are generalized to their WNSS, if detected. Bigrams (B) - the text is represented as a bag of bigrams (larger n-grams did not help). We added this view for a fair analysis of the above syntactic views. Generalized bigrams (Bg) - same as above, but the words are generalized to their WNSS. 4http://ir.dcs.gla.ac.uk/terrier 721 In all these representations we skip stop words and normalize all words to their WordNet lemmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P(Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P(Q|A) = 11 P(q|A) (3) qEQ P(q|</context>
<context position="29740" citStr="Berger et al., 2000" startWordPosition="4837" endWordPosition="4840">used to improve answer ranking quality. Our method is complementary to the above approaches. In fact, it is likely that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed by learning question-to-answer transformations, either using translation models (Berger et al., 2000; Soricut and Brill, 2006) or by exploiting the redundancy of the Web (Agichtein et al., 2001). Girju (2003) extracts non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that m</context>
</contexts>
<marker>Berger, Caruana, Cohn, Freytag, Mittal, 2000</marker>
<rawString>A. Berger, R. Caruana, D. Cohn, D. Freytag, and V. Mittal. 2000. Bridging the Lexical Chasm: Statistical Approaches to Answer Finding. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilotti</author>
<author>P Ogilvie</author>
<author>J Callan</author>
<author>E Nyberg</author>
</authors>
<title>Structured Retrieval for Question Answering.</title>
<date>2007</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="30326" citStr="Bilotti et al. (2007)" startWordPosition="4928" endWordPosition="4931">slation models (Berger et al., 2000; Soricut and Brill, 2006) or by exploiting the redundancy of the Web (Agichtein et al., 2001). Girju (2003) extracts non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that matching predicateargument frames constructed from the question and the expected answer types improves answer ranking. Cui et al. (2005) learned transformations of dependency paths from questions to answers to improve passage ranking. However, both approaches use similarity models at their core because they require the matching of the lexical elements in the search structures. On the other hand, our approach allows the learning of full transformations from question structures to answer structures using translation models applied to different text representations. Our answer rankin</context>
</contexts>
<marker>Bilotti, Ogilvie, Callan, Nyberg, 2007</marker>
<rawString>M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007. Structured Retrieval for Question Answering. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin</author>
<author>M Banko</author>
<author>S Dumais</author>
<author>A Ng</author>
</authors>
<title>Data-Intensive Question Answering.</title>
<date>2001</date>
<booktitle>Proc. of TREC.</booktitle>
<contexts>
<context position="15140" citStr="Brill et al., 2001" startWordPosition="2431" endWordPosition="2434">ernels based on several variants of the convolution kernels of Collins and Duffy (2001) did not yield improvements. We conjecture that the mistakes of the syntactic parser may be amplified in tree kernels, which consider an exponential number of sub-trees. Informativeness - we model the amount of information contained in the answer by counting the number of non-stop nouns, verbs, and adjectives in the answer text that do not appear in the question. FG4: Web Correlation Features Previous work has shown that the redundancy of a large collection (e.g., the web) can be used for answer validation (Brill et al., 2001; Magnini et al., 2002). In the same spirit, we add features that measure the correlation between question-answer pairs and large external collections: Web correlation - we measure the correlation between the question-answer pair and the web using the Corrected Conditional Probability (CCP) formula of Magnini et al. (2002): CCP(Q, A) _ hits(Q + A)/(hits(Q) hits(A)2/3) where hits returns the number of page hits from a search engine. When a query returns zero hits we iteratively relax it by dropping the keyword with the smallest priority. Keyword priorities are assigned using the heuristics of M</context>
</contexts>
<marker>Brill, Lin, Banko, Dumais, Ng, 2001</marker>
<rawString>E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. Data-Intensive Question Answering. Proc. of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12460" citStr="Brown et al., 1993" startWordPosition="1991" endWordPosition="1994">mmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P(Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P(Q|A) = 11 P(q|A) (3) qEQ P(q|A) = (1 − A)P.l(q|A) + AP.l(q|C) (4) P.l(q|A) = � (T(q|a)P.l(a|A)) (5) aEA where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, P,,tl(q|C). A is the smoothing parameter. P,,tl(q|C) is computed using the maximum likelihood estimator. P,,tl(q|A) is computed as the sum of the probabilities that the question term q is a translation of an answer term a, T(q|a), weighted by the probability that a is generated from A. The translat</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger.</title>
<date>2006</date>
<booktitle>Proc. of EMNLP.</booktitle>
<contexts>
<context position="10177" citStr="Ciaramita and Altun, 2006" startWordPosition="1613" endWordPosition="1616">or clarity, we group the features into four sets: features that model the similarity between questions and answers (FG1), features that encode question-to-answer transformations using a translation model (FG2), features that measure keyword density and frequency (FG3), and features that measure the correlation between question-answer pairs and other collections (FG4). Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). We detail each of these feature groups next. FG1: Similarity Features We measure the similarity between a question Q and an answer A using the length-normalized BM25 formula (Robertson and Walker, 1997). We chose this similarity formula because, out of all the IR models we tried, it provided the best ranking at the output of the answer retrieval component. For completeness we also include in the feature set the value of the tf ·idf similarity measure. For both formulas we use the implementations available in the Terrier IR platform4 with the default parameters. To understand the contribution</context>
<context position="18544" citStr="Ciaramita and Altun (2006)" startWordPosition="2995" endWordPosition="2998"> the approach proposed here is independent of the question type. We will explore answer ranking for other non-factoid question types in future work. 7The data will be available through the Yahoo! Webscope program (research-data-requests@yahoo-inc.com). The data was processed as follows. The text was split at the sentence level, tokenized and PoS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus et al., 1993). Each word was morphologically simplified using the morphological functions of the WordNet library8. Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006)9, which annotates text with a 46-label tagset. These tags, defined by WordNet lexicographers, provide a broad semantic categorization for nouns and verbs and include labels for nouns such as food, animal, body and feeling, and for verbs labels such as communication, contact, and possession. Next, we parsed all sentences with the dependency parser of Attardi et al. (2007)10. It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes, since the data has huge variability in terms of quality, style, genres, domains etc., and domain adapta</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger. Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>V Murdock</author>
<author>V Plachouras</author>
</authors>
<title>Semantic Associations for Contextual Advertising.</title>
<date>2008</date>
<journal>Journal ofElectronic Commerce Research - Special Issue on Online Advertising and Sponsored Search,</journal>
<volume>9</volume>
<issue>1</issue>
<pages>1--15</pages>
<contexts>
<context position="15817" citStr="Ciaramita et al., 2008" startWordPosition="2541" endWordPosition="2544">atures that measure the correlation between question-answer pairs and large external collections: Web correlation - we measure the correlation between the question-answer pair and the web using the Corrected Conditional Probability (CCP) formula of Magnini et al. (2002): CCP(Q, A) _ hits(Q + A)/(hits(Q) hits(A)2/3) where hits returns the number of page hits from a search engine. When a query returns zero hits we iteratively relax it by dropping the keyword with the smallest priority. Keyword priorities are assigned using the heuristics of Moldovan et al. (1999). Query-log correlation - as in (Ciaramita et al., 2008) we also compute the correlation between questionanswer pairs and a search-engine query-log corpus of more than 7.5 million queries, which shares 722 roughly the same time stamp with the communitygenerated question-answer corpus. We compute the Pointwise Mutual Information (PMI) and Chi square (x2) association measures between each questionanswer word pair in the query-log corpus. The largest and the average values are included as features, as well as the number of QA word pairs which appear in the top 10, 5, and 1 percentile of the PMI and x2 word pair rankings. 3 The Corpus The corpus is ext</context>
</contexts>
<marker>Ciaramita, Murdock, Plachouras, 2008</marker>
<rawString>M. Ciaramita, V. Murdock and V. Plachouras. 2008. Semantic Associations for Contextual Advertising. 2008. Journal ofElectronic Commerce Research - Special Issue on Online Advertising and Sponsored Search, 9(1), pp.1-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>Proc. of NIPS</booktitle>
<contexts>
<context position="14609" citStr="Collins and Duffy (2001)" startWordPosition="2339" endWordPosition="2342">p question words in the answer. Same sentence match - number of non-stop question terms matched in a single sentence in the answer. Overall match - number of non-stop question terms matched in the complete answer. These last two features are computed also for the other four text representations previously introduced (B, Bg, D, and Dg). Counting the number of matched dependencies is essentially a simplified tree kernel for QA (e.g., see (Moschitti et al., 2007)) matching only trees of depth 2. Experiments with full dependency tree kernels based on several variants of the convolution kernels of Collins and Duffy (2001) did not yield improvements. We conjecture that the mistakes of the syntactic parser may be amplified in tree kernels, which consider an exponential number of sub-trees. Informativeness - we model the amount of information contained in the answer by counting the number of non-stop nouns, verbs, and adjectives in the answer text that do not appear in the question. FG4: Web Correlation Features Previous work has shown that the redundancy of a large collection (e.g., the web) can be used for answer validation (Brill et al., 2001; Magnini et al., 2002). In the same spirit, we add features that mea</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. Proc. of NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>R Sun</author>
<author>K Li</author>
<author>M Kan</author>
<author>T Chua</author>
</authors>
<title>Question Answering Passage Retrieval Using Dependency Relations.</title>
<date>2005</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="30475" citStr="Cui et al. (2005)" startWordPosition="4950" endWordPosition="4953">non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that matching predicateargument frames constructed from the question and the expected answer types improves answer ranking. Cui et al. (2005) learned transformations of dependency paths from questions to answers to improve passage ranking. However, both approaches use similarity models at their core because they require the matching of the lexical elements in the search structures. On the other hand, our approach allows the learning of full transformations from question structures to answer structures using translation models applied to different text representations. Our answer ranking framework is closest in spirit to the system of Ko et al. (2007) or Higashinaka et al. (2008). However, the former was applied only to factoid QA a</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Question Answering Passage Retrieval Using Dependency Relations. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dredze</author>
<author>J Blitzer</author>
<author>P Pratim Talukdar</author>
<author>K Ganchev</author>
<author>J Graca</author>
<author>F Pereira</author>
</authors>
<title>Frustratingly Hard Domain Adaptation for Parsing.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL</booktitle>
<note>Shared Task.</note>
<contexts>
<context position="19222" citStr="Dredze et al., 2007" startWordPosition="3109" endWordPosition="3112">, defined by WordNet lexicographers, provide a broad semantic categorization for nouns and verbs and include labels for nouns such as food, animal, body and feeling, and for verbs labels such as communication, contact, and possession. Next, we parsed all sentences with the dependency parser of Attardi et al. (2007)10. It is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes, since the data has huge variability in terms of quality, style, genres, domains etc., and domain adaptation for the NLP tasks involved is still an open problem (Dredze et al., 2007). We used 60% of the questions for training, 20% for development, and 20% for test. The candidate answer set for a given question is composed by one positive example, i.e., its corresponding best answer, and as negative examples all the other answers retrieved in the top N by the retrieval component. 4 Experiments We evaluate our results using two measures: mean Precision at rank=1 (P@1) – i.e., the percentage of questions with the correct answer on the first position – and Mean Reciprocal Rank (MRR) – i.e., the score of a question is 1/k, where k is the position of the correct answer. We use </context>
</contexts>
<marker>Dredze, Blitzer, Talukdar, Ganchev, Graca, Pereira, 2007</marker>
<rawString>M. Dredze, J. Blitzer, P. Pratim Talukdar, K. Ganchev, J. Graca, and F. Pereira. 2007. Frustratingly Hard Domain Adaptation for Parsing. In Proc. of EMNLPCoNLL 2007 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A Noisy-Channel Approach to Question Answering.</title>
<date>2003</date>
<booktitle>Proc. of ACL.</booktitle>
<contexts>
<context position="12196" citStr="Echihabi and Marcu, 2003" startWordPosition="1944" endWordPosition="1947">iew for a fair analysis of the above syntactic views. Generalized bigrams (Bg) - same as above, but the words are generalized to their WNSS. 4http://ir.dcs.gla.ac.uk/terrier 721 In all these representations we skip stop words and normalize all words to their WordNet lemmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P(Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P(Q|A) = 11 P(q|A) (3) qEQ P(q|A) = (1 − A)P.l(q|A) + AP.l(q|C) (4) P.l(q|A) = � (T(q|a)P.l(a|A)) (5) aEA where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, P,,tl(q|C). A is the smoothing para</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. 2003. A Noisy-Channel Approach to Question Answering. Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<journal>Machine Learning,</journal>
<volume>37</volume>
<pages>277--296</pages>
<contexts>
<context position="9248" citStr="Freund and Schapire, 1999" startWordPosition="1481" endWordPosition="1484">α(xi − xj) is computed; note that if f is an inner product fα(xi − xj) = fα(xi) − fα(xj). Given a margin function g(i, j) and a positive rate T, if fα(xi − xj) G g(i, j)T, an update is performed: αt+1 = αt + (xi − xj)Tg(i,j) (2) By default we use g(i, j) = (1i− j 1), as a margin function, as suggested in (Shen and Joshi, 2005), and find T empirically on development data. Given that there are only two possible ranks in our setting, this function only generates two possible values. For regularization purposes, we use as a final model the average of all Perceptron models posited during training (Freund and Schapire, 1999). 2.2 Features In the scoring model we explore a rich set of features inspired by several state-of-the-art QA systems. We investigate how such features can be adapted and combined for non-factoid answer ranking, and perform a comparative feature analysis using a significant amount of real-world data. For clarity, we group the features into four sets: features that model the similarity between questions and answers (FG1), features that encode question-to-answer transformations using a translation model (FG2), features that measure keyword density and frequency (FG3), and features that measure t</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R.E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37, pp. 277-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic Detection of Causal Relations for Question Answering.</title>
<date>2003</date>
<booktitle>Proc. of ACL, Workshop on Multilingual Summarization and Question</booktitle>
<contexts>
<context position="29848" citStr="Girju (2003)" startWordPosition="4858" endWordPosition="4859">that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed by learning question-to-answer transformations, either using translation models (Berger et al., 2000; Soricut and Brill, 2006) or by exploiting the redundancy of the Web (Agichtein et al., 2001). Girju (2003) extracts non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that matching predicateargument frames constructed from the question and the expected answer types improves answer</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic Detection of Causal Relations for Question Answering. Proc. of ACL, Workshop on Multilingual Summarization and Question Answering. S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P. Morarescu. 2000. Falcon: Boosting Knowledge for Answer Engines. Proc. of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Higashinaka</author>
<author>H Isozaki</author>
</authors>
<title>Corpus-based Question Answering for why-Questions.</title>
<date>2008</date>
<booktitle>Proc. of IJCNLP.</booktitle>
<contexts>
<context position="2159" citStr="Higashinaka and Isozaki, 2008" startWordPosition="318" endWordPosition="322">ing data is available for this problem, most automated systems train either 1http://trec.nist.gov 2http://www.clef-campaign.org Q: How do you quiet a squeaky door? A: Spray WD-40 directly onto the hinges of the door. Open and close the door several times. Remove hinges if the door still squeaks. Remove any rust, dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back, open and close door several times again. Q: How to extract html tags from an html documents with c++? A: very carefully Table 1: Sample content from Yahoo! Answers. on small hand-annotated corpora built in house (Higashinaka and Isozaki, 2008) or on question-answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut and Brill, 2006). None of these situations is ideal: the cost of building the training corpus in the former setup is high; in the latter scenario the data tends to be domain-specific, hence unsuitable for the learning of open-domain models. On the other hand, recent years have seen an explosion of user-generated content (or social media). Of particular interest in our context are communitydriven question-answering sites, such as Yahoo! Answers3, where users answer questions posed by</context>
</contexts>
<marker>Higashinaka, Isozaki, 2008</marker>
<rawString>R. Higashinaka and H. Isozaki. 2008. Corpus-based Question Answering for why-Questions. Proc. of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
</authors>
<title>Finding Similar Questions in Large Question and Answer Archives.</title>
<date>2005</date>
<booktitle>Proc. of CIKM.</booktitle>
<contexts>
<context position="4654" citStr="Jeon et al., 2005" startWordPosition="717" endWordPosition="720">a is readily available to QA researchers and system developers. 2. Which features are most useful in this scenario? Are similarity models as effective as models that learn question-to-answer transformations? Does syntactic and semantic information help? For generality, we focus only on textual features extracted from the answer text and we ignore all meta data information that is not generally available. Notice that we concentrate on one component of a possible social-media QA system. In addition to answer ranking, a complete system would have to search for similar questions already answered (Jeon et al., 2005), and rank content quality using ”social” features such as the authority of users (Jeon et al., 2006; Agichtein et al., 2008). This is not the focus of our work: here we investigate the problem of learning an answer ranking model capable of dealing with complex questions, using a large number of, possible noisy, question-answer pairs. By focusing exclusively on textual content we increase the portability of our approach to other collections where “social” features might not available, e.g., Web search. The paper is organized as follows. We describe our approach, including all the features expl</context>
<context position="6158" citStr="Jeon et al., 2005" startWordPosition="965" endWordPosition="968"> System architecture. 2 Approach The architecture of the QA system analyzed in the paper, summarized in Figure 1, follows that of the most successful TREC systems. The first component, answer retrieval, extracts a set of candidate answers A for question Q from a large collection of answers, C, provided by a communitygenerated question-answering site. The retrieval component uses a state-of-the-art information retrieval (IR) model to extract A given Q. Since our focus is on exploring the usability of the answer content, we do not perform retrieval by finding similar questions already answered (Jeon et al., 2005), i.e., our answer collection C contains only the site’s answers without the corresponding questions answered. The second component, answer ranking, assigns to each answer AZ E A a score that represents the likelihood that AZ is a correct answer for Q, and ranks all answers in descending order of these scores. The scoring function is a linear combination of four different classes of features (detailed in Section 2.2). This function is the focus of the paper. To answer our first research objective we will compare the quality of the rankings provided by this component against the rankings genera</context>
<context position="28958" citStr="Jeon et al., 2005" startWordPosition="4718" endWordPosition="4721">nalysis shown in Table 3, which shows that features using semantic (Bg) and syntactic (D) representations contribute the most on top of the IR model (BM25(W)). 11The exception to this rule are the models FG1(B9) and FG1(D9). This is caused by the fact that the BM25 formula is less forgiving with errors of the NLP processors (due to the high idf scores assigned to bigrams and dependencies), and the WNSS tagger is the least robust component in our pipeline. 725 5 Related Work Content from community-built question-answer sites can be retrieved by searching for similar questions already answered (Jeon et al., 2005) and ranked using meta-data information like answerer authority (Jeon et al., 2006; Agichtein et al., 2008). Here we show that the answer text can be successfully used to improve answer ranking quality. Our method is complementary to the above approaches. In fact, it is likely that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding Similar Questions in Large Question and Answer Archives. Proc. of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
<author>S Park</author>
</authors>
<title>A Framework to Predict the Quality of Answers with Non-Textual Features.</title>
<date>2006</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="4754" citStr="Jeon et al., 2006" startWordPosition="734" endWordPosition="737">this scenario? Are similarity models as effective as models that learn question-to-answer transformations? Does syntactic and semantic information help? For generality, we focus only on textual features extracted from the answer text and we ignore all meta data information that is not generally available. Notice that we concentrate on one component of a possible social-media QA system. In addition to answer ranking, a complete system would have to search for similar questions already answered (Jeon et al., 2005), and rank content quality using ”social” features such as the authority of users (Jeon et al., 2006; Agichtein et al., 2008). This is not the focus of our work: here we investigate the problem of learning an answer ranking model capable of dealing with complex questions, using a large number of, possible noisy, question-answer pairs. By focusing exclusively on textual content we increase the portability of our approach to other collections where “social” features might not available, e.g., Web search. The paper is organized as follows. We describe our approach, including all the features explored for answer modeling, in Section 2. We introduce the corpus used in our empirical analysis in Se</context>
<context position="7231" citStr="Jeon et al., 2006" startWordPosition="1147" endWordPosition="1150"> paper. To answer our first research objective we will compare the quality of the rankings provided by this component against the rankings generated by the IR model used for answer retrieval. To answer the second research objective we will analyze the contribution of the proposed feature set to this function. Again, since our interest is in investigating the utility of the answer textual content, we use only information extracted from the answer text when learning the scoring function. We do not use any meta information (e.g., answerer credibility, click counts, etc.) (Agichtein et al., 2008; Jeon et al., 2006). Our QA approach combines three types of machine learning methodologies (as highlighted in Figure 1): the answer retrieval component uses unSimilarity Features Density/Frequency Features Web Correlation Features Translation Features Q (unsupervised) Answer Retrieval Answer Ranking (discriminative learning) Answer Collection Answers 720 supervised IR models, the answer ranking is implemented using discriminative learning, and finally, some of the ranking features are produced by question-to-answer translation models, which use class-conditional learning. 2.1 Ranking Model Learning with user-ge</context>
<context position="17789" citStr="Jeon et al., 2006" startWordPosition="2877" endWordPosition="2880"> 4 words each, out of which at least 1 is a noun and at least 1 is a verb. This step filters out questions like “How to be excellent?” and answers such as “I don’t know”. The outcome of this step forms our answer collection C. C contains 142,627 question-answer pairs.7. Arguably, all these filters could be improved. For example, the first step can be replaced by a question classifier (Li and Roth, 2005). Similarly, the second step can be implemented with a statistical classifier that ranks the quality of the content using both the textual and non-textual information available in the database (Jeon et al., 2006; Agichtein et al., 2008). We plan to further investigate these issues which are not the main object of this work. 6Nevertheless, the approach proposed here is independent of the question type. We will explore answer ranking for other non-factoid question types in future work. 7The data will be available through the Yahoo! Webscope program (research-data-requests@yahoo-inc.com). The data was processed as follows. The text was split at the sentence level, tokenized and PoS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus et al., 1993). Each word was morphologically simplifi</context>
<context position="29040" citStr="Jeon et al., 2006" startWordPosition="4730" endWordPosition="4733">ic (D) representations contribute the most on top of the IR model (BM25(W)). 11The exception to this rule are the models FG1(B9) and FG1(D9). This is caused by the fact that the BM25 formula is less forgiving with errors of the NLP processors (due to the high idf scores assigned to bigrams and dependencies), and the WNSS tagger is the least robust component in our pipeline. 725 5 Related Work Content from community-built question-answer sites can be retrieved by searching for similar questions already answered (Jeon et al., 2005) and ranked using meta-data information like answerer authority (Jeon et al., 2006; Agichtein et al., 2008). Here we show that the answer text can be successfully used to improve answer ranking quality. Our method is complementary to the above approaches. In fact, it is likely that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed </context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>J. Jeon, W. B. Croft, J. H. Lee, and S. Park. 2006. A Framework to Predict the Quality of Answers with Non-Textual Features. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ko</author>
<author>T Mitamura</author>
<author>E Nyberg</author>
</authors>
<title>Languageindependent Probabilistic Answer Ranking. for Question Answering.</title>
<date>2007</date>
<booktitle>Proc. of ACL.</booktitle>
<contexts>
<context position="20319" citStr="Ko et al. (2007)" startWordPosition="3302" endWordPosition="3305">an Reciprocal Rank (MRR) – i.e., the score of a question is 1/k, where k is the position of the correct answer. We use as baseline the output of our answer retrieval component (Figure 1). This component uses the BM25 criterion, the highest performing IR model in our experiments. Table 2 lists the results obtained using this baseline and our best model (“Ranking” in the table) on the testing partition. Since we are interested in the performance of the ranking model, we evaluate on the subset of questions where the correct answer is retrieved by answer retrieval in the top N answers (similar to Ko et al. (2007)). In the table we report 8http://wordnet.princeton.edu 9sourceforge.net/projects/supersensetag 10http://sourceforge.net/projects/desr 723 MRR P@1 N = 10 N = 15 N = 25 N = 50 N = 10 N = 15 N = 25 N = 50 recall@N 26.25% 29.04% 32.81% 38.09% 26.25% 29.04% 32.81% 38.09% Baseline 61.33 56.12 50.31 43.74 45.94 41.48 36.74 31.66 Ranking 68.72f0.01 63.84f0.01 57.76f0.07 50.72f0.01 54.22f0.01 49.59f0.03 43.98f0.09 37.99f0.01 Improvement +12.04% +13.75% +14.80% +15.95% +18.02% +19.55% +19.70% +19.99% Table 2: Overall results for the test partition. results for several N values. For completeness, we sho</context>
<context position="30992" citStr="Ko et al. (2007)" startWordPosition="5030" endWordPosition="5033">structed from the question and the expected answer types improves answer ranking. Cui et al. (2005) learned transformations of dependency paths from questions to answers to improve passage ranking. However, both approaches use similarity models at their core because they require the matching of the lexical elements in the search structures. On the other hand, our approach allows the learning of full transformations from question structures to answer structures using translation models applied to different text representations. Our answer ranking framework is closest in spirit to the system of Ko et al. (2007) or Higashinaka et al. (2008). However, the former was applied only to factoid QA and both are limited to similarity, redundancy and gazetteer-based features. Our model uses a larger feature set that includes correlation and transformation-based features and five different content representations. Our evaluation is also carried out on a larger scale. Our work is also related to that of Riezler et al. (2007) where SMT-based query expansion methods are used on data from FAQ pages. 6 Conclusions In this work we described an answer ranking engine for non-factoid questions built using a large commu</context>
</contexts>
<marker>Ko, Mitamura, Nyberg, 2007</marker>
<rawString>J. Ko, T. Mitamura, and E. Nyberg. 2007. Languageindependent Probabilistic Answer Ranking. for Question Answering. Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers: The Role of Semantic Information.</title>
<date>2005</date>
<journal>Natural Language Engineering.</journal>
<contexts>
<context position="17578" citStr="Li and Roth, 2005" startWordPosition="2844" endWordPosition="2847">on-answer pairs. Step 2: from the above corpus we remove the questions and answers of obvious low quality. We implement this filter with a simple heuristic by keeping only questions and answers that have at least 4 words each, out of which at least 1 is a noun and at least 1 is a verb. This step filters out questions like “How to be excellent?” and answers such as “I don’t know”. The outcome of this step forms our answer collection C. C contains 142,627 question-answer pairs.7. Arguably, all these filters could be improved. For example, the first step can be replaced by a question classifier (Li and Roth, 2005). Similarly, the second step can be implemented with a statistical classifier that ranks the quality of the content using both the textual and non-textual information available in the database (Jeon et al., 2006; Agichtein et al., 2008). We plan to further investigate these issues which are not the main object of this work. 6Nevertheless, the approach proposed here is independent of the question type. We will explore answer ranking for other non-factoid question types in future work. 7The data will be available through the Yahoo! Webscope program (research-data-requests@yahoo-inc.com). The dat</context>
</contexts>
<marker>Li, Roth, 2005</marker>
<rawString>X. Li and D. Roth. 2005. Learning Question Classifiers: The Role of Semantic Information. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Prevete</author>
<author>H Tanev</author>
</authors>
<date>2002</date>
<booktitle>Comparing Statistical and Content-Based Techniques for Answer Validation on the Web. Proc. of the VIII Convegno AI*IA.</booktitle>
<contexts>
<context position="15163" citStr="Magnini et al., 2002" startWordPosition="2435" endWordPosition="2438">ral variants of the convolution kernels of Collins and Duffy (2001) did not yield improvements. We conjecture that the mistakes of the syntactic parser may be amplified in tree kernels, which consider an exponential number of sub-trees. Informativeness - we model the amount of information contained in the answer by counting the number of non-stop nouns, verbs, and adjectives in the answer text that do not appear in the question. FG4: Web Correlation Features Previous work has shown that the redundancy of a large collection (e.g., the web) can be used for answer validation (Brill et al., 2001; Magnini et al., 2002). In the same spirit, we add features that measure the correlation between question-answer pairs and large external collections: Web correlation - we measure the correlation between the question-answer pair and the web using the Corrected Conditional Probability (CCP) formula of Magnini et al. (2002): CCP(Q, A) _ hits(Q + A)/(hits(Q) hits(A)2/3) where hits returns the number of page hits from a search engine. When a query returns zero hits we iteratively relax it by dropping the keyword with the smallest priority. Keyword priorities are assigned using the heuristics of Moldovan et al. (1999). </context>
</contexts>
<marker>Magnini, Negri, Prevete, Tanev, 2002</marker>
<rawString>B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002. Comparing Statistical and Content-Based Techniques for Answer Validation on the Web. Proc. of the VIII Convegno AI*IA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn TreeBank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="18349" citStr="Marcus et al., 1993" startWordPosition="2965" endWordPosition="2968">ual information available in the database (Jeon et al., 2006; Agichtein et al., 2008). We plan to further investigate these issues which are not the main object of this work. 6Nevertheless, the approach proposed here is independent of the question type. We will explore answer ranking for other non-factoid question types in future work. 7The data will be available through the Yahoo! Webscope program (research-data-requests@yahoo-inc.com). The data was processed as follows. The text was split at the sentence level, tokenized and PoS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus et al., 1993). Each word was morphologically simplified using the morphological functions of the WordNet library8. Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006)9, which annotates text with a 46-label tagset. These tags, defined by WordNet lexicographers, provide a broad semantic categorization for nouns and verbs and include labels for nouns such as food, animal, body and feeling, and for verbs labels such as communication, contact, and possession. Next, we parsed all sentences with the dependency parser of Attardi et al. (2007)10. It is important to realize </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn TreeBank. Computational Linguistics, 19(2), pp. 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Goodrum</author>
<author>R Girju</author>
<author>V Rus</author>
</authors>
<title>LASSO - A Tool for Surfing the Answer Net.</title>
<date>1999</date>
<booktitle>Proc. of TREC.</booktitle>
<contexts>
<context position="13739" citStr="Moldovan et al., 1999" startWordPosition="2200" endWordPosition="2203">thm implemented in the GIZA++ toolkit5. Similarly with the previous feature group, we add translation-based features for the five different text representations introduced above. By moving beyond the bag-of-word representation we hope to learn relevant transformations of structures, e.g., from the “squeaky” —* “door” dependency to “spray” &lt;-- “WD-40” in the Table 1 example. FG3: Density and Frequency Features These features measure the density and frequency of question terms in the answer text. Variants of these features were used previously for either answer or passage ranking in factoid QA (Moldovan et al., 1999; Harabagiu et al., 2000). 5http://www.fjoch.com/GIZA++.html Same word sequence - computes the number of nonstop question words that are recognized in the same order in the answer. Answer span - the largest distance (in words) between two non-stop question words in the answer. Same sentence match - number of non-stop question terms matched in a single sentence in the answer. Overall match - number of non-stop question terms matched in the complete answer. These last two features are computed also for the other four text representations previously introduced (B, Bg, D, and Dg). Counting the num</context>
<context position="15761" citStr="Moldovan et al. (1999)" startWordPosition="2532" endWordPosition="2535">1; Magnini et al., 2002). In the same spirit, we add features that measure the correlation between question-answer pairs and large external collections: Web correlation - we measure the correlation between the question-answer pair and the web using the Corrected Conditional Probability (CCP) formula of Magnini et al. (2002): CCP(Q, A) _ hits(Q + A)/(hits(Q) hits(A)2/3) where hits returns the number of page hits from a search engine. When a query returns zero hits we iteratively relax it by dropping the keyword with the smallest priority. Keyword priorities are assigned using the heuristics of Moldovan et al. (1999). Query-log correlation - as in (Ciaramita et al., 2008) we also compute the correlation between questionanswer pairs and a search-engine query-log corpus of more than 7.5 million queries, which shares 722 roughly the same time stamp with the communitygenerated question-answer corpus. We compute the Pointwise Mutual Information (PMI) and Chi square (x2) association measures between each questionanswer word pair in the query-log corpus. The largest and the average values are included as features, as well as the number of QA word pairs which appear in the top 10, 5, and 1 percentile of the PMI a</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Girju, Rus, 1999</marker>
<rawString>D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R. Goodrum, R. Girju, and V. Rus. 1999. LASSO - A Tool for Surfing the Answer Net. Proc. of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification.</title>
<date>2007</date>
<booktitle>Proc. of ACL.</booktitle>
<contexts>
<context position="14449" citStr="Moschitti et al., 2007" startWordPosition="2314" endWordPosition="2317">tes the number of nonstop question words that are recognized in the same order in the answer. Answer span - the largest distance (in words) between two non-stop question words in the answer. Same sentence match - number of non-stop question terms matched in a single sentence in the answer. Overall match - number of non-stop question terms matched in the complete answer. These last two features are computed also for the other four text representations previously introduced (B, Bg, D, and Dg). Counting the number of matched dependencies is essentially a simplified tree kernel for QA (e.g., see (Moschitti et al., 2007)) matching only trees of depth 2. Experiments with full dependency tree kernels based on several variants of the convolution kernels of Collins and Duffy (2001) did not yield improvements. We conjecture that the mistakes of the syntactic parser may be amplified in tree kernels, which consider an exponential number of sub-trees. Informativeness - we model the amount of information contained in the answer by counting the number of non-stop nouns, verbs, and adjectives in the answer text that do not appear in the question. FG4: Web Correlation Features Previous work has shown that the redundancy </context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>A. Moschitti, S. Quarteroni, R. Basili and S. Manandhar. 2007. Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification. Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>S Walker</author>
</authors>
<title>On relevance Weights with Little Relevance Information.</title>
<date>1997</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="10381" citStr="Robertson and Walker, 1997" startWordPosition="1645" endWordPosition="1648"> (FG2), features that measure keyword density and frequency (FG3), and features that measure the correlation between question-answer pairs and other collections (FG4). Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). We detail each of these feature groups next. FG1: Similarity Features We measure the similarity between a question Q and an answer A using the length-normalized BM25 formula (Robertson and Walker, 1997). We chose this similarity formula because, out of all the IR models we tried, it provided the best ranking at the output of the answer retrieval component. For completeness we also include in the feature set the value of the tf ·idf similarity measure. For both formulas we use the implementations available in the Terrier IR platform4 with the default parameters. To understand the contribution of our syntactic and semantic processors we compute the above similarity features for five different representations of the question and answer content: Words (W) - this is the traditional IR view where </context>
</contexts>
<marker>Robertson, Walker, 1997</marker>
<rawString>S. Robertson and S. Walker. 1997. On relevance Weights with Little Relevance Information. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>E Brill</author>
</authors>
<title>Automatic Question Answering Using the Web: Beyond the Factoid.</title>
<date>2006</date>
<journal>Journal of Information Retrieval - Special Issue on Web Information Retrieval,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="2288" citStr="Soricut and Brill, 2006" startWordPosition="337" endWordPosition="340">do you quiet a squeaky door? A: Spray WD-40 directly onto the hinges of the door. Open and close the door several times. Remove hinges if the door still squeaks. Remove any rust, dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back, open and close door several times again. Q: How to extract html tags from an html documents with c++? A: very carefully Table 1: Sample content from Yahoo! Answers. on small hand-annotated corpora built in house (Higashinaka and Isozaki, 2008) or on question-answer pairs harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut and Brill, 2006). None of these situations is ideal: the cost of building the training corpus in the former setup is high; in the latter scenario the data tends to be domain-specific, hence unsuitable for the learning of open-domain models. On the other hand, recent years have seen an explosion of user-generated content (or social media). Of particular interest in our context are communitydriven question-answering sites, such as Yahoo! Answers3, where users answer questions posed by other users and best answers are selected manually either by the asker or by all the participants in the thread. The data genera</context>
<context position="12221" citStr="Soricut and Brill, 2006" startWordPosition="1948" endWordPosition="1951"> the above syntactic views. Generalized bigrams (Bg) - same as above, but the words are generalized to their WNSS. 4http://ir.dcs.gla.ac.uk/terrier 721 In all these representations we skip stop words and normalize all words to their WordNet lemmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P(Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P(Q|A) = 11 P(q|A) (3) qEQ P(q|A) = (1 − A)P.l(q|A) + AP.l(q|C) (4) P.l(q|A) = � (T(q|a)P.l(a|A)) (5) aEA where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, P,,tl(q|C). A is the smoothing parameter. P,,tl(q|C) is comp</context>
<context position="29766" citStr="Soricut and Brill, 2006" startWordPosition="4841" endWordPosition="4845">r ranking quality. Our method is complementary to the above approaches. In fact, it is likely that an optimal retrieval engine from social media should combine all these three methodologies. Moreover, our approach might have applications outside of social media (e.g., for opendomain web-based QA), because the ranking model built is based only on open-domain knowledge and the analysis of textual content. In the QA literature, answer ranking for nonfactoid questions has typically been performed by learning question-to-answer transformations, either using translation models (Berger et al., 2000; Soricut and Brill, 2006) or by exploiting the redundancy of the Web (Agichtein et al., 2001). Girju (2003) extracts non-factoid answers by searching for certain semantic structures, e.g., causation relations as answers to causation questions. In this paper we combine several methodologies, including the above, into a single model. This approach allowed us to perform a systematic feature analysis on a large-scale real-world corpus and a comprehensive feature set. Recent work has showed that structured retrieval improves answer ranking for factoid questions: Bilotti et al. (2007) showed that matching predicateargument </context>
</contexts>
<marker>Soricut, Brill, 2006</marker>
<rawString>R. Soricut and E. Brill. 2006. Automatic Question Answering Using the Web: Beyond the Factoid. Journal of Information Retrieval - Special Issue on Web Information Retrieval, 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Joshi</author>
</authors>
<title>Ranking and Reranking with Perceptron,</title>
<date>2005</date>
<booktitle>Machine Learning. Special Issue on Learning in Speech and Language Technologies,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="8120" citStr="Shen and Joshi (2005)" startWordPosition="1272" endWordPosition="1275">Answer Ranking (discriminative learning) Answer Collection Answers 720 supervised IR models, the answer ranking is implemented using discriminative learning, and finally, some of the ranking features are produced by question-to-answer translation models, which use class-conditional learning. 2.1 Ranking Model Learning with user-generated content can involve arbitrarily large amounts of data. For this reason we choose as a ranking algorithm the Perceptron which is both accurate and efficient and can be trained with online protocols. Specifically, we implement the ranking Perceptron proposed by Shen and Joshi (2005), which reduces the ranking problem to a binary classification problem. The general intuition is to exploit the pairwise preferences induced from the data by training on pairs of patterns, rather than independently on each pattern. Given a weight vector α, the score for a pattern x (a candidate answer) is simply the inner product between the pattern and the weight vector: fα(x) = (x, α) (1) However, the error function depends on pairwise scores. In training, for each pair (xi, xj) E A, the score fα(xi − xj) is computed; note that if f is an inner product fα(xi − xj) = fα(xi) − fα(xj). Given a </context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>L. Shen and A. Joshi. 2005. Ranking and Reranking with Perceptron, Machine Learning. Special Issue on Learning in Speech and Language Technologies, 60(1-3), pp. 73-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>A Vasserman</author>
<author>I Tsochantaridis</author>
<author>V Mittal</author>
<author>Y Liu</author>
</authors>
<title>Statistical Machine Translation for Query Expansion in Answer Retrieval.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="12244" citStr="Riezler et al., 2007" startWordPosition="1952" endWordPosition="1955">s. Generalized bigrams (Bg) - same as above, but the words are generalized to their WNSS. 4http://ir.dcs.gla.ac.uk/terrier 721 In all these representations we skip stop words and normalize all words to their WordNet lemmas. FG2: Translation Features Berger et al. (2000) showed that similarity-based models are doomed to perform poorly for QA because they fail to “bridge the lexical chasm” between questions and answers. One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). In our model, we incorporate this approach by adding the probability that the question Q is a translation of the answer A, P(Q|A), as a feature. This probability is computed using IBM’s Model 1 (Brown et al., 1993): P(Q|A) = 11 P(q|A) (3) qEQ P(q|A) = (1 − A)P.l(q|A) + AP.l(q|C) (4) P.l(q|A) = � (T(q|a)P.l(a|A)) (5) aEA where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, P,,tl(q|C). A is the smoothing parameter. P,,tl(q|C) is computed using the maximum </context>
<context position="31402" citStr="Riezler et al. (2007)" startWordPosition="5096" endWordPosition="5099">ansformations from question structures to answer structures using translation models applied to different text representations. Our answer ranking framework is closest in spirit to the system of Ko et al. (2007) or Higashinaka et al. (2008). However, the former was applied only to factoid QA and both are limited to similarity, redundancy and gazetteer-based features. Our model uses a larger feature set that includes correlation and transformation-based features and five different content representations. Our evaluation is also carried out on a larger scale. Our work is also related to that of Riezler et al. (2007) where SMT-based query expansion methods are used on data from FAQ pages. 6 Conclusions In this work we described an answer ranking engine for non-factoid questions built using a large community-generated question-answer collection. On one hand, this study shows that we can effectively exploit large amounts of available Web data to do research on NLP for non-factoid QA systems, without any annotation or evaluation cost. This provides an excellent framework for large-scale experimentation with various models that otherwise might be hard to understand or evaluate. On the other hand, we expect th</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal and Y. Liu. 2007. Statistical Machine Translation for Query Expansion in Answer Retrieval. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>