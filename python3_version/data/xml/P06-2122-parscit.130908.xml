<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9913">
Inducing Word Alignments with Bilexical Synchronous Trees
</title>
<author confidence="0.980651">
Hao Zhang and Daniel Gildea
</author>
<affiliation confidence="0.966109">
Computer Science Department
University of Rochester
</affiliation>
<address confidence="0.294936">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.947349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826333333333">
This paper compares different bilexical
tree-based models for bilingual alignment.
EM training for the new model bene-
fits from the dynamic programming “hook
trick”. The model produces improved de-
pendency structure for both languages.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960353846154">
A major difficulty in statistical machine translation
is the trade-off between representational power
and computational complexity. Real-world cor-
pora for language pairs such as Chinese-English
have complex reordering relationships that are not
captured by current phrase-based MT systems, de-
spite their state-of-the-art performance measured
in competitive evaluations. Synchronous gram-
mar formalisms that are capable of modeling
such complex relationships while maintaining the
context-free property in each language have been
proposed for many years, (Aho and Ullman, 1972;
Wu, 1997; Yamada and Knight, 2001; Melamed,
2003; Chiang, 2005), but have not been scaled to
large corpora and long sentences until recently.
In Synchronous Context Free Grammars, there
are two sources of complexity, grammar branch-
ing factor and lexicalization. In this paper we fo-
cus on the second issue, constraining the gram-
mar to the binary-branching Inversion Transduc-
tion Grammar of Wu (1997). Lexicalization seems
likely to help models predict alignment patterns
between languages, and has been proposed by
Melamed (2003) and implemented by Alshawi et
al. (2000) and Zhang and Gildea (2005). However,
each piece of lexical information considered by a
model multiplies the number of states of dynamic
programming algorithms for inference, meaning
that we must choose how to lexicalize very care-
fully to control complexity.
In this paper we compare two approaches to
lexicalization, both of which incorporate bilexical
probabilities. One model uses bilexical probabil-
ities across languages, while the other uses bilex-
ical probabilities within one language. We com-
pare results on word-level alignment, and investi-
gate the implications of the choice of lexicaliza-
tion on the specifics of our alignment algorithms.
The new model, which bilexicalizes within lan-
guages, allows us to use the “hook trick” (Eis-
ner and Satta, 1999) and therefore reduces com-
plexity. We describe the application of the hook
trick to estimation with Expectation Maximization
(EM). Despite the theoretical benefits of the hook
trick, it is not widely used in statistical monolin-
gual parsers, because the savings do not exceed
those obtained with simple pruning. We speculate
that the advantages may be greater in an EM set-
ting, where parameters to guide pruning are not
(initially) available.
In order to better understand the model, we an-
alyze its performance in terms of both agreement
with human-annotated alignments, and agreement
with the dependencies produced by monolingual
parsers. We find that within-language bilexical-
ization does not improve alignment over cross-
language bilexicalization, but does improve recov-
ery of dependencies. We find that the hook trick
significantly speeds training, even in the presence
of pruning.
Section 2 describes the generative model. The
hook trick for EM is explained in Section 3. In
Section 4, we evaluate the model in terms of align-
ment error rate and dependency error rate. We
conclude with discussions in Section 5.
</bodyText>
<page confidence="0.971269">
953
</page>
<note confidence="0.912137">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 953–960,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.611738" genericHeader="method">
2 Bilexicalization of Inversion
Transduction Grammar
</sectionHeader>
<bodyText confidence="0.994188333333333">
The Inversion Transduction Grammar of Wu
(1997) models word alignment between a transla-
tion pair of sentences by assuming a binary syn-
chronous tree on top of both sides. Using EM
training, ITG can induce good alignments through
exploring the hidden synchronous trees from in-
stances of string pairs.
ITG consists of unary production rules that gen-
erate English/foreign word pairs e/f:
</bodyText>
<equation confidence="0.717847">
X → e/f
</equation>
<bodyText confidence="0.879148">
and binary production rules in two forms that gen-
erate subtree pairs, written:
</bodyText>
<equation confidence="0.954344">
X → [Y Z]
</equation>
<bodyText confidence="0.635827">
and
</bodyText>
<equation confidence="0.891354">
X → hY Zi
</equation>
<bodyText confidence="0.999258315789474">
The square brackets indicate the right hand side
rewriting order is the same for both languages.
The pointed brackets indicate there exists a type of
syntactic reordering such that the two right hand
side constituents rewrite in the opposite order in
the second language.
The unary rules account for the alignment links
across two sides. Either e or f may be a special
null word, handling insertions and deletions. The
two kinds of binary rules (called straight rules and
inverted rules) build up a coherent tree structure
on top of the alignment links. From a modeling
perspective, the synchronous tree that may involve
inversions tells a generative story behind the word
level alignment.
An example ITG tree for the sentence pair Je
les vois / I see them is shown in Figure 1(left). The
probability of the tree is the product rule probabil-
ities at each node:
</bodyText>
<equation confidence="0.993453166666666">
P(5 → A)
· P(A → [C B])
· P(C → I/Je)
· P(B → hC Ci)
· P(C → see/vois)
· P(C → them/les)
</equation>
<bodyText confidence="0.999846230769231">
The structural constraint of ITG, which is that
only binary permutations are allowed on each
level, has been demonstrated to be reasonable
by Zens and Ney (2003) and Zhang and Gildea
(2004). However, in the space of ITG-constrained
synchronous trees, we still have choices in making
the probabilistic distribution over the trees more
realistic. The original Stochastic ITG is the coun-
terpart of Stochastic CFG in the bitext space. The
probability of an ITG parse tree is simply a prod-
uct of the probabilities of the applied rules. Thus,
it only captures the fundamental features of word
links and reflects how often inversions occur.
</bodyText>
<subsectionHeader confidence="0.990619">
2.1 Cross-Language Bilexicalization
</subsectionHeader>
<bodyText confidence="0.9999573">
Zhang and Gildea (2005) described a model in
which the nonterminals are lexicalized by English
and foreign language word pairs so that the inver-
sions are dependent on lexical information on the
left hand side of synchronous rules. By introduc-
ing the mechanism of probabilistic head selection
there are four forms of probabilistic binary rules
in the model, which are the four possibilities cre-
ated by taking the cross-product of two orienta-
tions (straight and inverted) and two head choices:
</bodyText>
<equation confidence="0.9999745">
X(e/f) → [&apos; (e/f) Z]
X(e/f) → [Y Z(e/f)]
X(e/f) → hY (e/f) Zi
X(e/f) → hY Z(e/f)i
</equation>
<bodyText confidence="0.9351914">
where (e/f) is a translation pair.
A tree for our example sentence under this
model is shown in Figure 1(center). The tree’s
probability is again the product of rule probabil-
ities:
</bodyText>
<equation confidence="0.99597">
P(5 → A(see/vois))
· P(A(see/vois) → [CB(see/vois)])
· P(C → C(I/Je))
· P(B(see/vois) → hC(see/vois) Ci)
· P(C → C(them/les))
</equation>
<subsectionHeader confidence="0.993467">
2.2 Head-Modifier Bilexicalization
</subsectionHeader>
<bodyText confidence="0.999941692307692">
One disadvantage of the model above is that it
is not capable of modeling bilexical dependen-
cies on the right hand side of the rules. Thus,
while the probability of a production being straight
or inverted depends on a bilingual word pair, it
does not take head-modifier relations in either lan-
guage into account. However, modeling complete
bilingual bilexical dependencies as theorized in
Melamed (2003) implies a huge parameter space
of O(|V |2|T|2), where |V  |and |T |are the vo-
cabulary sizes of the two languages. So, in-
stead of modeling cross-language word transla-
tions and within-language word dependencies in
</bodyText>
<page confidence="0.996476">
954
</page>
<figure confidence="0.999902291666667">
S
A
S S
A(see/vois)
A(see)
B(see/vois)
B
C
C
C(I)
B(see)
C
C
C
C(them)
I/Je
C(see/vois)
C(I/Je)
C(them/les)
them/les
C(see)
I/Je
see/vois
see/vois them/les
</figure>
<figureCaption confidence="0.806822333333333">
Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicaliza-
tion (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates
inverted production.
</figureCaption>
<bodyText confidence="0.9983225">
a joint fashion, we factor them apart. We lexical-
ize the dependencies in the synchronous tree using
words from only one language and translate the
words into their counterparts in the other language
only at the bottom of the tree. Formally, we have
the following patterns of binary dependency rules:
</bodyText>
<equation confidence="0.99997125">
X(e) —* [Y (e) Z(e&apos;)]
X(e) —* [Y (e&apos;) Z(e)]
X(e) —* (Y (e) Z(e&apos;))
X(e) —* (Y (e&apos;) Z(e))
</equation>
<bodyText confidence="0.999896">
where e is an English head and e&apos; is an English
modifier.
Equally importantly, we have the unary lexical
rules that generate foreign words:
</bodyText>
<equation confidence="0.876087">
X(e) —* e/f
</equation>
<bodyText confidence="0.99997525">
To make the generative story complete, we also
have a top rule that goes from the unlexicalized
start symbol to the highest lexicalized nonterminal
in the tree:
</bodyText>
<equation confidence="0.569322">
5 —* X(e)
</equation>
<bodyText confidence="0.98677925">
Figure 1(right), shows our example sentence’s
tree under the new model. The probability of a
bilexical synchronous tree between the two sen-
tences is:
</bodyText>
<equation confidence="0.999677833333333">
P(5 —* A(see))
� P(A(see) —* [C(I) B(see)])
� P(C(I) —* I/Je)
� P(B(see) —* (C(see) C(them)))
� P(C(see) —* see/vois)
� P(C(them) —* them/les)
</equation>
<bodyText confidence="0.999478833333333">
Interestingly, the lexicalized B(see) predicts
not only the existence of C(them), but also that
there is an inversion involved going from C(see)
to C(them). This reflects the fact that direct ob-
ject pronouns come after the verb in English, but
before the verb in French. Thus, despite condi-
tioning on information about words from only one
language, the model captures syntactic reordering
information about the specific language pair it is
trained on. We are able to discriminate between
the straight and inverted binary nodes in our ex-
ample tree in a way that cross-language bilexical-
ization could not.
In terms of inferencing within the framework,
we do the usual Viterbi inference to find the best
bilexical synchronous tree and treat the depen-
dencies and the alignment given by the Viterbi
parse as the best ones, though mathematically the
best alignment should have the highest probabil-
ity marginalized over all dependencies constrained
by the alignment. We do unsupervised training to
obtain the parameters using EM. Both EM and
Viterbi inference can be done using the dynamic
programming framework of synchronous parsing.
</bodyText>
<sectionHeader confidence="0.93047" genericHeader="method">
3 Inside-Outside Parsing with the Hook
Trick
</sectionHeader>
<bodyText confidence="0.999823533333334">
ITG parsing algorithm is a CYK-style chart pars-
ing algorithm extended to bitext. Instead of build-
ing up constituents over spans on a string, an ITG
chart parser builds up constituents over subcells
within a cell defined by two strings. We use
O(X(e), s, t, u, v) to denote the inside probabil-
ity of X(e) which is over the cell of (s, t, u, v)
where (s, t) are indices into the source language
string and (u, v) are indices into the target lan-
guage string. We use α(X(e), s, t, u, v) to de-
note its outside probability. Figure 2 shows how
smaller cells adjacent along diagonals can be com-
bined to create a large cell. We number the sub-
cells counterclockwise. To analyze the complex-
ity of the algorithm with respect to input string
</bodyText>
<page confidence="0.989827">
955
</page>
<figure confidence="0.962305727272727">
2
� ✂ 3
4
✄☎1
v
U
u
s S t
U e
u
s S
</figure>
<figureCaption confidence="0.995508">
Figure 2: Left: Chart parsing over the bitext cell of (s, t, u, v). Right: One of the four hooks built for
</figureCaption>
<bodyText confidence="0.932467285714286">
four corners for more efficient parsing.
length, without loss of generality, we ignore the
nonterminal symbols X, Y , and Z to simplify the
derivation.
The inside algorithm in the context of bilexical
ITG is based on the following dynamic program-
ming equation:
</bodyText>
<equation confidence="0.9987594">
β (e,s,t,u,v)
β1(e) · β3(e0) · P([e0e]  |e)
+β2(e) · β4(e0) · P(hee0i  |e)
+β3(e) · β1(e0) · P([ee0]  |e)
+β4(e) · β2(e0) · P(he0ei  |e)
</equation>
<bodyText confidence="0.998004909090909">
So, on the right hand side, we sum up all possi-
ble ways (S, U) of splitting the left hand side cell
and all possible head words (e0) for the non-head
subcell. e, e0, s, t, u, v, S, and U all eight vari-
ables take O(n) values given that the lengths of
the source string and the target string are O(n).
Thus the entire DP algorithm takes O(n8) steps.
Fortunately, we can reduce the maximum num-
ber of interacting variables by factorizing the ex-
pression.
Let us keep the results of the summations over
</bodyText>
<equation confidence="0.924424571428571">
e0 as:
β+ 1 (e) = E β1(e0) · P([ee0]  |e)
e0
β1(e) · β+3 (e)
+ β2(e) · β+4 (e)
+ β3(e) · β+1 (e)
+ β4(e) · β+2 (e)
</equation>
<bodyText confidence="0.976715727272727">
We reduced one variable from the original ex-
pression. The maximum number of interacting
variables throughout the algorithm is 7. So the im-
proved inside algorithm has a time complexity of
O(n7).
The trick of reducing interacting variables in DP
for bilexical parsing has been pointed out by Eis-
ner and Satta (1999). Melamed (2003) discussed
the applicability of the so-called hook trick for
parsing bilexical multitext grammars. The name
hook is based on the observation that we combine
the non-head constituent with the bilexical rule to
create a special constituent that matches the head
like a hook as demonstrated in Figure 2. How-
ever, for EM, it is not clear from their discussions
how we can do the hook trick in the outside pass.
The bilexical rules in all four directions are anal-
ogous. To simplify the derivation for the outside
algorithm, we just focus on the first case: straight
rule with right head word.
The outside probability of the constituent
(e, S, t, U, v) in cell 1 being a head of such rules
</bodyText>
<equation confidence="0.994975157894737">
E=
S,U,e0
�
� � � �
�
� � � �
E=
S,U
�
� � � �
�
� � � �
is:
E (α(e) · β3(e0) · P([e0e]  |e))
s,u,e0
sE α (e) · E llβ3(e0) · P([e0e]  |e)
e0
E= � �
s,u α(e) · β+ 3 (e)
</equation>
<bodyText confidence="0.972739727272727">
The computation of each β+ involves four
boundary indices and two head words. So, we can
rely on DP to compute them in O(n6). Based on
these intermediate results, we have the equivalent
DP expression for computing inside probabilities:
β (e, s, t, u, v)
which indicates we can reuse β+ of the lower left
neighbors of the head to make the computation
feasible in O(n7).
On the other hand, the outside probability for
(e0, s, S, u, U) in cell 3 acting as a modifier of such
</bodyText>
<equation confidence="0.9883755">
β+ 2 (e) = E β2(e0) · P(he0ei  |e)
e0
β+ 3 (e) = E β3(e0) · P([e0e]  |e)
e0
β+ 4 (e) = E β4(e0) · P(hee0i  |e)
e0
</equation>
<page confidence="0.820575">
956
</page>
<bodyText confidence="0.456764">
a rule is:
</bodyText>
<equation confidence="0.993997625">
X (α(e) · β1(e) · P([e0e]  |e))
t,v,e
⎛ ⎛ ⎞ ⎞
⎝X
⎝P([e0e]  |e) · α(e) · β1(e) ⎠ ⎠
t,v
� �
P([e0, e]  |e) · α+ 3 (e)
</equation>
<bodyText confidence="0.996418571428571">
in which we memorize another kind of intermedi-
ate sum to make the computation no more complex
than O(n7).
We can think of α+3 as the outside probability
of the hook on cell 3 which matches cell 1. Gener-
ally, we need outside probabilities for hooks in all
four directions.
</bodyText>
<equation confidence="0.960223833333333">
X=
e
X=
e
Xα+ 1 (e) = α(e) · β3(e)
s,u
Xα+ 2 (e) = α(e) · β4(e)
t,u
Xα+3 (e) = α(e) · β1(e)
t,v
Xα+ 4 (e) = α(e) · β2(e)
s,v
</equation>
<bodyText confidence="0.999811">
Based on them, we can add up the outside prob-
abilities of a constituent acting as one of the two
children of each applicable rule on top of it to get
the total outside probability.
We finalize the derivation by simplifying the ex-
pression of the expected count of (e → [e0e]).
</bodyText>
<equation confidence="0.99462475">
EC(e → [e0e])
X= (P([e0e]  |e) · β3(e0) · α(e) · β1(e))
s,t,u,v,S,U
⎛ ⎛ ⎞ ⎞
⎝X
⎝P([e0e]  |e) · β3(e0) · α · β1 ⎠ ⎠
� �
P ([e0e]  |e) · β3(e0) · α+ 3 (e)
</equation>
<bodyText confidence="0.913271104166667">
which can be computed in O(n6) as long as we
have α+3 ready in a table. Overall we can do the
inside-outside algorithm for the bilexical ITG in
O(n7), by reducing a factor of n through interme-
diate DP.
The entire trick can be understood very clearly
if we imagine the bilexical rules are unary rules
that are applied on top of the non-head con-
stituents to reduce it to a virtual lexical constituent
(a hook) covering the same subcell while sharing
the head word with the head constituent. However,
if we build hooks looking for all words in a sen-
957 tence whenever a complete constituent is added to
the chart, we will build many hooks that are never
used, considering that the words outside of larger
cells are fewer and pruning might further reduce
the possible outside words. Blind guessing of what
might appear outside of the current cell will off-
set the saving we can achieve. Instead of actively
building hooks, which are intermediate results, we
can build them only when we need them an
cells using
as the beam ratio for sen-
tences up to 25 words in the experiments, without
harming alignment error rate, at least for the un-
lexicalized
The hook trick reduces the complexity of bilex-
ical
from
to
With the tic-tac-
toe pruning reducing the number of bitext cells to
work with, also due to the reason that the grammar
constant is very small for
the parsing algo-
rithm runs with an acceptable speed,
The probabilistic model has lots of parameters
of word pairs. Namely, there are
de-
pendency probabilities and
translation
probabilities, where
is the size of English vo-
cabulary and
is the size of the foreign lan-
guage vocabulary. The translation probabilities of
are backed off to a uniform distribu-
tion. We let the bilexical dependency probabili
</bodyText>
<figure confidence="0.827406714285714">
O(n4)
10−5
ITG.
ITG
O(n8)
O(n7).
ITG.
</figure>
<equation confidence="0.7464944">
O(|V|2)
O(|V||T|)
|V|
|T|
P(f|X(e))
</equation>
<bodyText confidence="0.883232142857143">
ties
back off to uni-lexical dependencies in the follow-
ing forms:
d then
cache them for future use. So the construction of
the hooks will be invoked by the heads when the
heads need to combine with adjacent cells.
</bodyText>
<subsectionHeader confidence="0.999456">
3.1 Pruning and Smoothing
</subsectionHeader>
<bodyText confidence="0.999435909090909">
We apply one of the pruning techniques used in
Zhang and Gildea (2005). The technique is gen-
eral enough to be applicable to any parsing algo-
rithm over bitext cells. Itis called tic-tac-toe prun-
ing since it involves an estimate of both the inside
probability of the cell (how likely the words within
the box in both dimensions are to align) and the
outside probability (how likely the words outside
the box in both dimensions are to align). By scor-
ing the bitext cells and throwing away the bad cells
that fall out of a beam, it can reduce over 70% of
</bodyText>
<equation confidence="0.808378444444444">
P ([Y (∗) Z(e0)]  |X(∗))
P ([Y (e0) Z(∗)]  |X(∗))
P (hY (∗) Z(e0)i  |X(∗))
P (hY (e0) Z(∗)i  |X(∗))
X=
s,S,u,U
X=
s,S,u,U
t,v
</equation>
<figure confidence="0.99918928">
0 5 10 15 20
sentence length
0 5 10 15 20 25
sentence length
without-hook
with-hook
without-hook
with-hook
seconds 700
600
500
400
300
200
100
0
seconds 140
120
100
80
60
40
20
0
(a) (b)
</figure>
<figureCaption confidence="0.930567">
Figure 3: Speedup for EM by the Hook Trick. (a) is without pruning. In (b), we apply pruning on the
bitext cells before parsing begins.
</figureCaption>
<bodyText confidence="0.999871666666667">
The two levels of distributions are interpolated
using a technique inspired by Witten-Bell smooth-
ing (Chen and Goodman, 1996). We use the ex-
pected count of the left hand side lexical nontermi-
nal to adjust the weight for the EM-trained bilexi-
cal probability. For example,
</bodyText>
<equation confidence="0.982474">
P([Y (e) Z(e&apos;)]  |X(e)) =
(1 − A)PEm([Y(e) Z(e&apos;)]  |X(e))
+ AP([Y (∗) Z(e&apos;)]  |X(∗))
</equation>
<bodyText confidence="0.942602">
where
</bodyText>
<equation confidence="0.524366">
A = 1/(1 + Expected Counts(X(e)))
</equation>
<sectionHeader confidence="0.98361" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999887023255814">
First of all, we are interested in finding out how
much speedup can be achieved by doing the hook
trick for EM. We implemented both versions in
C++ and turned off pruning for both. We ran the
two inside-outside parsing algorithms on a small
test set of 46 sentence pairs that are no longer than
25 words in both languages. Then we put the re-
sults into buckets of (1 − 4), (5 − 9), (10 − 14),
(15−19), and (20−24) according to the maximum
length of two sentences in each pair and took av-
erages of these timing results. Figure 3 (a) shows
clearly that as the sentences get longer the hook
trick is helping more and more. We also tried to
turn on pruning for both, which is the normal con-
dition for the parsers. Both are much faster due
to the effectiveness of pruning. The speedup ratio
is lower because the hooks will less often be used
again since many cells are pruned away. Figure 3
(b) shows the speedup curve in this situation.
We trained both the unlexicalized and the lex-
icalized ITGs on a parallel corpus of Chinese-
English newswire text. The Chinese data were
automatically segmented into tokens, and English
capitalization was retained. We replaced words
occurring only once with an unknown word token,
resulting in a Chinese vocabulary of 23,783 words
and an English vocabulary of 27,075 words.
We did two types of comparisons. In the first
comparison, we measured the performance of five
word aligners, including IBM models, ITG, the
lexical ITG (LITG) of Zhang and Gildea (2005),
and our bilexical ITG (BLITG), on a hand-aligned
bilingual corpus. All the models were trained us-
ing the same amount of data. We ran the ex-
periments on sentences up to 25 words long in
both languages. The resulting training corpus had
18,773 sentence pairs with a total of 276,113 Chi-
nese words and 315,415 English words.
For scoring the Viterbi alignments of each sys-
tem against gold-standard annotated alignments,
we use the alignment error rate (AER) of Och
and Ney (2000), which measures agreement at the
level of pairs of words:
</bodyText>
<equation confidence="0.985366">
AER = 1 − |A ∩ GP |+ |A ∩ GS|
|A |+ |GS|
</equation>
<bodyText confidence="0.999829538461538">
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in the
gold standard as “sure”, and GP is the set marked
as “possible” (including the “sure” pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS.
In our hand-aligned data, 47 sentence pairs are
no longer than 25 words in either language and
were used to evaluate the aligners.
A separate development set of hand-aligned
sentence pairs was used to control overfitting. The
subset of up to 25 words in both languages was
used. We chose the number of iterations for EM
</bodyText>
<page confidence="0.99208">
958
</page>
<table confidence="0.999965857142857">
Precision Recall Alignment Precision Recall Dependency
Error Rate Error Rate
IBM-1 .56 .42 .52
IBM-4 .67 .43 .47 ITG-lh .11 .11 .89
ITG .68 .52 .41 ITG-rh .22 .22 .78
LITG .69 .51 .41 LITG .13 .12 .88
BLITG .68 .51 .42 BLITG .24 .22 .77
</table>
<tableCaption confidence="0.964456333333333">
Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (G 25 words on
both sides). LITG stands for the cross-language Lexicalized ITG. BLITG is the within-English Bilexical
ITG. ITG-lh is ITG with left-head assumption on English. ITG-rh is with right-head assumption.
</tableCaption>
<table confidence="0.999974">
Precision Recall AER Precision Recall DER
ITG .59 .60 .41 ITG-rh .23 .23 .77
LITG .60 .57 .41 LITG .11 .11 .89
BLITG .58 .55 .44 BLITG .24 .24 .76
</table>
<tableCaption confidence="0.999329">
Table 2: Alignment and dependency results on a larger Chinese-English corpus.
</tableCaption>
<bodyText confidence="0.999971828571429">
training as the turning point of AER on the de-
velopment data set. The unlexicalized ITG was
trained for 3 iterations. LITG was trained for only
1 iteration, partly because it was initialized with
fully trained ITG parameters. BLITG was trained
for 3 iterations.
For comparison, we also included the results
from IBM Model 1 and Model 4. The numbers
of iterations for the training of the IBM models
were also chosen to be the turning points of AER
changing on the development data set.
We also want to know whether or not BLITG
can model dependencies better than LITG. For
this purpose, we also used the AER measurement,
since the goal is still getting higher precision/recall
for a set of recovered word links, although the de-
pendency word links are within one language. For
this reason, we rename AER to Dependency Error
Rate. Table 1(right) is the dependency results on
English side of the test data set. The dependency
results on Chinese are similar.
The gold standard dependencies were extracted
from Collins’ parser output on the sentences. The
LITG and BLITG dependencies were extracted
from the Viterbi synchronous trees by following
the head words.
For comparison, we also included two base-line
results. ITG-lh is unlexicalized ITG with left-head
assumption, meaning the head words always come
from the left branches. ITG-rh is ITG with right-
head assumption.
To make more confident conclusions, we also
did tests on a larger hand-aligned data set used in
Liu et al. (2005). We used 165 sentence pairs that
are up to 25 words in length on both sides.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999973074074074">
The BLITG model has two components, namely
the dependency model on the upper levels of the
tree structure and the word-level translation model
at the bottom. We hope that the two components
will mutually improve one another. The current
experiments indicate clearly that the word level
alignment does help inducing dependency struc-
tures on both sides. The precision and recall on
the dependency retrieval sub-task are almost dou-
bled for both languages from LITG which only
has a kind of uni-lexical dependency in each lan-
guage. Although 20% is a low number, given the
fact that the dependencies are learned basically
through contrasting sentences in two languages,
the result is encouraging. The results slightly im-
prove over ITG with right-head assumption for
English, which is based on linguistic insight. Our
results also echo the findings of Kuhn (2004).
They found that based on the guidance of word
alignment between English and multiple other lan-
guages, a modified EM training for PCFG on En-
glish can bootstrap a more accurate monolingual
probabilistic parser. Figure 4 is an example of the
dependency tree on the English side from the out-
put of BLITG, comparing against the parser out-
put.
We did not find that the feedback from the de-
</bodyText>
<page confidence="0.990666">
959
</page>
<figure confidence="0.999248125">
bright
for
cities
’s
China
China
14 open frontier
accomplishments
Economic reform frontier
open cities 14
bright
for are
are
accomplishments
Economic reform
’s
</figure>
<figureCaption confidence="0.999964">
Figure 4: Dependency tree extracted from parser output vs. Viterbi dependency tree from BLITG
</figureCaption>
<bodyText confidence="0.999508833333333">
pendencies help alignment. To get the reasons, we
need further and deeper analysis. One might guess
that the dependencies are modeled but are not yet
strong and good enough given the amount of train-
ing data. Since the training algorithm EM has the
problem of local maxima, we might also need to
adjust the training algorithm to obtain good pa-
rameters for the alignment task. Initializing the
model with good dependency parameters is a pos-
sible adjustment. We would also like to point out
that alignment task is simpler than decoding where
a stronger component of reordering is required to
produce a fluent English sentence. Investigating
the impact of bilexical dependencies on decoding
is our future work.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648875">
Albert V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite state head transducers.
Computational Linguistics, 26(1):45–60.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-96), pages 310–318, Santa Cruz, CA.
ACL.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Conference of the As-
sociation for Computational Linguistics (ACL-05),
pages 263–270, Ann Arbor, Michigan.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In 37th Annual Meeting of the
Association for Computational Linguistics.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd An-
nual Conference of the Association for Computa-
tional Linguistics (ACL-04).
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the 43rd Annual Conference of the Association
for Computational Linguistics (ACL-05), Ann Ar-
bor, Michigan.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meet-
ing of the North American chapter of the Associ-
ation for Computational Linguistics (NAACL-03),
Edmonton.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Conference of the Association for Com-
putational Linguistics (ACL-00), pages 440–447,
Hong Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Conference of the Association
for Computational Linguistics (ACL-01), Toulouse,
France.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING-04), Geneva,
Switzerland, August.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-05), Ann Arbor, MI.
</reference>
<page confidence="0.997527">
960
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970694">
<title confidence="0.99978">Inducing Word Alignments with Bilexical Synchronous Trees</title>
<author confidence="0.999879">Zhang Gildea</author>
<affiliation confidence="0.9999695">Computer Science Department University of Rochester</affiliation>
<address confidence="0.999815">Rochester, NY 14627</address>
<abstract confidence="0.995841714285714">This paper compares different bilexical tree-based models for bilingual alignment. EM training for the new model benefits from the dynamic programming “hook trick”. The model produces improved dependency structure for both languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Albert V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling,</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="990" citStr="Aho and Ullman, 1972" startWordPosition="131" endWordPosition="134"> structure for both languages. 1 Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005).</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Albert V. Aho and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1561" citStr="Alshawi et al. (2000)" startWordPosition="220" endWordPosition="223">een proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning that we must choose how to lexicalize very carefully to control complexity. In this paper we compare two approaches to lexicalization, both of which incorporate bilexical probabilities. One model uses bilexical probabilities across languages, while the other uses bilexical probabilities within one language. We compare results on word-level alignment, and investigate the implications of the choice of lexicalizatio</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite state head transducers. Computational Linguistics, 26(1):45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Conference of the Association for Computational Linguistics (ACL-96),</booktitle>
<pages>310--318</pages>
<publisher>ACL.</publisher>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="17635" citStr="Chen and Goodman, 1996" startWordPosition="3077" endWordPosition="3080">s that fall out of a beam, it can reduce over 70% of P ([Y (∗) Z(e0)] |X(∗)) P ([Y (e0) Z(∗)] |X(∗)) P (hY (∗) Z(e0)i |X(∗)) P (hY (e0) Z(∗)i |X(∗)) X= s,S,u,U X= s,S,u,U t,v 0 5 10 15 20 sentence length 0 5 10 15 20 25 sentence length without-hook with-hook without-hook with-hook seconds 700 600 500 400 300 200 100 0 seconds 140 120 100 80 60 40 20 0 (a) (b) Figure 3: Speedup for EM by the Hook Trick. (a) is without pruning. In (b), we apply pruning on the bitext cells before parsing begins. The two levels of distributions are interpolated using a technique inspired by Witten-Bell smoothing (Chen and Goodman, 1996). We use the expected count of the left hand side lexical nonterminal to adjust the weight for the EM-trained bilexical probability. For example, P([Y (e) Z(e&apos;)] |X(e)) = (1 − A)PEm([Y(e) Z(e&apos;)] |X(e)) + AP([Y (∗) Z(e&apos;)] |X(∗)) where A = 1/(1 + Expected Counts(X(e))) 4 Experiments First of all, we are interested in finding out how much speedup can be achieved by doing the hook trick for EM. We implemented both versions in C++ and turned off pruning for both. We ran the two inside-outside parsing algorithms on a small test set of 46 sentence pairs that are no longer than 25 words in both langua</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Conference of the Association for Computational Linguistics (ACL-96), pages 310–318, Santa Cruz, CA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1055" citStr="Chiang, 2005" startWordPosition="143" endWordPosition="144">istical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05), pages 263–270, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2320" citStr="Eisner and Satta, 1999" startWordPosition="337" endWordPosition="341">c programming algorithms for inference, meaning that we must choose how to lexicalize very carefully to control complexity. In this paper we compare two approaches to lexicalization, both of which incorporate bilexical probabilities. One model uses bilexical probabilities across languages, while the other uses bilexical probabilities within one language. We compare results on word-level alignment, and investigate the implications of the choice of lexicalization on the specifics of our alignment algorithms. The new model, which bilexicalizes within languages, allows us to use the “hook trick” (Eisner and Satta, 1999) and therefore reduces complexity. We describe the application of the hook trick to estimation with Expectation Maximization (EM). Despite the theoretical benefits of the hook trick, it is not widely used in statistical monolingual parsers, because the savings do not exceed those obtained with simple pruning. We speculate that the advantages may be greater in an EM setting, where parameters to guide pruning are not (initially) available. In order to better understand the model, we analyze its performance in terms of both agreement with human-annotated alignments, and agreement with the depende</context>
<context position="12056" citStr="Eisner and Satta (1999)" startWordPosition="2000" endWordPosition="2004">hus the entire DP algorithm takes O(n8) steps. Fortunately, we can reduce the maximum number of interacting variables by factorizing the expression. Let us keep the results of the summations over e0 as: β+ 1 (e) = E β1(e0) · P([ee0] |e) e0 β1(e) · β+3 (e) + β2(e) · β+4 (e) + β3(e) · β+1 (e) + β4(e) · β+2 (e) We reduced one variable from the original expression. The maximum number of interacting variables throughout the algorithm is 7. So the improved inside algorithm has a time complexity of O(n7). The trick of reducing interacting variables in DP for bilexical parsing has been pointed out by Eisner and Satta (1999). Melamed (2003) discussed the applicability of the so-called hook trick for parsing bilexical multitext grammars. The name hook is based on the observation that we combine the non-head constituent with the bilexical rule to create a special constituent that matches the head like a hook as demonstrated in Figure 2. However, for EM, it is not clear from their discussions how we can do the hook trick in the outside pass. The bilexical rules in all four directions are analogous. To simplify the derivation for the outside algorithm, we just focus on the first case: straight rule with right head wo</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Kuhn</author>
</authors>
<title>Experiments in parallel-text based grammar induction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-04).</booktitle>
<contexts>
<context position="23775" citStr="Kuhn (2004)" startWordPosition="4144" endWordPosition="4145">ts indicate clearly that the word level alignment does help inducing dependency structures on both sides. The precision and recall on the dependency retrieval sub-task are almost doubled for both languages from LITG which only has a kind of uni-lexical dependency in each language. Although 20% is a low number, given the fact that the dependencies are learned basically through contrasting sentences in two languages, the result is encouraging. The results slightly improve over ITG with right-head assumption for English, which is based on linguistic insight. Our results also echo the findings of Kuhn (2004). They found that based on the guidance of word alignment between English and multiple other languages, a modified EM training for PCFG on English can bootstrap a more accurate monolingual probabilistic parser. Figure 4 is an example of the dependency tree on the English side from the output of BLITG, comparing against the parser output. We did not find that the feedback from the de959 bright for cities ’s China China 14 open frontier accomplishments Economic reform frontier open cities 14 bright for are are accomplishments Economic reform ’s Figure 4: Dependency tree extracted from parser out</context>
</contexts>
<marker>Kuhn, 2004</marker>
<rawString>Jonas Kuhn. 2004. Experiments in parallel-text based grammar induction. In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics (ACL-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="22827" citStr="Liu et al. (2005)" startWordPosition="3986" endWordPosition="3989">ults on English side of the test data set. The dependency results on Chinese are similar. The gold standard dependencies were extracted from Collins’ parser output on the sentences. The LITG and BLITG dependencies were extracted from the Viterbi synchronous trees by following the head words. For comparison, we also included two base-line results. ITG-lh is unlexicalized ITG with left-head assumption, meaning the head words always come from the left branches. ITG-rh is ITG with righthead assumption. To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al. (2005). We used 165 sentence pairs that are up to 25 words in length on both sides. 5 Discussion The BLITG model has two components, namely the dependency model on the upper levels of the tree structure and the word-level translation model at the bottom. We hope that the two components will mutually improve one another. The current experiments indicate clearly that the word level alignment does help inducing dependency structures on both sides. The precision and recall on the dependency retrieval sub-task are almost doubled for both languages from LITG which only has a kind of uni-lexical dependency</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05), Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Multitext grammars and synchronous parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="1040" citStr="Melamed, 2003" startWordPosition="141" endWordPosition="142">ficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information consid</context>
<context position="7076" citStr="Melamed (2003)" startWordPosition="1125" endWordPosition="1126">ility is again the product of rule probabilities: P(5 → A(see/vois)) · P(A(see/vois) → [CB(see/vois)]) · P(C → C(I/Je)) · P(B(see/vois) → hC(see/vois) Ci) · P(C → C(them/les)) 2.2 Head-Modifier Bilexicalization One disadvantage of the model above is that it is not capable of modeling bilexical dependencies on the right hand side of the rules. Thus, while the probability of a production being straight or inverted depends on a bilingual word pair, it does not take head-modifier relations in either language into account. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O(|V |2|T|2), where |V |and |T |are the vocabulary sizes of the two languages. So, instead of modeling cross-language word translations and within-language word dependencies in 954 S A S S A(see/vois) A(see) B(see/vois) B C C C(I) B(see) C C C C(them) I/Je C(see/vois) C(I/Je) C(them/les) them/les C(see) I/Je see/vois see/vois them/les Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicalization (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates inverted prod</context>
<context position="12072" citStr="Melamed (2003)" startWordPosition="2005" endWordPosition="2006">hm takes O(n8) steps. Fortunately, we can reduce the maximum number of interacting variables by factorizing the expression. Let us keep the results of the summations over e0 as: β+ 1 (e) = E β1(e0) · P([ee0] |e) e0 β1(e) · β+3 (e) + β2(e) · β+4 (e) + β3(e) · β+1 (e) + β4(e) · β+2 (e) We reduced one variable from the original expression. The maximum number of interacting variables throughout the algorithm is 7. So the improved inside algorithm has a time complexity of O(n7). The trick of reducing interacting variables in DP for bilexical parsing has been pointed out by Eisner and Satta (1999). Melamed (2003) discussed the applicability of the so-called hook trick for parsing bilexical multitext grammars. The name hook is based on the observation that we combine the non-head constituent with the bilexical rule to create a special constituent that matches the head like a hook as demonstrated in Figure 2. However, for EM, it is not clear from their discussions how we can do the hook trick in the outside pass. The bilexical rules in all four directions are analogous. To simplify the derivation for the outside algorithm, we just focus on the first case: straight rule with right head word. The outside </context>
</contexts>
<marker>Melamed, 2003</marker>
<rawString>I. Dan Melamed. 2003. Multitext grammars and synchronous parsers. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Conference of the Association for Computational Linguistics (ACL-00),</booktitle>
<pages>440--447</pages>
<location>Hong Kong,</location>
<contexts>
<context position="19883" citStr="Och and Ney (2000)" startWordPosition="3475" endWordPosition="3478">t comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bilexical ITG (BLITG), on a hand-aligned bilingual corpus. All the models were trained using the same amount of data. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: AER = 1 − |A ∩ GP |+ |A ∩ GS| |A |+ |GS| where A is the set of word pairs aligned by the automatic system, GS is the set marked in the gold standard as “sure”, and GP is the set marked as “possible” (including the “sure” pairs). In our Chinese-English data, only one type of alignment was marked, meaning that GP = GS. In our hand-aligned data, 47 sentence pairs are no longer than 25 words in either language and were used to evaluate the aligners. A separate development set of hand-aligned sentence pairs was used to control overfitting. </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Conference of the Association for Computational Linguistics (ACL-00), pages 440–447, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1000" citStr="Wu, 1997" startWordPosition="135" endWordPosition="136">nguages. 1 Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, </context>
<context position="3689" citStr="Wu (1997)" startWordPosition="549" endWordPosition="550">prove recovery of dependencies. We find that the hook trick significantly speeds training, even in the presence of pruning. Section 2 describes the generative model. The hook trick for EM is explained in Section 3. In Section 4, we evaluate the model in terms of alignment error rate and dependency error rate. We conclude with discussions in Section 5. 953 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 953–960, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Bilexicalization of Inversion Transduction Grammar The Inversion Transduction Grammar of Wu (1997) models word alignment between a translation pair of sentences by assuming a binary synchronous tree on top of both sides. Using EM training, ITG can induce good alignments through exploring the hidden synchronous trees from instances of string pairs. ITG consists of unary production rules that generate English/foreign word pairs e/f: X → e/f and binary production rules in two forms that generate subtree pairs, written: X → [Y Z] and X → hY Zi The square brackets indicate the right hand side rewriting order is the same for both languages. The pointed brackets indicate there exists a type of sy</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01),</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1025" citStr="Yamada and Knight, 2001" startWordPosition="137" endWordPosition="140"> Introduction A major difficulty in statistical machine translation is the trade-off between representational power and computational complexity. Real-world corpora for language pairs such as Chinese-English have complex reordering relationships that are not captured by current phrase-based MT systems, despite their state-of-the-art performance measured in competitive evaluations. Synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the context-free property in each language have been proposed for many years, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical inf</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01), Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5248" citStr="Zens and Ney (2003)" startWordPosition="826" endWordPosition="829">d up a coherent tree structure on top of the alignment links. From a modeling perspective, the synchronous tree that may involve inversions tells a generative story behind the word level alignment. An example ITG tree for the sentence pair Je les vois / I see them is shown in Figure 1(left). The probability of the tree is the product rule probabilities at each node: P(5 → A) · P(A → [C B]) · P(C → I/Je) · P(B → hC Ci) · P(C → see/vois) · P(C → them/les) The structural constraint of ITG, which is that only binary permutations are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained synchronous trees, we still have choices in making the probabilistic distribution over the trees more realistic. The original Stochastic ITG is the counterpart of Stochastic CFG in the bitext space. The probability of an ITG parse tree is simply a product of the probabilities of the applied rules. Thus, it only captures the fundamental features of word links and reflects how often inversions occur. 2.1 Cross-Language Bilexicalization Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Engl</context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntax-based alignment: Supervised or unsupervised?</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5276" citStr="Zhang and Gildea (2004)" startWordPosition="831" endWordPosition="834">ucture on top of the alignment links. From a modeling perspective, the synchronous tree that may involve inversions tells a generative story behind the word level alignment. An example ITG tree for the sentence pair Je les vois / I see them is shown in Figure 1(left). The probability of the tree is the product rule probabilities at each node: P(5 → A) · P(A → [C B]) · P(C → I/Je) · P(B → hC Ci) · P(C → see/vois) · P(C → them/les) The structural constraint of ITG, which is that only binary permutations are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained synchronous trees, we still have choices in making the probabilistic distribution over the trees more realistic. The original Stochastic ITG is the counterpart of Stochastic CFG in the bitext space. The probability of an ITG parse tree is simply a product of the probabilities of the applied rules. Thus, it only captures the fundamental features of word links and reflects how often inversions occur. 2.1 Cross-Language Bilexicalization Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by English and foreign language wor</context>
</contexts>
<marker>Zhang, Gildea, 2004</marker>
<rawString>Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment: Supervised or unsupervised? In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1589" citStr="Zhang and Gildea (2005)" startWordPosition="225" endWordPosition="228">s, (Aho and Ullman, 1972; Wu, 1997; Yamada and Knight, 2001; Melamed, 2003; Chiang, 2005), but have not been scaled to large corpora and long sentences until recently. In Synchronous Context Free Grammars, there are two sources of complexity, grammar branching factor and lexicalization. In this paper we focus on the second issue, constraining the grammar to the binary-branching Inversion Transduction Grammar of Wu (1997). Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. (2000) and Zhang and Gildea (2005). However, each piece of lexical information considered by a model multiplies the number of states of dynamic programming algorithms for inference, meaning that we must choose how to lexicalize very carefully to control complexity. In this paper we compare two approaches to lexicalization, both of which incorporate bilexical probabilities. One model uses bilexical probabilities across languages, while the other uses bilexical probabilities within one language. We compare results on word-level alignment, and investigate the implications of the choice of lexicalization on the specifics of our al</context>
<context position="5780" citStr="Zhang and Gildea (2005)" startWordPosition="910" endWordPosition="913">ns are allowed on each level, has been demonstrated to be reasonable by Zens and Ney (2003) and Zhang and Gildea (2004). However, in the space of ITG-constrained synchronous trees, we still have choices in making the probabilistic distribution over the trees more realistic. The original Stochastic ITG is the counterpart of Stochastic CFG in the bitext space. The probability of an ITG parse tree is simply a product of the probabilities of the applied rules. Thus, it only captures the fundamental features of word links and reflects how often inversions occur. 2.1 Cross-Language Bilexicalization Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by English and foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. By introducing the mechanism of probabilistic head selection there are four forms of probabilistic binary rules in the model, which are the four possibilities created by taking the cross-product of two orientations (straight and inverted) and two head choices: X(e/f) → [&apos; (e/f) Z] X(e/f) → [Y Z(e/f)] X(e/f) → hY (e/f) Zi X(e/f) → hY Z(e/f)i where (e/f) is a translation pair. A tree for our</context>
<context position="16584" citStr="Zhang and Gildea (2005)" startWordPosition="2880" endWordPosition="2883">nd translation probabilities, where is the size of English vocabulary and is the size of the foreign language vocabulary. The translation probabilities of are backed off to a uniform distribution. We let the bilexical dependency probabili O(n4) 10−5 ITG. ITG O(n8) O(n7). ITG. O(|V|2) O(|V||T|) |V| |T| P(f|X(e)) ties back off to uni-lexical dependencies in the following forms: d then cache them for future use. So the construction of the hooks will be invoked by the heads when the heads need to combine with adjacent cells. 3.1 Pruning and Smoothing We apply one of the pruning techniques used in Zhang and Gildea (2005). The technique is general enough to be applicable to any parsing algorithm over bitext cells. Itis called tic-tac-toe pruning since it involves an estimate of both the inside probability of the cell (how likely the words within the box in both dimensions are to align) and the outside probability (how likely the words outside the box in both dimensions are to align). By scoring the bitext cells and throwing away the bad cells that fall out of a beam, it can reduce over 70% of P ([Y (∗) Z(e0)] |X(∗)) P ([Y (e0) Z(∗)] |X(∗)) P (hY (∗) Z(e0)i |X(∗)) P (hY (e0) Z(∗)i |X(∗)) X= s,S,u,U X= s,S,u,U t</context>
<context position="19406" citStr="Zhang and Gildea (2005)" startWordPosition="3393" endWordPosition="3396">ned away. Figure 3 (b) shows the speedup curve in this situation. We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of ChineseEnglish newswire text. The Chinese data were automatically segmented into tokens, and English capitalization was retained. We replaced words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words. We did two types of comparisons. In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bilexical ITG (BLITG), on a hand-aligned bilingual corpus. All the models were trained using the same amount of data. We ran the experiments on sentences up to 25 words long in both languages. The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: AER = 1 − |A ∩ GP |+ |A ∩ GS| |A |+ |GS| where A is the set of w</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05), Ann Arbor, MI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>