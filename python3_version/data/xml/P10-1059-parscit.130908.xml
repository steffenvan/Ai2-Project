<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.738519">
Sentence and Expression Level Annotation of Opinions in User-Generated
Discourse
Cigdem Toprak and Niklas Jakob and Iryna Gurevych
</title>
<author confidence="0.598873">
Ubiquitous Knowledge Processing Lab
</author>
<affiliation confidence="0.6754365">
Computer Science Department, Technische Universit¨at Darmstadt, Hochschulstraße 10
D-64289 Darmstadt, Germany
</affiliation>
<email confidence="0.866398">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.99147" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942055555556">
In this paper, we introduce a corpus of
consumer reviews from the rateitall and
the eopinions websites annotated with
opinion-related information. We present
a two-level annotation scheme. In the
first stage, the reviews are analyzed at
the sentence level for (i) relevancy to a
given topic, and (ii) expressing an eval-
uation about the topic. In the second
stage, on-topic sentences containing eval-
uations about the topic are further investi-
gated at the expression level for pinpoint-
ing the properties (semantic orientation,
intensity), and the functional components
of the evaluations (opinion terms, targets
and holders). We discuss the annotation
scheme, the inter-annotator agreement for
different subtasks and our observations.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941603448276">
There has been a huge interest in the automatic
identification and extraction of opinions from free
text in recent years. Opinion mining spans a va-
riety of subtasks including: creating opinion word
lexicons (Esuli and Sebastiani, 2006; Ding et al.,
2008), identifying opinion expressions (Riloff and
Wiebe, 2003; Fahrni and Klenner, 2008), identi-
fying polarities of opinions in context (Breck et
al., 2007; Wilson et al., 2005), extracting opinion
targets (Hu and Liu, 2004; Zhuang et al., 2006;
Cheng and Xu, 2008) and opinion holders (Kim
and Hovy, 2006; Choi et al., 2005).
Data-driven approaches for extracting opinion
expressions, their holders and targets require re-
liably annotated data at the expression level. In
previous research, expression level annotation of
opinions was extensively investigated on newspa-
per articles (Wiebe et al., 2005; Wilson and Wiebe,
2005; Wilson, 2008b) and on meeting dialogs (So-
masundaran et al., 2008; Wilson, 2008a).
Compared to the newspaper and meeting dialog
genres, little corpus-based work has been carried
out for interpreting the opinions and evaluations in
user-generated discourse. Due to the high popular-
ity of Web 2.0 communities1, the amount of user-
generated discourse and the interest in the analysis
of such discourse has increased over the last years.
To the best of our knowledge, there are two cor-
pora of user-generated discourse which are anno-
tated for opinion related information at the expres-
sion level: The corpus of Hu &amp; Liu (2004) consists
of customer reviews about consumer electronics,
and the corpus of Zhuang et al. (2006) consists of
movie reviews. Both corpora are tailored for ap-
plication specific needs, therefore, do not contain
certain related information explicitly annotated in
the discourse, which we consider important (see
Section 2). Furthermore, none of these works pro-
vide inter-annotator agreement studies.
Our goal is to create sentence and expression
level annotated corpus of customer reviews which
fulfills the following requirements: (1) It filters
individual sentences regarding their topic rele-
vancy and the existence of an opinion or factual
information which implies an evaluation. (2) It
identifies opinion expressions including the re-
spective opinion target, opinion holder, modi-
fiers, and anaphoric expressions if applicable. (3)
The semantic orientation of the opinion expres-
sion is identified while considering negation, and
the opinion expression is linked to the respective
holder and target in the discourse. Such a re-
source would (i) enable novel applications of opin-
ion mining such as a fine-grained identification of
opinion properties, e.g. opinion modification de-
tection including negation, and (ii) enhance opin-
ion target extraction and the polarity assignment
by linking the opinion expression with its target
</bodyText>
<footnote confidence="0.999012666666667">
1http://blog.nielsen.com/nielsenwire/
wp-content/uploads/2008/10/press_
release24.pdf
</footnote>
<page confidence="0.92459">
575
</page>
<note confidence="0.9456255">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999632111111111">
and providing anaphoric resolutions in discourse.
We present an annotation scheme which fulfills
the mentioned requirements, an inter-annotator
agreement study, and discuss our observations.
The rest of this paper is structured as follows:
Section 2 presents the related work. In Sections
3, we describe the annotation scheme. Section 4
presents the data and the annotation study, while
Section 5 summarizes the main conclusions.
</bodyText>
<sectionHeader confidence="0.973383" genericHeader="method">
2 Previous Opinion Annotated Corpora
</sectionHeader>
<subsectionHeader confidence="0.984486">
2.1 Newspaper Articles and Meeting Dialogs
</subsectionHeader>
<bodyText confidence="0.999944105263158">
Most prominent work concerning the expres-
sion level annotation of opinions is the Multi-
Perspective Question Answering (MPQA) corpus2
(Wiebe et al., 2005). It was extended several times
over the last years, either by adding new docu-
ments or annotating new types of opinion related
information (Wilson and Wiebe, 2005; Stoyanov
and Cardie, 2008; Wilson, 2008b). The MPQA
annotation scheme builds upon the private state
notion (Quirk et al., 1985) which describes men-
tal states including opinions, emotions, specula-
tions and beliefs among others. The annotation
scheme strives to represent the private states in
terms of their functional components (i.e. expe-
riencer holding an attitude towards a target). It
consists of frames (direct subjective, expressive
subjective element, objective speech event, agent,
attitude, and target frames) with slots represent-
ing various attributes and properties (e.g.intensity,
nested source) of the private states.
Wilson (2008a) adapts and extends the concepts
from the MPQA scheme to annotate subjective
content in meetings (AMI corpus), and creates the
AMIDA scheme. Besides subjective utterances,
the AMIDA scheme contains objective polar ut-
terances which annotates evaluations without ex-
pressing explicit opinion expressions.
Somasundaran et al. (2008) proposes opinion
frames for representing discourse level associa-
tions in meeting dialogs. The annotation scheme
focuses on two types of opinions, sentiment and
arguing. It annotates the opinion expression and
target spans. The link and link type attributes asso-
ciate the target with other targets in the discourse
through same or alternative relations. The opinion
frames are built based on the links between tar-
gets. Somasundaran et al. (2008) show that opin-
ion frames enable a coherent interpretation of the
</bodyText>
<footnote confidence="0.841705">
2http://www.cs.pitt.edu/mpqa/
</footnote>
<bodyText confidence="0.9965774375">
opinions in discourse and discover implicit evalu-
ations through link transitivity.
Similar to Somasundaran et al. (2008), Asher
et al. (2008) performs discourse level analysis of
opinions. They propose a scheme which first iden-
tifies and assigns categories to the opinion seg-
ments as reporting, judgment, advice, or senti-
ment; and then links the opinion segments with
each other via rhetorical relations including con-
trast, correction, support, result, or continuation.
However, in contrast to our scheme and other
schemes, instead of marking expression bound-
aries without any restriction they annotate an opin-
ion segment only if it contains an opinion word
from their lexicon, or if it has a rhetorical relation
to another opinion segment.
</bodyText>
<subsectionHeader confidence="0.983609">
2.2 User-generated Discourse
</subsectionHeader>
<bodyText confidence="0.995471857142857">
The two annotated corpora of user-generated con-
tent and their corresponding annotation schemes
are far less complex. Hu &amp; Liu (2004) present
a dataset of customer reviews for consumer elec-
tronics crawled from amazon.com. The follow-
ing example shows two annotations taken from the
corpus of Hu &amp; Liu (2004):
</bodyText>
<equation confidence="0.6995495">
camera[+2]##This is my first digital camera and what a toy
itis...
</equation>
<bodyText confidence="0.971396904761905">
size[+2][u]##it is small enough to fit easily in a coatpocket
orpurse.
The corpus provides only target and polarity anno-
tations, and do not contain opinion expression or
opinion modifier annotations which lead to these
polarity scores. The annotation scheme allows the
annotation of implicit features (indicated with the
the attribute [u]). Implicit features are not re-
solved to any actual product feature instances in
discourse. In fact, the actual positions of the prod-
uct features (or any anaphoric references to them)
are not explicitly marked in the discourse, i.e, it is
unclear to which mention of the feature the opin-
ion refers to.
In their paper on movie review mining and sum-
marization, Zhuang et al. (2006) introduce an an-
notated corpus of movie reviews from the Internet
Movie Database. The corpus is annotated regard-
ing movie features and corresponding opinions.
The following example shows an annotated sen-
tence:
</bodyText>
<footnote confidence="0.76943275">
hSentenceiI have never encountered a movie whose
supporting cast was so perfectly realized.hFO
Fword=“supporting cast” Ftype=“PAC” Oword=“perfect”
Otype=“PRO”/ih/Sentencei
</footnote>
<page confidence="0.998013">
576
</page>
<bodyText confidence="0.993174052631579">
The movie features (Fword) are attributed to one
of 20 predefined categories (Ftype). The opin-
ion words (Oword) and their semantic orientations
(Otype) are identified. Possible negations are di-
rectly reflected by the semantic orientation, but not
explicitly labeled in the sentence. (PD) in the fol-
lowing example indicates that the movie feature is
referenced by anaphora:
hSentenceiIt is utter nonsense and insulting to my
intelligence and sense of history. hFO Fword=“film(PD)”
Ftype=“OA” Oword=“nonsense, insulting”
Otype=“CON”/ih/Sentencei
However, similar to the corpus of Hu &amp; Liu (2004)
the referring pronouns are not explicitly marked in
discourse. It is therefore neither possible to au-
tomatically determine which pronoun creates the
link if there are more than one in a sentence, nor it
is denoted which antecedent, i.e. the actual men-
tion of the feature in the discourse it relates to.
</bodyText>
<sectionHeader confidence="0.994362" genericHeader="method">
3 Annotation Scheme
</sectionHeader>
<subsectionHeader confidence="0.99989">
3.1 Opinion versus Polar Facts
</subsectionHeader>
<bodyText confidence="0.999770894736842">
The goal of the annotation scheme is to capture the
evaluations regarding the topics being discussed in
the consumer reviews. The evaluations in con-
sumer reviews are either explicit expressions of
opinions, or facts which imply evaluations as dis-
cussed below.
Explicit expressions of opinions: Opinions are
private states (Wiebe et al., 2005; Quirk et al.,
1985) which are not open to objective observation
or verification. In this study, we focus on the opin-
ions stating the quality or value of an entity, ex-
perience or a proposition from one’s perspective.
(1) illustrates an example of an explicit expression
of an opinion. Similar to Wiebe et al. (2005), we
view opinions in terms of their functional compo-
nents, as opinion holders, e.g., the author in (1),
holding attitudes (polarity), e.g., negative attitude
indicated with the word nightmare, towards possi-
ble targets, e.g., Capella University.
</bodyText>
<equation confidence="0.899957">
(1) I had a nightmare with Capella University.3
</equation>
<bodyText confidence="0.917755571428571">
Facts implying evaluations: Besides opinions,
there are facts which can be objectively verified,
but still imply an evaluation of the quality or value
of an entity or a proposition. For instance, con-
sider the snippet below:
3We use authentic examples from the corpus without cor-
recting grammatical or spelling errors.
</bodyText>
<listItem confidence="0.969786166666667">
(2) In a 6-week class, I counted 3 comments from the
professors directly to me and two directed to my team.
(3) I found that I spent most of my time learning from my
fellow students.
(4) A standard response from my professors would be that of
a sentence fragment.
</listItem>
<bodyText confidence="0.999985108108108">
The example above provides an evaluation about
the professors without stating any explicit expres-
sions of opinions. We call such objectively verifi-
able, but evaluative sentences polar facts. Explicit
expressions of opinions typically contain specific
cues, i.e. opinion words, loaded with a positive or
negative connotation (e.g., nightmare). Even when
they are taken out of the context in which they ap-
pear, they evoke an evaluation. However, evalu-
ations in polar facts can only be inferred within
the context of the review. For instance, the targets
of the implied evalution in the polar facts (2), (3)
and (4) are the professors. However, (3) may have
been perceived as a positive statement if the re-
view was explaining how good the fellow students
were or how the course enforced team work etc.
The annotation scheme consists of two levels.
First, the sentence level scheme analyses each sen-
tence in terms of (i) its relevancy to the overall
topic of the review, and (ii) whether it contains
an evaluation (an opinion or a polar fact) about
the topic. Once the on-topic sentences contain-
ing evaluations are identified, the expression level
scheme first focuses either on marking the text
spans of the opinion expressions (if the sentence
contains an explicit expression of an opinion) or
marking the targets of the polar facts (if the sen-
tence is a polar fact). Upon marking an opin-
ion expression span, the target and holder of the
opinion is marked and linked to the marked opin-
ion expression. Furthermore, the expression level
scheme allows assigning polarities to the marked
opinion expression spans and targets of the polar
facts.
The following subsections introduce the sen-
tence and the expression level annotation schemes
in detail with examples.
</bodyText>
<subsectionHeader confidence="0.999755">
3.2 Sentence Level Annotation
</subsectionHeader>
<bodyText confidence="0.879713375">
The sentence annotation strives to identify the sen-
tences containing evaluations about the topic. In
consumer reviews people occasionally drift off the
actual topic being reviewed. For instance, as in
(5) taken from a review about an online university,
they tend to provide information about their back-
ground or other experiences.
(5) I am very fortunate and almost right out of high school
</bodyText>
<page confidence="0.979349">
577
</page>
<figureCaption confidence="0.999931">
Figure 1: The sentence level annotation scheme
</figureCaption>
<bodyText confidence="0.999834210526316">
with a very average GPA and only 20; I already make above
$45,000 a year as a programmer with a large health care
company for over a year and have had 3 promotions up in
the first year and a half.
Such sentences do not provide information about
the actual topic, but typically serve for justifying
the user’s point of view or provide a better under-
standing about her circumstances. However, they
are not valuable for an application aiming to ex-
tract opinions about a specific topic.
Reviews given to the annotators contain meta
information stating the topic, for instance, the
name of the university or the service being re-
viewed. A markable (i.e. an annotation unit) is
created for each sentence prior to the annotation
process. At this level, the annotation process is
therefore a sentence labeling task. The annotators
are able to see the whole review, and instructed to
label sentences in the context of the whole review.
Figure 1 presents the sentence level scheme. At-
tribute names are marked with oval circles and the
possible values are given in parenthesis. The fol-
lowing attributes are used:
topic relevant attribute is labeled as yes if the
sentence discusses the given topic itself or its as-
pects, properties or features as in examples (1)-
(4). Other possible values for this attribute include
none given which can be chosen in the absence of
meta data, or no if the sentence drifted off the topic
as in example (5).
opinionated attribute is labeled as yes if the
sentence contains any explicit expressions of opin-
ions about the given topic. This attribute is pre-
sented if the topic relevant attribute has been la-
beled as none given or yes. In other words, only
the on-topic sentences are considered in this step.
Examples (6)-(8) illustrate examples labeled as
topic relevant=yes and opinionated=yes.
</bodyText>
<listItem confidence="0.98458425">
(6) Many people are knocking Devry but I have seen them to
be a very great school. [Topic: Devry University]
(7) University of Phoenix was a surprising disappointment.
[Topic: University of Phoenix]
</listItem>
<bodyText confidence="0.992685565217391">
(8) Assignments were passed down, but when asked to
clarify the assignment because the syllabus had
contradicting, poorly worded, information, my professors
regularly responded....”refer to the syllabus”....but wait, the
syllabus IS the question. [Topic: University of Phoenix]
polar fact attribute is labeled as yes if the sen-
tence is a polar fact. This attribute is presented
if the opinionated attribute has been labeled as
no. Examples (2)-(4) demonstrate sentences la-
beled as topic relevant=yes, opinionated=no and
polar fact=yes.
polar fact polarity attribute represents the po-
larity of the evaluation in a polar fact sentence.
The possible values for this attribute include posi-
tive, negative, both. The value both is intended for
the polar fact sentences containing more than one
evaluation with contradicting polarities. At the
expression level analysis, the targets of the con-
tradicting polar fact evaluations are identified dis-
tinctly and assigned polarities of positive or neg-
ative later on. Examples (9)-(11) demonstrate ex-
amples of polar fact sentences with different val-
ues of the attribute polar fact polarity.
</bodyText>
<listItem confidence="0.993313444444444">
(9) There are students in the first programming class and
after taking this class twice they cannot write a single line of
code. [polar fact polarity=negative]
(10) The same class (i.e. computer class) being teach at Ivy
League schools are being offered at Devry.
[polar fact polarity=positive]
(11) The lectures are interactive and recorded, but you need
a consent from the instructor each time.
[polar fact polarity=both]
</listItem>
<subsectionHeader confidence="0.993238">
3.3 Expression Level Annotation
</subsectionHeader>
<bodyText confidence="0.999773277777778">
At the expression level, we focus on the topic
relevant sentences containing evaluations, i.e.,
sentences labeled as topic relevant=yes, opinion-
ated=yes or topic relevant=yes, opinionated=no,
polar fact=yes. If the sentence is a polarfact, then
the aim is to mark the target and label the polarity
of the evaluation. If the sentence is opinionated,
then, the aim is to mark the opinion expression
span, and label its polarity and strength (i.e. in-
tensity), and to link it to the target and the holder.
Figure 2 presents the expression level scheme.
At this stage, annotators mark text spans, and are
allowed to assign one of the five labels to the
marked span:
The polar target is used to label the targets of
the evaluations implied by polar facts. The is-
Reference attribute labels polar targets which are
anaphoric references. The polar target polarity
</bodyText>
<page confidence="0.993409">
578
</page>
<figureCaption confidence="0.999357">
Figure 2: The expression level annotation scheme
</figureCaption>
<bodyText confidence="0.99470425">
attribute is used to label the polarity as positive
or negative. If the isReference attribute is labeled
as true, then the referent attribute appears which
enables the annotator to resolve the reference to
its antecedent. Consider the example sentences
(12) and (13) below. The polar target in (13),
written bold, is labeled as isReference=true, po-
lar target polarity=negative. To resolve the ref-
erence, annotator first creates another polar target
markable for the antecedent, namely the bold text
span in (12), then, links the antecedent to the ref-
erent attribute of the polar target in (13).
</bodyText>
<listItem confidence="0.963424">
(12) Since classes already started, CTU told me they would
extend me so that I could complete the classes and get credit
once I got back.
(13) What they didn’t tell me is in order to extend, I also had
to be enrolled in the next semester.
</listItem>
<bodyText confidence="0.997438111111111">
The target annotation represents what the opin-
ion is about. Both polar targets and targets can be
the topic of the review or different aspects, i.e. fea-
tures of the topic. Similar to the polar targets, the
isReference attribute allows the identification of
the targets which are anaphoric references and the
referent attribute links them to their antecedents in
the discourse. Bold span in (14) shows an example
of a target in an opinionated sentence.
</bodyText>
<listItem confidence="0.957259">
(14) Capella Uhas incredible faculty in the Harold Abel
</listItem>
<subsectionHeader confidence="0.553689">
School of Psychology.
</subsectionHeader>
<bodyText confidence="0.999924357142857">
The holder type represents the holder of an
opinion in the discourse and is labeled in the same
manner as the targets and polar targets. In con-
sumer reviews, holders are most of the time the
authors of the reviews. To ease the annotation pro-
cess, the holder is not labeled when this is the au-
thor.
The modifier annotation labels the lexical items,
such as not, very, hardly etc., which affect the
strength of an opinion or shift its polarity. Upon
creation of a modifier markable, annotators are
asked to choose between negation, increase, de-
crease for identifying the influence of the modifier
on the opinion. For instance, the marked span in
</bodyText>
<listItem confidence="0.8799802">
(15) is labeled as modifier=increase as it gives the
impression that the author is really offended by the
negative comments about her university.
(15) I am quite honestly appauled by some of the negative
comments given for Capella University on this website.
</listItem>
<bodyText confidence="0.999743769230769">
The opinionexpression annotation is used to la-
bel the opinion terms in the sentence. This mark-
able type has five attributes, three of which, i.e.,
modifier, holder, and target are pointer attributes
to the previously defined markable types. The po-
larity attribute assesses the semantic orientation of
the attitude, where the strength attribute marks the
intensity of this attitude. The polarity and strength
attributes focus solely on the marked opinionex-
pression span, not the whole evaluation implied
in the sentence. For instance, the opinionexpres-
sion span in (16) is labeled as polarity=negative,
strength=average. We infer the polarity of the
evaluation only after considering the modifier, po-
larity and the strength attributes together. In (16),
the evaluation about the target is strongly negative
after considering all three attributes of the opinion-
expression annotation. In (17), the polarity of the
opinionexpression1 itself (complaints) is labeled
as negative. It is linked to the modifier1 which
is labeled as negation. Target1 (PhD journey) is
linked to the opinionexpression1. The overall eval-
uation regarding the target1 is positive after ap-
plying the affect of the modifier1 to the polarity
of the opinionexpression1, i.e., after negating the
negative polarity.
</bodyText>
<listItem confidence="0.8478275">
(16) I am quite honestly[modifier] appauled
by[opinionexpression] some of the negative comments
given for Capella University on this website[target].
(17) I have no[modifier1]
</listItem>
<footnote confidence="0.578243333333333">
complaints[opinionexpression1] about the entire PhD
journey[target1] and highly[modifier2]
recommend[opinionexpression2] this school[target2].
</footnote>
<bodyText confidence="0.997935666666667">
Finally, Figure 3 demonstrates all expression
level markables created for an opinionated sen-
tence and how they relate to each other.
</bodyText>
<page confidence="0.998351">
579
</page>
<figureCaption confidence="0.998725">
Figure 3: Expression level annotation example
</figureCaption>
<sectionHeader confidence="0.945836" genericHeader="method">
4 Annotation Study
</sectionHeader>
<bodyText confidence="0.999960384615385">
Each review has been annotated by two annotators
independently according to the annotation scheme
introduced above. We used the freely available
MMAX24 annotation tool capable of stand-off
multi-level annotations. Annotators were native
speaker linguistic students. They were trained on
15 reviews after reading the annotation manual.5
In the training stage, the annotators discussed with
each other if different decisions have been made
and were allowed to ask questions to clarify their
understanding of the scheme. Annotators had ac-
cess to the review text as a whole while making
their decisions.
</bodyText>
<subsectionHeader confidence="0.978473">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999976647058823">
The corpus consists of consumer reviews col-
lected from the review portals rateitall6 and eopin-
ions7. It contains reviews from two domains in-
cluding online universities, e.g., Capella Univer-
sity, Pheonix, University of Maryland University
College etc. and online services, e.g., PayPal,
egroups, eTrade, eCircles etc. These two domains
were selected with the project-relevant, domain-
specific research goals in mind. We selected a spe-
cific topic, e.g. Pheonix, if there were more than 3
reviews written about it. Table 1 shows descriptive
statistics regarding the data.
We used 118 reviews containing 1151 sentences
from the university domain for measuring the sen-
tence and expression level agreements. In the fol-
lowing subsections, we report the inter-annotator
agreement (IAA) at each level.
</bodyText>
<footnote confidence="0.9999214">
4http://mmax2.sourceforge.net/
5http://www.ukp.tu-darmstadt.de/
research/data/sentiment-analysis
6http://www.rateitall.com
7http://www.epinions.com
</footnote>
<table confidence="0.999572375">
University Service All
Reviews 240 234 474
Sentences 2786 6091 8877
Words 49624 102676 152300
Avg sent./rev. 11.6 26 18.7
Std. dev. sent./rev. 8.2 16 14.6
Avg. words/rev. 206.7 438.7 321.3
Std. dev. words/rev. 159.2 232.1 229.8
</table>
<tableCaption confidence="0.999683">
Table 1: Descriptive statistics about the corpus
</tableCaption>
<subsectionHeader confidence="0.988894">
4.2 Sentence Level Agreement
</subsectionHeader>
<bodyText confidence="0.999592111111111">
Sentence level markables were already created au-
tomatically prior to the annotation, i.e., the set of
annotation units were the same for both annota-
tors. We use Cohen’s kappa (n) (Cohen, 1960)
for measuring the IAA. The sentence level anno-
tation scheme has a hierarchical structure. A new
attribute is presented based on the decision made
for the previous attribute, for instance, opinionated
attribute is only presented if the topic relevant at-
tribute is labeled as yes or none given; polar fact
attribute is only presented if the opinionated at-
tribute is labeled as no etc. We calculate n for each
attribute considering only the markables which
were labeled the same by both annotators in the
previously required step. Table 2 shows the n val-
ues for each attribute, the size of the markable set
on which the value was calculated, and the per-
centage agreement.
</bodyText>
<table confidence="0.9991684">
Attribute Markables Agr. r,
topic relevant 1151 0.89 0.73
opinionated 682 0.80 0.61
polar fact 258 0.77 0.56
polar fact polarity 103 0.96 0.92
</table>
<tableCaption confidence="0.999092">
Table 2: Sentence level inter-annotator agreement
</tableCaption>
<bodyText confidence="0.99985025">
The agreement for topic relevancy shows that
it is possible to label this attribute reliably. The
sentences labeled as topic relevant by both anno-
tators correspond to 59% of all sentences, suggest-
ing that people often drift off the topic in consumer
reviews. This is usually the case when they pro-
vide information about their backgrounds or alter-
natives to the given topic.
On the other hand, we obtain moderate agree-
ment levels for the opinionated and polar fact at-
tributes. 62% of the topic relevant sentences were
labeled as opinionated by at least one annotator,
and the rest 38% constitute the topic relevant sen-
tences labeled as not opinionated by both anno-
tators. Nonetheless, they still contain evaluations
(polar facts), as 15% of the topic relevant sen-
</bodyText>
<page confidence="0.99043">
580
</page>
<bodyText confidence="0.9999785">
tences were labeled as polar facts by both anno-
tators. When we merge the attributes opinionated
and polar fact into a single category, we obtain n
of 0.75 and a percentage agreement of 87%. Thus,
we conclude that opinion-relevant sentences, ei-
ther in the form of an explicit expression of opin-
ion or a polar fact, can be labeled reliably in con-
sumer reviews. However, there is a thin border be-
tween polar facts and explicit expressions of opin-
ions.
To the best of our knowledge, similar annotation
efforts on consumer or movie reviews do not pro-
vide any agreement figures for direct comparison.
However, Wiebe et al. (2005) present an annota-
tion study where they mark textual spans for sub-
jective expressions in a newspaper corpus. They
report pairwise n values for three annotators rang-
ing between 0.72 - 0.84 for the sentence level sub-
jective/objective judgments. Wiebe et al. (2005)
mark subjective spans, and do not explicitly per-
form the sentence level labeling task. They calcu-
late the sentence level n values based on the ex-
istence of a subjective expression span in the sen-
tence. Although the task definitions, approaches
and the corpora have quite disparate characteris-
tics in both studies, we obtain comparable results
when we merge opinionated and polar fact cate-
gories.
</bodyText>
<subsectionHeader confidence="0.996511">
4.3 Expression Level Agreement
</subsectionHeader>
<bodyText confidence="0.999991971428571">
At the expression level, annotators focus only on
the sentences which were labeled as opinionated
or polar fact by both annotators. Annotators were
instructed to mark text spans, and then, assign
them the annotation types such as polar target,
opinionexpression etc. (see Figure 2). For calcu-
lating the text span agreement, we use the agree-
ment metric presented by Wiebe et al. (2005) and
Somasundaran et al. (2008). This metric corre-
sponds to the precision (P) and recall (R) metrics
in information retrieval where the decisions of one
annotator are treated as the system; the decisions
of the other annotator are treated as the gold stan-
dard; and the overlapping spans correspond to the
correctly retrieved documents.
Somasundaran et al. (2008) present a discourse
level annotation study in which opinion and tar-
get spans are marked and linked with each other
in a meeting transcript corpus. Following Soma-
sundaran et al. (2008), we compute three differ-
ent measures for the text span agreement: (i) exact
matching in which the text spans should perfectly
match; (ii) lenient (relaxed) matching in which the
overlap between spans is considered as a match,
and (iii) subset matching in which a span has to
be contained in another span in order to be consid-
ered as a match.8 Agreement naturally increases
as we relax the matching constraints. However,
there were no differences between the lenient and
the subset agreement values. Therefore, we report
only the exact and lenient matching agreement re-
sults for each annotation type in Table 3. The
same agreement results for the lenient and subset
matching indicates that inexact matches are still
very similar to each other, i.e., at least one span is
totally contained in the other.
Somasundaran et al. (2008) do not report any
F-measure. However, they report span agreement
results in terms of precision and recall ranging
between 0.44 - 0.87 for opinion spans and be-
tween 0.74 - 0.90 for the target spans. Wiebe et
al. (2005) use the lenient matching approach for
reporting text span agreements ranging between
0.59 - 0.81 for subjective expressions. We ob-
tain higher agreement values for both opinion ex-
pression and target spans. We attribute this to the
fact that the annotators look for opinion expression
and target spans within the opinionated sentences
which they agreed upon. Sentence level analysis
indeed increases the reliability at the expression
level. Compared to the high agreement on mark-
ing target spans, we obtain lower agreement val-
ues on marking polar target spans. We observe
that it is easier to attribute explicit expressions of
evaluations to topic relevant entities compared to
attributing evaluations implied by experiences to
specific topic relevant entities in the reviews.
We calculated the agreement on identifying
anaphoric references using the method introduced
in (Passonneau, 2004) which utilizes Krippen-
dorf’s α (Krippendorff, 2004) for computing reli-
ability for coreference annotation. We considered
the overlapping target and polar target spans to-
gether in this calculation, and obtained an α value
of 0.29. Compared to Passonneau (α values from
0.46 to 0.74), we obtain a much lower agreement
value. This may be due to the different definitions
and organizations of the annotation tasks. Passon-
neau requires prior marking of all noun phrases (or
instances which needs to be processed by the an-
</bodyText>
<footnote confidence="0.980965">
8An example of subset matching: waste of time vs. total
waste of time
</footnote>
<page confidence="0.987287">
581
</page>
<table confidence="0.999010714285714">
Span P Exact F P Lenient F
R R
opinionexpression 0.70 0.80 0.75 0.82 0.93 0.87
modifier 0.80 0.82 0.81 0.86 0.86 0.86
target 0.80 0.81 0.80 0.91 0.90 0.91
holder 0.75 0.72 0.73 0.93 0.88 0.91
polar target 0.67 0.42 0.51 0.75 0.49 0.59
</table>
<tableCaption confidence="0.999913">
Table 3: Inter-annotator agreement on text spans at the expression level
</tableCaption>
<bodyText confidence="0.999966136363636">
notator). Annotator’s task is to identify whether
an instance refers to another marked entity in the
discourse, and then, to identify corefering entity
chains. However, in our annotation process anno-
tators were tasked to identify only one entity as the
referent, and was free to choose it from anywhere
in the discourse. In other words, our chains con-
tain only one entity. It is possible that both annota-
tors performed correct resolutions, but still did not
overlap with each other, as they resolve to differ-
ent instances of the same entity in the discourse.
We plan to further investigate reference resolution
annotation discrepancies and perform corrections
in the future.
Some annotation types require additional at-
tributes to be labeled after marking the span.
For instance, upon marking a text span as a po-
lar target or an opinionexpression, one has to la-
bel the polarity and strength. We consider the
overlapping spans for each annotation type and
use n for reporting the agreement on these at-
tributes. Table 4 shows the n values.
</bodyText>
<table confidence="0.9557396">
Attribute Markables Agr. r,
polarity 329 0.97 0.94
strength 329 0.74 0.55
modifier 136 0.88 0.77
polar target polarity 63 0.80 0.67
</table>
<tableCaption confidence="0.7443725">
Table 4: Inter-annotator agreement at the expres-
sion level
</tableCaption>
<bodyText confidence="0.99998205882353">
We observe that the strength of the opinionex-
pression and the polar target polarity cannot be
labeled as reliably as the polarity of the opinion-
expression. 61% of the agreed upon polar targets
were labeled as negative by both annotators. On
the other hand, only 35% of the agreed upon opin-
ionexpressions were labeled as negative by both
annotators. There were no neutral instances. This
indicates that reviewers tend to report negative ex-
periences using polar facts, probably objectively
describing what has happened, but report posi-
tive experiences with explicit opinion expressions.
Distribution of the strength attribute was as fol-
lows: weak 6%, average 54%, and strong 40%.
The majority of the modifiers were annotated as
intensifiers (70%), while 20% of the modifiers
were labeled as negation.
</bodyText>
<subsectionHeader confidence="0.935366">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9997054">
We analyzed the discrepancies in the annotations
to gain insights about the challenges involved in
various opinion related labeling tasks. At the sen-
tence level, there were several trivial cases of dis-
agreement, for instance, failing to recognize topic
relevancy when the topic was not mentioned or
referenced explicitly in the sentence, as in (18).
Occasionally, annotators disagreed about whether
a sentence that was written as a reaction to the
other reviewers, as in (19), should be considered
as topic relevant or not. Another source of dis-
agreement included sentences similar to (20) and
(21). One annotator interpreted them as univer-
sally true statements regardless of the topic, while
the other attributed them to the discussed topic.
</bodyText>
<listItem confidence="0.954752142857143">
(18) Go to a state university ifyou know whats good for you!
(19) Those with sour grapes couldnt cut it, have an ax to
grind, and are devoting their time to smearing the school.
(20) As far as learning, you really have to WANT to learn
the material.
(21) On an aside, this type of education is not for the
undisciplined learner.
</listItem>
<bodyText confidence="0.9998155">
Annotators easily distinguished the evaluations
at the sentence level. However, they had diffi-
culties distinguishing between a polar fact and an
opinion. For instance, both annotators agreed that
the sentences (22) and (23) contain evaluations re-
garding the topic of the review. However, one an-
notator interpreted both sentences as objectively
verifiable facts giving a positive impression about
the school, while the other one treated them as
opinions.
</bodyText>
<listItem confidence="0.683223666666667">
(22) All this work in the first 2 Years!
(23) The school has a reputation for making students work
really hard.
</listItem>
<bodyText confidence="0.9995202">
Sentence level annotation increases the relia-
bility of the expression level annotation in terms
of marking text spans. However, annotators of-
ten had disagreements on labeling the strength at-
tribute. For instance, one annotator labeled the
</bodyText>
<page confidence="0.991364">
582
</page>
<bodyText confidence="0.999610533333333">
opinion expression in (24) as strong, while the
other one labeled it as average. We observe that
it is not easy to identify trivial causes of disagree-
ments regarding strength as its perception by each
individual is highly subjective. However, most of
the disagreements occurred between weak and av-
erage cases.
(24) the experience that i have when i visit student finance is
much like going to the dentist, except when i leave, nothing
is ever fixed.
We did not apply any consolidation steps during
our agreement studies. However, a final version of
the corpus will be produced by the third judge (one
of the co-authors) by consolidating the judgements
of the two annotators.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999995975609756">
We presented a corpus of consumer reviews from
the rateitall and eopinions websites annotated
with opinion related information. Existing opin-
ion annotated user-generated corpora suffer from
several limitations which result in difficulties for
interpreting the experimental results and for per-
forming error analysis. To name a few, they do
not explicitly link the functional components of
the opinions like targets, holders, or modifiers with
the opinion expression; some of them do not mark
opinion expression spans, none of them resolves
anaphoric references in discourse. Therefore, we
introduced a two level annotation scheme consist-
ing of the sentence and expression levels, which
overcomes the limitations of the existing review
corpora. The sentence level annotation labels sen-
tences for (i) relevancy to a given topic, and (ii)
expressing an evaluation about the topic. Similar
to (Wilson, 2008a), our annotation scheme allows
capturing evaluations made with factual (objec-
tive) sentences. The expression level annotation
further investigates on-topic sentences containing
evaluations for pinpointing the properties (polar-
ity, strength), and marking the functional com-
ponents of the evaluations (opinion terms, modi-
fiers, targets and holders), and linking them within
a discourse. We applied the annotation scheme
to the consumer review genre and presented an
extensive inter-annotator study providing insights
to the challenges involved in various opinion re-
lated labeling tasks in consumer reviews. Simi-
lar to the MPQA scheme, which is successfully
applied to the newspaper genre, the annotation
scheme treats opinions and evaluations as a com-
position of functional components and it is eas-
ily extendable. Therefore, we hypothesize that the
scheme can also be applied to other genres with
minor extensions or as it is. Finally, the corpus
and the annotation manual will be made available
at http://www.ukp.tu-darmstadt.de/
research/data/sentiment-analysis.
</bodyText>
<sectionHeader confidence="0.996586" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.926603875">
This research was funded partially by the German Fed-
eral Ministry of Economy and Technology under grant
01MQ07012 and partially by the German Research Founda-
tion (DFG) as part of the Research Training Group on Feed-
back Based Quality Management in eLearning under grant
1223. We are very grateful to Sandra K¨ubler for her help in
organizing the annotators, and to Lizhen Qu for his program-
ming support in harvesting the data.
</bodyText>
<sectionHeader confidence="0.99275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831911764706">
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. In Coling 2008: Companion vol-
ume: Posters, pages 7–10, Manchester, UK.
Eric Breck, Yejin Choi, and Claire Cardie. 2007.
Identifying expressions of opinion in context. In
Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence (IJCAI-2007),
pages 2683–2688, Hyderabad, India.
Xiwen Cheng and Feiyu Xu. 2008. Fine-grained opin-
ion topic and polarity identification. In Proceedings
of the 6th International Conference on Language
Resources and Evaluation, pages 2710–2714, Mar-
rekech, Morocco.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In HLT ’05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
355–362, Morristown, NJ, USA.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37–46.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Web Data Mining, WSDM 2008,
pages 231–240, Palo Alto, California, USA.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation, pages 417–422, Genova, Italy.
</reference>
<page confidence="0.988698">
583
</page>
<reference confidence="0.999621507246377">
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specific sentiment analysis of
adjectives. In Proceedings of the Symposium on
Affective Language in Human and Machine, AISB
2008 Convention, pages 60 – 63, Aberdeen, Scot-
land.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD’04: Proceed-
ings of the Tenth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 168–177, Seattle, Washington.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text at the
joint COLING-ACL Conference, pages 1–8, Sydney,
Australia.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methology. Sage Publications,
Thousand Oaks, California.
Rebecca J. Passonneau. 2004. Computing reliability
for coreference. In Proceedings of LREC, volume 4,
pages 1503–1506, Lisbon.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New York.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In EMNLP-
03: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
105–112.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2008. Discourse level opinion relations:
An annotation study. In In Proceedings of SIGdial
Workshop on Discourse and Dialogue, pages 129–
137, Columbus, Ohio.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
817–824, Manchester, UK.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39:165–210.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53–60, Ann Arbor, Michigan.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ’05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 347–354, Vancouver, British
Columbia, Canada.
Theresa Wilson. 2008a. Annotating subjective con-
tent in meetings. In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC’08), Marrakech, Morocco.
Theresa Ann Wilson. 2008b. Fine-grained Subjectiv-
ity and Sentiment Analysis: Recognizing the Inten-
sity, Polarity, and Attitudes of Private States. Ph.D.
thesis, University of Pittsburgh.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In CIKM
’06: Proceedings of the 15th ACM international
conference on Information and knowledge manage-
ment, pages 43–50, Arlington, Virginia, USA.
</reference>
<page confidence="0.998594">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429211">
<title confidence="0.989475">Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</title>
<author confidence="0.489907">Cigdem Toprak</author>
<author confidence="0.489907">Niklas Jakob</author>
<author confidence="0.489907">Iryna Gurevych</author>
<affiliation confidence="0.7704815">Ubiquitous Knowledge Processing Lab Computer Science Department, Technische Universit¨at Darmstadt, Hochschulstraße 10</affiliation>
<address confidence="0.9993">D-64289 Darmstadt, Germany</address>
<email confidence="0.999497">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.99964952631579">In this paper, we introduce a corpus of reviews from the annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at sentence level for to a topic, and an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Farah Benamara</author>
<author>Yvette Yannick Mathieu</author>
</authors>
<title>Distilling opinion in discourse: A preliminary study.</title>
<date>2008</date>
<booktitle>In Coling 2008: Companion volume: Posters,</booktitle>
<pages>7--10</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="6635" citStr="Asher et al. (2008)" startWordPosition="966" endWordPosition="969">iations in meeting dialogs. The annotation scheme focuses on two types of opinions, sentiment and arguing. It annotates the opinion expression and target spans. The link and link type attributes associate the target with other targets in the discourse through same or alternative relations. The opinion frames are built based on the links between targets. Somasundaran et al. (2008) show that opinion frames enable a coherent interpretation of the 2http://www.cs.pitt.edu/mpqa/ opinions in discourse and discover implicit evaluations through link transitivity. Similar to Somasundaran et al. (2008), Asher et al. (2008) performs discourse level analysis of opinions. They propose a scheme which first identifies and assigns categories to the opinion segments as reporting, judgment, advice, or sentiment; and then links the opinion segments with each other via rhetorical relations including contrast, correction, support, result, or continuation. However, in contrast to our scheme and other schemes, instead of marking expression boundaries without any restriction they annotate an opinion segment only if it contains an opinion word from their lexicon, or if it has a rhetorical relation to another opinion segment. </context>
</contexts>
<marker>Asher, Benamara, Mathieu, 2008</marker>
<rawString>Nicholas Asher, Farah Benamara, and Yvette Yannick Mathieu. 2008. Distilling opinion in discourse: A preliminary study. In Coling 2008: Companion volume: Posters, pages 7–10, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-2007),</booktitle>
<pages>2683--2688</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1463" citStr="Breck et al., 2007" startWordPosition="205" endWordPosition="208">, and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog g</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI-2007), pages 2683–2688, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiwen Cheng</author>
<author>Feiyu Xu</author>
</authors>
<title>Fine-grained opinion topic and polarity identification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2710--2714</pages>
<location>Marrekech, Morocco.</location>
<contexts>
<context position="1573" citStr="Cheng and Xu, 2008" startWordPosition="224" endWordPosition="227">ation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-gen</context>
</contexts>
<marker>Cheng, Xu, 2008</marker>
<rawString>Xiwen Cheng and Feiyu Xu. 2008. Fine-grained opinion topic and polarity identification. In Proceedings of the 6th International Conference on Language Resources and Evaluation, pages 2710–2714, Marrekech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1633" citStr="Choi et al., 2005" startWordPosition="235" endWordPosition="238">tasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 comm</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 355–362, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="24070" citStr="Cohen, 1960" startWordPosition="3725" endWordPosition="3726">tp://www.ukp.tu-darmstadt.de/ research/data/sentiment-analysis 6http://www.rateitall.com 7http://www.epinions.com University Service All Reviews 240 234 474 Sentences 2786 6091 8877 Words 49624 102676 152300 Avg sent./rev. 11.6 26 18.7 Std. dev. sent./rev. 8.2 16 14.6 Avg. words/rev. 206.7 438.7 321.3 Std. dev. words/rev. 159.2 232.1 229.8 Table 1: Descriptive statistics about the corpus 4.2 Sentence Level Agreement Sentence level markables were already created automatically prior to the annotation, i.e., the set of annotation units were the same for both annotators. We use Cohen’s kappa (n) (Cohen, 1960) for measuring the IAA. The sentence level annotation scheme has a hierarchical structure. A new attribute is presented based on the decision made for the previous attribute, for instance, opinionated attribute is only presented if the topic relevant attribute is labeled as yes or none given; polar fact attribute is only presented if the opinionated attribute is labeled as no etc. We calculate n for each attribute considering only the markables which were labeled the same by both annotators in the previously required step. Table 2 shows the n values for each attribute, the size of the markable</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Search and Web Data Mining, WSDM</booktitle>
<pages>231--240</pages>
<location>Palo Alto, California, USA.</location>
<contexts>
<context position="1312" citStr="Ding et al., 2008" startWordPosition="183" endWordPosition="186">ontaining evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; W</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the International Conference on Web Search and Web Data Mining, WSDM 2008, pages 231–240, Palo Alto, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>417--422</pages>
<location>Genova, Italy.</location>
<contexts>
<context position="1292" citStr="Esuli and Sebastiani, 2006" startWordPosition="179" endWordPosition="182"> stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (W</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 417–422, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela Fahrni</author>
<author>Manfred Klenner</author>
</authors>
<title>Old wine or warm beer: Target-specific sentiment analysis of adjectives.</title>
<date>2008</date>
<booktitle>In Proceedings of the Symposium on Affective Language in Human and Machine, AISB 2008 Convention, pages 60 – 63,</booktitle>
<location>Aberdeen, Scotland.</location>
<contexts>
<context position="1396" citStr="Fahrni and Klenner, 2008" startWordPosition="194" endWordPosition="197">ion level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2</context>
</contexts>
<marker>Fahrni, Klenner, 2008</marker>
<rawString>Angela Fahrni and Manfred Klenner. 2008. Old wine or warm beer: Target-specific sentiment analysis of adjectives. In Proceedings of the Symposium on Affective Language in Human and Machine, AISB 2008 Convention, pages 60 – 63, Aberdeen, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In KDD’04: Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="1531" citStr="Hu and Liu, 2004" startWordPosition="216" endWordPosition="219">gets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpretin</context>
<context position="2553" citStr="Hu &amp; Liu (2004)" startWordPosition="379" endWordPosition="382">son, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 communities1, the amount of usergenerated discourse and the interest in the analysis of such discourse has increased over the last years. To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the expression level: The corpus of Hu &amp; Liu (2004) consists of customer reviews about consumer electronics, and the corpus of Zhuang et al. (2006) consists of movie reviews. Both corpora are tailored for application specific needs, therefore, do not contain certain related information explicitly annotated in the discourse, which we consider important (see Section 2). Furthermore, none of these works provide inter-annotator agreement studies. Our goal is to create sentence and expression level annotated corpus of customer reviews which fulfills the following requirements: (1) It filters individual sentences regarding their topic relevancy and </context>
<context position="7396" citStr="Hu &amp; Liu (2004)" startWordPosition="1084" endWordPosition="1087">orting, judgment, advice, or sentiment; and then links the opinion segments with each other via rhetorical relations including contrast, correction, support, result, or continuation. However, in contrast to our scheme and other schemes, instead of marking expression boundaries without any restriction they annotate an opinion segment only if it contains an opinion word from their lexicon, or if it has a rhetorical relation to another opinion segment. 2.2 User-generated Discourse The two annotated corpora of user-generated content and their corresponding annotation schemes are far less complex. Hu &amp; Liu (2004) present a dataset of customer reviews for consumer electronics crawled from amazon.com. The following example shows two annotations taken from the corpus of Hu &amp; Liu (2004): camera[+2]##This is my first digital camera and what a toy itis... size[+2][u]##it is small enough to fit easily in a coatpocket orpurse. The corpus provides only target and polarity annotations, and do not contain opinion expression or opinion modifier annotations which lead to these polarity scores. The annotation scheme allows the annotation of implicit features (indicated with the the attribute [u]). Implicit features</context>
<context position="9334" citStr="Hu &amp; Liu (2004)" startWordPosition="1378" endWordPosition="1381">PRO”/ih/Sentencei 576 The movie features (Fword) are attributed to one of 20 predefined categories (Ftype). The opinion words (Oword) and their semantic orientations (Otype) are identified. Possible negations are directly reflected by the semantic orientation, but not explicitly labeled in the sentence. (PD) in the following example indicates that the movie feature is referenced by anaphora: hSentenceiIt is utter nonsense and insulting to my intelligence and sense of history. hFO Fword=“film(PD)” Ftype=“OA” Oword=“nonsense, insulting” Otype=“CON”/ih/Sentencei However, similar to the corpus of Hu &amp; Liu (2004) the referring pronouns are not explicitly marked in discourse. It is therefore neither possible to automatically determine which pronoun creates the link if there are more than one in a sentence, nor it is denoted which antecedent, i.e. the actual mention of the feature in the discourse it relates to. 3 Annotation Scheme 3.1 Opinion versus Polar Facts The goal of the annotation scheme is to capture the evaluations regarding the topics being discussed in the consumer reviews. The evaluations in consumer reviews are either explicit expressions of opinions, or facts which imply evaluations as di</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In KDD’04: Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text at the joint COLING-ACL Conference,</booktitle>
<pages>1--8</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1613" citStr="Kim and Hovy, 2006" startWordPosition="231" endWordPosition="234">nt for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popula</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text at the joint COLING-ACL Conference, pages 1–8, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methology. Sage Publications,</title>
<date>2004</date>
<location>Thousand Oaks, California.</location>
<contexts>
<context position="29930" citStr="Krippendorff, 2004" startWordPosition="4689" endWordPosition="4690">d sentences which they agreed upon. Sentence level analysis indeed increases the reliability at the expression level. Compared to the high agreement on marking target spans, we obtain lower agreement values on marking polar target spans. We observe that it is easier to attribute explicit expressions of evaluations to topic relevant entities compared to attributing evaluations implied by experiences to specific topic relevant entities in the reviews. We calculated the agreement on identifying anaphoric references using the method introduced in (Passonneau, 2004) which utilizes Krippendorf’s α (Krippendorff, 2004) for computing reliability for coreference annotation. We considered the overlapping target and polar target spans together in this calculation, and obtained an α value of 0.29. Compared to Passonneau (α values from 0.46 to 0.74), we obtain a much lower agreement value. This may be due to the different definitions and organizations of the annotation tasks. Passonneau requires prior marking of all noun phrases (or instances which needs to be processed by the an8An example of subset matching: waste of time vs. total waste of time 581 Span P Exact F P Lenient F R R opinionexpression 0.70 0.80 0.7</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methology. Sage Publications, Thousand Oaks, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Computing reliability for coreference.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>4</volume>
<pages>1503--1506</pages>
<location>Lisbon.</location>
<contexts>
<context position="29878" citStr="Passonneau, 2004" startWordPosition="4682" endWordPosition="4683"> expression and target spans within the opinionated sentences which they agreed upon. Sentence level analysis indeed increases the reliability at the expression level. Compared to the high agreement on marking target spans, we obtain lower agreement values on marking polar target spans. We observe that it is easier to attribute explicit expressions of evaluations to topic relevant entities compared to attributing evaluations implied by experiences to specific topic relevant entities in the reviews. We calculated the agreement on identifying anaphoric references using the method introduced in (Passonneau, 2004) which utilizes Krippendorf’s α (Krippendorff, 2004) for computing reliability for coreference annotation. We considered the overlapping target and polar target spans together in this calculation, and obtained an α value of 0.29. Compared to Passonneau (α values from 0.46 to 0.74), we obtain a much lower agreement value. This may be due to the different definitions and organizations of the annotation tasks. Passonneau requires prior marking of all noun phrases (or instances which needs to be processed by the an8An example of subset matching: waste of time vs. total waste of time 581 Span P Exa</context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>Rebecca J. Passonneau. 2004. Computing reliability for coreference. In Proceedings of LREC, volume 4, pages 1503–1506, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="5107" citStr="Quirk et al., 1985" startWordPosition="746" endWordPosition="749">4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, nested source) of the private states. Wilson (2008a) adapts and extends the concepts from the MPQA scheme to annotate subjective conte</context>
<context position="10051" citStr="Quirk et al., 1985" startWordPosition="1495" endWordPosition="1498">o automatically determine which pronoun creates the link if there are more than one in a sentence, nor it is denoted which antecedent, i.e. the actual mention of the feature in the discourse it relates to. 3 Annotation Scheme 3.1 Opinion versus Polar Facts The goal of the annotation scheme is to capture the evaluations regarding the topics being discussed in the consumer reviews. The evaluations in consumer reviews are either explicit expressions of opinions, or facts which imply evaluations as discussed below. Explicit expressions of opinions: Opinions are private states (Wiebe et al., 2005; Quirk et al., 1985) which are not open to objective observation or verification. In this study, we focus on the opinions stating the quality or value of an entity, experience or a proposition from one’s perspective. (1) illustrates an example of an explicit expression of an opinion. Similar to Wiebe et al. (2005), we view opinions in terms of their functional components, as opinion holders, e.g., the author in (1), holding attitudes (polarity), e.g., negative attitude indicated with the word nightmare, towards possible targets, e.g., Capella University. (1) I had a nightmare with Capella University.3 Facts imply</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In EMNLP03: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="1369" citStr="Riloff and Wiebe, 2003" startWordPosition="190" endWordPosition="193">estigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dial</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP03: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Discourse level opinion relations: An annotation study. In</title>
<date>2008</date>
<booktitle>In Proceedings of SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>129--137</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1999" citStr="Somasundaran et al., 2008" startWordPosition="287" endWordPosition="291">rni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 communities1, the amount of usergenerated discourse and the interest in the analysis of such discourse has increased over the last years. To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the expression level: The corpus of Hu &amp; Liu (2004) consists of customer reviews about consumer e</context>
<context position="5953" citStr="Somasundaran et al. (2008)" startWordPosition="864" endWordPosition="867">holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, nested source) of the private states. Wilson (2008a) adapts and extends the concepts from the MPQA scheme to annotate subjective content in meetings (AMI corpus), and creates the AMIDA scheme. Besides subjective utterances, the AMIDA scheme contains objective polar utterances which annotates evaluations without expressing explicit opinion expressions. Somasundaran et al. (2008) proposes opinion frames for representing discourse level associations in meeting dialogs. The annotation scheme focuses on two types of opinions, sentiment and arguing. It annotates the opinion expression and target spans. The link and link type attributes associate the target with other targets in the discourse through same or alternative relations. The opinion frames are built based on the links between targets. Somasundaran et al. (2008) show that opinion frames enable a coherent interpretation of the 2http://www.cs.pitt.edu/mpqa/ opinions in discourse and discover implicit evaluations thr</context>
<context position="27427" citStr="Somasundaran et al. (2008)" startWordPosition="4285" endWordPosition="4288">ask definitions, approaches and the corpora have quite disparate characteristics in both studies, we obtain comparable results when we merge opinionated and polar fact categories. 4.3 Expression Level Agreement At the expression level, annotators focus only on the sentences which were labeled as opinionated or polar fact by both annotators. Annotators were instructed to mark text spans, and then, assign them the annotation types such as polar target, opinionexpression etc. (see Figure 2). For calculating the text span agreement, we use the agreement metric presented by Wiebe et al. (2005) and Somasundaran et al. (2008). This metric corresponds to the precision (P) and recall (R) metrics in information retrieval where the decisions of one annotator are treated as the system; the decisions of the other annotator are treated as the gold standard; and the overlapping spans correspond to the correctly retrieved documents. Somasundaran et al. (2008) present a discourse level annotation study in which opinion and target spans are marked and linked with each other in a meeting transcript corpus. Following Somasundaran et al. (2008), we compute three different measures for the text span agreement: (i) exact matching</context>
<context position="28771" citStr="Somasundaran et al. (2008)" startWordPosition="4507" endWordPosition="4510"> considered as a match, and (iii) subset matching in which a span has to be contained in another span in order to be considered as a match.8 Agreement naturally increases as we relax the matching constraints. However, there were no differences between the lenient and the subset agreement values. Therefore, we report only the exact and lenient matching agreement results for each annotation type in Table 3. The same agreement results for the lenient and subset matching indicates that inexact matches are still very similar to each other, i.e., at least one span is totally contained in the other. Somasundaran et al. (2008) do not report any F-measure. However, they report span agreement results in terms of precision and recall ranging between 0.44 - 0.87 for opinion spans and between 0.74 - 0.90 for the target spans. Wiebe et al. (2005) use the lenient matching approach for reporting text span agreements ranging between 0.59 - 0.81 for subjective expressions. We obtain higher agreement values for both opinion expression and target spans. We attribute this to the fact that the annotators look for opinion expression and target spans within the opinionated sentences which they agreed upon. Sentence level analysis </context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2008</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2008. Discourse level opinion relations: An annotation study. In In Proceedings of SIGdial Workshop on Discourse and Dialogue, pages 129– 137, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>817--824</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="5005" citStr="Stoyanov and Cardie, 2008" startWordPosition="730" endWordPosition="733"> as follows: Section 2 presents the related work. In Sections 3, we describe the annotation scheme. Section 4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, nested source) of the private st</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>Veselin Stoyanov and Claire Cardie. 2008. Topic identification for fine-grained opinion analysis. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 817–824, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--165</pages>
<contexts>
<context position="1909" citStr="Wiebe et al., 2005" startWordPosition="273" endWordPosition="276">6; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 communities1, the amount of usergenerated discourse and the interest in the analysis of such discourse has increased over the last years. To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the exp</context>
<context position="4816" citStr="Wiebe et al., 2005" startWordPosition="699" endWordPosition="702">urse. We present an annotation scheme which fulfills the mentioned requirements, an inter-annotator agreement study, and discuss our observations. The rest of this paper is structured as follows: Section 2 presents the related work. In Sections 3, we describe the annotation scheme. Section 4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressi</context>
<context position="10030" citStr="Wiebe et al., 2005" startWordPosition="1491" endWordPosition="1494">e neither possible to automatically determine which pronoun creates the link if there are more than one in a sentence, nor it is denoted which antecedent, i.e. the actual mention of the feature in the discourse it relates to. 3 Annotation Scheme 3.1 Opinion versus Polar Facts The goal of the annotation scheme is to capture the evaluations regarding the topics being discussed in the consumer reviews. The evaluations in consumer reviews are either explicit expressions of opinions, or facts which imply evaluations as discussed below. Explicit expressions of opinions: Opinions are private states (Wiebe et al., 2005; Quirk et al., 1985) which are not open to objective observation or verification. In this study, we focus on the opinions stating the quality or value of an entity, experience or a proposition from one’s perspective. (1) illustrates an example of an explicit expression of an opinion. Similar to Wiebe et al. (2005), we view opinions in terms of their functional components, as opinion holders, e.g., the author in (1), holding attitudes (polarity), e.g., negative attitude indicated with the word nightmare, towards possible targets, e.g., Capella University. (1) I had a nightmare with Capella Uni</context>
<context position="26322" citStr="Wiebe et al. (2005)" startWordPosition="4105" endWordPosition="4108">es were labeled as polar facts by both annotators. When we merge the attributes opinionated and polar fact into a single category, we obtain n of 0.75 and a percentage agreement of 87%. Thus, we conclude that opinion-relevant sentences, either in the form of an explicit expression of opinion or a polar fact, can be labeled reliably in consumer reviews. However, there is a thin border between polar facts and explicit expressions of opinions. To the best of our knowledge, similar annotation efforts on consumer or movie reviews do not provide any agreement figures for direct comparison. However, Wiebe et al. (2005) present an annotation study where they mark textual spans for subjective expressions in a newspaper corpus. They report pairwise n values for three annotators ranging between 0.72 - 0.84 for the sentence level subjective/objective judgments. Wiebe et al. (2005) mark subjective spans, and do not explicitly perform the sentence level labeling task. They calculate the sentence level n values based on the existence of a subjective expression span in the sentence. Although the task definitions, approaches and the corpora have quite disparate characteristics in both studies, we obtain comparable re</context>
<context position="28989" citStr="Wiebe et al. (2005)" startWordPosition="4546" endWordPosition="4549">ere no differences between the lenient and the subset agreement values. Therefore, we report only the exact and lenient matching agreement results for each annotation type in Table 3. The same agreement results for the lenient and subset matching indicates that inexact matches are still very similar to each other, i.e., at least one span is totally contained in the other. Somasundaran et al. (2008) do not report any F-measure. However, they report span agreement results in terms of precision and recall ranging between 0.44 - 0.87 for opinion spans and between 0.74 - 0.90 for the target spans. Wiebe et al. (2005) use the lenient matching approach for reporting text span agreements ranging between 0.59 - 0.81 for subjective expressions. We obtain higher agreement values for both opinion expression and target spans. We attribute this to the fact that the annotators look for opinion expression and target spans within the opinionated sentences which they agreed upon. Sentence level analysis indeed increases the reliability at the expression level. Compared to the high agreement on marking target spans, we obtain lower agreement values on marking polar target spans. We observe that it is easier to attribut</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39:165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky,</booktitle>
<pages>53--60</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1933" citStr="Wilson and Wiebe, 2005" startWordPosition="277" endWordPosition="280">), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 communities1, the amount of usergenerated discourse and the interest in the analysis of such discourse has increased over the last years. To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the expression level: The corpu</context>
<context position="4978" citStr="Wilson and Wiebe, 2005" startWordPosition="726" endWordPosition="729">this paper is structured as follows: Section 2 presents the related work. In Sections 3, we describe the annotation scheme. Section 4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, neste</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, pages 53–60, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1485" citStr="Wilson et al., 2005" startWordPosition="209" endWordPosition="212"> components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-b</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Annotating subjective content in meetings.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="1947" citStr="Wilson, 2008" startWordPosition="281" endWordPosition="282">xpressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 communities1, the amount of usergenerated discourse and the interest in the analysis of such discourse has increased over the last years. To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the expression level: The corpus of Hu &amp; Liu </context>
<context position="5019" citStr="Wilson, 2008" startWordPosition="734" endWordPosition="735">ents the related work. In Sections 3, we describe the annotation scheme. Section 4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, nested source) of the private states. Wilson (</context>
<context position="36289" citStr="Wilson, 2008" startWordPosition="5725" endWordPosition="5726">performing error analysis. To name a few, they do not explicitly link the functional components of the opinions like targets, holders, or modifiers with the opinion expression; some of them do not mark opinion expression spans, none of them resolves anaphoric references in discourse. Therefore, we introduced a two level annotation scheme consisting of the sentence and expression levels, which overcomes the limitations of the existing review corpora. The sentence level annotation labels sentences for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. Similar to (Wilson, 2008a), our annotation scheme allows capturing evaluations made with factual (objective) sentences. The expression level annotation further investigates on-topic sentences containing evaluations for pinpointing the properties (polarity, strength), and marking the functional components of the evaluations (opinion terms, modifiers, targets and holders), and linking them within a discourse. We applied the annotation scheme to the consumer review genre and presented an extensive inter-annotator study providing insights to the challenges involved in various opinion related labeling tasks in consumer re</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Wilson. 2008a. Annotating subjective content in meetings. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Ann Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="1947" citStr="Wilson, 2008" startWordPosition="281" endWordPosition="282">xpressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and evaluations in user-generated discourse. Due to the high popularity of Web 2.0 communities1, the amount of usergenerated discourse and the interest in the analysis of such discourse has increased over the last years. To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the expression level: The corpus of Hu &amp; Liu </context>
<context position="5019" citStr="Wilson, 2008" startWordPosition="734" endWordPosition="735">ents the related work. In Sections 3, we describe the annotation scheme. Section 4 presents the data and the annotation study, while Section 5 summarizes the main conclusions. 2 Previous Opinion Annotated Corpora 2.1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al., 2005). It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). The MPQA annotation scheme builds upon the private state notion (Quirk et al., 1985) which describes mental states including opinions, emotions, speculations and beliefs among others. The annotation scheme strives to represent the private states in terms of their functional components (i.e. experiencer holding an attitude towards a target). It consists of frames (direct subjective, expressive subjective element, objective speech event, agent, attitude, and target frames) with slots representing various attributes and properties (e.g.intensity, nested source) of the private states. Wilson (</context>
<context position="36289" citStr="Wilson, 2008" startWordPosition="5725" endWordPosition="5726">performing error analysis. To name a few, they do not explicitly link the functional components of the opinions like targets, holders, or modifiers with the opinion expression; some of them do not mark opinion expression spans, none of them resolves anaphoric references in discourse. Therefore, we introduced a two level annotation scheme consisting of the sentence and expression levels, which overcomes the limitations of the existing review corpora. The sentence level annotation labels sentences for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. Similar to (Wilson, 2008a), our annotation scheme allows capturing evaluations made with factual (objective) sentences. The expression level annotation further investigates on-topic sentences containing evaluations for pinpointing the properties (polarity, strength), and marking the functional components of the evaluations (opinion terms, modifiers, targets and holders), and linking them within a discourse. We applied the annotation scheme to the consumer review genre and presented an extensive inter-annotator study providing insights to the challenges involved in various opinion related labeling tasks in consumer re</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Ann Wilson. 2008b. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>43--50</pages>
<location>Arlington, Virginia, USA.</location>
<contexts>
<context position="1552" citStr="Zhuang et al., 2006" startWordPosition="220" endWordPosition="223"> We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations. 1 Introduction There has been a huge interest in the automatic identification and extraction of opinions from free text in recent years. Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al., 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al., 2007; Wilson et al., 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al., 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al., 2005). Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al., 2005; Wilson and Wiebe, 2005; Wilson, 2008b) and on meeting dialogs (Somasundaran et al., 2008; Wilson, 2008a). Compared to the newspaper and meeting dialog genres, little corpus-based work has been carried out for interpreting the opinions and ev</context>
<context position="8355" citStr="Zhuang et al. (2006)" startWordPosition="1241" endWordPosition="1244">arget and polarity annotations, and do not contain opinion expression or opinion modifier annotations which lead to these polarity scores. The annotation scheme allows the annotation of implicit features (indicated with the the attribute [u]). Implicit features are not resolved to any actual product feature instances in discourse. In fact, the actual positions of the product features (or any anaphoric references to them) are not explicitly marked in the discourse, i.e, it is unclear to which mention of the feature the opinion refers to. In their paper on movie review mining and summarization, Zhuang et al. (2006) introduce an annotated corpus of movie reviews from the Internet Movie Database. The corpus is annotated regarding movie features and corresponding opinions. The following example shows an annotated sentence: hSentenceiI have never encountered a movie whose supporting cast was so perfectly realized.hFO Fword=“supporting cast” Ftype=“PAC” Oword=“perfect” Otype=“PRO”/ih/Sentencei 576 The movie features (Fword) are attributed to one of 20 predefined categories (Ftype). The opinion words (Oword) and their semantic orientations (Otype) are identified. Possible negations are directly reflected by t</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 43–50, Arlington, Virginia, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>