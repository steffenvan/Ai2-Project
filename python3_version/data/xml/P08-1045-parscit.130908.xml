<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.9883385">
Name Translation in Statistical Machine Translation
Learning When to Transliterate
</title>
<author confidence="0.993721">
Ulf Hermjakob and Kevin Knight
</author>
<affiliation confidence="0.9924825">
University of Southern California
Information Sciences Institute
</affiliation>
<address confidence="0.864229">
4676 Admiralty Way
Marina del Rey, CA 90292, USA
</address>
<email confidence="0.998723">
{ulf,knight}@isi.edu
</email>
<author confidence="0.993085">
Hal Daum´e III
</author>
<affiliation confidence="0.994655">
University of Utah
School of Computing
</affiliation>
<address confidence="0.8054705">
50 S Central Campus Drive
Salt Lake City, UT 84112, USA
</address>
<email confidence="0.99431">
me@hal3.name
</email>
<sectionHeader confidence="0.996588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916909090909">
We present a method to transliterate names
in the framework of end-to-end statistical
machine translation. The system is trained
to learn when to transliterate. For Arabic
to English MT, we developed and trained a
transliterator on a bitext of 7 million sen-
tences and Google’s English terabyte ngrams
and achieved better name translation accuracy
than 3 out of 4 professional translators. The
paper also includes a discussion of challenges
in name translation evaluation.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979203714285714">
State-of-the-art statistical machine translation
(SMT) is bad at translating names that are not very
common, particularly across languages with differ-
ent character sets and sound systems. For example,
consider the following automatic translation:1
SMT output musicians such as Bach
Correct translation composers such as Bach,
Mozart, Chopin, Beethoven, Schumann,
Rachmaninoff, Ravel and Prokofiev
The SMT system drops most names in this ex-
ample. “Name dropping” and mis-translation hap-
pens when the system encounters an unknown word,
mistakes a name for a common noun, or trains on
noisy parallel data. The state-of-the-art is poor for
</bodyText>
<footnote confidence="0.441895">
1taken from NIST02-05 corpora
</footnote>
<bodyText confidence="0.998486741935484">
two reasons. First, although names are important to
human readers, automatic MT scoring metrics (such
as BLEU) do not encourage researchers to improve
name translation in the context of MT. Names are
vastly outnumbered by prepositions, articles, adjec-
tives, common nouns, etc. Second, name translation
is a hard problem — even professional human trans-
lators have trouble with names. Here are four refer-
ence translations taken from the same corpus, with
mistakes underlined:
Ref1 composers such as Bach, missing name
Chopin, Beethoven, Shumann, Rakmaninov,
Ravel and Prokoviev
Ref2 musicians such as Bach, Mozart, Chopin,
Bethoven, Shuman, Rachmaninoff, Rafael and
Brokoviev
Ref3 composers including Bach, Mozart, Schopen,
Beethoven, missing name Raphael, Rahmaniev
and Brokofien
Ref4 composers such as Bach, Mozart, missing
name Beethoven, Schumann, Rachmaninov,
Raphael and Prokofiev
The task of transliterating names (independent of
end-to-end MT) has received a significant amount
of research, e.g., (Knight and Graehl, 1997; Chen et
al., 1998; Al-Onaizan, 2002). One approach is to
“sound out” words and create new, plausible target-
language spellings that preserve the sounds of the
source-language name as much as possible. Another
approach is to phonetically match source-language
names against a large list of target-language words
</bodyText>
<figure confidence="0.63010575">
Arabic input vAK.ñ ð P@ PñÓð pAK. ÉA.Ó �áJ�J��J�ƒñÓ
ÉJ��¯@Pð � ¬ñ�JJ��KAÒkPð�àAÓñ ~ƒð �á�¯ñê�JJ�K.ð
­J�J��¯ñ»ðQK.ð
�
</figure>
<page confidence="0.983826">
389
</page>
<note confidence="0.709272">
Proceedings of ACL-08: HLT, pages 389–397,
</note>
<page confidence="0.536866">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999383785714286">
and phrases. Most of this work has been discon-
nected from end-to-end MT, a problem which we
address head-on in this paper.
The simplest way to integrate name handling into
SMT is: (1) run a named-entity identification system
on the source sentence, (2) transliterate identified
entities with a special-purpose transliteration com-
ponent, and (3) run the SMT system on the source
sentence, as usual, but when looking up phrasal
translations for the words identified in step 1, instead
use the transliterations from step 2.
Many researchers have attempted this, and it does
not work. Typically, translation quality is degraded
rather than improved, for the following reasons:
</bodyText>
<listItem confidence="0.99422816">
• Automatic named-entity identification makes
errors. Some words and phrases that should
not be transliterated are nonetheless sent to the
transliteration component, which returns a bad
translation.
• Not all named entities should be transliterated.
Many named entities require a mix of translit-
eration and translation. For example, in the pair
AJ� �KPñ �®J�ËA» H. ñ �Jk./jnub kalyfurnya/Southern
California, the first Arabic word is translated,
and the second word is transliterated.
• Transliteration components make errors. The
base SMT system may translate a commonly-
occurring name just fine, due to the bitext it was
trained on, while the transliteration component
can easily supply a worse answer.
• Integration hobbles SMT’s use of longer
phrases. Even if the named-entity identifi-
cation and transliteration components operate
perfectly, adopting their translations means that
the SMT system may no longer have access to
longer phrases that include the name. For ex-
ample, our base SMT system translates •J��KP
© JK. ùË Z@P �PñË@ (as a whole phrase) to “Pre-
mier Li Peng”, based on its bitext knowledge.
</listItem>
<bodyText confidence="0.999620115384615">
However, if we force ©� J K. ù Ë to translate as
a separate phrase to “Li Peng”, then the term
Z@P jñË@ •J��P becomes ambiguous (with trans-
lations including “Prime Minister”, “Premier”,
etc.), and we observe incorrect choices being
subsequently made.
To spur better work in name handling, an ACE
entity-translation pilot evaluation was recently de-
veloped (Day, 2007). This evaluation involves
a mixture of entity identification and translation
concerns—for example, the scoring system asks for
coreference determination, which may or may not be
of interest for improving machine translation output.
In this paper, we adopt a simpler metric. We ask:
what percentage of source-language named entities
are translated correctly? This is a precision metric.
We can readily apply it to any base SMT system, and
to human translations as well. Our goal in augment-
ing a base SMT system is to increase this percentage.
A secondary goal is to make sure that our overall
translation quality (as measured by BLEU) does not
degrade as a result of the name-handling techniques
we introduce. We make all our measurements on an
Arabic/English newswire translation task.
Our overall technical approach is summarized
here, along with references to sections of this paper:
</bodyText>
<listItem confidence="0.999686833333333">
• We build a component for transliterating be-
tween Arabic and English (Section 3).
• We automatically learn to tag those words and
phrases in Arabic text, which we believe the
transliteration component will translate cor-
rectly (Section 4).
• We integrate suggested transliterations into the
base SMT search space, with their use con-
trolled by a feature function (Section 5).
• We evaluate both the base SMT system and the
augmented system in terms of entity translation
accuracy and BLEU (Sections 2 and 6).
</listItem>
<sectionHeader confidence="0.976713" genericHeader="introduction">
2 Evaluation
</sectionHeader>
<bodyText confidence="0.998948666666667">
In this section we present the evaluation method that
we use to measure our system and also discuss chal-
lenges in name transliteration evaluation.
</bodyText>
<subsectionHeader confidence="0.936963">
2.1 NEWA Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.99959">
General MT metrics such as BLEU, TER, METEOR
are not suitable for evaluating named entity transla-
tion and transliteration, because they are not focused
on named entities (NEs). Dropping a comma or a the
is penalized as much as dropping a name. We there-
fore use another metric, jointly developed with BBN
and LanguageWeaver.
</bodyText>
<page confidence="0.995335">
390
</page>
<bodyText confidence="0.9911885">
The general idea of the Named Entity Weak Ac-
curacy (NEWA) metric is to
</bodyText>
<listItem confidence="0.999969666666667">
• Count number of NEs in source text: N
• Count number of correctly translated NEs: C
• Divide C/N to get an accuracy figure
</listItem>
<bodyText confidence="0.9998038">
In NEWA, an NE is counted as correctly translated
if the target reference NE is found in the MT out-
put. The metric has the advantage that it is easy to
compute, has no special requirements on an MT sys-
tem (such as depending on source-target word align-
ment) and is tokenization independent.
In the result section of this paper, we will use the
NEWA metric to measure and compare the accuracy
of NE translations in our end-to-end SMT transla-
tions and four human reference translations.
</bodyText>
<subsectionHeader confidence="0.991947">
2.2 Annotated Corpus
</subsectionHeader>
<bodyText confidence="0.99782425">
BBN kindly provided us with an annotated Arabic
text corpus, in which named entities were marked
up with their type (e.g. GPE for Geopolitical Entity)
and one or more English translations. Example:
</bodyText>
<equation confidence="0.99679625">
ù¯ &lt;GPE alt=”Termoli”&gt;ùËñÓQJ��K&lt;/GPE&gt;
�
&lt;PER alt=”Abdullah II j Abdallah II”&gt; éÊË@ YJ.«
ùKA�JË@&lt;/PER&gt;
</equation>
<bodyText confidence="0.998555142857143">
The BBN annotations exhibit a number of issues.
For the English translations of the NEs, BBN anno-
tators looked at human reference translations, which
may introduce a bias towards those human transla-
tions. Specifically, the BBN annotations are some-
times wrong, because the reference translations were
wrong. Consider for example the Arabic phrase
ù Ëñ ÓQJ
K~ ù ¯ JQ �KPñ K. ©J� ’ Ó (mSn‘ burtran
fY tyrmulY), which means Powertrain plant in Ter-
moli. The mapping from tyrmulY to Termoli is not
obvious, and even less the one from burtran to Pow-
ertrain. The human reference translations for this
phrase are
</bodyText>
<listItem confidence="0.998254">
1. Portran site in Tremolo
2. Termoli plant (one name dropped)
3. Portran in Tirnoli
4. Portran assembly plant, in Tirmoli
</listItem>
<bodyText confidence="0.997338714285714">
The BBN annotators adopted the correct transla-
tion Termoli, but also the incorrect Portran. In
other cases the BBN annotators adopted both a cor-
rect (Khatami) and an incorrect translation (Kha-
timi) when referring to the former Iranian president,
which would reward a translation with such an in-
correct spelling.
</bodyText>
<listItem confidence="0.930641">
• &lt;PER alt=”KhatamijKhatimi”&gt;ùÒ.;A�k&lt;/PER&gt;
• &lt;GPE alt=”the American”&gt;aJ�»QJ�ÓAË@&lt;/GPE&gt;
In other cases, all translations are correct, but ad-
ditional correct translations are missing, as for “the
American” above, for which “the US” is an equally
valid alternative in the specific sentence it was anno-
tated in.
</listItem>
<bodyText confidence="0.998363363636364">
All this raises the question of what is a correct
answer. For most Western names, there is normally
only one correct spelling. We follow the same con-
ventions as standard media, paying attention to how
an organization or individual spells its own name,
e.g. Senator Jon Kyl, not Senator John Kyle. For
Arabic names, variation is generally acceptable if
there is no one clearly dominant spelling in English,
e.g. GaddafijGadhafijQaddafijQadhafi, as long as a
given variant is not radically rarer than the most con-
ventional or popular form.
</bodyText>
<subsectionHeader confidence="0.999236">
2.3 Re-Annotation
</subsectionHeader>
<bodyText confidence="0.999987">
Based on the issues we found with the BBN annota-
tions, we re-annotated a sub-corpus of 637 sentences
of the BBN gold standard.
We based this re-annotation on detailed annota-
tion guidelines and sample annotations that had pre-
viously been developed in cooperation with Lan-
guageWeaver, building on three iterations of test an-
notations with three annotators.
We checked each NE in every sentence, using
human reference translations, automatic translitera-
tor output, performing substantial Web research for
many rare names, and checked Google ngrams and
counts for the general Web and news archives to de-
termine whether a variant form met our threshold of
occurring at least 20% as often as the most dominant
form.
</bodyText>
<sectionHeader confidence="0.997947" genericHeader="method">
3 Transliterator
</sectionHeader>
<bodyText confidence="0.9859154">
This section describes how we transliterate Arabic
words or phrases. Given a word such as � ¬ñ�JJ��KAÒkP
or a phrase such as ÉJ
9@P •K�PñÓ, we want to find
the English transliteration for it. This is not just a
</bodyText>
<page confidence="0.993187">
391
</page>
<bodyText confidence="0.999989206896552">
romanization like rHmanynuf and murys rafyl for
the examples above, but a properly spelled English
name such as Rachmaninoffand Maurice Ravel. The
transliteration result can contain several alternatives,
e.g. RachmaninoffjRachmaninov. Unlike various
generative approaches (Knight and Graehl, 1997;
Stalls and Knight, 1998; Li et al., 2004; Matthews,
2007; Sherif and Kondrak, 2007; Kashani et al.,
2007), we do not synthesize an English spelling
from scratch, but rather find a translation in very
large lists of English words (3.4 million) and phrases
(47 million).
We develop a similarity metric for Arabic and En-
glish words. Since matching against millions of can-
didates is computationally prohibitive, we store the
English words and phrases in an index, such that
given an Arabic word or phrase, we quickly retrieve
a much smaller set of likely candidates and apply
our similarity metric to that smaller list.
We divide the task of transliteration into two
steps: given an Arabic word or phrase to translit-
erate, we (1) identify a list of English translitera-
tion candidates from indexed lists of English words
and phrases with counts (section 3.1) and (2) com-
pute for each English name candidate the cost for
the Arabic/English name pair (transliteration scor-
ing model, section 3.2).
We then combine the count information with the
transliteration cost according to the formula:
</bodyText>
<equation confidence="0.587503">
score(e) = log(count(e))/20 - translit cost(e,f)
</equation>
<subsectionHeader confidence="0.999728">
3.1 Indexing with consonant skeletons
</subsectionHeader>
<bodyText confidence="0.965518225806452">
We identify a list of English transliteration candi-
dates through what we call a consonant skeleton in-
dex. Arabic consonants are divided into 11 classes,
represented by letters b,f,g,j,k,l,m,n,r,s,t. In a one-
time pre-processing step, all 3,420,339 (unique) En-
glish words from our English unigram language
model (based on Google’s Web terabyte ngram col-
lection) that might be names or part of names
(mostly based on capitalization) are mapped to one
or more skeletons, e.g.
Rachmaninoff -+ rkmnnf, rmnnf, rsmnnf, rtsmnnf
This yields 10,381,377 skeletons (average of 3.0 per
word) for which a reverse index is created (with
counts). At run time, an Arabic word to be translit-
erated is mapped to its skeleton, e.g.
¬ñ�JJ��KAÒkP rmnnf
�
This skeleton serves as a key for the previously built
reverse index, which then yields the list of English
candidates with counts:
rmnnf -+ Rachmaninov (186,216), Rachmaninoff
(179,666), Armenonville (3,445), Rachmaninow
(1,636), plus 8 others.
Shorter words tend to produce more candidates, re-
sulting in slower transliteration, but since there are
relatively few unique short words, this can be ad-
dressed by caching transliteration results.
The same consonant skeleton indexing process is
applied to name bigrams (47,700,548 unique with
167,398,054 skeletons) and trigrams (46,543,712
unique with 165,536,451 skeletons).
</bodyText>
<subsectionHeader confidence="0.99577">
3.2 Transliteration scoring model
</subsectionHeader>
<bodyText confidence="0.9880415">
The cost of an Arabic/English name pair is com-
puted based on 732 rules that assign a cost to a pair
of Arabic and English substrings, allowing for one
or more context restrictions.
</bodyText>
<listItem confidence="0.9991838">
1. �3::q == ::0
2. j_ð::ough == ::0
3. h::ch == :[aou],::0.1
4. ~†::k == ,$:,$::0.1 ; ::0.2
5. Z:: == :,EC::0.1
</listItem>
<bodyText confidence="0.970898333333333">
The first example rule above assigns to the
straightforward pair j/q a cost of 0. The second rule
includes 2 letters on the Arabic and 4 on the English
side. The third rule restricts application to substring
pairs where the English side is preceded by the let-
ters a, o, or u. The fourth rule specifies a cost of 0.1
if the substrings occur at the end of (both) names,
0.2 otherwise. According to the fifth rule, the Ara-
bic letter Z may match an empty string on the En-
glish side, if there is an English consonant (EC) in
the right context of the English side.
The total cost is computed by always applying the
longest applicable rule, without branching, result-
ing in a linear complexity with respect to word-pair
length. Rules may include left and/or right context
for both Arabic and English. The match fails if no
rule applies or the accumulated cost exceeds a preset
limit.
Names may have n words on the English and m on
the Arabic side. For example, New York is one word
in Arabic and Abdullah is two words in Arabic. The
</bodyText>
<page confidence="0.992825">
392
</page>
<bodyText confidence="0.99997396875">
rules handle spaces (as well as digits, apostrophes
and other non-alphabetic material) just like regular
alphabetic characters, so that our system can handle
cases like where words in English and Arabic names
do not match one to one.
The French name Beaujolais (,k�_.�/bujulyh)
deviates from standard English spelling conventions
in several places. The accumulative cost from the
rules handling these deviations could become pro-
hibitive, with each cost element penalizing the same
underlying offense — being French. We solve this
problem by allowing for additional context in the
form of style flags. The rule for matching eau/3
specifies, in addition to a cost, an (output) style flag
+fr (as in French), which in turn serves as an ad-
ditional context for the rule that matches ais/,k.� at
a much reduced cost. Style flags are also used for
some Arabic dialects. Extended characters such as
´e, ¨o, and s¸ and spelling idiosyncrasies in names on
the English side of the bitext that come from various
third languages account for a significant portion of
the rule set.
Casting the transliteration model as a scoring
problem thus allows for very powerful rules with
strong contexts. The current set of rules has been
built by hand based on a bitext development corpus;
future work might include deriving such rules auto-
matically from a training set of transliterated names.
This transliteration scoring model described in
this section is used in two ways: (1) to transliter-
ate names at SMT decoding time, and (2) to identify
transliteration pairs in a bitext.
</bodyText>
<sectionHeader confidence="0.976552" genericHeader="method">
4 Learning what to transliterate
</sectionHeader>
<bodyText confidence="0.9998131">
As already mentioned in the introduction, named
entity (NE) identification followed by MT is a bad
idea. We don’t want to identify NEs per se anyway
— we want to identify things that our transliterator
will be good at handling, i.e., things that should be
transliterated. This might even include loanwords
like bnk (bank) and brlman (parliament), but would
exclude names such as National Basketball Associ-
ation that are often translated rather transliterated.
Our method follows these steps:
</bodyText>
<listItem confidence="0.998981083333333">
1. Take a bitext.
2. Mark the Arabic words and phrases that have a
recognizable transliteration on the English side.
3. Remove the English side of the bitext.
4. Divide the annotated Arabic corpus into a train-
ing and test corpus.
5. Train a monolingual Arabic tagger to identify
which words and phrases (in running Arabic)
are good candidates for transliteration (section
4.2)
6. Apply the tagger to test data and evaluate its
accuracy.
</listItem>
<subsectionHeader confidence="0.99923">
4.1 Mark-up of bitext
</subsectionHeader>
<bodyText confidence="0.997095454545455">
Given a tokenized (but unaligned and mixed-case)
bitext, we mark up that bitext with links between
Arabic and English words that appear to be translit-
erations. In the following example, linked words are
underlined, with numbers indicating what is linked.
English The meeting was attended by Omani (1)
Secretary of State for Foreign Affairs Yusif (2)
bin (3) Alawi (6) bin (8) Abdallah (10) and
Special Advisor to Sultan (12) Qabus (13)
for Foreign Affairs Umar (14) bin (17)
Abdul Munim (19) al-Zawawi (21).
</bodyText>
<equation confidence="0.8604518">
Arabic (translit.) uHDr allqa’ uzyr aldule
al‘manY (1) llsh’uun alkharjye yusf (2) bn (3)
‘luY (6) bn (8) ‘bd allh (10) ualmstshar alkhaS
llslTan (12) qabus (13) ll‘laqat alkharjye ‘mr (14)
bn (17) ‘bd almn‘m (19) alzuauY (21) .
</equation>
<bodyText confidence="0.9987395">
For each Arabic word, the linking algorithm tries
to find a matching word on the English side, using
the transliteration scoring model described in sec-
tion 3. If the matcher reaches the end of an Arabic
or English word before reaching the end of the other,
it continues to “consume” additional words until a
word-boundary observing match is found or the cost
threshold exceeded.
When there are several viable linking alternatives,
the algorithm considers the cost provided by the
transliteration scoring model, as well as context to
eliminate inferior alternatives, so that for example
the different occurrences of the name particle bin
in the example above are linked to the proper Ara-
bic words, based on the names next to them. The
number of links depends, of course, on the specific
corpus, but we typically identify about 3.0 links per
sentence.
The algorithm is enhanced by a number of heuris-
tics:
</bodyText>
<page confidence="0.997066">
393
</page>
<listItem confidence="0.985669230769231">
• English match candidates are restricted to cap-
italized words (with a few exceptions).
• We use a list of about 200 Arabic and English
stopwords and stopword pairs.
• We use lists of countries and their adjective
forms to bridge cross-POS translations such
as Italy’s president on the English and •J��KP
ùËA¢K
AË@ (”Italian president”) on the Arabic side.
• Arabic prefixes such as È/l- (”to”) are treated
in a special way, because they are translated,
not transliterated like the rest of the word. Link
(12) above is an example.
</listItem>
<bodyText confidence="0.9999665">
In this bitext mark-up process, we achieve 99.5%
precision and 95% recall based on a manual
visualization-tool based evaluation. Of the 5% re-
call error, 3% are due to noisy data in the bitext such
as typos, incorrect translations, or names missing on
one side of the bitext.
</bodyText>
<subsectionHeader confidence="0.999104">
4.2 Training of Arabic name tagger
</subsectionHeader>
<bodyText confidence="0.983818897435897">
The task of the Arabic name tagger (or more
precisely, “transliterate-me” tagger) is to predict
whether or not a word in an Arabic text should be
transliterated, and if so, whether it includes a prefix.
Prefixes such as ð/u- (“and”) have to be translated
rather than transliterated, so it is important to split
off any prefix from a name before transliterating that
name. This monolingual tagging task is not trivial,
as many Arabic words can be both a name and a non-
name. For example, oQK� �Qj. Ë@ (aljzyre) can mean both
Al-Jazeera and the island (or peninsula).
Features include the word itself plus two words
to the left and right, along with various prefixes,
suffixes and other characteristics of all of them, to-
talling about 250 features.
Some of our features depend on large corpus
statistics. For this, we divide the tagged Arabic
side of our training corpus into a stat section and
a core training section. From the stat section we col-
lect statistics as to how often every word, bigram or
trigram occurs, and what distribution of name/non-
name patterns these ngrams have. The name distri-
bution bigram
éK�PñºË@ �èQK� �Qj. Ë@ 3327 00:133 01:3193 11:1
�
(aljzyre alkurye/“peninsula Korean”) for example
tells us that in 3193 out of 3327 occurrences in the
stat corpus bitext, the first word is a marked up as
a non-name (”0”) and the second as a name (”1”),
which strongly suggests that in such a bigram con-
text, aljzyre better be translated as island or penin-
sula, and not be transliterated as Al-Jazeera.
We train our system on a corpus of 6 million stat
sentences, and 500, 000 core training sentences. We
employ a sequential tagger trained using the SEARN
algorithm (Daum´e III et al., 2006) with aggressive
updates (� = 1). Our base learning algorithm
is an averaged perceptron, as implemented in the
MEGAM package2.
</bodyText>
<table confidence="0.99912425">
Reference Precision Recall F-meas.
Raw test corpus 87.4% 95.7% 91.4%
Adjusted for GS 92.1% 95.9% 94.0%
deficiencies
</table>
<tableCaption confidence="0.999834">
Table 1: Accuracy of “transliterate-me” tagger
</tableCaption>
<bodyText confidence="0.9998136">
Testing on 10,000 sentences, we achieve preci-
sion of 87.4% and a recall of 95.7% with respect to
the automatically marked-up Gold Standard as de-
scribed in section 4.1. A manual error analysis of
500 sentences shows that a large portion are not er-
rors after all, but have been marked as errors because
of noise in the bitext and errors in the bitext mark-
up. After adjusting for these deficiencies in the gold
standard, we achieve precision of 92.1% and recall
of 95.9% in the name tagging task.
</bodyText>
<sectionHeader confidence="0.926396" genericHeader="method">
5 Integration with SMT
</sectionHeader>
<bodyText confidence="0.9998305">
We use the following method to integrate our
transliterator into the overall SMT system:
</bodyText>
<listItem confidence="0.93900625">
1. We tag the Arabic source text using the tagger
described in the previous section.
2. We apply the transliterator described in section
3 to the tagged items. We limit this transliter-
</listItem>
<bodyText confidence="0.845735333333333">
ation to words that occur up to 50 times in the
training corpus for single token names (or up
to 100 and 150 times for two and three-word
names). We do this because the general SMT
mechanism tends to do well on more common
names, but does poorly on rare names (and will
</bodyText>
<footnote confidence="0.950072">
2Freely available at http://hal3.name/megam
</footnote>
<page confidence="0.997078">
394
</page>
<bodyText confidence="0.7117605">
always drop names it has never seen in the
training bitext).
</bodyText>
<listItem confidence="0.738794111111111">
3. On the fly, we add transliterations to SMT
phrase table. Instead of a phrasal probability,
the transliterations have a special binary feature
set to 1. In a tuning step, the Minimim Error
Rate Training component of our SMT system
iteratively adjusts the set of rule weights, in-
cluding the weight associated with the translit-
eration feature, such that the English transla-
tions are optimized with respect to a set of
known reference translations according to the
BLEU translation metric.
4. At run-time, the transliterations then compete
with the translations generated by the gen-
eral SMT system. This means that the MT
system will not always use the transliterator
suggestions, depending on the combination of
language model, translation model, and other
component scores.
</listItem>
<subsectionHeader confidence="0.991995">
5.1 Multi-token names
</subsectionHeader>
<bodyText confidence="0.9996995">
We try to transliterate names as much as possible in
context. Consider for example the Arabic name:
</bodyText>
<equation confidence="0.93056">
�J�
��� �K�� �
</equation>
<bodyText confidence="0.981052782608696">
����K�(”yusf abu Sfye”)
If transliterated as single words without context,
the top results would be JosephjJosefjYusufjYosefj
Youssef, AbujAbojIvojApojIbo, and SephiajSofiaj
SophiajSafiehjSafia respectively. However, when
transliterating the three words together against our
list of 47 million English trigrams (section 3), the
transliterator will select the (correct) translation
YousefAbu Safieh. Note that Yousefwas not among
the top 5 choices, and that Safieh was only choice 4.
Similarly, when transliterating ul K�� 3 P� �P� ��
/umuzar ushuban (”and Mozart and Chopin”) with-
out context, the top results would be MoserjMauserj
MozerjMozartjMouser and ShuppanjShoppingj
SchwabenjSchuppanjShobana (with Chopin way
down on place 22). Checking our large English lists
for a matching name, name pattern, the transliterator
identifies the correct translation “, Mozart, Chopin”.
Note that the transliteration module provides the
overall SMT system with up to 5 alternatives,
augmented with a choice of English translations
for the Arabic prefixes like the comma and the
conjunction and in the last example.
</bodyText>
<sectionHeader confidence="0.990037" genericHeader="method">
6 End-to-End results
</sectionHeader>
<bodyText confidence="0.9977265">
We applied the NEWA metric (section 2) to both
our SMT translations as well as the four human ref-
erence translations, using both the original named-
entity translation annotation and the re-annotation:
</bodyText>
<table confidence="0.999076333333333">
Gold Standard BBN GS Re-annotated GS
Human 1 87.0% 85.0%
Human 2 85.3% 86.9%
Human 3 90.4% 91.8%
Human 4 86.5% 88.3%
SMT System 80.4% 89.7%
</table>
<tableCaption confidence="0.974225">
Table 2: Name translation accuracy with respect to BBN
and re-annotated Gold Standard on 1730 named entities
in 637 sentences.
</tableCaption>
<bodyText confidence="0.999521133333333">
Almost all scores went up with re-annotations, be-
cause the re-annotations more properly reward cor-
rect answers.
Based on the original annotations, all human
name translations were much better than our SMT
system. However, based on our re-annotation, the
results are quite different: our system has a higher
NEWA score and better name translations than 3 out
of 4 human annotators.
The evaluation results confirm that the original
annotation method produced a relative bias towards
the human translation its annotations were largely
based on, compared to other translations.
Table 3 provides more detailed NEWA results.
The addition of the transliteration module improves
our overall NEWA score from 87.8% to 89.7%, a
relative gain of 16% over base SMT system. For
names of persons (PER) and facilities (FAC), our
system outperforms all human translators. Hu-
mans performed much better on Person Nominals
(PER.Nom) such as Swede, Dutchmen, Americans.
Note that name translation quality varies greatly
between human translators, with error rates ranging
from 8.2-15.0% (absolute).
To make sure our name transliterator does not de-
grade the overall translation quality, we evaluated
our base SMT system with BLEU, as well as our
transliteration-augmented SMT system. Our stan-
dard newswire training set consists of 10.5 million
words of bitext (English side) and 1491 test sen-
</bodyText>
<page confidence="0.995679">
395
</page>
<table confidence="0.999793222222222">
NE Type Count Baseline SMT with Human 1 Human 2 Human 3 Human 4
SMT Transliteration
PER 342 266 (77.8%) 280 (81.9%) 210 (61.4%) 265 (77.5%) 278 (81.3%) 275 (80.4%)
GPE 910 863 (94.8%) 877 (96.4%) 867 (95.3%) 849 (93.3%) 885 (97.3%) 852 (93.6%)
ORG 332 280 (84.3%) 282 (84.9%) 263 (79.2%) 265 (79.8%) 293 (88.3%) 281 (84.6%)
FAC 27 18 (66.7%) 24 (88.9%) 21 (77.8%) 20 (74.1%) 22 (81.5%) 20 (74.1%)
PER.Nom 61 49 (80.3%) 48 (78.7%) 61 (100.0%) 56 (91.8%) 60 (98.4%) 57 (93.4%)
LOC 58 43 (74.1%) 41 (70.7%) 48 (82.8%) 48 (82.8%) 51 (87.9%) 43 (74.1%)
All types 1730 1519 (87.8%) 1552 (89.7%) 1470 (85.0%) 1503 (86.9%) 1589 (91.8%) 1528 (88.3%)
</table>
<tableCaption confidence="0.984852333333333">
Table 3: Name translation accuracy in end-to-end statistical machine translation (SMT) system for different named
entity (NE) types: Person (PER), Geopolitical Entity, which includes countries, provinces and towns (GPE), Organi-
zation (ORG), Facility (FAC), Nominal Person, e.g. Swede (PER.Nom), other location (LOC).
</tableCaption>
<bodyText confidence="0.997343923076923">
tences. The BLEU scores for the two systems were
50.70 and 50.96 respectively.
Finally, here are end-to-end machine translation
results for three sentences, with and without the
transliteration module, along with a human refer-
ence translation.
Old: Al-Basha leads a broad list of musicians such
as Bach.
New: Al-Basha leads a broad list of musical acts
such as Bach, Mozart, Beethoven, Chopin, Schu-
mann, Rachmaninoff, Ravel and Prokofiev.
Ref: Al-Bacha performs a long list of works by
composers such as Bach, Chopin, Beethoven,
Shumann, Rakmaninov, Ravel and Prokoviev.
Old: Earlier Israeli military correspondent turn
introduction programme ”Entertainment Bui”
New: Earlier Israeli military correspondent turn to
introduction of the programme ”Play Boy”
Ref: Former Israeli military correspondent turns
host for ”Playboy” program
Old: The Nikkei president company De Beers said
that ...
New: The company De Beers chairman Nicky Op-
penheimer said that ...
Ref: Nicky Oppenheimer, chairman of the De Beers
company, stated that ...
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999400214285714">
We have shown that a state-of-the-art statistical ma-
chine translation system can benefit from a dedi-
cated transliteration module to improve the transla-
tion of rare names. Improved named entity transla-
tion accuracy as measured by the NEWA metric in
general, and a reduction in dropped names in par-
ticular is clearly valuable to the human reader of
machine translated documents as well as for sys-
tems using machine translation for further informa-
tion processing. At the same time, there has been no
negative impact on overall quality as measured by
BLEU.
We believe that all components can be further im-
proved, e.g.
</bodyText>
<listItem confidence="0.987367333333333">
• Automatically retune the weights in the
transliteration scoring model.
• Improve robustness with respect to typos, in-
</listItem>
<bodyText confidence="0.932451642857143">
correct or missing translations, and badly
aligned sentences when marking up bitexts.
• Add more features for learning whether or not
a word should be transliterated, possibly using
source language morphology to better identify
non-name words never or rarely seen during
training.
Additionally, our transliteration method could be ap-
plied to other language pairs.
We find it encouraging that we already outper-
form some professional translators in name transla-
tion accuracy. The potential to exceed human trans-
lator performance arises from the patience required
to translate names right.
</bodyText>
<sectionHeader confidence="0.983428" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.6624825">
This research was supported under DARPA Contract
No. HR0011-06-C-0022.
</bodyText>
<page confidence="0.998619">
396
</page>
<sectionHeader confidence="0.993851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847">
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
Transliteration of Names in Arabic Text. In Proceed-
ings of the Association for Computational Linguistics
Workshop on Computational Approaches to Semitic
Languages.
Thorsten Brants, Alex Franz. 2006. Web 1T 5-gram
Version 1. Released by Google through the Linguis-
tic Data Consortium, Philadelphia, as LDC2006T13.
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, and
Shih-Chung Tsai. 1998. Proper Name Translation in
Cross-Language Information Retrieval. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics.
Hal Daum´e III, John Langford, and Daniel Marcu.
2006. Search-based Structured Prediction.
Submitted to the Machine Learning Journal.
http://pub.hal3.name/#daume06searn
David Day. 2007. Entity Translation 2007 Pilot Evalua-
tion (ET07). In proceedings of the Workshop on Auto-
matic Content Extraction (ACE). College Park, Mary-
land.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
Transliteration and Back-transliteration by Decision
Tree Learning. In Conference on Language Resources
and Evaluation.
Mehdi M. Kashani, Fred Popowich, and Fatiha Sadat.
2007. Automatic Transliteration of Proper Nouns
from Arabic to English. The Challenge ofArabic For
NLP/MT, 76-84.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association of Computational Lin-
guistics.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of the 35th Annual
Meeting of the Associationfor Computational Linguis-
tics.
Li Haizhou, Zhang Min, and Su Jian. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In Proceedings of the 42nd Annual Meeting on Asso-
ciation for ComputationalLinguistics.
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward Ma-
chine Transliteration by Learning Phonetic Similar-
ity. Sixth Conference on Natural Language Learning,
Taipei, Taiwan, 2002.
David Matthews. 2007. Machine Transliteration of
Proper Names. Master’s Thesis. School of Informat-
ics. University of Edinburgh.
Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001.
Using the Web as a Bilingual Dictionary. In Proceed-
ings of the Workshop on Data-driven Methods in Ma-
chine Translation.
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Irina
Temnikova, Anna Widiger, Wajdi Zaghouani, and Jan
Zizka. 2006. Multilingual Person Name Recognition
and Transliteration. CORELA - COgnition, REpre-
sentation, LAnguage, Poitiers, France. Volume 3/3,
number 2, pp. 115-123.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
Based Transliteration. In Proceedings of the 45th An-
nual Meeting on Association for Computational Lin-
guistics.
Richard Sproat, ChengXiang Zhai, and Tao Tao. 2006.
Named Entity Transliteration with Comparable Cor-
pora. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational Lin-
guistics.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. In
Proceedings of the COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages.
Stephen Wan and Cornelia Verspoor. 1998. Automatic
English-Chinese Name Transliteration for Develop-
ment of Multilingual Resources. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics. Montreal, Canada.
</reference>
<page confidence="0.998368">
397
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577301">
<title confidence="0.9984135">Name Translation in Statistical Machine Translation Learning When to Transliterate</title>
<author confidence="0.998148">Hermjakob Knight</author>
<affiliation confidence="0.9994115">University of Southern California Information Sciences Institute</affiliation>
<address confidence="0.99863">4676 Admiralty Way Marina del Rey, CA 90292, USA</address>
<author confidence="0.630201">Hal Daum´e</author>
<affiliation confidence="0.999809">University of Utah School of Computing</affiliation>
<address confidence="0.995639">50 S Central Campus Drive Salt Lake City, UT 84112, USA</address>
<email confidence="0.934002">me@hal3.name</email>
<abstract confidence="0.999552666666667">We present a method to transliterate names in the framework of end-to-end statistical machine translation. The system is trained to learn when to transliterate. For Arabic to English MT, we developed and trained a transliterator on a bitext of 7 million sentences and Google’s English terabyte ngrams and achieved better name translation accuracy than 3 out of 4 professional translators. The paper also includes a discussion of challenges in name translation evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine Transliteration of Names in Arabic Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics Workshop on Computational Approaches to Semitic Languages.</booktitle>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine Transliteration of Names in Arabic Text. In Proceedings of the Association for Computational Linguistics Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Released by Google through the Linguistic Data Consortium,</title>
<date>2006</date>
<pages>2006--13</pages>
<location>Philadelphia, as</location>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants, Alex Franz. 2006. Web 1T 5-gram Version 1. Released by Google through the Linguistic Data Consortium, Philadelphia, as LDC2006T13.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Sheng-Jie Huang</author>
<author>Yung-Wei Ding</author>
<author>Shih-Chung Tsai</author>
</authors>
<title>Proper Name Translation in Cross-Language Information Retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics. Hal Daum´e</booktitle>
<note>http://pub.hal3.name/#daume06searn</note>
<contexts>
<context position="2580" citStr="Chen et al., 1998" startWordPosition="375" endWordPosition="378">m the same corpus, with mistakes underlined: Ref1 composers such as Bach, missing name Chopin, Beethoven, Shumann, Rakmaninov, Ravel and Prokoviev Ref2 musicians such as Bach, Mozart, Chopin, Bethoven, Shuman, Rachmaninoff, Rafael and Brokoviev Ref3 composers including Bach, Mozart, Schopen, Beethoven, missing name Raphael, Rahmaniev and Brokofien Ref4 composers such as Bach, Mozart, missing name Beethoven, Schumann, Rachmaninov, Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research, e.g., (Knight and Graehl, 1997; Chen et al., 1998; Al-Onaizan, 2002). One approach is to “sound out” words and create new, plausible targetlanguage spellings that preserve the sounds of the source-language name as much as possible. Another approach is to phonetically match source-language names against a large list of target-language words Arabic input vAK.ñ ð P@ PñÓð pAK. ÉA.Ó �áJ�J��J�ƒñÓ ÉJ��¯@Pð � ¬ñ�JJ��KAÒkPð�àAÓñ ~ƒð �á�¯ñê�JJ�K.ð J�J��¯ñ»ðQK.ð � 389 Proceedings of ACL-08: HLT, pages 389–397, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics and phrases. Most of this work has been disconnected from end-</context>
</contexts>
<marker>Chen, Huang, Ding, Tsai, 1998</marker>
<rawString>Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, and Shih-Chung Tsai. 1998. Proper Name Translation in Cross-Language Information Retrieval. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics. Hal Daum´e III, John Langford, and Daniel Marcu. 2006. Search-based Structured Prediction. Submitted to the Machine Learning Journal. http://pub.hal3.name/#daume06searn</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Day</author>
</authors>
<title>Entity Translation</title>
<date>2007</date>
<booktitle>In proceedings of the Workshop on Automatic Content Extraction (ACE). College Park,</booktitle>
<location>Maryland.</location>
<contexts>
<context position="5266" citStr="Day, 2007" startWordPosition="792" endWordPosition="793">lations means that the SMT system may no longer have access to longer phrases that include the name. For example, our base SMT system translates •J��KP © JK. ùË Z@P �PñË@ (as a whole phrase) to “Premier Li Peng”, based on its bitext knowledge. However, if we force ©� J K. ù Ë to translate as a separate phrase to “Li Peng”, then the term Z@P jñË@ •J��P becomes ambiguous (with translations including “Prime Minister”, “Premier”, etc.), and we observe incorrect choices being subsequently made. To spur better work in name handling, an ACE entity-translation pilot evaluation was recently developed (Day, 2007). This evaluation involves a mixture of entity identification and translation concerns—for example, the scoring system asks for coreference determination, which may or may not be of interest for improving machine translation output. In this paper, we adopt a simpler metric. We ask: what percentage of source-language named entities are translated correctly? This is a precision metric. We can readily apply it to any base SMT system, and to human translations as well. Our goal in augmenting a base SMT system is to increase this percentage. A secondary goal is to make sure that our overall transla</context>
</contexts>
<marker>Day, 2007</marker>
<rawString>David Day. 2007. Entity Translation 2007 Pilot Evaluation (ET07). In proceedings of the Workshop on Automatic Content Extraction (ACE). College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byung-Ju Kang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic Transliteration and Back-transliteration by Decision Tree Learning.</title>
<date>2000</date>
<booktitle>In Conference on Language Resources and Evaluation.</booktitle>
<marker>Kang, Choi, 2000</marker>
<rawString>Byung-Ju Kang and Key-Sun Choi. 2000. Automatic Transliteration and Back-transliteration by Decision Tree Learning. In Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi M Kashani</author>
<author>Fred Popowich</author>
<author>Fatiha Sadat</author>
</authors>
<date>2007</date>
<booktitle>Automatic Transliteration of Proper Nouns from Arabic to English. The Challenge ofArabic For NLP/MT,</booktitle>
<pages>76--84</pages>
<contexts>
<context position="11425" citStr="Kashani et al., 2007" startWordPosition="1794" endWordPosition="1797">or This section describes how we transliterate Arabic words or phrases. Given a word such as � ¬ñ�JJ��KAÒkP or a phrase such as ÉJ 9@P •K�PñÓ, we want to find the English transliteration for it. This is not just a 391 romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoffand Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divide the task of transliteration into two steps: given an Arabic word or phrase to t</context>
</contexts>
<marker>Kashani, Popowich, Sadat, 2007</marker>
<rawString>Mehdi M. Kashani, Fred Popowich, and Fatiha Sadat. 2007. Automatic Transliteration of Proper Nouns from Arabic to English. The Challenge ofArabic For NLP/MT, 76-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.</booktitle>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Named entity transliteration and discovery from multilingual comparable corpora. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Machine Transliteration.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Associationfor Computational Linguistics.</booktitle>
<contexts>
<context position="2561" citStr="Knight and Graehl, 1997" startWordPosition="371" endWordPosition="374">ce translations taken from the same corpus, with mistakes underlined: Ref1 composers such as Bach, missing name Chopin, Beethoven, Shumann, Rakmaninov, Ravel and Prokoviev Ref2 musicians such as Bach, Mozart, Chopin, Bethoven, Shuman, Rachmaninoff, Rafael and Brokoviev Ref3 composers including Bach, Mozart, Schopen, Beethoven, missing name Raphael, Rahmaniev and Brokofien Ref4 composers such as Bach, Mozart, missing name Beethoven, Schumann, Rachmaninov, Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research, e.g., (Knight and Graehl, 1997; Chen et al., 1998; Al-Onaizan, 2002). One approach is to “sound out” words and create new, plausible targetlanguage spellings that preserve the sounds of the source-language name as much as possible. Another approach is to phonetically match source-language names against a large list of target-language words Arabic input vAK.ñ ð P@ PñÓð pAK. ÉA.Ó �áJ�J��J�ƒñÓ ÉJ��¯@Pð � ¬ñ�JJ��KAÒkPð�àAÓñ ~ƒð �á�¯ñê�JJ�K.ð J�J��¯ñ»ðQK.ð � 389 Proceedings of ACL-08: HLT, pages 389–397, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics and phrases. Most of this work has been dis</context>
<context position="11318" citStr="Knight and Graehl, 1997" startWordPosition="1776" endWordPosition="1779">a variant form met our threshold of occurring at least 20% as often as the most dominant form. 3 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as � ¬ñ�JJ��KAÒkP or a phrase such as ÉJ 9@P •K�PñÓ, we want to find the English transliteration for it. This is not just a 391 romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoffand Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to t</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine Transliteration. In Proceedings of the 35th Annual Meeting of the Associationfor Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Haizhou</author>
<author>Zhang Min</author>
<author>Su Jian</author>
</authors>
<title>A Joint Source-Channel Model for Machine Transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for ComputationalLinguistics.</booktitle>
<marker>Haizhou, Min, Jian, 2004</marker>
<rawString>Li Haizhou, Zhang Min, and Su Jian. 2004. A Joint Source-Channel Model for Machine Transliteration. In Proceedings of the 42nd Annual Meeting on Association for ComputationalLinguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Hsin-Hsi Chen</author>
</authors>
<date>2002</date>
<booktitle>Backward Machine Transliteration by Learning Phonetic Similarity. Sixth Conference on Natural Language Learning,</booktitle>
<location>Taipei, Taiwan,</location>
<marker>Lin, Chen, 2002</marker>
<rawString>Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward Machine Transliteration by Learning Phonetic Similarity. Sixth Conference on Natural Language Learning, Taipei, Taiwan, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Matthews</author>
</authors>
<title>Machine Transliteration of Proper Names. Master’s Thesis.</title>
<date>2007</date>
<institution>School of Informatics. University of Edinburgh.</institution>
<contexts>
<context position="11376" citStr="Matthews, 2007" startWordPosition="1788" endWordPosition="1789"> as the most dominant form. 3 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as � ¬ñ�JJ��KAÒkP or a phrase such as ÉJ 9@P •K�PñÓ, we want to find the English transliteration for it. This is not just a 391 romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoffand Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divide the task of transliteration in</context>
</contexts>
<marker>Matthews, 2007</marker>
<rawString>David Matthews. 2007. Machine Transliteration of Proper Names. Master’s Thesis. School of Informatics. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Teruka Saito</author>
<author>Kenji Suzuki</author>
</authors>
<title>Using the Web as a Bilingual Dictionary.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Data-driven Methods in Machine Translation.</booktitle>
<marker>Nagata, Saito, Suzuki, 2001</marker>
<rawString>Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the Web as a Bilingual Dictionary. In Proceedings of the Workshop on Data-driven Methods in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Camelia Ignat</author>
<author>Irina Temnikova</author>
<author>Anna Widiger</author>
<author>Wajdi Zaghouani</author>
<author>Jan Zizka</author>
</authors>
<title>Multilingual Person Name Recognition and Transliteration. CORELA - COgnition, REpresentation, LAnguage,</title>
<date>2006</date>
<volume>3</volume>
<pages>115--123</pages>
<location>Poitiers,</location>
<marker>Pouliquen, Steinberger, Ignat, Temnikova, Widiger, Zaghouani, Zizka, 2006</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Irina Temnikova, Anna Widiger, Wajdi Zaghouani, and Jan Zizka. 2006. Multilingual Person Name Recognition and Transliteration. CORELA - COgnition, REpresentation, LAnguage, Poitiers, France. Volume 3/3, number 2, pp. 115-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>SubstringBased Transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11402" citStr="Sherif and Kondrak, 2007" startWordPosition="1790" endWordPosition="1793">inant form. 3 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as � ¬ñ�JJ��KAÒkP or a phrase such as ÉJ 9@P •K�PñÓ, we want to find the English transliteration for it. This is not just a 391 romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoffand Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divide the task of transliteration into two steps: given an Ara</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. SubstringBased Transliteration. In Proceedings of the 45th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>ChengXiang Zhai</author>
<author>Tao Tao</author>
</authors>
<title>Named Entity Transliteration with Comparable Corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting on Association for Computational Linguistics.</booktitle>
<marker>Sproat, Zhai, Tao, 2006</marker>
<rawString>Richard Sproat, ChengXiang Zhai, and Tao Tao. 2006. Named Entity Transliteration with Comparable Corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Glover Stalls</author>
<author>Kevin Knight</author>
</authors>
<title>Translating Names and Technical Terms in Arabic Text.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="11343" citStr="Stalls and Knight, 1998" startWordPosition="1780" endWordPosition="1783">reshold of occurring at least 20% as often as the most dominant form. 3 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as � ¬ñ�JJ��KAÒkP or a phrase such as ÉJ 9@P •K�PñÓ, we want to find the English transliteration for it. This is not just a 391 romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoffand Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divi</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Bonnie Glover Stalls and Kevin Knight. 1998. Translating Names and Technical Terms in Arabic Text. In Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Cornelia Verspoor</author>
</authors>
<title>Automatic English-Chinese Name Transliteration for Development of Multilingual Resources.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Montreal, Canada.</location>
<marker>Wan, Verspoor, 1998</marker>
<rawString>Stephen Wan and Cornelia Verspoor. 1998. Automatic English-Chinese Name Transliteration for Development of Multilingual Resources. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics. Montreal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>