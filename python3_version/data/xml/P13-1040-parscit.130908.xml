<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000560">
<title confidence="0.994271">
Extracting bilingual terminologies from comparable corpora
</title>
<author confidence="0.997846">
Ahmet Aker, Monica Paramita, Robert Gaizauskas
</author>
<affiliation confidence="0.99829">
University of Sheffield
</affiliation>
<email confidence="0.992939">
ahmet.aker, m.paramita, r.gaizauskas@sheffield.ac.uk
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99916494117647">
In this paper we present a method for extracting
bilingual terminologies from comparable corpora.
In our approach we treat bilingual term extrac-
tion as a classification problem. For classification
we use an SVM binary classifier and training data
taken from the EUROVOC thesaurus. We test our
approach on a held-out test set from EUROVOC
and perform precision, recall and f-measure eval-
uations for 20 European language pairs. The per-
formance of our classifier reaches the 100% pre-
cision level for many language pairs. We also
perform manual evaluation on bilingual terms ex-
tracted from English-German term-tagged compa-
rable corpora. The results of this manual evalu-
ation showed 60-83% of the term pairs generated
are exact translations and over 90% exact or partial
translations.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999885771428572">
Bilingual terminologies are important for various
applications of human language technologies, in-
cluding cross-language information search and re-
trieval, statistical machine translation (SMT) in
narrow domains and computer-aided assistance
to human translators. Automatic construction of
bilingual terminology mappings has been investi-
gated in many earlier studies and various methods
have been applied to this task. These methods may
be distinguished by whether they work on parallel
or comparable corpora, by whether they assume
monolingual term recognition in source and target
languages (what Moore (2003) calls symmetrical
approaches) or only in the source (asymmetric ap-
proaches), and by the extent to which they rely on
linguistic knowledge as opposed to simply statis-
tical techniques.
We focus on techniques for bilingual term ex-
traction from comparable corpora – collections of
source-target language document pairs that are not
direct translations but are topically related. We
choose to focus on comparable corpora because
for many less widely spoken languages and for
technical domains where new terminology is con-
stantly being introduced, parallel corpora are sim-
ply not available. Techniques that can exploit such
corpora to deliver bilingual terminologies are of
significant practical interest in these cases.
The rest of the paper is structured as follows.
In Section 2 we outline our method. In Section
3 we review related work on bilingual term ex-
traction. Section 4 describes feature extraction for
term pair classification. In Section 5 we present
the data used in our evaluations and discuss our
results. Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.979045" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.99995868">
The method we present below for bilingual term
extraction is a symmetric approach, i.e. it assumes
a method exists for monolingual term extraction in
both source and target languages. We do not pre-
scribe what a term must be. In particular we do not
place any particular syntactic restrictions on what
constitutes an allowable term, beyond the require-
ment that terms must be contiguous sequences of
words in both source and target languages.
Our method works by first pairing each term ex-
tracted from a source language document 5 with
each term extracted from a target language doc-
ument T aligned with 5 in the comparable cor-
pus. We then treat term alignment as a binary
classification task, i.e. we extract features for each
source-target language potential term pair and de-
cide whether to classify the pair as a term equiv-
alent or not. For classification purposes we use
an SVM binary classifier. The training data for
the classifier is derived from EUROVOC (Stein-
berger et al., 2002), a term thesaurus covering
the activities of the EU and the European Parlia-
ment. We have run our approach on the 21 official
EU languages covered by EUROVOC, construct-
ing 20 language pairs with English as the source
</bodyText>
<page confidence="0.976405">
402
</page>
<note confidence="0.914349">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402–411,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999317277777778">
language. Considering all these languages allows
us to directly compare our method’s performance
on resource-rich (e.g. German, French, Spanish)
and under-resourced languages (e.g. Latvian, Bul-
garian, Estonian). We perform two different tests.
First, we evaluate the performance of the classifier
on a held-out term-pair list from EUROVOC us-
ing the standard measures of recall, precision and
F-measure. We run this evaluation on all 20 lan-
guage pairs. Secondly, we test the system’s per-
formance on obtaining bilingual terms from com-
parable corpora. This second test simulates the
situation of using the term alignment system in a
real world scenario. For this evaluation we col-
lected English-German comparable corpora from
Wikipedia, performed monolingual term tagging
and ran our tool over the term tagged corpora to
extract bilingual terms.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999979241935484">
Previous studies have investigated the extraction
of bilingual terms from parallel and comparable
corpora. For instance, Kupiec (1993) uses statisti-
cal techniques and extracts bilingual noun phrases
from parallel corpora tagged with terms. Daille
et al. (1994), Fan et al. (2009) and Okita et
al. (2010) also apply statistical methods to extract
terms/phrases from parallel corpora. In addition
to statistical methods Daille et al. use word trans-
lation information between two words within the
extracted terms as a further indicator of the correct
alignment. More recently, Bouamor et al. (2012)
use vector space models to align terms. The en-
tries in the vectors are co-occurrence statistics be-
tween the terms computed over the entire corpus.
Bilingual term alignment methods that work on
comparable corpora use essentially three sorts of
information: (1) cognate information, typically es-
timated using some sort of transliteration similar-
ity measure (2) context congruence, a measure of
the extent to which the words that the source term
co-occurs with have the same sort of distribution
and co-occur with words with the same sort dis-
tribution as do those words that co-occur with the
candidate term and (3) translation of component
words in the term and/or in context words, where
some limited dictionary exists. For example, in
Rapp (1995), Fung and McKeown (1997), Morin
et. al. (2007), Cao and Li (2002) and Ismail
and Manandhar (2010) the context of text units
is used to identify term mappings. Transliteration
and cognate-based information is exploited in Al-
Onaizan and Knight (2002), Knight and Graehl
(1998), Udupa et. al. (2008) and Aswani and
Gaizauskas (2010).
Very few approaches have treated term align-
ment as a classification problem suitable for ma-
chine learning (ML) techniques. So far as we
are aware, only Cao and Li (2002), who treat
only base noun phrase (NP) mapping, consider the
problem this way. However, it naturally lends it-
self to being viewed as a classification task, as-
suming a symmetric approach, since the differ-
ent information sources mentioned above can be
treated as features and each source-target language
potential term pairing can be treated as an in-
stance to be fed to a binary classifier which decides
whether to align them or not. Our work differs
from that of Cao and Li (2002) in several ways.
First they consider only terms consisting of noun-
noun pairs. Secondly for a given source language
term (N1, N2), target language candidate terms
are proposed by composing all translations (given
by a bilingual dictionary) of N1 into the target lan-
guage with all translations of N2. We remove both
these restrictions. By considering all terms pro-
posed by monolingual term extractors we consider
terms that are syntactically much richer than noun-
noun pairs. In addition, the term pairs we align are
not constrained by an assumption that their com-
ponent words must be translations of each other as
found in a particular dictionary resource.
</bodyText>
<sectionHeader confidence="0.992734" genericHeader="method">
4 Feature extraction
</sectionHeader>
<bodyText confidence="0.999979666666667">
To align or map source and target terms we use an
SVM binary classifier (Joachims, 2002) with a lin-
ear kernel and the trade-off between training error
and margin parameter c = 10. Within the classi-
fier we use language dependent and independent
features described in the following sections.
</bodyText>
<subsectionHeader confidence="0.998159">
4.1 Dictionary based features
</subsectionHeader>
<bodyText confidence="0.99986">
The dictionary based features are language depen-
dent and are computed using bilingual dictionar-
ies which are created with GIZA++ (Och and Ney,
2000; Och and Ney, 2003). The DGT-TM par-
allel data (Steinberger et al., 2012) was input to
GIZA++ to obtain the dictionaries. Dictionary en-
tries have the form (s, ti, pi), where s is a source
word, ti is the i-th translation of s in the dictio-
nary and pi is the probability that s is translated
by ti, the pi’s summing to 1 for each s in the dic-
tionary. From the dictionaries we removed all en-
tries with pi &lt; 0.05. In addition we also removed
</bodyText>
<page confidence="0.9988">
403
</page>
<bodyText confidence="0.81425565">
every entry from the dictionary where the source
word was less than four characters and the target
word more than five characters in length and vice
versa. This step is performed to try to eliminate
translation pairs where a stop word is translated
into a non-stop word. After performing these fil-
tering steps we use the dictionaries to extract the
following language dependent features:
• isFirstWordTranslated is a binary feature in-
dicating whether the first word in the source
term is a translation of the first word in the
target term. To address the issue of com-
pounding, e.g. for languages like German
where what is a multi-word term in En-
glish may be expressed as a single com-
pound word, we check whether the com-
pound source term has an initial prefix that
matches the translation of the first target
word, provided that translation is at least 5
character in length.
</bodyText>
<listItem confidence="0.996037259259259">
• isLastWordTranslated is a binary feature in-
dicating whether the last word in the source
term is a translation of the last word in the
target term. As with the previous feature in
case of compound terms we check whether
the source term ends with the translation of
the target last word.
• percentageOfTranslatedWords returns the
percentage of words in the source term which
have their translations in the target term. To
address compound terms we check for each
source word translation whether it appears
anywhere within the target term.
• percentageOfNotTranslatedWords returns
the percentage of words of the source term
which have no translations in the target term.
• longestTranslatedUnitInPercentage returns
the ratio of the number of words within the
longest contiguous sequence of source words
which has a translation in the target term to
the length of the source term, expressed as a
percentage. For compound terms we proceed
as with percentageOfTranslatedWords.
• longestNotTranslatedUnitInPercentage re-
turns the percentage of the number of words
within the longest sequence of source words
which have no translations in the target term.
</listItem>
<bodyText confidence="0.9867500625">
These six features are direction-dependent and
are computed in both directions, reversing which
language is taken as the source and which as
the target. We also compute another feature av-
eragePercentageOfTranslatedWords which builds
the average between the feature values of percent-
ageOfTranslatedWords from source to target and
target to source. Thus in total we have 13 dic-
tionary based features. Note for non-compound
terms if we compare two words for equality we do
not perform string match but rather use the Lev-
enshtein Distance (see Section 4.2) between the
two words and treat them as equal if the Leven-
shtein Distance returns &gt;= 0.95. This is per-
formed to capture words with morphological dif-
ferences. We set 0.95 experimentally.
</bodyText>
<subsectionHeader confidence="0.998364">
4.2 Cognate based features
</subsectionHeader>
<bodyText confidence="0.9974285">
Dictionaries mostly fail to return translation en-
tries for named entities (NEs) or specialized termi-
nology. Because of this we also use cognate based
methods to perform the mapping between source
and target words or vice versa. Aker et al. (2012)
have applied (1) Longest Common Subsequence
Ratio, (2) Longest Common Substring Ratio, (3)
Dice Similarity, (4) Needleman-Wunsch Distance
and (5) Levenshtein Distance in order to extract
parallel phrases from comparable corpora. We
adopt these measures within our classifier. Each
of them returns a score between 0 and 1.
</bodyText>
<listItem confidence="0.997816">
• Longest Common Subsequence Ratio
(LCSR): The longest common subsequence
(LCS) measure measures the longest com-
mon non-consecutive sequence of characters
between two strings. For instance, the words
“dollars” and “dolari” share a sequence of
5 non-consecutive characters in the same
ordering. We make use of dynamic program-
ming (Cormen et al., 2001) to implement
LCS, so that its computation is efficient and
can be applied to a large number of possible
term pairs quickly. We normalize relative to
the length of the longest term:
</listItem>
<equation confidence="0.9896065">
len[LCS(X, Y )]
LCSR(X, Y ) = max[len(X), len(Y )]
</equation>
<bodyText confidence="0.9999775">
where LCS is the longest common subse-
quence between two strings and characters
in this subsequence need not be contiguous.
The shorthand len stands for length.
</bodyText>
<listItem confidence="0.9969925">
• Longest Common Substring Ratio (LC-
STR): The longest common substring
(LCST) measure is similar to the LCS
measure, but measures the longest common
</listItem>
<page confidence="0.996671">
404
</page>
<bodyText confidence="0.997896">
consecutive string of characters that two
strings have in common. I.e. given two terms
we need to find the longest character n-gram
the terms share. The formula we use for the
LCSTR measure is a ratio as in the previous
measure:
</bodyText>
<equation confidence="0.8987615">
len[LCST (X, Y )]
LCSTR(X,Y ) = max[len(X), len(Y )]
</equation>
<listItem confidence="0.964734">
• Dice Similarity:
</listItem>
<equation confidence="0.993603">
dice =
2 ∗ LCST
len(X) + len(Y)
</equation>
<listItem confidence="0.8063605">
• Needlemann Wunsch Distance (NWD):
LCST
</listItem>
<equation confidence="0.9541285">
NWD =
min[len(X) + len(Y)]
</equation>
<listItem confidence="0.988244333333333">
• Levenshtein Distance (LD): This method
computes the minimum number of operations
necessary to transform one string into an-
other. The allowable operations are insertion,
deletion, and substitution. Compared to the
previous methods, which all return scores be-
tween 0 and 1, this method returns a score s
that lies between 0 and n. The number n rep-
resents the maximum number of operations
to convert an arbitrarily dissimilar string to a
given string. To have a uniform score across
all cognate methods we normalize s so that
it lies between 0 and 1, subtracting from 1 to
convert it from a distance measure to a simi-
larty measure:
</listItem>
<equation confidence="0.613287">
LD
LDnormalized = 1 max[len(X), len(Y)]
</equation>
<subsectionHeader confidence="0.8635375">
4.3 Cognate based features with term
matching
</subsectionHeader>
<bodyText confidence="0.999988641025641">
The cognate methods assume that the source and
target language strings being compared are drawn
from the same character set and fail to capture
the corresponding terms if this is not the case.
For instance, the cognate methods are not directly
applicable to the English-Bulgarian and English-
Greek language pairs, as both the Bulgarian and
Greek alphabets, which are Cyrillic-based, differ
from the English Latin-based alphabet. However,
the use of distinct alphabets is not the only prob-
lem when comparing source and target terms. Al-
though most EU languages use the Latin alpha-
bet, the occurrence of special characters and di-
acritics, as well spelling and phonetic variations,
are further challenges which are faced by term or
entity mapping methods, especially in determin-
ing the variants of the same mention of the entity
(Snae, 2007; Karimi et al., 2011).1 We address this
problem by mapping a source term to the target
language writing system or vice versa. For map-
ping we use simple character mappings between
the writing systems, such as α → a, φ → ph,
etc., from Greek to English. The rules allow one
character on the lefthand side (source language) to
map onto one or more characters on the righthand
side (target language). We created our rules man-
ually based on sound similarity between source
and target language characters. We created map-
ping rules for 20 EU language pairs using primar-
ily Wikipedia as a resource for describing phonetic
mappings to English.
After mapping a term from source to target lan-
guage we apply the cognate metrics described in
4.2 to the resulting mapped term and the original
term in the other language. Since we perform both
target to source and source to target mapping, the
number of cognate feature scores on the mapped
terms is 10 – 5 due to source to target mapping
and 5 due to target to source mapping.
</bodyText>
<subsectionHeader confidence="0.989529">
4.4 Combined features
</subsectionHeader>
<bodyText confidence="0.999898">
We also combined dictionary and cognate based
features. The combined features are as follows:
</bodyText>
<listItem confidence="0.810065105263158">
• isFirstWordCovered is a binary feature indi-
cating whether the first word in the source
term has a translation (i.e. has a translation
entry in the dictionary regardless of the score)
or transliteration (i.e. if one of the cognate
metric scores is above 0.72) in the target term.
The threshold 0.7 for transliteration similar-
ity is set experimentally using the training
data. To do this we iteratively ran feature
extraction, trained the classifier and recorded
precision on the training data using a thresh-
old value chosen from the interval [0, 1] in
steps of 0.1. We selected as final threshold
value, the lowest value for which the preci-
sion score was the same as when the thresh-
old value was set to 1.
• isLastWordCovered is similar to the previ-
ous feature one but indicates whether the last
word in the source term has a translation or
</listItem>
<footnote confidence="0.99928975">
1Assuming the terms are correctly spelled, otherwise the
misspelling is another problem.
2Note that we use the cognate scores obtained on the char-
acter mapped terms.
</footnote>
<page confidence="0.997116">
405
</page>
<bodyText confidence="0.938247">
transliteration in the target term. If this is the
case, 1 is returned otherwise 0.
</bodyText>
<listItem confidence="0.9908725">
• percentageOfCoverage returns the percent-
age of source term words which have a trans-
lation or transliteration in the target term.
• percentageOfNonCoverage returns the per-
centage of source term words which have nei-
ther a translation nor transliteration in the tar-
get term.
• difBetweenCoverageAndNonCoverage
returns the difference between the last two
features.
</listItem>
<bodyText confidence="0.9999107">
Like the dictionary based features, these five
features are direction-dependent and are computed
in both directions – source to target and target to
source, resulting in 10 combined features.
In total we have 38 features – 13 features based
on dictionary translation as described in Section
4.1, 5 cognate related features as outlined in Sec-
tion 4.2, 10 cognate related features derived from
character mappings over terms as described in
Section 4.3 and 10 combined features.
</bodyText>
<sectionHeader confidence="0.999447" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.969673">
5.1 Data Sources
</subsectionHeader>
<bodyText confidence="0.999760666666667">
In our experiments we use two different data re-
sources: EUROVOC terms and comparable cor-
pora collected from Wikipedia.
</bodyText>
<subsubsectionHeader confidence="0.672362">
5.1.1 EUROVOC terms
</subsubsectionHeader>
<bodyText confidence="0.999463">
EUROVOC is a term thesaurus covering the ac-
tivities of the EU and the European Parliament in
particular. It contains 6797 term entries in 24 dif-
ferent languages including 22 EU languages and
Croatian and Serbian (Steinberger et al., 2002).
</bodyText>
<subsubsectionHeader confidence="0.791055">
5.1.2 Comparable Corpora
</subsubsectionHeader>
<bodyText confidence="0.999743363636364">
We also built comparable corpora in the infor-
mation technology (IT) and automotive domains
by gathering documents from Wikipedia for the
English-German language pair. First, we man-
ually chose one seed document in English as a
starting point for crawling in each domain3. We
then identified all articles to which the seed doc-
ument is linked and added them to the crawling
queue. This process is performed recursively for
each document in the queue. Since our aim is to
build a comparable corpus, we only added English
</bodyText>
<footnote confidence="0.992981">
3http://en.wikipedia.org/wiki/Information technology for
IT and http://en.wikipedia.org/wiki/Automotive industry for
automotive domain.
</footnote>
<bodyText confidence="0.999904571428572">
documents which have an inter-language link in
Wikipedia to a German document. We set a max-
imum depth of 3 in the recursion to limit size of
the crawling set, i.e. documents are crawled only
if they are within 3 clicks of the seed documents.
A score is then calculated to represent the impor-
tance of each document di in this domain:
</bodyText>
<equation confidence="0.939715">
scoredi =
</equation>
<bodyText confidence="0.9999929">
where n is the total number of documents in the
queue, freqdij is 1 if di is linked to dj, or 0 other-
wise, and depthdj is the number of clicks between
dj and the seed document. After all documents in
the queue were assigned a score, we gathered the
top 1000 documents and used inter-language link
information to extract the corresponding article in
the target language.
We pre-processed each Wikipedia article by
performing monolingual term tagging using
TWSC (Pinnis et al., 2012). TWSC is a term ex-
traction tool which identifies terms ranging from
one to four tokens in length. First, it POS-tags
each document. For German POS-tagging we
use TreeTagger (Schmid, 1995). Next, it uses
term grammar rules, in the form of sequences of
POS tags or non-stop words, to identify candidate
terms. Finally, it filters the candidate terms us-
ing various statistical measures, such as pointwise
mutual information and TF*IDF.
</bodyText>
<subsectionHeader confidence="0.995887">
5.2 Performance test of the classifier
</subsectionHeader>
<bodyText confidence="0.99516525">
To test the classifier’s performance we evaluated it
against a list of positive and negative examples of
bilingual term pairs using the measures of preci-
sion, recall and F-measure. We used 21 EU offi-
cial languages, including English, and paired each
non-English language with English, leading to 20
language pairs.4 In the evaluation we used 600
positive term pairs taken randomly from the EU-
ROVOC term list. We also created around 1.3M
negative term pairs by pairing a source term with
200 randomly chosen distinct target terms. We
select such a large number to simulate the real
application scenario where the classifier will be
confronted with a huge number of negative cases
4Note that we do not use the Maltese-English language
pair, as for this pair we found that 5861 out of 6797 term
pairs were identical, i.e. the English and the Maltese terms
were the same. Excluding Maltese, the average number of
identical terms between a non-English language and English
in the EUROVOC data is 37.7 (out of a possible 6797).
</bodyText>
<equation confidence="0.472861">
n freqdij
j=1 depthdj
</equation>
<page confidence="0.998819">
406
</page>
<tableCaption confidence="0.9958475">
Table 1: Wikipedia term pairs processed and judged as pos-
itive by the classifier.
</tableCaption>
<table confidence="0.999146">
Processed Positive
DE IT 11597K 3249
DE Automotive 12307K 1772
</table>
<bodyText confidence="0.999938125">
and a relatively small number of positive pairs.
The 600 positive examples contain 200 single term
pairs (i.e. single word on both sides), 200 term
pairs with a single word on only one side (either
source or target) and 200 term pairs with more
than one word on each side. For training we took
the remaining 6200 positive term pairs from EU-
ROVOC and constructed another 6200 term pairs
as negative examples, leading to total of 12400
term pairs. To construct the 6200 negative exam-
ples we used the 6200 terms on the source side
and paired each source term with an incorrect tar-
get term. Note that we ensure that in both train-
ing and testing the set of negative and positive
examples do not overlap. Furthermore, we per-
formed data selection for each language pair sep-
arately. This means that the same pairs found
in, e.g., English-German are not necessarily the
same as in English-Italian. The reason for this is
that the translation lengths, in number of words,
vary between language pairs. For instance adult
education is translated into Erwachsenenbildung
in German and contains just a single word (al-
though compound). The same term is translated
into istruzione degli adulti in Italian and contains
three words. For this reason we carry out the data
preparation process separately for each language
pair in order to obtain the three term pair sets con-
sisting of term pairs with only a single word on
each side, term pairs with a single word on just
one side and term pairs with multiple words on
both sides.
</bodyText>
<subsectionHeader confidence="0.998982">
5.3 Manual evaluation
</subsectionHeader>
<bodyText confidence="0.999868777777778">
For this evaluation we used the Wikipedia com-
parable corpora collected for the English-German
(EN-DE) language pair. For each pair of
Wikipedia articles we used the terms tagged by
TWSC and aligned each source term with every
target term. This means if both source and target
articles contain 100 terms then this leads to 10K
term pairs. We extracted features for each pair
of terms and ran the classifier to decide whether
the pair is positive or negative. Table 1 shows the
number of term pairs processed and the count of
pairs classified as positive. Table 2 shows five
positive term pairs extracted from the English-
German comparable corpora for each of the IT and
automotive domains. We manually assessed a sub-
set of the positive examples. We asked human as-
sessors to categorize each term pair into one of the
following categories:
</bodyText>
<listItem confidence="0.997976642857143">
1. Equivalence: The terms are exact transla-
tions/transliterations of each other.
2. Inclusion: Not an exact transla-
tion/transliteration, but an exact transla-
tion/transliteration of one term is entirely
contained within the term in the other lan-
guage, e.g: “F1 car racing” vs “Autorennen
(car racing)”.
3. Overlap: Not category 1 or 2, but the terms
share at least one translated/transliterated
word, e.g: “hybrid electric vehicles” vs “hy-
bride bauteile (hybrid components)”.
4. Unrelated: No word in either term is a trans-
lation/transliteration of a word in the other.
</listItem>
<bodyText confidence="0.9984776">
In the evaluation we randomly selected 300
pairs for each domain and showed them to two
German native speakers who were fluent in En-
glish. We asked the assessors to place each of the
term pair into one of the categories 1 to 4.
</bodyText>
<sectionHeader confidence="0.58141" genericHeader="evaluation">
5.4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.608251">
5.4.1 Performance test of the classifier
</subsectionHeader>
<bodyText confidence="0.999928272727273">
The results of the classifier evaluation are shown
in Table 3. The results show that the overall per-
formance of the classifier is very good. In many
cases the precision scores reach 100%. The low-
est precision score is obtained for Lithuanian (LT)
with 67%. For this language we performed an er-
ror analysis. In total there are 221 negative ex-
amples classified as positive. All these terms are
multi-term, i.e. each term pair contains at least
two words on each side. For the majority of the
misclassified terms – 209 in total – 50% or more
of the words on one side are either translations or
cognates of words on the other side. Of these, 187
contained 50% or more translation due to cognate
words – examples of such cases are capital in-
crease – kapitalo eksportas or Arab organisation
– Arabu lyga with the cognates capital – kapitalo
and Arab – Arabu respectively. For the remain-
der, 50% or more of the words on one side are
dictionary translations of words on the other side.
In order to understand the reason why the classi-
fier treats such cases as positive we examined the
</bodyText>
<page confidence="0.999341">
407
</page>
<tableCaption confidence="0.999069">
Table 2: Example positive pairs for English-German.
</tableCaption>
<table confidence="0.999529666666667">
IT Automotive
chromatographic technique — chromatographie methode distribution infrastructure — versorgungsinfrastruktur
electrolytic capacitor — elektrolytkondensatoren ambient temperature — außenlufttemperatur
natural user interfaces — nat¨urliche benutzerschnittstellen higher cetane number — erh¨ohter cetanzahl
anode voltage — anodenspannung fuel tank — kraftstoffpumpe
digital subscriber loop — digitaler teilnehmeranschluss hydrogen powered vehicle — wasserstoff fahrzeug
</table>
<tableCaption confidence="0.997458">
Table 3: Classifier performance results on EUROVOC data (P stands for precision, R for recall and F for F-measure). Each
language is paired with English. The test set contains 600 positive and 1359400 negative examples.
</tableCaption>
<table confidence="0.99912625">
ET HU NL DA SV DE LV FI PT SL FR IT LT SK CS RO PL ES EL BG
P 1 1 .98 1 1 .98 1 1 .7 1 1 1 .67 .81 1 1 1 1 1 1
R .67 .72 .82 .69 .81 .77 .78 .65 .82 .66 .66 .7 .77 .84 .72 .78 .69 .8 .78 .79
F .80 .83 .89 .81 .89 .86 .87 .78 .75 .79 .79 .82 .71 .91 .83 .87 .81 .88 .87 .88
</table>
<bodyText confidence="0.99929084375">
training data and found 467 positive pairs which
had the same characteristics as the negative exam-
ples in the testing set classified. We removed these
467 entries from the training set and re-trained the
classifier. The results with the new classifier are
99% precision, 68% recall and 80% F score.
In addition to Lithuanian, two further lan-
guages, Portuguese (PT) and Slovak (SK), also
had substantially lower precision scores. For these
languages we also removed positive entries falling
into the same problem categories as the LT ones
and trained new classifiers with the filtered train-
ing data. The precision results increased substan-
tially for both PT and SK – 95% precision, 76%
recall, 84% F score for PT and 94% precision,
72% recall, 81% F score for SK. The recall scores
are lower than the precision scores, ranging from
65% to 84%. We have investigated the recall prob-
lem for FI, which has the lowest recall score at
65%. We observed that all the missing term pairs
were not cognates. Thus, the only way these terms
could be recognized as positive is if they are found
in the GIZA++ dictionaries. However, due to data
sparsity in these dictionaries this did not happen in
these cases. For these term pairs either the source
or target terms were not found in the dictionar-
ies. For instance, for the term pair offshoring —
uudelleensijoittautuminen the GIZA++ dictionary
contains the entry offshoring but according to the
dictionary it is not translated into uudelleensijoit-
tautuminen, which is the matching term in EU-
ROVOC.
</bodyText>
<subsectionHeader confidence="0.905002">
5.4.2 Manual evaluation
</subsectionHeader>
<bodyText confidence="0.96958675">
The results of the manual evaluation are shown in
Table 4. From the results we can see that both as-
sessors judge above 80% of the IT domain terms
as category 1 – the category containing equivalent
</bodyText>
<tableCaption confidence="0.999015">
Table 4: Results of the EN-DE manual evaluation by two
annotators. Numbers reported per category are percentages.
</tableCaption>
<table confidence="0.9989016">
Domain Ann. 1 2 3 4
IT P1 81 6 6 7
P2 83 7 7 3
Automotive P1 66 12 16 6
P2 60 15 16 9
</table>
<bodyText confidence="0.999569833333333">
term pairs. Only a small proportion of the term
pairs are judged as belonging to category 4 (3–7%)
– the category containing unrelated term pairs. For
the automotive domain the proportion of equiva-
lent term pairs varies between 60 and 66%. For
unrelated term pairs this is below 10% for both as-
sessors.
We investigated the inter-annotator agreement.
Across the four classes the percentage agreement
was 83% for the automotive domain term pairs and
86% for the IT domain term pairs. The kappa
statistic, n, was .69 for the automotive domain
pairs and .52 for the IT domain. We also consid-
ered two class agreement where we treated term
pairs within categories 2 and 3 as belonging to
category 4 (i.e. as “incorrect” translations). In
this case, for the automotive domain the percent-
age agreement was 90% and n = 0.72 and for the
IT domain percentage agreement was 89% with
n = 0.55. The agreement in the automotive do-
main is higher than in the IT one although both
judges were computer scientists. We analyzed
the differences and found that they differ in cases
where the German and the English term are both in
English. One of the annotators treated such cases
as correct translation, whereas the other did not.
We also checked to ensure our technique was
not simply rediscovering our dictionaries. Since
the GIZA++ dictionaries contain only single
word–single word mappings, we examined the
</bodyText>
<page confidence="0.99737">
408
</page>
<bodyText confidence="0.981777393442623">
newly aligned term pairs that consisted of one
word on both source and target sides. Taking both
the IT and automotive domains together, our al-
gorithm proposed 5021 term pairs of which 2751
(55%) were word-word term pairs. 462 of these
(i.e. 17% of the word-word term pairs or 9% of
the overall set of aligned term pairs) were already
in either the EN-DE or DE-EN GIZA++ dictionar-
ies. Thus, of our newly extracted term pairs a rela-
tively small proportion are rediscovered dictionary
entries. We also checked our evaluation data to see
what proportion of the assessed term pairs were
already to be found in the GIZA++ dictionaries.
A total of 600 term pairs were put in front of the
judges of which 198 (33%) were word-word term
pairs. Of these 15 (less than 8% of the word-word
pairs and less then 3% of the overall assessed set of
assessed term pairs) were word-word pairs already
in the dictionaries. We conclude that our evalua-
tion results are not unduly affected by assessing
term pairs which were given to the algorithm.
Error analysis For both domains we performed
an error analysis for the unrelated, i.e. category
4 term pairs. We found that in both domains the
main source of errors is due to terms with different
meanings but similar spellings such as the follow-
ing example (1).
(1) accelerator —decelerator
For this example the cognate methods, e.g. the
Levenshtein similarity measure, returns a score of
0.81. This problem could be addressed in different
ways. First, it could be resolved by applying a very
high threshold for the cognate methods. Any cog-
nate score below that threshold could be regarded
as zero – as we did for the combined features (cf.
Section 4.4). However, setting a similarity thresh-
old higher than 0.9 – to filter out cases as in (1)
– will cause real cognates with greater variation
in the spellings to be missed. This will, in par-
ticular, affect languages with a lot of inflection,
such as Latvian. Another approach to address this
problem would be to take the contextual or dis-
tributional properties of the terms into considera-
tion. To achieve this, training data consisting of
term pairs along with contextual information is re-
quired. However, such training data does not cur-
rently exist (i.e. resources like EUROVOC do not
contain contextual information) and it would need
to be collected as a first step towards applying this
approach to the problem.
Partial Translation The assessors assigned 6 –
7% of the term pairs in the IT domain and 12 –
16% in the automotive domain to categories 2 and
3. In both categories the term pairs share transla-
tions or cognates.
Clearly, if humans such as professional transla-
tors are the end users of these terms, then it could
be helpful for them to find some translation units
within the terms. In category 2 this will be the en-
tire translation of one term in the other such as the
following examples.5
</bodyText>
<listItem confidence="0.969295">
(2) visible graphical interface — grafische be-
nutzerschnittstelle
(3) modern turbocharger systems — moderne
turbolader
</listItem>
<bodyText confidence="0.999941333333333">
In example (3) the a translation of the German
term is to be found entirely within in the English
term but the English term has the additional word
visible, a translation of which is not found in the
German term. In example (4), again the transla-
tion of the German term is entirely found in the
English term, but as in the previous example, one
of the English words – systems – in this case, has
no match within the German term. In category 3
there are only single word translation overlaps be-
tween the terms as shown in the following exam-
ples.
</bodyText>
<listItem confidence="0.93870525">
(4) national standard language —
niederl¨andischen standardsprache
(5) thermoplastic material — thermoplastische
elastomere
</listItem>
<bodyText confidence="0.999846416666667">
In example (5) standard language is translated
to standardsprache and in example (6) thermo-
plastic to thermoplastische. The other words
within the terms are not translations of each other.
Another application of the extracted term pairs
is to use them to enhance existing parallel corpora
to train SMT systems. In this case, including the
partially correct terms may introduce noise. This
is especially the case for the terms within category
3. However, the usefulness of terms in both these
scenarios requires further investigation, which we
aim to do in future work.
</bodyText>
<footnote confidence="0.889596">
5In our data it is always the case that the target term is
entirely translated within the English one and the other way
round.
</footnote>
<page confidence="0.998374">
409
</page>
<sectionHeader confidence="0.998793" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999883219512195">
In this paper we presented an approach to align
terms identified by a monolingual term extractor in
bilingual comparable corpora using a binary clas-
sifier. We trained the classifier using data from
the EUROVOC thesaurus. Each candidate term
pair was pre-processed to extract various features
which are cognate-based or dictionary-based. We
measured the performance of our classifier using
Information Retrieval (IR) metrics and a manual
evaluation. In the IR evaluation we tested the per-
formance of the classifier on a held out test set
taken from EUROVOC. We used 20 EU language
pairs with English being always the source lan-
guage. The performance of our classifier in this
evaluation reached the 100% precision level for
many language pairs. In the manual evaluation
we had our algorithm extract pairs of terms from
Wikipedia articles – articles forming comparable
corpora in the IT and automotive domains – and
asked native speakers to categorize a selection of
the term pairs into categories reflecting the level
of translation of the terms. In the manual evalu-
ation we used the English-German language pair
and showed that over 80% of the extracted term
pairs were exact translations in the IT domain and
over 60% in the automotive domain. For both do-
mains over 90% of the extracted term pairs were
either exact or partial translations.
We also performed an error analysis and high-
lighted problem cases, which we plan to address
in future work. Exploring ways to add contextual
or distributional features to our term representa-
tions is also an avenue for future work, though it
clearly significantly complicates the approach, one
of whose advantages is its simplicitiy. Further-
more, we aim to extend the existing dictionaries
and possibly our training data with terms extracted
from comparable corpora. Finally, we plan to in-
vestigate the usefulness of the terms in different
application scenarios, including computer assisted
translation and machine translation.
</bodyText>
<sectionHeader confidence="0.996512" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999875888888889">
The research reported was funded by the TaaS
project, European Union Seventh Framework Pro-
gramme, grant agreement no. 296312. The au-
thors would like to thank the manual annotators
for their helpful contributions. We would also like
to thank partners at Tilde SIA and at the University
of Zagreb for supplying the TWSC term extraction
tool, developed within the EU funded project AC-
CURAT.
</bodyText>
<sectionHeader confidence="0.990291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989818">
A. Aker, Y. Feng, and R. Gaizauskas. 2012. Auto-
matic bilingual phrase extraction from comparable
corpora. In 24th International Conference on Com-
putational Linguistics (COLING 2012), IIT Bom-
bay, Mumbai, India, 2012. Association for Compu-
tational Linguistics.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in arabic text. In Proceedings of
the ACL-02 workshop on Computational approaches
to semitic languages, pages 1–13. Association for
Computational Linguistics.
N. Aswani and R. Gaizauskas. 2010. English-hindi
transliteration using multiple similarity metrics. In
Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation (LREC
2010), Valetta, Malta.
D. Bouamor, N. Semmar, and P. Zweigenbaum. 2012.
Identifying bilingual multi-word expressions for sta-
tistical machine translation. In LREC 2012, Eigth
International Conference on Language Resources
and Evaluation, pages 674-679, Istanbul, Turkey,
2012. ELRA.
Y. Cao and H. Li. 2002. Base noun phrase translation
using web data and the em algorithm. In Proceed-
ings of the 19th international conference on Com-
putational linguistics-Volume 1, pages 1–7. Associ-
ation for Computational Linguistics.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and
C. Stein. 2001. Introduction to Algorithms. The
MIT Press, 2nd revised edition, September.
B. Daille, ´E. Gaussier, and J.M. Lang´e. 1994. Towards
automatic extraction of monolingual and bilingual
terminology. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 515–
521. Association for Computational Linguistics.
X. Fan, N. Shimizu, and H. Nakagawa. 2009. Auto-
matic extraction of bilingual terms from a chinese-
japanese parallel corpus. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 41–45. ACM.
P. Fung and K. McKeown. 1997. Finding terminol-
ogy translations from non-parallel corpora. In Pro-
ceedings of the 5th Annual Workshop on Very Large
Corpora, pages 192–202.
A. Ismail and S. Manandhar. 2010. Bilingual lexi-
con extraction from comparable corpora using in-
domain terms. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 481–489. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.974464">
410
</page>
<reference confidence="0.998974890410959">
T. Joachims. 2002. Learning to classify text using sup-
port vector machines: Methods, theory and algo-
rithms, volume 186. Kluwer Academic Publishers
Norwell, MA, USA:.
S. Karimi, F. Scholer, and A. Turpin. 2011. Ma-
chine transliteration survey. ACM Computing Sur-
veys (CSUR), 43(3):17.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4):599–612.
J. Kupiec. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st annual meeting on Association for
Computational Linguistics, pages 17–22. Associa-
tion for Computational Linguistics.
R. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In In Proceed-
ings of the tenth conference on European chapter
of the Association for Computational Linguistics-
Volume 1, pages 259266. Association for Compu-
tational Linguistics.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 664–671, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
F. J. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation. In
Proceedings of the 18th conference on Computa-
tional linguistics, pages 1086–1090, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
F. J. Och Och and H. Ney. 2003. A systematic compar-
ison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51.
T. Okita, A. Maldonado Guerra, Y. Graham, and
A. Way. 2010. Multi-word expression-sensitive
word alignment. Association for Computational
Linguistics.
M¯arcis Pinnis, Nikola Ljubeˇsi´c, Dan S¸tef˘anescu, In-
guna Skadin¸a, Marko Tadi´c, and Tatiana Gornostay.
2012. Term extraction, tagging, and mapping tools
for under-resourced languages. In Proc. of the 10th
Conference on Terminology and Knowledge Engi-
neering (TKE 2012), June, pages 20–21.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320–322. Association for Computational
Linguistics.
Helmut Schmid. 1995. Treetagger— a lan-
guage independent part-of-speech tagger. Insti-
tut f¨ur Maschinelle Sprachverarbeitung, Universit¨at
Stuttgart, page 43.
C. Snae. 2007. A comparison and analysis of
name matching algorithms. International Journal
of Applied Science. Engineering and Technology,
4(1):252–257.
R. Steinberger, B. Pouliquen, and J. Hagman. 2002.
Cross-lingual document similarity calculation using
the multilingual thesaurus eurovoc. Computational
Linguistics and Intelligent Text Processing, pages
101–121.
R. Steinberger, A. Eisele, S. Klocek, S. Pilos, and
P. Schlter. 2012. Dgt-tm: A freely available trans-
lation memory in 22 languages. In Proceedings of
LREC, pages 454–459.
R. Udupa, K. Saravanan, A. Kumaran, and J. Jagarla-
mudi. 2008. Mining named entity transliteration
equivalents from comparable corpora. In Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 1423–1424. ACM.
</reference>
<page confidence="0.998003">
411
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964560">
<title confidence="0.992804">Extracting bilingual terminologies from comparable corpora</title>
<author confidence="0.989203">Ahmet Aker</author>
<author confidence="0.989203">Monica Paramita</author>
<author confidence="0.989203">Robert</author>
<affiliation confidence="0.999655">University of Sheffield</affiliation>
<email confidence="0.997584">ahmet.aker,m.paramita,r.gaizauskas@sheffield.ac.uk</email>
<abstract confidence="0.998929888888889">In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extraction as a classification problem. For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure evaluations for 20 European language pairs. The performance of our classifier reaches the 100% precision level for many language pairs. We also perform manual evaluation on bilingual terms extracted from English-German term-tagged comparable corpora. The results of this manual evaluation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aker</author>
<author>Y Feng</author>
<author>R Gaizauskas</author>
</authors>
<title>Automatic bilingual phrase extraction from comparable corpora.</title>
<date>2012</date>
<booktitle>In 24th International Conference on Computational Linguistics (COLING 2012), IIT</booktitle>
<location>Bombay, Mumbai, India,</location>
<contexts>
<context position="11849" citStr="Aker et al. (2012)" startWordPosition="1900" endWordPosition="1903">res. Note for non-compound terms if we compare two words for equality we do not perform string match but rather use the Levenshtein Distance (see Section 4.2) between the two words and treat them as equal if the Levenshtein Distance returns &gt;= 0.95. This is performed to capture words with morphological differences. We set 0.95 experimentally. 4.2 Cognate based features Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology. Because of this we also use cognate based methods to perform the mapping between source and target words or vice versa. Aker et al. (2012) have applied (1) Longest Common Subsequence Ratio, (2) Longest Common Substring Ratio, (3) Dice Similarity, (4) Needleman-Wunsch Distance and (5) Levenshtein Distance in order to extract parallel phrases from comparable corpora. We adopt these measures within our classifier. Each of them returns a score between 0 and 1. • Longest Common Subsequence Ratio (LCSR): The longest common subsequence (LCS) measure measures the longest common non-consecutive sequence of characters between two strings. For instance, the words “dollars” and “dolari” share a sequence of 5 non-consecutive characters in th</context>
</contexts>
<marker>Aker, Feng, Gaizauskas, 2012</marker>
<rawString>A. Aker, Y. Feng, and R. Gaizauskas. 2012. Automatic bilingual phrase extraction from comparable corpora. In 24th International Conference on Computational Linguistics (COLING 2012), IIT Bombay, Mumbai, India, 2012. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine transliteration of names in arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages,</booktitle>
<pages>1--13</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Y. Al-Onaizan and K. Knight. 2002. Machine transliteration of names in arabic text. In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, pages 1–13. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Aswani</author>
<author>R Gaizauskas</author>
</authors>
<title>English-hindi transliteration using multiple similarity metrics.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Valetta,</location>
<contexts>
<context position="6576" citStr="Aswani and Gaizauskas (2010)" startWordPosition="1016" endWordPosition="1019">curs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in AlOnaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being viewed as a classification task, assuming a symmetric approach, since the different information sources mentioned above can be treated as features and each source-target language potential term pairing can be treated as an instance to be fed to a binary classifier which decides whether to align them or no</context>
</contexts>
<marker>Aswani, Gaizauskas, 2010</marker>
<rawString>N. Aswani and R. Gaizauskas. 2010. English-hindi transliteration using multiple similarity metrics. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC 2010), Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bouamor</author>
<author>N Semmar</author>
<author>P Zweigenbaum</author>
</authors>
<title>Identifying bilingual multi-word expressions for statistical machine translation.</title>
<date>2012</date>
<booktitle>In LREC 2012, Eigth International Conference on Language Resources and Evaluation,</booktitle>
<pages>674--679</pages>
<publisher>ELRA.</publisher>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="5499" citStr="Bouamor et al. (2012)" startWordPosition="842" endWordPosition="845">ual terms. 3 Related Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate</context>
</contexts>
<marker>Bouamor, Semmar, Zweigenbaum, 2012</marker>
<rawString>D. Bouamor, N. Semmar, and P. Zweigenbaum. 2012. Identifying bilingual multi-word expressions for statistical machine translation. In LREC 2012, Eigth International Conference on Language Resources and Evaluation, pages 674-679, Istanbul, Turkey, 2012. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Cao</author>
<author>H Li</author>
</authors>
<title>Base noun phrase translation using web data and the em algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6312" citStr="Cao and Li (2002)" startWordPosition="975" endWordPosition="978">mparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in AlOnaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being viewed as a classification task, assuming </context>
</contexts>
<marker>Cao, Li, 2002</marker>
<rawString>Y. Cao and H. Li. 2002. Base noun phrase translation using web data and the em algorithm. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
<author>C Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>The MIT Press,</publisher>
<note>2nd revised edition,</note>
<contexts>
<context position="12522" citStr="Cormen et al., 2001" startWordPosition="2001" endWordPosition="2004">2) Longest Common Substring Ratio, (3) Dice Similarity, (4) Needleman-Wunsch Distance and (5) Levenshtein Distance in order to extract parallel phrases from comparable corpora. We adopt these measures within our classifier. Each of them returns a score between 0 and 1. • Longest Common Subsequence Ratio (LCSR): The longest common subsequence (LCS) measure measures the longest common non-consecutive sequence of characters between two strings. For instance, the words “dollars” and “dolari” share a sequence of 5 non-consecutive characters in the same ordering. We make use of dynamic programming (Cormen et al., 2001) to implement LCS, so that its computation is efficient and can be applied to a large number of possible term pairs quickly. We normalize relative to the length of the longest term: len[LCS(X, Y )] LCSR(X, Y ) = max[len(X), len(Y )] where LCS is the longest common subsequence between two strings and characters in this subsequence need not be contiguous. The shorthand len stands for length. • Longest Common Substring Ratio (LCSTR): The longest common substring (LCST) measure is similar to the LCS measure, but measures the longest common 404 consecutive string of characters that two strings have</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. 2001. Introduction to Algorithms. The MIT Press, 2nd revised edition, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>´E Gaussier</author>
<author>J M Lang´e</author>
</authors>
<title>Towards automatic extraction of monolingual and bilingual terminology.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 1,</booktitle>
<pages>515--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Daille, Gaussier, Lang´e, 1994</marker>
<rawString>B. Daille, ´E. Gaussier, and J.M. Lang´e. 1994. Towards automatic extraction of monolingual and bilingual terminology. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 515– 521. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Fan</author>
<author>N Shimizu</author>
<author>H Nakagawa</author>
</authors>
<title>Automatic extraction of bilingual terms from a chinesejapanese parallel corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Universal Communication Symposium,</booktitle>
<pages>41--45</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5183" citStr="Fan et al. (2009)" startWordPosition="793" endWordPosition="796">rms from comparable corpora. This second test simulates the situation of using the term alignment system in a real world scenario. For this evaluation we collected English-German comparable corpora from Wikipedia, performed monolingual term tagging and ran our tool over the term tagged corpora to extract bilingual terms. 3 Related Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, ty</context>
</contexts>
<marker>Fan, Shimizu, Nakagawa, 2009</marker>
<rawString>X. Fan, N. Shimizu, and H. Nakagawa. 2009. Automatic extraction of bilingual terms from a chinesejapanese parallel corpus. In Proceedings of the 3rd International Universal Communication Symposium, pages 41–45. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K McKeown</author>
</authors>
<title>Finding terminology translations from non-parallel corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Annual Workshop on Very Large Corpora,</booktitle>
<pages>192--202</pages>
<contexts>
<context position="6271" citStr="Fung and McKeown (1997)" startWordPosition="967" endWordPosition="970">ilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in AlOnaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being v</context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>P. Fung and K. McKeown. 1997. Finding terminology translations from non-parallel corpora. In Proceedings of the 5th Annual Workshop on Very Large Corpora, pages 192–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ismail</author>
<author>S Manandhar</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using indomain terms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>481--489</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6344" citStr="Ismail and Manandhar (2010)" startWordPosition="980" endWordPosition="983">ssentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in AlOnaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being viewed as a classification task, assuming a symmetric approach, since the </context>
</contexts>
<marker>Ismail, Manandhar, 2010</marker>
<rawString>A. Ismail and S. Manandhar. 2010. Bilingual lexicon extraction from comparable corpora using indomain terms. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 481–489. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to classify text using support vector machines: Methods, theory and algorithms, volume 186.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers</publisher>
<location>Norwell, MA, USA:.</location>
<contexts>
<context position="7991" citStr="Joachims, 2002" startWordPosition="1257" endWordPosition="1258">didate terms are proposed by composing all translations (given by a bilingual dictionary) of N1 into the target language with all translations of N2. We remove both these restrictions. By considering all terms proposed by monolingual term extractors we consider terms that are syntactically much richer than nounnoun pairs. In addition, the term pairs we align are not constrained by an assumption that their component words must be translations of each other as found in a particular dictionary resource. 4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form (s, ti, pi), where s is a source word, ti is the i-th translation </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning to classify text using support vector machines: Methods, theory and algorithms, volume 186. Kluwer Academic Publishers Norwell, MA, USA:.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Karimi</author>
<author>F Scholer</author>
<author>A Turpin</author>
</authors>
<title>Machine transliteration survey.</title>
<date>2011</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="15043" citStr="Karimi et al., 2011" startWordPosition="2428" endWordPosition="2431">rectly applicable to the English-Bulgarian and EnglishGreek language pairs, as both the Bulgarian and Greek alphabets, which are Cyrillic-based, differ from the English Latin-based alphabet. However, the use of distinct alphabets is not the only problem when comparing source and target terms. Although most EU languages use the Latin alphabet, the occurrence of special characters and diacritics, as well spelling and phonetic variations, are further challenges which are faced by term or entity mapping methods, especially in determining the variants of the same mention of the entity (Snae, 2007; Karimi et al., 2011).1 We address this problem by mapping a source term to the target language writing system or vice versa. For mapping we use simple character mappings between the writing systems, such as α → a, φ → ph, etc., from Greek to English. The rules allow one character on the lefthand side (source language) to map onto one or more characters on the righthand side (target language). We created our rules manually based on sound similarity between source and target language characters. We created mapping rules for 20 EU language pairs using primarily Wikipedia as a resource for describing phonetic mapping</context>
</contexts>
<marker>Karimi, Scholer, Turpin, 2011</marker>
<rawString>S. Karimi, F. Scholer, and A. Turpin. 2011. Machine transliteration survey. ACM Computing Surveys (CSUR), 43(3):17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="6521" citStr="Knight and Graehl (1998)" startWordPosition="1007" endWordPosition="1010">xtent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in AlOnaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturally lends itself to being viewed as a classification task, assuming a symmetric approach, since the different information sources mentioned above can be treated as features and each source-target language potential term pairing can be treated as an instance to be fed to a bina</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics,</booktitle>
<pages>17--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5038" citStr="Kupiec (1993)" startWordPosition="772" endWordPosition="773">recision and F-measure. We run this evaluation on all 20 language pairs. Secondly, we test the system’s performance on obtaining bilingual terms from comparable corpora. This second test simulates the situation of using the term alignment system in a real world scenario. For this evaluation we collected English-German comparable corpora from Wikipedia, performed monolingual term tagging and ran our tool over the term tagged corpora to extract bilingual terms. 3 Related Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 17–22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Learning translations of namedentity phrases from parallel corpora. In</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 1,</booktitle>
<pages>259266</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1596" citStr="Moore (2003)" startWordPosition="226" endWordPosition="227">on Bilingual terminologies are important for various applications of human language technologies, including cross-language information search and retrieval, statistical machine translation (SMT) in narrow domains and computer-aided assistance to human translators. Automatic construction of bilingual terminology mappings has been investigated in many earlier studies and various methods have been applied to this task. These methods may be distinguished by whether they work on parallel or comparable corpora, by whether they assume monolingual term recognition in source and target languages (what Moore (2003) calls symmetrical approaches) or only in the source (asymmetric approaches), and by the extent to which they rely on linguistic knowledge as opposed to simply statistical techniques. We focus on techniques for bilingual term extraction from comparable corpora – collections of source-target language document pairs that are not direct translations but are topically related. We choose to focus on comparable corpora because for many less widely spoken languages and for technical domains where new terminology is constantly being introduced, parallel corpora are simply not available. Techniques tha</context>
</contexts>
<marker>Moore, 2003</marker>
<rawString>R. Moore. 2003. Learning translations of namedentity phrases from parallel corpora. In In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 1, pages 259266. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Morin</author>
<author>B Daille</author>
<author>K Takeuchi</author>
<author>K Kageura</author>
</authors>
<title>Bilingual terminology mining - using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>664--671</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>E. Morin, B. Daille, K. Takeuchi, and K. Kageura. 2007. Bilingual terminology mining - using brain, not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 664–671, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>1086--1090</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8370" citStr="Och and Ney, 2000" startWordPosition="1316" endWordPosition="1319">ned by an assumption that their component words must be translations of each other as found in a particular dictionary resource. 4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form (s, ti, pi), where s is a source word, ti is the i-th translation of s in the dictionary and pi is the probability that s is translated by ti, the pi’s summing to 1 for each s in the dictionary. From the dictionaries we removed all entries with pi &lt; 0.05. In addition we also removed 403 every entry from the dictionary where the source word was less than four characters and the target word more than five characters in length and vice versa. T</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th conference on Computational linguistics, pages 1086–1090, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8390" citStr="Och and Ney, 2003" startWordPosition="1320" endWordPosition="1323">n that their component words must be translations of each other as found in a particular dictionary resource. 4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form (s, ti, pi), where s is a source word, ti is the i-th translation of s in the dictionary and pi is the probability that s is translated by ti, the pi’s summing to 1 for each s in the dictionary. From the dictionaries we removed all entries with pi &lt; 0.05. In addition we also removed 403 every entry from the dictionary where the source word was less than four characters and the target word more than five characters in length and vice versa. This step is performe</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Okita</author>
<author>A Maldonado Guerra</author>
<author>Y Graham</author>
<author>A Way</author>
</authors>
<title>Multi-word expression-sensitive word alignment. Association for Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="5207" citStr="Okita et al. (2010)" startWordPosition="798" endWordPosition="801">rpora. This second test simulates the situation of using the term alignment system in a real world scenario. For this evaluation we collected English-German comparable corpora from Wikipedia, performed monolingual term tagging and ran our tool over the term tagged corpora to extract bilingual terms. 3 Related Work Previous studies have investigated the extraction of bilingual terms from parallel and comparable corpora. For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. Daille et al. (1994), Fan et al. (2009) and Okita et al. (2010) also apply statistical methods to extract terms/phrases from parallel corpora. In addition to statistical methods Daille et al. use word translation information between two words within the extracted terms as a further indicator of the correct alignment. More recently, Bouamor et al. (2012) use vector space models to align terms. The entries in the vectors are co-occurrence statistics between the terms computed over the entire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using </context>
</contexts>
<marker>Okita, Guerra, Graham, Way, 2010</marker>
<rawString>T. Okita, A. Maldonado Guerra, Y. Graham, and A. Way. 2010. Multi-word expression-sensitive word alignment. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M¯arcis Pinnis</author>
<author>Nikola Ljubeˇsi´c</author>
<author>Dan S¸tef˘anescu</author>
<author>Inguna Skadin¸a</author>
<author>Marko Tadi´c</author>
<author>Tatiana Gornostay</author>
</authors>
<title>Term extraction, tagging, and mapping tools for under-resourced languages.</title>
<date>2012</date>
<booktitle>In Proc. of the 10th Conference on Terminology and Knowledge Engineering (TKE</booktitle>
<pages>20--21</pages>
<marker>Pinnis, Ljubeˇsi´c, S¸tef˘anescu, Skadin¸a, Tadi´c, Gornostay, 2012</marker>
<rawString>M¯arcis Pinnis, Nikola Ljubeˇsi´c, Dan S¸tef˘anescu, Inguna Skadin¸a, Marko Tadi´c, and Tatiana Gornostay. 2012. Term extraction, tagging, and mapping tools for under-resourced languages. In Proc. of the 10th Conference on Terminology and Knowledge Engineering (TKE 2012), June, pages 20–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Identifying word translations in nonparallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>320--322</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6246" citStr="Rapp (1995)" startWordPosition="965" endWordPosition="966">ire corpus. Bilingual term alignment methods that work on comparable corpora use essentially three sorts of information: (1) cognate information, typically estimated using some sort of transliteration similarity measure (2) context congruence, a measure of the extent to which the words that the source term co-occurs with have the same sort of distribution and co-occur with words with the same sort distribution as do those words that co-occur with the candidate term and (3) translation of component words in the term and/or in context words, where some limited dictionary exists. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al. (2007), Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. Transliteration and cognate-based information is exploited in AlOnaizan and Knight (2002), Knight and Graehl (1998), Udupa et. al. (2008) and Aswani and Gaizauskas (2010). Very few approaches have treated term alignment as a classification problem suitable for machine learning (ML) techniques. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. However, it naturall</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>R. Rapp. 1995. Identifying word translations in nonparallel texts. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 320–322. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Treetagger— a language independent part-of-speech tagger. Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart,</title>
<date>1995</date>
<pages>43</pages>
<contexts>
<context position="20187" citStr="Schmid, 1995" startWordPosition="3288" endWordPosition="3289">reqdij is 1 if di is linked to dj, or 0 otherwise, and depthdj is the number of clicks between dj and the seed document. After all documents in the queue were assigned a score, we gathered the top 1000 documents and used inter-language link information to extract the corresponding article in the target language. We pre-processed each Wikipedia article by performing monolingual term tagging using TWSC (Pinnis et al., 2012). TWSC is a term extraction tool which identifies terms ranging from one to four tokens in length. First, it POS-tags each document. For German POS-tagging we use TreeTagger (Schmid, 1995). Next, it uses term grammar rules, in the form of sequences of POS tags or non-stop words, to identify candidate terms. Finally, it filters the candidate terms using various statistical measures, such as pointwise mutual information and TF*IDF. 5.2 Performance test of the classifier To test the classifier’s performance we evaluated it against a list of positive and negative examples of bilingual term pairs using the measures of precision, recall and F-measure. We used 21 EU official languages, including English, and paired each non-English language with English, leading to 20 language pairs.4</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Treetagger— a language independent part-of-speech tagger. Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart, page 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Snae</author>
</authors>
<title>A comparison and analysis of name matching algorithms.</title>
<date>2007</date>
<journal>International Journal of Applied Science. Engineering and Technology,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="15021" citStr="Snae, 2007" startWordPosition="2426" endWordPosition="2427">s are not directly applicable to the English-Bulgarian and EnglishGreek language pairs, as both the Bulgarian and Greek alphabets, which are Cyrillic-based, differ from the English Latin-based alphabet. However, the use of distinct alphabets is not the only problem when comparing source and target terms. Although most EU languages use the Latin alphabet, the occurrence of special characters and diacritics, as well spelling and phonetic variations, are further challenges which are faced by term or entity mapping methods, especially in determining the variants of the same mention of the entity (Snae, 2007; Karimi et al., 2011).1 We address this problem by mapping a source term to the target language writing system or vice versa. For mapping we use simple character mappings between the writing systems, such as α → a, φ → ph, etc., from Greek to English. The rules allow one character on the lefthand side (source language) to map onto one or more characters on the righthand side (target language). We created our rules manually based on sound similarity between source and target language characters. We created mapping rules for 20 EU language pairs using primarily Wikipedia as a resource for descr</context>
</contexts>
<marker>Snae, 2007</marker>
<rawString>C. Snae. 2007. A comparison and analysis of name matching algorithms. International Journal of Applied Science. Engineering and Technology, 4(1):252–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>J Hagman</author>
</authors>
<title>Cross-lingual document similarity calculation using the multilingual thesaurus eurovoc.</title>
<date>2002</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>101--121</pages>
<contexts>
<context position="3638" citStr="Steinberger et al., 2002" startWordPosition="558" endWordPosition="562">terms must be contiguous sequences of words in both source and target languages. Our method works by first pairing each term extracted from a source language document 5 with each term extracted from a target language document T aligned with 5 in the comparable corpus. We then treat term alignment as a binary classification task, i.e. we extract features for each source-target language potential term pair and decide whether to classify the pair as a term equivalent or not. For classification purposes we use an SVM binary classifier. The training data for the classifier is derived from EUROVOC (Steinberger et al., 2002), a term thesaurus covering the activities of the EU and the European Parliament. We have run our approach on the 21 official EU languages covered by EUROVOC, constructing 20 language pairs with English as the source 402 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402–411, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics language. Considering all these languages allows us to directly compare our method’s performance on resource-rich (e.g. German, French, Spanish) and under-resourced languages (e.g. Latvian, Bu</context>
<context position="18495" citStr="Steinberger et al., 2002" startWordPosition="3008" endWordPosition="3011">slation as described in Section 4.1, 5 cognate related features as outlined in Section 4.2, 10 cognate related features derived from character mappings over terms as described in Section 4.3 and 10 combined features. 5 Experiments 5.1 Data Sources In our experiments we use two different data resources: EUROVOC terms and comparable corpora collected from Wikipedia. 5.1.1 EUROVOC terms EUROVOC is a term thesaurus covering the activities of the EU and the European Parliament in particular. It contains 6797 term entries in 24 different languages including 22 EU languages and Croatian and Serbian (Steinberger et al., 2002). 5.1.2 Comparable Corpora We also built comparable corpora in the information technology (IT) and automotive domains by gathering documents from Wikipedia for the English-German language pair. First, we manually chose one seed document in English as a starting point for crawling in each domain3. We then identified all articles to which the seed document is linked and added them to the crawling queue. This process is performed recursively for each document in the queue. Since our aim is to build a comparable corpus, we only added English 3http://en.wikipedia.org/wiki/Information technology for</context>
</contexts>
<marker>Steinberger, Pouliquen, Hagman, 2002</marker>
<rawString>R. Steinberger, B. Pouliquen, and J. Hagman. 2002. Cross-lingual document similarity calculation using the multilingual thesaurus eurovoc. Computational Linguistics and Intelligent Text Processing, pages 101–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>A Eisele</author>
<author>S Klocek</author>
<author>S Pilos</author>
<author>P Schlter</author>
</authors>
<title>Dgt-tm: A freely available translation memory in 22 languages.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>454--459</pages>
<contexts>
<context position="8443" citStr="Steinberger et al., 2012" startWordPosition="1329" endWordPosition="1332">ons of each other as found in a particular dictionary resource. 4 Feature extraction To align or map source and target terms we use an SVM binary classifier (Joachims, 2002) with a linear kernel and the trade-off between training error and margin parameter c = 10. Within the classifier we use language dependent and independent features described in the following sections. 4.1 Dictionary based features The dictionary based features are language dependent and are computed using bilingual dictionaries which are created with GIZA++ (Och and Ney, 2000; Och and Ney, 2003). The DGT-TM parallel data (Steinberger et al., 2012) was input to GIZA++ to obtain the dictionaries. Dictionary entries have the form (s, ti, pi), where s is a source word, ti is the i-th translation of s in the dictionary and pi is the probability that s is translated by ti, the pi’s summing to 1 for each s in the dictionary. From the dictionaries we removed all entries with pi &lt; 0.05. In addition we also removed 403 every entry from the dictionary where the source word was less than four characters and the target word more than five characters in length and vice versa. This step is performed to try to eliminate translation pairs where a stop </context>
</contexts>
<marker>Steinberger, Eisele, Klocek, Pilos, Schlter, 2012</marker>
<rawString>R. Steinberger, A. Eisele, S. Klocek, S. Pilos, and P. Schlter. 2012. Dgt-tm: A freely available translation memory in 22 languages. In Proceedings of LREC, pages 454–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>K Saravanan</author>
<author>A Kumaran</author>
<author>J Jagarlamudi</author>
</authors>
<title>Mining named entity transliteration equivalents from comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>1423--1424</pages>
<publisher>ACM.</publisher>
<marker>Udupa, Saravanan, Kumaran, Jagarlamudi, 2008</marker>
<rawString>R. Udupa, K. Saravanan, A. Kumaran, and J. Jagarlamudi. 2008. Mining named entity transliteration equivalents from comparable corpora. In Proceeding of the 17th ACM conference on Information and knowledge management, pages 1423–1424. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>