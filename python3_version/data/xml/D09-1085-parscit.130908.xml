<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.996234">
Unbounded Dependency Recovery for Parser Evaluation
</title>
<author confidence="0.983743">
Mark Steedman
</author>
<affiliation confidence="0.9908945">
University of Edinburgh
School of Informatics
</affiliation>
<email confidence="0.934777">
steedman@inf.ed.ac.uk
</email>
<author confidence="0.991521">
Laura Rimell and Stephen Clark
</author>
<affiliation confidence="0.9692985">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.9726455">
laura.rimell@cl.cam.ac.uk
stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993757" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974833333333">
This paper introduces a new parser eval-
uation corpus containing around 700 sen-
tences annotated with unbounded depen-
dencies, from seven different grammatical
constructions. We run a series of off-the-
shelf parsers on the corpus to evaluate how
well state-of-the-art parsing technology is
able to recover such dependencies. The
overall results range from 25% accuracy
to 59%. These low scores call into ques-
tion the validity of using Parseval scores
as a general measure of parsing capability.
We discuss the importance of parsers be-
ing able to recover unbounded dependen-
cies, given their relatively low frequency
in corpora. We also analyse the various er-
rors made on these constructions by one of
the more successful parsers.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9256919">
Statistical parsers are now obtaining Parseval
scores of over 90% on the WSJ section of the Penn
Treebank (Bod, 2003; Petrov and Klein, 2007;
Huang, 2008; Carreras et al., 2008). McClosky et
al. (2006) report an F-score of 92.1% using self-
training applied to the reranker of Charniak and
Johnson (2005). Such scores, in isolation, may
suggest that statistical parsing is close to becom-
ing a solved problem, and that further incremental
improvements will lead to parsers becoming as ac-
curate as POS taggers.
A single score in isolation can be misleading,
however, for a number of reasons. First, the single
score is an aggregate over a highly skewed distri-
bution of all constituent types; evaluations which
look at individual constituent or dependency types
show that the accuracies on some, semantically
important, constructions, such as coordination and
PP-attachment, are much lower (Collins, 1999).
Second, it is well known that the accuracy of
parsers trained on the Penn Treebank degrades
when they are applied to different genres and do-
mains (Gildea, 2001). Finally, some researchers
have argued that the Parseval metrics (Black et al.,
1991) are too forgiving with respect to certain er-
rors and that an evaluation based on syntactic de-
pendencies, for which scores are typically lower,
is a better test of parser performance (Lin, 1995;
Carroll et al., 1998).
In this paper we focus on the first issue, that the
performance of parsers on some constructions is
much lower than the overall score. The construc-
tions that we focus on are various unbounded de-
pendency constructions. These are interesting for
parser evaluation for the following reasons: one,
they provide a strong test of the parser’s knowl-
edge of the grammar of the language, since many
instances of unbounded dependencies are diffi-
cult to recover using shallow techniques in which
the grammar is only superficially represented; and
two, recovering these dependencies is necessary
to completely represent the underlying predicate-
argument structure of a sentence, useful for appli-
cations such as Question Answering and Informa-
tion Extraction.
To give an example of the sorts of constructions
we are considering, and the (in)ability of parsers
to recover the corresponding unbounded depen-
dencies, none of the parsers that we have tested
were able to recover the dependencies shown in
bold from the following sentences:
We have also developed techniques for recognizing and
locating underground nuclear tests through the waves in the
ground which they generate.
By Monday, they hope to have a sheaf of documents both
sides can trust.
By means of charts showing wave-travel times and depths
in the ocean at various locations , it is possible to estimate
the rate of approach and probable time of arrival at Hawaii
of a tsunami getting under way at any spot in the Pacific.
</bodyText>
<page confidence="0.978637">
813
</page>
<note confidence="0.9966165">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813–821,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999819125">
The contributions of this paper are as follows.
First, we present the first set of results for the
recovery of a variety of unbounded dependen-
cies, for a range of existing parsers. Second, we
describe the creation of a publicly available un-
bounded dependency test suite, and give statistics
summarising properties of these dependencies in
naturally occurring text. Third, we demonstrate
that performing the evaluation is surprisingly dif-
ficult, because of different conventions across the
parsers as to how the underlying grammar is rep-
resented. Fourth, we show that current parsing
technology is very poor at representing some im-
portant elements of the argument structure of sen-
tences, and argue for a more focused construction-
based parser evaluation as a complement to exist-
ing grammatical relation-based evaluations. We
also perform an error-analysis for one of the more
successful parsers.
There has been some prior work on evaluating
parsers on long-range dependencies, but no work
we are aware of that has the scope and focus of
this paper. Clark et al. (2004) evaluated a CCG
parser on a small corpus of object extraction cases.
Johnson (2002) began the body of work on insert-
ing traces into the output of Penn Treebank (PTB)
parsers, followed by Levy and Manning (2004),
among others. This PTB work focused heavily
on the representation in the Treebank, evaluat-
ing against patterns in the trace annotation. In
this paper we have tried to be more “formalism-
independent” and construction focused.
</bodyText>
<sectionHeader confidence="0.924341" genericHeader="method">
2 Unbounded Dependency Corpus
</sectionHeader>
<subsectionHeader confidence="0.988151">
2.1 The constructions
</subsectionHeader>
<bodyText confidence="0.994850296296296">
An unbounded dependency construction contains
a word or phrase which appears to have been
moved, while being interpreted in the position
of the resulting “gap”. An unlimited number
of clause boundaries may intervene between the
moved element and the gap (hence “unbounded”).
The seven constructions in our corpus were cho-
sen for being relatively frequent in text, compared
to other unbounded dependency types, and rela-
tively easy to identify. An example of each con-
struction, along with its associated dependencies,
is shown in Table 1. Here we give a brief descrip-
tion of each construction.
Object extraction from a relative clause is
characterised by a relative pronoun (a wh-word or
that) introducing a clause from which an argument
in object position has apparently been extracted:
the paper which I wrote. Our corpus includes
cases where the extracted word is (semantically)
the object of a preposition in the verb phrase: the
agency that I applied to.
Object extraction from a reduced relative
clause is essentially the same, except that there is
no overt relative pronoun: the paper I wrote; the
agency I applied to. We did not include participial
reduced relatives such as the paper written by the
professor.
</bodyText>
<subsectionHeader confidence="0.612883">
Subject extraction from a relative clause is
</subsectionHeader>
<bodyText confidence="0.986758358974359">
characterised by the apparent extraction of an ar-
gument from subject position: the instrument that
measures depth. A relative pronoun is obligatory
in this construction. Our corpus includes passive
subjects: the instrument which was used by the
professor.
Free relatives contain relative pronouns with-
out antecedents: I heard what she said, where
what does not refer to any other noun in the sen-
tence. Free relatives can usually be paraphrased by
noun phrases such as the thing she said (a standard
diagnostic for distinguishing them from embedded
interrogatives like I wonder what she said). The
majority of sentences in our corpus are object free
relatives, but we also included some adverbial free
relatives: She told us how to do it.
Object wh-questions are questions in which the
wh-word is the semantic object of the verb: What
did you eat?. Objects of prepositions are included:
What city does she live in?. Also included are a
few cases where the wh-word is arguably adver-
bial, but is selected for by the verb: Where is the
park located?.
Right node raising (RNR) is characterised by
coordinated phrases from which a shared element
apparently moves to the right: Mary saw and Su-
san bought the book. This construction is unique
within our corpus in that the “raised” element can
have a wide variety of grammatical functions. Ex-
amples include: noun phrase object of verb, noun
phrase object of preposition (material about or
messages from the communicator), a combination
of the two (applied for and won approval), prepo-
sitional phrase modifier (president and chief exec-
utive of the company), infinitival modifier (the will
and the capacity to prevent the event), and modi-
fied noun (a good or a bad decision).
Subject extraction from an embedded clause
is characterised by a semantic subject which is ap-
</bodyText>
<page confidence="0.996252">
814
</page>
<subsectionHeader confidence="0.724704">
Object extraction from a relative clause
</subsectionHeader>
<bodyText confidence="0.995063">
Each must match Wisman’s “pie” with the fragment that he carries with him.
dobj(carries, fragment)
</bodyText>
<subsectionHeader confidence="0.752729">
Object extraction from a reduced relative clause
</subsectionHeader>
<bodyText confidence="0.880435">
Put another way, the decline in the yield suggests stocks have gotten pretty rich in price relative to the
dividends they pay, some market analysts say.
dobj(pay, dividends)
</bodyText>
<subsectionHeader confidence="0.987932">
Subject extraction from a relative clause
</subsectionHeader>
<bodyText confidence="0.9471585">
It consists of a series of pipes and a pressure-measuring chamber which record the rise and fall of the
water surface.
nsubj(record, series)
nsubj(record, chamber)
</bodyText>
<subsectionHeader confidence="0.643708">
Free relative
</subsectionHeader>
<bodyText confidence="0.9829515">
He tried to ignore what his own common sense told him, but it wasn’t possible; her motives were too
blatant.
</bodyText>
<equation confidence="0.353248">
dobj(told, what)
Object wh-question
</equation>
<bodyText confidence="0.9834045">
What city does the Tour de France end in?
pobj(in, city)
</bodyText>
<subsectionHeader confidence="0.542313">
Right node raising
</subsectionHeader>
<bodyText confidence="0.96003775">
For the third year in a row, consumers voted Bill Cosby first and James Garner second in persuasiveness
as spokesmen in TV commercials, according to Video Storyboard Tests, New York.
prep(first, in)
prep(second, in)
</bodyText>
<subsectionHeader confidence="0.943558">
Subject extraction from an embedded clause
</subsectionHeader>
<bodyText confidence="0.937015">
In assigning to God the responsibility which he learned could not rest with his doctors, Eisenhower
gave evidence of that weakening of the moral intuition which was to characterize his administration
in the years to follow.
nsubj(rest, responsibility)
</bodyText>
<tableCaption confidence="0.991228">
Table 1: Examples of the seven constructions in the unbounded dependency corpus.
</tableCaption>
<bodyText confidence="0.999894555555556">
parently extracted across two clause boundaries,
as shown in the following bracketing (where *
marks the origin of the extracted element): the
responsibility which [the government said [* lay
with the voters]]. Our corpus includes sentences
where the embedded clause is a so-called small
clause, i.e. one with a null copula verb: the plan
that she considered foolish, where plan is the se-
mantic subject of foolish.
</bodyText>
<subsectionHeader confidence="0.998975">
2.2 The data
</subsectionHeader>
<bodyText confidence="0.9999518">
The corpus consists of approximately 100 sen-
tences for each of the seven constructions; 80 of
these were reserved for each construction for test-
ing, giving a test set of 560 sentences in total, and
the remainder were used for initial experimenta-
tion (for example to ensure that default settings for
the various parsers were appropriate for this data).
We did not annotate the full sentences, since we
are only interested in the unbounded dependencies
and full annotation of such a corpus would be ex-
tremely time-consuming.
With the exception of the question construc-
tion, all sentences were taken from the PTB, with
roughly half from the WSJ sections (excluding
2-21 which provided the training data for many
</bodyText>
<page confidence="0.991456">
815
</page>
<bodyText confidence="0.999993">
of the parsers in our set) and half from Brown
(roughly balanced across the different sections).
The questions were taken from the question data
in Rimell and Clark (2008), which was obtained
from various years of the TREC QA track. We
chose to use the PTB as the main source because
the use of traces in the PTB annotation provides a
starting point for the identification of unbounded
dependencies.
Sentences were selected for the corpus by a
combination of automatic and manual processes.
A regular expression applied to PTB trees, search-
ing for appropriate traces for a particular con-
struction, was first used to extract a set of can-
didate sentences. All candidates were manually
reviewed and, if selected, annotated with one or
more grammatical relations representing the rel-
evant unbounded dependencies in the sentence.
Some of the annotation in the treebank makes
identification of some constructions straightfor-
ward; for example right node raising is explicitly
represented as RNR. Indeed it may have been pos-
sible to fully automate this process with use of
the tgrep search tool. However, in order to ob-
tain reliable statistics regarding frequency of oc-
currence, and to ensure a high-quality resource,
we used fairly broad regular expressions to iden-
tify the original set followed by manual review.
We chose to represent the dependencies as
grammatical relations (GRs) since this format
seemed best suited to represent the kind of seman-
tic relationship we are interested in. GRs are head-
based dependencies that have been suggested as a
more appropriate representation for general parser
evaluation than phrase-structure trees (Carroll et
al., 1998). Table 1 gives examples of how GRs
are used to represent the relevant dependencies.
The particular GR scheme we used was based on
the Stanford scheme (de Marneffe et al., 2006);
however, the specific GR scheme is not too crucial
since the whole sentence is not being represented
in the corpus, only the unbounded dependencies.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999486142857143">
The five parsers described in Section 3.2 were used
to parse the test sentences in the corpus, and the
percentage of dependencies in the test set recov-
ered by each parser for each construction was cal-
culated. The details of how the parsers were run
and how the parser output was matched against
the gold standard are given in Section 3.3. This
</bodyText>
<table confidence="0.999457142857143">
Construction WSJ Brown Overall
Obj rel clause 2.3 1.1 1.4
Obj reduced rel 2.7 2.8 2.8
Sbj rel clause 10.1 5.7 7.4
Free rel 2.6 0.9 1.3
RNR 2.2 0.9 1.2
Sbj embedded 2.0 0.3 0.4
</table>
<tableCaption confidence="0.784153">
Table 2: Frequency of constructions in the PTB
(percentage of sentences).
</tableCaption>
<bodyText confidence="0.999940461538462">
is essentially a recall evaluation, and so is open
to abuse; for example, a program which returns all
the possible word pairs in a sentence, together with
all possible labels, would score 100%. However,
this is easily guarded against: we simply assume
that each parser is being run in a “standard” mode,
and that each parser has already been evaluated on
a full corpus of GRs in order to measure precision
and recall across all dependency types. (Calculat-
ing precision for the unbounded dependency eval-
uation would be difficult since that would require
us to know how many incorrect unbounded depen-
dencies were returned by each parser.)
</bodyText>
<subsectionHeader confidence="0.999489">
3.1 Statistics relating to the constructions
</subsectionHeader>
<bodyText confidence="0.99997512">
Table 2 shows the percentage of sentences in the
PTB, from those sections that were examined,
which contain an example of each type of un-
bounded dependency. Perhaps not surprisingly,
root subject extractions from relative clauses are
by far the most common, with the remaining con-
structions occurring in roughly between 1 and 2%
of sentences. Note that, although examples of
each individual construction are relatively rare, the
combined total is over 10% (assuming that each
construction occurs independently). Section 6
contains a discussion regarding the frequency of
occurrence of these events and the consequences
of this for parser performance.
Table 3 shows the average and maximum dis-
tance between head and dependent for each con-
struction, as measured by the difference between
word indices. This is a fairly crude measure of
distance but gives some indication of how “long-
range” the dependencies are for each construc-
tion. The cases of object extraction from a relative
clause and subject extraction from an embedded
clause provide the longest dependencies, on aver-
age. The following sentence gives an example of
how far apart the head and dependent can be in a
</bodyText>
<page confidence="0.993732">
816
</page>
<table confidence="0.99813875">
Construction Avg Dist Max Dist
Obj rel clause 6.8 21
Obj reduced rel 3.4 8
Sbj rel clause 4.4 18
Free rel 3.4 16
Obj wh-question 4.8 9
RNR 4.8 23
Sbj embedded 7.0 21
</table>
<tableCaption confidence="0.9017005">
Table 3: Distance between head and dependent.
subject embedded construction:
</tableCaption>
<bodyText confidence="0.735182">
the same stump which had impaled the car of
many a guest in the past thirty years and which he
refused to have removed.
</bodyText>
<subsectionHeader confidence="0.999139">
3.2 The parsers
</subsectionHeader>
<bodyText confidence="0.981948215189874">
The parsers that we chose to evaluate are the C&amp;C
CCG parser (Clark and Curran, 2007), the Enju
HPSG parser (Miyao and Tsujii, 2005), the RASP
parser (Briscoe et al., 2006), the Stanford parser
(Klein and Manning, 2003), and the DCU post-
processor of PTB parsers (Cahill et al., 2004),
based on LFG and applied to the output of the
Charniak and Johnson reranking parser. Of course
we were unable to evaluate every publicly avail-
able parser, but we believe these are representative
of current wide-coverage robust parsing technol-
ogy.1
The C&amp;C parser is based on CCGbank (Hock-
enmaier and Steedman, 2007), a CCG version of
the Penn Treebank. It is ideally suited for this eval-
uation because CCG was designed to capture the
unbounded dependencies being considered. The
Enju parser was designed with a similar motiva-
tion to C&amp;C, and is also based on an automat-
ically extracted grammar derived from the PTB,
but the grammar formalism is HPSG rather than
CCG. Both parsers produce head-word dependen-
cies reflecting the underlying predicate-argument
structure of a sentence, and so in theory should be
straightforward to evaluate.
The RASP parser is based on a manually con-
structed POS tag-sequence grammar, with a sta-
tistical parse selection component and a robust
1One obvious omission is any form of dependency parser
(McDonald et al., 2005; Nivre and Scholz, 2004). However,
the dependencies returned by these parsers are local, and it
would be non-trivial to infer from a series of links whether a
long-range dependency had been correctly represented. Also,
dependency parsers are not significantly better at recovering
head-based dependencies than constituent parsers based on
the PTB (McDonald et al., 2005).
partial-parsing technique which allows it to re-
turn output for sentences which do not obtain a
full spanning analysis according to the grammar.
RASP has not been designed to capture many of the
dependencies in our corpus; for example, the tag-
sequence grammar has no explicit representation
of verb subcategorisation, and so may not know
that there is a missing object in the case of extrac-
tion from a relative clause (though it does recover
some of these dependencies). However, RASP is
a popular parser used in a number of applications,
and it returns dependencies in a suitable format for
evaluation, and so we considered it to be an appro-
priate and useful member of our parser set.
The Stanford parser is representative of a large
number of PTB parsers, exemplified by Collins
(1997) and Charniak (2000). The Parseval scores
reported for the Stanford parser are not the highest
in the literature, but are competitive enough for our
purposes. The advantage of the Stanford parser is
that it returns dependencies in a suitable format for
our evaluation. The dependencies are obtained by
a set of manually defined rules operating over the
phrase-structure trees returned by the parser (de
Marneffe et al., 2006). Like RASP, the Stanford
parser has not been designed to capture unbounded
dependencies; in particular it does not make use of
any of the trace information in the PTB. However,
we wanted to include a “standard” PTB parser in
our set to see which of the unbounded dependency
constructions it is able to deal with.
Finally, there is a body of work on inserting
trace information into the output of PTB parsers
(Johnson, 2002; Levy and Manning, 2004), which
is the annotation used in the PTB for representing
unbounded dependencies. The work which deals
with the PTB representation directly, such as John-
son (2002), is difficult for us to evaluate because it
does not produce explicit dependencies. However,
the DCU post-processor is ideal because it does
produce dependencies in a GR format. It has also
obtained competitive scores on general GR evalu-
ation corpora (Cahill et al., 2004).
</bodyText>
<subsectionHeader confidence="0.998658">
3.3 Parser evaluation
</subsectionHeader>
<bodyText confidence="0.999818833333333">
The parsers were run essentially out-of-the-box
when parsing the test sentences. The one excep-
tion was C&amp;C, which required some minor adjust-
ing of parameters, as described in the parser doc-
umentation, to obtain close to full coverage on the
data. In addition, the C&amp;C parser comes with a
</bodyText>
<page confidence="0.994785">
817
</page>
<table confidence="0.9969805">
Obj RC Obj Red Sbj RC Free Obj Q RNR Sbj Embed Total
C&amp;C 59.3 62.6 80.0 72.6 (81.2) 27.5 49.4 22.4 (59.7) 53.6
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.4
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 35.7
Rasp 16.5 1.1 53.7 17.9 27.5 34.5 15.3 25.3
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 38.1
</table>
<tableCaption confidence="0.992922">
Table 4: Parser accuracy on the unbounded dependency corpus; the highest score for each construction
</tableCaption>
<bodyText confidence="0.994405901408451">
is in bold; the figures in brackets for C&amp;C derive from the use of a separate question model.
specially designed question model, and so we ap-
plied both this and the standard model to the object
wh-question cases.
The parser output was evaluated against each
dependency in the corpus. Due to the various GR
schemes used by the parsers, an exact match on the
dependency label could not always be expected.
We considered a correctly recovered dependency
to be one where the gold-standard head and depen-
dent were correctly identified, and the label was
an “acceptable match” to the gold-standard label.
To be an acceptable match, the label had to indi-
cate the grammatical function of the extracted el-
ement at least to the level of distinguishing active
subjects, passive subjects, objects, and adjuncts.
For example, we allowed an obj (object) relation
as a close enough match for dobj (direct object)
in the corpus, even though obj does not distin-
guish different kinds of objects, but we did not al-
low generic “relative pronoun” relations that are
underspecified for the grammatical role of the ex-
tracted element.
The differences in GR schemes were such that
we ended up performing a time-consuming largely
manual evaluation. We list here some of the key
differences that made the evaluation difficult.
In some cases, the parser’s set of labels was less
fine-grained than the gold standard. For example,
RASP represents the direct objects of both verbs
and prepositions as dobj (direct object), whereas
the gold-standard uses pobj for the preposition
case. We counted the RASP output as correctly
matching the gold standard.
In other cases, the label on the dependency
containing the gold-standard head and depen-
dent was too underspecified to be acceptable by
itself. For example, where the gold-standard
relation was dobj(placed,buckets), DCU
produced relmod(buckets,placed) with
a generic “relative modifier” label. However,
the correct label could be recovered from else-
where in the parser output, specifically a com-
bination of relpro(buckets,which) and
obj(placed,which). In this case we counted
the DCU output as correctly matching the gold
standard.
In some constructions the Stanford scheme,
upon which the gold-standard was based, makes
different choices about heads than other schemes.
For example, in the the phrase Honolulu, which is
the center of the warning system, the corpus con-
tains a subject dependency with center as the head:
nsubj(center,Honolulu). Other schemes,
however, treat the auxiliary verb is as the head of
the dependency, rather than the predicate nominal
center. As long as the difference in head selec-
tion was due solely to the idiosyncracies of the GR
schemes involved, we counted the relation as cor-
rect.
Finally, the different GR schemes treat coordi-
nation differently. In the corpus, coordinated ele-
ments are always represented with two dependen-
cies. Thus the phrase they may half see and half
imagine the old splendor has two gold-standard
dependencies: dobj(see,splendor) and
dobj(imagine,splendor). If a parser pro-
duced only the former dependency, but appeared
to have the coordination correct, then we awarded
two marks, even though the second dependency
was not explicitly represented.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.99913125">
Accuracies for the various parsers are shown in Ta-
ble 4, with the highest score for each construction
in bold. Enju and C&amp;C are the top performers,
operating at roughly the same level of accuracy
across most of the constructions. Use of the C&amp;C
question model made a huge difference for the wh-
object construction (81.2% vs. 27.5%), showing
that adaptation techniques specific to a particular
</bodyText>
<page confidence="0.994992">
818
</page>
<bodyText confidence="0.999577416666667">
construction can be successful (Rimell and Clark,
2008).
In order to learn more from these results, in Sec-
tion 5 we analyse the various errors made by the
C&amp;C parser on each construction. The conclusions
that we arrive at for the C&amp;C parser we would also
expect to apply to Enju, on the whole, since the de-
sign of the two parsers is so similar. In fact, some
of the recommendations for improvement on this
corpus, such as the need for a better parsing model
to make better attachment decisions, are parser in-
dependent.
The poor performance of RASP on this corpus
is clearly related to a lack of subcategorisation in-
formation, since this is crucial for recovering ex-
tracted arguments. For Stanford, incorporating the
trace information from the PTB into the statistical
model in some way is likely to help. The C&amp;C and
Enju parsers do this through their respective gram-
mar formalisms. Our informal impression of the
DCU post-processor is that it has much of the ma-
chinery available to recover the dependencies that
the Enju and C&amp;C parsers do, but for some reason
which is unclear to us it performs much worse.
</bodyText>
<subsectionHeader confidence="0.457043">
5 Analysis of the C&amp;C Parser
</subsectionHeader>
<bodyText confidence="0.99955072">
We categorised the errors made by the C&amp;C parser
on the development data for each construction. We
chose the C&amp;C parser for the analysis because it
was one of the top performers and we have more
knowledge of its workings than those of Enju.
The C&amp;C parser first uses a supertagger to as-
sign a small number of CCG lexical categories (es-
sentially subcategorisation frames) to each word in
the sentence. These categories are then combined
using a set of combinatory rules to build a CCG
derivation. The parser uses a log-linear probabil-
ity model to select the highest-scoring derivation
(Clark and Curran, 2007). In general, errors in de-
pendency recovery may occur if the correct lexical
category is not assigned by the supertagger for one
or more of the words in a sentence, or if an incor-
rect derivation is chosen by the parsing model.
For unbounded dependency recovery, one
source of errors (labeled type 1 in Table 5) is the
wrong lexical category being assigned to the word
(normally a verb or preposition) governing the ex-
traction site. In these testaments that I would sub-
mit here, if submit is assigned a category for an
intransitive rather than transitive verb, the verb-
object relation will not be recovered.
</bodyText>
<table confidence="0.99929">
1a 1b 1c 1d 2 3 Errs Tot
ObjRC 6 5 2 13 20
ObjRed 2 1 1 1 3 8 23
SbjRC 8 1 9 43
Free 1 1 2 22
ObjQ 2 2 4 25
RNR 2 1 7 3 13 28
SbjEmb 3 2 1 4 10 13
Subtotal 6 2 12 4
Total 24 21 14 59 174
</table>
<tableCaption confidence="0.980812">
Table 5: Error analysis for C&amp;C. Errs is the to-
</tableCaption>
<bodyText confidence="0.945324641025641">
tal number of errors for a construction, Tot is the
number of dependencies of that type in the devel-
opment data.
There are a number of reasons why the wrong
category may be assigned. First, the lexicon may
not contain enough information about possible
categories for the word (1a), or the necessary cat-
egory may not exist in the parser’s grammar at all
(1b). Even if the grammar contains the correct cat-
egory and the lexicon makes it available, the pars-
ing model may not choose it (1c). Finally, a POS-
tagging error on the word may mislead the parser
into assigning the wrong category (1d).2
A second source of errors (type 2) is attach-
ment decisions that the parser makes indepen-
dently of the unbounded dependency. In Morgan
... carried in several buckets of water from the
spring which he poured into the copper boiler, the
parser assigns the correct categories for the rela-
tive pronoun and verb, but chooses spring rather
than buckets as the head of the relativized NP (i.e.
the object of pour). Most attachment errors in-
volve prepositional phrases (PPs) and coordina-
tion, which have long been known to be areas
where parsers need improvement.
Finally, errors in unbounded dependency recov-
ery may be due to complex errors in the surround-
ing parse context (type 3). We will not comment
more on these cases since they do not tell us much
about unbounded dependencies in particular.
Table 5 shows the distribution of error types
across constructions for the C&amp;C parser. Subject
relative clauses, for example, did not have any er-
rors of type 1, because a verb with an extracted
2We considered an error to be type 1 only when the cate-
gory error occurred on the word governing the extraction site,
except in the subject embedded sentences, where we also in-
cluded the embedding verb, since the category of this verb is
key to dependency recovery.
</bodyText>
<page confidence="0.996119">
819
</page>
<bodyText confidence="0.99996540625">
subject does not require a special lexical category.
Most of the errors here are of type 2. For exam-
ple, in a series ofpipes and a pressure-measuring
chamber which record the rise and fall of the wa-
ter surface, the parser attaches the relative clause
to chamber but not to series.
Subject embedded sentences show a different
pattern. Many of the errors can be attributed to
problems with the lexicon and grammar (1a and
1b). For example, in shadows that they imagined
were Apaches, the word imagined never appears in
the training data with the correct category, and so
the required entry is missing from the lexicon.
Object extraction from a relative clause had
a higher number of errors involving the parsing
model (1c). In the first carefree, dreamless sleep
that she had known, the transitive category is
available for known, but not selected by the model.
The majority of the errors made by the parser
are due to insufficient grammar coverage or weak-
ness in the parsing model due to sparsity of head
dependency data, the same fundamental problems
that have dogged automatic parsing since its in-
ception. Hence one view of statistical parsing is
that it has allowed us to solve the easy problems,
but we are still no closer to a general solution for
the recovery of the “difficult” dependencies. One
possibility is to create more training data target-
ing these constructions – effectively “active learn-
ing by construction” – in the way that Rimell and
Clark (2008) were able to build a question parser.
We leave this idea for future work.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999979447368421">
Unbounded dependencies are rare events, out in
the Zipfian “long tail”. They will always consti-
tute a fraction of a percent of the overall total of
head-dependencies in any corpus, a proportion too
small to make a significant impact on global mea-
sures of parser accuracy, when expressive parsers
are compared to those that merely approximate
human grammar using finite-state or context-free
covers. This will remain the case even when such
measures are based on dependencies, rather than
on parse trees.
Nevertheless, unbounded dependencies remain
highly significant in a much more important sense.
They support the constructions that are central to
those applications of parsing technology for which
precision is as important as recall, such as open-
domain question-answering. As low-power ap-
proximate parsing methods improve (as they must
if they are ever to be usable at all for such tasks),
we predict that the impact of the constructions we
examine here will become evident. No matter how
infrequent object questions like “What do frogs
eat?” are, if they are answered as if they were sub-
ject questions (“Herons”), users will rightly reject
any excuse in terms of the overall statistical distri-
bution of related bags of words.
Whether such improvements in parsers come
from the availability of more human-labeled data,
or from a breakthrough in unsupervised machine
learning, we predict an imminent “Uncanny Val-
ley” in parsing applications, due to the inability of
parsers to recover certain semantically important
dependencies, of the kind familiar from humanoid
robotics and photorealistic animation. In such ap-
plications, the closer the superficial resemblance
to human behavior gets, the more disturbing sub-
tle departures become, and the more they induce
mistrust and revulsion in the user.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999472333333334">
In this paper we have demonstrated that current
parsing technology is poor at recovering some
of the unbounded dependencies which are crucial
for fully representing the underlying predicate-
argument structure of a sentence. We have also
argued that correct recovery of such dependen-
cies will become more important as parsing tech-
nology improves, despite the relatively low fre-
quency of occurrence of the corresponding gram-
matical constructions. We also see this more fo-
cused parser evaluation methodology — in this
case construction-focused — as a way of improv-
ing parsing technology, as an alternative to the
exclusive focus on incremental improvements in
overall accuracy measures such as Parseval.
</bodyText>
<sectionHeader confidence="0.997493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992385">
Laura Rimell and Stephen Clark were supported
by EPSRC grant EP/E035698/1. Mark Steed-
man was supported by EU IST Cognitive Systems
grant IP FP6-2004-IST-4-27657 (PACO-PLUS).
We would like to thank Aoife Cahill for produc-
ing the DCU data.
</bodyText>
<page confidence="0.994021">
820
</page>
<sectionHeader confidence="0.990241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863476635514">
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of English grammars. In HLT ’91: Proceed-
ings of the Workshop on Speech and Natural Lan-
guage, pages 306–311.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the 10th Meeting of
the EACL, pages 19–26, Budapest, Hungary.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In
Proceedings of the Interactive Demo Session of
COLING/ACL-06, Sydney, Australia.
A. Cahill, M. Burke, R. O’Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of the 42nd Meeting of the ACL, pages 320–327,
Barcelona, Spain.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Dynamic programming and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Natural Language Learning
(CoNLL-08), pages 9–16, Manchester, UK.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proceedings of the 1st LREC Conference,
pages 447–454, Granada, Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Meeting of
the ACL, pages 173–180, Michigan, Ann Arbor.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of the NAACL, pages 132–139, Seattle, WA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using
CCG. In Proceedings of the EMNLP Conference,
pages 111–118, Barcelona, Spain.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Meeting of the ACL, pages 16–23, Madrid,
Spain.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th LREC Conference, Genoa,
Italy.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 EMNLP Con-
ference, Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceed-
ings of the 46th Meeting of the ACL, pages 586–594,
Columbus, Ohio.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, pages 136–143, Philadelphia, PA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of Association for Computa-
tional Linguistics, pages 423–430, Sapporo, Japan.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: cor-
recting the surface dependency approximation. In
Proceedings of the 42nd Meeting of the ACL, pages
328–335, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420–1425, Montreal, Canada.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics Confer-
ence, pages 152–159, Brooklyn, NY.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd Meet-
ing of the ACL, pages 91–98, Michigan, Ann Arbor.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Meeting of the
ACL, pages 83–90, Michigan, Ann Arbor.
J. Nivre and M. Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of
COLING-04, pages 64–70, Geneva, Switzerland.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
the HLT/NAACL Conference, Rochester, NY.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 EMNLP Conference,
pages 475–484, Honolulu, Hawai‘i.
</reference>
<page confidence="0.998396">
821
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.537803">
<title confidence="0.999183">Unbounded Dependency Recovery for Parser Evaluation</title>
<author confidence="0.999482">Mark</author>
<affiliation confidence="0.9957295">University of School of</affiliation>
<email confidence="0.980967">steedman@inf.ed.ac.uk</email>
<author confidence="0.636921">Rimell</author>
<affiliation confidence="0.9336195">University of Computer</affiliation>
<email confidence="0.991335">stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.999586368421053">This paper introduces a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies, from seven different grammatical constructions. We run a series of off-theshelf parsers on the corpus to evaluate how well state-of-the-art parsing technology is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In HLT ’91: Proceedings of the Workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="2159" citStr="Black et al., 1991" startWordPosition="326" endWordPosition="329">ation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998). In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score. The constructions that we focus on are various unbounded dependency constructions. These are interesting for parser evaluation for the following reasons: one, they provide a strong test of the parser’s knowledge of the grammar of the language, sin</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In HLT ’91: Proceedings of the Workshop on Speech and Natural Language, pages 306–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of a new DOP model.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>pages</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1127" citStr="Bod, 2003" startWordPosition="164" endWordPosition="165">state-of-the-art parsing technology is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008). McClosky et al. (2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at in</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of a new DOP model. In Proceedings of the 10th Meeting of the EACL, pages 19–26, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Interactive Demo Session of COLING/ACL-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="16172" citStr="Briscoe et al., 2006" startWordPosition="2626" endWordPosition="2629">es an example of how far apart the head and dependent can be in a 816 Construction Avg Dist Max Dist Obj rel clause 6.8 21 Obj reduced rel 3.4 8 Sbj rel clause 4.4 18 Free rel 3.4 16 Obj wh-question 4.8 9 RNR 4.8 23 Sbj embedded 7.0 21 Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C&amp;C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C&amp;C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju par</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the Interactive Demo Session of COLING/ACL-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R O’Donovan</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>320--327</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>A. Cahill, M. Burke, R. O’Donovan, J. van Genabith, and A. Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations. In Proceedings of the 42nd Meeting of the ACL, pages 320–327, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Dynamic programming and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Natural Language Learning (CoNLL-08),</booktitle>
<pages>9--16</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="1188" citStr="Carreras et al., 2008" startWordPosition="172" endWordPosition="175">ecover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008). McClosky et al. (2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accura</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. Dynamic programming and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Natural Language Learning (CoNLL-08), pages 9–16, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st LREC Conference,</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<contexts>
<context position="2375" citStr="Carroll et al., 1998" startWordPosition="363" endWordPosition="366">endency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998). In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score. The constructions that we focus on are various unbounded dependency constructions. These are interesting for parser evaluation for the following reasons: one, they provide a strong test of the parser’s knowledge of the grammar of the language, since many instances of unbounded dependencies are difficult to recover using shallow techniques in which the grammar is only superficially represented; and two, recovering these dependencies is necessary to completely </context>
<context position="12828" citStr="Carroll et al., 1998" startWordPosition="2054" endWordPosition="2057">mate this process with use of the tgrep search tool. However, in order to obtain reliable statistics regarding frequency of occurrence, and to ensure a high-quality resource, we used fairly broad regular expressions to identify the original set followed by manual review. We chose to represent the dependencies as grammatical relations (GRs) since this format seemed best suited to represent the kind of semantic relationship we are interested in. GRs are headbased dependencies that have been suggested as a more appropriate representation for general parser evaluation than phrase-structure trees (Carroll et al., 1998). Table 1 gives examples of how GRs are used to represent the relevant dependencies. The particular GR scheme we used was based on the Stanford scheme (de Marneffe et al., 2006); however, the specific GR scheme is not too crucial since the whole sentence is not being represented in the corpus, only the unbounded dependencies. 3 Experiments The five parsers described in Section 3.2 were used to parse the test sentences in the corpus, and the percentage of dependencies in the test set recovered by each parser for each construction was calculated. The details of how the parsers were run and how t</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the 1st LREC Conference, pages 447–454, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL,</booktitle>
<pages>173--180</pages>
<location>Michigan, Ann Arbor.</location>
<contexts>
<context position="1313" citStr="Charniak and Johnson (2005)" startWordPosition="193" endWordPosition="196">lidity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008). McClosky et al. (2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Meeting of the ACL, pages 173–180, Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="18513" citStr="Charniak (2000)" startWordPosition="3011" endWordPosition="3012">he dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies). However, RASP is a popular parser used in a number of applications, and it returns dependencies in a suitable format for evaluation, and so we considered it to be an appropriate and useful member of our parser set. The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000). The Parseval scores reported for the Stanford parser are not the highest in the literature, but are competitive enough for our purposes. The advantage of the Stanford parser is that it returns dependencies in a suitable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. However, we wanted to incl</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st Meeting of the NAACL, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="16085" citStr="Clark and Curran, 2007" startWordPosition="2611" endWordPosition="2614"> embedded clause provide the longest dependencies, on average. The following sentence gives an example of how far apart the head and dependent can be in a 816 Construction Avg Dist Max Dist Obj rel clause 6.8 21 Obj reduced rel 3.4 8 Sbj rel clause 4.4 18 Free rel 3.4 16 Obj wh-question 4.8 9 RNR 4.8 23 Sbj embedded 7.0 21 Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C&amp;C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C&amp;C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation becaus</context>
<context position="25887" citStr="Clark and Curran, 2007" startWordPosition="4235" endWordPosition="4238">the C&amp;C Parser We categorised the errors made by the C&amp;C parser on the development data for each construction. We chose the C&amp;C parser for the analysis because it was one of the top performers and we have more knowledge of its workings than those of Enju. The C&amp;C parser first uses a supertagger to assign a small number of CCG lexical categories (essentially subcategorisation frames) to each word in the sentence. These categories are then combined using a set of combinatory rules to build a CCG derivation. The parser uses a log-linear probability model to select the highest-scoring derivation (Clark and Curran, 2007). In general, errors in dependency recovery may occur if the correct lexical category is not assigned by the supertagger for one or more of the words in a sentence, or if an incorrect derivation is chosen by the parsing model. For unbounded dependency recovery, one source of errors (labeled type 1 in Table 5) is the wrong lexical category being assigned to the word (normally a verb or preposition) governing the extraction site. In these testaments that I would submit here, if submit is assigned a category for an intransitive rather than transitive verb, the verbobject relation will not be reco</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
</authors>
<title>Object-extraction and question-parsing using CCG.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="5071" citStr="Clark et al. (2004)" startWordPosition="797" endWordPosition="800">cause of different conventions across the parsers as to how the underlying grammar is represented. Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 Unbounded Dependency Corpus 2.1 The constructions An unbounded dependency construction contains a word or phrase which appears to have been moved, while being in</context>
</contexts>
<marker>Clark, Steedman, Curran, 2004</marker>
<rawString>Stephen Clark, Mark Steedman, and James R. Curran. 2004. Object-extraction and question-parsing using CCG. In Proceedings of the EMNLP Conference, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="18493" citStr="Collins (1997)" startWordPosition="3008" endWordPosition="3009">o capture many of the dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies). However, RASP is a popular parser used in a number of applications, and it returns dependencies in a suitable format for evaluation, and so we considered it to be an appropriate and useful member of our parser set. The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000). The Parseval scores reported for the Stanford parser are not the highest in the literature, but are competitive enough for our purposes. The advantage of the Stanford parser is that it returns dependencies in a suitable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. Howeve</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Meeting of the ACL, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1911" citStr="Collins, 1999" startWordPosition="287" endWordPosition="288">d Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998). In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score.</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th LREC Conference,</booktitle>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th LREC Conference, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 EMNLP Conference,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2073" citStr="Gildea, 2001" startWordPosition="315" endWordPosition="316">will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998). In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score. The constructions that we focus on are various unbounded dependency constructions. These are interesting for parser evaluation for the following reasons: one, th</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 EMNLP Conference, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="16600" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2696" endWordPosition="2700"> have removed. 3.2 The parsers The parsers that we chose to evaluate are the C&amp;C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C&amp;C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to C&amp;C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG. Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, w</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Meeting of the ACL,</booktitle>
<pages>586--594</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1164" citStr="Huang, 2008" startWordPosition="170" endWordPosition="171"> is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008). McClosky et al. (2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency ty</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the 46th Meeting of the ACL, pages 586–594, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>136--143</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5155" citStr="Johnson (2002)" startWordPosition="813" endWordPosition="814">resented. Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 Unbounded Dependency Corpus 2.1 The constructions An unbounded dependency construction contains a word or phrase which appears to have been moved, while being interpreted in the position of the resulting “gap”. An unlimited number of clause boun</context>
<context position="19341" citStr="Johnson, 2002" startWordPosition="3150" endWordPosition="3151">itable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. However, we wanted to include a “standard” PTB parser in our set to see which of the unbounded dependency constructions it is able to deal with. Finally, there is a body of work on inserting trace information into the output of PTB parsers (Johnson, 2002; Levy and Manning, 2004), which is the annotation used in the PTB for representing unbounded dependencies. The work which deals with the PTB representation directly, such as Johnson (2002), is difficult for us to evaluate because it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). 3.3 Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences. The one exception was C&amp;C, which </context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Mark Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings of the 40th Meeting of the ACL, pages 136–143, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="16219" citStr="Klein and Manning, 2003" startWordPosition="2633" endWordPosition="2636">dependent can be in a 816 Construction Avg Dist Max Dist Obj rel clause 6.8 21 Obj reduced rel 3.4 8 Sbj rel clause 4.4 18 Free rel 3.4 16 Obj wh-question 4.8 9 RNR 4.8 23 Sbj embedded 7.0 21 Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C&amp;C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C&amp;C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to C</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>328--335</pages>
<location>Barcelona,</location>
<contexts>
<context position="5282" citStr="Levy and Manning (2004)" startWordPosition="834" endWordPosition="837">argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 Unbounded Dependency Corpus 2.1 The constructions An unbounded dependency construction contains a word or phrase which appears to have been moved, while being interpreted in the position of the resulting “gap”. An unlimited number of clause boundaries may intervene between the moved element and the gap (hence “unbounded”). The seven constructions in our corpus were chos</context>
<context position="19366" citStr="Levy and Manning, 2004" startWordPosition="3152" endWordPosition="3155">or our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. However, we wanted to include a “standard” PTB parser in our set to see which of the unbounded dependency constructions it is able to deal with. Finally, there is a body of work on inserting trace information into the output of PTB parsers (Johnson, 2002; Levy and Manning, 2004), which is the annotation used in the PTB for representing unbounded dependencies. The work which deals with the PTB representation directly, such as Johnson (2002), is difficult for us to evaluate because it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). 3.3 Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences. The one exception was C&amp;C, which required some minor adjus</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Roger Levy and Christopher Manning. 2004. Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation. In Proceedings of the 42nd Meeting of the ACL, pages 328–335, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>1420--1425</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2352" citStr="Lin, 1995" startWordPosition="361" endWordPosition="362">uent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower (Collins, 1999). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Finally, some researchers have argued that the Parseval metrics (Black et al., 1991) are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance (Lin, 1995; Carroll et al., 1998). In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score. The constructions that we focus on are various unbounded dependency constructions. These are interesting for parser evaluation for the following reasons: one, they provide a strong test of the parser’s knowledge of the grammar of the language, since many instances of unbounded dependencies are difficult to recover using shallow techniques in which the grammar is only superficially represented; and two, recovering these dependencies is n</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proceedings of IJCAI-95, pages 1420–1425, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics Conference,</booktitle>
<pages>152--159</pages>
<location>Brooklyn, NY.</location>
<contexts>
<context position="1212" citStr="McClosky et al. (2006)" startWordPosition="176" endWordPosition="179">. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008). McClosky et al. (2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantical</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics Conference, pages 152–159, Brooklyn, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL,</booktitle>
<pages>91--98</pages>
<location>Michigan, Ann Arbor.</location>
<contexts>
<context position="17334" citStr="McDonald et al., 2005" startWordPosition="2817" endWordPosition="2820">e the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to C&amp;C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG. Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 1One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004). However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented. Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005). partial-parsing technique which allows it to return output for sentences which do not obtain a full spanning analysis according to the grammar. RASP has not been designed to capture many of the dependencies in our corpus; for e</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Meeting of the ACL, pages 91–98, Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL,</booktitle>
<pages>83--90</pages>
<location>Michigan, Ann Arbor.</location>
<contexts>
<context position="16132" citStr="Miyao and Tsujii, 2005" startWordPosition="2619" endWordPosition="2622">es, on average. The following sentence gives an example of how far apart the head and dependent can be in a 816 Construction Avg Dist Max Dist Obj rel clause 6.8 21 Obj reduced rel 3.4 8 Sbj rel clause 4.4 18 Free rel 3.4 16 Obj wh-question 4.8 9 RNR 4.8 23 Sbj embedded 7.0 21 Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C&amp;C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C&amp;C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dep</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Meeting of the ACL, pages 83–90, Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="17359" citStr="Nivre and Scholz, 2004" startWordPosition="2821" endWordPosition="2824">ncies being considered. The Enju parser was designed with a similar motivation to C&amp;C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG. Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 1One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004). However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented. Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005). partial-parsing technique which allows it to return output for sentences which do not obtain a full spanning analysis according to the grammar. RASP has not been designed to capture many of the dependencies in our corpus; for example, the tagsequence g</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M. Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING-04, pages 64–70, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the HLT/NAACL Conference,</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="1151" citStr="Petrov and Klein, 2007" startWordPosition="166" endWordPosition="169">e-art parsing technology is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers. 1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank (Bod, 2003; Petrov and Klein, 2007; Huang, 2008; Carreras et al., 2008). McClosky et al. (2006) report an F-score of 92.1% using selftraining applied to the reranker of Charniak and Johnson (2005). Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers. A single score in isolation can be misleading, however, for a number of reasons. First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of the HLT/NAACL Conference, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 EMNLP Conference,</booktitle>
<pages>475--484</pages>
<location>Honolulu, Hawai‘i.</location>
<contexts>
<context position="11343" citStr="Rimell and Clark (2008)" startWordPosition="1817" endWordPosition="1820">e to ensure that default settings for the various parsers were appropriate for this data). We did not annotate the full sentences, since we are only interested in the unbounded dependencies and full annotation of such a corpus would be extremely time-consuming. With the exception of the question construction, all sentences were taken from the PTB, with roughly half from the WSJ sections (excluding 2-21 which provided the training data for many 815 of the parsers in our set) and half from Brown (roughly balanced across the different sections). The questions were taken from the question data in Rimell and Clark (2008), which was obtained from various years of the TREC QA track. We chose to use the PTB as the main source because the use of traces in the PTB annotation provides a starting point for the identification of unbounded dependencies. Sentences were selected for the corpus by a combination of automatic and manual processes. A regular expression applied to PTB trees, searching for appropriate traces for a particular construction, was first used to extract a set of candidate sentences. All candidates were manually reviewed and, if selected, annotated with one or more grammatical relations representing</context>
<context position="24196" citStr="Rimell and Clark, 2008" startWordPosition="3938" endWordPosition="3941">he former dependency, but appeared to have the coordination correct, then we awarded two marks, even though the second dependency was not explicitly represented. 4 Results Accuracies for the various parsers are shown in Table 4, with the highest score for each construction in bold. Enju and C&amp;C are the top performers, operating at roughly the same level of accuracy across most of the constructions. Use of the C&amp;C question model made a huge difference for the whobject construction (81.2% vs. 27.5%), showing that adaptation techniques specific to a particular 818 construction can be successful (Rimell and Clark, 2008). In order to learn more from these results, in Section 5 we analyse the various errors made by the C&amp;C parser on each construction. The conclusions that we arrive at for the C&amp;C parser we would also expect to apply to Enju, on the whole, since the design of the two parsers is so similar. In fact, some of the recommendations for improvement on this corpus, such as the need for a better parsing model to make better attachment decisions, are parser independent. The poor performance of RASP on this corpus is clearly related to a lack of subcategorisation information, since this is crucial for rec</context>
<context position="30039" citStr="Rimell and Clark (2008)" startWordPosition="4987" endWordPosition="4990">ted by the model. The majority of the errors made by the parser are due to insufficient grammar coverage or weakness in the parsing model due to sparsity of head dependency data, the same fundamental problems that have dogged automatic parsing since its inception. Hence one view of statistical parsing is that it has allowed us to solve the easy problems, but we are still no closer to a general solution for the recovery of the “difficult” dependencies. One possibility is to create more training data targeting these constructions – effectively “active learning by construction” – in the way that Rimell and Clark (2008) were able to build a question parser. We leave this idea for future work. 6 Discussion Unbounded dependencies are rare events, out in the Zipfian “long tail”. They will always constitute a fraction of a percent of the overall total of head-dependencies in any corpus, a proportion too small to make a significant impact on global measures of parser accuracy, when expressive parsers are compared to those that merely approximate human grammar using finite-state or context-free covers. This will remain the case even when such measures are based on dependencies, rather than on parse trees. Neverthe</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of the 2008 EMNLP Conference, pages 475–484, Honolulu, Hawai‘i.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>