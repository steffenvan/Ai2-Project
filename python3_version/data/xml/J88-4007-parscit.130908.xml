<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<note confidence="0.208025">
Book Reviews The Computational Analysis of English: A Corpus-based Approach
</note>
<bodyText confidence="0.668372">
nameâ€”see Schilizenberger (1956). For a summary of the issues,
see Ryckman (1986), chap. 5.
</bodyText>
<listItem confidence="0.9414356">
2. Carnap and Bar-Hillel (1952), Bar-Hillel (1952). The present
book seems in part responsive to this program, having the same
title as Bar-Hillel (1964).
3. See papers collected in Hintikka and Suppes (1970).
4. Dretske (1981), Israel and Perry (forthcoming). Peer commen-
</listItem>
<bodyText confidence="0.892381941176471">
tary in Dretske (1983), especially that of Haber, did not accept
Dretske&apos;s attempted analogies to the metrics of Shannon and
Weaver. The notion of &amp;quot;information pickup&amp;quot; implies a pre-
established harmony of the world and the mind, disregarding the
well-known arbitrariness of language.
5. While Fodor (1986) does gives a cogent criticism of attempts to
locate information &amp;quot;in the world&amp;quot;, the alternative &amp;quot;intentional&amp;quot;
conception that he advocates relies on questionable assumptions
of an &amp;quot;internal code&amp;quot; wherein such information is &amp;quot;encoded&amp;quot;.
The problem, of course, lies in unpacking this metaphor. Falling
into the custom of taking the computational metaphor of mind
literally, he resuscitates our old familiar homunculus (in compu-
tational disguise as the &amp;quot;executive&amp;quot;) to provide a way out of the
problem of node labels being of higher logical type than the
nodes that they label. A simpler resolution follows from Harris&apos;s
recognition that natural language has no separate metalanguage.
See also Fodor (forthcoming).
</bodyText>
<listItem confidence="0.992835166666667">
6. See especially Harris (1982), and Harris, Gottfried, Ryckman, et
al. (in press).
7. This thus cuts deeper than the naive rule-counting metrics for
adjudication of grammars advocated not so long ago by genera-
tivists (see Ryckman 1986).
8. This work is reported in depth in Harris et al. (in press). These
science languages occupy a place between natural language and
mathematics, the chief difference from the former being that
operator-argument likelihoods are much more strongly defined,
amounting in most cases to simple binary selection rather than a
graded scale. One of the many interesting aspects of this
research is determining empirically the form of argumentation in
science. The logical apparatus of deduction and other forms of
inference are required only for various uses to which language
may be put, rather than being the semantic basis for natural
language, as has sometimes been claimed.
9. This is a refinement of the notion of distributional meaning
developed in, e.g., Harris (1954).
</listItem>
<bodyText confidence="0.51118725">
10. The case of zero likelihood is covered by the word classes of the
first constraint.
11. An example is the elision of one of a small set of operators
including appear, arrive, show up, which have high likelihood
</bodyText>
<equation confidence="0.845366">
under expect, in I expect John momentarily. The adverb mo-
</equation>
<bodyText confidence="0.900185125">
mentarily can only modify the elided to arrive, etc., since neither
expect nor John is asserted to be momentary. The infinitive to,
the suffix -ly ,and the status of momentarily as a modifier are the
results of other reductions that are described in detail in Harris
(1982).
12. For a computer implementation, see Johnson (1987). I am
grateful to Tom Ryckman for helpful comments on an early draft
of this review.
</bodyText>
<figure confidence="0.6650312">
THE COMPUTATIONAL ANALYSIS OF ENGLISH: A
CORPUS-BASED APPROACH
Roger Garside, Geoffrey Leech, and Geoffrey
Sampson, (eds.)
(University of Lancaster and University of Leeds)
London: Longman, 1987, xii+ 196 pp.
ISBN 0-582-29149-6; (Sb)
Reviewed by
Michael Lesk
Bell Communications Research
</figure>
<bodyText confidence="0.997140222222222">
Why is it so remarkable to have a book whose analysis
of language is entirely based on actual writing? Profes-
sors Garside, Leech, and Sampson have the refreshing
view that the analysis of language ought to be based on
real language, and have presented 12 papers resulting
from their studies using the Lancaster-Oslo-Bergen
corpus of a million words of British English. They
present studies of spelling correction, part-of-speech
assignment, parsing, and speech synthesis based on
probability techniques derived from corpus studies. The
methods here work on arbitrary texts and with reason-
able efficiency.
English includes a great variety of constructions that
pose a dilemma for any strict grammar: to include
everything and face great ambiguity, or to be extremely
prescriptive and reject much. The authors solve this
problem by using probabilities to balance both frequent
and infrequent constructions, and to emphasize low-
level simple algorithms over deep interpretation.
For anyone trying to make practical use of text, this
book is extremely enlightening. English is not an infe-
rior substitute for Prolog, and treating it as such is not
only a mismatch, but also unnecessary for many tasks.
The simple use of probabilities can perform many tasks
that at first glance might be thought to require under-
standing. Methods for doing these are explained clearly
in the book.
The most detailed result described is the technique of
tagging, or assigning parts of speech statistically. By
using both the individual probabilities of different parts
of speech for a single word, and the combined proba-
bility of sequences of two parts of speech, tagging can
be done with 96-97% accuracy. This relatively simple
algorithm, relying for performance on statistical data
accumulated over a large sample of English rather than
upon some kind of model of language, is typical of the
results presented in this book. The algorithm runs on
any input, from any subject area, and does a useful job
without claiming to &amp;quot;understand&amp;quot; natural language.
Just as we have learned that computers can play master-
level chess by exhaustive evaluation of all possible
moves, without any grand strategy or even plausible
move selection, it seems that many linguistic tasks do
not require understanding or modeling, but merely
experience, translated into probability data.
</bodyText>
<page confidence="0.878409">
90 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<subsectionHeader confidence="0.319782">
Book Reviews Semantic Interpretation and the Resolution of Ambiguity
</subsectionHeader>
<bodyText confidence="0.999737705882353">
Similar discussions apply to parsing. Fifty thousand
words of the corpus have been parsed by hand, and this
has been used to make a table of the relative frequencies
of different syntactic constructions. Assuming that the
correct parse tree is the one ma-de of the most probable
constituents (to greatly oversimplify in the interests of
saving space), a program was written to parse with
about 50% accuracy. Since the preparation of this book,
continuing work by Eric Atwell and Geoffrey Sampson
at Leeds has greatly improved on this figure, using a
simulated annealing technique (see Sampson 1986).
Other chapters of the book discuss the history of
corpora in linguistic research, a defense of probabilistic
methods, a discussion of speech synthesis and an out-
line of a sophisticated spelling corrector. Not much has
been done on speech synthesis, partly becaiise we do
not as yet have good data on the relation between
syntax and prosody. The spelling corrector is aimed at
errors of word selection, i.e., finding words that, al-
though they appear in the dictionary, should not appear
in the particular sentence being studied (e.g., &amp;quot;They
kingdom come, thy will be done&amp;quot;). All these tools
follow the same model: reliance on statistics from the
corpus.
It is a great relief to read a book like this, which is
based on real texts rather than upon the imaginary
language, sharing a few word forms with English, that is
studied at MIT and some other research institutes (see
Postal 1988). It is amazing that computers, which are
distinguished for their ability to deal with vast quantities
of bytes and their incompetence with even simple
patterns and models, have been used in linguistics
primarily for the implementation of complex logical
models. This book is a start on the exploitation of large
database methods for linguistic information. It is re-
markable for the performance of its methods combined
with their simplicity. Unlike many books on linguistics,
it is easy to understand; it makes one think of the
Moliere character who suddenly found out he had been
speaking prose all his life.
I heartily recommend this book to anyone who
wishes to process language for a useful purpose. Other
workers such as John Sinclair (1987) and Yaacov
Choueka (1988) have also used large text databases for
deriving linguistic information. When I was an under-
graduate, one of my professors said that &amp;quot;mathematical
intuition means having seen the problem before.&amp;quot; Sim-
ilarly, there is no substitute in linguistics for knowing
that a particular construction is likely because it has
appeared many times. This book is a testimony to the
superiority of experience over fantasy.
</bodyText>
<sectionHeader confidence="0.987276" genericHeader="abstract">
REFERENCES
</sectionHeader>
<reference confidence="0.907443666666667">
Choueka, Yaacov 1988 Looking for Needles in a Haystack. In
Proceedings of the RIAO 88, 609-623.
Postal, Paul 1988 Advances in Linguistic Rhetoric. In Natural Lan-
guage and Linguistic Theory 6:129-137.
Sampson, Geoffrey 1986 Simulated Annealing as a Parsing Tech-
nique. In University of Leeds Working Papers in Linguistics and
Phonetics 4:43-60.
Sinclair, John 1987 Looking up. Collins, London, England; Glasgow,
Scotland.
</reference>
<bodyText confidence="0.7797388">
Michael Lesk is division manager of computer science re-
search at Bell Communications Research, 445 South St.,
Morristown, NJ 07960. He uses machine-readable dictionaries
in his research on text handling and retrieval. E-mail:
lesk@wind.bellcore.com
</bodyText>
<sectionHeader confidence="0.9967445" genericHeader="keywords">
SEMANTIC INTERPRETATION AND THE RESOLUTION OF
AMBIGUITY
</sectionHeader>
<subsectionHeader confidence="0.601465">
Graeme Hirst
</subsectionHeader>
<affiliation confidence="0.43698">
(University of Toronto)
Cambridge, England: Cambridge University Press,
</affiliation>
<figure confidence="0.669192">
1987, xiv +263 pp.
ISBN 0-521-32203-0; (hb) $39.50 [20% discount to
ACL members]
Reviewed by
Karen Sparck Jones
University of Cambridge
</figure>
<bodyText confidence="0.996818696969697">
Hirst&apos;s book presents an approach to natural language
interpretation, using as his vehicle a description of the
experimental system he built. It therefore has to be
evaluated as a contribution on how to build NLP
systems from both theoretical and practical points of
view. It also has to be considered for teaching purposes,
since Hirst has vamped up what was originally a thesis
with some pedagogic exposition and test exercises, as
well as a substantial and useful bibliography.
Hirst is very clear about his aims and very honest
about what he has tackled. He presents detail well and
provides excellent summaries, so the essential proper-
ties of his work are well laid out.
His goal was to build an interpretation system that
could handle serious lexical and structural ambiguity,
and handle it in a principled way. His concern is thus
essentially computational; he does not make any claims
for the psycholinguistic relevance of what he is doing,
but he is, on the other hand, willing to exploit psycho-
linguistically derived support for good processing strat-
egies.
The system consists of a syntactic parser, Paragram,
a semantic interpreter, Absity, and two disambiguation
processors: the Polaroid Word (PW) subsystem for
lexical disambiguation and the Semantic Enquiry Desk
for structural disambiguation. The system builds an
explicit meaning representation in the frame language
Frail.
Hirst&apos;s design is motivated by two goals: to allow
processes of different sorts to use different kinds of
information but to interact to construct a sentence
representation; and to do this in the theoretically well-
founded way exemplified by Montague&apos;s work by doing
</bodyText>
<page confidence="0.287072">
Computational Linguistics, Volume 14, Number 4, December 1988 91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000401">
<title confidence="0.898442">Book Reviews The Computational Analysis of English: A Corpus-based Approach</title>
<abstract confidence="0.991152436363636">nameâ€”see Schilizenberger (1956). For a summary of the issues, see Ryckman (1986), chap. 5. 2. Carnap and Bar-Hillel (1952), Bar-Hillel (1952). The present book seems in part responsive to this program, having the same title as Bar-Hillel (1964). 3. See papers collected in Hintikka and Suppes (1970). 4. Dretske (1981), Israel and Perry (forthcoming). Peer commentary in Dretske (1983), especially that of Haber, did not accept Dretske&apos;s attempted analogies to the metrics of Shannon and Weaver. The notion of &amp;quot;information pickup&amp;quot; implies a preestablished harmony of the world and the mind, disregarding the well-known arbitrariness of language. 5. While Fodor (1986) does gives a cogent criticism of attempts to locate information &amp;quot;in the world&amp;quot;, the alternative &amp;quot;intentional&amp;quot; conception that he advocates relies on questionable assumptions of an &amp;quot;internal code&amp;quot; wherein such information is &amp;quot;encoded&amp;quot;. The problem, of course, lies in unpacking this metaphor. Falling into the custom of taking the computational metaphor of mind literally, he resuscitates our old familiar homunculus (in computational disguise as the &amp;quot;executive&amp;quot;) to provide a way out of the problem of node labels being of higher logical type than the nodes that they label. A simpler resolution follows from Harris&apos;s recognition that natural language has no separate metalanguage. See also Fodor (forthcoming). 6. See especially Harris (1982), and Harris, Gottfried, Ryckman, et al. (in press). 7. This thus cuts deeper than the naive rule-counting metrics for adjudication of grammars advocated not so long ago by generativists (see Ryckman 1986). 8. This work is reported in depth in Harris et al. (in press). These science languages occupy a place between natural language and mathematics, the chief difference from the former being that operator-argument likelihoods are much more strongly defined, amounting in most cases to simple binary selection rather than a graded scale. One of the many interesting aspects of this research is determining empirically the form of argumentation in science. The logical apparatus of deduction and other forms of inference are required only for various uses to which language may be put, rather than being the semantic basis for natural language, as has sometimes been claimed. 9. This is a refinement of the notion of distributional meaning developed in, e.g., Harris (1954). 10. The case of zero likelihood is covered by the word classes of the first constraint. 11. An example is the elision of one of a small set of operators arrive, show up, have high likelihood in I expect John momentarily. adverb moonly modify the elided arrive, etc., neither asserted to be momentary. The infinitive suffix the status of a modifier are the results of other reductions that are described in detail in Harris (1982). 12. For a computer implementation, see Johnson (1987). I am grateful to Tom Ryckman for helpful comments on an early draft of this review.</abstract>
<title confidence="0.593795">COMPUTATIONAL ANALYSIS OF ENGLISH: CORPUS-BASED APPROACH</title>
<author confidence="0.597592">Roger Garside</author>
<author confidence="0.597592">Geoffrey Leech</author>
<author confidence="0.597592">Geoffrey Sampson</author>
<affiliation confidence="0.824619">(University of Lancaster and University of Leeds)</affiliation>
<note confidence="0.865107">London: Longman, 1987, xii+ 196 pp. ISBN 0-582-29149-6; (Sb) Reviewed by</note>
<author confidence="0.999555">Michael Lesk</author>
<affiliation confidence="0.995202">Bell Communications Research</affiliation>
<abstract confidence="0.996498642857143">Why is it so remarkable to have a book whose analysis of language is entirely based on actual writing? Professors Garside, Leech, and Sampson have the refreshing view that the analysis of language ought to be based on real language, and have presented 12 papers resulting from their studies using the Lancaster-Oslo-Bergen corpus of a million words of British English. They present studies of spelling correction, part-of-speech assignment, parsing, and speech synthesis based on probability techniques derived from corpus studies. The methods here work on arbitrary texts and with reasonable efficiency. English includes a great variety of constructions that pose a dilemma for any strict grammar: to include everything and face great ambiguity, or to be extremely prescriptive and reject much. The authors solve this problem by using probabilities to balance both frequent and infrequent constructions, and to emphasize lowlevel simple algorithms over deep interpretation. For anyone trying to make practical use of text, this is extremely enlightening. English is not an inferior substitute for Prolog, and treating it as such is not only a mismatch, but also unnecessary for many tasks. The simple use of probabilities can perform many tasks that at first glance might be thought to require understanding. Methods for doing these are explained clearly in the book. The most detailed result described is the technique of tagging, or assigning parts of speech statistically. By using both the individual probabilities of different parts of speech for a single word, and the combined probability of sequences of two parts of speech, tagging can be done with 96-97% accuracy. This relatively simple algorithm, relying for performance on statistical data accumulated over a large sample of English rather than upon some kind of model of language, is typical of the results presented in this book. The algorithm runs on any input, from any subject area, and does a useful job without claiming to &amp;quot;understand&amp;quot; natural language. Just as we have learned that computers can play masterlevel chess by exhaustive evaluation of all possible moves, without any grand strategy or even plausible move selection, it seems that many linguistic tasks do not require understanding or modeling, but merely experience, translated into probability data. Linguistics, Volume 14, Number 4, December 1988 Book Reviews Semantic Interpretation and the Resolution of Ambiguity Similar discussions apply to parsing. Fifty thousand words of the corpus have been parsed by hand, and this has been used to make a table of the relative frequencies of different syntactic constructions. Assuming that the parse tree is the one of the most probable constituents (to greatly oversimplify in the interests of saving space), a program was written to parse with about 50% accuracy. Since the preparation of this book, continuing work by Eric Atwell and Geoffrey Sampson at Leeds has greatly improved on this figure, using a simulated annealing technique (see Sampson 1986). Other chapters of the book discuss the history of corpora in linguistic research, a defense of probabilistic methods, a discussion of speech synthesis and an outline of a sophisticated spelling corrector. Not much has been done on speech synthesis, partly becaiise we do not as yet have good data on the relation between syntax and prosody. The spelling corrector is aimed at errors of word selection, i.e., finding words that, although they appear in the dictionary, should not appear in the particular sentence being studied (e.g., &amp;quot;They kingdom come, thy will be done&amp;quot;). All these tools follow the same model: reliance on statistics from the corpus. It is a great relief to read a book like this, which is based on real texts rather than upon the imaginary language, sharing a few word forms with English, that is studied at MIT and some other research institutes (see Postal 1988). It is amazing that computers, which are distinguished for their ability to deal with vast quantities of bytes and their incompetence with even simple patterns and models, have been used in linguistics primarily for the implementation of complex logical models. This book is a start on the exploitation of large database methods for linguistic information. It is remarkable for the performance of its methods combined with their simplicity. Unlike many books on linguistics, it is easy to understand; it makes one think of the Moliere character who suddenly found out he had been speaking prose all his life. I heartily recommend this book to anyone who wishes to process language for a useful purpose. Other workers such as John Sinclair (1987) and Yaacov Choueka (1988) have also used large text databases for deriving linguistic information. When I was an undergraduate, one of my professors said that &amp;quot;mathematical intuition means having seen the problem before.&amp;quot; Similarly, there is no substitute in linguistics for knowing that a particular construction is likely because it has appeared many times. This book is a testimony to the superiority of experience over fantasy.</abstract>
<note confidence="0.335959">REFERENCES Choueka, Yaacov 1988 Looking for Needles in a Haystack. In of the RIAO 88, Paul 1988 Advances in Linguistic Rhetoric. In Lan-</note>
<title confidence="0.485164">and Linguistic Theory</title>
<author confidence="0.622267">Geoffrey Simulated Annealing as a Parsing Tech- Sampson</author>
<affiliation confidence="0.896301">In of Leeds Working Papers in Linguistics and</affiliation>
<address confidence="0.621511">John 1987 up. London, England; Glasgow, Scotland.</address>
<note confidence="0.42476875">Lesk division manager of computer science research at Bell Communications Research, 445 South St., Morristown, NJ 07960. He uses machine-readable dictionaries in his research on text handling and retrieval. E-mail:</note>
<email confidence="0.993928">lesk@wind.bellcore.com</email>
<title confidence="0.8079265">SEMANTIC INTERPRETATION AND THE RESOLUTION OF AMBIGUITY</title>
<author confidence="0.999362">Graeme Hirst</author>
<affiliation confidence="0.835776">(University of Toronto) Cambridge, England: Cambridge University Press,</affiliation>
<address confidence="0.673049">1987, xiv +263 pp.</address>
<note confidence="0.980797333333333">ISBN 0-521-32203-0; (hb) $39.50 [20% discount to ACL members] Reviewed by</note>
<author confidence="0.972156">Karen Sparck Jones</author>
<affiliation confidence="0.991259">University of Cambridge</affiliation>
<abstract confidence="0.99535796969697">Hirst&apos;s book presents an approach to natural language interpretation, using as his vehicle a description of the experimental system he built. It therefore has to be evaluated as a contribution on how to build NLP systems from both theoretical and practical points of view. It also has to be considered for teaching purposes, since Hirst has vamped up what was originally a thesis with some pedagogic exposition and test exercises, as well as a substantial and useful bibliography. Hirst is very clear about his aims and very honest about what he has tackled. He presents detail well and provides excellent summaries, so the essential properties of his work are well laid out. His goal was to build an interpretation system that could handle serious lexical and structural ambiguity, and handle it in a principled way. His concern is thus essentially computational; he does not make any claims for the psycholinguistic relevance of what he is doing, but he is, on the other hand, willing to exploit psycholinguistically derived support for good processing strategies. system consists of a syntactic parser, semantic interpreter, two disambiguation the Word subsystem for disambiguation and the Enquiry Desk for structural disambiguation. The system builds an explicit meaning representation in the frame language Frail. Hirst&apos;s design is motivated by two goals: to allow processes of different sorts to use different kinds of information but to interact to construct a sentence representation; and to do this in the theoretically wellfounded way exemplified by Montague&apos;s work by doing</abstract>
<intro confidence="0.670569">Computational Linguistics, Volume 14, Number 4, December 1988 91</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaacov Choueka</author>
</authors>
<title>Looking for Needles in a Haystack.</title>
<date>1988</date>
<booktitle>In Proceedings of the RIAO</booktitle>
<volume>88</volume>
<pages>609--623</pages>
<marker>Choueka, 1988</marker>
<rawString>Choueka, Yaacov 1988 Looking for Needles in a Haystack. In Proceedings of the RIAO 88, 609-623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Postal</author>
</authors>
<date>1988</date>
<booktitle>Advances in Linguistic Rhetoric. In Natural Language and Linguistic Theory</booktitle>
<pages>6--129</pages>
<contexts>
<context position="7395" citStr="Postal 1988" startWordPosition="1173" endWordPosition="1174">ot as yet have good data on the relation between syntax and prosody. The spelling corrector is aimed at errors of word selection, i.e., finding words that, although they appear in the dictionary, should not appear in the particular sentence being studied (e.g., &amp;quot;They kingdom come, thy will be done&amp;quot;). All these tools follow the same model: reliance on statistics from the corpus. It is a great relief to read a book like this, which is based on real texts rather than upon the imaginary language, sharing a few word forms with English, that is studied at MIT and some other research institutes (see Postal 1988). It is amazing that computers, which are distinguished for their ability to deal with vast quantities of bytes and their incompetence with even simple patterns and models, have been used in linguistics primarily for the implementation of complex logical models. This book is a start on the exploitation of large database methods for linguistic information. It is remarkable for the performance of its methods combined with their simplicity. Unlike many books on linguistics, it is easy to understand; it makes one think of the Moliere character who suddenly found out he had been speaking prose all </context>
</contexts>
<marker>Postal, 1988</marker>
<rawString>Postal, Paul 1988 Advances in Linguistic Rhetoric. In Natural Language and Linguistic Theory 6:129-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>Simulated Annealing as a Parsing Technique. In</title>
<date>1986</date>
<booktitle>University of Leeds Working Papers in Linguistics and Phonetics</booktitle>
<pages>4--43</pages>
<contexts>
<context position="6509" citStr="Sampson 1986" startWordPosition="1022" endWordPosition="1023">n of Ambiguity Similar discussions apply to parsing. Fifty thousand words of the corpus have been parsed by hand, and this has been used to make a table of the relative frequencies of different syntactic constructions. Assuming that the correct parse tree is the one ma-de of the most probable constituents (to greatly oversimplify in the interests of saving space), a program was written to parse with about 50% accuracy. Since the preparation of this book, continuing work by Eric Atwell and Geoffrey Sampson at Leeds has greatly improved on this figure, using a simulated annealing technique (see Sampson 1986). Other chapters of the book discuss the history of corpora in linguistic research, a defense of probabilistic methods, a discussion of speech synthesis and an outline of a sophisticated spelling corrector. Not much has been done on speech synthesis, partly becaiise we do not as yet have good data on the relation between syntax and prosody. The spelling corrector is aimed at errors of word selection, i.e., finding words that, although they appear in the dictionary, should not appear in the particular sentence being studied (e.g., &amp;quot;They kingdom come, thy will be done&amp;quot;). All these tools follow t</context>
</contexts>
<marker>Sampson, 1986</marker>
<rawString>Sampson, Geoffrey 1986 Simulated Annealing as a Parsing Technique. In University of Leeds Working Papers in Linguistics and Phonetics 4:43-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Looking up.</title>
<date>1987</date>
<location>Collins, London, England; Glasgow, Scotland.</location>
<marker>Sinclair, 1987</marker>
<rawString>Sinclair, John 1987 Looking up. Collins, London, England; Glasgow, Scotland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>