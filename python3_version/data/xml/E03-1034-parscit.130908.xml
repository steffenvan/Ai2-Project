<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007350">
<title confidence="0.998501">
Evaluating and Combining Approaches to
Selectional Preference Acquisition
</title>
<author confidence="0.997556">
Carsten Brockmann
</author>
<affiliation confidence="0.998794">
School of Informatics
The University of Edinburgh
</affiliation>
<address confidence="0.8352135">
2 Buccleuch Place
Edinburgh EH8 9LW, UK
</address>
<email confidence="0.997861">
Carsten.Brockmann@ed.ac.uk
</email>
<author confidence="0.965206">
MireIla Lapata
</author>
<affiliation confidence="0.9975925">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.9220705">
Regent Court, 211 Portobello Street
Sheffield Si 4DP, UK
</address>
<email confidence="0.999482">
mlap@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.994813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853294117647">
Previous work on the induction of se-
lectional preferences has been mainly
carried out for English and has concen-
trated almost exclusively on verbs and
their direct objects. In this paper, we
focus on class-based models of selec-
tional preferences for German verbs and
take into account not only direct ob-
jects, but also subjects and prepositional
complements. We evaluate model per-
formance against human judgments and
show that there is no single method that
overall performs best. We explore a va-
riety of parametrizations for our mod-
els and demonstrate that model combi-
nation enhances agreement with human
ratings.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993935483871">
Selectional preferences or constraints are the se-
mantic restrictions that a word imposes on the
environment in which it occurs. A verb like eat
typically takes animate entities as its subject and
edible entities as its object. Selectional prefer-
ences can most easily be observed in situations
where they are violated. For example, in the sen-
tence &amp;quot;The mountain eats sincerity.&amp;quot; both sub-
ject and object preferences for the verb eat are
violated. The problem of quantifying the degree
to which a given predicate (e.g., eat) semanti-
cally fits its arguments has received a lot of atten-
tion within computational linguistics. Several ap-
proaches have been developed for the induction of
selectional preferences, and almost all of them rely
on the availability of large machine-readable cor-
pora.
Probably the most primitive corpus-based
model of selectional preferences is co-occurrence
frequency. Inspection in a corpus of the types of
nouns eat admits as its objects will reveal that
food, meal, meat, or lunch are frequent com-
plements, whereas river, mountain, or moon are
rather unlikely. The obvious disadvantage of the
frequency-based approach is that no generaliza-
tions emerge with respect to the observed pref-
erences as it embodies no notion of semantic re-
latedness or proximity Ideally, one would like to
infer from the corpus that eat is semantically con-
gruent with food-related objects and incongruent
with natural objects. Another related limitation of
the frequency-based account is that it cannot make
any predictions for words that never occurred in
the corpus. A zero co-occurrence count might be
due to insufficient evidence or might reflect the
fact that a given word combination is inherently
implausible.
For the above reasons, most approaches
model the selectional preferences of predicates
(e.g., verbs, nouns, adjectives) by combining ob-
served frequencies with knowledge about the se-
mantic classes of their arguments. The classes can
be induced directly from the corpus (Pereira et al.,
1993; Brown et al., 1992; Lapata et al., 2001) or
taken from a manually crafted taxonomy (Resnik,
1993; Li and Abe, 1998; Clark and Weir, 2002;
Ciaramita and Johnson, 2000; Abney and Light,
1999). In the latter case the taxonomy is used
to provide a mapping from words to conceptual
classes, and in most cases WordNet (Miller et al.,
1990) is employed for this purpose.
Although most approaches agree on how se-
lectional preferences must be represented, i.e.,
as a mapping cv : (p,r,c) —&gt; a that maps each
predicate p and the semantic class c of its argu-
ment with respect to role r to a real number a
(Light and Greiff, 2002), there is little agreement
on how selectional preferences must be modeled
(e.g., whether to use a probability model or not)
and evaluated (e.g., whether to use a task-based
evaluation or not). Furthermore, previous work has
almost exclusively focused on verbal selectional
</bodyText>
<page confidence="0.996296">
27
</page>
<bodyText confidence="0.999950980392157">
preferences in English with the exception of La-
pata et al. (1999, 2001), who look at adjective-
noun combinations, again for English. Verbs tend
to impose stricter selectional preferences on their
arguments than adjectives or nouns and thus pro-
vide a natural test bed for models of selectional
preferences. However, research on verbal selec-
tional preferences has been relatively narrow in
scope as it has primarily focused on verbs and their
direct objects, ignoring the selectional preferences
pertaining to subjects and prepositional comple-
ments.
The induction of selectional preferences typ-
ically addresses two related problems: (a) find-
ing an appropriate class that best fits the predi-
cate in question and (b) coming up with a sta-
tistical model or a measure that estimates how
well a predicate fits its arguments. Resnik (1993)
defines selectional association, an information-
theoretic measure of semantic fit of a particular
semantic class c as an argument to a predicate p.
Li and Abe (1998) use the Minimum Description
Length (MDL) principle to select the the appro-
priate class c, Clark and Weir (2002) employ hy-
pothesis testing. Abney and Light (1999) propose
Hidden Markov Models as a way of deriving se-
lectional preferences over words, senses, or even
classes, whereas Ciaramita and Johnson (2000)
use Bayesian Belief Networks to quantify selec-
tional preferences.
Although there is no standard way to evalu-
ate different approaches to selectional preferences,
two types of evaluation are usually conducted:
task-based evaluation and comparisons against hu-
man judgments. Word sense disambiguation re-
sults are reported by Resnik (1997), Abney and
Light (1999), Ciaramita and Johnson (2000) and
Carroll and McCarthy (2000) (however, on a dif-
ferent data set). Among the first three approaches,
Ciaramita and Johnson (2000) obtain the best
results. Li and Abe (1998) evaluate their sys-
tem on the task of prepositional phrase attach-
ment, whereas Clark and Weir (2002) use pseudo-
disambiguation,&apos; a somewhat artificial task, and
show that their approach outperforms Li and Abe
(1998) and Resnik (1993).
Another way to evaluate a model&apos;s performance
is agreement with human ratings. This can be done
by selecting predicate-argument structures ran-
domly, using the model to predict the degree of se-
mantic fit and then looking at how well the ratings
</bodyText>
<footnote confidence="0.99426425">
1The task is to decide which of two verbs v1 and 1,2 is
more likely to take a noun n as its object. The method being
tested must reconstruct which of the unseen (vi, n) and (v2, n)
is a valid verb-object combination.
</footnote>
<bodyText confidence="0.99975659375">
correlate with the model&apos;s predictions (Resnik,
1993; Lapata et al., 1999; Lapata et al., 2001). This
approach seems more appropriate for languages
for which annotated corpora with word senses are
not available. It is more direct than disambigua-
tion which relies on the assumption that models
of selectional preferences have to infer the appro-
priate semantic class and therefore perform dis-
ambiguation as a side effect. It is also more nat-
ural than pseudo-disambiguation which relies on
artificially constructed data sets. Large-scale com-
parative studies have not, however, assessed the
strengths and weaknesses of the proposed meth-
ods as far as modeling human data is concerned.
In this paper, we undertake such a comparative
study by looking at selectional preferences of Ger-
man verbs. In contrast to previous work, we take
into account not only verbs and their direct ob-
jects, but also subjects and prepositional comple-
ments. We focus on three previously well-studied
models, Resnik&apos;s (1993) selectional association,
Li and Abe&apos;s (1998) MDL and Clark and Weir&apos;s
(2002) probability estimation method. For com-
parison, we also employ two models that do not in-
corporate any notion of semantic class, namely co-
occurrence frequency and conditional probability.
In the remainder of this paper, we briefly review
the models of selectional preferences we consider
(Section 2). Section 3 details our experiments,
evaluation methodology, and reports our results.
Section 4 offers some discussion and concluding
remarks.
</bodyText>
<sectionHeader confidence="0.752096" genericHeader="method">
2 Models of Selectional Preferences
</sectionHeader>
<bodyText confidence="0.999537769230769">
Co-occurrence Frequency. We can quantify the
semantic fit between a verb and its arguments by
simply counting f (v,r.n), the number of times a
noun n co-occurs with a verb v in a grammatical
relation r.
Conditional Probability. As we discuss below,
most class-based approaches to selectional pref-
erences rely on the estimation of the conditional
probability P(nlv, r), where n is represented by its
corresponding classes in the taxonomy. Here we
concentrate solely on the nouns as attested in the
corpus without making reference to a taxonomy
and estimate the following:
</bodyText>
<equation confidence="0.996911">
P(n v, r) = f (v, r.n)
f (v,&apos;r)
P(Idr,n) = f (v, r, n)
f (r,n)
</equation>
<page confidence="0.988695">
28
</page>
<bodyText confidence="0.99994625">
In (1) it is the verb that imposes the semantic pref-
erences on its arguments, whereas in (2) selec-
tional preferences are expressed in the other direc-
tion, i.e. arguments select for their predicates.
Selectional Association. Resnik (1993) was the
first to propose a measure of the the semantic fit
of a particular semantic class c as an argument to
a verb v. Selectional association (see (3) and (4))
represents the contribution of a particular seman-
tic class c to the total quantity of information pro-
vided by a verb about the semantic classes of its
argument, when measured as the relative entropy
between the prior distribution of classes P(c) and
the posterior distribution P (clv, r) of the argument
classes for a particular verb v. The latter distribu-
tion is estimated as shown in (5).
</bodyText>
<equation confidence="0.999311">
f (c)
r) = v,r, (5)
</equation>
<bodyText confidence="0.999934054054054">
The estimation of P(clv, r) would be a straight-
forward task if each word was always represented
in the taxonomy by a single concept or if we had
a corpus labeled explicitly with taxonomic infor-
mation. Lacking such a corpus we need to take
into consideration the fact that words in a tax-
onomy may belong to more than one conceptual
class. Counts of verb-argument configurations are
constructed for each conceptual class by dividing
the contribution of the argument by the number of
classes it belongs to (Resnik, 1993):
where syn(c) is the synset of concept c, i.e., the set
of synonymous words that can be used to denote
the concept (for example, syn((beve r age)) =
{beverage, drink, drinkable, potable}), and cn(n)
is the set of concepts that can be denoted by noun
n (more formally, cn(n) = {c n c syn(c)}).
Tree Cut Models. Li and Abe (1998) use MDL
to select from a hierarchy a set of classes that
represent the selectional preferences for a given
verb. These preferences are probabilities of the
form P(n r) where n is a noun represented by
a class in the taxonomy, v is a verb and r is an
argument slot. Li and Abe&apos;s algorithm operates
on thesaurus-like hierarchies where each leaf node
stands for a noun, each internal node stands for the
class of nouns below it, and a noun is uniquely rep-
resented by a leaf node. Li and Abe derive a sep-
arate model for each verb by partitioning the leaf
nodes (i.e., nouns) of the thesaurus tree and associ-
ating a probability with each class in the partition.
More formally, a tree cut model M is defined
as a pair of a tree cut F, which is a set of classes
ci , c2, , ck, and a parameter vector 0 specifying
a probability distribution over the members of F
with the constraint that the probabilities sum to
one.
</bodyText>
<equation confidence="0.999554">
EP(cilv,r) = 1
i=1
</equation>
<bodyText confidence="0.9826704">
To select the tree cut model that best tits the
data, Li and Abe (1998) employ the MDL prin-
ciple (Rissanen, 1978) by considering the cost in
bits of describing both the model itself and the ob-
served data (in our case verb-argument combina-
tions).
Given a data sample S encoded by a tree cut
model /12/ = (F, 6) with tree cut F and estimated
parameters 6, the total description length in bits
L(M, S) is given by equation (8):
</bodyText>
<equation confidence="0.729161666666667">
L(M,S) = log1G log IS (8)
— E logPia(nlv, r)
nes
</equation>
<bodyText confidence="0.999973333333333">
where IQ is the cardinality of the set of all pos-
sible tree cuts, k is the number of classes on the
cut F, 1,51 is the sample size, and Pn,4- (n r) is the
probability of a noun, which is estimated by dis-
tributing the probability of a given class equally
among the nouns that can be denoted by it:
</bodyText>
<equation confidence="0.998071">
Pia(clv,r) (9)
Vn syn(c) : Pft(n1 = Isyn(c)
</equation>
<bodyText confidence="0.998348272727273">
Class-based Probability. Clark and Weir
(2002) are, strictly speaking, not concerned
with the induction of selectional preferences
but with the problem of estimating conditional
probabilities of the form shown in (1) in the
face of sparse data. However, their probability
estimation method can be naturally applied to
the selectional preference acquisition problem
as it is suited not only for the estimation of the
appropriate probabilities but also for finding a
suitable class for the predicates of interest. Clark
</bodyText>
<equation confidence="0.996313333333333">
A (v, r,c) =P(clv,r) log P p(c(lcv)&apos;r)
Ti
P(clv,r)
77 =EP(clv, r) log p(c)
nEsyn(0 cn(n)
E f (v, r, n
f(v,r,c) = )
(6)
(7)
</equation>
<page confidence="0.973113">
29
</page>
<bodyText confidence="0.9518835">
and Weir obtain the probability P( v
P(c v, r) using Bayes&apos; theorem:
</bodyText>
<equation confidence="0.9980955">
v, = P (v1c , r)P(clr)
P(v1r)
</equation>
<bodyText confidence="0.999022">
They suggest the following way for finding a
set of concepts c&apos; (where c&apos; denotes the set of con-
cepts dominated by c&apos;, including c&apos; itself) as a gen-
eralization for concept c (where c can be either n
or one of its hypernyms): Initially, c&apos; is set to c,
then c&apos; is set to successive hypernyms of c until a
node in the hierarchy is reached where P(c&apos; v, r)
changes significantly. This is determined by com-
paring estimates of P(c v, r) for each child c of
c&apos; using hypothesis testing. The null hypothesis
is that the probabilities p(v c, r) are the same for
each child c of c&apos;. If there is a significant differ-
ence between them, the null hypothesis is rejected
and classes that are lower in the hierarchy than c&apos;
are used. Selecting the right level of generaliza-
tion crucially depends on the type of statistic used
(in their experiments Clark and Weir use the Pear-
son chi-square statistic X2 and the log-likelihood
chi-square statistic G2). The appropriate level of
significance a can be tuned experimentally.
Once a suitable class is found, the similarity-
class probability P is estimated:
</bodyText>
<table confidence="0.865530333333333">
Psc (C v, r) = P(vIr)
, (11)
L„,ec 13(v1 [v,r,c1,01-11cyl rrj
</table>
<bodyText confidence="0.99993425">
where [v, r, c] denotes the class chosen for concept
c in relation r to verb v, P denotes a relative fre-
quency estimate, and C the set of concepts in the
hierarchy. The denominator is a normalization fac-
tor. Again, since we are not dealing with word
sense disambiguated data, counts for each noun
are distributed evenly among all senses of the noun
(see (5)).
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998324">
3.1 Parameter Settings
</subsectionHeader>
<bodyText confidence="0.999971016129032">
In our experiments, we compared the performance
of the five methods discussed above against hu-
man judgments. Before discussing the details of
our evaluation we present our general experimen-
tal setup (e.g., the corpora and hierarchy used) and
the different types of parameters we explored.
All our experiments were conducted on data ob-
tained from the German Siiddeutsche Zeitung (SZ)
corpus, a 179 million word collection of newspa-
per texts. The corpus was parsed using the gram-
matical relation recognition component of SMES, a
robust information extraction core system for the
processing of German text (Neumann et al., 1997).
SMES incorporates a tokenizer that maps the text
into a stream of tokens. The tokens are then an-
alyzed morphologically (compound recognition,
assignment of part-of-speech tags), and a chunk
parser identifies phrases and clauses by means of
finite state grammars. The grammatical relations
recognizer operates on the output of the parser
while exploiting a large subcategorization lexicon.
Although SMES recognizes a variety of grammati-
cal relations, in our experiments we focused solely
on relations of the form (v,r,n) where r can be a
subject, direct object, or prepositional object (see
the examples in Table 2).
For the class-based models, the hierarchy avail-
able in GermaNet (Hamp and Feldweg, 1997)
was used. The experiments reported in this pa-
per make use of the noun taxonomy of Ger-
maNet (version 3.0, 23,053 noun synsets), and
the information encoded in it in terms of the hy-
ponymy/hypernymy relation.
Certain modifications to the original GermaNet
hierarchy were necessary for the implementation
of Li and Abe&apos;s method (1998). The GermaNet
noun hierarchy is a directed acyclic graph (DAG)
whereas their algorithm operates on trees. A solu-
tion to this problem is given by Li and Abe, who
transform the DAG into a tree by copying each
subgraph having multiple parents. An additional
modification is needed since in GermaNet, nouns
do not only occur as leaves of the hierarchy, but
also at internal nodes. Following Wagner (2000)
and McCarthy (2001), we created a new leaf for
each internal node, containing a copy of the inter-
nal node&apos;s nouns. This guarantees that all nouns
are present at the leaf level.
Finally, the algorithm requires that the em-
ployed hierarchy has a single root node. In Word-
Net and GermaNet, nouns are not contained in a
single hierarchy; instead they are partitioned ac-
cording to a set of semantic primitives which are
treated as the unique beginners of separate hi-
erarchies. This means that an artificial concept
(root) has to be created and connected to the
existing top-level classes. Although WordNet has
only nine classes without a hypernym, GermaNet
contains 502. Of these, 125 have one or more
daughters.
The number of classes below (root) has an im-
mediate effect on the tree cut model: With a large
</bodyText>
<equation confidence="0.645024666666667">
P(c
c. r) from
(10)
</equation>
<page confidence="0.815328">
30
</page>
<table confidence="0.5441465">
SelA TCM SimC
highest mean
highest mean G2 x2 G2 x2
highest, 33 c.b.r., 40 c.b.r., a = .0005, a = .05,
mean 49 c.b.r., 125 c.b.r. a = .3, a = .75, a = .995
c.b.r.: classes below (root)
</table>
<tableCaption confidence="0.998675">
Table 1: Explored parameter settings
</tableCaption>
<bodyText confidence="0.999862740740741">
number of classes, many of the cuts returned by
MDL are over-generalizing at the (root) level.
We therefore varied the the number of classes be-
low (root) in order to observe how this affects
the generalization outcome. We excluded from the
hierarchy classes with less than or equal to 10, 20,
and 30 hyponyms. This resulted in 49, 40, and 33
classes below (r o ot ). We also experimented with
the full 125 classes (see Table 1).
All of the class-based methods produce a value
for each class c to which an argument noun n be-
longs. Since n can be ambiguous and its appropri-
ate sense is not known, a unique class is typically
chosen by simply selecting the class which max-
imizes the quantity of interest (see (3), (9), and
(11)). An alternative is to consider the mean value
over all classes. In our experiments, we compare
the effect of these distinct selection procedures.
Finally, for Clark and Weir&apos;s (2002) approach,
two parameters are important for finding an appro-
priate generalization class: (a) the statistic for per-
forming significance testing and (b) the a value
for determining the significance level. Here, we
experimented with the X2 and G2 statistics and ran
our experiments for the following different a val-
ues: .0005, .05, .3, .75, and .995. The parameter
settings we explored are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.998571">
3.2 Eliciting Judgments on Selectional
Preferences
</subsectionHeader>
<bodyText confidence="0.999965088235295">
In order to evaluate the methods introduced in Sec-
tion 2, we first established an independent measure
of how well a verb fits its arguments by eliciting
judgments from human subjects (Resnik, 1993;
Lapata et al., 2001; Lapata et al., 1999). In this sec-
tion, we describe our method for assembling the
set of experimental materials and collecting plau-
sibility ratings for these stimuli.
Materials and Design. As mentioned earlier,
co-occurrence triples of the form (v, r, n) were ex-
tracted from the output of SMES. In order to reduce
the risk of ratings being influenced by verb/noun
combinations unfamiliar to the participants, we re-
moved triples that had a verb or a noun with fre-
quency less than one per million Ten verbs were
selected randomly for each grammatical relation.
For each verb we divided the set of triples into
three bands (High, Medium, and Low), based on
an equal division of the range of log-transformed
co-occurrence frequency, and randomly chose one
noun from each band. The division ensured that
the experimental stimuli represented likely and un-
likely verb-argument combinations and enabled us
to investigate how the different models perform
with low/high counts. Example stimuli are shown
in Table 2.
Our experimental design consisted of the factors
grammatical relation (Rel), verb (Verb), and prob-
ability band (Band). The factors Rel and Band had
three levels each, and the factor Verb had 10 lev-
els. This yielded a total of Rel x Verb x Band = 3 x
10 x 3 = 90 stimuli. The 90 verb/noun pairs were
paraphrased to create sentences. For the direct/PP-
object sentences, one of 10 common human first
names (five female, five male) was added as sub-
ject where possible, or else an inanimate subject
which appeared frequently in the corpus was cho-
sen.
Procedure. The experimental paradigm was
Magnitude Estimation (ME), a technique stan-
dardly used in psychophysics to measure judg-
ments of sensory stimuli (Stevens, 1975), which
Bard et al. (1996) and Cowart (1997) have applied
to the elicitation of linguistic judgments. ME has
been shown to provide fine-grained measurements
of linguistic acceptability which are robust enough
to yield statistically significant results, while being
highly replicable both within and across speakers.
ME requires subjects to assign numbers to a se-
ries of linguistic stimuli in a proportional fashion.
Subjects are first exposed to a modulus item, to
which they assign an arbitrary number. All other
stimuli are rated proportionally to the modulus. In
this way, each subject can establish their own rat-
ing scale.
In the present experiment, the subjects were
instructed to judge how acceptable the 90 sen-
tences were in proportion to a modulus sentence.
The experiment was conducted remotely over the
Internet using WebExp 2.1 (Keller et al., 1998),
an interactive software package for administer-
ing web-based psychological experiments. Sub-
jects first saw a set of instructions that explained
the ME technique and included some examples,
and had to fill in a short questionnaire including
basic demographic information. Each subject saw
90 experimental stimuli. A random stimulus order
was generated for each subject.
</bodyText>
<page confidence="0.999521">
31
</page>
<table confidence="0.9995605">
Relation Verb Co-occurrence Frequency Band
High Medium Low
SUBJ stagnieren Umsatz 1.77 Preis .85 Arbeitslosigkeit .48
stagnate turnover price unemployment
OBJ erlegen Tier .60 Jahr .30 Gesetz 0
shoot ani mal year law
PP-OBJ denken an Riicktritt 1.54 Freund .78 Kleinigkeit 0
think of resignation friend detail
</table>
<tableCaption confidence="0.97807">
Table 2: Example stimuli (with log co-occurrence frequencies in the SZ corpus)
</tableCaption>
<table confidence="0.9916105">
Rating ISAgr Freq CondP SelA TCM SimC
SUBJ .790 .386* .010 .408* .281 .268
[highest] [mean, 40 c.b.r.] [mean, G2, a = .75]
OBJ .810 .360 .399* .430* .251 .611***
[mean] [mean, 40 c.b.r.] [highest, G2, a = .05]
PP-OBJ .820 .168 .335 .330 .319 .597***
[mean] [mean, 33 c.b.r.] [highest, G2, a = .3]
overall .810 .301** .374*** .374*** .341*&amp;quot; .232*
[highest] [mean, 40 c.b.r.] [highest, G2, a = .3]
* p &lt; .05 *** p &lt; .001 c.b.r.: classes below (root)
</table>
<tableCaption confidence="0.999687">
Table 3: Best correlations between human ratings and selectional preference models
</tableCaption>
<bodyText confidence="0.98379675">
Subjects. The experiment was completed by
61 volunteers, all self-reported native speakers of
German. Subjects were recruited via postings to
Usenet newsgroups.
</bodyText>
<subsectionHeader confidence="0.823794">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999924074074074">
The data were first normalized by dividing each
numerical judgment by the modulus value that the
subject had assigned to the reference sentence.
This operation creates a common scale for all
subjects. Then the data were transformed by tak-
ing the decadic logarithm. This transformation en-
sures that the judgments are normally distributed
and is standard practice for magnitude estimation
data (Bard et al., 1996). All analyses were con-
ducted on the normalized, log-transformed judg-
ments.
Using correlation analysis we explored the lin-
ear relationship between the human judgments and
the methods discussed in Section 2. As shown in
Table 1 there are 30 distinct parameter instantia-
tions for the class-based models. There are no pa-
rameters for co-occurrence frequency and condi-
tional probability. Table 3 lists the best correlation
coefficients per method, indicating the respective
parameters where appropriate. For each grammat-
ical relation, the optimal coefficient is emphasized.
In Table 3, we also show how well humans
agree in their judgments (inter-subject agreement,
ISAgr) and thus provide an upper bound for
the task which allows us to interpret how well
the models are doing in relation to humans. We
performed correlations on the elicited judgments
using leave-one-out resampling (Weiss and Ku-
likowski, 1991). We divided the set of the sub-
jects&apos; responses with size m into a set of size m — 1
(i.e., the response data of all but one subject) and
a set of size one (i.e., the response data of a sin-
gle subject). We then correlated the mean rating
of the former set with the rating of the latter. This
was repeated m times and the average agreement
is reported in Table 3.
As shown in Table 3, all five models are sig-
nificantly correlated with the human ratings, al-
though the correlation coefficients are not as high
as the inter-subject agreement (ISAgr). Selec-
tional association (SelA) and conditional probabil-
ity (CondP) reveal the highest overall correlations.
CondP as expressed in (2) outperformed (1) which
was excluded from further comparisons. As far as
the individual argument relations are concerned,
the similarity-class probability (SimC) performs
best at modeling the selectional preferences for
prepositional and direct objects. Clark and Weir&apos;s
(2002) pseudo-disambiguation experiments also
show that their method outperforms tree cut mod-
els (TCM) and SelA at modeling the semantic fit
between verbs and their direct objects. Our results
additionally generalize to PP-objects. SelA is the
best predictor for subject-related selectional pref-
</bodyText>
<page confidence="0.996178">
32
</page>
<table confidence="0.9999128">
Factor Eigenvalue Variance Cumulative
SimC 7.969 53.1% 53.1%
TCM 3.251 21.7% 74.8%
SelA 1.185 7.9% 82.7%
CondP 0.853 5.7% 88.4%
</table>
<tableCaption confidence="0.999515">
Table 4: Principal component factors
</tableCaption>
<bodyText confidence="0.998785375">
erences, whereas co-occurrence frequency (Freq)
is the second best.
With respect to the class selection method, bet-
ter results are obtained when the highest class is
chosen. This is true for SelA and SimC but not for
TCM where the mean generally yields better per-
formance. Recall from Section 3.1 that for TCM
the number of classes below (root) was varied
from 125 to 33. As can be seen from Table 3, bet-
ter results are obtained with 40 and 33 classes,
i.e., with a relatively small number of classes be-
low (root). Finally, in agreement with Clark and
Weir, for SimC the best results were obtained with
the G2 statistic. Also note that different a values
seem to be appropriate for different argument re-
lations.
</bodyText>
<subsectionHeader confidence="0.9714">
3.4 Model Combination
</subsectionHeader>
<bodyText confidence="0.987654717391305">
An obvious question is whether a better fit with
the experimental data can be obtained via model
combination. As discussed earlier different mod-
els seem to provide complementary information
when it comes to modeling different argument re-
lations. A straightforward way to combine our dif-
ferent models is multiple linear regression. Recall
that we have 30 variants of class-based models
(only the best performing ones are shown in Ta-
ble 3), some of which are expectedly highly corre-
lated. After removing models with high intercor-
relation (r &gt; .99, 15 out of 30), principal compo-
nents factor analysis (PCFA) was performed on all
90 items, keeping the factors that explained more
than 5% of the variance (see Table 4).
Multiple regression on all 90 observations
with all four factors and forward selection (with
p &gt; .05 for removal from the model) yielded
the regression equation in (12). The corresponding
correlation coefficient is .47 (p &lt; .001).
Rating = .091 CondP ± .068 TCM
+.103 SelA ± .052
Equation (12) was derived from the entire data
set (i.e., 90 verb-argument combinations). Ideally,
one would need to conduct another experiment
with a new set of materials in order to determine
whether (12) generalizes to unseen data. In default
of a second experiment which we plan for the fu-
ture, we investigated how well model combination
performs on unseen data by using 10-fold cross-
validation.
Our data set was split into 10 disjoint subsets
each containing 9 items. We repeated the PCFA
procedure and the multiple regression analysis 10
times, each time using 81 items as training data
and the remaining 9 as test data. Then we per-
formed a correlation analysis between the pre-
dicted values for the unseen items of each fold and
the human ratings. Effectively, this analysis treats
the whole data set as unseen. However notice that
for each test/train set split we obtain different re-
gression equations since the PCFA yields differ-
ent factors for different data sets. Comparison be-
tween the estimated values and the human ratings
yielded a correlation coefficient of .40 (p &lt; .001)
outperforming any single model.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999972878787879">
In this paper, we evaluated five models for the ac-
quisition of selectional preferences. We focused
on German verbs and their subjects, direct objects,
and PP-objects. We placed emphasis on class-
based models of selectional preferences, explored
their parameter space, and showed that the exist-
ing models, developed primarily for English, also
generalize to German. We proposed to evaluate the
different models against human ratings and argued
that such an evaluation methodology allows us to
assess the feasibility of the task and to compute
performance upper bounds.
Our results indicate that there is no method
which overall performs best; it seems that differ-
ent methods are suited for different argument re-
lations (i.e., SimC for objects, SelA for subjects).
The more sophisticated class-based approaches do
not always yield better results when compared to
simple frequency-based models. This is in agree-
ment with Lapata et al. (1999) who found that co-
occurrence frequency is the best predictor of the
plausibility of adjective-noun pairs. Model com-
bination seems promising in that a better fit with
experimental data is obtained. However, note that
none of our models (including the ones obtained
via multiple regression) seem to attain results rea-
sonably close to the upper bound.
In the future, we plan to consider web-based
frequencies for our probability estimates (Keller
et al., 2002) as well as Abney and Light&apos;s
(1999) Hidden Markov Models and Ciaramita
and Johnson&apos;s (2000) Bayesian Belief Networks.
We will also expand our evaluation methodol-
</bodyText>
<equation confidence="0.420584">
(12)
</equation>
<page confidence="0.993932">
33
</page>
<bodyText confidence="0.896049">
ogy to adjective-noun and noun-noun combina-
tions and conduct further rating experiments to
cross-validate our combined models.
</bodyText>
<sectionHeader confidence="0.990937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999697952830188">
Steve Abney and Marc Light. 1999. Hiding a semantic
class hierarchy in a Markov model. In Proceedings
of the ACL Workshop on Unsupervised Learning in
Natural Language Processing, pages 1-8, College
Park, MD.
Ellen Gurman Bard, Dan Robertson, and Antonella So-
race. 1996. Magnitude estimation of linguistic ac-
ceptability. Language, 72(1):32-68.
Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational
Linguistics, 18(4):467-479.
John Carroll and Diana McCarthy. 2000. Word sense
disambiguation using automatically acquired verbal
preferences. Computers and the Humanities, 34(1-
2):109-114.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
restrictions with Bayesian networks. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 187-193, Saarbriicken,
Germany.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2): 187-206.
Wayne Cowart. 1997. Experimental Syntax: Apply-
ing Objective Methods to Sentence Judgments. Sage
Publications, Thousand Oaks, CA.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a lexical-semantic net for German. In Proceedings
of the Workshop on Automatic Information Extrac-
tion and Building of Lexical Semantic Resources
for NLP Applications at the 35th ACL and the 8th
EACL, pages 9-15, Madrid, Spain.
Frank Keller, Martin Corley, Steffan Corley, Lars
Konieczny, and Amalia Todirascu. 1998. Web-
Exp: A Java toolbox for web-based psychological
experiments. Technical Report HCRC/TR-99, Hu-
man Communication Research Centre, University of
Edinburgh, UK.
Frank Keller, Maria Lapata, and Olga Ourioupina.
2002. Using the web to overcome data sparse-
ness. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
230-237, Philadelphia, PA.
Maria Lapata, Scott McDonald, and Frank Keller.
1999. Determinants of adjective-noun plausibility.
In Proceedings of the 9th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 30-36, Bergen, Norway.
Maria Lapata, Frank Keller, and Scott McDonald.
2001. Evaluating smoothing algorithms against
plausibility judgments. In Proceedings of the
39th Annual Meeting of the Association for Com-
putational Linguistics, pages 346-353, Toulouse,
France.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217-244.
Marc Light and Warren Greiff. 2002. Statistical mod-
els for the induction and use of selectional prefer-
ences. Cognitive Science, 87:1-13.
Diana McCarthy. 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex, UK.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: An on-line lexi-
cal database. International Journal of Lexicogra-
phy, 3(4):235-244.
Giinter Neumann, Rolf Backofen, Judith Baur, Markus
Becker, and Christian Braun. 1997. An informa-
tion extraction core system for real world German
text processing. In Proceedings of the 5th ACL Con-
ference on Applied Natural Language Processing,
pages 209-216, Washington, DC.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 183-
190, Columbus, OH.
Philip Stuart Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA.
Philip Resnik. 1997. Selectional preferences and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?, pages 52-57, Washington,
DC.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14:465-471.
S. S. Stevens. 1975. Psychophysics: Introduction to
Its Perceptual, Neural, and Social Prospects. John
Wiley &amp; Sons, New York, NY.
Andreas Wagner. 2000. Enriching a lexical semantic
net with selectional preferences by means of statisti-
cal corpus analysis. In Proceedings of the 1st Work-
shop on Ontology Learning at the 14th ECM, pages
37-42, Berlin, Germany.
Sholom M. Weiss and Casimir A Kulikowski. 1991.
Computer Systems that Learn: Classification and
Prediction Methods from Statistics, Neural Nets,
Machine Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
</reference>
<page confidence="0.999322">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.460684">
<title confidence="0.99946">Evaluating and Combining Approaches to Selectional Preference Acquisition</title>
<author confidence="0.999932">Carsten Brockmann</author>
<affiliation confidence="0.938813333333333">School of Informatics The University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.994092">Edinburgh EH8 9LW, UK</address>
<email confidence="0.663558">Carsten.Brockmann@ed.ac.uk</email>
<author confidence="0.98447">MireIla Lapata</author>
<affiliation confidence="0.999886">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.96856">Regent Court, 211 Portobello Street Sheffield Si 4DP, UK</address>
<email confidence="0.997892">mlap@dcs.shef.ac.uk</email>
<abstract confidence="0.995774166666667">Previous work on the induction of selectional preferences has been mainly carried out for English and has concentrated almost exclusively on verbs and their direct objects. In this paper, we focus on class-based models of selectional preferences for German verbs and take into account not only direct objects, but also subjects and prepositional complements. We evaluate model performance against human judgments and show that there is no single method that overall performs best. We explore a variety of parametrizations for our models and demonstrate that model combination enhances agreement with human ratings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steve Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a semantic class hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>College Park, MD.</location>
<contexts>
<context position="3192" citStr="Abney and Light, 1999" startWordPosition="492" endWordPosition="495"> co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-bas</context>
<context position="5054" citStr="Abney and Light (1999)" startWordPosition="800" endWordPosition="803">complements. The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class that best fits the predicate in question and (b) coming up with a statistical model or a measure that estimates how well a predicate fits its arguments. Resnik (1993) defines selectional association, an informationtheoretic measure of semantic fit of a particular semantic class c as an argument to a predicate p. Li and Abe (1998) use the Minimum Description Length (MDL) principle to select the the appropriate class c, Clark and Weir (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambiguation results are reported by Resnik (1997), Abney and Light (1999), Ciaramita and Johnson (2000) and Carroll and McCarthy (2000) (however, on a different data </context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Steve Abney and Marc Light. 1999. Hiding a semantic class hierarchy in a Markov model. In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing, pages 1-8, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Gurman Bard</author>
<author>Dan Robertson</author>
<author>Antonella Sorace</author>
</authors>
<title>Magnitude estimation of linguistic acceptability.</title>
<date>1996</date>
<journal>Language,</journal>
<pages>72--1</pages>
<contexts>
<context position="20772" citStr="Bard et al. (1996)" startWordPosition="3495" endWordPosition="3498">e factors Rel and Band had three levels each, and the factor Verb had 10 levels. This yielded a total of Rel x Verb x Band = 3 x 10 x 3 = 90 stimuli. The 90 verb/noun pairs were paraphrased to create sentences. For the direct/PPobject sentences, one of 10 common human first names (five female, five male) was added as subject where possible, or else an inanimate subject which appeared frequently in the corpus was chosen. Procedure. The experimental paradigm was Magnitude Estimation (ME), a technique standardly used in psychophysics to measure judgments of sensory stimuli (Stevens, 1975), which Bard et al. (1996) and Cowart (1997) have applied to the elicitation of linguistic judgments. ME has been shown to provide fine-grained measurements of linguistic acceptability which are robust enough to yield statistically significant results, while being highly replicable both within and across speakers. ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to which they assign an arbitrary number. All other stimuli are rated proportionally to the modulus. In this way, each subject can establish their own rating scale.</context>
<context position="23483" citStr="Bard et al., 1996" startWordPosition="3923" endWordPosition="3926">and selectional preference models Subjects. The experiment was completed by 61 volunteers, all self-reported native speakers of German. Subjects were recruited via postings to Usenet newsgroups. 3.3 Results The data were first normalized by dividing each numerical judgment by the modulus value that the subject had assigned to the reference sentence. This operation creates a common scale for all subjects. Then the data were transformed by taking the decadic logarithm. This transformation ensures that the judgments are normally distributed and is standard practice for magnitude estimation data (Bard et al., 1996). All analyses were conducted on the normalized, log-transformed judgments. Using correlation analysis we explored the linear relationship between the human judgments and the methods discussed in Section 2. As shown in Table 1 there are 30 distinct parameter instantiations for the class-based models. There are no parameters for co-occurrence frequency and conditional probability. Table 3 lists the best correlation coefficients per method, indicating the respective parameters where appropriate. For each grammatical relation, the optimal coefficient is emphasized. In Table 3, we also show how we</context>
</contexts>
<marker>Bard, Robertson, Sorace, 1996</marker>
<rawString>Ellen Gurman Bard, Dan Robertson, and Antonella Sorace. 1996. Magnitude estimation of linguistic acceptability. Language, 72(1):32-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>Brown, Pietra, de Souza, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="5623" citStr="Carroll and McCarthy (2000)" startWordPosition="884" endWordPosition="887">r (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambiguation results are reported by Resnik (1997), Abney and Light (1999), Ciaramita and Johnson (2000) and Carroll and McCarthy (2000) (however, on a different data set). Among the first three approaches, Ciaramita and Johnson (2000) obtain the best results. Li and Abe (1998) evaluate their system on the task of prepositional phrase attachment, whereas Clark and Weir (2002) use pseudodisambiguation,&apos; a somewhat artificial task, and show that their approach outperforms Li and Abe (1998) and Resnik (1993). Another way to evaluate a model&apos;s performance is agreement with human ratings. This can be done by selecting predicate-argument structures randomly, using the model to predict the degree of semantic fit and then looking at h</context>
</contexts>
<marker>Carroll, McCarthy, 2000</marker>
<rawString>John Carroll and Diana McCarthy. 2000. Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities, 34(1-2):109-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional restrictions with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>187--193</pages>
<location>Saarbriicken, Germany.</location>
<contexts>
<context position="3168" citStr="Ciaramita and Johnson, 2000" startWordPosition="488" endWordPosition="491">ccurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., w</context>
<context position="5202" citStr="Ciaramita and Johnson (2000)" startWordPosition="823" endWordPosition="826">s the predicate in question and (b) coming up with a statistical model or a measure that estimates how well a predicate fits its arguments. Resnik (1993) defines selectional association, an informationtheoretic measure of semantic fit of a particular semantic class c as an argument to a predicate p. Li and Abe (1998) use the Minimum Description Length (MDL) principle to select the the appropriate class c, Clark and Weir (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambiguation results are reported by Resnik (1997), Abney and Light (1999), Ciaramita and Johnson (2000) and Carroll and McCarthy (2000) (however, on a different data set). Among the first three approaches, Ciaramita and Johnson (2000) obtain the best results. Li and Abe (1998) evaluate their system on the task of</context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2000. Explaining away ambiguity: Learning verb selectional restrictions with Bayesian networks. In Proceedings of the 18th International Conference on Computational Linguistics, pages 187-193, Saarbriicken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>187--206</pages>
<contexts>
<context position="3139" citStr="Clark and Weir, 2002" startWordPosition="484" endWordPosition="487">for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model o</context>
<context position="5004" citStr="Clark and Weir (2002)" startWordPosition="792" endWordPosition="795">erences pertaining to subjects and prepositional complements. The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class that best fits the predicate in question and (b) coming up with a statistical model or a measure that estimates how well a predicate fits its arguments. Resnik (1993) defines selectional association, an informationtheoretic measure of semantic fit of a particular semantic class c as an argument to a predicate p. Li and Abe (1998) use the Minimum Description Length (MDL) principle to select the the appropriate class c, Clark and Weir (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambiguation results are reported by Resnik (1997), Abney and Light (1999), Ciaramita and Johnson (2000) and Carroll </context>
<context position="12092" citStr="Clark and Weir (2002)" startWordPosition="2014" endWordPosition="2017">ons). Given a data sample S encoded by a tree cut model /12/ = (F, 6) with tree cut F and estimated parameters 6, the total description length in bits L(M, S) is given by equation (8): L(M,S) = log1G log IS (8) — E logPia(nlv, r) nes where IQ is the cardinality of the set of all possible tree cuts, k is the number of classes on the cut F, 1,51 is the sample size, and Pn,4- (n r) is the probability of a noun, which is estimated by distributing the probability of a given class equally among the nouns that can be denoted by it: Pia(clv,r) (9) Vn syn(c) : Pft(n1 = Isyn(c) Class-based Probability. Clark and Weir (2002) are, strictly speaking, not concerned with the induction of selectional preferences but with the problem of estimating conditional probabilities of the form shown in (1) in the face of sparse data. However, their probability estimation method can be naturally applied to the selectional preference acquisition problem as it is suited not only for the estimation of the appropriate probabilities but also for finding a suitable class for the predicates of interest. Clark A (v, r,c) =P(clv,r) log P p(c(lcv)&apos;r) Ti P(clv,r) 77 =EP(clv, r) log p(c) nEsyn(0 cn(n) E f (v, r, n f(v,r,c) = ) (6) (7) 29 an</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2): 187-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Cowart</author>
</authors>
<title>Experimental Syntax: Applying Objective Methods to Sentence Judgments. Sage Publications,</title>
<date>1997</date>
<location>Thousand Oaks, CA.</location>
<contexts>
<context position="20790" citStr="Cowart (1997)" startWordPosition="3500" endWordPosition="3501">had three levels each, and the factor Verb had 10 levels. This yielded a total of Rel x Verb x Band = 3 x 10 x 3 = 90 stimuli. The 90 verb/noun pairs were paraphrased to create sentences. For the direct/PPobject sentences, one of 10 common human first names (five female, five male) was added as subject where possible, or else an inanimate subject which appeared frequently in the corpus was chosen. Procedure. The experimental paradigm was Magnitude Estimation (ME), a technique standardly used in psychophysics to measure judgments of sensory stimuli (Stevens, 1975), which Bard et al. (1996) and Cowart (1997) have applied to the elicitation of linguistic judgments. ME has been shown to provide fine-grained measurements of linguistic acceptability which are robust enough to yield statistically significant results, while being highly replicable both within and across speakers. ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to which they assign an arbitrary number. All other stimuli are rated proportionally to the modulus. In this way, each subject can establish their own rating scale. In the present ex</context>
</contexts>
<marker>Cowart, 1997</marker>
<rawString>Wayne Cowart. 1997. Experimental Syntax: Applying Objective Methods to Sentence Judgments. Sage Publications, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet -a lexical-semantic net for German.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications at the 35th ACL and the 8th EACL,</booktitle>
<pages>9--15</pages>
<location>Madrid,</location>
<contexts>
<context position="15665" citStr="Hamp and Feldweg, 1997" startWordPosition="2619" endWordPosition="2622">ens are then analyzed morphologically (compound recognition, assignment of part-of-speech tags), and a chunk parser identifies phrases and clauses by means of finite state grammars. The grammatical relations recognizer operates on the output of the parser while exploiting a large subcategorization lexicon. Although SMES recognizes a variety of grammatical relations, in our experiments we focused solely on relations of the form (v,r,n) where r can be a subject, direct object, or prepositional object (see the examples in Table 2). For the class-based models, the hierarchy available in GermaNet (Hamp and Feldweg, 1997) was used. The experiments reported in this paper make use of the noun taxonomy of GermaNet (version 3.0, 23,053 noun synsets), and the information encoded in it in terms of the hyponymy/hypernymy relation. Certain modifications to the original GermaNet hierarchy were necessary for the implementation of Li and Abe&apos;s method (1998). The GermaNet noun hierarchy is a directed acyclic graph (DAG) whereas their algorithm operates on trees. A solution to this problem is given by Li and Abe, who transform the DAG into a tree by copying each subgraph having multiple parents. An additional modification </context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -a lexical-semantic net for German. In Proceedings of the Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications at the 35th ACL and the 8th EACL, pages 9-15, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Martin Corley</author>
<author>Steffan Corley</author>
<author>Lars Konieczny</author>
<author>Amalia Todirascu</author>
</authors>
<title>WebExp: A Java toolbox for web-based psychological experiments.</title>
<date>1998</date>
<tech>Technical Report HCRC/TR-99,</tech>
<institution>Human Communication Research Centre, University of Edinburgh, UK.</institution>
<contexts>
<context position="21606" citStr="Keller et al., 1998" startWordPosition="3626" endWordPosition="3629">ant results, while being highly replicable both within and across speakers. ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to which they assign an arbitrary number. All other stimuli are rated proportionally to the modulus. In this way, each subject can establish their own rating scale. In the present experiment, the subjects were instructed to judge how acceptable the 90 sentences were in proportion to a modulus sentence. The experiment was conducted remotely over the Internet using WebExp 2.1 (Keller et al., 1998), an interactive software package for administering web-based psychological experiments. Subjects first saw a set of instructions that explained the ME technique and included some examples, and had to fill in a short questionnaire including basic demographic information. Each subject saw 90 experimental stimuli. A random stimulus order was generated for each subject. 31 Relation Verb Co-occurrence Frequency Band High Medium Low SUBJ stagnieren Umsatz 1.77 Preis .85 Arbeitslosigkeit .48 stagnate turnover price unemployment OBJ erlegen Tier .60 Jahr .30 Gesetz 0 shoot ani mal year law PP-OBJ den</context>
</contexts>
<marker>Keller, Corley, Corley, Konieczny, Todirascu, 1998</marker>
<rawString>Frank Keller, Martin Corley, Steffan Corley, Lars Konieczny, and Amalia Todirascu. 1998. WebExp: A Java toolbox for web-based psychological experiments. Technical Report HCRC/TR-99, Human Communication Research Centre, University of Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Maria Lapata</author>
<author>Olga Ourioupina</author>
</authors>
<title>Using the web to overcome data sparseness.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<location>Philadelphia, PA.</location>
<marker>Keller, Lapata, Ourioupina, 2002</marker>
<rawString>Frank Keller, Maria Lapata, and Olga Ourioupina. 2002. Using the web to overcome data sparseness. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 230-237, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Scott McDonald</author>
<author>Frank Keller</author>
</authors>
<title>Determinants of adjective-noun plausibility.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>30--36</pages>
<location>Bergen,</location>
<contexts>
<context position="3962" citStr="Lapata et al. (1999" startWordPosition="625" endWordPosition="629">loyed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-based evaluation or not). Furthermore, previous work has almost exclusively focused on verbal selectional 27 preferences in English with the exception of Lapata et al. (1999, 2001), who look at adjectivenoun combinations, again for English. Verbs tend to impose stricter selectional preferences on their arguments than adjectives or nouns and thus provide a natural test bed for models of selectional preferences. However, research on verbal selectional preferences has been relatively narrow in scope as it has primarily focused on verbs and their direct objects, ignoring the selectional preferences pertaining to subjects and prepositional complements. The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class t</context>
<context position="6533" citStr="Lapata et al., 1999" startWordPosition="1040" endWordPosition="1043"> task, and show that their approach outperforms Li and Abe (1998) and Resnik (1993). Another way to evaluate a model&apos;s performance is agreement with human ratings. This can be done by selecting predicate-argument structures randomly, using the model to predict the degree of semantic fit and then looking at how well the ratings 1The task is to decide which of two verbs v1 and 1,2 is more likely to take a noun n as its object. The method being tested must reconstruct which of the unseen (vi, n) and (v2, n) is a valid verb-object combination. correlate with the model&apos;s predictions (Resnik, 1993; Lapata et al., 1999; Lapata et al., 2001). This approach seems more appropriate for languages for which annotated corpora with word senses are not available. It is more direct than disambiguation which relies on the assumption that models of selectional preferences have to infer the appropriate semantic class and therefore perform disambiguation as a side effect. It is also more natural than pseudo-disambiguation which relies on artificially constructed data sets. Large-scale comparative studies have not, however, assessed the strengths and weaknesses of the proposed methods as far as modeling human data is conc</context>
<context position="19048" citStr="Lapata et al., 1999" startWordPosition="3207" endWordPosition="3210">zation class: (a) the statistic for performing significance testing and (b) the a value for determining the significance level. Here, we experimented with the X2 and G2 statistics and ran our experiments for the following different a values: .0005, .05, .3, .75, and .995. The parameter settings we explored are shown in Table 1. 3.2 Eliciting Judgments on Selectional Preferences In order to evaluate the methods introduced in Section 2, we first established an independent measure of how well a verb fits its arguments by eliciting judgments from human subjects (Resnik, 1993; Lapata et al., 2001; Lapata et al., 1999). In this section, we describe our method for assembling the set of experimental materials and collecting plausibility ratings for these stimuli. Materials and Design. As mentioned earlier, co-occurrence triples of the form (v, r, n) were extracted from the output of SMES. In order to reduce the risk of ratings being influenced by verb/noun combinations unfamiliar to the participants, we removed triples that had a verb or a noun with frequency less than one per million Ten verbs were selected randomly for each grammatical relation. For each verb we divided the set of triples into three bands (</context>
<context position="29602" citStr="Lapata et al. (1999)" startWordPosition="4919" endWordPosition="4922">marily for English, also generalize to German. We proposed to evaluate the different models against human ratings and argued that such an evaluation methodology allows us to assess the feasibility of the task and to compute performance upper bounds. Our results indicate that there is no method which overall performs best; it seems that different methods are suited for different argument relations (i.e., SimC for objects, SelA for subjects). The more sophisticated class-based approaches do not always yield better results when compared to simple frequency-based models. This is in agreement with Lapata et al. (1999) who found that cooccurrence frequency is the best predictor of the plausibility of adjective-noun pairs. Model combination seems promising in that a better fit with experimental data is obtained. However, note that none of our models (including the ones obtained via multiple regression) seem to attain results reasonably close to the upper bound. In the future, we plan to consider web-based frequencies for our probability estimates (Keller et al., 2002) as well as Abney and Light&apos;s (1999) Hidden Markov Models and Ciaramita and Johnson&apos;s (2000) Bayesian Belief Networks. We will also expand our </context>
</contexts>
<marker>Lapata, McDonald, Keller, 1999</marker>
<rawString>Maria Lapata, Scott McDonald, and Frank Keller. 1999. Determinants of adjective-noun plausibility. In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics, pages 30-36, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
<author>Frank Keller</author>
<author>Scott McDonald</author>
</authors>
<title>Evaluating smoothing algorithms against plausibility judgments.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>346--353</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3043" citStr="Lapata et al., 2001" startWordPosition="467" endWordPosition="470">nother related limitation of the frequency-based account is that it cannot make any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agre</context>
<context position="6555" citStr="Lapata et al., 2001" startWordPosition="1044" endWordPosition="1047">their approach outperforms Li and Abe (1998) and Resnik (1993). Another way to evaluate a model&apos;s performance is agreement with human ratings. This can be done by selecting predicate-argument structures randomly, using the model to predict the degree of semantic fit and then looking at how well the ratings 1The task is to decide which of two verbs v1 and 1,2 is more likely to take a noun n as its object. The method being tested must reconstruct which of the unseen (vi, n) and (v2, n) is a valid verb-object combination. correlate with the model&apos;s predictions (Resnik, 1993; Lapata et al., 1999; Lapata et al., 2001). This approach seems more appropriate for languages for which annotated corpora with word senses are not available. It is more direct than disambiguation which relies on the assumption that models of selectional preferences have to infer the appropriate semantic class and therefore perform disambiguation as a side effect. It is also more natural than pseudo-disambiguation which relies on artificially constructed data sets. Large-scale comparative studies have not, however, assessed the strengths and weaknesses of the proposed methods as far as modeling human data is concerned. In this paper, </context>
<context position="19026" citStr="Lapata et al., 2001" startWordPosition="3203" endWordPosition="3206"> appropriate generalization class: (a) the statistic for performing significance testing and (b) the a value for determining the significance level. Here, we experimented with the X2 and G2 statistics and ran our experiments for the following different a values: .0005, .05, .3, .75, and .995. The parameter settings we explored are shown in Table 1. 3.2 Eliciting Judgments on Selectional Preferences In order to evaluate the methods introduced in Section 2, we first established an independent measure of how well a verb fits its arguments by eliciting judgments from human subjects (Resnik, 1993; Lapata et al., 2001; Lapata et al., 1999). In this section, we describe our method for assembling the set of experimental materials and collecting plausibility ratings for these stimuli. Materials and Design. As mentioned earlier, co-occurrence triples of the form (v, r, n) were extracted from the output of SMES. In order to reduce the risk of ratings being influenced by verb/noun combinations unfamiliar to the participants, we removed triples that had a verb or a noun with frequency less than one per million Ten verbs were selected randomly for each grammatical relation. For each verb we divided the set of trip</context>
</contexts>
<marker>Lapata, Keller, McDonald, 2001</marker>
<rawString>Maria Lapata, Frank Keller, and Scott McDonald. 2001. Evaluating smoothing algorithms against plausibility judgments. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 346-353, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--2</pages>
<contexts>
<context position="3117" citStr="Li and Abe, 1998" startWordPosition="480" endWordPosition="483">e any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use</context>
<context position="4892" citStr="Li and Abe (1998)" startWordPosition="773" endWordPosition="776">narrow in scope as it has primarily focused on verbs and their direct objects, ignoring the selectional preferences pertaining to subjects and prepositional complements. The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class that best fits the predicate in question and (b) coming up with a statistical model or a measure that estimates how well a predicate fits its arguments. Resnik (1993) defines selectional association, an informationtheoretic measure of semantic fit of a particular semantic class c as an argument to a predicate p. Li and Abe (1998) use the Minimum Description Length (MDL) principle to select the the appropriate class c, Clark and Weir (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambi</context>
<context position="10301" citStr="Li and Abe (1998)" startWordPosition="1670" endWordPosition="1673">ed to take into consideration the fact that words in a taxonomy may belong to more than one conceptual class. Counts of verb-argument configurations are constructed for each conceptual class by dividing the contribution of the argument by the number of classes it belongs to (Resnik, 1993): where syn(c) is the synset of concept c, i.e., the set of synonymous words that can be used to denote the concept (for example, syn((beve r age)) = {beverage, drink, drinkable, potable}), and cn(n) is the set of concepts that can be denoted by noun n (more formally, cn(n) = {c n c syn(c)}). Tree Cut Models. Li and Abe (1998) use MDL to select from a hierarchy a set of classes that represent the selectional preferences for a given verb. These preferences are probabilities of the form P(n r) where n is a noun represented by a class in the taxonomy, v is a verb and r is an argument slot. Li and Abe&apos;s algorithm operates on thesaurus-like hierarchies where each leaf node stands for a noun, each internal node stands for the class of nouns below it, and a noun is uniquely represented by a leaf node. Li and Abe derive a separate model for each verb by partitioning the leaf nodes (i.e., nouns) of the thesaurus tree and as</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Warren Greiff</author>
</authors>
<title>Statistical models for the induction and use of selectional preferences.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<pages>87--1</pages>
<contexts>
<context position="3621" citStr="Light and Greiff, 2002" startWordPosition="572" endWordPosition="575">993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-based evaluation or not). Furthermore, previous work has almost exclusively focused on verbal selectional 27 preferences in English with the exception of Lapata et al. (1999, 2001), who look at adjectivenoun combinations, again for English. Verbs tend to impose stricter selectional preferences on their arguments than adjectives or nouns and thus provide a natural test bed for models of selectional preferences. However, research </context>
</contexts>
<marker>Light, Greiff, 2002</marker>
<rawString>Marc Light and Warren Greiff. 2002. Statistical models for the induction and use of selectional preferences. Cognitive Science, 87:1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex, UK.</institution>
<contexts>
<context position="16417" citStr="McCarthy (2001)" startWordPosition="2746" endWordPosition="2747">nformation encoded in it in terms of the hyponymy/hypernymy relation. Certain modifications to the original GermaNet hierarchy were necessary for the implementation of Li and Abe&apos;s method (1998). The GermaNet noun hierarchy is a directed acyclic graph (DAG) whereas their algorithm operates on trees. A solution to this problem is given by Li and Abe, who transform the DAG into a tree by copying each subgraph having multiple parents. An additional modification is needed since in GermaNet, nouns do not only occur as leaves of the hierarchy, but also at internal nodes. Following Wagner (2000) and McCarthy (2001), we created a new leaf for each internal node, containing a copy of the internal node&apos;s nouns. This guarantees that all nouns are present at the leaf level. Finally, the algorithm requires that the employed hierarchy has a single root node. In WordNet and GermaNet, nouns are not contained in a single hierarchy; instead they are partitioned according to a set of semantic primitives which are treated as the unique beginners of separate hierarchies. This means that an artificial concept (root) has to be created and connected to the existing top-level classes. Although WordNet has only nine class</context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>Diana McCarthy. 2001. Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences. Ph.D. thesis, University of Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="3336" citStr="Miller et al., 1990" startWordPosition="518" endWordPosition="521">the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-based evaluation or not). Furthermore, previous work has almost exclusively focused on verbal selectional 27 preferences in English with the except</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giinter Neumann</author>
<author>Rolf Backofen</author>
<author>Judith Baur</author>
<author>Markus Becker</author>
<author>Christian Braun</author>
</authors>
<title>An information extraction core system for real world German text processing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>209--216</pages>
<location>Washington, DC.</location>
<contexts>
<context position="14959" citStr="Neumann et al., 1997" startWordPosition="2510" endWordPosition="2513">ur experiments, we compared the performance of the five methods discussed above against human judgments. Before discussing the details of our evaluation we present our general experimental setup (e.g., the corpora and hierarchy used) and the different types of parameters we explored. All our experiments were conducted on data obtained from the German Siiddeutsche Zeitung (SZ) corpus, a 179 million word collection of newspaper texts. The corpus was parsed using the grammatical relation recognition component of SMES, a robust information extraction core system for the processing of German text (Neumann et al., 1997). SMES incorporates a tokenizer that maps the text into a stream of tokens. The tokens are then analyzed morphologically (compound recognition, assignment of part-of-speech tags), and a chunk parser identifies phrases and clauses by means of finite state grammars. The grammatical relations recognizer operates on the output of the parser while exploiting a large subcategorization lexicon. Although SMES recognizes a variety of grammatical relations, in our experiments we focused solely on relations of the form (v,r,n) where r can be a subject, direct object, or prepositional object (see the exam</context>
</contexts>
<marker>Neumann, Backofen, Baur, Becker, Braun, 1997</marker>
<rawString>Giinter Neumann, Rolf Backofen, Judith Baur, Markus Becker, and Christian Braun. 1997. An information extraction core system for real world German text processing. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, pages 209-216, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="3001" citStr="Pereira et al., 1993" startWordPosition="459" endWordPosition="462">ts and incongruent with natural objects. Another related limitation of the frequency-based account is that it cannot make any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Lig</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183-190, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stuart Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3099" citStr="Resnik, 1993" startWordPosition="478" endWordPosition="479"> it cannot make any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —&gt; a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.</context>
<context position="4727" citStr="Resnik (1993)" startWordPosition="748" endWordPosition="749">ives or nouns and thus provide a natural test bed for models of selectional preferences. However, research on verbal selectional preferences has been relatively narrow in scope as it has primarily focused on verbs and their direct objects, ignoring the selectional preferences pertaining to subjects and prepositional complements. The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class that best fits the predicate in question and (b) coming up with a statistical model or a measure that estimates how well a predicate fits its arguments. Resnik (1993) defines selectional association, an informationtheoretic measure of semantic fit of a particular semantic class c as an argument to a predicate p. Li and Abe (1998) use the Minimum Description Length (MDL) principle to select the the appropriate class c, Clark and Weir (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different ap</context>
<context position="5997" citStr="Resnik (1993)" startWordPosition="947" endWordPosition="948">ion are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambiguation results are reported by Resnik (1997), Abney and Light (1999), Ciaramita and Johnson (2000) and Carroll and McCarthy (2000) (however, on a different data set). Among the first three approaches, Ciaramita and Johnson (2000) obtain the best results. Li and Abe (1998) evaluate their system on the task of prepositional phrase attachment, whereas Clark and Weir (2002) use pseudodisambiguation,&apos; a somewhat artificial task, and show that their approach outperforms Li and Abe (1998) and Resnik (1993). Another way to evaluate a model&apos;s performance is agreement with human ratings. This can be done by selecting predicate-argument structures randomly, using the model to predict the degree of semantic fit and then looking at how well the ratings 1The task is to decide which of two verbs v1 and 1,2 is more likely to take a noun n as its object. The method being tested must reconstruct which of the unseen (vi, n) and (v2, n) is a valid verb-object combination. correlate with the model&apos;s predictions (Resnik, 1993; Lapata et al., 1999; Lapata et al., 2001). This approach seems more appropriate for</context>
<context position="8882" citStr="Resnik (1993)" startWordPosition="1418" endWordPosition="1419">ased approaches to selectional preferences rely on the estimation of the conditional probability P(nlv, r), where n is represented by its corresponding classes in the taxonomy. Here we concentrate solely on the nouns as attested in the corpus without making reference to a taxonomy and estimate the following: P(n v, r) = f (v, r.n) f (v,&apos;r) P(Idr,n) = f (v, r, n) f (r,n) 28 In (1) it is the verb that imposes the semantic preferences on its arguments, whereas in (2) selectional preferences are expressed in the other direction, i.e. arguments select for their predicates. Selectional Association. Resnik (1993) was the first to propose a measure of the the semantic fit of a particular semantic class c as an argument to a verb v. Selectional association (see (3) and (4)) represents the contribution of a particular semantic class c to the total quantity of information provided by a verb about the semantic classes of its argument, when measured as the relative entropy between the prior distribution of classes P(c) and the posterior distribution P (clv, r) of the argument classes for a particular verb v. The latter distribution is estimated as shown in (5). f (c) r) = v,r, (5) The estimation of P(clv, r</context>
<context position="19005" citStr="Resnik, 1993" startWordPosition="3201" endWordPosition="3202">for finding an appropriate generalization class: (a) the statistic for performing significance testing and (b) the a value for determining the significance level. Here, we experimented with the X2 and G2 statistics and ran our experiments for the following different a values: .0005, .05, .3, .75, and .995. The parameter settings we explored are shown in Table 1. 3.2 Eliciting Judgments on Selectional Preferences In order to evaluate the methods introduced in Section 2, we first established an independent measure of how well a verb fits its arguments by eliciting judgments from human subjects (Resnik, 1993; Lapata et al., 2001; Lapata et al., 1999). In this section, we describe our method for assembling the set of experimental materials and collecting plausibility ratings for these stimuli. Materials and Design. As mentioned earlier, co-occurrence triples of the form (v, r, n) were extracted from the output of SMES. In order to reduce the risk of ratings being influenced by verb/noun combinations unfamiliar to the participants, we removed triples that had a verb or a noun with frequency less than one per million Ten verbs were selected randomly for each grammatical relation. For each verb we di</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Stuart Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preferences and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?,</booktitle>
<pages>52--57</pages>
<location>Washington, DC.</location>
<contexts>
<context position="5537" citStr="Resnik (1997)" startWordPosition="873" endWordPosition="874">gth (MDL) principle to select the the appropriate class c, Clark and Weir (2002) employ hypothesis testing. Abney and Light (1999) propose Hidden Markov Models as a way of deriving selectional preferences over words, senses, or even classes, whereas Ciaramita and Johnson (2000) use Bayesian Belief Networks to quantify selectional preferences. Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments. Word sense disambiguation results are reported by Resnik (1997), Abney and Light (1999), Ciaramita and Johnson (2000) and Carroll and McCarthy (2000) (however, on a different data set). Among the first three approaches, Ciaramita and Johnson (2000) obtain the best results. Li and Abe (1998) evaluate their system on the task of prepositional phrase attachment, whereas Clark and Weir (2002) use pseudodisambiguation,&apos; a somewhat artificial task, and show that their approach outperforms Li and Abe (1998) and Resnik (1993). Another way to evaluate a model&apos;s performance is agreement with human ratings. This can be done by selecting predicate-argument structures</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preferences and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?, pages 52-57, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="11344" citStr="Rissanen, 1978" startWordPosition="1871" endWordPosition="1872">nd a noun is uniquely represented by a leaf node. Li and Abe derive a separate model for each verb by partitioning the leaf nodes (i.e., nouns) of the thesaurus tree and associating a probability with each class in the partition. More formally, a tree cut model M is defined as a pair of a tree cut F, which is a set of classes ci , c2, , ck, and a parameter vector 0 specifying a probability distribution over the members of F with the constraint that the probabilities sum to one. EP(cilv,r) = 1 i=1 To select the tree cut model that best tits the data, Li and Abe (1998) employ the MDL principle (Rissanen, 1978) by considering the cost in bits of describing both the model itself and the observed data (in our case verb-argument combinations). Given a data sample S encoded by a tree cut model /12/ = (F, 6) with tree cut F and estimated parameters 6, the total description length in bits L(M, S) is given by equation (8): L(M,S) = log1G log IS (8) — E logPia(nlv, r) nes where IQ is the cardinality of the set of all possible tree cuts, k is the number of classes on the cut F, 1,51 is the sample size, and Pn,4- (n r) is the probability of a noun, which is estimated by distributing the probability of a given</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465-471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Stevens</author>
</authors>
<title>Psychophysics: Introduction to Its Perceptual, Neural, and Social Prospects.</title>
<date>1975</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="20746" citStr="Stevens, 1975" startWordPosition="3492" endWordPosition="3493">bility band (Band). The factors Rel and Band had three levels each, and the factor Verb had 10 levels. This yielded a total of Rel x Verb x Band = 3 x 10 x 3 = 90 stimuli. The 90 verb/noun pairs were paraphrased to create sentences. For the direct/PPobject sentences, one of 10 common human first names (five female, five male) was added as subject where possible, or else an inanimate subject which appeared frequently in the corpus was chosen. Procedure. The experimental paradigm was Magnitude Estimation (ME), a technique standardly used in psychophysics to measure judgments of sensory stimuli (Stevens, 1975), which Bard et al. (1996) and Cowart (1997) have applied to the elicitation of linguistic judgments. ME has been shown to provide fine-grained measurements of linguistic acceptability which are robust enough to yield statistically significant results, while being highly replicable both within and across speakers. ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion. Subjects are first exposed to a modulus item, to which they assign an arbitrary number. All other stimuli are rated proportionally to the modulus. In this way, each subject can establi</context>
</contexts>
<marker>Stevens, 1975</marker>
<rawString>S. S. Stevens. 1975. Psychophysics: Introduction to Its Perceptual, Neural, and Social Prospects. John Wiley &amp; Sons, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Wagner</author>
</authors>
<title>Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Workshop on Ontology Learning at the 14th ECM,</booktitle>
<pages>37--42</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="16397" citStr="Wagner (2000)" startWordPosition="2743" endWordPosition="2744">ynsets), and the information encoded in it in terms of the hyponymy/hypernymy relation. Certain modifications to the original GermaNet hierarchy were necessary for the implementation of Li and Abe&apos;s method (1998). The GermaNet noun hierarchy is a directed acyclic graph (DAG) whereas their algorithm operates on trees. A solution to this problem is given by Li and Abe, who transform the DAG into a tree by copying each subgraph having multiple parents. An additional modification is needed since in GermaNet, nouns do not only occur as leaves of the hierarchy, but also at internal nodes. Following Wagner (2000) and McCarthy (2001), we created a new leaf for each internal node, containing a copy of the internal node&apos;s nouns. This guarantees that all nouns are present at the leaf level. Finally, the algorithm requires that the employed hierarchy has a single root node. In WordNet and GermaNet, nouns are not contained in a single hierarchy; instead they are partitioned according to a set of semantic primitives which are treated as the unique beginners of separate hierarchies. This means that an artificial concept (root) has to be created and connected to the existing top-level classes. Although WordNet</context>
</contexts>
<marker>Wagner, 2000</marker>
<rawString>Andreas Wagner. 2000. Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis. In Proceedings of the 1st Workshop on Ontology Learning at the 14th ECM, pages 37-42, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sholom M Weiss</author>
<author>Casimir A Kulikowski</author>
</authors>
<date>1991</date>
<booktitle>Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="24389" citStr="Weiss and Kulikowski, 1991" startWordPosition="4061" endWordPosition="4065">he class-based models. There are no parameters for co-occurrence frequency and conditional probability. Table 3 lists the best correlation coefficients per method, indicating the respective parameters where appropriate. For each grammatical relation, the optimal coefficient is emphasized. In Table 3, we also show how well humans agree in their judgments (inter-subject agreement, ISAgr) and thus provide an upper bound for the task which allows us to interpret how well the models are doing in relation to humans. We performed correlations on the elicited judgments using leave-one-out resampling (Weiss and Kulikowski, 1991). We divided the set of the subjects&apos; responses with size m into a set of size m — 1 (i.e., the response data of all but one subject) and a set of size one (i.e., the response data of a single subject). We then correlated the mean rating of the former set with the rating of the latter. This was repeated m times and the average agreement is reported in Table 3. As shown in Table 3, all five models are significantly correlated with the human ratings, although the correlation coefficients are not as high as the inter-subject agreement (ISAgr). Selectional association (SelA) and conditional probab</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>Sholom M. Weiss and Casimir A Kulikowski. 1991. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>