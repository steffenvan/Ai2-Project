<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000108">
<title confidence="0.977635">
The impact of language models and loss functions on repair disfluency
detection
</title>
<author confidence="0.983096">
Simon Zwarts and Mark Johnson
</author>
<affiliation confidence="0.9770275">
Centre for Language Technology
Macquarie University
</affiliation>
<email confidence="0.97085">
{simon.zwarts|mark.johnson|}@mq.edu.au
</email>
<bodyText confidence="0.995974050847458">
specifically targets the repair disfluencies. By com-
bining language models and using an appropriate
loss function in a log-linear reranker we are able to
achieve f-scores which are higher than previously re-
ported.
Often in natural language processing algorithms,
more data is more important than better algorithms
(Brill and Banko, 2001). It is this insight that drives
the first part of the work described in this paper. This
paper investigates how we can use language models
trained on large corpora to increase repair detection
accuracy performance.
There are three main innovations in this paper.
First, we investigate the use of a variety of language
models trained from text or speech corpora of vari-
ous genres and sizes. The largest available language
models are based on written text: we investigate the
effect of written text language models as opposed to
language models based on speech transcripts. Sec-
ond, we develop a new set of reranker features ex-
plicitly designed to capture important properties of
speech repairs. Many of these features are lexically
grounded and provide a large performance increase.
Third, we utilise a loss function, approximate ex-
pected f-score, that explicitly targets the asymmetric
evaluation metrics used in the disfluency detection
task. We explain how to optimise this loss func-
tion, and show that this leads to a marked improve-
ment in disfluency detection. This is consistent with
Jansche (2005) and Smith and Eisner (2006), who
observed similar improvements when using approx-
imate f-score loss for other problems. Similarly we
introduce a loss function based on the edit-f-score in
our domain.
Abstract
Unrehearsed spoken language often contains
disfluencies. In order to correctly inter-
pret a spoken utterance, any such disfluen-
cies must be identified and removed or other-
wise dealt with. Operating on transcripts of
speech which contain disfluencies, we study
the effect of language model and loss func-
tion on the performance of a linear reranker
that rescores the 25-best output of a noisy-
channel model. We show that language mod-
els trained on large amounts of non-speech
data improve performance more than a lan-
guage model trained on a more modest amount
of speech data, and that optimising f-score
rather than log loss improves disfluency detec-
tion performance.
Our approach uses a log-linear reranker, oper-
ating on the top n analyses of a noisy chan-
nel model. We use large language models,
introduce new features into this reranker and
examine different optimisation strategies. We
obtain a disfluency detection f-scores of 0.838
which improves upon the current state-of-the-
art.
</bodyText>
<sectionHeader confidence="0.950518" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983708673913044">
Most spontaneous speech contains disfluencies such
as partial words, filled pauses (e.g., “uh”, “um”,
“huh”), explicit editing terms (e.g., “I mean”), par-
enthetical asides and repairs. Of these, repairs
pose particularly difficult problems for parsing and
related Natural Language Processing (NLP) tasks.
This paper presents a model of disfluency detec-
tion based on the noisy channel framework, which703
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 703–711,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
Together, these three improvements are enough to
boost detection performance to a higher f-score than
previously reported in literature. Zhang et al. (2006)
investigate the use of ‘ultra large feature spaces’ as
an aid for disfluency detection. Using over 19 mil-
lion features, they report a final f-score in this task of
0.820. Operating on the same body of text (Switch-
board), our work leads to an f-score of 0.838, this is
a 9% relative improvement in residual f-score.
The remainder of this paper is structured as fol-
lows. First in Section 2 we describe related work.
Then in Section 3 we present some background on
disfluencies and their structure. Section 4 describes
appropriate evaluation techniques. In Section 5 we
describe the noisy channel model we are using. The
next three sections describe the new additions: Sec-
tion 6 describe the corpora used for language mod-
els, Section 7 describes features used in the log-
linear model employed by the reranker and Section 8
describes appropriate loss functions which are criti-
cal for our approach. We evaluate the new model in
Section 9. Section 10 draws up a conclusion.
cus of our work typically have this characteristic.
Noisy channel models have done well on the dis-
fluency detection task in the past; the work of John-
son and Charniak (2004) first explores such an ap-
proach. Johnson et al. (2004) adds some hand-
written rules to the noisy channel model and use a
maximum entropy approach, providing results com-
parable to Zhang et al. (2006), which are state-of-the
art results.
Kahn et al. (2005) investigated the role of
prosodic cues in disfluency detection, although the
main focus of their work was accurately recovering
and parsing a fluent version of the sentence. They
report a 0.782 f-score for disfluency detection.
</bodyText>
<sectionHeader confidence="0.991916" genericHeader="keywords">
3 Speech Disfluencies
</sectionHeader>
<bodyText confidence="0.9998172">
We follow the definitions of Shriberg (1994) regard-
ing speech disfluencies. She identifies and defines
three distinct parts of a speech disfluency, referred
to as the reparandum, the interregnum and the re-
pair. Consider the following utterance:
</bodyText>
<figure confidence="0.512705181818182">
uh, I mean
 |{z }
interregnum
to Denver
 |{z }
repair
reparandum
z } |{
I want a flight to Boston,
(1)
on Friday
</figure>
<sectionHeader confidence="0.998466" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999963">
A number of different techniques have been pro-
posed for automatic disfluency detection. Schuler
et al. (2010) propose a Hierarchical Hidden Markov
Model approach; this is a statistical approach which
builds up a syntactic analysis of the sentence and
marks those subtrees which it considers to be made
up of disfluent material. Although they are inter-
ested not only in disfluency but also a syntactic anal-
ysis of the utterance, including the disfluencies be-
ing analysed, their model’s final f-score for disflu-
ency detection is lower than that of other models.
Snover et al. (2004) investigate the use of purely
lexical features combined with part-of-speech tags
to detect disfluencies. This approach is compared to
approaches which use primarily prosodic cues, and
appears to perform equally well. However, the au-
thors note that this model finds it difficult to identify
disfluencies which by themselves are very fluent. As
we will see later, the individual components of a dis-
fluency do not have to be disfluent by themselves.
This can occur when a speaker edits her speech for
meaning-related reasons, rather than errors that arise
from performance. The edit repairs which are the fo-704
The reparandum to Boston is the part of the utterance
that is ‘edited out’; the interregnum uh, I mean is a
filled pause, which need not always be present; and
the repair to Denver replaces the reparandum.
Shriberg and Stolcke (1998) studied the location
and distribution of repairs in the Switchboard cor-
pus (Godfrey and Holliman, 1997), the primary cor-
pus for speech disfluency research, but did not pro-
pose an actual model of repairs. They found that the
overall distribution of speech disfluencies in a large
corpus can be fit well by a model that uses only in-
formation on a very local level. Our model, as ex-
plained in section 5, follows from this observation.
As our domain of interest we use the Switchboard
corpus. This is a large corpus consisting of tran-
scribed telephone conversations between two part-
ners. In the Treebank III (Marcus et al., 1999) cor-
pus there is annotation available for the Switchboard
corpus, which annotates which parts of utterances
are in a reparandum, interregnum or repair.
</bodyText>
<sectionHeader confidence="0.8924235" genericHeader="method">
4 Evaluation metrics for disfluency
detection systems
</sectionHeader>
<bodyText confidence="0.999960888888889">
Disfluency detection systems like the one described
here identify a subset of the word tokens in each
transcribed utterance as “edited” or disfluent. Per-
haps the simplest way to evaluate such systems is
to calculate the accuracy of labelling they produce,
i.e., the fraction of words that are correctly labelled
(i.e., either “edited” or “not edited”). However,
as Charniak and Johnson (2001) observe, because
only 5.9% of words in the Switchboard corpus are
“edited”, the trivial baseline classifier which assigns
all words the “not edited” label achieves a labelling
accuracy of 94.1%.
Because the labelling accuracy of the trivial base-
line classifier is so high, it is standard to use a dif-
ferent evaluation metric that focuses more on the de-
tection of “edited” words. We follow Charniak and
Johnson (2001) and report the f-score of our disflu-
ency detection system. The f-score f is:
</bodyText>
<equation confidence="0.991099666666667">
2c
f = (2)
g + e
</equation>
<bodyText confidence="0.9999495">
where g is the number of “edited” words in the gold
test corpus, e is the number of “edited” words pro-
posed by the system on that corpus, and c is the num-
ber of the “edited” words proposed by the system
that are in fact correct. A perfect classifier which
correctly labels every word achieves an f-score of
1, while the trivial baseline classifiers which label
every word as “edited” or “not edited” respectively
achieve a very low f-score.
Informally, the f-score metric focuses more on
the “edited” words than it does on the “not edited”
words. As we will see in section 8, this has implica-
tions for the choice of loss function used to train the
classifier.
</bodyText>
<sectionHeader confidence="0.997547" genericHeader="method">
5 Noisy Channel Model
</sectionHeader>
<bodyText confidence="0.999824266666667">
Following Johnson and Charniak (2004), we use a
noisy channel model to propose a 25-best list of
possible speech disfluency analyses. The choice of
this model is driven by the observation that the re-
pairs frequently seem to be a “rough copy” of the
reparandum, often incorporating the same or very
similar words in roughly the same word order. That705
is, they seem to involve “crossed” dependencies be-
tween the reparandum and the repair. Example (3)
shows the crossing dependencies. As this exam-
ple also shows, the repair often contains many of
the same words that appear in the reparandum. In
fact, in our Switchboard training corpus we found
that 62reparandum also appeared in the associated
repair,
</bodyText>
<equation confidence="0.778823333333333">
� o Bostonn uh, I mean, `o Denver
Y Y Y �
reparandum interregnum repair
</equation>
<subsectionHeader confidence="0.880805">
5.1 Informal Description
</subsectionHeader>
<bodyText confidence="0.998614">
Given an observed sentence Y we wish to find the
most likely source sentence X, where
</bodyText>
<equation confidence="0.999914">
X� = argmax
X P(Y |X)P(X) (4)
</equation>
<bodyText confidence="0.999870789473684">
In our model the unobserved X is a substring of the
complete utterance Y .
Noisy-channel models are used in a similar way
in statistical speech recognition and machine trans-
lation. The language model assigns a probability
P(X) to the string X, which is a substring of the
observed utterance Y . The channel model P(Y |X)
generates the utterance Y , which is a potentially dis-
fluent version of the source sentence X. A repair
can potentially begin before any word of X. When
a repair has begun, the channel model incrementally
processes the succeeding words from the start of the
repair. Before each succeeding word either the re-
pair can end or else a sequence of words can be in-
serted in the reparandum. At the end of each re-
pair, a (possibly null) interregnum is appended to the
reparandum.
We will look at these two components in the next
two Sections in more detail.
</bodyText>
<subsectionHeader confidence="0.988994">
5.2 Language Model
</subsectionHeader>
<bodyText confidence="0.999977222222222">
Informally, the task of language model component
of the noisy channel model is to assess fluency of
the sentence with disfluency removed. Ideally we
would like to have a model which assigns a very
high probability to disfluency-free utterances and a
lower probability to utterances still containing dis-
fluencies. For computational complexity reasons, as
described in the next section, inside the noisy chan-
nel model we use a bigram language model. This
</bodyText>
<equation confidence="0.644749">
(3)
</equation>
<bodyText confidence="0.999869857142857">
bigram language model is trained on the fluent ver-
sion of the Switchboard corpus (training section).
We realise that a bigram model might not be able
to capture more complex language behaviour. This
motivates our investigation of a range of additional
language models, which are used to define features
used in the log-linear reranker as described below.
</bodyText>
<subsectionHeader confidence="0.983194">
5.3 Channel Model
</subsectionHeader>
<bodyText confidence="0.999983">
The intuition motivating the channel model design
is that the words inserted into the reparandum are
very closely related to those in the repair. Indeed,
in our training data we find that 62% of the words
in the reparandum are exact copies of words in the
repair; this identity is strong evidence of a repair.
The channel model is designed so that exact copy
reparandum words will have high probability.
Because these repair structures can involve an un-
bounded number of crossed dependencies, they can-
not be described by a context-free or finite-state
grammar. This motivates the use of a more expres-
sive formalism to describe these repair structures.
We assume that X is a substring of Y , i.e., that the
source sentence can be obtained by deleting words
from Y , so for a fixed observed utterance Y there
are only a finite number of possible source sen-
tences. However, the number of possible source sen-
tences, X, grows exponentially with the length of Y ,
so exhaustive search is infeasible. Tree Adjoining
Grammars (TAG) provide a systematic way of for-
malising the channel model, and their polynomial-
time dynamic programming parsing algorithms can
be used to search for likely repairs, at least when
used with simple language models like a bigram
language model. In this paper we first identify the
25 most likely analyses of each sentence using the
TAG channel model together with a bigram lan-
guage model.
Further details of the noisy channel model can be
found in Johnson and Charniak (2004).
</bodyText>
<subsectionHeader confidence="0.753002">
5.4 Reranker
</subsectionHeader>
<bodyText confidence="0.999937571428572">
To improve performance over the standard noisy
channel model we use a reranker, as previously sug-
gest by Johnson and Charniak (2004). We rerank a
25-best list of analyses. This choice is motivated by
an oracle experiment we performed, probing for the
location of the best analysis in a 100-best list. This706
experiment shows that in 99.5% of the cases the best
analysis is located within the first 25, and indicates
that an f-score of 0.958 should be achievable as the
upper bound on a model using the first 25 best anal-
yses. We therefore use the top 25 analyses from the
noisy channel model in the remainder of this paper
and use a reranker to choose the most suitable can-
didate among these.
</bodyText>
<sectionHeader confidence="0.998054" genericHeader="method">
6 Corpora for language modelling
</sectionHeader>
<bodyText confidence="0.997344671232877">
We would like to use additional data to model
the fluent part of spoken language. However, the
Switchboard corpus is one of the largest widely-
available disfluency-annotated speech corpora. It is
reasonable to believe that for effective disfluency de-
tection Switchboard is not large enough and more
text can provide better analyses. Schwartz et al.
(1994), although not focusing on disfluency detec-
tion, show that using written language data for mod-
elling spoken language can improve performance.
We turn to three other bodies of text and investi-
gate the use of these corpora for our task, disfluency
detection. We will describe these corpora in detail
here.
The predictions made by several language models
are likely to be strongly correlated, even if the lan-
guage models are trained on different corpora. This
motivates the choice for log-linear learners, which
are built to handle features which are not necessar-
ily independent. We incorporate information from
the external language models by defining a reranker
feature for each external language model. The value
of this feature is the log probability assigned by the
language model to the candidate underlying fluent
substring X
For each of our corpora (including Switchboard)
we built a 4-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995). For each analy-
sis we calculate the probability under that language
model for the candidate underlying fluent substring
X. We use this log probability as a feature in the
reranker. We use the SRILM toolkit (Stolcke, 2002)
both for estimating the model from the training cor-
pus as well as for computing the probabilities of the
underlying fluent sentences X of the different anal-
ysis.
As previously described, Switchboard is our pri-
mary corpus for our model. The language model
part of the noisy channel model already uses a bi-
gram language model based on Switchboard, but in
the reranker we would like to also use 4-grams for
reranking. Directly using Switchboard to build a 4-
gram language model is slightly problematic. When
we use the training data of Switchboard both for lan-
guage fluency prediction and the same training data
also for the loss function, the reranker will overesti-
mate the weight associated with the feature derived
from the Switchboard language model, since the flu-
ent sentence itself is part of the language model
training data. We solve this by dividing the Switch-
board training data into 20 folds. For each fold we
use the 19 other folds to construct a language model
and then score the utterance in this fold with that
language model.
The largest widely-available corpus for language
modelling is the Web 1T 5-gram corpus (Brants and
Franz, 2006). This data set, collected by Google
Inc., contains English word n-grams and their ob-
served frequency counts. Frequency counts are pro-
duced from this billion-token corpus of web text.
Because of the noise1 present in this corpus there is
an ongoing debate in the scientific community of the
use of this corpus for serious language modelling.
The Gigaword Corpus (Graff and Cieri, 2003)
is a large body of newswire text. The corpus con-
tains 1.6 · 109 tokens, however fluent newswire text
is not necessarily of the same domain as disfluency
removed speech.
The Fisher corpora Part I (David et al., 2004) and
Part II (David et al., 2005) are large bodies of tran-
scribed text. Unlike Switchboard there is no disflu-
ency annotation available for Fisher. Together the
two Fisher corpora consist of 2.2 · 107 tokens.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="method">
7 Features
</sectionHeader>
<bodyText confidence="0.998070384615384">
The log-linear reranker, which rescores the 25-best
lists produced by the noisy-channel model, can
also include additional features besides the noisy-
channel log probabilities. As we show below, these
additional features can make a substantial improve-
ment to disfluency detection performance. Our
reranker incorporates two kinds of features. The first
are log-probabilities of various scores computed by
the noisy-channel model and the external language
models. We only include features which occur at
least 5 times in our training data.
The noisy channel and language model features
consist of:
</bodyText>
<listItem confidence="0.999122888888889">
1. LMP: 4 features indicating the probabilities of
the underlying fluent sentences under the lan-
guage models, as discussed in the previous sec-
tion.
2. NCLogP: The Log Probability of the entire
noisy channel model. Since by itself the noisy
channel model is already doing a very good job,
we do not want this information to be lost.
3. LogFom: This feature is the log of the “fig-
</listItem>
<bodyText confidence="0.895655708333333">
ure of merit” used to guide search in the noisy
channel model when it is producing the 25-best
list for the reranker. The log figure of merit is
the sum of the log language model probability
and the log channel model probability plus 1.5
times the number of edits in the sentence. This
feature is redundant, i.e., it is a linear combina-
tion of other features available to the reranker
model: we include it here so the reranker has
direct access to all of the features used by the
noisy channel model.
4. NCTransOdd: We include as a feature parts of
the noisy channel model itself, i.e. the channel
model probability. We do this so that the task
to choosing appropriate weights of the channel
model and language model can be moved from
the noisy channel model to the log-linear opti-
misation algorithm.
The boolean indicator features consist of the fol-
lowing 3 groups of features operating on words and
their edit status; the latter indicated by one of three
possible flags: when the word is not part of a dis-
fluency or E when it is part of the reparandum or I
when it is part of the interregnum.
</bodyText>
<footnote confidence="0.742290571428571">
1. CopyFlags X Y: When there is an exact copy
in the input text of length X (1 &lt; X &lt; 3) and
the gap between the copies is Y (0 &lt; Y &lt; 3)
this feature is the sequence of flags covering the
two copies. Example: CopyFlags 1 0 (E
1We do not mean speech disfluencies here, but noise in web-
text; web-text is often poorly written and unedited text. 707
</footnote>
<page confidence="0.99631">
708
</page>
<bodyText confidence="0.9995028">
) records a feature when two identical words
are present, directly consecutive and the first
one is part of a disfluency (Edited) while the
second one is not. There are 745 different in-
stances of these features.
</bodyText>
<listItem confidence="0.995383166666667">
2. WordsFlags L n R: This feature records the
immediate area around an n-gram (n &lt; 3).
L denotes how many flags to the left and R
(0 &lt; R &lt; 1) how many to the right are includes
in this feature (Both L and R range over 0 and
1). Example: WordsFlags 1 1 0 (need
) is a feature that fires when a fluent word is
followed by the word ‘need’ (one flag to the
left, none to the right). There are 256808 of
these features present.
3. SentenceEdgeFlags B L: This feature indi-
cates the location of a disfluency in an ut-
</listItem>
<bodyText confidence="0.934168066666667">
terance. The Boolean B indicates whether
this features records sentence initial or sen-
tence final behaviour, L (1 &lt; L &lt; 3)
records the length of the flags. Example
SentenceEdgeFlags 1 1 (I) is a fea-
ture recording whether a sentence ends on an
interregnum. There are 22 of these features
present.
We give the following analysis as an example:
but E but that does n’t work
The language model features are the probability
calculated over the fluent part. NCLogP, Log-
Fom and NCTransOdd are present with their asso-
ciated value. The following binary flags are present:
CopyFlags 1 0 (E )
</bodyText>
<equation confidence="0.99712725">
WordsFlags:0:1:0 (but E)
WordsFlags:0:1:0 (but )
WordsFlags:1:1:0 (E but )
WordsFlags:1:1:0 ( that )
WordsFlags:0:2:0 (but E but ) etc.2
SentenceEdgeFlags:0:1 (E)
SentenceEdgeFlags:0:2 (E )
SentenceEdgeFlags:0:3 (E )
</equation>
<bodyText confidence="0.991772">
These three kinds of boolean indicator features to-
gether constitute the extended feature set.
</bodyText>
<footnote confidence="0.636144">
2An exhaustive list here would be too verbose.
</footnote>
<sectionHeader confidence="0.384711" genericHeader="method">
8 Loss functions for reranker training
</sectionHeader>
<bodyText confidence="0.999750375">
We formalise the reranker training procedure as fol-
lows. We are given a training corpus T containing
information about n possibly disfluent sentences.
For the ith sentence T specifies the sequence of
words xi, a set Yi of 25-best candidate “edited” la-
bellings produced by the noisy channel model, as
well as the correct “edited” labelling y⋆ i E Yi.3
We are also given a vector f = (f1, ... , fm)
of feature functions, where each fj maps a word
sequence x and an “edit” labelling y for x to a
real value fj(x, y). Abusing notation somewhat,
we write f(x, y) = (f1(x, y), ... , fm(x, y)). We
interpret a vector w = (w1, ... , wm) of feature
weights as defining a conditional probability distri-
bution over a candidate set Y of “edited” labellings
for a string x as follows:
</bodyText>
<equation confidence="0.990569">
exp(w - f(x, y))
Py′∈Y exp(w - f(x, y′))
</equation>
<bodyText confidence="0.999609666666667">
We estimate the feature weights w from the train-
ing data T by finding a feature weight vector wb that
optimises a regularised objective function:
</bodyText>
<equation confidence="0.981671">
wb = argmin
w
</equation>
<bodyText confidence="0.999422">
Here α is the regulariser weight and LT is a loss
function. We investigate two different loss functions
in this paper. LogLoss is the negative log conditional
likelihood of the training data:
</bodyText>
<equation confidence="0.9988345">
LogLossT(w) = Xm − log P(y⋆i  |xi, Yi)
i=1
</equation>
<bodyText confidence="0.998644333333333">
Optimising LogLoss finds the wb that define (regu-
larised) conditional Maximum Entropy models.
It turns out that optimising LogLoss yields sub-
optimal weight vectors wb here. LogLoss is a sym-
metric loss function (i.e., each mistake is equally
weighted), while our f-score evaluation metric
weights “edited” labels more highly, as explained
in section 4. Because our data is so skewed (i.e.,
“edited” words are comparatively infrequent), we
</bodyText>
<footnote confidence="0.95939275">
3In the situation where the true “edited” labelling does not
appear in the 25-best list Yi produced by the noisy-channel
model, we choose y⋆i to be a labelling in Yi closest to the true
labelling.
</footnote>
<equation confidence="0.996167833333333">
Pw(y  |x, Y) =
w2j
LT(w) + α
m
X
j=1
</equation>
<bodyText confidence="0.9991985">
can improve performance by using an asymmetric
loss function.
Inspired by our evaluation metric, we devised an
approximate expected f-score loss function FLoss.
</bodyText>
<equation confidence="0.974487333333333">
[c]
FLossT(w) = 1 −
2Ewg + Ew[e]
</equation>
<bodyText confidence="0.9996295">
This approximation assumes that the expectations
approximately distribute over the division: see Jan-
sche (2005) and Smith and Eisner (2006) for other
approximations to expected f-score and methods for
optimising them. We experimented with other asym-
metric loss functions (e.g., the expected error rate)
and found that they gave very similar results.
An advantage of FLoss is that it and its deriva-
tives with respect to w (which are required for
numerical optimisation) are easy to calculate ex-
actly. For example, the expected number of correct
“edited” words is:
</bodyText>
<equation confidence="0.99220225">
Ew[cy⋆  |Yi], where:
i
�Ew[cy⋆i  |Yi] = cy⋆i (y) Pw(y  |xi, Yi)
y∈Yi
</equation>
<bodyText confidence="0.928634333333333">
and cy⋆(y) is the number of correct “edited” labels
in y given the gold labelling y⋆. The derivatives of
FLoss are:
</bodyText>
<equation confidence="0.984083166666667">
∂FLossT
∂wj (w) =
1 FLossT (w)∂Ew[e] − 2∂Ew[c]
g + Ew[e] ∂wj ∂wj
where:
∂Ew[c] n ∂Ew[cy⋆ i  |xi, Yi]
=
∂wj i=1 ∂wj
∂Ew[cy⋆  |x, Y]
=
∂wj
Ew[fjcy⋆  |x, Y] − Ew[fj  |x, Y] Ew[cy⋆  |x, Y].
</equation>
<bodyText confidence="0.610062">
∂E[e]/∂wj is given by a similar formula.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="evaluation">
9 Results
</sectionHeader>
<bodyText confidence="0.999975396226416">
We follow Charniak and Johnson (2001) and split
the corpus into main training data, held-out train-
ing data and test data as follows: main training con-
sisted of all sw[23]*.dps files, held-out training con-
sisted of all sw4[5-9]*.dps files and test consisted of709
all sw4[0-1]*.dps files. However, we follow (John-
son and Charniak, 2004) in deleting all partial words
and punctuation from the training and test data (they
argued that this is more realistic in a speech process-
ing application).
Table 1 shows the results for the different models
on held-out data. To avoid over-fitting on the test
data, we present the f-scores over held-out training
data instead of test data. We used the held-out data
to select the best-performing set of reranker features,
which consisted of features for all of the language
models plus the extended (i.e., indicator) features,
and used this model to analyse the test data. The f-
score of this model on test data was 0.838. In this
table, the set of Extended Features is defined as all
the boolean features as described in Section 7.
We first observe that adding different external lan-
guage models does increase the final score. The
difference between the external language models is
relatively small, although the differences in choice
are several orders of magnitude. Despite the pu-
tative noise in the corpus, a language model built
on Google’s Web1T data seems to perform very
well. Only the model where Switchboard 4-grams
are used scores slightly lower, we explain this be-
cause the internal bigram model of the noisy chan-
nel model is already trained on Switchboard and so
this model adds less new information to the reranker
than the other models do.
Including additional features to describe the prob-
lem space is very productive. Indeed the best per-
forming model is the model which has all extended
features and all language model features. The dif-
ferences among the different language models when
extended features are present are relatively small.
We assume that much of the information expressed
in the language models overlaps with the lexical fea-
tures.
We find that using a loss function related to our
evaluation metric, rather than optimising LogLoss,
consistently improves edit-word f-score. The stan-
dard LogLoss function, which estimates the “max-
imum entropy” model, consistently performs worse
than the loss function minimising expected errors.
The best performing model (Base + Ext. Feat.
+ All LM, using expected f-score loss) scores an f-
score of 0.838 on test data. The results as indicated
by the f-score outperform state-of-the-art models re-
</bodyText>
<equation confidence="0.956589666666667">
n
Ew[c] =
i=1
</equation>
<table confidence="0.999164461538462">
Model F-score
Base (noisy channel, no reranking) 0.756
Model log loss expected f-score loss
Base + Switchboard 0.776 0.791
Base + Fisher 0.771 0.797
Base + Gigaword 0.777 0.797
Base + Web1T 0.781 0.798
Base + Ext. Feat. 0.824 0.827
Base + Ext. Feat. + Switchboard 0.827 0.828
Base + Ext. Feat. + Fisher 0.841 0.856
Base + Ext. Feat. + Gigaword 0.843 0.852
Base + Ext. Feat. + Web1T 0.843 0.850
Base + Ext. Feat. + All LM 0.841 0.857
</table>
<tableCaption confidence="0.999977">
Table 1: Edited word detection f-score on held-out data for a variety of language models and loss functions
</tableCaption>
<bodyText confidence="0.9870355">
ported in literature operating on identical data, even
though we use vastly less features than other do.
data, even though we use vastly fewer features than
others do.
</bodyText>
<sectionHeader confidence="0.976672" genericHeader="conclusions">
10 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999914555555556">
We have described a disfluency detection algorithm
which we believe improves upon current state-of-
the-art competitors. This model is based on a noisy
channel model which scores putative analyses with
a language model; its channel model is inspired by
the observation that reparandum and repair are of-
ten very similar. As Johnson and Charniak (2004)
noted, although this model performs well, a log-
linear reranker can be used to increase performance.
We built language models from a variety of
speech and non-speech corpora, and examine the ef-
fect they have on disfluency detection. We use lan-
guage models derived from different larger corpora
effectively in a maximum reranker setting. We show
that the actual choice for a language model seems
to be less relevant and newswire text can be used
equally well for modelling fluent speech.
We describe different features to improve disflu-
ency detection even further. Especially these fea-
tures seem to boost performance significantly.
Finally we investigate the effect of different loss
functions. We observe that using a loss function di-
rectly optimising our interest yields a performance
increase which is at least at large as the effect of us-
ing very large language models.
We obtained an f-score which outperforms other
models reported in literature operating on identical710
</bodyText>
<sectionHeader confidence="0.994732" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999504625">
This work was supported was supported under Aus-
tralian Research Council’s Discovery Projects fund-
ing scheme (project number DP110102593) and
by the Australian Research Council as part of the
Thinking Head Project the Thinking Head Project,
ARC/NHMRC Special Research Initiative Grant #
TS0669874. We thank the anonymous reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872408163265">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Published by Linguistic Data Consortium,
Philadelphia.
Erik Brill and Michele Banko. 2001. Mitigating the
Paucity-of-Data Problem: Exploring the Effect of
Training Corpus Size on Classifier Performance for
Natural Language Processing. In Proceedings of the
First International Conference on Human Language
Technology Research.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118–126.
Christopher Cieri David, David Miller, and Kevin
Walker. 2004. Fisher English Training Speech Part
1 Transcripts. Published by Linguistic Data Consor-
tium, Philadelphia.
Christopher Cieri David, David Miller, and Kevin
Walker. 2005. Fisher English Training Speech Part
2 Transcripts. Published by Linguistic Data Consor-
tium, Philadelphia.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Published by Linguistic
Data Consortium, Philadelphia.
David Graff and Christopher Cieri. 2003. English gi-
gaword. Published by Linguistic Data Consortium,
Philadelphia.
Martin Jansche. 2005. Maximum Expected F-Measure
Training of Logistic Regression Models. In Proceed-
ings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 692–699, Vancouver, British
Columbia, Canada, October. Association for Compu-
tational Linguistics.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 33–39.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An Improved Model for Recognizing Disfluen-
cies in Conversational Speech. In Proceedings of the
Rich Transcription Fall Workshop.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
Use of Prosody in Parsing Conversational Speech. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 233–240, Vancouver,
British Columbia, Canada.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, pages 181–
184.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Published by Linguistic Data Consortium, Philadel-
phia.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-Coverage Parsing us-
ing Human-Like Memory Constraints. Computational
Linguistics, 36(1):1–30.
Richard Schwartz, Long Nguyen, Francis Kubala,
George Chou, George Zavaliagkos, and John
Makhoul. 1994. On Using Written Language
Training Data for Spoken Language Modeling. In
Proceedings of the Human Language Technology
Workshop, pages 94–98.
Elizabeth Shriberg and Andreas Stolcke. 1998. How
far do speakers back up in repairs? A quantitative711
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 2183–
2186.
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disuencies. Ph.D. thesis, University of Cali-
fornia, Berkeley.
David A. Smith and Jason Eisner. 2006. Minimum Risk
Annealing for Training Log-Linear Models. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
787–794.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2004. A Lexically-Driven Algorithm for Disfluency
Detection. In Proceedings of Human Language Tech-
nologies and North American Association for Compu-
tational Linguistics, pages 157–160.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large fea-
ture spaces. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 561–568.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.525593">
<title confidence="0.9939625">The impact of language models and loss functions on repair disfluency detection</title>
<author confidence="0.981923">Simon Zwarts</author>
<author confidence="0.981923">Mark</author>
<affiliation confidence="0.957943">Centre for Language</affiliation>
<abstract confidence="0.9913443">Macquarie specifically targets the repair disfluencies. By combining language models and using an appropriate loss function in a log-linear reranker we are able to achieve f-scores which are higher than previously reported. Often in natural language processing algorithms, more data is more important than better algorithms (Brill and Banko, 2001). It is this insight that drives the first part of the work described in this paper. This paper investigates how we can use language models trained on large corpora to increase repair detection accuracy performance. There are three main innovations in this paper. First, we investigate the use of a variety of language models trained from text or speech corpora of various genres and sizes. The largest available language models are based on written text: we investigate the effect of written text language models as opposed to language models based on speech transcripts. Second, we develop a new set of reranker features explicitly designed to capture important properties of speech repairs. Many of these features are lexically grounded and provide a large performance increase. Third, we utilise a loss function, approximate expected f-score, that explicitly targets the asymmetric evaluation metrics used in the disfluency detection task. We explain how to optimise this loss function, and show that this leads to a marked improvement in disfluency detection. This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. Similarly we introduce a loss function based on the edit-f-score in our domain. Abstract Unrehearsed spoken language often contains In order to correctly pret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisychannel model. We show that language models trained on large amounts of non-speech data improve performance more than a language model trained on a more modest amount of speech data, and that optimising f-score rather than log loss improves disfluency detection performance. Our approach uses a log-linear reranker, operon the top of a noisy channel model. We use large language models, introduce new features into this reranker and examine different optimisation strategies. We a disfluency detection f-scores of which improves upon the current state-of-theart.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Published by Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia.</location>
<contexts>
<context position="17014" citStr="Brants and Franz, 2006" startWordPosition="2817" endWordPosition="2820">ta of Switchboard both for language fluency prediction and the same training data also for the loss function, the reranker will overestimate the weight associated with the feature derived from the Switchboard language model, since the fluent sentence itself is part of the language model training data. We solve this by dividing the Switchboard training data into 20 folds. For each fold we use the 19 other folds to construct a language model and then score the utterance in this fold with that language model. The largest widely-available corpus for language modelling is the Web 1T 5-gram corpus (Brants and Franz, 2006). This data set, collected by Google Inc., contains English word n-grams and their observed frequency counts. Frequency counts are produced from this billion-token corpus of web text. Because of the noise1 present in this corpus there is an ongoing debate in the scientific community of the use of this corpus for serious language modelling. The Gigaword Corpus (Graff and Cieri, 2003) is a large body of newswire text. The corpus contains 1.6 · 109 tokens, however fluent newswire text is not necessarily of the same domain as disfluency removed speech. The Fisher corpora Part I (David et al., 2004</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Published by Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Brill</author>
<author>Michele Banko</author>
</authors>
<title>Mitigating the Paucity-of-Data Problem: Exploring the Effect of Training Corpus Size on Classifier Performance for Natural Language Processing.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research.</booktitle>
<marker>Brill, Banko, 2001</marker>
<rawString>Erik Brill and Michele Banko. 2001. Mitigating the Paucity-of-Data Problem: Exploring the Effect of Training Corpus Size on Classifier Performance for Natural Language Processing. In Proceedings of the First International Conference on Human Language Technology Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="8260" citStr="Charniak and Johnson (2001)" startWordPosition="1323" endWordPosition="1326">ank III (Marcus et al., 1999) corpus there is annotation available for the Switchboard corpus, which annotates which parts of utterances are in a reparandum, interregnum or repair. 4 Evaluation metrics for disfluency detection systems Disfluency detection systems like the one described here identify a subset of the word tokens in each transcribed utterance as “edited” or disfluent. Perhaps the simplest way to evaluate such systems is to calculate the accuracy of labelling they produce, i.e., the fraction of words that are correctly labelled (i.e., either “edited” or “not edited”). However, as Charniak and Johnson (2001) observe, because only 5.9% of words in the Switchboard corpus are “edited”, the trivial baseline classifier which assigns all words the “not edited” label achieves a labelling accuracy of 94.1%. Because the labelling accuracy of the trivial baseline classifier is so high, it is standard to use a different evaluation metric that focuses more on the detection of “edited” words. We follow Charniak and Johnson (2001) and report the f-score of our disfluency detection system. The f-score f is: 2c f = (2) g + e where g is the number of “edited” words in the gold test corpus, e is the number of “edi</context>
<context position="25037" citStr="Charniak and Johnson (2001)" startWordPosition="4231" endWordPosition="4234">its derivatives with respect to w (which are required for numerical optimisation) are easy to calculate exactly. For example, the expected number of correct “edited” words is: Ew[cy⋆ |Yi], where: i �Ew[cy⋆i |Yi] = cy⋆i (y) Pw(y |xi, Yi) y∈Yi and cy⋆(y) is the number of correct “edited” labels in y given the gold labelling y⋆. The derivatives of FLoss are: ∂FLossT ∂wj (w) = 1 FLossT (w)∂Ew[e] − 2∂Ew[c] g + Ew[e] ∂wj ∂wj where: ∂Ew[c] n ∂Ew[cy⋆ i |xi, Yi] = ∂wj i=1 ∂wj ∂Ew[cy⋆ |x, Y] = ∂wj Ew[fjcy⋆ |x, Y] − Ew[fj |x, Y] Ew[cy⋆ |x, Y]. ∂E[e]/∂wj is given by a similar formula. 9 Results We follow Charniak and Johnson (2001) and split the corpus into main training data, held-out training data and test data as follows: main training consisted of all sw[23]*.dps files, held-out training consisted of all sw4[5, 6, 7, 8, 9]*.dps files and test consisted of709 all sw4[0, 1]*.dps files. However, we follow (Johnson and Charniak, 2004) in deleting all partial words and punctuation from the training and test data (they argued that this is more realistic in a speech processing application). Table 1 shows the results for the different models on held-out data. To avoid over-fitting on the test data, we present the f-scores o</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics, pages 118–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri David</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>Fisher English Training Speech Part 1 Transcripts. Published by Linguistic Data Consortium,</title>
<date>2004</date>
<location>Philadelphia.</location>
<contexts>
<context position="17615" citStr="David et al., 2004" startWordPosition="2919" endWordPosition="2922">s and Franz, 2006). This data set, collected by Google Inc., contains English word n-grams and their observed frequency counts. Frequency counts are produced from this billion-token corpus of web text. Because of the noise1 present in this corpus there is an ongoing debate in the scientific community of the use of this corpus for serious language modelling. The Gigaword Corpus (Graff and Cieri, 2003) is a large body of newswire text. The corpus contains 1.6 · 109 tokens, however fluent newswire text is not necessarily of the same domain as disfluency removed speech. The Fisher corpora Part I (David et al., 2004) and Part II (David et al., 2005) are large bodies of transcribed text. Unlike Switchboard there is no disfluency annotation available for Fisher. Together the two Fisher corpora consist of 2.2 · 107 tokens. 7 Features The log-linear reranker, which rescores the 25-best lists produced by the noisy-channel model, can also include additional features besides the noisychannel log probabilities. As we show below, these additional features can make a substantial improvement to disfluency detection performance. Our reranker incorporates two kinds of features. The first are log-probabilities of vario</context>
</contexts>
<marker>David, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri David, David Miller, and Kevin Walker. 2004. Fisher English Training Speech Part 1 Transcripts. Published by Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri David</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>Fisher English Training Speech Part 2 Transcripts. Published by Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<contexts>
<context position="17648" citStr="David et al., 2005" startWordPosition="2926" endWordPosition="2929">, collected by Google Inc., contains English word n-grams and their observed frequency counts. Frequency counts are produced from this billion-token corpus of web text. Because of the noise1 present in this corpus there is an ongoing debate in the scientific community of the use of this corpus for serious language modelling. The Gigaword Corpus (Graff and Cieri, 2003) is a large body of newswire text. The corpus contains 1.6 · 109 tokens, however fluent newswire text is not necessarily of the same domain as disfluency removed speech. The Fisher corpora Part I (David et al., 2004) and Part II (David et al., 2005) are large bodies of transcribed text. Unlike Switchboard there is no disfluency annotation available for Fisher. Together the two Fisher corpora consist of 2.2 · 107 tokens. 7 Features The log-linear reranker, which rescores the 25-best lists produced by the noisy-channel model, can also include additional features besides the noisychannel log probabilities. As we show below, these additional features can make a substantial improvement to disfluency detection performance. Our reranker incorporates two kinds of features. The first are log-probabilities of various scores computed by the noisy-c</context>
</contexts>
<marker>David, Miller, Walker, 2005</marker>
<rawString>Christopher Cieri David, David Miller, and Kevin Walker. 2005. Fisher English Training Speech Part 2 Transcripts. Published by Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward Holliman</author>
</authors>
<date>1997</date>
<booktitle>Switchboard-1 Release 2. Published by Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7140" citStr="Godfrey and Holliman, 1997" startWordPosition="1138" endWordPosition="1141">ves are very fluent. As we will see later, the individual components of a disfluency do not have to be disfluent by themselves. This can occur when a speaker edits her speech for meaning-related reasons, rather than errors that arise from performance. The edit repairs which are the fo-704 The reparandum to Boston is the part of the utterance that is ‘edited out’; the interregnum uh, I mean is a filled pause, which need not always be present; and the repair to Denver replaces the reparandum. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus (Godfrey and Holliman, 1997), the primary corpus for speech disfluency research, but did not propose an actual model of repairs. They found that the overall distribution of speech disfluencies in a large corpus can be fit well by a model that uses only information on a very local level. Our model, as explained in section 5, follows from this observation. As our domain of interest we use the Switchboard corpus. This is a large corpus consisting of transcribed telephone conversations between two partners. In the Treebank III (Marcus et al., 1999) corpus there is annotation available for the Switchboard corpus, which annota</context>
</contexts>
<marker>Godfrey, Holliman, 1997</marker>
<rawString>John J. Godfrey and Edward Holliman. 1997. Switchboard-1 Release 2. Published by Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<title>English gigaword. Published by Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia.</location>
<contexts>
<context position="17399" citStr="Graff and Cieri, 2003" startWordPosition="2880" endWordPosition="2883">d we use the 19 other folds to construct a language model and then score the utterance in this fold with that language model. The largest widely-available corpus for language modelling is the Web 1T 5-gram corpus (Brants and Franz, 2006). This data set, collected by Google Inc., contains English word n-grams and their observed frequency counts. Frequency counts are produced from this billion-token corpus of web text. Because of the noise1 present in this corpus there is an ongoing debate in the scientific community of the use of this corpus for serious language modelling. The Gigaword Corpus (Graff and Cieri, 2003) is a large body of newswire text. The corpus contains 1.6 · 109 tokens, however fluent newswire text is not necessarily of the same domain as disfluency removed speech. The Fisher corpora Part I (David et al., 2004) and Part II (David et al., 2005) are large bodies of transcribed text. Unlike Switchboard there is no disfluency annotation available for Fisher. Together the two Fisher corpora consist of 2.2 · 107 tokens. 7 Features The log-linear reranker, which rescores the 25-best lists produced by the noisy-channel model, can also include additional features besides the noisychannel log prob</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English gigaword. Published by Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
</authors>
<title>Maximum Expected F-Measure Training of Logistic Regression Models.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>692--699</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1641" citStr="Jansche (2005)" startWordPosition="251" endWordPosition="252">ten text language models as opposed to language models based on speech transcripts. Second, we develop a new set of reranker features explicitly designed to capture important properties of speech repairs. Many of these features are lexically grounded and provide a large performance increase. Third, we utilise a loss function, approximate expected f-score, that explicitly targets the asymmetric evaluation metrics used in the disfluency detection task. We explain how to optimise this loss function, and show that this leads to a marked improvement in disfluency detection. This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. Similarly we introduce a loss function based on the edit-f-score in our domain. Abstract Unrehearsed spoken language often contains disfluencies. In order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisych</context>
<context position="24134" citStr="Jansche (2005)" startWordPosition="4069" endWordPosition="4071">r data is so skewed (i.e., “edited” words are comparatively infrequent), we 3In the situation where the true “edited” labelling does not appear in the 25-best list Yi produced by the noisy-channel model, we choose y⋆i to be a labelling in Yi closest to the true labelling. Pw(y |x, Y) = w2j LT(w) + α m X j=1 can improve performance by using an asymmetric loss function. Inspired by our evaluation metric, we devised an approximate expected f-score loss function FLoss. [c] FLossT(w) = 1 − 2Ewg + Ew[e] This approximation assumes that the expectations approximately distribute over the division: see Jansche (2005) and Smith and Eisner (2006) for other approximations to expected f-score and methods for optimising them. We experimented with other asymmetric loss functions (e.g., the expected error rate) and found that they gave very similar results. An advantage of FLoss is that it and its derivatives with respect to w (which are required for numerical optimisation) are easy to calculate exactly. For example, the expected number of correct “edited” words is: Ew[cy⋆ |Yi], where: i �Ew[cy⋆i |Yi] = cy⋆i (y) Pw(y |xi, Yi) y∈Yi and cy⋆(y) is the number of correct “edited” labels in y given the gold labelling </context>
</contexts>
<marker>Jansche, 2005</marker>
<rawString>Martin Jansche. 2005. Maximum Expected F-Measure Training of Logistic Regression Models. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 692–699, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A TAGbased noisy channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--39</pages>
<contexts>
<context position="4737" citStr="Johnson and Charniak (2004)" startWordPosition="742" endWordPosition="746">es appropriate evaluation techniques. In Section 5 we describe the noisy channel model we are using. The next three sections describe the new additions: Section 6 describe the corpora used for language models, Section 7 describes features used in the loglinear model employed by the reranker and Section 8 describes appropriate loss functions which are critical for our approach. We evaluate the new model in Section 9. Section 10 draws up a conclusion. cus of our work typically have this characteristic. Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Johnson et al. (2004) adds some handwritten rules to the noisy channel model and use a maximum entropy approach, providing results comparable to Zhang et al. (2006), which are state-of-the art results. Kahn et al. (2005) investigated the role of prosodic cues in disfluency detection, although the main focus of their work was accurately recovering and parsing a fluent version of the sentence. They report a 0.782 f-score for disfluency detection. 3 Speech Disfluencies We follow the definitions of Shriberg (1994) regarding speech disfluencies. She identifies and </context>
<context position="9491" citStr="Johnson and Charniak (2004)" startWordPosition="1542" endWordPosition="1545"> words proposed by the system on that corpus, and c is the number of the “edited” words proposed by the system that are in fact correct. A perfect classifier which correctly labels every word achieves an f-score of 1, while the trivial baseline classifiers which label every word as “edited” or “not edited” respectively achieve a very low f-score. Informally, the f-score metric focuses more on the “edited” words than it does on the “not edited” words. As we will see in section 8, this has implications for the choice of loss function used to train the classifier. 5 Noisy Channel Model Following Johnson and Charniak (2004), we use a noisy channel model to propose a 25-best list of possible speech disfluency analyses. The choice of this model is driven by the observation that the repairs frequently seem to be a “rough copy” of the reparandum, often incorporating the same or very similar words in roughly the same word order. That705 is, they seem to involve “crossed” dependencies between the reparandum and the repair. Example (3) shows the crossing dependencies. As this example also shows, the repair often contains many of the same words that appear in the reparandum. In fact, in our Switchboard training corpus w</context>
<context position="13585" citStr="Johnson and Charniak (2004)" startWordPosition="2245" endWordPosition="2248">, the number of possible source sentences, X, grows exponentially with the length of Y , so exhaustive search is infeasible. Tree Adjoining Grammars (TAG) provide a systematic way of formalising the channel model, and their polynomialtime dynamic programming parsing algorithms can be used to search for likely repairs, at least when used with simple language models like a bigram language model. In this paper we first identify the 25 most likely analyses of each sentence using the TAG channel model together with a bigram language model. Further details of the noisy channel model can be found in Johnson and Charniak (2004). 5.4 Reranker To improve performance over the standard noisy channel model we use a reranker, as previously suggest by Johnson and Charniak (2004). We rerank a 25-best list of analyses. This choice is motivated by an oracle experiment we performed, probing for the location of the best analysis in a 100-best list. This706 experiment shows that in 99.5% of the cases the best analysis is located within the first 25, and indicates that an f-score of 0.958 should be achievable as the upper bound on a model using the first 25 best analyses. We therefore use the top 25 analyses from the noisy channe</context>
<context position="25346" citStr="Johnson and Charniak, 2004" startWordPosition="4278" endWordPosition="4282">gold labelling y⋆. The derivatives of FLoss are: ∂FLossT ∂wj (w) = 1 FLossT (w)∂Ew[e] − 2∂Ew[c] g + Ew[e] ∂wj ∂wj where: ∂Ew[c] n ∂Ew[cy⋆ i |xi, Yi] = ∂wj i=1 ∂wj ∂Ew[cy⋆ |x, Y] = ∂wj Ew[fjcy⋆ |x, Y] − Ew[fj |x, Y] Ew[cy⋆ |x, Y]. ∂E[e]/∂wj is given by a similar formula. 9 Results We follow Charniak and Johnson (2001) and split the corpus into main training data, held-out training data and test data as follows: main training consisted of all sw[23]*.dps files, held-out training consisted of all sw4[5, 6, 7, 8, 9]*.dps files and test consisted of709 all sw4[0, 1]*.dps files. However, we follow (Johnson and Charniak, 2004) in deleting all partial words and punctuation from the training and test data (they argued that this is more realistic in a speech processing application). Table 1 shows the results for the different models on held-out data. To avoid over-fitting on the test data, we present the f-scores over held-out training data instead of test data. We used the held-out data to select the best-performing set of reranker features, which consisted of features for all of the language models plus the extended (i.e., indicator) features, and used this model to analyse the test data. The fscore of this model on</context>
<context position="28707" citStr="Johnson and Charniak (2004)" startWordPosition="4841" endWordPosition="4844">ion f-score on held-out data for a variety of language models and loss functions ported in literature operating on identical data, even though we use vastly less features than other do. data, even though we use vastly fewer features than others do. 10 Conclusion and Future work We have described a disfluency detection algorithm which we believe improves upon current state-ofthe-art competitors. This model is based on a noisy channel model which scores putative analyses with a language model; its channel model is inspired by the observation that reparandum and repair are often very similar. As Johnson and Charniak (2004) noted, although this model performs well, a loglinear reranker can be used to increase performance. We built language models from a variety of speech and non-speech corpora, and examine the effect they have on disfluency detection. We use language models derived from different larger corpora effectively in a maximum reranker setting. We show that the actual choice for a language model seems to be less relevant and newswire text can be used equally well for modelling fluent speech. We describe different features to improve disfluency detection even further. Especially these features seem to bo</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A TAGbased noisy channel model of speech repairs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 33–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
<author>Matthew Lease</author>
</authors>
<title>An Improved Model for Recognizing Disfluencies in Conversational Speech.</title>
<date>2004</date>
<booktitle>In Proceedings of the Rich Transcription Fall Workshop.</booktitle>
<contexts>
<context position="4792" citStr="Johnson et al. (2004)" startWordPosition="753" endWordPosition="756">e the noisy channel model we are using. The next three sections describe the new additions: Section 6 describe the corpora used for language models, Section 7 describes features used in the loglinear model employed by the reranker and Section 8 describes appropriate loss functions which are critical for our approach. We evaluate the new model in Section 9. Section 10 draws up a conclusion. cus of our work typically have this characteristic. Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Johnson et al. (2004) adds some handwritten rules to the noisy channel model and use a maximum entropy approach, providing results comparable to Zhang et al. (2006), which are state-of-the art results. Kahn et al. (2005) investigated the role of prosodic cues in disfluency detection, although the main focus of their work was accurately recovering and parsing a fluent version of the sentence. They report a 0.782 f-score for disfluency detection. 3 Speech Disfluencies We follow the definitions of Shriberg (1994) regarding speech disfluencies. She identifies and defines three distinct parts of a speech disfluency, re</context>
</contexts>
<marker>Johnson, Charniak, Lease, 2004</marker>
<rawString>Mark Johnson, Eugene Charniak, and Matthew Lease. 2004. An Improved Model for Recognizing Disfluencies in Conversational Speech. In Proceedings of the Rich Transcription Fall Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Mari Ostendorf</author>
</authors>
<title>Effective Use of Prosody in Parsing Conversational Speech.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>233--240</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4991" citStr="Kahn et al. (2005)" startWordPosition="787" endWordPosition="790">del employed by the reranker and Section 8 describes appropriate loss functions which are critical for our approach. We evaluate the new model in Section 9. Section 10 draws up a conclusion. cus of our work typically have this characteristic. Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Johnson et al. (2004) adds some handwritten rules to the noisy channel model and use a maximum entropy approach, providing results comparable to Zhang et al. (2006), which are state-of-the art results. Kahn et al. (2005) investigated the role of prosodic cues in disfluency detection, although the main focus of their work was accurately recovering and parsing a fluent version of the sentence. They report a 0.782 f-score for disfluency detection. 3 Speech Disfluencies We follow the definitions of Shriberg (1994) regarding speech disfluencies. She identifies and defines three distinct parts of a speech disfluency, referred to as the reparandum, the interregnum and the repair. Consider the following utterance: uh, I mean |{z } interregnum to Denver |{z } repair reparandum z } |{ I want a flight to Boston, (1) on </context>
</contexts>
<marker>Kahn, Lease, Charniak, Johnson, Ostendorf, 2005</marker>
<rawString>Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective Use of Prosody in Parsing Conversational Speech. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 233–240, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="15640" citStr="Kneser and Ney, 1995" startWordPosition="2584" endWordPosition="2587">nguage models are likely to be strongly correlated, even if the language models are trained on different corpora. This motivates the choice for log-linear learners, which are built to handle features which are not necessarily independent. We incorporate information from the external language models by defining a reranker feature for each external language model. The value of this feature is the log probability assigned by the language model to the candidate underlying fluent substring X For each of our corpora (including Switchboard) we built a 4-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995). For each analysis we calculate the probability under that language model for the candidate underlying fluent substring X. We use this log probability as a feature in the reranker. We use the SRILM toolkit (Stolcke, 2002) both for estimating the model from the training corpus as well as for computing the probabilities of the underlying fluent sentences X of the different analysis. As previously described, Switchboard is our primary corpus for our model. The language model part of the noisy channel model already uses a bigram language model based on Switchboard, but in the reranker we would li</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 181– 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank-3. Published by Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7662" citStr="Marcus et al., 1999" startWordPosition="1231" endWordPosition="1234">ed the location and distribution of repairs in the Switchboard corpus (Godfrey and Holliman, 1997), the primary corpus for speech disfluency research, but did not propose an actual model of repairs. They found that the overall distribution of speech disfluencies in a large corpus can be fit well by a model that uses only information on a very local level. Our model, as explained in section 5, follows from this observation. As our domain of interest we use the Switchboard corpus. This is a large corpus consisting of transcribed telephone conversations between two partners. In the Treebank III (Marcus et al., 1999) corpus there is annotation available for the Switchboard corpus, which annotates which parts of utterances are in a reparandum, interregnum or repair. 4 Evaluation metrics for disfluency detection systems Disfluency detection systems like the one described here identify a subset of the word tokens in each transcribed utterance as “edited” or disfluent. Perhaps the simplest way to evaluate such systems is to calculate the accuracy of labelling they produce, i.e., the fraction of words that are correctly labelled (i.e., either “edited” or “not edited”). However, as Charniak and Johnson (2001) o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3. Published by Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Broad-Coverage Parsing using Human-Like Memory Constraints.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="5722" citStr="Schuler et al. (2010)" startWordPosition="906" endWordPosition="909">urately recovering and parsing a fluent version of the sentence. They report a 0.782 f-score for disfluency detection. 3 Speech Disfluencies We follow the definitions of Shriberg (1994) regarding speech disfluencies. She identifies and defines three distinct parts of a speech disfluency, referred to as the reparandum, the interregnum and the repair. Consider the following utterance: uh, I mean |{z } interregnum to Denver |{z } repair reparandum z } |{ I want a flight to Boston, (1) on Friday 2 Related work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees which it considers to be made up of disfluent material. Although they are interested not only in disfluency but also a syntactic analysis of the utterance, including the disfluencies being analysed, their model’s final f-score for disfluency detection is lower than that of other models. Snover et al. (2004) investigate the use of purely lexical features combined with part-of-speech tags to detect disfluencies. This approach is compar</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-Coverage Parsing using Human-Like Memory Constraints. Computational Linguistics, 36(1):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Schwartz</author>
<author>Long Nguyen</author>
<author>Francis Kubala</author>
<author>George Chou</author>
<author>George Zavaliagkos</author>
<author>John Makhoul</author>
</authors>
<title>On Using Written Language Training Data for Spoken Language Modeling.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<pages>94--98</pages>
<contexts>
<context position="14681" citStr="Schwartz et al. (1994)" startWordPosition="2432" endWordPosition="2435">ble as the upper bound on a model using the first 25 best analyses. We therefore use the top 25 analyses from the noisy channel model in the remainder of this paper and use a reranker to choose the most suitable candidate among these. 6 Corpora for language modelling We would like to use additional data to model the fluent part of spoken language. However, the Switchboard corpus is one of the largest widelyavailable disfluency-annotated speech corpora. It is reasonable to believe that for effective disfluency detection Switchboard is not large enough and more text can provide better analyses. Schwartz et al. (1994), although not focusing on disfluency detection, show that using written language data for modelling spoken language can improve performance. We turn to three other bodies of text and investigate the use of these corpora for our task, disfluency detection. We will describe these corpora in detail here. The predictions made by several language models are likely to be strongly correlated, even if the language models are trained on different corpora. This motivates the choice for log-linear learners, which are built to handle features which are not necessarily independent. We incorporate informat</context>
</contexts>
<marker>Schwartz, Nguyen, Kubala, Chou, Zavaliagkos, Makhoul, 1994</marker>
<rawString>Richard Schwartz, Long Nguyen, Francis Kubala, George Chou, George Zavaliagkos, and John Makhoul. 1994. On Using Written Language Training Data for Spoken Language Modeling. In Proceedings of the Human Language Technology Workshop, pages 94–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
</authors>
<title>How far do speakers back up in repairs? A quantitative711 model.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>2183--2186</pages>
<contexts>
<context position="7036" citStr="Shriberg and Stolcke (1998)" startWordPosition="1122" endWordPosition="1125">. However, the authors note that this model finds it difficult to identify disfluencies which by themselves are very fluent. As we will see later, the individual components of a disfluency do not have to be disfluent by themselves. This can occur when a speaker edits her speech for meaning-related reasons, rather than errors that arise from performance. The edit repairs which are the fo-704 The reparandum to Boston is the part of the utterance that is ‘edited out’; the interregnum uh, I mean is a filled pause, which need not always be present; and the repair to Denver replaces the reparandum. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus (Godfrey and Holliman, 1997), the primary corpus for speech disfluency research, but did not propose an actual model of repairs. They found that the overall distribution of speech disfluencies in a large corpus can be fit well by a model that uses only information on a very local level. Our model, as explained in section 5, follows from this observation. As our domain of interest we use the Switchboard corpus. This is a large corpus consisting of transcribed telephone conversations between two partners. In the Treebank</context>
</contexts>
<marker>Shriberg, Stolcke, 1998</marker>
<rawString>Elizabeth Shriberg and Andreas Stolcke. 1998. How far do speakers back up in repairs? A quantitative711 model. In Proceedings of the International Conference on Spoken Language Processing, pages 2183– 2186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disuencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="5286" citStr="Shriberg (1994)" startWordPosition="834" endWordPosition="835">etection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Johnson et al. (2004) adds some handwritten rules to the noisy channel model and use a maximum entropy approach, providing results comparable to Zhang et al. (2006), which are state-of-the art results. Kahn et al. (2005) investigated the role of prosodic cues in disfluency detection, although the main focus of their work was accurately recovering and parsing a fluent version of the sentence. They report a 0.782 f-score for disfluency detection. 3 Speech Disfluencies We follow the definitions of Shriberg (1994) regarding speech disfluencies. She identifies and defines three distinct parts of a speech disfluency, referred to as the reparandum, the interregnum and the repair. Consider the following utterance: uh, I mean |{z } interregnum to Denver |{z } repair reparandum z } |{ I want a flight to Boston, (1) on Friday 2 Related work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees w</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disuencies. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum Risk Annealing for Training Log-Linear Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>787--794</pages>
<contexts>
<context position="1669" citStr="Smith and Eisner (2006)" startWordPosition="254" endWordPosition="257">odels as opposed to language models based on speech transcripts. Second, we develop a new set of reranker features explicitly designed to capture important properties of speech repairs. Many of these features are lexically grounded and provide a large performance increase. Third, we utilise a loss function, approximate expected f-score, that explicitly targets the asymmetric evaluation metrics used in the disfluency detection task. We explain how to optimise this loss function, and show that this leads to a marked improvement in disfluency detection. This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. Similarly we introduce a loss function based on the edit-f-score in our domain. Abstract Unrehearsed spoken language often contains disfluencies. In order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisychannel model. We show that la</context>
<context position="24162" citStr="Smith and Eisner (2006)" startWordPosition="4073" endWordPosition="4076"> (i.e., “edited” words are comparatively infrequent), we 3In the situation where the true “edited” labelling does not appear in the 25-best list Yi produced by the noisy-channel model, we choose y⋆i to be a labelling in Yi closest to the true labelling. Pw(y |x, Y) = w2j LT(w) + α m X j=1 can improve performance by using an asymmetric loss function. Inspired by our evaluation metric, we devised an approximate expected f-score loss function FLoss. [c] FLossT(w) = 1 − 2Ewg + Ew[e] This approximation assumes that the expectations approximately distribute over the division: see Jansche (2005) and Smith and Eisner (2006) for other approximations to expected f-score and methods for optimising them. We experimented with other asymmetric loss functions (e.g., the expected error rate) and found that they gave very similar results. An advantage of FLoss is that it and its derivatives with respect to w (which are required for numerical optimisation) are easy to calculate exactly. For example, the expected number of correct “edited” words is: Ew[cy⋆ |Yi], where: i �Ew[cy⋆i |Yi] = cy⋆i (y) Pw(y |xi, Yi) y∈Yi and cy⋆(y) is the number of correct “edited” labels in y given the gold labelling y⋆. The derivatives of FLoss</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum Risk Annealing for Training Log-Linear Models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>A Lexically-Driven Algorithm for Disfluency Detection.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technologies and North American Association for Computational Linguistics,</booktitle>
<pages>157--160</pages>
<contexts>
<context position="6193" citStr="Snover et al. (2004)" startWordPosition="984" endWordPosition="987">to Boston, (1) on Friday 2 Related work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees which it considers to be made up of disfluent material. Although they are interested not only in disfluency but also a syntactic analysis of the utterance, including the disfluencies being analysed, their model’s final f-score for disfluency detection is lower than that of other models. Snover et al. (2004) investigate the use of purely lexical features combined with part-of-speech tags to detect disfluencies. This approach is compared to approaches which use primarily prosodic cues, and appears to perform equally well. However, the authors note that this model finds it difficult to identify disfluencies which by themselves are very fluent. As we will see later, the individual components of a disfluency do not have to be disfluent by themselves. This can occur when a speaker edits her speech for meaning-related reasons, rather than errors that arise from performance. The edit repairs which are t</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2004</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2004. A Lexically-Driven Algorithm for Disfluency Detection. In Proceedings of Human Language Technologies and North American Association for Computational Linguistics, pages 157–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="15862" citStr="Stolcke, 2002" startWordPosition="2623" endWordPosition="2624">ependent. We incorporate information from the external language models by defining a reranker feature for each external language model. The value of this feature is the log probability assigned by the language model to the candidate underlying fluent substring X For each of our corpora (including Switchboard) we built a 4-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995). For each analysis we calculate the probability under that language model for the candidate underlying fluent substring X. We use this log probability as a feature in the reranker. We use the SRILM toolkit (Stolcke, 2002) both for estimating the model from the training corpus as well as for computing the probabilities of the underlying fluent sentences X of the different analysis. As previously described, Switchboard is our primary corpus for our model. The language model part of the noisy channel model already uses a bigram language model based on Switchboard, but in the reranker we would like to also use 4-grams for reranking. Directly using Switchboard to build a 4- gram language model is slightly problematic. When we use the training data of Switchboard both for language fluency prediction and the same tra</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Fuliang Weng</author>
<author>Zhe Feng</author>
</authors>
<title>A progressive feature selection algorithm for ultra large feature spaces.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>561--568</pages>
<contexts>
<context position="3596" citStr="Zhang et al. (2006)" startWordPosition="546" endWordPosition="549">ms (e.g., “I mean”), parenthetical asides and repairs. Of these, repairs pose particularly difficult problems for parsing and related Natural Language Processing (NLP) tasks. This paper presents a model of disfluency detection based on the noisy channel framework, which703 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 703–711, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Together, these three improvements are enough to boost detection performance to a higher f-score than previously reported in literature. Zhang et al. (2006) investigate the use of ‘ultra large feature spaces’ as an aid for disfluency detection. Using over 19 million features, they report a final f-score in this task of 0.820. Operating on the same body of text (Switchboard), our work leads to an f-score of 0.838, this is a 9% relative improvement in residual f-score. The remainder of this paper is structured as follows. First in Section 2 we describe related work. Then in Section 3 we present some background on disfluencies and their structure. Section 4 describes appropriate evaluation techniques. In Section 5 we describe the noisy channel model</context>
<context position="4935" citStr="Zhang et al. (2006)" startWordPosition="778" endWordPosition="781">ls, Section 7 describes features used in the loglinear model employed by the reranker and Section 8 describes appropriate loss functions which are critical for our approach. We evaluate the new model in Section 9. Section 10 draws up a conclusion. cus of our work typically have this characteristic. Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Johnson et al. (2004) adds some handwritten rules to the noisy channel model and use a maximum entropy approach, providing results comparable to Zhang et al. (2006), which are state-of-the art results. Kahn et al. (2005) investigated the role of prosodic cues in disfluency detection, although the main focus of their work was accurately recovering and parsing a fluent version of the sentence. They report a 0.782 f-score for disfluency detection. 3 Speech Disfluencies We follow the definitions of Shriberg (1994) regarding speech disfluencies. She identifies and defines three distinct parts of a speech disfluency, referred to as the reparandum, the interregnum and the repair. Consider the following utterance: uh, I mean |{z } interregnum to Denver |{z } rep</context>
</contexts>
<marker>Zhang, Weng, Feng, 2006</marker>
<rawString>Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A progressive feature selection algorithm for ultra large feature spaces. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 561–568.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>