<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999292">
The Computational Complexity of the
Correct-Prefix Property for TAGs
</title>
<author confidence="0.950604">
Mark-Jan Nederhof*
</author>
<bodyText confidence="0.836081857142857">
German Research Center for Artificial
Intelligence
A new upper bound is presented for the computational complexity of the parsing problem for
TAGs, under the constraint that input is read from left to right in such a way that errors in the
input are observed as soon as possible, which is called the &amp;quot;correct-prefix property.&amp;quot; The former
upper bound, 0(n9), is now improved to 0(n6), which is the same as that of practical parsing
algorithms for TAGs without the additional constraint of the correct-prefix property.
</bodyText>
<sectionHeader confidence="0.994271" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999194666666667">
Traditionally, parsers and recognizers for regular and context-free languages process
input from left to right. If a syntax error occurs in the input they often detect that
error immediately after its position is reached. The position of the syntax error can
be defined as the rightmost symbol of the shortest prefix of the input that cannot be
extended to be a correct sentence in the language L.
In formal notation, this prefix for a given erroneous input w L is defined as the
string va, where w = vax, for some x, such that vy E L, for some y, but vaz L, for
any z. (The symbols v, w, ... denote strings, and a denotes an input symbol.) The
occurrence of a in w indicates the error position.
If the error is detected as soon as it is reached, then all prefixes of the input that
have been processed at preceding stages are correct prefixes, or more precisely, they are
prefixes of some correct strings in the language. Hence, we speak of the correct-prefix
property.&apos;
An important application can be found in the area of grammar checking: upon
finding an ungrammatical sentence in a document, a grammar checker may report to
the user the presumed position of the error, obtained from a parsing algorithm with
the correct-prefix property.
For context-free and regular languages, the correct-prefix property can be satis-
fied without additional costs of space or time. Surprisingly, it has been claimed by
Schabes and Waters (1995) that this property is problematic for the mildly context-
sensitive languages represented by tree-adjoining grammars (TAGs): the best practical
parsing algorithms for TAGs have time complexity 0 (n6 ) (Vijay-Shankar and Joshi
[1985]; see Satta [1994] and Rajasekaran and Yooseph [19951 for lower theoretical upper
bounds), whereas the only published algorithm with the correct-prefix property—that
by Schabes and Joshi (1988)—has complexity 0 (n9 ) .
In this paper we present an algorithm that satisfies the correct-prefix property and
operates in 0(n6) time. This algorithm merely recognizes input, but it can be extended
</bodyText>
<footnote confidence="0.979476666666667">
* DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de
1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid
prefix property is used.
</footnote>
<note confidence="0.8312365">
C) 1999 Association for Computational Linguistics
Computational Linguistics Volume 25, Number 3
</note>
<bodyText confidence="0.999631888888889">
to be a parsing algorithm with the ideas from Schabes (1994), which also suggest how
it can be extended to handle substitution in addition to adjunction. The complexity
results carry over to linear indexed grammars, combinatory categorial grammars, and
head grammars, since these formalisms are equivalent to TAGs (Vijay-Shanker and
Weir 1993, 1994).
We present the actual algorithm in Section 3, after the necessary notation has been
discussed in Section 2. The correctness proofs are discussed in Section 4, and the time
complexity in Section 5. The ideas in this paper give rise to a number of questions for
further research, as discussed in Section 6.
</bodyText>
<sectionHeader confidence="0.985364" genericHeader="keywords">
2. Definitions
</sectionHeader>
<bodyText confidence="0.982337256410256">
Our definition of TAGs simplifies the explanation of the algorithm, but differs slightly
from standard treatment such as that of Joshi (1987).
A tree-adjoining grammar is a 4-tuple (E, NT, I, A), where E is the set of terminals,
I is the set of initial trees, and A is the set of auxiliary trees. We refer to the trees in
/ U A as elementary trees. The set NT, the set of nonterminals, does not play any role
in this paper.
We refer to the root of an elementary tree t as R. Each auxiliary tree has exactly
one distinguished leaf, which is called the foot. We refer to the foot of an auxiliary
tree t as F.
We use variables N and M to range over nodes in elementary trees. We assume
that the sets of nodes belonging to distinct elementary trees are pairwise disjoint.
For each leaf N in an elementary tree, except when it is a foot, we define label(N)
to be the label of the node, which is either a terminal from E or the empty string c.
For all other nodes, label is undefined.
For each node N that is not a leaf or that is a foot, Adj(N) is the set of auxiliary trees
that can be adjoined at N, plus possibly the special element nil. For all other nodes,
Adj is undefined. If a set Adj(N) contains nil, then this indicates that adjunction at N
is not obligatory.
For each nonleaf node N we define children(N) as the (nonempty) list of daughter
nodes. For all other nodes, children is undefined. An example of a TAG is given in
Figure 1.
The language described by a TAG is given by the set of strings that are the yields
of derived trees. A derived tree is obtained from an initial tree by performing the
following operation on each node N, except when it is a leaf: The tree is excised at N,
and between the two halves a fresh instance of an auxiliary tree, which is taken from
the set Adj(N), is inserted, or the element nil is taken from Adj(N), in which case no
new nodes are added to the tree. Insertion of the new auxiliary tree, which from now
on will be called adjunction, is done in such a way that the bottom half of the excised
tree is connected to the foot of the auxiliary tree. The new nodes that are added to the
tree as a result are recursively subjected to the same operation. This process ends in a
complete derived tree once all nodes have been treated.
An example of the derivation of a string is given in Figure 2. We start with initial
tree al and treat Rai, for which we find Adj(Rai) = {b2, nil} . We opt to select nil,
so that no new nodes are added. However in the figure we do split Rai in order to
mark it as having been treated. Next we treat 4, and we opt to adjoin bl, taken
from Adj(Nali) = {131, b3}. After another &amp;quot;nil-adjunction&amp;quot; at Rbi, we adjoin b2 at 4.
Note that this is an obligatory adjunction, since Adj(N1 ) does not contain nil. Some
more nil-adjunctions lead to a derived tree with yield acdb, which is therefore in the
language described by the TAG.
</bodyText>
<page confidence="0.996206">
346
</page>
<figure confidence="0.9610901875">
Correct-Prefix Property for TAGs
Nederhof
Initial trees Auxiliary trees
Fbl
(b1)
a
Ne3
b c
Adj (Rai) = Ad] (Ra2) Adj (RH) = Ad.] (Rb2) =
{b2, nil} {b3, nil} {bl, b2, nil} {b3, nil}
Ad.) (N2-,1 ) = Ad.) (N2) = Adj(4) = Ad] (Fb2) =
{bl, b3} {bl, b21 {b2} {nil}
Adj (NZ2) = Adj (Fbi) =
{nil} {nil}
(al) (a2)
Rai
Adj(Rb3) = {bl, nil}
Adi (N3) = {b2}
Ad/ (N13) = {b3, nil}
Adj(Fb3) {nil}
Figure 1
A tree-adjoining grammar.
a
1
bl
Rb
Fb1
adjoin
bl at ArL
1
al
Rai nil-
adjunction
N11
a
Initial tree:
a
Derived
tree:
three nil-
adj unctions
a
adjoin
b2 at Art1,1
Fbl
I nil-
adjunction
Fbi
</figure>
<figureCaption confidence="0.987099">
Figure 2
</figureCaption>
<subsectionHeader confidence="0.488412">
Derivation of the string acdb.
</subsectionHeader>
<bodyText confidence="0.931677333333333">
In order to avoid cluttering the picture with details, we have omitted the names
of nodes at which (nil-)adjunction has been applied. We will reintroduce these names
later. A further point worth mentioning is that here we treat the nodes in preorder: we
traverse the tree top-down and left-to-right, and perform adjunction at each node the
first time it is encountered.2 Any other strategy would lead to the same set of derived
trees, but we chose preorder treatment since this matches the algorithm we present
below.
2 The tree that is being traversed grows in size during the traversal, contrary to traditional usage of the
notion of &amp;quot;traversal.&amp;quot;
</bodyText>
<page confidence="0.992106">
347
</page>
<note confidence="0.415974">
Computational Linguistics Volume 25, Number 3
</note>
<sectionHeader confidence="0.590429" genericHeader="introduction">
3. The Algorithm
</sectionHeader>
<bodyText confidence="0.981502340425532">
The input to the recognition algorithm is given by the string a1a2 ... an, where n is the
length of the input. Integers i such that 0 &lt; i &lt; n will be used to indicate &amp;quot;positions&amp;quot;
in the input string. Where we refer to the input between positions i and j we mean
the string a,±1...a1.
The algorithm operates by means of least fixed-point iteration: a table is gradually
filled with elements derived from other elements, until no more new ones can be found.
A number of &amp;quot;steps&amp;quot; indicate how table elements are to be derived from others.3
For the description of the steps we use a pseudoformal notation. Each step consists
of a list of antecedents and a consequent. The antecedents are the conditions under
which an incarnation of the step is executed. The consequent is a new table element
that the step then adds to the parse table, unless of course it is already present. An
antecedent may be a table element, in which case the condition that it represents is
membership in the table.
The main table elements, or items, are 6-tuples [h, N —&gt; a • 0, i, j, fi, f2]. Here, N
is a node from some elementary tree t, and at3 is the list of the daughter nodes of N.
The daughters in a together generate the input between positions i and j. The whole
elementary tree generates input from position h onwards.
Internal to the elementary tree, there may be adjunctions; in fact, the traversal of
the tree (implying (nil-)adjunctions at all nodes) has been completed up to the end
of a. Furthermore, tree t may itself be an auxiliary tree, in which case it is adjoined
in another tree. Then, the foot may be dominated by one of the daughters in a, and
the foot generates the part of the input between positions fi and f2. When the tree is
not an auxiliary tree, or when the foot is not dominated by one of the daughters in a,
then fi and f2 both have the dummy value &amp;quot;—&amp;quot;.
Whether t is an initial or an auxiliary tree, it is part of a derived tree of which
everything to the left of the end of a generates the input between positions 0 and j.
The traversal has been completed up to the end of a.
See Figure 3 for an illustration of the meaning of items. We assume Rt and Ft
are the root and foot of the elementary tree t to which N belongs; Ft may not exist,
as explained above. R is the root of some initial tree. The solid lines indicate what
has been established; the dashed lines indicate what is merely predicted. If Ft ex-
ists, the subtree below Ft indicates the lower half of the derived tree in which t was
adjoined.
The shaded areas labeled by I, II, and III have not yet been traversed. In particular
it has not yet been established that these parts of the derived tree together generate
the input between positions j and n.
For technical reasons, we assume an additional node for each elementary tree t,
which we denote by T. This node has only one daughter, viz, the actual root node R.
We also assume an additional node for each auxiliary tree t, which we denote by I.
This is the unique daughter of the actual foot node Ft; we set children(Ft) = 1.
In summary, an item indicates how a part of an elementary tree contributes to the
recognition of some derived tree.
Figure 4 illustrates the items needed for recognition of the derived tree from the
running example. We have simplified the notation of items by replacing the names of
leaves (other than foot nodes) by their labels.
3 A &amp;quot;step&amp;quot; is more accurately called an &amp;quot;inference rule&amp;quot; in the literature on deductive parsing (Shieber,
Schabes, and Pereira 1995). For the sake of convenience we will apply the shorter term.
</bodyText>
<page confidence="0.974685">
348
</page>
<figure confidence="0.9348875">
Nederhof Correct-Prefix Property for TAGs
0 h i f2 j
Figure 3
An item [h, N ---+ a • 13, i, j, f2].
a
a c
0 1 2 3
1: T • Rai, 0, 0, -] =
</figure>
<listItem confidence="0.63586275">
2: [0, Rai. -4 • a Ncia , 0, 0, -] =
3: [0, Rai a • Nall, 0, 1, -] -=-
4: [1, T • Rbi, 1,1,-,-]=
5: [1, Rbi -) • N1 Fb1, 1,1,
</listItem>
<equation confidence="0.818063132075472">
6: [1,T--*• Rb2, 1, 1, -7 =
7: [1, Rb2 --&gt; • Fb2 d, 1,1,-, -1 =
8: [1, Fb2 -&gt; • 1, 1,1,-,-]
9: [1,N1 -&gt; • c, 1, 1, -, -]
20 10: [1, N1 c •, 1, 2, -, -]
11: [1, Fb2 -+1., 1,2, 1, 2]
12: [1, Rb2 -) Fb2 • d, 1, 2, 1, 2] =
16_09 13: [1, Rb2 Fb2 d 1, 3, 1, 2] =
14: [1,T Rb2 1,3,1,2]
17 18 15a: [AY,i c 1, 3,-, -]
15: [1, Rbi ---&gt; • Fbi, 1,3,-,-]=
16: [1, Fbi -4 • 3, 3, -, -1
17: [0, N,1]. -&gt; • b, 3,3, -, -]
18: [0,N1 -&gt;b., 3, 4, -, --]
19: [1, Fin -? I to, 3,4,3,4]
20: [1, Rbi Fbi 0, 1, 4, 3, 4] =
21: [1,T-*1, 4, 3, 4]
22a: [N1 b 1, 4, -, -]
22: [0, Rai -+ a Nall •, 0, 4, -, -1 =
4 23: [0, T Rai 41, 0, 4, -, -] =-
(mit)
(Pred 2)1
(Scan 1)2
(Pred 1)3
(Pred 2)4
(Pred 1)5
(Pred 2)6
(Pred 2)7
(Pred 3)8 + 5
(Scan 1)9
(Comp 1)10 + 8 + 5
(Comp 2)11 + 7
(Scan 1)12
(Comp 2)13+ 6
(Adj 0)14+ 10
(Adj 2)15a + 5
(Pred 2)15
(Pred 3)16 + 3
(Scan 1)17
(Comp 1)18 + 16 + 3
(Comp 2)19 + 15
(Comp 2)20 + 4
(Adj 0)21 + 18
(Adj 2)22a + 3
(Comp 3)22 + 1
6 14
n„ rINRb2
&apos; 62 ,,vb212 13
yFb2
8±11
9 10
bl
Vi
</equation>
<figureCaption confidence="0.859188">
Figure 4
</figureCaption>
<bodyText confidence="0.963082">
The items needed for recognition of a derived tree.
There is one special kind of item, with only five fields instead of six. This is
used as an intermediate result in the adjunctor steps to be discussed in Section
3.5.
</bodyText>
<page confidence="0.985122">
349
</page>
<figure confidence="0.849879428571429">
Computational Linguistics Volume 25, Number 3
0
mit
Figure 5
The initialization.
h i f2 j j+1
Scan 1
</figure>
<figureCaption confidence="0.916629">
Figure 6
</figureCaption>
<bodyText confidence="0.81629">
The first scanner step.
</bodyText>
<subsectionHeader confidence="0.992282">
3.1 Initializer
</subsectionHeader>
<bodyText confidence="0.989494">
The initializer step predicts initial trees t starting at position 0; see Figure 5.
</bodyText>
<equation confidence="0.9980885">
t E I
[0, T —&gt; • Rt, 0, 0, —, —]
</equation>
<bodyText confidence="0.985297">
For the running example, item 1 in Figure 4 results from this step.
</bodyText>
<subsectionHeader confidence="0.999324">
3.2 Scanner
</subsectionHeader>
<bodyText confidence="0.972310307692308">
The scanner steps try to shift the dot rightward in case the next node in line is labeled
with a terminal or €, which means the node is a leaf but not a foot. Figure 6 sketches
the situation with respect to the input positions mentioned in the step. The depicted
structure is part of at least one derived tree consistent with the input between positions
0 and j + 1, as explained earlier.
[h, N —&gt; a • MO, f2], (Scan 1)
label(M)= ai±i
[h, N —&gt; aM • /3, i, j +1, f2]
[h, N a • MO, i, j, fi, f2], (Scan 2)
label(M)= c
[h, N —&gt; aM• i13, i, j, fi, f21
For the running example in Figure 4, Scan 1 derives, among others, item 3 from
item 2, and item 13 from item 12.
</bodyText>
<page confidence="0.936124">
350
</page>
<figure confidence="0.90904975">
Correct-Prefix Property for TAGs
Nederhof
h i j h if h if k
Pred 1 Pred 2 Pred 3
</figure>
<figureCaption confidence="0.997728">
Figure 7
</figureCaption>
<bodyText confidence="0.516449">
The three predictor steps.
</bodyText>
<subsectionHeader confidence="0.948674">
3.3 Predictor
</subsectionHeader>
<bodyText confidence="0.81270875">
The first predictor step predicts a fresh occurrence of an auxiliary tree t, indicated in
Figure 7. The second predicts a list of daughters -y lower down in the tree, abstaining
from adjunction at the current node M. The third predicts the lower half of a tree in
which the present tree t was adjoined.
</bodyText>
<equation confidence="0.952917">
[h, N —&gt; a • MO, f2],
t E Adj(M)
[j, T •Rt, j, j, —]
</equation>
<table confidence="0.912963333333333">
[h, N —4. a • MO, i, j, f2], (Pred 2)
nil E Adj(M), (Pred 3)
children(M)= -y
M *7, j, it I
[j, Ft —&gt; • ±, k, k, —1,
[14 N—&gt; a • MO, i, j, f2],
t E Adj(M),
children(M) = -y
[h, M—* • k, k, H
</table>
<bodyText confidence="0.997073">
For the running example, Pred 1 derives item 4 from item 3 and item 6 from
item 5. Pred 2 derives, among others, item 5 from item 4. Pred 3 derives item 9 from
items 8 and 5, and item 17 from items 16 and 3.
</bodyText>
<subsectionHeader confidence="0.971324">
3.4 Completer
</subsectionHeader>
<bodyText confidence="0.99991575">
The first completer step completes recognition of the lower half of a tree in which an
auxiliary tree t was adjoined, and asserts recognition of the foot of t; see Figure 8. The
second and third completer steps complete recognition of a list of daughter nodes
and initiate recognition of the list of nodes 3 to the right of the mother node of -y.
</bodyText>
<equation confidence="0.986548571428571">
[h, M —&gt; &apos;y•, k, 1,
t E Adj(M),
U, Ft —+ • _L., k, k, —],
[h, N a • MO, i.. j, f2]
[j, Ft —, 1., k, 1, k, 1]
(Pred 1)
(Comp 1)
</equation>
<page confidence="0.987004">
351
</page>
<figure confidence="0.964957">
Computational Linguistics Volume 25, Number 3
hijk 1 hij A f2k
Comp 1 Comp 2
</figure>
<figureCaption confidence="0.991483">
Figure 8
</figureCaption>
<bodyText confidence="0.936653333333333">
Two of the completer steps.
[h, M —&gt; *, j, (Comp 2)
[h, N a • MO, i, j, —], (Comp 3)
M dominates foot of tree
[h, N aM • 0, i, k, f2]
[h, j, k, —],
{h, N a • MO, i, j, f2}
[h, N aM • 0, i, k, f2]
See Figure 4 for use of these three steps in the running example.
</bodyText>
<subsectionHeader confidence="0.945028">
3.5 Adjunctor
</subsectionHeader>
<bodyText confidence="0.998726785714286">
The adjunctor steps perform the actual recognition of an adjunction of an auxiliary
tree t in another tree at some node M. The first adjunctor step deals with the case in
which the other tree is again adjoined in a third tree (the two darkly shaded areas in
Figure 9) and M dominates the foot node. The second adjunctor step deals with the
case in which the other tree is either an initial tree, or has its foot elsewhere, i.e., not
dominated by M.
The two respective cases of adjunction are realized by step Adj 0 plus step Adj 1,
and by step Adj 0 plus step Adj 2. The auxiliary step Adj 0 introduces items of a
somewhat different form than those considered up to now, viz. [M -y j, k,
The interpretation is suggested in Figure 10: at M a tree has been adjoined. The ad-
joined tree and the lower half of the tree that M occurs in together generate the input
from j to k. The depicted structure is part of at least one derived tree consistent with
the input between positions 0 and k. In the case in which M dominates a foot node,
as suggested in the figure, fl and have a value other than &amp;quot;—&amp;quot;.
</bodyText>
<equation confidence="0.762664666666667">
[j, T Re*, j, k, fi, f21,
[h, M—&gt; 70, f2,
t E Adj(M) (Adj 0)
</equation>
<page confidence="0.6460965">
7°, j, f;,
352
</page>
<table confidence="0.810809846153846">
Nederhof Correct-Prefix Property for TAGs
fl f2&apos; f2 h if f1 f2 k
Adj 1 Adj 2
Figure 9 (Adj 1)
The two adjunctor steps, implicitly combined with Adj 0. (Adj 2)
[M-7., j,
M dominates foot of tree t&apos;,
[h, Ft, —&gt;
[h, N —&gt; a • MO, i, j, —]
[h, N—&gt; aM • )3, i, k,
[A4 —&gt; j, k, —1,
[h, N —&gt; a • MO, i, j, f;,
[h, N aM• k,
</table>
<bodyText confidence="0.998995">
For the running example, Adj 0 derives the intermediate item 15a from items 14
and 10 and from this and item 5, Adj 2 derives item 15. Similarly, Adj 0 and Adj 2
together derive item 22. There are no applications of Adj 1 in this example.
An alternative formulation of the adjunctor steps, without Adj 0, could be the
following:
</bodyText>
<table confidence="0.947101333333333">
U, —&gt; Rt., j, (Adj 1&apos;)
[h, 7*, fi, f2t f.;, (Adj 2&apos;)
t c Adj(M),
M dominates foot of tree t&apos;,
[h, -4 J*;,
[h, N a•MO, ij, —1
[h, N—&gt; aM• k,
U. Rt 40, j, k, fd,
[h, f2,
t E Adj(M),
[h, N a • M3, j,
[h, N —&gt; aM• k,
</table>
<page confidence="0.98674">
353
</page>
<figure confidence="0.9275805">
Computational Linguistics Volume 25, Number 3
fl&apos; f2&apos;
</figure>
<figureCaption confidence="0.949292">
Figure 10
</figureCaption>
<bodyText confidence="0.990589411764706">
An item [M -y., j,
That this formulation is equivalent to the original combination of the three steps
Adj 0, Adj 1, and Adj 2 can be argued in two stages.
First, the h in [1/, M f2, f, .f.] or [h, *, fi, f2, —I occurring as
second antecedent of Adj 1&apos; or Adj 2&apos;, respectively, can be replaced by a fresh variable
h&apos; without affecting the correctness of the algorithm. In particular, the occurrence of h
in the second antecedent of Adj 1&apos; is redundant because of the inclusion of the fifth
antecedent [h, Ft, --+ 1., A&apos;, f, f, g. Note that, conversely, this fifth antecedent is
redundant with respect to the second antecedent, since existence of an item [h, M
&apos;T f2, f, ffl, such that M dominates the foot of a tree t&apos;, implies the existence of
an item [h, Ft, —&gt; For further explanation, see Section 4.
Second, the first three antecedents of Adj 1&apos; and Adj 2&apos; can be split off to obtain
Adj 0, Adj 1, and Adj 2, justified by principles that are the basis for optimization of
database queries (Ullman 1982).
The rationale for the original formulation of the adjunction steps as opposed to
the alternative formulation by Adj 1&apos; and Adj 2&apos; lies in the consideration of time
complexity, as will be discussed in Section 5.
</bodyText>
<sectionHeader confidence="0.528022" genericHeader="method">
4. Properties
</sectionHeader>
<bodyText confidence="0.9988064">
The first claim we make about the algorithm pertains to its correctness as a recognizer:
Claim
After completion of the algorithm, the item [0, T Re 0, n, —], for some t E
is in the table if and only if the input is in the language described by the grammar.
Note that the input is in the language if and only if the input is the yield of a
derived tree.
The idea behind the proof of the &amp;quot;if&amp;quot; part is that for any derived tree constructed
from the grammar we can indicate a top-down and left-to-right tree traversal that is
matched by corresponding items that are computed by steps of the algorithm. The
tree traversal and the corresponding items are exemplified by the numbers 1, , 23
in Figure 4.
For the &amp;quot;only if&amp;quot; part, we can show for each step separately that the invariant
suggested in Figure 3 is preserved. To simplify the proof one can look only at the last
five fields of items [h, N —&gt; a • 0, i, j, fi, f2], h being irrelevant for the above claim.
We do, however, need h for the proof of the following claim:
</bodyText>
<page confidence="0.995256">
354
</page>
<figure confidence="0.990949">
Correct-Prefix Property for TAGs
Nederhof
0 h i j (a) 0 h i j (b)
</figure>
<figureCaption confidence="0.924835">
Figure 11
</figureCaption>
<bodyText confidence="0.41504">
Pred 1 preserves the invariant.
</bodyText>
<subsectionHeader confidence="0.888611">
Claim
</subsectionHeader>
<bodyText confidence="0.999551625">
The algorithm satisfies the correct-prefix property, provided the grammar is reduced.
A TAG is reduced if it does not contain any elementary trees that cannot be part
of any derived tree. (One reason why an auxiliary tree might not be a part of any
derived tree is that at some node it may have obligatory adjunction of itself, leading
to &amp;quot;infinite adjunction.&amp;quot;)
Again, the proof relies on the invariant sketched in Figure 3. The invariant can be
proven correct by verifying that if the items in the antecedents of some step satisfy
the invariant, then so does the item in the consequent.
A slight technical problem is caused by the obligatory adjunctions. The shaded
areas in Figure 3, for example, represent not merely subtrees of elementary trees, but
subtrees of a derived tree, which means that at each node either adjunction or nil-
adjunction has been performed.
This issue arises when we prove that Pred 1 preserves the invariant. Figure 11(a)
represents the interpretation of the first antecedent of this step, [h, N —› a •MO, i, j,
f; without loss of generality we only consider the case that fi = f2 = —. We may
assume that below M some subtree exists, and that at M itself either adjunction with
auxiliary tree t&apos; or nil-adjunction has been applied; the figure shows the former case.
In order to justify the item from the consequent, [j, T —&gt; • Rt, j, j, —], we
construct the tree in Figure 11(b), which is the same as that in Figure 11(a), except
that t&apos; is replaced by auxiliary tree t, which has been traversed so that at all nodes
either adjunction or nil-adjunction has been applied, including the nodes introduced
recursively through adjunctions. Such a finite traversal must exist since the grammar
is reduced.
For the other steps we do not need the assumption that the grammar is reduced
in order to prove that the invariant is preserved. For example, for Adj 1 we may
reason as follows: The item [M j, k, f, .E1, the first antecedent, informs us of
the existence of the structure in the shaded area of Figure 12(a). Similarly, the items
[h, Ft, —4 g and [h, N —› a • MO, i, j, —1 provide the shaded areas
of Figures 12(b) and 12(c). Note that in the case of the first or third item, we do not
use all the information that the item provides. In particular, the information that the
structures are part of a derived tree consistent with the input between positions 0 and
k (in the case of (a)) or j (in the case of (c)) is not needed.
</bodyText>
<page confidence="0.995146">
355
</page>
<figure confidence="0.99643425">
Computational Linguistics Volume 25, Number 3
f2&apos;
(a)
k 0 h
f;&apos;
(c)
0 h f2&apos; f2 k
(d)
</figure>
<figureCaption confidence="0.983203">
Figure 12
</figureCaption>
<bodyText confidence="0.989421461538462">
Adj 1 preserves the invariant.
The combined information from these three items ensures the existence of the
derived tree depicted in Figure 12(d), which justifies the consequent of Adj 1, viz.
[h, N aM • 0, i, k,
The other steps can be proven to preserve the invariant in similar ways.
Now the second claim follows: if the input up to position j has been read resulting
in an item of the form [h, N —&gt; ca • 0, i, j, fi, f21, then there is a string y such that
al ... aly is in the language. This y is the concatenation of the yields of the subtrees
labeled I, II, and III in Figure 3.
The full proofs of the two claims above are straightforward but tedious. Further-
more, our new algorithm is related to many existing recognition algorithms for TAGs
(Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and
Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published
</bodyText>
<page confidence="0.994921">
356
</page>
<note confidence="0.253625">
Nederhof Correct-Prefix Property for TAGs
</note>
<bodyText confidence="0.6651075">
together with proofs of correctness. Therefore, including full proofs for our new algo-
rithm does not seem necessary.
</bodyText>
<sectionHeader confidence="0.587246" genericHeader="method">
5. Complexity
</sectionHeader>
<bodyText confidence="0.999896625">
The steps presented in pseudoformal notation in Section 3 can easily be composed
into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such
a way that the order of the time complexity is determined by the maximal number of
different combinations of antecedents per step. If we restrict ourselves to the order of
the time complexity expressed in the length of the input, this means that the complexity
is given by 0(nP), where p is the largest number of input positions in any step.
However, a better realization of the algorithm exists that allows us to exclude
the variables for input positions that occur only once in a step, which we will call
irrelevant input positions. This realization relies on the fact that an intermediate step
may be applied that reduces an item I with q input positions to another item I&apos; with
q&apos; &lt; q input positions, omitting those that are irrelevant. That reduced item I&apos; then
takes the place of I in the antecedent of the actual step. This has a strong relationship
to optimization of database queries (Ullman 1982).
For example, there are nine variables in Comp 1, of which i,fi,f2,f;,./ are all
irrelevant, since they occur only once in that step. An alternative formulation of this
step is therefore given by the combination of the following three steps:
</bodyText>
<table confidence="0.873832111111111">
[h, M k, (Omit 5-6)
[h, y., k, 1, ?, ?] (Omit 3-5-6)
[h, N a • MO, i, j, f2] (Comp 1&apos;)
[h, N a•M13, ?, j, ?, ?]
[h, M —&gt; &apos;y fa, k, 1, ?, ?],
t E Adj(M),
[j, Ft —› • 1, k, k, —1,
[11, N a•M[, ?, j, ?, ?]
[j, Ft -41.,k, 1, k, I]
</table>
<bodyText confidence="0.9987751">
The question marks indicate omitted input positions. Items containing question
marks are distinguished from items without them, and from items with question marks
in different fields.
In Comp 1&apos; there are now only four input positions left. The contribution of this
step to the overall time complexity is therefore 0(n4) rather than 0(n9). The contribu-
tion of Omit 5-6 and Omit 3-5-6 to the time complexity is 0(n5).
For the entire algorithm, the maximum number of relevant input positions per
step is six. Thereby, the complexity of left-to-right recognition for TAGs under the
constraint of the correct-prefix property is 0(n6). There are five steps that contain six
relevant input positions, viz. Comp 2, Comp 3, Adj 0, Adj 1, and Adj 2.
</bodyText>
<page confidence="0.988261">
357
</page>
<note confidence="0.704768">
Computational Linguistics Volume 25, Number 3
</note>
<bodyText confidence="0.999931882352941">
In terms of the size of the grammar G, the complexity is 0(1G12), since at most
two elementary trees are simultaneously considered in a single step. Note that in
some steps we address several parts of a single elementary tree, such as the two parts
represented by the items [h, Fp g and [h, N a • MO, i, j, —]
in Adj 1. However, the second of these items uniquely identifies the second field of
the first item, and therefore this pair of items amounts to only one factor of 1G1 in the
time complexity.
The complexity of 0(n6) that we have achieved depends on two ideas: first, the
use of Adj 0, Adj 1, and Adj 2 instead of Adj 1&apos; and Adj 2&apos;, and second, the exclusion
of irrelevant variables above. Both are needed. The exclusion of irrelevant variables
alone, in combination with Adj 1&apos; and Adj 2&apos;, leads to a complexity of 0(n8). Without
excluding irrelevant variables, we obtain a complexity of 0(n9) due to Comp 1, which
uses nine input positions.
The question arises where the exact difference lies between our algorithm and
that of Schabes and Joshi (1988), and whether their algorithm could be improved to
obtain the same time complexity as ours, using techniques similar to those discussed
above. This question is difficult to answer precisely because of the significant difference
between the types of items that are used in the respective algorithms. However, some
general considerations suggest that the algorithm from Schabes and Joshi (1988) is
inherently more expensive.
First, the items from the new algorithm have five input positions, which implies
that storage of the parse table requires a space complexity of 0(n8). The items from the
older algorithm have effectively six input positions, which leads to a space complexity
of 0(n6).
Second, the &amp;quot;Right Completor&amp;quot; from Schabes and Joshi (1988), which roughly
corresponds with our adjunctor steps, has nine relevant input positions. This step can
be straightforwardly broken up into smaller steps that each have fewer relevant input
positions, but it seems difficult to reduce the maximal number of positions to six.
A final remark on Schabes and Joshi (1988) concerns the time complexity in terms
of the size of the grammar that they report, viz. 0(1G12). This would be the same upper
bound as in the case of the new algorithm. However, the correct complexity seems to
be 0(1G13), since each item contains references to two nodes of the same elementary
tree, and the combination in &amp;quot;Right Completor&amp;quot; of two items entails the simultaneous
use of three distinct nodes from the grammar.
</bodyText>
<sectionHeader confidence="0.959484" genericHeader="method">
6. Further Research
</sectionHeader>
<bodyText confidence="0.999826714285714">
The algorithm in the present paper operates in a top-down manner, being very similar
to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the &amp;quot;dotted&amp;quot;
items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (top-
down, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over
to head-driven parsing. An obvious question is whether such parsing techniques can
also be used to produce variants of left-to-right parsing for TAGs. Thus, one may
conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary
TAGs that operates in 0(n6) and that has the correct-prefix property.
Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker
(1990) and Nederhof (1998). However, for these algorithms the correct-prefix property
is not satisfied.
Development of advanced parsing algorithms for TAGs with the correct-prefix
property is not at all straightforward. In the case of context-free grammars, the addi-
tional benefit of LR parsing, in comparison to, for example, top-down parsing, lies in
</bodyText>
<page confidence="0.99042">
358
</page>
<note confidence="0.272473">
Nederhof Correct-Prefix Property for TAGs
</note>
<bodyText confidence="0.999905545454545">
the ability to process multiple grammar rules simultaneously. If this is to be carried
over to TAGs, then multiple elementary trees must be handled simultaneously. This
is difficult to combine with the mechanism we used to satisfy the correct-prefix prop-
erty, which relies on filtering out hypotheses with respect to &amp;quot;left context.&amp;quot; Filtering
out such hypotheses requires detailed investigation of that left context, which, how-
ever, precludes treating multiple elementary trees simultaneously. An exception may
be the case when a TAG contains many, almost identical, elementary trees. It is not
clear whether this case occurs often in practice.
Therefore, further research is needed not only to precisely define advanced parsing
algorithms for TAGs with the correct-prefix property, but also to determine whether
there are any benefits for practical grammars.
</bodyText>
<sectionHeader confidence="0.992203" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991485">
Most of the presented research was carried
out within the framework of the Priority
Programme Language and Speech
Technology (TST) while the author was
employed at the University of Groningen.
The TST-Programme is sponsored by NWO
(Dutch Organization for Scientific Research).
An error in a previous version of this paper
was found and corrected with the help of
Giorgio Satta.
</bodyText>
<sectionHeader confidence="0.989789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890256410256">
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 13(2):94-102, February.
Joshi, Aravind K. 1987. An introduction to
tree adjoining grammars. In Alexis
Manaster-Ramer, editor, Mathematics of
Language. John Benjamins Publishing
Company, Amsterdam, pages 87-114.
Lang, Bernard. 1988. The systematic
construction of Earley parsers:
Application to the production of 0(n6)
Earley parsers for tree adjoining
grammars. Unpublished paper, December.
Nederhof, Mark-Jan. 1994. An optimal
tabular parsing algorithm. In Proceedings
of the 32nd Annual Meeting, pages 117-124,
Las Cruces, NM, June. Association for
Computational Linguistics.
Nederhof, Mark-Jan. 1998. An alternative
LR algorithm for TAGs. In COLING-ACL
&apos;98 36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, volume 2, pages 946-952,
Montreal, Quebec, Canada, August.
Nederhof, Mark-Jan and Giorgio Satta. 1994.
An extended theory of head-driven
parsing. In Proceedings of the 32nd Annual
Meeting, pages 210-217, Las Cruces, NM,
June. Association for Computational
Linguistics.
Rajasekaran, Sanguthevar and Shibu
Yooseph. 1995. TAL recognition in
0(M(n2)) time. In Proceedings of the 33rd
Annual Meeting, pages 166-173,
Cambridge, MA, June. Association for
Computational Linguistics.
Satta, Giorgio. 1994. Tree-adjoining
grammar parsing and Boolean matrix
multiplication. Computational Linguistics,
20(2):173-191.
Schabes, Yves. 1994. Left to right parsing of
lexicalized tree-adjoining grammars.
Computational Intelligence, 10(4):506-524.
Schabes, Yves and Aravind K. Joshi. 1988.
An Earley-type parsing algorithm for tree
adjoining grammars. In Proceedings of the
26th Annual Meeting, pages 258-269,
Buffalo, NY, June. Association for
Computational Linguistics.
Schabes, Yves and Stuart M. Shieber. 1994.
An alternative conception of
tree-adjoining derivation. Computational
Linguistics, 20(1):91-124.
Schabes, Yves and K. Vijay-Shanker. 1990.
Deterministic left to right parsing of tree
adjoining languages. h-t Proceedings of the
28th Annual Meeting, pages 276-283,
Pittsburgh, PA, June. Association for
Computational Linguistics.
Schabes, Yves and Richard C. Waters. 1995.
Tree insertion grammar: A cubic-time,
parsable formalism that lexicalizes
context-free grammar without changing
the trees produced. Computational
Linguistics, 21(4):479-513.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24:3-36.
Sippu, Seppo and Eljas Soisalon-Soinirten.
1988. Parsing Theory, Vol. I: Languages and
Parsing. Volume 15 of EATCS Monographs
on Theoretical Computer Science.
Springer-Verlag.
Ullman, Jeffrey D. 1982. Principles of Database
Systems. Computer Science Press.
Vijay-Shankar, K. and Aravind K. Joshi.
</reference>
<page confidence="0.973318">
359
</page>
<note confidence="0.358298">
Computational Linguistics Volume 25, Number 3
</note>
<reference confidence="0.998060230769231">
1985. Some computational properties of
tree adjoining grammars. In Proceedings of
the 23rd Annual Meeting, pages 82-93,
Chicago, IL, July. Association for
Computational Linguistics.
Vijay-Shanker, K. and David J. Weir. 1993.
Parsing some constrained grammar
formalisms. Computational Linguistics,
19(4):591-636.
Vijay-Shanker, K. and David J. Weir. 1994.
The equivalence of four extensions of
context-free grammars. Mathematical
Systems Theory, 27:511-546.
</reference>
<page confidence="0.998252">
360
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.398526">
<title confidence="0.8939308">The Computational Complexity of the Correct-Prefix Property for TAGs Mark-Jan Nederhof* German Research Center for Artificial Intelligence</title>
<abstract confidence="0.9528534">A new upper bound is presented for the computational complexity of the parsing problem for TAGs, under the constraint that input is read from left to right in such a way that errors in the input are observed as soon as possible, which is called the &amp;quot;correct-prefix property.&amp;quot; The former bound, is now improved to which is the same as that of practical parsing algorithms for TAGs without the additional constraint of the correct-prefix property.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--2</pages>
<contexts>
<context position="28834" citStr="Earley 1970" startWordPosition="5445" endWordPosition="5446">ix. A final remark on Schabes and Joshi (1988) concerns the time complexity in terms of the size of the grammar that they report, viz. 0(1G12). This would be the same upper bound as in the case of the new algorithm. However, the correct complexity seems to be 0(1G13), since each item contains references to two nodes of the same elementary tree, and the combination in &amp;quot;Right Completor&amp;quot; of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the &amp;quot;dotted&amp;quot; items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in 0(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vij</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94-102, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language.</booktitle>
<pages>87--114</pages>
<editor>In Alexis Manaster-Ramer, editor,</editor>
<publisher>John Benjamins Publishing Company,</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="3793" citStr="Joshi (1987)" startWordPosition="606" endWordPosition="607">dexed grammars, combinatory categorial grammars, and head grammars, since these formalisms are equivalent to TAGs (Vijay-Shanker and Weir 1993, 1994). We present the actual algorithm in Section 3, after the necessary notation has been discussed in Section 2. The correctness proofs are discussed in Section 4, and the time complexity in Section 5. The ideas in this paper give rise to a number of questions for further research, as discussed in Section 6. 2. Definitions Our definition of TAGs simplifies the explanation of the algorithm, but differs slightly from standard treatment such as that of Joshi (1987). A tree-adjoining grammar is a 4-tuple (E, NT, I, A), where E is the set of terminals, I is the set of initial trees, and A is the set of auxiliary trees. We refer to the trees in / U A as elementary trees. The set NT, the set of nonterminals, does not play any role in this paper. We refer to the root of an elementary tree t as R. Each auxiliary tree has exactly one distinguished leaf, which is called the foot. We refer to the foot of an auxiliary tree t as F. We use variables N and M to range over nodes in elementary trees. We assume that the sets of nodes belonging to distinct elementary tr</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Joshi, Aravind K. 1987. An introduction to tree adjoining grammars. In Alexis Manaster-Ramer, editor, Mathematics of Language. John Benjamins Publishing Company, Amsterdam, pages 87-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>The systematic construction of Earley parsers: Application to the production of 0(n6) Earley parsers for tree adjoining grammars. Unpublished paper,</title>
<date>1988</date>
<contexts>
<context position="23526" citStr="Lang 1988" startWordPosition="4530" endWordPosition="4531">0, i, k, The other steps can be proven to preserve the invariant in similar ways. Now the second claim follows: if the input up to position j has been read resulting in an item of the form [h, N —&gt; ca • 0, i, j, fi, f21, then there is a string y such that al ... aly is in the language. This y is the concatenation of the yields of the subtrees labeled I, II, and III in Figure 3. The full proofs of the two claims above are straightforward but tedious. Furthermore, our new algorithm is related to many existing recognition algorithms for TAGs (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published 356 Nederhof Correct-Prefix Property for TAGs together with proofs of correctness. Therefore, including full proofs for our new algorithm does not seem necessary. 5. Complexity The steps presented in pseudoformal notation in Section 3 can easily be composed into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such a way that the order of the time complexity is determined by the maximal number of different combinations of antecedents per step. If we restrict ourselv</context>
</contexts>
<marker>Lang, 1988</marker>
<rawString>Lang, Bernard. 1988. The systematic construction of Earley parsers: Application to the production of 0(n6) Earley parsers for tree adjoining grammars. Unpublished paper, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>An optimal tabular parsing algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>117--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM,</location>
<contexts>
<context position="29022" citStr="Nederhof 1994" startWordPosition="5477" endWordPosition="5478">e case of the new algorithm. However, the correct complexity seems to be 0(1G13), since each item contains references to two nodes of the same elementary tree, and the combination in &amp;quot;Right Completor&amp;quot; of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the &amp;quot;dotted&amp;quot; items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in 0(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix property is not satisfied. Development of advanced parsing algorithms for TAGs with the correct-pref</context>
</contexts>
<marker>Nederhof, 1994</marker>
<rawString>Nederhof, Mark-Jan. 1994. An optimal tabular parsing algorithm. In Proceedings of the 32nd Annual Meeting, pages 117-124, Las Cruces, NM, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>An alternative LR algorithm for TAGs.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>946--952</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="29471" citStr="Nederhof (1998)" startWordPosition="5545" endWordPosition="5546">by the use of the &amp;quot;dotted&amp;quot; items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in 0(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix property is not satisfied. Development of advanced parsing algorithms for TAGs with the correct-prefix property is not at all straightforward. In the case of context-free grammars, the additional benefit of LR parsing, in comparison to, for example, top-down parsing, lies in 358 Nederhof Correct-Prefix Property for TAGs the ability to process multiple grammar rules simultaneously. If this is to be carried over to TAGs, then multiple elementary trees must be handled simultaneously. This is difficult to combine with the mechanism we used to sati</context>
</contexts>
<marker>Nederhof, 1998</marker>
<rawString>Nederhof, Mark-Jan. 1998. An alternative LR algorithm for TAGs. In COLING-ACL &apos;98 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 2, pages 946-952, Montreal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>An extended theory of head-driven parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>210--217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM,</location>
<contexts>
<context position="28927" citStr="Nederhof and Satta (1994)" startWordPosition="5460" endWordPosition="5463">erms of the size of the grammar that they report, viz. 0(1G12). This would be the same upper bound as in the case of the new algorithm. However, the correct complexity seems to be 0(1G13), since each item contains references to two nodes of the same elementary tree, and the combination in &amp;quot;Right Completor&amp;quot; of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the &amp;quot;dotted&amp;quot; items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in 0(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix prope</context>
</contexts>
<marker>Nederhof, Satta, 1994</marker>
<rawString>Nederhof, Mark-Jan and Giorgio Satta. 1994. An extended theory of head-driven parsing. In Proceedings of the 32nd Annual Meeting, pages 210-217, Las Cruces, NM, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanguthevar Rajasekaran</author>
<author>Shibu Yooseph</author>
</authors>
<title>TAL recognition in 0(M(n2)) time.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting,</booktitle>
<pages>166--173</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<marker>Rajasekaran, Yooseph, 1995</marker>
<rawString>Rajasekaran, Sanguthevar and Shibu Yooseph. 1995. TAL recognition in 0(M(n2)) time. In Proceedings of the 33rd Annual Meeting, pages 166-173, Cambridge, MA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
</authors>
<title>Tree-adjoining grammar parsing and Boolean matrix multiplication.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="28927" citStr="Satta (1994)" startWordPosition="5462" endWordPosition="5463">ize of the grammar that they report, viz. 0(1G12). This would be the same upper bound as in the case of the new algorithm. However, the correct complexity seems to be 0(1G13), since each item contains references to two nodes of the same elementary tree, and the combination in &amp;quot;Right Completor&amp;quot; of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the &amp;quot;dotted&amp;quot; items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in 0(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix prope</context>
</contexts>
<marker>Satta, 1994</marker>
<rawString>Satta, Giorgio. 1994. Tree-adjoining grammar parsing and Boolean matrix multiplication. Computational Linguistics, 20(2):173-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Left to right parsing of lexicalized tree-adjoining grammars.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<pages>10--4</pages>
<contexts>
<context position="3041" citStr="Schabes (1994)" startWordPosition="487" endWordPosition="488">t-prefix property—that by Schabes and Joshi (1988)—has complexity 0 (n9 ) . In this paper we present an algorithm that satisfies the correct-prefix property and operates in 0(n6) time. This algorithm merely recognizes input, but it can be extended * DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid prefix property is used. C) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 to be a parsing algorithm with the ideas from Schabes (1994), which also suggest how it can be extended to handle substitution in addition to adjunction. The complexity results carry over to linear indexed grammars, combinatory categorial grammars, and head grammars, since these formalisms are equivalent to TAGs (Vijay-Shanker and Weir 1993, 1994). We present the actual algorithm in Section 3, after the necessary notation has been discussed in Section 2. The correctness proofs are discussed in Section 4, and the time complexity in Section 5. The ideas in this paper give rise to a number of questions for further research, as discussed in Section 6. 2. D</context>
<context position="23596" citStr="Schabes 1994" startWordPosition="4540" endWordPosition="4541">n similar ways. Now the second claim follows: if the input up to position j has been read resulting in an item of the form [h, N —&gt; ca • 0, i, j, fi, f21, then there is a string y such that al ... aly is in the language. This y is the concatenation of the yields of the subtrees labeled I, II, and III in Figure 3. The full proofs of the two claims above are straightforward but tedious. Furthermore, our new algorithm is related to many existing recognition algorithms for TAGs (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published 356 Nederhof Correct-Prefix Property for TAGs together with proofs of correctness. Therefore, including full proofs for our new algorithm does not seem necessary. 5. Complexity The steps presented in pseudoformal notation in Section 3 can easily be composed into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such a way that the order of the time complexity is determined by the maximal number of different combinations of antecedents per step. If we restrict ourselves to the order of the time complexity expressed in the length of the </context>
</contexts>
<marker>Schabes, 1994</marker>
<rawString>Schabes, Yves. 1994. Left to right parsing of lexicalized tree-adjoining grammars. Computational Intelligence, 10(4):506-524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>An Earley-type parsing algorithm for tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting,</booktitle>
<pages>258--269</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Buffalo, NY,</location>
<contexts>
<context position="2477" citStr="Schabes and Joshi (1988)" startWordPosition="401" endWordPosition="404">property. For context-free and regular languages, the correct-prefix property can be satisfied without additional costs of space or time. Surprisingly, it has been claimed by Schabes and Waters (1995) that this property is problematic for the mildly contextsensitive languages represented by tree-adjoining grammars (TAGs): the best practical parsing algorithms for TAGs have time complexity 0 (n6 ) (Vijay-Shankar and Joshi [1985]; see Satta [1994] and Rajasekaran and Yooseph [19951 for lower theoretical upper bounds), whereas the only published algorithm with the correct-prefix property—that by Schabes and Joshi (1988)—has complexity 0 (n9 ) . In this paper we present an algorithm that satisfies the correct-prefix property and operates in 0(n6) time. This algorithm merely recognizes input, but it can be extended * DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid prefix property is used. C) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 to be a parsing algorithm with the ideas from Schabes (1994), which also suggest how it can be e</context>
<context position="23515" citStr="Schabes and Joshi 1988" startWordPosition="4526" endWordPosition="4529"> Adj 1, viz. [h, N aM • 0, i, k, The other steps can be proven to preserve the invariant in similar ways. Now the second claim follows: if the input up to position j has been read resulting in an item of the form [h, N —&gt; ca • 0, i, j, fi, f21, then there is a string y such that al ... aly is in the language. This y is the concatenation of the yields of the subtrees labeled I, II, and III in Figure 3. The full proofs of the two claims above are straightforward but tedious. Furthermore, our new algorithm is related to many existing recognition algorithms for TAGs (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published 356 Nederhof Correct-Prefix Property for TAGs together with proofs of correctness. Therefore, including full proofs for our new algorithm does not seem necessary. 5. Complexity The steps presented in pseudoformal notation in Section 3 can easily be composed into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such a way that the order of the time complexity is determined by the maximal number of different combinations of antecedents per step. If we restr</context>
<context position="27203" citStr="Schabes and Joshi (1988)" startWordPosition="5182" endWordPosition="5185">items amounts to only one factor of 1G1 in the time complexity. The complexity of 0(n6) that we have achieved depends on two ideas: first, the use of Adj 0, Adj 1, and Adj 2 instead of Adj 1&apos; and Adj 2&apos;, and second, the exclusion of irrelevant variables above. Both are needed. The exclusion of irrelevant variables alone, in combination with Adj 1&apos; and Adj 2&apos;, leads to a complexity of 0(n8). Without excluding irrelevant variables, we obtain a complexity of 0(n9) due to Comp 1, which uses nine input positions. The question arises where the exact difference lies between our algorithm and that of Schabes and Joshi (1988), and whether their algorithm could be improved to obtain the same time complexity as ours, using techniques similar to those discussed above. This question is difficult to answer precisely because of the significant difference between the types of items that are used in the respective algorithms. However, some general considerations suggest that the algorithm from Schabes and Joshi (1988) is inherently more expensive. First, the items from the new algorithm have five input positions, which implies that storage of the parse table requires a space complexity of 0(n8). The items from the older a</context>
</contexts>
<marker>Schabes, Joshi, 1988</marker>
<rawString>Schabes, Yves and Aravind K. Joshi. 1988. An Earley-type parsing algorithm for tree adjoining grammars. In Proceedings of the 26th Annual Meeting, pages 258-269, Buffalo, NY, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Stuart M Shieber</author>
</authors>
<title>An alternative conception of tree-adjoining derivation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--1</pages>
<contexts>
<context position="23581" citStr="Schabes and Shieber 1994" startWordPosition="4536" endWordPosition="4539">o preserve the invariant in similar ways. Now the second claim follows: if the input up to position j has been read resulting in an item of the form [h, N —&gt; ca • 0, i, j, fi, f21, then there is a string y such that al ... aly is in the language. This y is the concatenation of the yields of the subtrees labeled I, II, and III in Figure 3. The full proofs of the two claims above are straightforward but tedious. Furthermore, our new algorithm is related to many existing recognition algorithms for TAGs (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published 356 Nederhof Correct-Prefix Property for TAGs together with proofs of correctness. Therefore, including full proofs for our new algorithm does not seem necessary. 5. Complexity The steps presented in pseudoformal notation in Section 3 can easily be composed into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such a way that the order of the time complexity is determined by the maximal number of different combinations of antecedents per step. If we restrict ourselves to the order of the time complexity expressed in the</context>
</contexts>
<marker>Schabes, Shieber, 1994</marker>
<rawString>Schabes, Yves and Stuart M. Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Deterministic left to right parsing of tree adjoining languages. h-t</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting,</booktitle>
<pages>276--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="29451" citStr="Schabes and Vijay-Shanker (1990)" startWordPosition="5540" endWordPosition="5543">m (Earley 1970), which is emphasized by the use of the &amp;quot;dotted&amp;quot; items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in 0(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix property is not satisfied. Development of advanced parsing algorithms for TAGs with the correct-prefix property is not at all straightforward. In the case of context-free grammars, the additional benefit of LR parsing, in comparison to, for example, top-down parsing, lies in 358 Nederhof Correct-Prefix Property for TAGs the ability to process multiple grammar rules simultaneously. If this is to be carried over to TAGs, then multiple elementary trees must be handled simultaneously. This is difficult to combine with the mecha</context>
</contexts>
<marker>Schabes, Vijay-Shanker, 1990</marker>
<rawString>Schabes, Yves and K. Vijay-Shanker. 1990. Deterministic left to right parsing of tree adjoining languages. h-t Proceedings of the 28th Annual Meeting, pages 276-283, Pittsburgh, PA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Tree insertion grammar: A cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="2053" citStr="Schabes and Waters (1995)" startWordPosition="341" endWordPosition="344">receding stages are correct prefixes, or more precisely, they are prefixes of some correct strings in the language. Hence, we speak of the correct-prefix property.&apos; An important application can be found in the area of grammar checking: upon finding an ungrammatical sentence in a document, a grammar checker may report to the user the presumed position of the error, obtained from a parsing algorithm with the correct-prefix property. For context-free and regular languages, the correct-prefix property can be satisfied without additional costs of space or time. Surprisingly, it has been claimed by Schabes and Waters (1995) that this property is problematic for the mildly contextsensitive languages represented by tree-adjoining grammars (TAGs): the best practical parsing algorithms for TAGs have time complexity 0 (n6 ) (Vijay-Shankar and Joshi [1985]; see Satta [1994] and Rajasekaran and Yooseph [19951 for lower theoretical upper bounds), whereas the only published algorithm with the correct-prefix property—that by Schabes and Joshi (1988)—has complexity 0 (n9 ) . In this paper we present an algorithm that satisfies the correct-prefix property and operates in 0(n6) time. This algorithm merely recognizes input, b</context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Schabes, Yves and Richard C. Waters. 1995. Tree insertion grammar: A cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced. Computational Linguistics, 21(4):479-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Shieber, Stuart M., Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seppo Sippu</author>
<author>Eljas Soisalon-Soinirten</author>
</authors>
<date>1988</date>
<booktitle>Parsing Theory, Vol. I: Languages and Parsing. Volume 15 of EATCS Monographs on Theoretical Computer Science.</booktitle>
<publisher>Springer-Verlag.</publisher>
<marker>Sippu, Soisalon-Soinirten, 1988</marker>
<rawString>Sippu, Seppo and Eljas Soisalon-Soinirten. 1988. Parsing Theory, Vol. I: Languages and Parsing. Volume 15 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey D Ullman</author>
</authors>
<title>Principles of Database Systems.</title>
<date>1982</date>
<booktitle>In Proceedings of the 23rd Annual Meeting,</booktitle>
<pages>82--93</pages>
<publisher>Computer Science</publisher>
<institution>for Computational Linguistics.</institution>
<location>Chicago, IL,</location>
<contexts>
<context position="18799" citStr="Ullman 1982" startWordPosition="3651" endWordPosition="3652">ence of h in the second antecedent of Adj 1&apos; is redundant because of the inclusion of the fifth antecedent [h, Ft, --+ 1., A&apos;, f, f, g. Note that, conversely, this fifth antecedent is redundant with respect to the second antecedent, since existence of an item [h, M &apos;T f2, f, ffl, such that M dominates the foot of a tree t&apos;, implies the existence of an item [h, Ft, —&gt; For further explanation, see Section 4. Second, the first three antecedents of Adj 1&apos; and Adj 2&apos; can be split off to obtain Adj 0, Adj 1, and Adj 2, justified by principles that are the basis for optimization of database queries (Ullman 1982). The rationale for the original formulation of the adjunction steps as opposed to the alternative formulation by Adj 1&apos; and Adj 2&apos; lies in the consideration of time complexity, as will be discussed in Section 5. 4. Properties The first claim we make about the algorithm pertains to its correctness as a recognizer: Claim After completion of the algorithm, the item [0, T Re 0, n, —], for some t E is in the table if and only if the input is in the language described by the grammar. Note that the input is in the language if and only if the input is the yield of a derived tree. The idea behind the </context>
<context position="24880" citStr="Ullman 1982" startWordPosition="4758" endWordPosition="4759">largest number of input positions in any step. However, a better realization of the algorithm exists that allows us to exclude the variables for input positions that occur only once in a step, which we will call irrelevant input positions. This realization relies on the fact that an intermediate step may be applied that reduces an item I with q input positions to another item I&apos; with q&apos; &lt; q input positions, omitting those that are irrelevant. That reduced item I&apos; then takes the place of I in the antecedent of the actual step. This has a strong relationship to optimization of database queries (Ullman 1982). For example, there are nine variables in Comp 1, of which i,fi,f2,f;,./ are all irrelevant, since they occur only once in that step. An alternative formulation of this step is therefore given by the combination of the following three steps: [h, M k, (Omit 5-6) [h, y., k, 1, ?, ?] (Omit 3-5-6) [h, N a • MO, i, j, f2] (Comp 1&apos;) [h, N a•M13, ?, j, ?, ?] [h, M —&gt; &apos;y fa, k, 1, ?, ?], t E Adj(M), [j, Ft —› • 1, k, k, —1, [11, N a•M[, ?, j, ?, ?] [j, Ft -41.,k, 1, k, I] The question marks indicate omitted input positions. Items containing question marks are distinguished from items without them, an</context>
</contexts>
<marker>Ullman, 1982</marker>
<rawString>Ullman, Jeffrey D. 1982. Principles of Database Systems. Computer Science Press. Vijay-Shankar, K. and Aravind K. Joshi. 1985. Some computational properties of tree adjoining grammars. In Proceedings of the 23rd Annual Meeting, pages 82-93, Chicago, IL, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>Parsing some constrained grammar formalisms.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--4</pages>
<contexts>
<context position="3323" citStr="Vijay-Shanker and Weir 1993" startWordPosition="526" endWordPosition="529">usweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid prefix property is used. C) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 to be a parsing algorithm with the ideas from Schabes (1994), which also suggest how it can be extended to handle substitution in addition to adjunction. The complexity results carry over to linear indexed grammars, combinatory categorial grammars, and head grammars, since these formalisms are equivalent to TAGs (Vijay-Shanker and Weir 1993, 1994). We present the actual algorithm in Section 3, after the necessary notation has been discussed in Section 2. The correctness proofs are discussed in Section 4, and the time complexity in Section 5. The ideas in this paper give rise to a number of questions for further research, as discussed in Section 6. 2. Definitions Our definition of TAGs simplifies the explanation of the algorithm, but differs slightly from standard treatment such as that of Joshi (1987). A tree-adjoining grammar is a 4-tuple (E, NT, I, A), where E is the set of terminals, I is the set of initial trees, and A is th</context>
<context position="23555" citStr="Vijay-Shanker and Weir 1993" startWordPosition="4532" endWordPosition="4535">e other steps can be proven to preserve the invariant in similar ways. Now the second claim follows: if the input up to position j has been read resulting in an item of the form [h, N —&gt; ca • 0, i, j, fi, f21, then there is a string y such that al ... aly is in the language. This y is the concatenation of the yields of the subtrees labeled I, II, and III in Figure 3. The full proofs of the two claims above are straightforward but tedious. Furthermore, our new algorithm is related to many existing recognition algorithms for TAGs (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published 356 Nederhof Correct-Prefix Property for TAGs together with proofs of correctness. Therefore, including full proofs for our new algorithm does not seem necessary. 5. Complexity The steps presented in pseudoformal notation in Section 3 can easily be composed into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such a way that the order of the time complexity is determined by the maximal number of different combinations of antecedents per step. If we restrict ourselves to the order of the time c</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>Vijay-Shanker, K. and David J. Weir. 1993. Parsing some constrained grammar formalisms. Computational Linguistics, 19(4):591-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>27--511</pages>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>Vijay-Shanker, K. and David J. Weir. 1994. The equivalence of four extensions of context-free grammars. Mathematical Systems Theory, 27:511-546.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>