<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006156">
<title confidence="0.9965505">
An Empirical Study on Development Set Selection Strategy
for Machine Translation Learning*
</title>
<author confidence="0.997836">
Cong Hui12, Hai Zhao12; Yan Song3, Bao-Liang Lu12
</author>
<affiliation confidence="0.9979768">
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
3Department of Chinese, Translation and Linguistics, City University of Hong Kong
</affiliation>
<email confidence="0.993165">
huicong@sjtu.edu.cn, {zhaohai,blu}@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.982867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979">
This paper describes a statistical machine
translation system for our participation
for the WMT10 shared task. Based on
MOSES, our system is capable of translat-
ing German, French and Spanish into En-
glish. Our main contribution in this work
is about effective parameter tuning. We
discover that there is a significant perfor-
mance gap as different development sets
are adopted. Finally, ten groups of devel-
opment sets are used to optimize the model
weights, and this does help us obtain a sta-
ble evaluation result.
</bodyText>
<sectionHeader confidence="0.995174" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856214285714">
We present a machine translation system that rep-
resents our participation for the WMT10 shared
task from Brain-like Computing and Machine In-
telligence Lab of Shanghai Jiao Tong University
(SJTU-BCMI Lab). The system is based on the
state-of-the-art SMT toolkit MOSES (Koehn et al.,
2007). We use it to translate German, French and
Spanish into English. Though different develop-
ment sets used for training parameter tuning will
certainly lead to quite different performance, we
empirically find that the more sets we combine to-
gether, the more stable the performance is, and a
development set similar with test set will help the
performance improvement.
</bodyText>
<sectionHeader confidence="0.865111" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.8806125">
The basic model of the our system is a log-linear
model (Och and Ney, 2002). For given source lan-
</bodyText>
<note confidence="0.971272571428572">
Phis work was partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Basic
Research Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
corresponding author
</note>
<bodyText confidence="0.9608235">
guage strings, the target language string t will be
obtained by the following equation,
</bodyText>
<equation confidence="0.993486">
P, = arg max{pλ- (tI1 sJ1)}
t�
exp[Em 1 λmhm(tI1, sJ1 )�
= arg max{ M
ti E¯tl exp[Em=1 λmhm
</equation>
<bodyText confidence="0.9871185">
(�tI1, s01
where hm is the m-th feature function and λm is
the m-th model weight. There are four main parts
of features in the model: translation model, lan-
guage model, reordering model and word penalty.
The whole model has been well implemented by
the state-of-the-art statistical machine translation
toolkit MOSES.
For each language that is required to translated
into English, two sets of bilingual corpora are pro-
vided by the shared task organizer. The first set
is the new release (version 5) of Europarl cor-
pus which is the smaller. The second is a com-
bination of other available data sets which is the
larger. In detail, two corpora, europarl-v5 and
news-commentary10 are for German, europarl-v5
and news-commentary10 plus undoc for French
and Spanish, respectively. Details of training data
are in Table 1. Only sentences with length 1 to 40
are acceptable for our task. We used the larger set
for our primary submission.
We adopt word alignment toolkit GIZA++ (Och
and Ney, 2003) to learn word-level alignment with
its default setting and grow-diag-final-and param-
eters. Given a sentence pair and its corresponding
word-level alignment, phrases will be extracted by
using the approach in (Och and Ney, 2004). Phrase
probability is estimated by its relative frequency
in the training corpus. Lexical reordering is deter-
mined by using the default setting of MOSES with
msd-bidirectional parameter.
For training the only language model (English),
the data sets are extracted from monolingual parts
of both europarl-v5 and news-commentary10,
</bodyText>
<page confidence="0.834952">
67
</page>
<note confidence="0.8729585">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67–71,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.999356142857143">
sentences words(s) words(t)
de small 1540549 35.76M 38.53M
large 1640818 37.95M 40.64M
fr small 1683156 44.02M 44.20M
large 8997997 251.60M 228.50M
es small 1650152 43.17M 41.25M
large 7971200 236.24M 207.79M
</table>
<tableCaption confidence="0.778346">
Table 1: Bilingual training corpora from Ger-
man(de), French(fr) and Spanish(es) to English.
</tableCaption>
<bodyText confidence="0.997544692307692">
which include 1968914 sentences and 47.48M
words. And SRILM is adopted with 5-gram, in-
terpolate and kndiscount settings (Stolcke, 2002)
.
The next step is to estimate feature weights by
optimizing translation performance on a develop-
ment set. We consider various combinations of 10
development sets with 18207 sentences to get a
stable performance in our primary submission.
We use the default toolkits which are provided
by WMT10 organizers for preprocessing (i.e., to-
kenize) and postprocessing (i.e., detokenize, re-
caser).
</bodyText>
<sectionHeader confidence="0.924357" genericHeader="method">
3 Development Set Selection
</sectionHeader>
<subsectionHeader confidence="0.998247">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.9992486">
Given the previous feature functions, the model
weights will be obtained by optimizing the follow-
ing maximum mutual information criterion, which
can be derived from the maximum entropy princi-
ple:
</bodyText>
<equation confidence="0.998106666666667">
��M = arg max{
)&apos;M
1
</equation>
<bodyText confidence="0.999981153846154">
As usual, minimum error rate training (MERT) is
adopted for log-linear model parameter estimation
(Och, 2003). There are many improvements on
MERT in existing work (Bertoldi et al., 2009; Fos-
ter and Kuhn, 2009), but there is no demonstration
that the weights with better performance on the
development set would lead to a better result on
the unseen test set. In our experiments, we found
that different development sets will cause signifi-
cant BLEU score differences, even as high as one
percent. Thus the remained problem will be how
to effectively choose the development set to obtain
a better and more stable performance.
</bodyText>
<subsectionHeader confidence="0.999364">
3.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998281846153846">
Our empirical study will be demonstrated through
German to English translation on the smaller cor-
pus. The development sets are all development
sets and test sets from the previous WMT shared
translation task as shown in Table 2, and labeled
as dev-0 to dev-9. Meanwhile, we denote 10 batch
sets from batch-0 to batch-9 where the batch-i set
is the combination of dev- sets from dev-0 to dev-i.
The test set is newstest2009, which includes 2525
sentences, 54K German words and 58K English
words, and news-test2008, which includes 2051
sentences, 41K German words and 43K English
words.
</bodyText>
<table confidence="0.998494">
id name sent w(de) w(en)
dev-0 dev2006 2000 49K 53K
dev-1 devtest2006 2000 48K 52K
dev-2 nc-dev2007 1057 23K 23K
dev-3 nc-devtest2007 1064 24K 23K
dev-4 nc-test2007 2007 45K 44K
dev-5 nc-test2008 2028 45K 44K
dev-6 news-dev2009 2051 41K 43K
dev-7 test2006 2000 49K 54K
dev-8 test2007 2000 49K 54K
dev-9 test2008 2000 50K 54K
</table>
<tableCaption confidence="0.99186">
Table 2: Development data.
</tableCaption>
<subsectionHeader confidence="0.99034">
3.3 On the Scale of Development Set
</subsectionHeader>
<bodyText confidence="0.999987421052632">
Having 20 different development sets (10 dev- sets
and batch- sets), 20 models are correspondingly
trained.The decode results on the test set are sum-
marized in Table 3 and Figure 1. The dotted lines
are the performances of 10 different development
sets on the two test sets, we will see that there
is a huge gap between the highest and the lowest
score, and there is not an obvious rule to follow. It
will bring about unsatisfied results if a poor devel-
opment set is chosen. The solid lines represents
the performances of 10 incremental batch sets on
the two test sets, the batch processing still gives a
poor performance at the beginning, but the results
become better and more stable when the develop-
ment sets are continuously enlarged. This sort of
results suggest that a combined development set
may produce reliable results in the worst case. Our
primary submission used the combined develop-
ment set and the results as Table 4.
</bodyText>
<figure confidence="0.990528083333333">
S
log p)&apos;M (ti  |si�}
i=1
68
2.5
2
1.5
1
0.5
09−Ddev
09−Dtest
0
0 1 2 3 4 5 6 7 8 9
id 09-dev 09-batch 08-dev 08-batch
0 16.46 16.46 16.38 16.38
1 16.67 16.25 16.66 16.44
2 16.74 16.20 16.94 16.22
3 16.15 16.83 16.18 17.02
4 16.44 16.73 16.64 16.89
5 16.50 16.97 16.75 17.13
6 17.15 17.03 17.67 17.24
7 16.51 17.00 16.34 17.09
8 17.03 16.97 17.15 17.22
9 16.25 16.99 16.24 17.26
</figure>
<tableCaption confidence="0.99245825">
Table 3: BLEU scores on the two test
sets(newstest2009 &amp; news-test2008), which use
two data set sequences(dev- sequence &amp; batch- se-
quence) to optimize model weights.
</tableCaption>
<table confidence="0.942887666666667">
CE
de-en fr-en es-en
18.90 24.30 26.40
</table>
<tableCaption confidence="0.998238">
Table 4: BLEU scores of our primary submission.
</tableCaption>
<subsectionHeader confidence="0.920436">
3.4 On BLEU Score Difference
</subsectionHeader>
<bodyText confidence="0.999916333333333">
To compare BLEU score differences between test
set and development set, we consider two groups
of BLEU score differences, For each development
set, dev-i, the BLEU score difference will be com-
puted between b1 from which adopts itself as the
development set and b2 from which adopts test
set as the development set. For the test set, the
BLEU score difference will be computed between
bi from which adopts each development set, dev-i,
as the development set and b2 from which adopts
itself as the development set.
These two groups of results are illustrated in
Figure 2 (the best score of the test set under self
tuning, newstest2009 is 17.91). The dotted lines
have the inverse trend with the dotted in Figure
1(because the addition of these two values is con-
stant), and the solid lines have the same trend
with the dotted, which means that the good per-
formance is mutual between test set and develop-
ment sets: if tuning using A set could make a good
result over B set, then vice versa.
</bodyText>
<subsectionHeader confidence="0.601442">
3.5 On the Similarity between Development
Set and Test Set
</subsectionHeader>
<bodyText confidence="0.99949475">
This experiment is motivated by (Utiyama et al.,
2009), where they used BLEU score to measure
the similarity of a sentences pair and then ex-
tracted sentences similar with those in test set to
</bodyText>
<figure confidence="0.62203">
DATASETID
</figure>
<figureCaption confidence="0.99936">
Figure 2: The trend of BLEU score differences
</figureCaption>
<bodyText confidence="0.999985269230769">
construct a specific tuning set. In our experiment,
we will try to measure data set similarity instead.
Given two sets of sentences, one is called as candi-
date(cnd) set and the other reference(ref) set. For
any cnd sentence, we let the whole ref set to be its
reference and then multi-references BLEU score is
computed for cnd set. There comes a problem that
the sentence penalty will be constant for any cnd
sentence, we turn to calculate the average length
of whose sentences which have common n-gram
with the given cnd sentence.
Now we may define three measures. The mea-
sure which uses dev- and batch- sets as cnd sets
and news-test2009 set as ref set is defined as
precision-BLEU, and the measure which uses the
above sets on the contrary way is defined as recall-
BLEU. Then F1-BLEU is defined as the harmonic
mean of precision-BLEU and recall-BLEU. These
results are illustrated in Figure 3. From the fig-
ure, we find that F1-BLEU plays an important
role to predict the goodness of a development set,
F1-BLEU scores of batch- sets have an ascending
curve and batch data set sequence will cause a sta-
ble good test performance, the point on dev- sets
which has high F1-BLEU(eg, dev-0,4,5) would
also has a good test performance.
</bodyText>
<subsectionHeader confidence="0.508071">
3.6 Related Work
</subsectionHeader>
<bodyText confidence="0.999092714285714">
The special challenge of the WMT shared task is
domain adaptation, which is a hot topic in recent
years and more relative to our experiments. Many
existing works are about this topic (Koehn and
Schroeder, 2007; Nakov, 2008; Nakov and Ng,
2009; Paul et al., 2009; Haque et al., 2009). How-
ever, most of previous works focus on language
</bodyText>
<page confidence="0.898854">
69
</page>
<bodyText confidence="0.999966142857143">
model, translation phrase table, lexicons model
and factored translation model, few of them pay
attention to the domain adaptation on the develop-
ment set. For future work we consider to use some
machine learning approaches to select sentences in
development sets more relevant with the test set in
order to further improve translation performance.
</bodyText>
<sectionHeader confidence="0.993264" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999958">
In this paper, we present our machine translation
system for the WMT10 shared task and perform an
empirical study on the development set selection.
According to our experimental results, Choosing
different development sets would play an impor-
tant role for translation performance. We find that
a development set with higher F1-BLEU yields
better and more stable results.
</bodyText>
<sectionHeader confidence="0.956427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999533514705883">
Nicola Bertoldi, Barry Haddow, and Jean Baptiste
Fouet. 2009. Improved Minimum Error Rate Train-
ing in Moses. The Prague Bulletin of Mathematical
Linguistics, 91:7–16.
George Foster and Roland Kuhn. 2009. Stabiliz-
ing minimum error rate training. In Proceedings
of the 4th Workshop on Statistical Machine Trans-
lation(WMT), Boulder, Colorado, USA.
Rejwanul Haque, Sudip Kumar Naskar, Josef Van Gen-
abith, and Andy Way. 2009. Experiments on Do-
main Adaptation for EnglishłHindi SMT. In 7th In-
ternational Conference on Natural Language Pro-
cessing(ICNLP), Hyderabad, India.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation(WMT), Prague, Czech
Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics(ACL), Prague, Czech Republic.
Preslav Nakov and Hwee Tou Ng. 2009. NUS
at WMT09: domain adaptation experiments for
English-Spanish machine translation of news com-
mentary text. In Proceedings of the 4th Workshop on
Statistical Machine Translation(WMT), Singapore.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the 3rd Workshop on
Statistical Machine Translation(WMT), Columbus,
Ohio, USA.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics(ACL), Philadelphia, Pennsylva-
nian, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Compu-
tational Linguistics(ACL), Sapporo, Japan.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2009. NICT@ WMT09: model adaptation and
transliteration for Spanish-English SMT. In Pro-
ceedings of the 4th Workshop on Statistical Machine
Translation(WMT), Singapore.
Andreas Stolcke. 2002. SRILM: an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing(ICSLP),
Denver, Colorado, USA.
Masao Utiyama, Hirofumi Yamamoto, and Eiichiro
Sumita. 2009. Two methods for stabilizing MERT:
NICT at IWSLT 2009. In Proceedings of Inter-
national Workshop on Spoken Language Transla-
tion(IWSLT), Tokyo, Japan.
</reference>
<figure confidence="0.986487214285714">
70
17.5
16.5
15.5
18
17
16
15
09−dev
09−batch
08−dev
08−batch
0 1 2 3 4 5 6 7 8 9
DATA SET ID
</figure>
<figureCaption confidence="0.9535715">
Figure 1: The BLEU score trend in Tabel 3, we will see that the batch lines output a stable and good
performance.
</figureCaption>
<figure confidence="0.768052">
DATA SET ID
</figure>
<figureCaption confidence="0.9574895">
Figure 3: The precision(p), recall(r) and F1(f) BLEU score on the dev(Dev) and batch(Batch) sets based
on the comparison with news-test2009 set.
</figureCaption>
<figure confidence="0.992125230769231">
30
25
20
15
10
pDev
pBatch
rDev
rBatch
fDev
fBatch
0 1 2 3 4 5 6 7 8 9
71
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.106092">
<title confidence="0.998686">An Empirical Study on Development Set Selection Machine Translation</title>
<author confidence="0.6687855">Hai Bao-Liang for Brain-Like Computing</author>
<author confidence="0.6687855">Machine</author>
<affiliation confidence="0.653976">Department of Computer Science and Engineering, Shanghai Jiao Tong Key Laboratory for Intelligent Computing and Intelligent</affiliation>
<address confidence="0.3904435">Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, of Chinese, Translation and Linguistics, City University of Hong</address>
<abstract confidence="0.997916142857143">This paper describes a statistical machine translation system for our participation for the WMT10 shared task. Based on MOSES, our system is capable of translating German, French and Spanish into English. Our main contribution in this work is about effective parameter tuning. We discover that there is a significant performance gap as different development sets are adopted. Finally, ten groups of development sets are used to optimize the model weights, and this does help us obtain a stable evaluation result.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Barry Haddow</author>
<author>Jean Baptiste Fouet</author>
</authors>
<title>Improved Minimum Error Rate Training in Moses.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--7</pages>
<contexts>
<context position="5380" citStr="Bertoldi et al., 2009" startWordPosition="824" endWordPosition="827"> our primary submission. We use the default toolkits which are provided by WMT10 organizers for preprocessing (i.e., tokenize) and postprocessing (i.e., detokenize, recaser). 3 Development Set Selection 3.1 Motivation Given the previous feature functions, the model weights will be obtained by optimizing the following maximum mutual information criterion, which can be derived from the maximum entropy principle: ��M = arg max{ )&apos;M 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 3.2 Experimental Settings Our empirical study will be demonstrated through German to English translation on the smaller corpus. The development sets are all deve</context>
</contexts>
<marker>Bertoldi, Haddow, Fouet, 2009</marker>
<rawString>Nicola Bertoldi, Barry Haddow, and Jean Baptiste Fouet. 2009. Improved Minimum Error Rate Training in Moses. The Prague Bulletin of Mathematical Linguistics, 91:7–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Stabilizing minimum error rate training.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation(WMT),</booktitle>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="5404" citStr="Foster and Kuhn, 2009" startWordPosition="828" endWordPosition="832">. We use the default toolkits which are provided by WMT10 organizers for preprocessing (i.e., tokenize) and postprocessing (i.e., detokenize, recaser). 3 Development Set Selection 3.1 Motivation Given the previous feature functions, the model weights will be obtained by optimizing the following maximum mutual information criterion, which can be derived from the maximum entropy principle: ��M = arg max{ )&apos;M 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 3.2 Experimental Settings Our empirical study will be demonstrated through German to English translation on the smaller corpus. The development sets are all development sets and test se</context>
</contexts>
<marker>Foster, Kuhn, 2009</marker>
<rawString>George Foster and Roland Kuhn. 2009. Stabilizing minimum error rate training. In Proceedings of the 4th Workshop on Statistical Machine Translation(WMT), Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rejwanul Haque</author>
<author>Sudip Kumar Naskar</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<date>2009</date>
<booktitle>Experiments on Domain Adaptation for EnglishłHindi SMT. In 7th International Conference on Natural Language Processing(ICNLP),</booktitle>
<location>Hyderabad, India.</location>
<marker>Haque, Naskar, Van Genabith, Way, 2009</marker>
<rawString>Rejwanul Haque, Sudip Kumar Naskar, Josef Van Genabith, and Andy Way. 2009. Experiments on Domain Adaptation for EnglishłHindi SMT. In 7th International Conference on Natural Language Processing(ICNLP), Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd Workshop on Statistical Machine Translation(WMT),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11161" citStr="Koehn and Schroeder, 2007" startWordPosition="1834" endWordPosition="1837">BLEU and recall-BLEU. These results are illustrated in Figure 3. From the figure, we find that F1-BLEU plays an important role to predict the goodness of a development set, F1-BLEU scores of batch- sets have an ascending curve and batch data set sequence will cause a stable good test performance, the point on dev- sets which has high F1-BLEU(eg, dev-0,4,5) would also has a good test performance. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relevant with the test set in order to further improve translation performance. 4 Conclusion In this paper, we present our machine translation system for the WMT10 shared task and perform an empirical stu</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the 2nd Workshop on Statistical Machine Translation(WMT), Prague, Czech Republic.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics(ACL),</booktitle>
<location>Alexandra</location>
<contexts>
<context position="1387" citStr="Koehn et al., 2007" startWordPosition="199" endWordPosition="202">English. Our main contribution in this work is about effective parameter tuning. We discover that there is a significant performance gap as different development sets are adopted. Finally, ten groups of development sets are used to optimize the model weights, and this does help us obtain a stable evaluation result. 1 Introduction We present a machine translation system that represents our participation for the WMT10 shared task from Brain-like Computing and Machine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT toolkit MOSES (Koehn et al., 2007). We use it to translate German, French and Spanish into English. Though different development sets used for training parameter tuning will certainly lead to quite different performance, we empirically find that the more sets we combine together, the more stable the performance is, and a development set similar with test set will help the performance improvement. 2 System Description The basic model of the our system is a log-linear model (Och and Ney, 2002). For given source lanPhis work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No.</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics(ACL), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>NUS at WMT09: domain adaptation experiments for English-Spanish machine translation of news commentary text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation(WMT),</booktitle>
<contexts>
<context position="11194" citStr="Nakov and Ng, 2009" startWordPosition="1840" endWordPosition="1843">illustrated in Figure 3. From the figure, we find that F1-BLEU plays an important role to predict the goodness of a development set, F1-BLEU scores of batch- sets have an ascending curve and batch data set sequence will cause a stable good test performance, the point on dev- sets which has high F1-BLEU(eg, dev-0,4,5) would also has a good test performance. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relevant with the test set in order to further improve translation performance. 4 Conclusion In this paper, we present our machine translation system for the WMT10 shared task and perform an empirical study on the development set selecti</context>
</contexts>
<marker>Nakov, Ng, 2009</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2009. NUS at WMT09: domain adaptation experiments for English-Spanish machine translation of news commentary text. In Proceedings of the 4th Workshop on Statistical Machine Translation(WMT), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improving English-Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on Statistical Machine Translation(WMT),</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="11174" citStr="Nakov, 2008" startWordPosition="1838" endWordPosition="1839"> results are illustrated in Figure 3. From the figure, we find that F1-BLEU plays an important role to predict the goodness of a development set, F1-BLEU scores of batch- sets have an ascending curve and batch data set sequence will cause a stable good test performance, the point on dev- sets which has high F1-BLEU(eg, dev-0,4,5) would also has a good test performance. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relevant with the test set in order to further improve translation performance. 4 Conclusion In this paper, we present our machine translation system for the WMT10 shared task and perform an empirical study on the dev</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Improving English-Spanish statistical machine translation: Experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing. In Proceedings of the 3rd Workshop on Statistical Machine Translation(WMT), Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics(ACL),</booktitle>
<location>Philadelphia, Pennsylvanian, USA.</location>
<contexts>
<context position="1849" citStr="Och and Ney, 2002" startWordPosition="275" endWordPosition="278">achine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT toolkit MOSES (Koehn et al., 2007). We use it to translate German, French and Spanish into English. Though different development sets used for training parameter tuning will certainly lead to quite different performance, we empirically find that the more sets we combine together, the more stable the performance is, and a development set similar with test set will help the performance improvement. 2 System Description The basic model of the our system is a log-linear model (Och and Ney, 2002). For given source lanPhis work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315). corresponding author guage strings, the target language string t will be obtained by the following equation, P, = arg max{pλ- (tI1 sJ1)} t� exp[Em 1 λmhm(tI1, sJ1 )� = arg max{ M ti E¯tl exp[Em=1 λmhm (�tI1, s01 where hm is the m-th feature function and λm is the m-th model wei</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics(ACL), Philadelphia, Pennsylvanian, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3360" citStr="Och and Ney, 2003" startWordPosition="524" endWordPosition="527"> English, two sets of bilingual corpora are provided by the shared task organizer. The first set is the new release (version 5) of Europarl corpus which is the smaller. The second is a combination of other available data sets which is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table 1. Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++ (Och and Ney, 2003) to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in (Och and Ney, 2004). Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, 67 Proceedings of the Joint 5th Workshop </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3588" citStr="Och and Ney, 2004" startWordPosition="558" endWordPosition="561">ich is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table 1. Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++ (Och and Ney, 2003) to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in (Och and Ney, 2004). Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, 67 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67–71, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics sentences words(s) words(t) de small 1540549 35.76M 38.53M large 1640818 37.95M </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics(ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5303" citStr="Och, 2003" startWordPosition="813" endWordPosition="814">velopment sets with 18207 sentences to get a stable performance in our primary submission. We use the default toolkits which are provided by WMT10 organizers for preprocessing (i.e., tokenize) and postprocessing (i.e., detokenize, recaser). 3 Development Set Selection 3.1 Motivation Given the previous feature functions, the model weights will be obtained by optimizing the following maximum mutual information criterion, which can be derived from the maximum entropy principle: ��M = arg max{ )&apos;M 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 3.2 Experimental Settings Our empirical study will be demonstrated through German to</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics(ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>NICT@ WMT09: model adaptation and transliteration for Spanish-English SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation(WMT),</booktitle>
<contexts>
<context position="11213" citStr="Paul et al., 2009" startWordPosition="1844" endWordPosition="1847">e 3. From the figure, we find that F1-BLEU plays an important role to predict the goodness of a development set, F1-BLEU scores of batch- sets have an ascending curve and batch data set sequence will cause a stable good test performance, the point on dev- sets which has high F1-BLEU(eg, dev-0,4,5) would also has a good test performance. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relevant with the test set in order to further improve translation performance. 4 Conclusion In this paper, we present our machine translation system for the WMT10 shared task and perform an empirical study on the development set selection. According to ou</context>
</contexts>
<marker>Paul, Finch, Sumita, 2009</marker>
<rawString>Michael Paul, Andrew Finch, and Eiichiro Sumita. 2009. NICT@ WMT09: model adaptation and transliteration for Spanish-English SMT. In Proceedings of the 4th Workshop on Statistical Machine Translation(WMT), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In 7th International Conference on Spoken Language Processing(ICSLP),</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="4544" citStr="Stolcke, 2002" startWordPosition="695" endWordPosition="696">ngs of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67–71, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics sentences words(s) words(t) de small 1540549 35.76M 38.53M large 1640818 37.95M 40.64M fr small 1683156 44.02M 44.20M large 8997997 251.60M 228.50M es small 1650152 43.17M 41.25M large 7971200 236.24M 207.79M Table 1: Bilingual training corpora from German(de), French(fr) and Spanish(es) to English. which include 1968914 sentences and 47.48M words. And SRILM is adopted with 5-gram, interpolate and kndiscount settings (Stolcke, 2002) . The next step is to estimate feature weights by optimizing translation performance on a development set. We consider various combinations of 10 development sets with 18207 sentences to get a stable performance in our primary submission. We use the default toolkits which are provided by WMT10 organizers for preprocessing (i.e., tokenize) and postprocessing (i.e., detokenize, recaser). 3 Development Set Selection 3.1 Motivation Given the previous feature functions, the model weights will be obtained by optimizing the following maximum mutual information criterion, which can be derived from th</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM: an extensible language modeling toolkit. In 7th International Conference on Spoken Language Processing(ICSLP), Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Two methods for stabilizing MERT: NICT at IWSLT</title>
<date>2009</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation(IWSLT),</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="9511" citStr="Utiyama et al., 2009" startWordPosition="1546" endWordPosition="1549">ts itself as the development set. These two groups of results are illustrated in Figure 2 (the best score of the test set under self tuning, newstest2009 is 17.91). The dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to DATASETID Figure 2: The trend of BLEU score differences construct a specific tuning set. In our experiment, we will try to measure data set similarity instead. Given two sets of sentences, one is called as candidate(cnd) set and the other reference(ref) set. For any cnd sentence, we let the whole ref set to be its reference and then multi-references BLEU score is computed for cnd set. There comes a problem that the sentence penalty will be constant for any c</context>
</contexts>
<marker>Utiyama, Yamamoto, Sumita, 2009</marker>
<rawString>Masao Utiyama, Hirofumi Yamamoto, and Eiichiro Sumita. 2009. Two methods for stabilizing MERT: NICT at IWSLT 2009. In Proceedings of International Workshop on Spoken Language Translation(IWSLT), Tokyo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>