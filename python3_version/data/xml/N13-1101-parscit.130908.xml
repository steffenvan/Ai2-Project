<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005028">
<title confidence="0.999572">
Coherence Modeling for the Automated Assessment of
Spontaneous Spoken Responses
</title>
<author confidence="0.924482">
Xinhao Wang, Keelan Evanini, Klaus Zechner
</author>
<affiliation confidence="0.865351">
Educational Testing Service
</affiliation>
<address confidence="0.8793135">
660 Rosedale Road
Princeton, NJ 08541, USA
</address>
<email confidence="0.996228">
xwang002,kevanini,kzechner@ets.org
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926136363636">
This study focuses on modeling discourse co-
herence in the context of automated assess-
ment of spontaneous speech from non-native
speakers. Discourse coherence has always
been used as a key metric in human scoring
rubrics for various assessments of spoken lan-
guage. However, very little research has been
done to assess a speaker&apos;s coherence in auto-
mated speech scoring systems. To address
this, we present a corpus of spoken responses
that has been annotated for discourse coher-
ence quality. Then, we investigate the use of
several features originally developed for es-
says to model coherence in spoken responses.
An analysis on the annotated corpus shows
that the prediction accuracy for human holistic
scores of an automated speech scoring system
can be improved by around 10% relative after
the addition of the coherence features. Fur-
ther experiments indicate that a weighted F-
Measure of 73% can be achieved for the au-
tomated prediction of the coherence scores.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963367346939">
In recent years, much research has been conducted
into developing automated assessment systems to
automatically score spontaneous speech from non-
native speakers with the goals of reducing the bur-
den on human raters, improving reliability, and
generating feedback that can be used by language
learners. Various features related to different as-
pects of speaking proficiency have been exploited,
such as delivery features for pronunciation, proso-
dy, and fluency (Strik and Cucchiarini, 1999; Chen
et al., 2009; Cheng, 2011; Higgins et al., 2011), as
well as language use features for vocabulary and
grammar, and content features (Chen and Zechner,
2011; Xie et al., 2012). However, discourse-level
features related to topic development have rarely
been investigated in the context of automated
speech scoring. This is despite the fact that an im-
portant criterion in the human scoring rubrics for
speaking assessments is the evaluation of coher-
ence, which refers to the conceptual relations be-
tween different units within a response.
Methods for automatically assessing discourse
coherence in text documents have been widely
studied in the context of applications such as natu-
ral language generation, document summarization,
and assessment of text readability. For example,
Foltz et al. (1998) measured the overall coherence
of a text by utilizing Latent Semantic Analysis
(LSA) to calculate the semantic relatedness be-
tween adjacent sentences. Barzilay and Lee (2004)
introduced an HMM-based model for the docu-
ment-level analysis of topics and topic transitions.
Barzilay and Lapata (2005; 2008) presented an
approach to coherence modeling which focused on
the entities in the text and their grammatical transi-
tions between adjacent sentences, and calculated
the entity transition probabilities on the document
level. Pitler et al. (2010) provided a summary of
the performance of several different types of
features for automated coherence evaluation, such
as cohesive devices, adjacent sentence similarity,
Coh-Metrix (Graesser et al., 2004), word co-
occurrence patterns, and entity-grid.
In addition to studies on well-formed text, re-
searchers have also addressed coherence modeling
on text produced by language learners, which may
contain many spelling and grammar errors.
Utilizing LSA and Random Indexing methods,
Higgins et al. (2004) measured the global
</bodyText>
<page confidence="0.975246">
814
</page>
<subsectionHeader confidence="0.294587">
Proceedings of NAACL-HLT 2013, pages 814–819,
</subsectionHeader>
<bodyText confidence="0.992616843137255">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
coherence of students’ essays by calculating the
semantic relatedness between sentences and the
corresponding prompts. In addition, Burstein et. al
(2010) combined entity-grid features with writing
quality features produced by an automated assess-
ment system of essays to predict the coherence
scores of student essays. Recently, Yannakoudakis
and Briscoe (2012) systematically analyzed a vari-
ety of coherence modeling methods within the
framework of an automated assessment system for
non-native free text responses and indicated that
features based on Incremental Semantic Analysis
(ISA), local histograms of words, the part-of-
speech IBM model, and word length were the most
effective.
In contrast to these previous studies involving
well-formed text or learner text containing errors,
this paper focuses on modeling coherence in spon-
taneous spoken responses as well as investigating
discourse features in an attempt to extend the con-
struct coverage of an automated speech scoring
system. In a related study, Hassanali et al. (2012)
investigated coherence modeling for spoken lan-
guage in the context of a story retelling task for the
automated diagnosis of children with language im-
pairment. They annotated transcriptions of chil-
dren&apos;s narratives with coherence scores as well as
markers of narrative structure and narrative quali-
ty; furthermore they built models to predict the
coherence scores based on Coh-Metrix features
and the manually annotated narrative features. The
current study differs from this one in that it deals
with free spontaneous spoken responses provided
by students at a university level; these responses
therefore contain more varied and more complicat-
ed information than the child narratives.
The main contributions of this paper can be
summarized as follows: First, we obtained coher-
ence annotations on a corpus of spontaneous spo-
ken responses drawn from a university-level
English language proficiency assessment, and
demonstrated an improvement of around 10% rela-
tive in the accuracy of the automated prediction of
human holistic scores with the addition of the co-
herence annotations. Second, we applied the entity-
grid features and writing quality features from an
automated essay scoring system to predict the co-
herence scores; the experimental results have
shown promising correlations between some of
these features and the coherence scores.
</bodyText>
<sectionHeader confidence="0.744584" genericHeader="introduction">
2 Data and Annotation
</sectionHeader>
<subsectionHeader confidence="0.917587">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999923333333333">
For this study, we collected 600 spoken responses
from the international TOEFL® iBT assessment of
English proficiency for non-native speakers. 100
responses were drawn from each of 6 different test
questions comprising two different speaking tasks:
1) providing an opinion based on personal experi-
ence (N = 200) and 2) summarizing or discussing
material provided in a reading and/or listening pas-
sage (N = 400). The spoken responses were all
transcribed by humans with punctuation and capi-
talization. The average number of words contained
in the responses was 104.4 (st. dev. = 34.4) and the
average number of sentences was 5.5 (st. dev. =
2.1).
The spoken responses were all provided with
holistic English proficiency scores on a scale of 1 -
4 by expert human raters in the context of opera-
tional, high-stakes scoring for the spoken language
assessment. The scoring rubrics address the fol-
lowing three main aspects of speaking proficiency:
delivery (pronunciation, fluency, prosody), lan-
guage use (grammar and lexical choice), and topic
development (content and coherence). In order to
ensure a sufficient quantity of responses from each
proficiency level for training and evaluating the
coherence prediction features, the spoken respons-
es selected for this study were balanced based on
the human scores as follows: 25 responses were
selected randomly from each of the 4 score points
(1 - 4) for each of the 6 test questions. In some
cases, more than one response was selected from a
given test-taker; in total, 471 distinct test-takers are
represented in the data set.
</bodyText>
<subsectionHeader confidence="0.999626">
2.2 Annotation and Analysis
</subsectionHeader>
<bodyText confidence="0.999771363636364">
The coherence annotation guidelines used for the
spoken responses in this study were modified
based on the annotation guidelines developed for
written essays described in Burstein et al. (2010).
According to these guidelines, expert annotators
provided each response with a score on a scale of 1
- 3. The three score points were defined as follows:
3 = highly coherent (contains no instances of con-
fusing arguments or examples), 2 = somewhat co-
herent (contains some awkward points in which the
speaker&apos;s line of argument is unclear), 1 = barely
</bodyText>
<page confidence="0.991455">
815
</page>
<bodyText confidence="0.99987064">
coherent (the entire response was confusing and
hard to follow; it was intuitively incoherent as a
whole and the annotators had difficulties in identi-
fying specific weak points). For responses receiv-
ing a coherence score of 2, the annotators were
required to highlight the specific awkward points
in the response. In addition, the annotators were
specifically required to ignore disfluencies and
grammatical errors as much as possible; thus, they
were instructed to not label sentences or clauses as
awkward points solely because of the presence of
disfluent or ungrammatical speech.
Two annotators (not drawn from the pool of ex-
pert human raters who provided the holistic scores)
made independent coherence annotations for all
600 spoken responses. The distribution of annota-
tions across the three score points is presented in
Table 1. The two annotators achieved a moderate
inter-annotator agreement (Landis and Koch, 1977)
of x = 0.68 on the 3-point scale. The average of the
two coherence scores provided by the two annota-
tors correlates with the holistic speaking proficien-
cy scores at r = 0.66, indicating that the overall
proficiency scores of spoken responses can benefit
from the discourse coherence annotations.
</bodyText>
<table confidence="0.999202666666667">
1 2 3
# 1 160 (27%) 278 (46%) 162 (27%)
# 2 125 (21%) 251 (42%) 224 (37%)
</table>
<tableCaption confidence="0.9883635">
Table 1. Distribution of coherence annotations from two
annotators
</tableCaption>
<bodyText confidence="0.999534230769231">
Furthermore, coherence features based on the
human annotations were examined within the con-
text of an automated spoken language assessment
system, SpeechRaterSM (Zechner et al., 2007;
2009). We extracted 96 features related to pronun-
ciation, prosody, fluency, language use, and con-
tent development using SpeechRater. These
features were either extracted directly from the
speech signal or were based on the output of an
automatic speech recognition system (with a word
error rate of around 28%1 ). By utilizing a decision
tree classifier (the J48 implementation from Weka
(Hall et al., 2009)), 4-fold cross validation was
</bodyText>
<footnote confidence="0.8010386">
1 Both the training and evaluation sets used to develop the
speech recognizer consist of similar spoken responses drawn
from the same assessment. However, there is no response
overlap between these sets and the corpus used for discourse
coherence annotation in this study.
</footnote>
<bodyText confidence="0.999924777777778">
conducted on the 600 responses to train and evalu-
ate a scoring model for predicting the holistic pro-
ficiency scores. The resulting correlation between
the predicted scores (based on the 96 baseline
SpeechRater features) and the human holistic pro-
ficiency scores was r = 0.667.
In order to model a spoken response&apos;s coher-
ence, three different features were extracted from
the human annotations. Firstly, the average of the
two annotators’ coherence scores was directly used
as a feature with a 5-point scale (henceforth
Coh_5). Secondly, following the work in Burstein
et al. (2010), we collapsed the average coherence
scores into a 2-point scale to deal with the
difficulty in distinguishing somewhat and highly
coherent responses. For this second feature
(henceforth Coh_2), scores 1 and 1.5 were mapped
to score 1, and scores 2, 2.5, and 3 were mapped to
score 2. Finally, the number of awkward points
was also counted as a feature (henceforth Awk).
As shown in Table 2, when these three coherence
features were combined separately with the
SpeechRater features, the correlations could be
improved from r = 0.667 to r &gt; 0.7. Meanwhile,
the accuracy (i.e., the percentage of correctly pre-
dicted holistic scores) could be improved from
0.487 to a range between 0.535 and 0.543.
</bodyText>
<table confidence="0.997631714285714">
Features r Accuracy
SpeechRater 0.667 0.487
SpeechRater+Coh_5 0.714 0.540
SpeechRater+Coh_2 0.705 0.543
SpeechRater+Awk 0.702 0.535
SpeechRater+Coh_5+Awk 0.703 0.537
SpeechRater+Coh_2+Awk 0.701 0.542
</table>
<tableCaption confidence="0.994896">
Table 2. Improvement to an automated speech scoring
</tableCaption>
<bodyText confidence="0.870655071428571">
system after the addition of human-assigned coherence
scores and measures, showing both Pearson r correla-
tions and the ratio of correctly matched holistic scores
between the system and human experts
These experimental results demonstrate that the
automatic scoring system can benefit from coher-
ence modeling either by directly using a human-
assigned coherence score or the identified awk-
ward points. However, the use of both kinds of
annotations does not provide further improvement.
When collapsing the average scores into a 2-point
scale, there was a 0.009 correlation drop (not sta-
tistically significant), but the accuracy was slightly
improved. In addition, due to the relatively small
</bodyText>
<page confidence="0.996676">
816
</page>
<bodyText confidence="0.9999825">
size of the set of available coherence annotations,
we adopted the collapsed 2-point scale instead of
the 5-point scale for the coherence prediction ex-
periments in the next section.
</bodyText>
<subsectionHeader confidence="0.986749">
2.3 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999987739130435">
As demonstrated in Section 2.2, the collapsed av-
erage coherence score can be used to improve the
performance of an automated speech scoring sys-
tem. Therefore, this study treats coherence predic-
tion as a binary classification task: low-coherent
vs. high-coherent, where the low-coherent re-
sponses are those with average scores 1 and 1.5,
and the high-coherent responses are those with av-
erage scores 2, 2.5, and 3.
For coherence modeling, we again use the J48
decision tree from the Weka machine learning
toolkit (Hall et al., 2009) and run 4-fold cross-
validation on the 600 annotated responses. The
correlation coefficient (r) and the weighted aver-
age F-Measure2 are used as evaluation metrics.
In this experiment, we examine the performance
of the entity-grid features and a set of features pro-
duced by the e-rater® system (an automated writ-
ing assessment system for learner essays) (Attali
and Burstein, 2006) to predict the coherence scores
of the spontaneous spoken responses, where all the
features are extracted from human transcriptions of
the responses.
</bodyText>
<subsectionHeader confidence="0.998888">
2.4 Entity Grid and e-rater Features
</subsectionHeader>
<bodyText confidence="0.999417108695652">
First, we applied the algorithm from Barzilay and
Lapata (2008) to extract entity-grid features, which
calculated the vector of entity transition probabili-
ties across adjacent sentences. Several different
methods of representing the entities can be used
before generating the entity-grid. First, all the enti-
ties can be described by their syntactic roles in-
cluding S (Subject), O (Object), and X (Other).
Alternatively, these roles can also be reduced to P
(Present) or N (Absent). Furthermore, entities can
be defined as salient, when they appear two or
more times, otherwise as non-salient. In this study,
2 The data distribution in the experimental corpus is unbal-
anced: 71% of the responses are high-coherent and 29% are
low-coherent. Therefore, we adopt the weighted average F-
Measure to evaluate the performance of coherence prediction:
first, the F1-Measure of each category is calculated, and then
the percentages of responses in each category are used as
weights to obtain the final weighted average F-Measure.
we generated there basic entity grids: EG_SOX
(entity grid with the syntactic roles S, O, and X),
EG_REDUCED (entity grid with the reduced rep-
resentations P and N), and EG_SALIENT (entity
grid with salient and non-salient entities). In addi-
tion to these entity-grid features, we also used 130
writing quality features related to grammar, usage,
mechanics, and style from e-rater to model the co-
herence.
A baseline system for this task would simply as-
sign the majority class (high-coherent) to all of the
responses; this baseline achieves an F-Measure of
0.587. Table 3 shows that the EG_REDUCED and
e-rater features can obtain F-Measures of 0.677
and 0.726 as well as correlations with human
scores of 0.20 and 0.33, respectively. However, the
combination of the two sets of features only brings
a very small improvement (from 0.33 to 0.34). In
addition, our experiments show that by introducing
the component of co-reference resolution for entity
grid building, we can only get a very slight im-
provement on EG_SALIENT, but no improvement
on EG_SOX and EG_REDUCED. That may be
because it is generally more difficult to parse the
transcriptions of spoken language than well-
formed text, and more errors are introduced during
the process of co-reference resolution.
</bodyText>
<table confidence="0.999091222222222">
r F-Measure
Baseline 0.0 0.587
EG_SOX 0.16 0.664
EG_REDUCED 0.2 0.677
EG_SALIENT 0.2 0.678
e-rater 0.33 0.726
EG_SOX +e-rater 0.30 0.714
EG_REDUCED +e-rater 0.34 0.73
EG_SALIENT + e-rater 0.26 0.695
</table>
<tableCaption confidence="0.985243">
Table 3. Performance of entity grid and e-rater features
on the coherence modeling task
</tableCaption>
<subsectionHeader confidence="0.970065">
2.5 Discussion and Future Work
</subsectionHeader>
<bodyText confidence="0.999888222222222">
In order to further analyze these features, the cor-
relation coefficients between various features and
the average coherence scores (on a five-point
scale) were calculated; Figure 1 shows the histo-
gram of these correlation values. As the figure
shows, there are a total of approximately 50 fea-
tures with correlations larger than 0.1. Four of the
entity-grid features have correlations between 0.15
and 0.29. As for the writing quality features, some
</bodyText>
<page confidence="0.992024">
817
</page>
<bodyText confidence="0.958250259259259">
of them show high correlations with the average
coherence scores, despite the fact that they are not
explicitly related to discourse coherence, such as
the number of good lexical collocations.
Based on the above analysis, we plan to investi-
gate additional superficial features explicitly relat-
ed to discourse coherence, such as the distribution
of conjunctions, pronouns, and discourse connec-
tives. Moreover, based on the research on well-
formed texts and learner essays, we will attempt to
examine more effective features and models to bet-
ter cover the discourse aspects of spontaneous
speech. For example, local semantic features relat-
ed to inter-sentential coherence and the ISA feature
will be investigated on spoken responses. In addi-
tion, we will apply the features and build coher-
ence models using the output of automatic speech
recognition in addition to human transcriptions.
Finally, various coherence features or models will
be integrated into a practical automated scoring
system, and further experiments will be performed
to measure their effect on the performance of au-
tomated assessment of spontaneous spoken re-
sponses.
Figure1. Histogram of entity-grid and writing quality
features based on their correlations with coherence
scores
</bodyText>
<sectionHeader confidence="0.999287" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999945636363636">
In this paper, we present a corpus of coherence
annotations for spontaneous spoken responses pro-
vided in the context of an English speaking profi-
ciency assessment. Entity-grid features and fea-
tures from an automated essay scoring system were
examined for coherence modeling of spoken re-
sponses. The analysis on the annotated corpus
showed promising results for improving the per-
formance of an automated scoring system by
means of modeling the coherence of spoken re-
sponses.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999876">
The authors wish to express our thanks to the dis-
course annotators Melissa Lopez and Matt Mulhol-
land for their dedicated work and our colleagues
Jill Burstein and Slava Andreyev for their support
in generating entity-grid features.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998784114285714">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater® V.2.0. Journal of Technology,
Learning, and Assessment. 4(3): 159-174.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. Proceedings of
NAACL-HLT, 113-120.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach.
Proceedings of ACL, 141-148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach.
Computational Linguistics, 34(1):1-34.
Jill Burstein, Joel Tetreault and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. Proceedings of NAACL-HLT, 681-
684. Los Angeles, California.
Lei Chen, Klaus Zechner and Xiaoming Xi. 2009.
Improved pronunciation features for construct-
driven assessment of non-native spontaneous
speech. Proceedings of NAACL-HLT, 442-449.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for
automated scoring of spontaneous non-native
speech. Proceedings of ACL, 722-731.
Jian Cheng. 2011. Automatic assessment of prosody in
high-stakes English tests. Proceedings of
Interspeech , 27-31.
Peter W. Foltz, Walter Kintsch and Thomas K.
Landauer. 1998. The measurement of textual
coherence with Latent Semantic Analysis. Discourse
Processes, 25(2&amp;3):285-307.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse and Zhiqiang Cai. 2004. Coh-Metrix:
Analysis of text on cohesion and language. Behavior
</reference>
<page confidence="0.980849">
818
</page>
<reference confidence="0.999317791666667">
Research Methods, Instruments, &amp; Computers,
36(2):193-202.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10-18.
Khairun-nisa Hassanali, Yang Liu and Thamar Solorio.
2012. Coherence in child language narratives: A
case study of annotation and automatic prediction of
coherence. Proceedings of the Interspeech
Workshop on Child, Computer and Interaction.
Derrick Higgins, Jill Burstein, Daniel Marcu and
Claudia Gentile. 2004. Evaluating multiple aspects
of coherence in student essays. Proceedings of
NAACL-HLT, 185-192.
Derrick Higgins, Xiaoming Xi, Klaus Zechner and
David Williamson. 2011. A three-stage approach to
the automated scoring of spontaneous. Computer
Speech and Language, 25:282-306.
J. Richard Landis and Gary G. Koch. 1977. The
measurement of observer agreement for categorical
data. Biometrics, 33(1):159-174.
Emily Pitler, Annie Louis and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. Proceedings of ACL.
544–554. Uppsala.
Helmer Strik and Catia Cucchiarini. 1999. Automatic
assessment of second language learners&apos; fluency.
Proceedings of the 14th International Congress of
Phonetic Sciences, 759-762. Berkeley, CA.
Shasha Xie, Keelan Evanini and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. Proceedings of NAACL-HLT, 103-111.
Helen Yannakoudakis and Ted Briscoe. 2012. Modeling
coherence in ESOL learner texts. Proceedings of the
7th Workshop on the Innovative Use of NLP for
Building Educational Applications, 33-43. Montreal.
Klaus Zechner, Derrick Higgins and Xiaoming Xi.
2007. SpeechRaterSM
to scoring spontaneous non-native speech.
Proceedings of the International Speech
Communication Association Special Interest Group
on Speech and Language Technology in Education,
128-131.
Klaus Zechner, Derrick Higgins, Xiaoming Xi and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883-895.
</reference>
<figure confidence="0.229789">
: A construct-driven approach
</figure>
<page confidence="0.989307">
819
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896092">
<title confidence="0.988166">Coherence Modeling for the Automated Assessment Spontaneous Spoken Responses</title>
<author confidence="0.983978">Xinhao Wang</author>
<author confidence="0.983978">Keelan Evanini</author>
<author confidence="0.983978">Klaus</author>
<affiliation confidence="0.976903">Educational Testing</affiliation>
<address confidence="0.998515">660 Rosedale Princeton, NJ 08541,</address>
<email confidence="0.983008">xwang002,kevanini,kzechner@ets.org</email>
<abstract confidence="0.997960739130435">This study focuses on modeling discourse coherence in the context of automated assessment of spontaneous speech from non-native speakers. Discourse coherence has always been used as a key metric in human scoring rubrics for various assessments of spoken language. However, very little research has been done to assess a speaker&apos;s coherence in automated speech scoring systems. To address this, we present a corpus of spoken responses that has been annotated for discourse coherence quality. Then, we investigate the use of several features originally developed for essays to model coherence in spoken responses. An analysis on the annotated corpus shows that the prediction accuracy for human holistic scores of an automated speech scoring system can be improved by around 10% relative after the addition of the coherence features. Further experiments indicate that a weighted F- Measure of 73% can be achieved for the automated prediction of the coherence scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater® V.2.0.</title>
<date>2006</date>
<journal>Journal of Technology, Learning, and Assessment.</journal>
<volume>4</volume>
<issue>3</issue>
<pages>159--174</pages>
<contexts>
<context position="13858" citStr="Attali and Burstein, 2006" startWordPosition="2141" endWordPosition="2144"> responses are those with average scores 1 and 1.5, and the high-coherent responses are those with average scores 2, 2.5, and 3. For coherence modeling, we again use the J48 decision tree from the Weka machine learning toolkit (Hall et al., 2009) and run 4-fold crossvalidation on the 600 annotated responses. The correlation coefficient (r) and the weighted average F-Measure2 are used as evaluation metrics. In this experiment, we examine the performance of the entity-grid features and a set of features produced by the e-rater® system (an automated writing assessment system for learner essays) (Attali and Burstein, 2006) to predict the coherence scores of the spontaneous spoken responses, where all the features are extracted from human transcriptions of the responses. 2.4 Entity Grid and e-rater Features First, we applied the algorithm from Barzilay and Lapata (2008) to extract entity-grid features, which calculated the vector of entity transition probabilities across adjacent sentences. Several different methods of representing the entities can be used before generating the entity-grid. First, all the entities can be described by their syntactic roles including S (Subject), O (Object), and X (Other). Alterna</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater® V.2.0. Journal of Technology, Learning, and Assessment. 4(3): 159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2680" citStr="Barzilay and Lee (2004)" startWordPosition="403" endWordPosition="406">mportant criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patte</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. Proceedings of NAACL-HLT, 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="2801" citStr="Barzilay and Lapata (2005" startWordPosition="421" endWordPosition="424">o the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on t</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. Proceedings of ACL, 141-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--1</pages>
<contexts>
<context position="14109" citStr="Barzilay and Lapata (2008)" startWordPosition="2179" endWordPosition="2182">run 4-fold crossvalidation on the 600 annotated responses. The correlation coefficient (r) and the weighted average F-Measure2 are used as evaluation metrics. In this experiment, we examine the performance of the entity-grid features and a set of features produced by the e-rater® system (an automated writing assessment system for learner essays) (Attali and Burstein, 2006) to predict the coherence scores of the spontaneous spoken responses, where all the features are extracted from human transcriptions of the responses. 2.4 Entity Grid and e-rater Features First, we applied the algorithm from Barzilay and Lapata (2008) to extract entity-grid features, which calculated the vector of entity transition probabilities across adjacent sentences. Several different methods of representing the entities can be used before generating the entity-grid. First, all the entities can be described by their syntactic roles including S (Subject), O (Object), and X (Other). Alternatively, these roles can also be reduced to P (Present) or N (Absent). Furthermore, entities can be defined as salient, when they appear two or more times, otherwise as non-salient. In this study, 2 The data distribution in the experimental corpus is u</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Joel Tetreault</author>
<author>Slava Andreyev</author>
</authors>
<title>Using entity-based features to model coherence in student essays.</title>
<date>2010</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>681--684</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="7904" citStr="Burstein et al. (2010)" startWordPosition="1198" endWordPosition="1201">and evaluating the coherence prediction features, the spoken responses selected for this study were balanced based on the human scores as follows: 25 responses were selected randomly from each of the 4 score points (1 - 4) for each of the 6 test questions. In some cases, more than one response was selected from a given test-taker; in total, 471 distinct test-takers are represented in the data set. 2.2 Annotation and Analysis The coherence annotation guidelines used for the spoken responses in this study were modified based on the annotation guidelines developed for written essays described in Burstein et al. (2010). According to these guidelines, expert annotators provided each response with a score on a scale of 1 - 3. The three score points were defined as follows: 3 = highly coherent (contains no instances of confusing arguments or examples), 2 = somewhat coherent (contains some awkward points in which the speaker&apos;s line of argument is unclear), 1 = barely 815 coherent (the entire response was confusing and hard to follow; it was intuitively incoherent as a whole and the annotators had difficulties in identifying specific weak points). For responses receiving a coherence score of 2, the annotators we</context>
<context position="11099" citStr="Burstein et al. (2010)" startWordPosition="1711" endWordPosition="1714">urse coherence annotation in this study. conducted on the 600 responses to train and evaluate a scoring model for predicting the holistic proficiency scores. The resulting correlation between the predicted scores (based on the 96 baseline SpeechRater features) and the human holistic proficiency scores was r = 0.667. In order to model a spoken response&apos;s coherence, three different features were extracted from the human annotations. Firstly, the average of the two annotators’ coherence scores was directly used as a feature with a 5-point scale (henceforth Coh_5). Secondly, following the work in Burstein et al. (2010), we collapsed the average coherence scores into a 2-point scale to deal with the difficulty in distinguishing somewhat and highly coherent responses. For this second feature (henceforth Coh_2), scores 1 and 1.5 were mapped to score 1, and scores 2, 2.5, and 3 were mapped to score 2. Finally, the number of awkward points was also counted as a feature (henceforth Awk). As shown in Table 2, when these three coherence features were combined separately with the SpeechRater features, the correlations could be improved from r = 0.667 to r &gt; 0.7. Meanwhile, the accuracy (i.e., the percentage of corre</context>
</contexts>
<marker>Burstein, Tetreault, Andreyev, 2010</marker>
<rawString>Jill Burstein, Joel Tetreault and Slava Andreyev. 2010. Using entity-based features to model coherence in student essays. Proceedings of NAACL-HLT, 681-684. Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
</authors>
<title>Klaus Zechner and Xiaoming Xi.</title>
<date>2009</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>442--449</pages>
<marker>Chen, 2009</marker>
<rawString>Lei Chen, Klaus Zechner and Xiaoming Xi. 2009. Improved pronunciation features for constructdriven assessment of non-native spontaneous speech. Proceedings of NAACL-HLT, 442-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miao Chen</author>
<author>Klaus Zechner</author>
</authors>
<title>Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech.</title>
<date>2011</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>722--731</pages>
<contexts>
<context position="1865" citStr="Chen and Zechner, 2011" startWordPosition="281" endWordPosition="284">rch has been conducted into developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text reada</context>
</contexts>
<marker>Chen, Zechner, 2011</marker>
<rawString>Miao Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. Proceedings of ACL, 722-731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Cheng</author>
</authors>
<title>Automatic assessment of prosody in high-stakes English tests.</title>
<date>2011</date>
<booktitle>Proceedings of Interspeech ,</booktitle>
<pages>27--31</pages>
<contexts>
<context position="1735" citStr="Cheng, 2011" startWordPosition="262" endWordPosition="263">of 73% can be achieved for the automated prediction of the coherence scores. 1 Introduction In recent years, much research has been conducted into developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widel</context>
</contexts>
<marker>Cheng, 2011</marker>
<rawString>Jian Cheng. 2011. Automatic assessment of prosody in high-stakes English tests. Proceedings of Interspeech , 27-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas K Landauer</author>
</authors>
<title>The measurement of textual coherence with Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="2505" citStr="Foltz et al. (1998)" startWordPosition="377" endWordPosition="380">However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several diff</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter W. Foltz, Walter Kintsch and Thomas K. Landauer. 1998. The measurement of textual coherence with Latent Semantic Analysis. Discourse Processes, 25(2&amp;3):285-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur C Graesser</author>
<author>Danielle S McNamara</author>
<author>Max M Louwerse</author>
<author>Zhiqiang Cai</author>
</authors>
<title>Coh-Metrix: Analysis of text on cohesion and language.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<pages>36--2</pages>
<contexts>
<context position="3255" citStr="Graesser et al., 2004" startWordPosition="486" endWordPosition="489">een adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on text produced by language learners, which may contain many spelling and grammar errors. Utilizing LSA and Random Indexing methods, Higgins et al. (2004) measured the global 814 Proceedings of NAACL-HLT 2013, pages 814–819, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics coherence of students’ essays by calculating the semantic relatedness between sentences and the corresponding prompts. In addition, Burstein et. al (</context>
</contexts>
<marker>Graesser, McNamara, Louwerse, Cai, 2004</marker>
<rawString>Arthur C. Graesser, Danielle S. McNamara, Max M. Louwerse and Zhiqiang Cai. 2004. Coh-Metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments, &amp; Computers, 36(2):193-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<pages>11--1</pages>
<contexts>
<context position="10214" citStr="Hall et al., 2009" startWordPosition="1571" endWordPosition="1574">ations from two annotators Furthermore, coherence features based on the human annotations were examined within the context of an automated spoken language assessment system, SpeechRaterSM (Zechner et al., 2007; 2009). We extracted 96 features related to pronunciation, prosody, fluency, language use, and content development using SpeechRater. These features were either extracted directly from the speech signal or were based on the output of an automatic speech recognition system (with a word error rate of around 28%1 ). By utilizing a decision tree classifier (the J48 implementation from Weka (Hall et al., 2009)), 4-fold cross validation was 1 Both the training and evaluation sets used to develop the speech recognizer consist of similar spoken responses drawn from the same assessment. However, there is no response overlap between these sets and the corpus used for discourse coherence annotation in this study. conducted on the 600 responses to train and evaluate a scoring model for predicting the holistic proficiency scores. The resulting correlation between the predicted scores (based on the 96 baseline SpeechRater features) and the human holistic proficiency scores was r = 0.667. In order to model a</context>
<context position="13478" citStr="Hall et al., 2009" startWordPosition="2081" endWordPosition="2084"> for the coherence prediction experiments in the next section. 2.3 Experimental Design As demonstrated in Section 2.2, the collapsed average coherence score can be used to improve the performance of an automated speech scoring system. Therefore, this study treats coherence prediction as a binary classification task: low-coherent vs. high-coherent, where the low-coherent responses are those with average scores 1 and 1.5, and the high-coherent responses are those with average scores 2, 2.5, and 3. For coherence modeling, we again use the J48 decision tree from the Weka machine learning toolkit (Hall et al., 2009) and run 4-fold crossvalidation on the 600 annotated responses. The correlation coefficient (r) and the weighted average F-Measure2 are used as evaluation metrics. In this experiment, we examine the performance of the entity-grid features and a set of features produced by the e-rater® system (an automated writing assessment system for learner essays) (Attali and Burstein, 2006) to predict the coherence scores of the spontaneous spoken responses, where all the features are extracted from human transcriptions of the responses. 2.4 Entity Grid and e-rater Features First, we applied the algorithm </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khairun-nisa Hassanali</author>
<author>Yang Liu</author>
<author>Thamar Solorio</author>
</authors>
<title>Coherence in child language narratives: A case study of annotation and automatic prediction of coherence.</title>
<date>2012</date>
<booktitle>Proceedings of the Interspeech Workshop on Child, Computer and Interaction.</booktitle>
<contexts>
<context position="4739" citStr="Hassanali et al. (2012)" startWordPosition="702" endWordPosition="705">odeling methods within the framework of an automated assessment system for non-native free text responses and indicated that features based on Incremental Semantic Analysis (ISA), local histograms of words, the part-ofspeech IBM model, and word length were the most effective. In contrast to these previous studies involving well-formed text or learner text containing errors, this paper focuses on modeling coherence in spontaneous spoken responses as well as investigating discourse features in an attempt to extend the construct coverage of an automated speech scoring system. In a related study, Hassanali et al. (2012) investigated coherence modeling for spoken language in the context of a story retelling task for the automated diagnosis of children with language impairment. They annotated transcriptions of children&apos;s narratives with coherence scores as well as markers of narrative structure and narrative quality; furthermore they built models to predict the coherence scores based on Coh-Metrix features and the manually annotated narrative features. The current study differs from this one in that it deals with free spontaneous spoken responses provided by students at a university level; these responses ther</context>
</contexts>
<marker>Hassanali, Liu, Solorio, 2012</marker>
<rawString>Khairun-nisa Hassanali, Yang Liu and Thamar Solorio. 2012. Coherence in child language narratives: A case study of annotation and automatic prediction of coherence. Proceedings of the Interspeech Workshop on Child, Computer and Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Claudia Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="3552" citStr="Higgins et al. (2004)" startWordPosition="530" endWordPosition="533">tween adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on text produced by language learners, which may contain many spelling and grammar errors. Utilizing LSA and Random Indexing methods, Higgins et al. (2004) measured the global 814 Proceedings of NAACL-HLT 2013, pages 814–819, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics coherence of students’ essays by calculating the semantic relatedness between sentences and the corresponding prompts. In addition, Burstein et. al (2010) combined entity-grid features with writing quality features produced by an automated assessment system of essays to predict the coherence scores of student essays. Recently, Yannakoudakis and Briscoe (2012) systematically analyzed a variety of coherence modeling methods within the framework</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>Derrick Higgins, Jill Burstein, Daniel Marcu and Claudia Gentile. 2004. Evaluating multiple aspects of coherence in student essays. Proceedings of NAACL-HLT, 185-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>Klaus Zechner</author>
<author>David Williamson</author>
</authors>
<title>A three-stage approach to the automated scoring of spontaneous.</title>
<date>2011</date>
<journal>Computer Speech and Language,</journal>
<pages>25--282</pages>
<contexts>
<context position="1758" citStr="Higgins et al., 2011" startWordPosition="264" endWordPosition="267"> achieved for the automated prediction of the coherence scores. 1 Introduction In recent years, much research has been conducted into developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the contex</context>
</contexts>
<marker>Higgins, Xi, Zechner, Williamson, 2011</marker>
<rawString>Derrick Higgins, Xiaoming Xi, Klaus Zechner and David Williamson. 2011. A three-stage approach to the automated scoring of spontaneous. Computer Speech and Language, 25:282-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--1</pages>
<contexts>
<context position="9185" citStr="Landis and Koch, 1977" startWordPosition="1403" endWordPosition="1406"> response. In addition, the annotators were specifically required to ignore disfluencies and grammatical errors as much as possible; thus, they were instructed to not label sentences or clauses as awkward points solely because of the presence of disfluent or ungrammatical speech. Two annotators (not drawn from the pool of expert human raters who provided the holistic scores) made independent coherence annotations for all 600 spoken responses. The distribution of annotations across the three score points is presented in Table 1. The two annotators achieved a moderate inter-annotator agreement (Landis and Koch, 1977) of x = 0.68 on the 3-point scale. The average of the two coherence scores provided by the two annotators correlates with the holistic speaking proficiency scores at r = 0.66, indicating that the overall proficiency scores of spoken responses can benefit from the discourse coherence annotations. 1 2 3 # 1 160 (27%) 278 (46%) 162 (27%) # 2 125 (21%) 251 (42%) 224 (37%) Table 1. Distribution of coherence annotations from two annotators Furthermore, coherence features based on the human annotations were examined within the context of an automated spoken language assessment system, SpeechRaterSM (</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic evaluation of linguistic quality in multidocument summarization.</title>
<date>2010</date>
<booktitle>Proceedings of ACL.</booktitle>
<pages>544--554</pages>
<location>Uppsala.</location>
<contexts>
<context position="3051" citStr="Pitler et al. (2010)" startWordPosition="458" endWordPosition="461">tion, and assessment of text readability. For example, Foltz et al. (1998) measured the overall coherence of a text by utilizing Latent Semantic Analysis (LSA) to calculate the semantic relatedness between adjacent sentences. Barzilay and Lee (2004) introduced an HMM-based model for the document-level analysis of topics and topic transitions. Barzilay and Lapata (2005; 2008) presented an approach to coherence modeling which focused on the entities in the text and their grammatical transitions between adjacent sentences, and calculated the entity transition probabilities on the document level. Pitler et al. (2010) provided a summary of the performance of several different types of features for automated coherence evaluation, such as cohesive devices, adjacent sentence similarity, Coh-Metrix (Graesser et al., 2004), word cooccurrence patterns, and entity-grid. In addition to studies on well-formed text, researchers have also addressed coherence modeling on text produced by language learners, which may contain many spelling and grammar errors. Utilizing LSA and Random Indexing methods, Higgins et al. (2004) measured the global 814 Proceedings of NAACL-HLT 2013, pages 814–819, Atlanta, Georgia, 9–14 June </context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2010</marker>
<rawString>Emily Pitler, Annie Louis and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multidocument summarization. Proceedings of ACL. 544–554. Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmer Strik</author>
<author>Catia Cucchiarini</author>
</authors>
<title>Automatic assessment of second language learners&apos; fluency.</title>
<date>1999</date>
<booktitle>Proceedings of the 14th International Congress of Phonetic Sciences,</booktitle>
<pages>759--762</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="1703" citStr="Strik and Cucchiarini, 1999" startWordPosition="254" endWordPosition="257">r experiments indicate that a weighted FMeasure of 73% can be achieved for the automated prediction of the coherence scores. 1 Introduction In recent years, much research has been conducted into developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence i</context>
</contexts>
<marker>Strik, Cucchiarini, 1999</marker>
<rawString>Helmer Strik and Catia Cucchiarini. 1999. Automatic assessment of second language learners&apos; fluency. Proceedings of the 14th International Congress of Phonetic Sciences, 759-762. Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Exploring content features for automated speech scoring.</title>
<date>2012</date>
<booktitle>Proceedings of NAACL-HLT,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="1884" citStr="Xie et al., 2012" startWordPosition="285" endWordPosition="288">nto developing automated assessment systems to automatically score spontaneous speech from nonnative speakers with the goals of reducing the burden on human raters, improving reliability, and generating feedback that can be used by language learners. Various features related to different aspects of speaking proficiency have been exploited, such as delivery features for pronunciation, prosody, and fluency (Strik and Cucchiarini, 1999; Chen et al., 2009; Cheng, 2011; Higgins et al., 2011), as well as language use features for vocabulary and grammar, and content features (Chen and Zechner, 2011; Xie et al., 2012). However, discourse-level features related to topic development have rarely been investigated in the context of automated speech scoring. This is despite the fact that an important criterion in the human scoring rubrics for speaking assessments is the evaluation of coherence, which refers to the conceptual relations between different units within a response. Methods for automatically assessing discourse coherence in text documents have been widely studied in the context of applications such as natural language generation, document summarization, and assessment of text readability. For example</context>
</contexts>
<marker>Xie, Evanini, Zechner, 2012</marker>
<rawString>Shasha Xie, Keelan Evanini and Klaus Zechner. 2012. Exploring content features for automated speech scoring. Proceedings of NAACL-HLT, 103-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
</authors>
<title>Modeling coherence in ESOL learner texts.</title>
<date>2012</date>
<booktitle>Proceedings of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>33--43</pages>
<location>Montreal.</location>
<contexts>
<context position="4067" citStr="Yannakoudakis and Briscoe (2012)" startWordPosition="600" endWordPosition="603"> which may contain many spelling and grammar errors. Utilizing LSA and Random Indexing methods, Higgins et al. (2004) measured the global 814 Proceedings of NAACL-HLT 2013, pages 814–819, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics coherence of students’ essays by calculating the semantic relatedness between sentences and the corresponding prompts. In addition, Burstein et. al (2010) combined entity-grid features with writing quality features produced by an automated assessment system of essays to predict the coherence scores of student essays. Recently, Yannakoudakis and Briscoe (2012) systematically analyzed a variety of coherence modeling methods within the framework of an automated assessment system for non-native free text responses and indicated that features based on Incremental Semantic Analysis (ISA), local histograms of words, the part-ofspeech IBM model, and word length were the most effective. In contrast to these previous studies involving well-formed text or learner text containing errors, this paper focuses on modeling coherence in spontaneous spoken responses as well as investigating discourse features in an attempt to extend the construct coverage of an auto</context>
</contexts>
<marker>Yannakoudakis, Briscoe, 2012</marker>
<rawString>Helen Yannakoudakis and Ted Briscoe. 2012. Modeling coherence in ESOL learner texts. Proceedings of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications, 33-43. Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
</authors>
<title>SpeechRaterSM to scoring spontaneous non-native speech.</title>
<date>2007</date>
<booktitle>Proceedings of the International Speech Communication Association Special Interest Group on Speech and Language Technology in Education,</booktitle>
<pages>128--131</pages>
<contexts>
<context position="9805" citStr="Zechner et al., 2007" startWordPosition="1506" endWordPosition="1509"> of x = 0.68 on the 3-point scale. The average of the two coherence scores provided by the two annotators correlates with the holistic speaking proficiency scores at r = 0.66, indicating that the overall proficiency scores of spoken responses can benefit from the discourse coherence annotations. 1 2 3 # 1 160 (27%) 278 (46%) 162 (27%) # 2 125 (21%) 251 (42%) 224 (37%) Table 1. Distribution of coherence annotations from two annotators Furthermore, coherence features based on the human annotations were examined within the context of an automated spoken language assessment system, SpeechRaterSM (Zechner et al., 2007; 2009). We extracted 96 features related to pronunciation, prosody, fluency, language use, and content development using SpeechRater. These features were either extracted directly from the speech signal or were based on the output of an automatic speech recognition system (with a word error rate of around 28%1 ). By utilizing a decision tree classifier (the J48 implementation from Weka (Hall et al., 2009)), 4-fold cross validation was 1 Both the training and evaluation sets used to develop the speech recognizer consist of similar spoken responses drawn from the same assessment. However, there</context>
</contexts>
<marker>Zechner, Higgins, Xi, 2007</marker>
<rawString>Klaus Zechner, Derrick Higgins and Xiaoming Xi. 2007. SpeechRaterSM to scoring spontaneous non-native speech. Proceedings of the International Speech Communication Association Special Interest Group on Speech and Language Technology in Education, 128-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>David M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken English.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<pages>51--10</pages>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Klaus Zechner, Derrick Higgins, Xiaoming Xi and David M. Williamson. 2009. Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communication, 51(10):883-895.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>