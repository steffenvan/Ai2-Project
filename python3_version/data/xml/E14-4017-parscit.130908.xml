<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002567">
<title confidence="0.6780074">
Painless Semi-Supervised Morphological Segmentation using Conditional
Random Fields
Teemu Ruokolainena Oskar Kohonenb Sami Virpiojab Mikko Kurimoa
a Department of Signal Processing and Acoustics, Aalto University
b Department of Information and Computer Science, Aalto University
</title>
<email confidence="0.991073">
firstname.lastname@aalto.fi
</email>
<sectionHeader confidence="0.993648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751705882353">
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, that is the surface
forms of morphemes. We extend a re-
cent segmentation approach based on con-
ditional random fields from purely super-
vised to semi-supervised learning by ex-
ploiting available unsupervised segmenta-
tion techniques. We integrate the unsu-
pervised techniques into the conditional
random field model via feature set aug-
mentation. Experiments on three di-
verse languages show that this straight-
forward semi-supervised extension greatly
improves the segmentation accuracy of the
purely supervised CRFs in a computation-
ally efficient manner.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872606557377">
We discuss data-driven morphological segmenta-
tion, in which word forms are segmented into
morphs, the surface forms of morphemes. This
type of morphological analysis can be useful for
alleviating language model sparsity inherent to
morphologically rich languages (Hirsimäki et al.,
2006; Creutz et al., 2007; Turunen and Kurimo,
2011; Luong et al., 2013). Particularly, we focus
on a low-resource learning setting, in which only
a small amount of annotated word forms are avail-
able for model training, while unannotated word
forms are available in abundance.
We study morphological segmentation using
conditional random fields (CRFs), a discrimina-
tive model for sequential tagging and segmenta-
tion (Lafferty et al., 2001). Recently, Ruoko-
lainen et al. (2013) showed that the CRFs can
yield competitive segmentation accuracy com-
pared to more complex, previous state-of-the-
art techniques. While CRFs yielded generally
the highest accuracy compared to their reference
methods (Poon et al., 2009; Kohonen et al., 2010),
on the smallest considered annotated data sets of
100 word forms, they were outperformed by the
semi-supervised Morfessor algorithm (Kohonen et
al., 2010). However, Ruokolainen et al. (2013)
trained the CRFs solely on the annotated data,
without any use of the available unannotated data.
In this work, we extend the CRF-based ap-
proach to leverage unannotated data in a straight-
forward and computationally efficient manner via
feature set augmentation, utilizing predictions of
unsupervised segmentation algorithms. Experi-
ments on three diverse languages show that the
semi-supervised extension substantially improves
the segmentation accuracy of the CRFs. The ex-
tension also provides higher accuracies on all the
considered data set sizes and languages compared
to the semi-supervised Morfessor (Kohonen et al.,
2010).
In addition to feature set augmentation, there
exists numerous approaches for semi-supervised
CRF model estimation, exemplified by minimum
entropy regularization (Jiao et al., 2006), gen-
eralized expectations criteria (Mann and McCal-
lum, 2008), and posterior regularization (He et al.,
2013). In this work, we employ the feature-based
approach due to its simplicity and the availabil-
ity of useful unsupervised segmentation methods.
Varying feature set augmentation approaches have
been successfully applied in several related tasks,
such as Chinese word segmentation (Wang et al.,
2011; Sun and Xu, 2011) and chunking (Turian et
al., 2010).
The paper is organized as follows. In Section 2,
we describe the CRF-based morphological seg-
mentation approach following (Ruokolainen et al.,
2013), and then show how to extend this approach
to leverage unannotated data in an efficient man-
ner. Our experimental setup and results are dis-
cussed in Sections 3 and 4, respectively. Finally,
</bodyText>
<page confidence="0.986228">
84
</page>
<note confidence="0.8203315">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84–89,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.506748">
we present conclusions on the work in Section 5.
</bodyText>
<sectionHeader confidence="0.988152" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.995649">
2.1 Supervised Morphological Segmentation
using CRFs
</subsectionHeader>
<bodyText confidence="0.903920451612903">
We present the morphological segmentation task
as a sequential labeling problem by assigning each
character to one of three classes, namely {be-
ginning of a multi-character morph (B), middle
of a multi-character morph (M), single character
morph (S)}. We then perform the sequential label-
ing using linear-chain CRFs (Lafferty et al., 2001).
Formally, the linear-chain CRF model distribu-
tion for label sequence y = (y1, y2,. . . , yT) and
a word form x = (x1, x2, ... , xT) is written as a
conditional probability
( )
exp w · φ(yt−1, yt, x, t) ,
where t indexes the character positions, w denotes
the model parameter vector, and φ the vector-
valued feature extracting function. The model pa-
rameters w are estimated discrimatively based on
a training set of exemplar input-output pairs (x, y)
using, for example, the averaged perceptron algo-
rithm (Collins, 2002). Subsequent to estimation,
the CRF model segments test word forms using
the Viterbi algorithm (Lafferty et al., 2001).
We next describe the feature set
{φi(yt−1, yt, x, t)}|φ|
i=1 by defining emission
and transition features. Denoting the label set {B,
M, S} as Y, the emission feature set is defined as
{χm(x, t)1(yt = y0t)  |m ∈ 1..M ,∀y0t ∈ Y} ,
where the indicator function 1(yt = y0t) returns
one if and only if yt = y0t and zero otherwise, that
is
</bodyText>
<equation confidence="0.9989225">
1 _ = f 1 if yt = yt (3)
(yt — yt�) l 0 otherwise ,
</equation>
<bodyText confidence="0.999976777777778">
and {χm(x, t)}Mm=1 is the set of functions describ-
ing the character position t. Following Ruoko-
lainen et al. (2013), we employ binary functions
that describe the position t of word x using all left
and right substrings up to a maximum length δ.
The maximum substring length δmax is considered
a hyper-parameter to be adjusted using a develop-
ment set. While the emission features associate
the input to labels, the transition feature set
</bodyText>
<equation confidence="0.994929">
{1(yt−1 = y0t−1)1(yt = y0t)  |y0t, y0t−1 ∈ Y} (4)
</equation>
<bodyText confidence="0.999693">
captures the dependencies between adjacent labels
as irrespective of the input x.
</bodyText>
<subsectionHeader confidence="0.999877">
2.2 Leveraging Unannotated Data
</subsectionHeader>
<bodyText confidence="0.977988464285714">
In order to utilize unannotated data, we explore a
straightforward approach based on feature set aug-
mentation. We exploit predictions of unsupervised
segmentation algorithms by defining variants of
the features described in Section 2.1. The idea is
to compensate the weaknesses of the CRF model
trained on the small annotated data set using the
strengths of the unsupervised methods that learn
from large amounts of unannotated data.
For example, consider utilizing predictions of
the unsupervised Morfessor algorithm (Creutz and
Lagus, 2007) in the CRF model. In order to ac-
complish this, we first learn the Morfessor model
from the unannotated training data, and then ap-
ply the learned model on the word forms in the
annotated training set. Assuming the annotated
training data includes the English word drivers,
the Morfessor algorithm might, for instance, re-
turn a (partially correct) segmentation driv + ers.
We present this segmentation by defining a func-
tion υ(t), which returns 0 or 1, if the position t is
in the middle of a segment or in the beginning of a
segment, respectively, as in
t 1 2 3 4 5 6 7
xt d r i v e r s
υ(t) 1 0 0 0 1 0 0
Now, given a set of U functions {υu(t)}Uu=1, we
define variants of the emission features in (2) as
</bodyText>
<equation confidence="0.9812805">
{υu(x, t)χm(x, t)1(yt = y0t) |
∀u ∈ 1..U ,∀m ∈ 1..M ,∀y0t ∈ Y}. (5)
</equation>
<bodyText confidence="0.9999076">
By adding the expanded features of form (5), the
CRF model learns to associate the output of the
unsupervised algorithms in relation to the sur-
rounding substring context. Similarly, an ex-
panded transition feature is written as
</bodyText>
<equation confidence="0.9992925">
{υu(x,t)1(yt−1 = y0t−1)1(yt = y0t) |
∀u ∈ 1..U ,∀y0t,y0t−1 ∈ Y}. (6)
</equation>
<bodyText confidence="0.999992125">
After defining the augmented feature set, the
CRF model parameters can be estimated in a stan-
dard manner on the small, annotated training data
set. Subsequent to CRF training, the Morfessor
model is applied on the test instances in order to
allow the feature set augmentation and standard
decoding with the estimated CRF model. We ex-
pect the Morfessor features to specifically improve
</bodyText>
<equation confidence="0.992879666666667">
T
p(y  |x; w) ∝ ri
t=2
</equation>
<page confidence="0.983399">
85
</page>
<bodyText confidence="0.999903761904762">
segmentation of compound words (for example,
brain+storm), which are modeled with high ac-
curacy by the unsupervised Morfessor algorithm
(Creutz and Lagus, 2007), but can not be learned
from the small number of annotated examples
available for the supervised CRF training.
As another example of a means to augment the
feature set, we make use of the fact that the output
of the unsupervised algorithms does not have to be
binary (zeros and ones). To this end, we employ
the classic letter successor variety (LSV) scores
presented originally by (Harris, 1955).1 The LSV
scores utilize the insight that the predictability of
successive letters should be high within morph
segments, and low at the boundaries. Conse-
quently, a high variety of letters following a prefix
indicates a high probability of a boundary. We use
a variant of the LSV values presented by Çöltekin
(2010), in which we first normalize the scores by
the average score at each position t, and subse-
qently logarithmize the normalized value. While
LSV score tracks predictability given prefixes, the
same idea can be utilized for suffixes, providing
the letter predecessor variety (LPV). Subsequent
to augmenting the feature set using the functions
LSV (t) and LPV (t), the CRF model learns to
associate high successor and predecessor values
(low predictability) to high probability of a seg-
ment boundary. Appealingly, the Harris features
can be obtained in a computationally inexpensive
manner, as they merely require counting statistics
from the unannotated data.
The feature set augmentation approach de-
scribed above is computationally efficient, if the
computational overhead from the unsupervised
methods is small. This is because the CRF param-
eter estimation is still based on the small amount
of labeled examples as described in Section 2.1,
while the number of features incorporated in the
CRF model (equal to the number of parameters)
grows linearly in the number of exploited unsu-
pervised algorithms.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.965513">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9986895">
We perform the experiments on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al., 2009; Ku-
</bodyText>
<footnote confidence="0.972286">
1We also experimented on modifying the output of the
Morfessor algorithm from binary to probabilistic, but these
soft cues provided no consistent advantage over the standard
binary output.
</footnote>
<table confidence="0.999518">
English Finnish Turkish
Train (unann.) 384,903 2,206,719 617,298
Train (ann.) 1,000 1,000 1,000
Devel. 694 835 763
Test 10,000 10,000 10,000
</table>
<tableCaption confidence="0.9896295">
Table 1: Number of word types in the Morpho
Challenge data set.
</tableCaption>
<bodyText confidence="0.887417857142857">
rimo et al., 2010) consisting of manually prepared
morphological segmentations in English, Finnish
and Turkish. We follow the experiment setup, in-
cluding data partitions and evaluation metrics, de-
scribed by Ruokolainen et al. (2013). Table 1
shows the total number of instances available for
model estimation and testing.
</bodyText>
<subsectionHeader confidence="0.996912">
3.2 CRF Feature Extraction and Training
</subsectionHeader>
<bodyText confidence="0.999930129032258">
The substring features included in the CRF model
are described in Section 2.1. We include all sub-
strings which occur in the training data. The Mor-
fessor and Harris (successor and predecessor va-
riety) features employed by the semi-supervised
extension are described in Section 2.2. We ex-
perimented on two variants of the Morfessor al-
gorithm, namely, the Morfessor Baseline (Creutz
and Lagus, 2002) and Morfessor Categories-MAP
(Creutz and Lagus, 2005), CatMAP for short. The
Baseline models were trained on word types and
the perplexity thresholds of the CatMAP models
were set equivalently to the reference runs in Mor-
pho Challenge 2010 (English: 450, Finnish: 250,
Turkish: 100); otherwise the default parameters
were used. The Harris features do not require any
hyper-parameters.
The CRF model (supervised and semi-
supervised) is trained using the averaged
perceptron algorithm (Collins, 2002). The num-
ber of passes over the training set made by the
perceptron algorithm, and the maximum length of
substring features are optimized on the held-out
development sets.
The experiments are run on a standard desktop
computer using a Python-based single-threaded
CRF implementation. For Morfessor Baseline, we
use the recently published implementation by Vir-
pioja et al. (2013). For Morfessor CatMAP, we
used the Perl implementation by Creutz and La-
gus (2005).
</bodyText>
<page confidence="0.993517">
86
</page>
<subsectionHeader confidence="0.69477">
3.3 Reference Methods
</subsectionHeader>
<bodyText confidence="0.9999752">
We compare our method’s performance with
the fully supervised CRF model and the semi-
supervised Morfessor algorithm (Kohonen et al.,
2010). For semi-supervised Morfessor, we use the
Python implementation by Virpioja et al. (2013).
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99996335483871">
Segmentation accuracies for all languages are pre-
sented in Table 2. The columns titled Train (ann.)
and Train (unann.) denote the number of anno-
tated and unannotated training instances utilized
by the method, respectively. To summarize, the
semi-supervised CRF extension greatly improved
the segmentation accuracy of the purely super-
vised CRFs, and also provided higher accuracies
compared to the semi-supervised Morfessor algo-
rithm2.
Appealingly, the semi-supervised CRF exten-
sion already provided consistent improvement
over the supervised CRFs, when utilizing the com-
putationally inexpensive Harris features. Addi-
tional gains were then obtained using the Morfes-
sor features. On all languages, highest accuracies
were obtained using a combination of Harris and
CatMAP features.
Running the CRF parameter estimation (includ-
ing hyper-parameters) consumed typically up to a
few minutes. Computing statistics for the Harris
features also took up roughly a few minutes on
all languages. Learning the unsupervised Mor-
fessor algorithm consumed 3, 47, and 20 min-
utes for English, Finnish, and Turkish, respec-
tively. Meanwhile, CatMAP model estimation
was considerably slower, consuming roughly 10,
50, and 7 hours for English, Finnish and Turkish,
respectively. Training and decoding with semi-
supervised Morfessor took 21, 111, and 47 hours
for English, Finnish and Turkish, respectively.
</bodyText>
<sectionHeader confidence="0.998809" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999886">
We extended a recent morphological segmenta-
tion approach based on CRFs from purely super-
vised to semi-supervised learning. We accom-
plished this in an efficient manner using feature set
augmentation and available unsupervised segmen-
tation techniques. Experiments on three diverse
</bodyText>
<footnote confidence="0.9207174">
2The improvements over the supervised CRFs and semi-
supervised Morfessor were statistically significant (confi-
dence level 0.95) according to the standard 1-sided Wilcoxon
signed-rank test performed on 10 randomly divided, non-
overlapping subsets of the complete test sets.
</footnote>
<table confidence="0.999663235294117">
Method Train (ann.) Train (unann.) F1
English
CRF 100 0 78.8
S-MORF. 100 384,903 83.7
CRF (Harris) 100 384,903 80.9
CRF (BL+Harris) 100 384,903 82.6
CRF (CM+Harris) 100 384,903 84.4
CRF 1,000 0 85.9
S-MORF. 1,000 384,903 84.3
CRF (Harris) 1,000 384,903 87.6
CRF (BL+Harris) 1,000 384,903 87.9
CRF (CM+Harris) 1,000 384,903 88.4
Finnish
CRF 100 0 65.5
S-MORF. 100 2,206,719 70.4
CRF (Harris) 100 2,206,719 78.9
CRF (BL+Harris) 100 2,206,719 79.3
CRF (CM+Harris) 100 2,206,719 82.0
CRF 1,000 0 83.8
S-MORF. 1,000 2,206,719 76.4
CRF (Harris) 1,000 2,206,719 88.3
CRF (BL+Harris) 1,000 2,206,719 88.9
CRF (CM+Harris) 1,000 2,206,719 89.4
Turkish
CRF 100 0 77.7
S-MORF. 100 617,298 78.2
CRF (Harris) 100 617,298 82.6
CRF (BL+Harris) 100 617,298 84.9
CRF (CM+Harris) 100 617,298 85.5
CRF 1,000 0 88.6
S-MORF. 1,000 617,298 87.0
CRF (Harris) 1,000 617,298 90.1
CRF (BL+Harris) 1,000 617,298 91.7
CRF (CM+Harris) 1,000 617,298 91.8
</table>
<tableCaption confidence="0.997192">
Table 2: Results on test data. CRF (BL+Harris)
</tableCaption>
<bodyText confidence="0.9573153">
denotes semi-supervised CRF extension using
Morfessor Baseline and Harris features, while
CRF (CM+Harris) denotes CRF extension em-
ploying Morfessor CatMAP and Harris features.
languages showed that this straightforward semi-
supervised extension greatly improves the seg-
mentation accuracy of the supervised CRFs, while
being computationally efficient. The extension
also outperformed the semi-supervised Morfessor
algorithm on all data set sizes and languages.
</bodyText>
<sectionHeader confidence="0.980167" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999939142857143">
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012–2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
</bodyText>
<page confidence="0.99898">
87
</page>
<sectionHeader confidence="0.924774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.94202154054054">
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1–8. Association for Computational
Linguistics.
Qagrı Qöltekin. 2010. Improving successor variety
for morphological segmentation. In Proceedings of
the 20th Meeting of Computational Linguistics in the
Netherlands.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02 Workshop on Morpho-
logical and Phonological Learning, pages 21–30,
Philadelphia, PA, USA, July. Association for Com-
putational Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Timo Honkela, Ville Könönen,
Matti Pöllä, and Olli Simula, editors, Proceedings of
AKRR’05, International and Interdisciplinary Con-
ference on Adaptive Knowledge Representation and
Reasoning, pages 106–113, Espoo, Finland, June.
Helsinki University of Technology, Laboratory of
Computer and Information Science.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1):3:1–3:34, January.
Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraçlar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):3:1–3:29, December.
Zellig Harris. 1955. From phoneme to morpheme.
Language, 31(2):190–222.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 38–46,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Teemu Hirsimäki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkkönen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515–541,
October.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 209–216. Association for Computational Lin-
guistics.
Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010. Semi-supervised learning of concatenative
morphology. In Proceedings of the 11th Meeting of
the ACL Special Interest Group on Computational
Morphology and Phonology, pages 78–86, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Mikko Kurimo, Sami Virpioja, Ville Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Mikko Kurimo, Sami Virpioja, and Ville Turunen.
2010. Overview and results of Morpho Chal-
lenge 2010. In Proceedings of the Morpho Chal-
lenge 2010 Workshop, pages 7–24, Espoo, Finland,
September. Aalto University School of Science and
Technology, Department of Information and Com-
puter Science. Technical Report TKK-ICS-R37.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Carla E. Brodley and Andrea Po-
horeckyj Danyluk, editors, Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282–289, Williamstown, MA, USA. Mor-
gan Kaufmann.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 29–37. Association for Computa-
tional Linguistics, August.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings
of ACL-08: HLT, pages 870–878. Association for
Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 209–217.
Association for Computational Linguistics.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
</reference>
<page confidence="0.987423">
88
</page>
<reference confidence="0.994490129032258">
the Seventeenth Conference on Computational Nat-
ural Language Learning (CoNLL), pages 29–37. As-
sociation for Computational Linguistics, August.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979. As-
sociation for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Ville Turunen and Mikko Kurimo. 2011. Speech re-
trieval from unsegmented Finnish audio using statis-
tical morpheme-like units for segmentation, recog-
nition, and retrieval. ACM Transactions on Speech
and Language Processing, 8(1):1:1–1:25, October.
Sami Virpioja, Peter Smit, Stig-Arne Grönroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python im-
plementation and extensions for Morfessor Baseline.
Report 25/2013 in Aalto University publication se-
ries SCIENCE + TECHNOLOGY, Department of
Signal Processing and Acoustics, Aalto University.
Yiou Wang, Yoshimasa Tsuruoka Jun’ichi Kazama,
Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang,
and Kentaro Torisawa. 2011. Improving Chinese
word segmentation and POS tagging with semi-
supervised methods using large auto-analyzed data.
In IJCNLP, pages 309–317.
</reference>
<page confidence="0.999747">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325535">
<title confidence="0.933239">Painless Semi-Supervised Morphological Segmentation using Conditional Random Fields</title>
<author confidence="0.758022">Oskar Sami Mikko</author>
<degree confidence="0.6188685">of Signal Processing and Acoustics, Aalto of Information and Computer Science, Aalto</degree>
<email confidence="0.926197">firstname.lastname@aalto.fi</email>
<abstract confidence="0.999153666666666">We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely supervised to semi-supervised learning by exploiting available unsupervised segmentation techniques. We integrate the unsupervised techniques into the conditional random field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<volume>10</volume>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4973" citStr="Collins, 2002" startWordPosition="739" endWordPosition="740"> (S)}. We then perform the sequential labeling using linear-chain CRFs (Lafferty et al., 2001). Formally, the linear-chain CRF model distribution for label sequence y = (y1, y2,. . . , yT) and a word form x = (x1, x2, ... , xT) is written as a conditional probability ( ) exp w · φ(yt−1, yt, x, t) , where t indexes the character positions, w denotes the model parameter vector, and φ the vectorvalued feature extracting function. The model parameters w are estimated discrimatively based on a training set of exemplar input-output pairs (x, y) using, for example, the averaged perceptron algorithm (Collins, 2002). Subsequent to estimation, the CRF model segments test word forms using the Viterbi algorithm (Lafferty et al., 2001). We next describe the feature set {φi(yt−1, yt, x, t)}|φ| i=1 by defining emission and transition features. Denoting the label set {B, M, S} as Y, the emission feature set is defined as {χm(x, t)1(yt = y0t) |m ∈ 1..M ,∀y0t ∈ Y} , where the indicator function 1(yt = y0t) returns one if and only if yt = y0t and zero otherwise, that is 1 _ = f 1 if yt = yt (3) (yt — yt�) l 0 otherwise , and {χm(x, t)}Mm=1 is the set of functions describing the character position t. Following Ruok</context>
<context position="11848" citStr="Collins, 2002" startWordPosition="1885" endWordPosition="1886"> We experimented on two variants of the Morfessor algorithm, namely, the Morfessor Baseline (Creutz and Lagus, 2002) and Morfessor Categories-MAP (Creutz and Lagus, 2005), CatMAP for short. The Baseline models were trained on word types and the perplexity thresholds of the CatMAP models were set equivalently to the reference runs in Morpho Challenge 2010 (English: 450, Finnish: 250, Turkish: 100); otherwise the default parameters were used. The Harris features do not require any hyper-parameters. The CRF model (supervised and semisupervised) is trained using the averaged perceptron algorithm (Collins, 2002). The number of passes over the training set made by the perceptron algorithm, and the maximum length of substring features are optimized on the held-out development sets. The experiments are run on a standard desktop computer using a Python-based single-threaded CRF implementation. For Morfessor Baseline, we use the recently published implementation by Virpioja et al. (2013). For Morfessor CatMAP, we used the Perl implementation by Creutz and Lagus (2005). 86 3.3 Reference Methods We compare our method’s performance with the fully supervised CRF model and the semisupervised Morfessor algorith</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), volume 10, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qagrı Qöltekin</author>
</authors>
<title>Improving successor variety for morphological segmentation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 20th Meeting of Computational Linguistics in the Netherlands.</booktitle>
<marker>Qöltekin, 2010</marker>
<rawString>Qagrı Qöltekin. 2010. Improving successor variety for morphological segmentation. In Proceedings of the 20th Meeting of Computational Linguistics in the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning,</booktitle>
<pages>21--30</pages>
<editor>In Mike Maxwell, editor,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="11350" citStr="Creutz and Lagus, 2002" startWordPosition="1809" endWordPosition="1812">t setup, including data partitions and evaluation metrics, described by Ruokolainen et al. (2013). Table 1 shows the total number of instances available for model estimation and testing. 3.2 CRF Feature Extraction and Training The substring features included in the CRF model are described in Section 2.1. We include all substrings which occur in the training data. The Morfessor and Harris (successor and predecessor variety) features employed by the semi-supervised extension are described in Section 2.2. We experimented on two variants of the Morfessor algorithm, namely, the Morfessor Baseline (Creutz and Lagus, 2002) and Morfessor Categories-MAP (Creutz and Lagus, 2005), CatMAP for short. The Baseline models were trained on word types and the perplexity thresholds of the CatMAP models were set equivalently to the reference runs in Morpho Challenge 2010 (English: 450, Finnish: 250, Turkish: 100); otherwise the default parameters were used. The Harris features do not require any hyper-parameters. The CRF model (supervised and semisupervised) is trained using the averaged perceptron algorithm (Collins, 2002). The number of passes over the training set made by the perceptron algorithm, and the maximum length </context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Mike Maxwell, editor, Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 21–30, Philadelphia, PA, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text.</title>
<date>2005</date>
<booktitle>In Timo Honkela, Ville Könönen, Matti Pöllä, and Olli Simula, editors, Proceedings of AKRR’05, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning,</booktitle>
<pages>106--113</pages>
<institution>Helsinki University of Technology, Laboratory of Computer and Information Science.</institution>
<location>Espoo, Finland,</location>
<contexts>
<context position="11404" citStr="Creutz and Lagus, 2005" startWordPosition="1816" endWordPosition="1819">ics, described by Ruokolainen et al. (2013). Table 1 shows the total number of instances available for model estimation and testing. 3.2 CRF Feature Extraction and Training The substring features included in the CRF model are described in Section 2.1. We include all substrings which occur in the training data. The Morfessor and Harris (successor and predecessor variety) features employed by the semi-supervised extension are described in Section 2.2. We experimented on two variants of the Morfessor algorithm, namely, the Morfessor Baseline (Creutz and Lagus, 2002) and Morfessor Categories-MAP (Creutz and Lagus, 2005), CatMAP for short. The Baseline models were trained on word types and the perplexity thresholds of the CatMAP models were set equivalently to the reference runs in Morpho Challenge 2010 (English: 450, Finnish: 250, Turkish: 100); otherwise the default parameters were used. The Harris features do not require any hyper-parameters. The CRF model (supervised and semisupervised) is trained using the averaged perceptron algorithm (Collins, 2002). The number of passes over the training set made by the perceptron algorithm, and the maximum length of substring features are optimized on the held-out de</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In Timo Honkela, Ville Könönen, Matti Pöllä, and Olli Simula, editors, Proceedings of AKRR’05, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning, pages 106–113, Espoo, Finland, June. Helsinki University of Technology, Laboratory of Computer and Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="6621" citStr="Creutz and Lagus, 2007" startWordPosition="1018" endWordPosition="1021"> adjacent labels as irrespective of the input x. 2.2 Leveraging Unannotated Data In order to utilize unannotated data, we explore a straightforward approach based on feature set augmentation. We exploit predictions of unsupervised segmentation algorithms by defining variants of the features described in Section 2.1. The idea is to compensate the weaknesses of the CRF model trained on the small annotated data set using the strengths of the unsupervised methods that learn from large amounts of unannotated data. For example, consider utilizing predictions of the unsupervised Morfessor algorithm (Creutz and Lagus, 2007) in the CRF model. In order to accomplish this, we first learn the Morfessor model from the unannotated training data, and then apply the learned model on the word forms in the annotated training set. Assuming the annotated training data includes the English word drivers, the Morfessor algorithm might, for instance, return a (partially correct) segmentation driv + ers. We present this segmentation by defining a function υ(t), which returns 0 or 1, if the position t is in the middle of a segment or in the beginning of a segment, respectively, as in t 1 2 3 4 5 6 7 xt d r i v e r s υ(t) 1 0 0 0 </context>
<context position="8260" citStr="Creutz and Lagus, 2007" startWordPosition="1318" endWordPosition="1321"> y0t−1)1(yt = y0t) | ∀u ∈ 1..U ,∀y0t,y0t−1 ∈ Y}. (6) After defining the augmented feature set, the CRF model parameters can be estimated in a standard manner on the small, annotated training data set. Subsequent to CRF training, the Morfessor model is applied on the test instances in order to allow the feature set augmentation and standard decoding with the estimated CRF model. We expect the Morfessor features to specifically improve T p(y |x; w) ∝ ri t=2 85 segmentation of compound words (for example, brain+storm), which are modeled with high accuracy by the unsupervised Morfessor algorithm (Creutz and Lagus, 2007), but can not be learned from the small number of annotated examples available for the supervised CRF training. As another example of a means to augment the feature set, we make use of the fact that the output of the unsupervised algorithms does not have to be binary (zeros and ones). To this end, we employ the classic letter successor variety (LSV) scores presented originally by (Harris, 1955).1 The LSV scores utilize the insight that the predictability of successive letters should be high within morph segments, and low at the boundaries. Consequently, a high variety of letters following a pr</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1):3:1–3:34, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraçlar, and Andreas Stolcke.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraçlar, and Andreas Stolcke. 2007. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. ACM Transactions on Speech and Language Processing, 5(1):3:1–3:29, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>From phoneme to morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="8657" citStr="Harris, 1955" startWordPosition="1388" endWordPosition="1389">atures to specifically improve T p(y |x; w) ∝ ri t=2 85 segmentation of compound words (for example, brain+storm), which are modeled with high accuracy by the unsupervised Morfessor algorithm (Creutz and Lagus, 2007), but can not be learned from the small number of annotated examples available for the supervised CRF training. As another example of a means to augment the feature set, we make use of the fact that the output of the unsupervised algorithms does not have to be binary (zeros and ones). To this end, we employ the classic letter successor variety (LSV) scores presented originally by (Harris, 1955).1 The LSV scores utilize the insight that the predictability of successive letters should be high within morph segments, and low at the boundaries. Consequently, a high variety of letters following a prefix indicates a high probability of a boundary. We use a variant of the LSV values presented by Çöltekin (2010), in which we first normalize the scores by the average score at each position t, and subseqently logarithmize the normalized value. While LSV score tracks predictability given prefixes, the same idea can be utilized for suffixes, providing the letter predecessor variety (LPV). Subseq</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Zellig Harris. 1955. From phoneme to morpheme. Language, 31(2):190–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luheng He</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Graph-based posterior regularization for semi-supervised structured prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>38--46</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3104" citStr="He et al., 2013" startWordPosition="439" endWordPosition="442">mentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our ex</context>
</contexts>
<marker>He, Gillenwater, Taskar, 2013</marker>
<rawString>Luheng He, Jennifer Gillenwater, and Ben Taskar. 2013. Graph-based posterior regularization for semi-supervised structured prediction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 38–46, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Hirsimäki</author>
<author>Mathias Creutz</author>
<author>Vesa Siivola</author>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Janne Pylkkönen</author>
</authors>
<title>Unlimited vocabulary speech recognition with morph language models applied to Finnish.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="1264" citStr="Hirsimäki et al., 2006" startWordPosition="169" endWordPosition="172">. We integrate the unsupervised techniques into the conditional random field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner. 1 Introduction We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. This type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques.</context>
</contexts>
<marker>Hirsimäki, Creutz, Siivola, Kurimo, Virpioja, Pylkkönen, 2006</marker>
<rawString>Teemu Hirsimäki, Mathias Creutz, Vesa Siivola, Mikko Kurimo, Sami Virpioja, and Janne Pylkkönen. 2006. Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language, 20(4):515–541, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
<author>Russell Greiner</author>
<author>Dale Schuurmans</author>
</authors>
<title>Semisupervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>209--216</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2995" citStr="Jiao et al., 2006" startWordPosition="423" endWordPosition="426">rd and computationally efficient manner via feature set augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al.,</context>
</contexts>
<marker>Jiao, Wang, Lee, Greiner, Schuurmans, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semisupervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 209–216. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Krista Lagus</author>
</authors>
<title>Semi-supervised learning of concatenative morphology.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>78--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1992" citStr="Kohonen et al., 2010" startWordPosition="280" endWordPosition="283">rce learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally the highest accuracy compared to their reference methods (Poon et al., 2009; Kohonen et al., 2010), on the smallest considered annotated data sets of 100 word forms, they were outperformed by the semi-supervised Morfessor algorithm (Kohonen et al., 2010). However, Ruokolainen et al. (2013) trained the CRFs solely on the annotated data, without any use of the available unannotated data. In this work, we extend the CRF-based approach to leverage unannotated data in a straightforward and computationally efficient manner via feature set augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension sub</context>
<context position="12472" citStr="Kohonen et al., 2010" startWordPosition="1980" endWordPosition="1983">he number of passes over the training set made by the perceptron algorithm, and the maximum length of substring features are optimized on the held-out development sets. The experiments are run on a standard desktop computer using a Python-based single-threaded CRF implementation. For Morfessor Baseline, we use the recently published implementation by Virpioja et al. (2013). For Morfessor CatMAP, we used the Perl implementation by Creutz and Lagus (2005). 86 3.3 Reference Methods We compare our method’s performance with the fully supervised CRF model and the semisupervised Morfessor algorithm (Kohonen et al., 2010). For semi-supervised Morfessor, we use the Python implementation by Virpioja et al. (2013). 4 Results Segmentation accuracies for all languages are presented in Table 2. The columns titled Train (ann.) and Train (unann.) denote the number of annotated and unannotated training instances utilized by the method, respectively. To summarize, the semi-supervised CRF extension greatly improved the segmentation accuracy of the purely supervised CRFs, and also provided higher accuracies compared to the semi-supervised Morfessor algorithm2. Appealingly, the semi-supervised CRF extension already provide</context>
</contexts>
<marker>Kohonen, Virpioja, Lagus, 2010</marker>
<rawString>Oskar Kohonen, Sami Virpioja, and Krista Lagus. 2010. Semi-supervised learning of concatenative morphology. In Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 78–86, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville Turunen</author>
<author>Graeme W Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Overview and results of Morpho Challenge</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece,</location>
<contexts>
<context position="10194" citStr="Kurimo et al., 2009" startWordPosition="1629" endWordPosition="1632">equire counting statistics from the unannotated data. The feature set augmentation approach described above is computationally efficient, if the computational overhead from the unsupervised methods is small. This is because the CRF parameter estimation is still based on the small amount of labeled examples as described in Section 2.1, while the number of features incorporated in the CRF model (equal to the number of parameters) grows linearly in the number of exploited unsupervised algorithms. 3 Experimental Setup 3.1 Data We perform the experiments on the Morpho Challenge 2009/2010 data set (Kurimo et al., 2009; Ku1We also experimented on modifying the output of the Morfessor algorithm from binary to probabilistic, but these soft cues provided no consistent advantage over the standard binary output. English Finnish Turkish Train (unann.) 384,903 2,206,719 617,298 Train (ann.) 1,000 1,000 1,000 Devel. 694 835 763 Test 10,000 10,000 10,000 Table 1: Number of word types in the Morpho Challenge data set. rimo et al., 2010) consisting of manually prepared morphological segmentations in English, Finnish and Turkish. We follow the experiment setup, including data partitions and evaluation metrics, describe</context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2009</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and results of Morpho Challenge 2009. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville Turunen</author>
</authors>
<title>Overview and results of Morpho Challenge</title>
<date>2010</date>
<booktitle>In Proceedings of the Morpho Challenge 2010 Workshop,</booktitle>
<tech>Technical Report TKK-ICS-R37.</tech>
<pages>7--24</pages>
<institution>Aalto University School of Science and Technology, Department of Information and Computer Science.</institution>
<location>Espoo, Finland,</location>
<marker>Kurimo, Virpioja, Turunen, 2010</marker>
<rawString>Mikko Kurimo, Sami Virpioja, and Ville Turunen. 2010. Overview and results of Morpho Challenge 2010. In Proceedings of the Morpho Challenge 2010 Workshop, pages 7–24, Espoo, Finland, September. Aalto University School of Science and Technology, Department of Information and Computer Science. Technical Report TKK-ICS-R37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Carla E. Brodley and Andrea Pohoreckyj Danyluk, editors, Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Williamstown, MA, USA.</location>
<contexts>
<context position="1699" citStr="Lafferty et al., 2001" startWordPosition="236" endWordPosition="239">he surface forms of morphemes. This type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally the highest accuracy compared to their reference methods (Poon et al., 2009; Kohonen et al., 2010), on the smallest considered annotated data sets of 100 word forms, they were outperformed by the semi-supervised Morfessor algorithm (Kohonen et al., 2010). However, Ruokolainen et al. (2013) trained the CRFs solely on the annotated data, without any use of the available unannotated data. In this work, we</context>
<context position="4453" citStr="Lafferty et al., 2001" startWordPosition="643" endWordPosition="646"> the European Chapter of the Association for Computational Linguistics, pages 84–89, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics we present conclusions on the work in Section 5. 2 Methods 2.1 Supervised Morphological Segmentation using CRFs We present the morphological segmentation task as a sequential labeling problem by assigning each character to one of three classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), single character morph (S)}. We then perform the sequential labeling using linear-chain CRFs (Lafferty et al., 2001). Formally, the linear-chain CRF model distribution for label sequence y = (y1, y2,. . . , yT) and a word form x = (x1, x2, ... , xT) is written as a conditional probability ( ) exp w · φ(yt−1, yt, x, t) , where t indexes the character positions, w denotes the model parameter vector, and φ the vectorvalued feature extracting function. The model parameters w are estimated discrimatively based on a training set of exemplar input-output pairs (x, y) using, for example, the averaged perceptron algorithm (Collins, 2002). Subsequent to estimation, the CRF model segments test word forms using the Vit</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Carla E. Brodley and Andrea Pohoreckyj Danyluk, editors, Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, Williamstown, MA, USA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>29--37</pages>
<contexts>
<context position="1332" citStr="Luong et al., 2013" startWordPosition="181" endWordPosition="184">field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner. 1 Introduction We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. This type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally the highest accuracy compared to their</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL), pages 29–37. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3056" citStr="Mann and McCallum, 2008" startWordPosition="431" endWordPosition="435"> augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon Mann and Andrew McCallum. 2008. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL-08: HLT, pages 870–878. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>209--217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1969" citStr="Poon et al., 2009" startWordPosition="276" endWordPosition="279">ocus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally the highest accuracy compared to their reference methods (Poon et al., 2009; Kohonen et al., 2010), on the smallest considered annotated data sets of 100 word forms, they were outperformed by the semi-supervised Morfessor algorithm (Kohonen et al., 2010). However, Ruokolainen et al. (2013) trained the CRFs solely on the annotated data, without any use of the available unannotated data. In this work, we extend the CRF-based approach to leverage unannotated data in a straightforward and computationally efficient manner via feature set augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-s</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 209–217. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Ruokolainen</author>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
</authors>
<title>Supervised morphological segmentation in a low-resource learning setting using conditional random fields.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>29--37</pages>
<contexts>
<context position="1736" citStr="Ruokolainen et al. (2013)" startWordPosition="241" endWordPosition="245">s type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally the highest accuracy compared to their reference methods (Poon et al., 2009; Kohonen et al., 2010), on the smallest considered annotated data sets of 100 word forms, they were outperformed by the semi-supervised Morfessor algorithm (Kohonen et al., 2010). However, Ruokolainen et al. (2013) trained the CRFs solely on the annotated data, without any use of the available unannotated data. In this work, we extend the CRF-based approach to lev</context>
<context position="3601" citStr="Ruokolainen et al., 2013" startWordPosition="514" endWordPosition="517">(Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, 84 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84–89, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics we present conclusions on the work in Section 5. 2 Methods 2.1 Supervised Morphological Segmentation using CRFs We present the morphological segmentation task as a sequential labeling problem by ass</context>
<context position="5594" citStr="Ruokolainen et al. (2013)" startWordPosition="856" endWordPosition="860">002). Subsequent to estimation, the CRF model segments test word forms using the Viterbi algorithm (Lafferty et al., 2001). We next describe the feature set {φi(yt−1, yt, x, t)}|φ| i=1 by defining emission and transition features. Denoting the label set {B, M, S} as Y, the emission feature set is defined as {χm(x, t)1(yt = y0t) |m ∈ 1..M ,∀y0t ∈ Y} , where the indicator function 1(yt = y0t) returns one if and only if yt = y0t and zero otherwise, that is 1 _ = f 1 if yt = yt (3) (yt — yt�) l 0 otherwise , and {χm(x, t)}Mm=1 is the set of functions describing the character position t. Following Ruokolainen et al. (2013), we employ binary functions that describe the position t of word x using all left and right substrings up to a maximum length δ. The maximum substring length δmax is considered a hyper-parameter to be adjusted using a development set. While the emission features associate the input to labels, the transition feature set {1(yt−1 = y0t−1)1(yt = y0t) |y0t, y0t−1 ∈ Y} (4) captures the dependencies between adjacent labels as irrespective of the input x. 2.2 Leveraging Unannotated Data In order to utilize unannotated data, we explore a straightforward approach based on feature set augmentation. We e</context>
<context position="10824" citStr="Ruokolainen et al. (2013)" startWordPosition="1725" endWordPosition="1728">We also experimented on modifying the output of the Morfessor algorithm from binary to probabilistic, but these soft cues provided no consistent advantage over the standard binary output. English Finnish Turkish Train (unann.) 384,903 2,206,719 617,298 Train (ann.) 1,000 1,000 1,000 Devel. 694 835 763 Test 10,000 10,000 10,000 Table 1: Number of word types in the Morpho Challenge data set. rimo et al., 2010) consisting of manually prepared morphological segmentations in English, Finnish and Turkish. We follow the experiment setup, including data partitions and evaluation metrics, described by Ruokolainen et al. (2013). Table 1 shows the total number of instances available for model estimation and testing. 3.2 CRF Feature Extraction and Training The substring features included in the CRF model are described in Section 2.1. We include all substrings which occur in the training data. The Morfessor and Harris (successor and predecessor variety) features employed by the semi-supervised extension are described in Section 2.2. We experimented on two variants of the Morfessor algorithm, namely, the Morfessor Baseline (Creutz and Lagus, 2002) and Morfessor Categories-MAP (Creutz and Lagus, 2005), CatMAP for short. </context>
</contexts>
<marker>Ruokolainen, Kohonen, Virpioja, Kurimo, 2013</marker>
<rawString>Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, and Mikko Kurimo. 2013. Supervised morphological segmentation in a low-resource learning setting using conditional random fields. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL), pages 29–37. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3417" citStr="Sun and Xu, 2011" startWordPosition="486" endWordPosition="489">t al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, 84 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84–89, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics we present con</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3452" citStr="Turian et al., 2010" startWordPosition="492" endWordPosition="495">ture set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, 84 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84–89, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics we present conclusions on the work in Section 5. </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ville Turunen</author>
<author>Mikko Kurimo</author>
</authors>
<title>Speech retrieval from unsegmented Finnish audio using statistical morpheme-like units for segmentation, recognition, and retrieval.</title>
<date>2011</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1311" citStr="Turunen and Kurimo, 2011" startWordPosition="177" endWordPosition="180">to the conditional random field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner. 1 Introduction We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. This type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally the highest accur</context>
</contexts>
<marker>Turunen, Kurimo, 2011</marker>
<rawString>Ville Turunen and Mikko Kurimo. 2011. Speech retrieval from unsegmented Finnish audio using statistical morpheme-like units for segmentation, recognition, and retrieval. ACM Transactions on Speech and Language Processing, 8(1):1:1–1:25, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Peter Smit</author>
<author>Stig-Arne Grönroos</author>
<author>Mikko Kurimo</author>
</authors>
<title>Morfessor 2.0: Python implementation and extensions for Morfessor Baseline. Report 25/2013</title>
<date>2013</date>
<booktitle>in Aalto University publication series SCIENCE + TECHNOLOGY, Department of Signal Processing and Acoustics,</booktitle>
<institution>Aalto University.</institution>
<contexts>
<context position="12226" citStr="Virpioja et al. (2013)" startWordPosition="1940" endWordPosition="1944">, Finnish: 250, Turkish: 100); otherwise the default parameters were used. The Harris features do not require any hyper-parameters. The CRF model (supervised and semisupervised) is trained using the averaged perceptron algorithm (Collins, 2002). The number of passes over the training set made by the perceptron algorithm, and the maximum length of substring features are optimized on the held-out development sets. The experiments are run on a standard desktop computer using a Python-based single-threaded CRF implementation. For Morfessor Baseline, we use the recently published implementation by Virpioja et al. (2013). For Morfessor CatMAP, we used the Perl implementation by Creutz and Lagus (2005). 86 3.3 Reference Methods We compare our method’s performance with the fully supervised CRF model and the semisupervised Morfessor algorithm (Kohonen et al., 2010). For semi-supervised Morfessor, we use the Python implementation by Virpioja et al. (2013). 4 Results Segmentation accuracies for all languages are presented in Table 2. The columns titled Train (ann.) and Train (unann.) denote the number of annotated and unannotated training instances utilized by the method, respectively. To summarize, the semi-super</context>
</contexts>
<marker>Virpioja, Smit, Grönroos, Kurimo, 2013</marker>
<rawString>Sami Virpioja, Peter Smit, Stig-Arne Grönroos, and Mikko Kurimo. 2013. Morfessor 2.0: Python implementation and extensions for Morfessor Baseline. Report 25/2013 in Aalto University publication series SCIENCE + TECHNOLOGY, Department of Signal Processing and Acoustics, Aalto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
</authors>
<title>Yoshimasa Tsuruoka Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.</title>
<date>2011</date>
<booktitle>IJCNLP,</booktitle>
<pages>309--317</pages>
<marker>Wang, 2011</marker>
<rawString>Yiou Wang, Yoshimasa Tsuruoka Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving Chinese word segmentation and POS tagging with semisupervised methods using large auto-analyzed data. In IJCNLP, pages 309–317.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>