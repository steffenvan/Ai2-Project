<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.075737">
<title confidence="0.992757">
Serious Game Environments for Language and Culture Education
</title>
<author confidence="0.908385">
Alicia Sagae, W. Lewis Johnson, and Rebecca Row
</author>
<affiliation confidence="0.845504">
Alelo, Inc.
</affiliation>
<address confidence="0.9581845">
12910 Culver Boulevard, Suite J
Los Angeles, CA 90066, USA
</address>
<email confidence="0.985163">
{asagae, ljhonson, rrow}@alelo.com
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999511">
In this demonstration we will present technol-
ogies that enable learners to engage in spoken
conversations in foreign languages, integrat-
ing intelligent tutoring and serious game ca-
pabilities into a package that helps learners
quickly acquire communication skills. Con-
versational AI technologies based on the
SAIBA framework for dialog modeling are
realized in this 3-D game environment. Par-
ticipants will be introduced to tools for author-
ing dialogs in this framework, and will have
an opportunity to experience learning with
Alelo products, including the Operational
Language and Culture Training System
(OLCTS).
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958692307692">
Alelo&apos;s language and culture education environ-
ments, including The Tactical Language and Cul-
ture Training System (TLCTS) and the Operational
Language and Culture Training System (OLCTS),
are AI-enhanced learning platforms that help
learners quickly acquire communication skills in
foreign languages and cultures. They have been
developed by Alelo, Inc. based on a prototype de-
veloped at the University of Southern California
(USC).
These environments utilize an integrated combi-
nation of intelligent tutoring system and serious
game technologies. Trainees work through a series
of interactive lessons and exercises, called the Skill
Builder, focusing on mission-oriented communica-
tion skills. The lessons make extensive use of au-
tomated speech recognition focused on learner
language, and provide learners with feedback on
their performance. Cultural notes describing cus-
toms and nonverbal gestures are integrated into the
Skill Builder lessons. Trainees apply their skills in
an interactive Arcade Game, where they use spo-
ken commands in the target language to navigate a
town grid, and in a Mission Game, where they par-
ticipate in real-time dialog with simulated local
people in order to accomplish their mission.
</bodyText>
<sectionHeader confidence="0.935824" genericHeader="method">
2 Systems that Impact Learners
</sectionHeader>
<bodyText confidence="0.999517631578947">
Five TLCTS/OLCTS training courses have been
developed so far: Tactical IraqiTM, focusing on Ira-
qi Arabic, Tactical PashtoTM and Tactical DariTM
focusing on the predominant dialects spoken in
Afghanistan, Tactical FrenchTM for Sahel Africa,
and Operational IndonesianTM. TLCTS courses are
complete training courses, providing all of the
training materials needed to conduct basic training
in foreign language and culture. For example, Tac-
tical IraqiTM includes eighteen Mission Game
scenes, ten Arcade Game levels, and sixty-three
Skill Builder scenes comprising over 2000 lesson
pages. Additional scenes and lessons are under
development.
While the platform imposes no limit on content
size, the material developed so far or these systems
typically covers 80-120 hours of training. In-game
reference materials, including glossaries, summa-
ries of lesson content, and grammar notes, are
</bodyText>
<page confidence="0.985138">
29
</page>
<note confidence="0.457362">
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 29–32,
</note>
<page confidence="0.307522">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<bodyText confidence="0.999872769230769">
available both as part of the training package and
as is a support Web site. Manuals, comprising tu-
torials and training guidelines, help with initial
orientation, training management, and trouble-
shooting. The OLCTS versions of these courses
include supplementary exercises delivered on
handheld devices and on the web, giving trainees a
range of platforms for &amp;quot;train-anywhere&amp;quot; access.
TLCTS rapidly transitioned into widespread use.
Computer labs for training with TLCTS courses
have been established in numerous locations in the
USA and around the world. An estimated twenty-
five thousand US military users have trained with
the system, and consistently rate it highly. It has
also been made available to service members in
allied military forces.
Although the Tactical Language and Culture
concept was originally developed under military
funding, the approach can be applied quite general-
ly to language and culture learning. The key is that
the courses are task-oriented: the learner has a task
to carry out, the Skill Builder helps the learner to
acquire the skills necessary to carry out the task,
and the Mission Game gives the learner an oppor-
tunity to practice the task in compelling simulated
settings.
</bodyText>
<sectionHeader confidence="0.985132" genericHeader="method">
3 Conversational Agent Technologies
</sectionHeader>
<bodyText confidence="0.999992333333333">
Simulated dialogs are executed by the virtual hu-
man architecture described in (Johnson &amp; Valente,
2008). The architecture adopts a variant of the
SAIBA framework (Vilhjalmsson &amp; Marsella,
2005), which separates intent planning (the choice
of what to communicate) from production of be-
lievable behavior (how to realize the communica-
tion). An overview of the social simulation
process is given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998528">
3.1 Rule-Driven Behavior
</subsectionHeader>
<bodyText confidence="0.999237666666667">
Virtual human behavior is generated by a series of
components that include explicit models of speech
and language (for natural language understanding
and generation) as well as behavior-mapping rules
that implicitly reflect the subject-matter expertise
of the rule authors. These rules generally occur at
the level of communicative acts (Traum &amp; Hin-
kelman, 1992). A simple example of such a rule,
expressed in natural language, is shown below:
</bodyText>
<figure confidence="0.999438173076923">
IF the learner says that your home is beautiful, (1)
THEN reply that it is quite plain
Input Processing
Player Speech
+- Gesture
Automated
Speech
Recognizer
World Model
(physical/social)
Utterance
Behavior
Specification
&lt;BML&gt;
Dialog
Context
Social Simulation Manager (API)
Behavior
Interpretation
Interpretation
Rules
Behavior
Generation
Translation
Rules
Cultural
Context
Environmental
Context
Communicative
Act &lt;FML&gt;
Communicative
Act &lt;FML&gt;
Intent Planning
Agentget
Social Simulation
Module
Conversation
Cache
Agent Models
Personality Model
Language Model
Culture Model
Input
Processing
Behavior
Realization
Simulation
Management
Game
Engine
Player
</figure>
<figureCaption confidence="0.999783">
Figure 1. Dialog simulation architecture in Alelo language and culture training systems
</figureCaption>
<page confidence="0.982293">
30
</page>
<figureCaption confidence="0.999317">
Figure 2. Screen shot of a Mission Game dialog in Operational DariTM
</figureCaption>
<subsectionHeader confidence="0.998945">
3.2 Collaborative Authoring
</subsectionHeader>
<bodyText confidence="0.999846434782609">
Rules like (1) are created by a content development
team with expertise in linguistics and cultural anth-
ropology. This work is supported by a set of web-
based collaborative authoring tools, called Kona
and TIDE. Kona is used to create lesson content
for the Skill Builder, while TIDE is a graphical
editor used to encode dialog rules as transitions in
a Finite State Machine.
Kona gives authors access to a database of les-
son content, specified in XML format. The authors
can selectively lock and edit lessons in the data-
base, and view and edit different fields in the spe-
cification of each page in the lesson. The author
can edit the written descriptions of the image on
the page, the cultural notes, and the enabling learn-
ing objectives (ELOs) covered in the page. In oth-
er views, authors can link in images and sound
recordings, and make notes and comments for oth-
er authors to review. The lesson specifications are
then automatically translated into the internal data
format used in OLCTS, so that authors can review
the lessons as they appear in the training applica-
tion.
</bodyText>
<sectionHeader confidence="0.988207" genericHeader="method">
4 The Demonstration
</sectionHeader>
<bodyText confidence="0.999972461538461">
The demonstration will give participants an oppor-
tunity to use OLCTS, and other Alelo interactive
language and culture training products, and learn
about their supporting authoring tools. It is in-
tended for people who are interested in gaining an
in-depth understanding of AIED (artificial intelli-
gence in education) technology for serious games,
and the development tools used to create them.
The demo will be conducted by a presenter, who
will give live demonstrations of the software, and
an assistant presenter who will coach the partici-
pants in the use of the game and supporting author-
ing tools.
</bodyText>
<subsectionHeader confidence="0.983215">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.984926333333333">
First, the participants will get a hands-on intro-
duction to one of the Operational Language and
Culture courses. Under supervision of a presenter,
</bodyText>
<page confidence="0.999722">
31
</page>
<bodyText confidence="0.999988681818182">
the participants will learn to say a few phrases in
the Skill Builder and use the phrases that they have
learned in the Mission Game. This portion can be
tailored on the fly to the interests of participants,
and can take from 5 to 30 minutes to complete.
Depending on time and interest, participants
may also have an opportunity to work with an
OLCTS course in more depth. They can be called
upon to learn some basic communication skills in
Dari and apply them in the Mission Game. This
will give participants a firsthand understanding of
how each component of OLCTS supports learning,
how the components support each other, and how
artificial intelligence technology is applied in the
learning experience.
Finally, the presenter will demo some of the au-
thoring tools used to create OLCTS content. The
participants will propose modifications or exten-
sions to an existing OLCTS course. The presenter
will use the authoring tools in real time to make the
modifications, following the recommendations of
the participants.
</bodyText>
<subsectionHeader confidence="0.8231565">
4.2 Example: Engaging in a Dialog in Opera-
tional DariTM
</subsectionHeader>
<bodyText confidence="0.9941643">
For a video summary of the demonstration, please
visit http://www.alelo.com/movie_tlt-6min.html.
The user experience in the Mission Game is one
engaging component of this demonstration. An
example script for a Mission Game interaction in
Alelo&apos;s Operational DariTM course is given in the
following sections.
A sample of a Mission Game screen is shown in
Figure 2. The player controls the figure in the cen-
ter-left. At this point in the demonstration, the
player has received a briefing that describes a
communication task that he or she should accom-
plish in this exercise. To complete the task, the
player must engage the virtual human, or non-
player character (NPC) shown on the right.
Organizing rebuilding operations is one example
of such a task. The NPC is a host-national charac-
ter in Afghanistan. The player should check on the
status of their shared plan for rebuilding and give
constructive feedback. This type of communica-
tion task can require finesse and delicacy on the
part of the player in order to be culturally appro-
priate. It draws on the learner&apos;s understanding and
skill with face-saving, a prominent feature of many
cultures worldwide.
The learner must initiate the conversation by
speaking into a headset-mounted microphone. He
or she clicks on the microphone icon, shown in
Figure 3, speaks, then clicks on the icon again to
indicate the end of the turn.
</bodyText>
<figureCaption confidence="0.996587">
Figure 2. Push the microphone button to speak during a
dialog, push again to stop.
</figureCaption>
<bodyText confidence="0.999863444444444">
Recognized player speech is posted to a dialog his-
tory window that appears near the top of the virtual
scene, as shown in Figure 1. The NPC responds
using spoken output, creating a realistic and engag-
ing practice environment. During the dialog, the
player may view hints that display key phrases in
Dari. Once the player has discussed all of the host
national&apos;s training mistakes, the dialog ends in suc-
cess.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999576272727273">
H. Vilhjalmsson and S. Marsella. &amp;quot;Social Performance
Framework&amp;quot;, in Proceedings of the AAAI Workshop
on Modular Construction of Human-Like Intelli-
gence. 2005.
W. L. Johnson and A. Valente. “Tactical Language and
Culture Training Systems: Using Artificial Intelli-
gence to Teach Foreign Languages and Cultures”, in
Proceedings of IAAI 2008. March 2008.
David R. Traum and Elizabeth A. Hinkelman. &amp;quot;Conver-
sation Acts in Task-Oriented Spoken Dialogue&amp;quot;, in
Computational Intelligence, 8(3):575--599, 1992.
</reference>
<page confidence="0.9993">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290908">
<title confidence="0.981578">Serious Game Environments for Language and Culture Education</title>
<author confidence="0.890127">Alicia Sagae</author>
<author confidence="0.890127">W Lewis Johnson</author>
<author confidence="0.890127">Rebecca</author>
<affiliation confidence="0.774345">Alelo,</affiliation>
<address confidence="0.9995715">12910 Culver Boulevard, Suite Los Angeles, CA 90066,</address>
<email confidence="0.997931">asagae@alelo.com</email>
<email confidence="0.997931">ljhonson@alelo.com</email>
<email confidence="0.997931">rrow@alelo.com</email>
<abstract confidence="0.992945">In this demonstration we will present technologies that enable learners to engage in spoken conversations in foreign languages, integrating intelligent tutoring and serious game capabilities into a package that helps learners quickly acquire communication skills. Conversational AI technologies based on the SAIBA framework for dialog modeling are realized in this 3-D game environment. Participants will be introduced to tools for authoring dialogs in this framework, and will have an opportunity to experience learning with Alelo products, including the Operational</abstract>
<title confidence="0.716982">Language and Culture Training System</title>
<intro confidence="0.54613">(OLCTS).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Vilhjalmsson</author>
<author>S Marsella</author>
</authors>
<title>Social Performance Framework&amp;quot;,</title>
<date>2005</date>
<booktitle>in Proceedings of the AAAI Workshop on Modular Construction of Human-Like Intelligence.</booktitle>
<contexts>
<context position="4583" citStr="Vilhjalmsson &amp; Marsella, 2005" startWordPosition="677" endWordPosition="680">ally developed under military funding, the approach can be applied quite generally to language and culture learning. The key is that the courses are task-oriented: the learner has a task to carry out, the Skill Builder helps the learner to acquire the skills necessary to carry out the task, and the Mission Game gives the learner an opportunity to practice the task in compelling simulated settings. 3 Conversational Agent Technologies Simulated dialogs are executed by the virtual human architecture described in (Johnson &amp; Valente, 2008). The architecture adopts a variant of the SAIBA framework (Vilhjalmsson &amp; Marsella, 2005), which separates intent planning (the choice of what to communicate) from production of believable behavior (how to realize the communication). An overview of the social simulation process is given in Figure 1. 3.1 Rule-Driven Behavior Virtual human behavior is generated by a series of components that include explicit models of speech and language (for natural language understanding and generation) as well as behavior-mapping rules that implicitly reflect the subject-matter expertise of the rule authors. These rules generally occur at the level of communicative acts (Traum &amp; Hinkelman, 1992).</context>
</contexts>
<marker>Vilhjalmsson, Marsella, 2005</marker>
<rawString>H. Vilhjalmsson and S. Marsella. &amp;quot;Social Performance Framework&amp;quot;, in Proceedings of the AAAI Workshop on Modular Construction of Human-Like Intelligence. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Johnson</author>
<author>A Valente</author>
</authors>
<title>Tactical Language and Culture Training Systems: Using Artificial Intelligence to Teach Foreign Languages and Cultures”,</title>
<date>2008</date>
<booktitle>in Proceedings of IAAI</booktitle>
<contexts>
<context position="4493" citStr="Johnson &amp; Valente, 2008" startWordPosition="664" endWordPosition="667">llied military forces. Although the Tactical Language and Culture concept was originally developed under military funding, the approach can be applied quite generally to language and culture learning. The key is that the courses are task-oriented: the learner has a task to carry out, the Skill Builder helps the learner to acquire the skills necessary to carry out the task, and the Mission Game gives the learner an opportunity to practice the task in compelling simulated settings. 3 Conversational Agent Technologies Simulated dialogs are executed by the virtual human architecture described in (Johnson &amp; Valente, 2008). The architecture adopts a variant of the SAIBA framework (Vilhjalmsson &amp; Marsella, 2005), which separates intent planning (the choice of what to communicate) from production of believable behavior (how to realize the communication). An overview of the social simulation process is given in Figure 1. 3.1 Rule-Driven Behavior Virtual human behavior is generated by a series of components that include explicit models of speech and language (for natural language understanding and generation) as well as behavior-mapping rules that implicitly reflect the subject-matter expertise of the rule authors.</context>
</contexts>
<marker>Johnson, Valente, 2008</marker>
<rawString>W. L. Johnson and A. Valente. “Tactical Language and Culture Training Systems: Using Artificial Intelligence to Teach Foreign Languages and Cultures”, in Proceedings of IAAI 2008. March 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Elizabeth A Hinkelman</author>
</authors>
<title>Conversation Acts in Task-Oriented Spoken Dialogue&amp;quot;,</title>
<date>1992</date>
<journal>in Computational Intelligence,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="5182" citStr="Traum &amp; Hinkelman, 1992" startWordPosition="767" endWordPosition="771">almsson &amp; Marsella, 2005), which separates intent planning (the choice of what to communicate) from production of believable behavior (how to realize the communication). An overview of the social simulation process is given in Figure 1. 3.1 Rule-Driven Behavior Virtual human behavior is generated by a series of components that include explicit models of speech and language (for natural language understanding and generation) as well as behavior-mapping rules that implicitly reflect the subject-matter expertise of the rule authors. These rules generally occur at the level of communicative acts (Traum &amp; Hinkelman, 1992). A simple example of such a rule, expressed in natural language, is shown below: IF the learner says that your home is beautiful, (1) THEN reply that it is quite plain Input Processing Player Speech +- Gesture Automated Speech Recognizer World Model (physical/social) Utterance Behavior Specification &lt;BML&gt; Dialog Context Social Simulation Manager (API) Behavior Interpretation Interpretation Rules Behavior Generation Translation Rules Cultural Context Environmental Context Communicative Act &lt;FML&gt; Communicative Act &lt;FML&gt; Intent Planning Agentget Social Simulation Module Conversation Cache Agent </context>
</contexts>
<marker>Traum, Hinkelman, 1992</marker>
<rawString>David R. Traum and Elizabeth A. Hinkelman. &amp;quot;Conversation Acts in Task-Oriented Spoken Dialogue&amp;quot;, in Computational Intelligence, 8(3):575--599, 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>