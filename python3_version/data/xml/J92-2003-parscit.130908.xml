<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975352">
Feature Structures and Nonmonotonicity
</title>
<author confidence="0.923012">
Gosse Bouma*
</author>
<affiliation confidence="0.510628">
Rijksuniversiteit Groningen
</affiliation>
<bodyText confidence="0.994268625">
Unification-based grammar formalisms use feature structures to represent linguistic knowledge.
The only operation defined on feature structures, unification, is information-combining and
monotonic. Several authors have proposed nonmonotonic extensions of this formalism, as for
a linguistically adequate description of certain natural language phenomena some kind of default
reasoning seems essential. We argue that the effect of these proposals can be captured by means
of one general, nonmonotonic, operation on feature structures, called default unification. We
provide a formal semantics of the operation and demonstrate how some of the phenomena used to
motivate nonmonotonic extensions of unification-based formalisms can be handled.
</bodyText>
<sectionHeader confidence="0.991544" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9998937">
While monotonicity is often desirable from a formal and computational perspective, it
is at odds with a considerable body of linguistic work. Default principles, default rules,
and default feature-values can be found in many linguistic formalisms and are used
prominently in work on phonology, morphology, and syntax. In spite of their great
expressive power and flexibility, unification-based grammar formalisms (see Shieber
1986a, for an introduction) are in general not very successful in modeling such de-
vices. Unification is an information-combining, monotonic, operation on feature struc-
tures, whereas the implementation of default devices typically requires some form of
nonmonotonicity. In this paper, we present a nonmonotonic operation on feature struc-
tures, which enables us to implement the effects of a number of default devices used
in linguistics. As the operation is defined in terms of feature structures only, an impor-
tant characteristic of unification-based formalisms, namely that linguistic knowledge
is encoded in the form of feature structures, is preserved.
In the next section, we present an overview of linguistic phenomena that are best
described using defaults. We also argue that previous proposals for handling these
phenomena in a unification-based setting are unsatisfactory. Section 3 provides the
formal background for the central part of the paper, Section 4, in which a definition
of default unification is presented. Section 5 briefly presents some applications of this
operation and the final section draws some conclusions concerning the role of non-
monotonicity in unification-based formalisms.
</bodyText>
<sectionHeader confidence="0.992618" genericHeader="keywords">
2. Previous Work
</sectionHeader>
<bodyText confidence="0.999799">
There are a number of phenomena that suggest that unification-based grammar for-
malisms might profit from the addition of some form of nonmonotonicity, and several
authors have in fact suggested such extensions. In this section, we argue that these
proposals suffer from a number of shortcomings. Most importantly, previous propos-
als have either been highly restricted in scope or have been presented in very informal
</bodyText>
<note confidence="0.699407333333333">
* Computational Linguistics Department, Postbus 716, 9700 AS Groningen, The Netherlands
© 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.99807725">
terms, thus leaving a number of questions concerning the exact behavior of the pro-
posed extensions unanswered.
An overview of the issues that call for the addition of non-monotonic reasoning
and of some of the proposals in that direction is presented below.
</bodyText>
<listItem confidence="0.954891105263158">
• Exceptional Rules. Consider a language in which the vast majority of
verbs cannot precede its subject, whereas a small number of exceptional
verbs can. The rule accounting for inverted structures would probably
require that verbs occurring in it be marked as +INV (i.e. (INV) : +). As a
consequence, all regular verbs must be marked explicitly as -INV (to
prevent them from occurring in the inversion rule). Note that, in a
unification-based grammar, there is no need to mark the exceptional
verbs as +INV, which leads to the rather counterintuitive situation that
regular verbs need to be marked extra, whereas the exceptional ones can
remain underspecified. A more natural solution would be to assign all
verbs the specification -INV by default (either by means of template
inheritance or by means of lexical feature specification defaults as used
in Generalized Phrase Structure Grammar [GPSG; Gazdar et al. 19851)
and to overwrite or block this specification in the exceptional cases. The
possibility of incorporating an overwrite operation in a unification-based
formalism is mentioned in Shieber (1986a, p. 60).
• Feature Percolation Principles. Both GPSG and Head-driven Phrase
Structure Grammar (HPSG; Pollard and Sag 1987) adopt the so-called
Head Feature Convention (HFC). In GPSG, the HFC is a default principle:
</listItem>
<bodyText confidence="0.970585769230769">
head features will normally have identical values on mother and head,
but specific rules may assign incompatible values to specific head
features. In unification-based formalisms, it is impossible to express this
principle directly. Adding the constraint (X0 head) = (X, head) to every
rule of the form X0 Xi • .. Xn (with X, (1 &lt;i &lt; n) the head of the rule
and assuming all head features to be collected under head) will not do, as
it rules out the possibility of exceptions altogether. Shieber (1986b)
therefore proposes to add this constraint conservatively, which means that,
if the rule already contains conflicting information for some head feature
f, the constraint is replaced by a set of constraints (X0 head f&apos;) =
(X, head f&apos;), for all head features f&apos; f.
• Structuring the Lexicon. Flickinger, Pollard, and Wasow (1985),
Flickinger (1987), De Smedt (1990), Daelemans (1988), and others, have
argued that the encoding and maintenance of the detailed lexical
descriptions typical for lexicalist grammar formalisms benefits greatly
from the use of (nonmonotonic) inheritance. In Flickinger, Pollard, and
Wasow (1985), for instance, lexical information is organized in the form
of frames, which are comparable to the templates (i.e., feature structures
that may be used as part of the definition of other feature structures) of
PATR-II (Shieber 1986a). A frame or specific lexical entry may inherit
from more general frames. Frames can be used to encode information
economically and, perhaps more importantly, as a means to express
linguistic generalizations. For instance, all properties typical of verbs are
defined in the VERB-frame, and properties typical of auxiliaries are
defined in the AUX-frame. The AUX-frame may inherit from the
VERB-frame, thus capturing the fact that an auxiliary is a kind of verb.
</bodyText>
<page confidence="0.995683">
184
</page>
<note confidence="0.901122">
Gosse Bouma Feature Structures and Nonmonotonicity
</note>
<bodyText confidence="0.9999399">
In this approach, a mechanism that allows inheritance of information by
default (i.e., a mechanism in which local information may exclude the
inheritance of more general information) is of great importance. Without
such a mechanism, a frame may contain only properties that hold
without exception for all items that inherit from this frame. In practice,
however, one often wants to define the properties that are typical for a
given class in the form of a frame, without ruling out the possibility that
exceptions might exist. In unification-based formalisms, templates can
play the role of frames, but as unification is used to implement
inheritance, nonmonotonic inheritance is impossible.
</bodyText>
<listItem confidence="0.797329787878788">
• Inflectional Morphology. In PATR-II the lexicon is a list of inflected
word forms associated with feature structures. The only tools available
for capturing lexical generalizations are templates (see above) and lexical
rules. Lexical rules may transform the feature structure of a lexical entry.
An example is the rule for agentless passive (Shieber 1986a, p. 62), which
transforms the feature structure for transitive past participles into a feature
structure for participles occurring in agentless passive constructions.
Lexical rules can only change the feature structure of a lexical entry, not
its word form, and thus, the scope of these rules is rather restricted.
While the examples in Flickinger, Pollard, and Wasow (1985) and Evans
and Gazdar (1989a,b) suggest that the latter restriction can be easily
removed, it is not so obvious how a unification-based grammar
formalism can cope with the combination of rules and exceptions typical
for (inflectional) morphology. For instance, it is possible to formulate a
rule that describes past tense formation in English, but it is not so easy
to exclude the application of this rule to irregular verbs and to describe
(nonredundantly) past tense formation of these irregular verbs. Evans
and Gazdar (1989a,b) present the DATR-formalism, which, among other
things, contains a nonmonotonic inference system that enables an elegant
account of the blocking-phenomenon just described. The examples used
throughout their presentation are all drawn from inflectional
morphology and illustrate once more the importance of default
reasoning in this area of linguistics.
• Gapping. In Kaplan (1987) it is observed that gapping constructions and
other forms of nonconstituent conjunction can be analyzed in Lexical
Functional Grammar (Bresnan and Kaplan, 1982) as the conjunction of
two functional-structures (f-structures), one of which may be incomplete.
The missing information in the incomplete f-structure can be filled in if it
is merged with the complete f-structure, using an operation called
priority union. Priority union of two f-structures A and B is defined as an
operation that extends A with information from B that is not included
(or filled in) in A. As not all information in B is present in the priority
union of A and B, this operation introduces nonmonotonicity.
</listItem>
<bodyText confidence="0.9930225">
The proposals for incorporating the kind of default reasoning that is required for
each of the phenomena above are both rather diverse and idiosyncratic and, further-
more, suffer from a number of shortcomings.
The Head Feature Convention and Feature Specification Defaults of GPSG, for instance,
appear to be motivated with a very particular set of linguistic phenomena in mind
and also are rather intimately connected to peculiarities of the GPSG-formalism. What
</bodyText>
<page confidence="0.995145">
185
</page>
<note confidence="0.338712">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.98513788">
is particularly striking is the fact that two different conceptions of default appear to
play a role: a head feature is exempt from the HFC only if this would otherwise lead
to an inconsistency, whereas a feature is exempt from having the value specified in
some feature specification default (among others) if this feature covaries with another
feature.
Overwrite and add conservatively are also highly restricted operations. From the
examples given in Shieber (1986a) it seems as if overwriting can only be used to
add or substitute (nonmonotonically) one atomic feature value in a given (possibly
complex) feature structure (which acts as default). Add conservatively, on the other
hand, is only used to add one reentrancy (as far as possible) to a given feature structure
(which acts as nondefault). An additional restriction is that add conservatively is well
behaved only for the kind of feature structures used in GPSG (that is, feature structures
in which limited use is made of covariation or reentrancy). Consider for instance the
example in (1).1 Adding the constraint (X0 head) (X1 head) to (1) conservatively
could result in either la or lb.
Example 1
As add conservatively and overwriting are, in a sense, mirror images of each
other, it is tempting to generalize the definitions of these operations and to think of
them as operations on arbitrary feature structures, whose effect is equivalent to that
of priority union. Thus, given two feature structures FSD (the default) and FSND (the
nondefault), adding FSD to FSND conservatively would be equivalent to overwriting
FSD with FSND, and to the priority union of FSND and FSD (i.e. FSND/FSD in the
notation of Kaplan [1987]). However, in light of the example above, it should be clear
that such a generalization is highly problematic. Other examples worth considering
are 2 and 3.
</bodyText>
<footnote confidence="0.960101">
1 Whether this kind of situation can occur in GPSG probably depends on whether one is willing to
conclude from examples such as:
</footnote>
<note confidence="0.472522">
S [COMP a] {[SLIBCAT all , H[COMP NIL] (Gazdar et al. 1985, p. 248)
</note>
<tableCaption confidence="0.170353">
that covariation of arbitrary categories is in principle not excluded in this formalism.
</tableCaption>
<figure confidence="0.946218303030303">
1
f
g :
f : a
g : b
f : a
g : all
f g::ab
f- : 1 b
g: 1 b
— ]]
fg:: b I ]
head:
X0:
head:
• :
head:
:
a.
head:
• :
head:
X0:
b.
head:
• :
186
Gosse Bouma Feature Structures and Nonmonotonicity
Example 2
FSD [f:a FSND =
g:b
Example 3
FSD [ f : a 1 FSND [g:b]
</figure>
<bodyText confidence="0.916257225">
Again, if we try to combine the two feature structures along the lines of any one of
the operations mentioned above, there are at least two possible results (note that in
Example 3, we could either preserve the information that features f and g are reentrant,
or preserve the information that f : a), and there is no telling which one is correct.
Two conclusions can be drawn at this point. First of all, on the basis of the ex-
amples just given, it can be concluded that a nonmonotonic operation on feature
structures that relies (only) on the fact that the result should be consistent must be
very restricted indeed, as more generic versions will always run into the problem that
there can be several mutually exclusive solutions to solving a given unification conflict.
Second, claims that the operations add conservatively, overwriting, and priority union are
equivalent are unwarranted, as no definitions of these operations are available that are
sufficiently explicit to determine what their result would be in moderately complex
examples such as 1-3.
The approach exemplified by Flickinger (1987) and others is to use a general-
purpose knowledge representation formalism to represent linguistic information and
model default inheritance. Feature structures are defined as classes of some sort, which
may inherit from other, more generic, classes. The inheritance strategy used says that
information in the generic class is to be included in the specific class as well, as
long as the specific class does not contain local information that is in conflict with
the information to be inherited. Such an inheritance strategy will run into problems,
however, if reentrancies are generally allowed. For instance, think of the examples
presented above as involving a generic class FSD from which a specific class FSND
inherits. The inheritance procedure in, for instance, Flickinger (1987, p. 59ff) does not
say anything about which one of the possible results will be chosen.
The work of Evans and Gazdar (1989a,b), finally, is not easily incorporated in a
unification-based formalism, as they use semantic nets instead of feature structures to
represent linguistic information. That is, although the syntax of DATR is suggestively
similar to that of, for instance, PATR-II, DATR descriptions do in fact denote graphs
that differ rather substantially from the graphs used to represent feature structures (see
Evans and Gazdar 1989b). The nonmonotonic reasoning facilities of DATR therefore
are not directly applicable in a unification-based formalism either.
We conclude that a formally explicit definition of a nonmonotonic operation on
feature structures is still missing. In particular, the interaction of reentrancy and non-
monotonicity is a subtle issue, which has not been given the attention it deserves. That
there is a need for nonmonotonic devices is obvious from the fact that several authors
have found it necessary to introduce partial solutions for dealing with nonmonotonic-
ity in a unification-based setting. The intuitions underlying these proposals appear to
be compatible, if not identical, and thus it seems attractive to consider an operation
that subsumes the effects of the proposals so far. Default Unification, as defined below,
is an attempt to provide such an operation.
</bodyText>
<page confidence="0.988742">
187
</page>
<note confidence="0.326245">
Computational Linguistics Volume 18, Number 2
</note>
<sectionHeader confidence="0.474682" genericHeader="introduction">
3. Feature Structures and Unification
</sectionHeader>
<bodyText confidence="0.9996556">
Feature structures are often depicted as matrices of attribute-value pairs where values
are either atoms or feature structures themselves and, furthermore, values may be
shared by different attributes in the feature structure. Feature structures can be defined
using a description language, such as the one found in PATR-II (Shieber 1986a) or in
Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b.
</bodyText>
<equation confidence="0.8601155">
Example 4
a. ( (f) = a
(g f) = a
(g f) = (g g) )
- f : a
b. [ g :7 a ]
</equation>
<bodyText confidence="0.999054">
Following the approach of Kasper and Rounds (1986; 1990), and others, we represent
feature structures formally as finite (acyclic) automata (the definition below is taken
from Dawar and Vijay-Shanker 1990):
</bodyText>
<subsectionHeader confidence="0.415885">
Definition
</subsectionHeader>
<bodyText confidence="0.9970905">
A finite acyclic automaton A is a 7-tuple
(Q, E,F, 6, qo, F, A) where:
</bodyText>
<listItem confidence="0.996642818181818">
1. Q is a nonempty finite set of states,
2. E is a countable set (the alphabet),
3. F is a countable set (the output alphabet),
4. .5 : Q x E Q is a finite partial function (the transition function),
5. qo E Q,
6. F C Q,
7. A : F F is a total function (the output function),
8. the directed graph (Q, E) is acyclic, where pEq iff for some
1 E E,5(p,1) = q,
9. for every q e Q, there exists a directed path from qo to q in (Q, E), and
10. for every q E F,S(q,1) is not defined for any 1.
</listItem>
<bodyText confidence="0.9783785">
We will frequently write QA, EA, etc. for the set of states of automaton A, the alphabet
of A, etc.
The relationship between the matrix notation and the automaton concept should
be obvious. The following automaton M is, for instance, equivalent to the matrix in 4b.
</bodyText>
<page confidence="0.99061">
188
</page>
<subsectionHeader confidence="0.367326">
Gosse Bouma Feature Structures and Nonmonotonicity
</subsectionHeader>
<bodyText confidence="0.621435">
Example 5
</bodyText>
<equation confidence="0.9918808">
QM = {q0,qi,g2,q3} 6m(q2,g) q3
Em = {f ,g} SA4(112, f
FM = {a} Fm = fqi,c131
6m(q0, f) = qi Am (qi) = a
8A4(qo,g) = q2 Am(q3) = a
</equation>
<bodyText confidence="0.965602">
Note that 6m(q0, gf) = om(q0, gg) = q3,2 which represents the fact that the two paths
(gf) and (gg) are reentrant. Unification is defined in terms of subsumption, a relation
that imposes a partial ordering on automata:
</bodyText>
<sectionHeader confidence="0.451221" genericHeader="method">
Definition
</sectionHeader>
<bodyText confidence="0.6845025">
An automaton A subsumes an automaton B (A ç B) iff there is a homomorphism h
from A to B such that:
</bodyText>
<listItem confidence="0.983995333333333">
1. h(6A(q , 1) =
2. )B(h(q)) = Ail(q) for all q E FA, and
3. h(go)= gos-
</listItem>
<bodyText confidence="0.9905284">
Intuitively, A E B if B extends the information in A. A = B if A C B and B C A.
Unification of two automata A and B (A LJ B) is the least upper bound of these automata
under subsumption. If no upper bound exists, unification fails.
The semantics of descriptions (sets of formulae of the description language) is
given in terms of satisfaction:
</bodyText>
<sectionHeader confidence="0.310341" genericHeader="method">
Definition
</sectionHeader>
<bodyText confidence="0.77822425">
An automaton A = (Q, E, A) satisfies a description D (A = D) or a formula
(A =q5) in the following cases:
A=D iff for all ED:A
A a iff Q = F = {go} and A(q0) = a,
</bodyText>
<equation confidence="0.9451115">
A (p) = D iff 6(q0,p) is defined and qo/p D,
A = (pi) (p2) iff 6(go,pi) = 6(q0,p2)•
</equation>
<bodyText confidence="0.933508375">
go/p is the automaton obtained from A by making 8(q0, p) the initial state and
removing all inaccessible states. There is always a unique minimal element in the
subsumption hierarchy that satisfies a description D. This element is the denotation
of D.&apos;
2 5(q, pi) is defined for pl E E* as 6(6(q, p), I).
3 Much of the formal work on feature structures is concerned with the semantics of feature structure
descriptions involving disjunction and negation. Such descriptions do not denote a unique feature
structure, but denote sets of feature structures. Such extensions are not taken into consideration here.
</bodyText>
<page confidence="0.988484">
189
</page>
<note confidence="0.402579">
Computational Linguistics Volume 18, Number 2
</note>
<sectionHeader confidence="0.805462" genericHeader="method">
4. Default Unification
</sectionHeader>
<bodyText confidence="0.99847">
Default reasoning with feature structures requires the ability to modify feature struc-
tures nonmonotonically. Unification does not have this ability, as it can only replace a
feature structure by more specific instances of that structure. Below, we define default
unification as an operation that merges parts of one feature structure (the default ar-
gument) with another feature structure (the nondefault argument). We write AU!B for
the default unification of the default feature structure A and the nondefault feature
structure B. The operation has the following characteristics:
</bodyText>
<listItem confidence="0.859225333333333">
1. It has a declarative semantics and is procedurally neutral. That is, if
A = A&apos; and B = B&apos;, then (AU!B) = (A&apos;U!B&apos;).
2. It is monotonic only with respect to the nondefault argument. That is,
B (ALA) is always true, but in general A E (ALJ!B) will not hold.
3. It never fails. If A is fully incompatible with B, (AU!B) = B.
4. It gives a unique result.
5. Reentrancies in the nondefault argument may be replaced by a weaker set
of reentrancies if necessary (this is the add conservatively operation of
Shieber (1986b)).
</listItem>
<bodyText confidence="0.99976525">
Intuitions about default unification appear to be more clear in those cases where
feature structures do not contain any reentrancies. Therefore, we will first define de-
fault unification for this case, moving to the general case in Section 4.2. Section 4.3.
deals with the incorporation of add conservatively.
</bodyText>
<subsectionHeader confidence="0.97909">
4.1 Default Unification without Reentrancies
</subsectionHeader>
<bodyText confidence="0.995603">
Subsumption suggests a straightforward definition of an operation that has properties
1-4 above.
</bodyText>
<subsectionHeader confidence="0.499522">
Definition
</subsectionHeader>
<bodyText confidence="0.990917666666667">
Default Unification (first version) AU!B = A&apos; U B, where A&apos; is the maximal (i.e. most
specific) element in the subsumption ordering such that A&apos; E A and A&apos; U B is defined.
From this definition of U!, it follows immediately that properties 1-3 hold. The
fact that default unification has a unique result follows from the fact that A&apos; is unique
(up to isomorphism).4 Note furthermore that from the requirement that A&apos; must be the
maximal it follows that no information contained in A is left out in Au!B unnecessarily.
4 Unicity of A&apos; is proved as follows: Assume that there is an A&amp;quot; such that (1) A&amp;quot; 0 A&apos;, (2) A&apos; L A and
A&amp;quot; E A, (3) A&apos; U B and A&amp;quot; U B are defined, and (4) both A&apos; and A&amp;quot; are maximal. We show that these
assumptions are inconsistent. From (2) it follows that A&apos; U A&amp;quot; is defined and (A&apos; U A&amp;quot;) L A. From (3) it
follows that (A&apos; U A&amp;quot;) U B is defined (since, if there are no reentrancies, it holds in general that if X U Y,
Y U Z, and X u Z are defined, XU YU Z is defined). But then, if (A&apos; U A&amp;quot;) = A&apos; or (A&apos; U A&amp;quot;) = A&amp;quot;,
either condition (1) or (4) is not met, or, if A&apos; u A&amp;quot; 0 A&apos; 0 A&amp;quot;, condition (4) is not met. 0
</bodyText>
<page confidence="0.954811">
190
</page>
<figure confidence="0.7339220625">
Gosse Bouma Feature Structures and Nonmonotonicity
An example of default unification is presented below (where nil is used to represent
the empty feature structure):
Example 6
_
f : a
a]
1 &apos;
r:f : a 1
g :
g [ : Li
g : a _
A=
B=
A&apos;=
A&apos; U B =
1 f : a
[ g : [ f:a
g: [ g : bj I]
f : a _
g : r: nil
[ ,i
:
g
g : nil
_
f : a
[f :a
g: [,:a 11
g: I II
g:b
_
</figure>
<bodyText confidence="0.9969175">
The definition of default unification above relies crucially on the fact that there
is a unique maximal element A&apos; unifiable with B. In Section 2, we argued that such
an approach is only feasible for a limited domain. In particular, once reentrancies
are introduced, A&apos; is no longer guaranteed to be unique, and the definition above is
therefore not easily generalized. Fortunately, it is also possible to define AU!B without
requiring unifiability of some element A&apos; with B explicitly. This definition, which will
be extended below, defines Au!B in terms of the difference of the two arguments A
and B.
</bodyText>
<figure confidence="0.2448909">
Definition
Difference (first version) The difference of two automata A and B is the maximal
element A - B that meets the following conditions:
I. A - B E A,
2. if 64—B (lb, p) is defined, then there is no prefix p&apos; of p such that
613(q0, pi) E F13,
3. if 6A—B(qo, p) E FA—B then 813(110, p) is undefined.
Definition
Default Unification (second version)
ilLl!B = (A - B) U B.
</figure>
<bodyText confidence="0.985886666666667">
It should be obvious that characteristics 1-3 continue to hold. Uniqueness follows
in this case from the fact that the difference operation will give a unique result. (A - B
can be constructed from A by checking for each state in A whether it must be removed
</bodyText>
<page confidence="0.981972">
191
</page>
<note confidence="0.421024">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.791953">
or not and ensuring that the resulting automaton is connected.) For instance, assuming
A and B to be defined as in Example 6, we find that A - B is:
</bodyText>
<equation confidence="0.517578">
Example 7
A—B= [g:[g:[f:a] ]]
</equation>
<bodyText confidence="0.998323222222222">
Note that in A -B all parts that are identical in A and B are removed, whereas this was
not the case for A&apos;, as defined in Definition 3.1. The outcome of default unification,
however, is identical in both cases. The reason for this restriction on A - B will become
apparent below.
While default unification monotonically extends the nondefault argument (i.e. B E
AUIB) and nonmonotonically extends the default argument, the operation itself is
monotonic in its default argument and nonmonotonic in its nondefault argument. The
theorem below proves monotonicity for the default argument; that is, a more specific
default argument will lead to a more specific outcome of default unification:
</bodyText>
<figure confidence="0.582596">
Theorem 1
For all feature structures A, B, and C, not containing reentrancies, if A E B then
(AU!C) E (BLI!C).
Proof
It suffices to show that (A — C) E (B - C), or in other words:
I. if A(6A_c(go,p)) - a then A(/3-c(qo, P)) a, and
2. if sii_c(tio,p) is defined then 613-c(q0,p) is defined.
</figure>
<bodyText confidence="0.915696">
If these two conditions are met, there is a homomorphism from A - C to B - C as
required by the definition of subsumption. (Remember that there are no reentrancies.)
Case (1): If A(bA-c(qo, P)) = a, then (i) A(Ss(q0,p)) = a (since A-CEAL B) and
from the definition of A - C it follows that (ii) there is no prefix p&apos; of p such that
6c(130, p&apos;) E Fc nor is Sc(q0, p) defined. From (i) and (ii) it follows that SB-c(qo,p) is
defined and that A(h_c(q0,p)) = a.
Case (2): If SA_c(qcr, p) is defined and 6A_c(q0,p) FA-C (otherwise this case re-
duces to case (1)), it follows that (i) 68(), p) is defined, and (ii) there is no prefix p&apos; of
p such that Sc(cio,p&apos;) E Fc. From (i) and (ii) it follows that SB_c(tio,p) is defined. •
Note, however, that addition of nondefault information does not necessarily lead
to a more specific result. That is, the dual of Theorem 1. does not hold:
</bodyText>
<subsectionHeader confidence="0.684317">
Example 8
</subsectionHeader>
<bodyText confidence="0.949531">
if B E C then (Au!B) C (AU!C)
The reason is that addition of nondefault information may lead to a larger amount
of default information being removed, and thus, the resulting feature-structures ALAB
and ALAC can be incompatible. An example that falsifies 8 is presented below.
</bodyText>
<page confidence="0.973472">
192
</page>
<figure confidence="0.982018333333333">
Gosse Bouma Feature Structures and Nonmonotonicity
Example 9
A= [f : a)
B [g : b] AU!B =
C [ f : b
g:b j AU!C =
Finally, for feature structures without reentrancies, the following distribution law
holds:
Theorem 2
For all feature structures A, B, and C, not containing any reentrancies and such that
A LJ B is defined, (A Li B)U!C = (AU!C) U (BU!C)
Proof
</figure>
<equation confidence="0.51239475">
Since (A UB)U!C = ((A U B) - C) UC and (AU!C) U (BU!C) = ((A - C)UC)U ((B - C) UC) =
(A - C) U (B - C) U C, it suffices to prove that (A U B) - C = (A - C) U (B - C). Let
D = (A U B) - C and E = (A - C) u (B - C). It must be shown that (1) D E E and (2)
E D.
Case (1): If A(SD(qo, p)) =- a, then (0 A(6AuB(qo,P)) = a and thus A(64(qo,P)) = a
or A(8B(qo , p)) = a (since there are no reentrancies) and (ii) there is no prefix p&apos; of
p such that Sc(q0,p&apos;) E Fc, nor is bc(c/o,P) defined. From (i) and (ii) it follows that
(q0, p)) = a or A(6/3_ c (go, P)) = a, and thus that (SE (q0, p)) = a. Similarly, if
</equation>
<bodyText confidence="0.794504333333333">
(go, p) is defined (but not an end state), it follows that SA(go, p) or 6B (110, p) is defined
and that there is no prefix p&apos; of p such that bc (go, P) E Fc. Therefore, either bil-c (go, P)
or 613_c(q0, p) is defined, and thus 4(1 10, p) is defined. It follows that D E E.
Case (2): If A(6E(qo, P)) = a, then A(6A_c(q0,p)) a or )&apos;(6a-c(q0,P)) -= a (since
there are no reentrancies). Therefore, A (5AuB(q0, 10)) = a and also, there is no prefix p&apos;
of p such that k(q0, p&apos;) E Fc, nor is bC(qo, p) defined. It follows that A(6D(q 0 , p)) = a.
</bodyText>
<equation confidence="0.586051">
Similarly if SE (q0, p) is defined but not an end state, 6D (go, p) is defined. It follows that
E E D. •
</equation>
<bodyText confidence="0.99990275">
As long as Theorem 2. holds, it is possible to define default unification by de-
composing the default argument into simpler feature structures and adding these
(nonmonotonically) to the nondefault argument. This approach appears to underlie
some of the previous proposals, but is inadequate once reentrancies enter the picture.
</bodyText>
<subsectionHeader confidence="0.981321">
4.2 Default Unification with Reentrancies
</subsectionHeader>
<bodyText confidence="0.902666">
Taking reentrancies into account requires an extension of the difference operation.
If we allow either default or nondefault information to refer to an extension of a
nondefault or default reentrancy, respectively, there is in general no unique maximal
element subsuming A and unifiable with B. A slight modification of Examples 2 and 3
will illustrate this.
</bodyText>
<figure confidence="0.872789454545455">
Example 10
A- [&amp;quot;&amp;quot;all B=
L g : [f : b] j
f : a
g : b
f : b
g : b
193
Computational Linguistics Volume 18, Number 2
Example 11 A= If: Vv.:eta}, B= [ g : [f : b]
g :
</figure>
<listItem confidence="0.5972828">
In Example 10, there is default information that refers to an extension of a non-
default reentrancy. A - B could be constructed from A by removing either the fact that
(if) : a or (gf) : b. In 11, nondefault information refers to an extension of a default
reentrancy. In this case, we could either remove the reentrancy (and the fact that
(gf) : a) or remove the fact that (if) : a and (gf) : a and preserve the reentrancy. Neither
</listItem>
<bodyText confidence="0.969090666666666">
solution subsumes the other. To avoid such problems, it is best to avoid interaction
between reentrancies and other information altogether and to treat reentrant nodes
in a similar fashion as atomic nodes. That is, we remove default reentrancies if they
refer to defined parts of the nondefault automaton, and default information in general
is removed if it refers to extensions of nondefault reentrancies. Thus, the difference
operation can be extended as follows:
</bodyText>
<subsectionHeader confidence="0.721733">
Definition
</subsectionHeader>
<bodyText confidence="0.9835945">
Difference (final version) The difference of A and B is the maximal element A - B in
the subsumption ordering that meets the following conditions:
</bodyText>
<listItem confidence="0.9621825">
1. A - B E A,
2. if sA—B(qo,p) is defined, then there is no prefix p&apos; of p such that
613(q0,p&apos;) E FB or 6B(qo,11 = 6B(q0,P&amp;quot;)(P&apos; 19&amp;quot;),
3. if 6A_B(q0, p) E FA-B then 68 (q0, p) is undefined,
4. (4) if 6A-B(qo,P) = 64-B(qo, P&apos;)(P P&apos;) then bs(qo,p) and 613(q0, /9&apos;) are
undefined.
</listItem>
<bodyText confidence="0.984314">
The definition of default unification remains as before:
</bodyText>
<subsectionHeader confidence="0.491076">
Definition
Default Unification (= second version)
</subsectionHeader>
<equation confidence="0.578926">
ALIB = (A - B) u B.
</equation>
<bodyText confidence="0.999564888888889">
Again, characteristics 1-4 of default unification mentioned in the introduction of
this section hold. Uniqueness of the result follows from the fact that A - B is unique.
(A - B can be constructed in this case as follows: for all paths p, if 5(q0,P) = 6A(go, 11,
and p is defined in B, introduce a new value for S4(q0,p) such that the automata that
have &amp;(q0, p) and 6(q0,p&apos;) as initial state are isomorphic. Next, check for all states in
the modified automaton whether they must be removed and ensure that the resulting
automaton is connected.)
The monotonicity properties of default unification also remain as before. The the-
orem below is the relevant generalization of Theorem 1.
</bodyText>
<subsectionHeader confidence="0.526485">
Theorem 3
</subsectionHeader>
<bodyText confidence="0.839091">
For all feature structures A, B, and C, if A C B then (Au!C) C (BLAC)
</bodyText>
<subsectionHeader confidence="0.707862">
Proof
</subsectionHeader>
<bodyText confidence="0.951559">
It suffices to show that A-CCB- C, or in other words:
</bodyText>
<listItem confidence="0.502019">
1. if A(6A_c(cio,p)) =a, then A(SB_c(qo, p)) =a,
</listItem>
<page confidence="0.979225">
194
</page>
<note confidence="0.430423">
Gosse Bouma Feature Structures and Nonmonotonicity
</note>
<equation confidence="0.4046885">
2. if si4_c(q0,p) = 64-c(q0,pt) then B-c(go, P) = 6B-c(q0,p&apos;), and
3. if SA_c(q0, p) is defined, then 6B-c(qa,p) is defined.
</equation>
<bodyText confidence="0.759981866666667">
If these three conditions are met, there is a homomorphism from A - C to B - C as
required by the definition of subsumption.
Case (1): If )A-c(qo, p)) = a, then (i) A(6B(q0,p)) =a(sinceA-CCAC B)
and (ii) from the definition of A - C, it follows that there is no prefix p&apos; of p such
that 6c(q0,p&apos;) E Fc or 6c (q0,p&apos;) = 6c(q0,p&amp;quot;), nor is 6c(q0, p) defined. From (i) and (ii) it
follows that 6B_c(q0, p) is defined and that A(6B_c(qo,P)) = a.
Case(2): Similarly, if 6A_c (go, p) = 6A-c(q0, p&apos;), then (i) SB(qo,p) = 6B(qo,p&apos;), and
(ii) there is no prefix p&apos; of p such that 6c(qo,p&apos;) E Fc or 6c(110,p&apos;) = 8c(q0,p&amp;quot;) nor is
8c(cio, p) defined. From (i) and (ii) it follows that 63_c(q0, p) = 6B-c(qo 13&apos;).
Case (3): If 8,1_c(q0,p) is defined and 8A-c(q0,p) FA-C (otherwise this case re-
duces to case (1)) and 6A_c(q0,p) not reentrant (otherwise this case reduces to case
(2)), it follows that (i) 8B(go,p) is defined, and (ii) there is no prefix p&apos; of p such that
6c (go, p&apos;) E Pc or 6c(q0,P&apos;) 6c(q0, p&amp;quot;). From (i) and (ii) it follows that 6B_c(qo, P) is
defined. •
The distribution law, however, continues to hold only in one direction:
</bodyText>
<figure confidence="0.894378666666667">
Theorem 4
For all feature structures A, B, and C, such that A U B is defined, (ALAC) Li (BLAC)
(A Li BPC
</figure>
<subsectionHeader confidence="0.521709">
Proof
</subsectionHeader>
<bodyText confidence="0.999812888888889">
As in the previous section, it suffices to prove that (A - C) U (B - C) C (A LI B) - C.
From the fact that X C X&apos; and Y L Y&apos; implies (X U Y) E (X&apos; LI Y&apos;), it follows that
((A - C) U (B - C)) E (A U B). Now, as in the previous proof, if some path p is atomic,
reentrant, or merely defined in (A - C)U(B - C), it follows that (i) p is atomic, reentrant,
or defined in A U B and (ii) there is no atomic or reentrant path p&apos; in C that is a prefix
of p, nor is p defined in C if p is atomic or reentrant in (A - C) U (B - C). It follows
that p is atomic, reentrant, or defined in (A Ll B) - C. •
An illustration of this result is given below. Note that 12 also illustrates that the
converse of Theorem 4. no longer holds.
</bodyText>
<equation confidence="0.553804333333333">
Example 12
A=
B= [ g : a
C [ g : b
U B)U!C = [
(Au!C) (BU!C) = [
</equation>
<page confidence="0.863692">
195
</page>
<note confidence="0.437784">
Computational Linguistics Volume 18, Number 2
</note>
<subsectionHeader confidence="0.994829">
4.3 Add Conservatively
</subsectionHeader>
<bodyText confidence="0.999903333333333">
Defining default unification as (A — B) U B will fail to capture the idea of Shieber&apos;s
(1986b) add conservatively, as the difference operation completely removes a default
reentrancy if one of the paths leading to it is also defined in the nondefault argument.
However, linguistic applications, such as an encoding of the Head Feature Convention,
indicate that a more subtle approach should be taken. In particular, if a default struc-
ture contains the information that (p) (p&apos;), whereas in the nondefault structure (p1)
is defined for some feature 1, we want to treat only I as an exception to the general
rule that (p) (p&apos;), and preserve the information that (p1&apos;) =(p&apos;1&apos;) (for 1).
We implement this idea using the following operation:
</bodyText>
<sectionHeader confidence="0.511446" genericHeader="method">
Definition
</sectionHeader>
<bodyText confidence="0.9853925">
Let A and B be automata. The extension of A relative to B (Ext(A, B)) is the minimal
(i.e. most general) element Ext (A , B) such that
</bodyText>
<listItem confidence="0.6074615">
1. A E Ext(A, B),
2. if SA(q0, p) = A(qo , p&apos;) and 813(q0, pql) is defined (for some pql E E*), then
6ExnA,B)(q0,pq1&apos;) — 6Ext(A,B)(q0, p&apos; (V&apos;) (wherever possible) for all 11 E E.
• The automaton A is extended, sometimes somewhat redundantly, with reentrant
</listItem>
<bodyText confidence="0.846513769230769">
paths that are extensions of paths already reentrant in A. Ext(A,B) is nevertheless
usually more informative than A itself, as the addition of a path pi blocks unification
with feature structures in which p receives an atomic value. Note furthermore that path
extensions are not always possible; that is, if 5A(go, p) E FA and SB(qo, pl) is defined,
there is no extension of A in which pl is defined. (This explains the wherever possible).
In order to get all relevant path-extensions, E will in general be the set of all features
defined in the grammar, although in particular cases E can be restricted to a smaller
set (the set of head-features, for instance).
We are now ready to give a definition of default unification that incorporates
the effects of add conservatively. To avoid confusion, we use the operator Llac! for this
extended version of default unification.
Definition
Default Unification (final version)
</bodyText>
<equation confidence="0.662493">
ALJac!B (Ext(A,B) — B) LI B.
</equation>
<bodyText confidence="0.962104">
An example of default unification involving reentrancies is presented below. We
assume that the set of features E = Tj, gl.
</bodyText>
<equation confidence="0.611400857142857">
Example 13
A=
B=
Ext(A,B) =
[fg::
[f:[f:a]]
f:E
</equation>
<page confidence="0.80163075">
3
f:E
fgl
196
</page>
<table confidence="0.719532">
Gosse Bouma Feature Structures and Nonmonotonicity
Ext (A, B) —B = f: [g:]
(Ext(A,B) — B) B = g•
[f : nil 1
j
r f : a 1 -
•
f :nil
6 [ g•.3
</table>
<bodyText confidence="0.99913775">
The example shows that default unification is slightly more restrictive than add con-
servatively, since the original reentrancy is removed even though A and B would have
been unifiable. The reason is of course that this will guarantee uniqueness of the result
of default unification, whereas this is not the case for add conservatively.
</bodyText>
<sectionHeader confidence="0.810373" genericHeader="method">
5. Linguistic Applications of Default Unification
</sectionHeader>
<bodyText confidence="0.999244333333333">
In this section, we sketch how default unification can be incorporated in a grammar
formalism and argue briefly that this can be an alternative for some of the extensions
mentioned in Section 2.
</bodyText>
<subsectionHeader confidence="0.999562">
5.1 Nonmonotonic Template Inheritance
</subsectionHeader>
<bodyText confidence="0.999984133333333">
In grammar formalisms such as PATR-II, feature structures are defined as sets of
equations and templates. Each equation or template denotes a feature structure (i.e.
the minimal feature structure that satisfies the equation or the equations that make
up the template definition), and the denotation of a set of such elements is simply the
unification of all their denotations. Incorporation of default unification requires that a
distinction is made between default and nondefault information. In the notation used
here, nondefault information is prefixed by a &amp;quot;!&amp;quot;. The feature structure denoted by a
definition that contains both default and nondefault information is arrived at by first
unifying all default information and unifying all nondefault information. Next, the
two feature structures are combined by means of default unification (Llac!).
If templates are incorporated as default information, the feature structure denoted
by the template is inherited nonmonontonically. (Monotonic inheritance is possible as
well of course: this is achieved by prefixing a template with &amp;quot;!&amp;quot;.) As an illustration,
consider the following fragment, in which an attempt is made to encode some of the
peculiarities of the English auxiliary system in a lexicalist grammar:
</bodyText>
<equation confidence="0.980174285714286">
Example 14 NP : ( (cat) = n
(nform) = norm )•
VERB : ( (cat) = v
(aux) = —
(iv) = —
VP : ( VERB
(subcat first) = NP
(subcat rest) = empty
AUX : ( VERB
!(aux) =
!(inv) =
(subcat first) = VP
(subcat rest first) = NP
(subcat rest rest) = empty
</equation>
<footnote confidence="0.785126">
!(subcat first subcat first nform) =
(subcat rest first nform) )•
</footnote>
<page confidence="0.985124">
197
</page>
<note confidence="0.632872">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999224">
Adding the equations ! (aux) : + and !(inv) : +5 to the definition of AUX has an ef-
fect comparable to that of the overwrite-operation of (Shieber 1986a, p. 60). The AUX
template inherits from VERB by default, but the equations just mentioned block in-
heritance of the values for (iv) and (aux). However, default unification allows us to
do more. An auxiliary does not subcategorize for an ordinary NP subject, nor does
it subcategorize for a complement VP that subcategorizes for an ordinary NP subject.
Rather, the restrictions to be placed on the nform of the subject are inherited from the
embedded VP:
</bodyText>
<subsectionHeader confidence="0.898463">
Example 15
</subsectionHeader>
<bodyText confidence="0.9148852">
a. it will annoy Kim that she lost
b. *Sue will annoy Kim that she lost
This dependency between elements of the subcat list is encoded in the final equation,
which also suppresses (or overwrites) the default value for (nform). The denotation of
AUX is thus:
</bodyText>
<equation confidence="0.971641857142857">
Example 16
rest :
cat: v
aux: —
inv : —
first : [
nform:f
cat : np
subcat
rest : empty
cat : np
[first :
nform :
rest : empty
- cat : v
aux: +
inv : +
subcat:
first :
1 I
1
</equation>
<bodyText confidence="0.9998404">
The nonmonotonic inheritance regime is flexible enough to allow for exceptions
to exceptions. Gazdar et al. (1985, p. 65) observe that at least in some dialects of
English, the auxiliary might cannot occur in inverted structures. This is expressed in
the following lexical entry, in which might inherits nonmonotonically from AUX, which
itself inherits nonmonotonically from VERB:
</bodyText>
<equation confidence="0.963227666666667">
Example 17
might : ( AUX
!(inv) = — ).
</equation>
<bodyText confidence="0.9999396">
There is an important difference between the approach to nonmonotonic inheri-
tance sketched here and the majority of inheritance-based formalisms used for Knowl-
edge Representation, which has to do with the way in which templates are evaluated.
If a template is used as part of the definition of another feature structure, all we need
to know to determine the denotation of this feature structure is the denotation of this
</bodyText>
<footnote confidence="0.9163685">
5 Note that the feature INV as used here indicates only whether a (lexical) item may occur in an inverted
structure. It does not distinguish between inverted and noninverted clauses.
</footnote>
<page confidence="0.991019">
198
</page>
<note confidence="0.795682">
Gosse Bouma Feature Structures and Nonmonotonicity
</note>
<bodyText confidence="0.999666">
template (which is a feature structure). How this template was defined (as a set of
equations or as a combination of (more general) templates, as a combination of de-
fault and nondefault information or not) is completely irrelevant to its meaning. Thus,
the denotation of AUX would remain as before, if we defined it as:
</bodyText>
<equation confidence="0.9885724">
Example 18
AUX: ( (cat) = v
(aux) = +
(mo) = +
).
</equation>
<bodyText confidence="0.999843222222222">
Consequently, the denotation of might is not affected by this change in definition either.
The role of classes (or frames) in inheritance-based systems, however, as described
in, for instance, Touretzky (1986), is rather different. To determine the denotation of a
class might that inherits from a class AUX, we not only need to know the contents of
AUX, but also the classes from which AUX inherits. The latter is important for resolv-
ing multiple-inheritance conflicts. If the class might inherits from both AUX and VERB,
for instance, and AUX in its turn inherits from VERB as well, information inherited
from AUX must take precedence over information from VERB, as the former is more
specific than the latter. In our nonmonotonic inheritance mechanism for templates,
such reasoning is impossible. Adding the template VERB as default information to the
definition of the template (or lexical entry) might would lead to a unification failure
of the default information, and thus the definition as a whole would be considered as
illega1.6 This is as it should be, we believe, given the fact that the inheritance hierarchy
as such should not play a role in determining the meaning of templates. The denota-
tion of the template AUX is the feature structure in 16 (i.e., whether it is defined as
in 14 or as in 18 is irrelevant), and from that it is impossible to conclude that AUX
inherits from VERB, and thus the kind of reasoning used to justify the resolution of
feature conflicts used in Touretzky (1986) is not applicable in our case.
</bodyText>
<subsectionHeader confidence="0.999599">
5.2 Lexical Defaults
</subsectionHeader>
<bodyText confidence="0.99988375">
The definition of auxiliaries above is still unsatisfactory in that it predicts that auxil-
iaries subcategorize for verbal complements that are specified as (aux) = —. Clearly,
this requirement is too strong (although it is correct for the auxiliary do). One way to
solve this problem is to redefine the AUX-template as:
</bodyText>
<figure confidence="0.2877884">
Example 19
AUX : (
(subcat first cat) = v
(subcat first subcat first) = NP
(subcat first subcat rest) = empty
</figure>
<footnote confidence="0.63725">
6 Of course, it is possible to combine incompatible default information if we impose the correct ordering
explicitly. This can be done by using definitions (i.e. a set of equations in brackets) in definitions:
</footnote>
<equation confidence="0.7042525">
might : ( (VERB !AUX)
!(inv) = — ).
</equation>
<bodyText confidence="0.779591">
This is equivalent to the definition of might given in 17, albeit more complex and possibly misleading.
</bodyText>
<page confidence="0.9929">
199
</page>
<note confidence="0.604584">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999896318181818">
This solution seems inelegant, however, as it reconstructs part of the VP-template in
order to express the correct subcategorization requirements. Thus, the obvious gen-
eralization that an auxiliary subcategorizes for a VP is missed by this redefinition.
The source of this inelegance is the fact that VP inherits from VERB, and that VERB
contains default information about properties typical for verbs. However, while these
properties hold for the vast majority of verbs, it is not the case that if an element
subcategorizes for a verbal complement, the default properties need to hold for the
complement as well. What is needed here is a distinction between properties that hold
by default for all members of a class and default properties that can be assumed to
hold if a lexical item subcategorizes for members of this class. While the latter can be
expressed safely by means of templates, the former are more adequately expressed in
the form of lexical defaults.
The extension of unification-based formalisms with lexical defaults can be imple-
mented using default unification. The effect of lexical defaults is comparable to that
of lexical Feature Specification Defaults in GPSG. A lexical default is a statement of the
form Name: Ant = Cons, where Ant and Cons are feature structure descriptions. The
interpretation of lexical defaults is that the feature structure of each lexical entry that
is subsumed by the antecedent of a lexical default is extended, by means of default
unification, with the contents of the consequent. Lexical entries are thus compiled in
two stages: first, the denotation of the feature structure description is computed and
next, the lexical defaults are applied to this feature structure.
Consider for example the following lexical defaults:
</bodyText>
<equation confidence="0.5783275">
Example 20
FSD1 : ( VERB ( (aux) = — ).
FSD2 : ( (aux) = — ( (inv) = — ).
The fragment in 14 and 17 is assumed to be redefined as follows:
Example 21
VERB : (cat) = v )•
</equation>
<table confidence="0.975447875">
VP: VERB ).
AUX: (subcat first) = NP
might: (subcat rest) = empty
VERB
(aux) = +
(subcat first) = VP
AUX
(iv) = —
</table>
<bodyText confidence="0.90408225">
Each verbal lexical item will be extended with the information (aux) = —, unless it
is an auxiliary of course, since in that case, the lexical entry is already specified as
(aux) = +. Only nonauxiliary verbs are extended with the information (iv) = —
(FSD2).7 Auxiliaries remain unspecified for this feature, thus capturing the fact that
</bodyText>
<footnote confidence="0.8358455">
7 The evaluation of these two lexical defaults is thus order-sensitive. The same situation can in principle
arise in GPSG as well, although the particular example given here is avoided in GKPS by
</footnote>
<page confidence="0.984065">
200
</page>
<note confidence="0.787046">
Gosse Bouma Feature Structures and Nonmonotonicity
</note>
<bodyText confidence="0.9982492">
auxiliaries can, but not necessarily do, occur in inverted structures.8 The exceptional
character of might is expressed in this case by adding explicitly the information that it
cannot invert.
The problem sketched at the beginning of this section is now resolved. An auxiliary
subcategorizes for a VP, which in its turn inherits from the template VERB. However,
since the latter template no longer contains default information that should hold for
lexical entries only, an auxiliary no longer subcategorizes for verbal complements that
are (aux) : —. Auxiliaries that subcategorize for a restricted set of verbal complements,
such as do, which requires a (aux) : — complement, can be encoded by adding the
relevant constraint to their lexical entries.
</bodyText>
<subsectionHeader confidence="0.999912">
5.3 Specialization of Reentrancies
</subsectionHeader>
<bodyText confidence="0.999411">
Another important property of default unification is that it enables us to define ex-
ceptions to a reentrancy. Consider for instance the following GPSG rule (where H
indicates the head of the rule):
</bodyText>
<equation confidence="0.7902905">
Example 22
S X2 H[—subj]
</equation>
<bodyText confidence="0.9998155">
The symbol S can be analyzed as the feature structure in 23. Applying the Head Feature
Convention to the rule in 22 amounts to adding to H all head features compatible with
head features in S. Using default unification, this is implemented in 24 as a default
reentrancy that equates the head features of S and H.
</bodyText>
<equation confidence="0.895215181818182">
Example 23
S : ( (head n) = —
(head v) = +
(head bar) = 2
(head sub]) = + ).
Example 24
S-rule : X0 —&gt; X1 X2;
( (X0) = S
(Xi head bar) = 2
!(X2 head sub]) = —
(X0 head) =- (X2 head)
</equation>
<bodyText confidence="0.999892">
The final equation in 24 both implements the HFC and defines X2 as the head daughter.
An exception to the reentrancy is the fact that (X2 head sub]) = —, which is therefore
represented as nondefault information. In this approach, the HFC is part of the rules
itself and thus, the effect of Shieber&apos;s (1986b) special-purpose compilations step, which
adds the HFC conservatively, is achieved directly.
</bodyText>
<footnote confidence="0.696247666666667">
implementing the effect of FSD2 above as a feature coocurrence restriction.
8 Note that, as in GPSG, the feature INV plays a double role by indicating both an item&apos;s potential to
occur in inverted structures as well as indicating whether a given structure is inverted or not.
</footnote>
<page confidence="0.990998">
201
</page>
<note confidence="0.67899">
Computational Linguistics Volume 18, Number 2
</note>
<sectionHeader confidence="0.976877" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.999995225806452">
We have shown in the preceding sections that it is possible to incorporate nonmono-
tonicity in a unification-based formalism, while at the same time preserving the idea
that linguistic knowledge is represented in the form of feature structures.
In spite of their great flexibility, unification-based formalisms are in general not
very well equipped to deal with linguistic rules or generalizations that have a default
character and for which exceptions exist. In Sections 4 and 5 we hope to have demon-
strated that a nonmonotonic operation on feature structures combined with straight-
forward extensions of the description languages used in unification-based formalisms
enables a satisfactory account of the phenomena mentioned in the introduction. The
applications illustrate that default unification can be used to give linguistically ap-
pealing implementations of certain natural language phenomena, not that it would be
impossible to account for these facts using unification only. Thus, default unification
serves to extend the expressive power of unification-based formalisms, but leaves the
representation method of unification-based formalisms, in which linguistic objects are
represented as feature structures, unchanged. Comparing default unification to earlier
proposals, we believe that an advantage of our approach is that it is general, in the
sense that one operation is used to achieve the effects of overwriting, add conservatively,
nonmonotonic template inheritance, and priority union. Also, whereas previous proposals
do not seem to be well behaved for feature structures containing reentrancies, default
unification is defined for feature structures of arbitrary complexity.
Dorre et al. (1990) suggest that the use of nonmonotonic devices in unification-
based formalisms will, for the time being, be limited to off-line extensions of these
formalisms; that is, extensions whose effect can be computed at compile time and re-
sult in ordinary feature structures. They also note that while there may be linguistic
arguments in favor of more dynamic notions of default reasoning, from a computa-
tional point of view the off-line approach is clearly preferred. Default unification, as
used in the previous section, is an example of an off-line extension, as the effects of
nonmonotonic template inheritance, lexical defaults, and the meaning of rule defini-
tions in which default and non-default information is combined, can be computed at
compile time. Again, this emphasizes the point that incorporation of default unification
in principle only extends the expressive power of unification-based formalisms.
</bodyText>
<sectionHeader confidence="0.990576" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999390583333333">
A syntactic approach to default unification
is presented in Bouma (1990). The reactions
on that paper made it clear to me that
default unification should be defined not
only for feature structure descriptions, but
also for feature structures themselves. For
helpful questions, suggestions, and
comments on the material presented here, I
would like to thank Bob Carpenter, John
Nerbonne, audiences in Tilburg, Groningen,
Tubingen, and Düsseldorf, and three
anonymous CL reviewers.
</bodyText>
<sectionHeader confidence="0.996345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992505">
Bouma, Gosse (1990). &amp;quot;Defaults in
unification grammar.&amp;quot; In Proceedings, 28th
Annual Meeting of the Association for
Computational Linguistics, Pittsburgh, PA,
165-172.
Bresnan, Joan, and Kaplan, Ronald (1982).
&amp;quot;Lexical functional grammar: A formal
system for grammatical representation.&amp;quot;
In The Mental Representation of Grammatical
Relations, edited by J. Bresnan, 173-281.
Cambridge, MA: The MIT Press.
Dawar, Anuj, and Vijay-Shanker, K. (1990).
&amp;quot;An interpretation of negation in feature
structure descriptions.&amp;quot; Computational
Linguistics, 16(1), 11-21.
Daelemans, Walter (1988). &amp;quot;A model of
Dutch morphophonology and its
applications.&amp;quot; Al Communications, 1(2),
18-25.
Dorre, Jochen; Eisele, Andreas; Wedekind,
Jürgen; Calder, Jo; and Reape, Mike
(1990). A Survey of Linguistically Motivated
Extensions to Unification-Based Formalisms.
DYANA Deliverable R3.1.A., Centre for
</reference>
<page confidence="0.973254">
202
</page>
<note confidence="0.574592">
Gosse Bouma Feature Structures and Nonmonotonicity
</note>
<reference confidence="0.997927362068965">
Cognitive Science, University of
Edinburgh.
Evans, Roger, and Gazdar, Gerald (1989a).
&amp;quot;Inference in DATR.&amp;quot; In Proceedings,
Fourth Conference of the European Chapter of
the ACL,&amp;quot; University of Manchester, 66-71.
Evans, Roger, and Gazdar, Gerald (1989b).
&amp;quot;The semantics of DATR.&amp;quot; In Proceedings,
Seventh Conference of the Society for the Study
of Artificial Intelligence and the Simulation of
Behaviour, edited by A. Cohn, 79-87.
London: Pitman Publ.
Flickinger, Daniel (1987). Lexical Rules in the
Hierarchical Lexicon. Doctoral dissertation,
Stanford University, Stanford, CA.
Flickinger, Daniel; Pollard, Carl; and Wasow,
Thomas (1985). &amp;quot;Structure-sharing in
lexical representation.&amp;quot; In Proceedings, 23rd
Annual Meeting of the Association for
Computational Linguistics. Chicago, Illinois,
262-267.
Gazdar, Gerald; Klein, Ewan; Pullum,
Geoffrey; and Sag, Ivan (1985). Generalized
Phrase Structure Grammar. London:
Blackwell.
Kaplan, Ronald (1987). &amp;quot;Three seductions of
computational psycholinguistics.&amp;quot; In
Linguistic Theory and Computer Applications,
edited by P. Whitelock, H. Somers,
P. Bennett, R. Johnson, and M. McGee
Wood, 149-188. London: Academic Press.
Kasper, Robert, and Rounds, William (1986).
&amp;quot;A logical semantics for feature
structures.&amp;quot; In Proceedings, 26th Annual
Meeting of the Association for Computational
Linguistics. New York, NY, 257-266.
Kasper, Robert, and Rounds, William (1990).
&amp;quot;The logic of unification in grammar.&amp;quot;
Linguistics and Philosophy, 13(1), 35-58.
Pollard, Carl, and Sag, Ivan (1987).
Information-Based Syntax and Semantics,
Volume 1: Fundamentals. CSLI Lecture
Notes 13. Chicago: University of Chicago
Press.
Shieber, Stuart (1986a). An Introduction to
Unification-Based Approaches to Grammar.
CSLI Lecture Notes 4. Chicago: University
of Chicago Press.
Shieber, Stuart (1986b). &amp;quot;A simple
reconstruction of GPSG.&amp;quot; In Proceedings,
COLING 1986. Bonn, Germany, 211-215.
De Smedt, Koenraad (1990). Incremental
Sentence Generation. Doctoral dissertation,
Katholieke Universiteit Nijmegen,
Nijmegen, The Netherlands.
Touretzky, David (1986). The Mathematics of
Inheritance Systems. Los Altos, CA:
Morgan Kaufmann.
</reference>
<page confidence="0.999203">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516146">
<title confidence="0.999616">Feature Structures and Nonmonotonicity</title>
<author confidence="0.883758">Gosse Bouma</author>
<affiliation confidence="0.554537">Rijksuniversiteit Groningen</affiliation>
<abstract confidence="0.99073375">Unification-based grammar formalisms use feature structures to represent linguistic knowledge. The only operation defined on feature structures, unification, is information-combining and monotonic. Several authors have proposed nonmonotonic extensions of this formalism, as for a linguistically adequate description of certain natural language phenomena some kind of default reasoning seems essential. We argue that the effect of these proposals can be captured by means one general, nonmonotonic, operation on feature structures, called unification. provide a formal semantics of the operation and demonstrate how some of the phenomena used to motivate nonmonotonic extensions of unification-based formalisms can be handled.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
</authors>
<title>Defaults in unification grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>165--172</pages>
<location>Pittsburgh, PA,</location>
<marker>Bouma, 1990</marker>
<rawString>Bouma, Gosse (1990). &amp;quot;Defaults in unification grammar.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, PA, 165-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Ronald Kaplan</author>
</authors>
<title>Lexical functional grammar: A formal system for grammatical representation.&amp;quot; In The Mental Representation of Grammatical Relations,</title>
<date>1982</date>
<pages>173--281</pages>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<note>edited by</note>
<contexts>
<context position="9042" citStr="Bresnan and Kaplan, 1982" startWordPosition="1360" endWordPosition="1363">dundantly) past tense formation of these irregular verbs. Evans and Gazdar (1989a,b) present the DATR-formalism, which, among other things, contains a nonmonotonic inference system that enables an elegant account of the blocking-phenomenon just described. The examples used throughout their presentation are all drawn from inflectional morphology and illustrate once more the importance of default reasoning in this area of linguistics. • Gapping. In Kaplan (1987) it is observed that gapping constructions and other forms of nonconstituent conjunction can be analyzed in Lexical Functional Grammar (Bresnan and Kaplan, 1982) as the conjunction of two functional-structures (f-structures), one of which may be incomplete. The missing information in the incomplete f-structure can be filled in if it is merged with the complete f-structure, using an operation called priority union. Priority union of two f-structures A and B is defined as an operation that extends A with information from B that is not included (or filled in) in A. As not all information in B is present in the priority union of A and B, this operation introduces nonmonotonicity. The proposals for incorporating the kind of default reasoning that is requir</context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>Bresnan, Joan, and Kaplan, Ronald (1982). &amp;quot;Lexical functional grammar: A formal system for grammatical representation.&amp;quot; In The Mental Representation of Grammatical Relations, edited by J. Bresnan, 173-281. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anuj Dawar</author>
<author>K Vijay-Shanker</author>
</authors>
<title>An interpretation of negation in feature structure descriptions.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>11--21</pages>
<contexts>
<context position="16549" citStr="Dawar and Vijay-Shanker 1990" startWordPosition="2601" endWordPosition="2604">ue pairs where values are either atoms or feature structures themselves and, furthermore, values may be shared by different attributes in the feature structure. Feature structures can be defined using a description language, such as the one found in PATR-II (Shieber 1986a) or in Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b. Example 4 a. ( (f) = a (g f) = a (g f) = (g g) ) - f : a b. [ g :7 a ] Following the approach of Kasper and Rounds (1986; 1990), and others, we represent feature structures formally as finite (acyclic) automata (the definition below is taken from Dawar and Vijay-Shanker 1990): Definition A finite acyclic automaton A is a 7-tuple (Q, E,F, 6, qo, F, A) where: 1. Q is a nonempty finite set of states, 2. E is a countable set (the alphabet), 3. F is a countable set (the output alphabet), 4. .5 : Q x E Q is a finite partial function (the transition function), 5. qo E Q, 6. F C Q, 7. A : F F is a total function (the output function), 8. the directed graph (Q, E) is acyclic, where pEq iff for some 1 E E,5(p,1) = q, 9. for every q e Q, there exists a directed path from qo to q in (Q, E), and 10. for every q E F,S(q,1) is not defined for any 1. We will frequently write QA, </context>
</contexts>
<marker>Dawar, Vijay-Shanker, 1990</marker>
<rawString>Dawar, Anuj, and Vijay-Shanker, K. (1990). &amp;quot;An interpretation of negation in feature structure descriptions.&amp;quot; Computational Linguistics, 16(1), 11-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>A model of Dutch morphophonology and its applications.&amp;quot;</title>
<date>1988</date>
<journal>Al Communications,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>18--25</pages>
<contexts>
<context position="5555" citStr="Daelemans (1988)" startWordPosition="835" endWordPosition="836">X, head) to every rule of the form X0 Xi • .. Xn (with X, (1 &lt;i &lt; n) the head of the rule and assuming all head features to be collected under head) will not do, as it rules out the possibility of exceptions altogether. Shieber (1986b) therefore proposes to add this constraint conservatively, which means that, if the rule already contains conflicting information for some head feature f, the constraint is replaced by a set of constraints (X0 head f&apos;) = (X, head f&apos;), for all head features f&apos; f. • Structuring the Lexicon. Flickinger, Pollard, and Wasow (1985), Flickinger (1987), De Smedt (1990), Daelemans (1988), and others, have argued that the encoding and maintenance of the detailed lexical descriptions typical for lexicalist grammar formalisms benefits greatly from the use of (nonmonotonic) inheritance. In Flickinger, Pollard, and Wasow (1985), for instance, lexical information is organized in the form of frames, which are comparable to the templates (i.e., feature structures that may be used as part of the definition of other feature structures) of PATR-II (Shieber 1986a). A frame or specific lexical entry may inherit from more general frames. Frames can be used to encode information economicall</context>
</contexts>
<marker>Daelemans, 1988</marker>
<rawString>Daelemans, Walter (1988). &amp;quot;A model of Dutch morphophonology and its applications.&amp;quot; Al Communications, 1(2), 18-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jochen Dorre</author>
<author>Andreas Eisele</author>
<author>Jürgen Wedekind</author>
<author>Jo Calder</author>
<author>Mike Reape</author>
</authors>
<title>A Survey of Linguistically Motivated Extensions to Unification-Based Formalisms. DYANA Deliverable R3.1.A.,</title>
<date>1990</date>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<contexts>
<context position="49170" citStr="Dorre et al. (1990)" startWordPosition="8475" endWordPosition="8478">e representation method of unification-based formalisms, in which linguistic objects are represented as feature structures, unchanged. Comparing default unification to earlier proposals, we believe that an advantage of our approach is that it is general, in the sense that one operation is used to achieve the effects of overwriting, add conservatively, nonmonotonic template inheritance, and priority union. Also, whereas previous proposals do not seem to be well behaved for feature structures containing reentrancies, default unification is defined for feature structures of arbitrary complexity. Dorre et al. (1990) suggest that the use of nonmonotonic devices in unificationbased formalisms will, for the time being, be limited to off-line extensions of these formalisms; that is, extensions whose effect can be computed at compile time and result in ordinary feature structures. They also note that while there may be linguistic arguments in favor of more dynamic notions of default reasoning, from a computational point of view the off-line approach is clearly preferred. Default unification, as used in the previous section, is an example of an off-line extension, as the effects of nonmonotonic template inheri</context>
</contexts>
<marker>Dorre, Eisele, Wedekind, Calder, Reape, 1990</marker>
<rawString>Dorre, Jochen; Eisele, Andreas; Wedekind, Jürgen; Calder, Jo; and Reape, Mike (1990). A Survey of Linguistically Motivated Extensions to Unification-Based Formalisms. DYANA Deliverable R3.1.A., Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>Inference in DATR.&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, Fourth Conference of the European Chapter of</booktitle>
<pages>66--71</pages>
<institution>University of Manchester,</institution>
<contexts>
<context position="7997" citStr="Evans and Gazdar (1989" startWordPosition="1205" endWordPosition="1208">ailable for capturing lexical generalizations are templates (see above) and lexical rules. Lexical rules may transform the feature structure of a lexical entry. An example is the rule for agentless passive (Shieber 1986a, p. 62), which transforms the feature structure for transitive past participles into a feature structure for participles occurring in agentless passive constructions. Lexical rules can only change the feature structure of a lexical entry, not its word form, and thus, the scope of these rules is rather restricted. While the examples in Flickinger, Pollard, and Wasow (1985) and Evans and Gazdar (1989a,b) suggest that the latter restriction can be easily removed, it is not so obvious how a unification-based grammar formalism can cope with the combination of rules and exceptions typical for (inflectional) morphology. For instance, it is possible to formulate a rule that describes past tense formation in English, but it is not so easy to exclude the application of this rule to irregular verbs and to describe (nonredundantly) past tense formation of these irregular verbs. Evans and Gazdar (1989a,b) present the DATR-formalism, which, among other things, contains a nonmonotonic inference system</context>
<context position="14480" citStr="Evans and Gazdar (1989" startWordPosition="2274" endWordPosition="2277">that information in the generic class is to be included in the specific class as well, as long as the specific class does not contain local information that is in conflict with the information to be inherited. Such an inheritance strategy will run into problems, however, if reentrancies are generally allowed. For instance, think of the examples presented above as involving a generic class FSD from which a specific class FSND inherits. The inheritance procedure in, for instance, Flickinger (1987, p. 59ff) does not say anything about which one of the possible results will be chosen. The work of Evans and Gazdar (1989a,b), finally, is not easily incorporated in a unification-based formalism, as they use semantic nets instead of feature structures to represent linguistic information. That is, although the syntax of DATR is suggestively similar to that of, for instance, PATR-II, DATR descriptions do in fact denote graphs that differ rather substantially from the graphs used to represent feature structures (see Evans and Gazdar 1989b). The nonmonotonic reasoning facilities of DATR therefore are not directly applicable in a unification-based formalism either. We conclude that a formally explicit definition of </context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger, and Gazdar, Gerald (1989a). &amp;quot;Inference in DATR.&amp;quot; In Proceedings, Fourth Conference of the European Chapter of the ACL,&amp;quot; University of Manchester, 66-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>The semantics of DATR.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, Seventh Conference of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour, edited by A. Cohn,</booktitle>
<pages>79--87</pages>
<publisher>Pitman Publ.</publisher>
<location>London:</location>
<contexts>
<context position="7997" citStr="Evans and Gazdar (1989" startWordPosition="1205" endWordPosition="1208">ailable for capturing lexical generalizations are templates (see above) and lexical rules. Lexical rules may transform the feature structure of a lexical entry. An example is the rule for agentless passive (Shieber 1986a, p. 62), which transforms the feature structure for transitive past participles into a feature structure for participles occurring in agentless passive constructions. Lexical rules can only change the feature structure of a lexical entry, not its word form, and thus, the scope of these rules is rather restricted. While the examples in Flickinger, Pollard, and Wasow (1985) and Evans and Gazdar (1989a,b) suggest that the latter restriction can be easily removed, it is not so obvious how a unification-based grammar formalism can cope with the combination of rules and exceptions typical for (inflectional) morphology. For instance, it is possible to formulate a rule that describes past tense formation in English, but it is not so easy to exclude the application of this rule to irregular verbs and to describe (nonredundantly) past tense formation of these irregular verbs. Evans and Gazdar (1989a,b) present the DATR-formalism, which, among other things, contains a nonmonotonic inference system</context>
<context position="14480" citStr="Evans and Gazdar (1989" startWordPosition="2274" endWordPosition="2277">that information in the generic class is to be included in the specific class as well, as long as the specific class does not contain local information that is in conflict with the information to be inherited. Such an inheritance strategy will run into problems, however, if reentrancies are generally allowed. For instance, think of the examples presented above as involving a generic class FSD from which a specific class FSND inherits. The inheritance procedure in, for instance, Flickinger (1987, p. 59ff) does not say anything about which one of the possible results will be chosen. The work of Evans and Gazdar (1989a,b), finally, is not easily incorporated in a unification-based formalism, as they use semantic nets instead of feature structures to represent linguistic information. That is, although the syntax of DATR is suggestively similar to that of, for instance, PATR-II, DATR descriptions do in fact denote graphs that differ rather substantially from the graphs used to represent feature structures (see Evans and Gazdar 1989b). The nonmonotonic reasoning facilities of DATR therefore are not directly applicable in a unification-based formalism either. We conclude that a formally explicit definition of </context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger, and Gazdar, Gerald (1989b). &amp;quot;The semantics of DATR.&amp;quot; In Proceedings, Seventh Conference of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour, edited by A. Cohn, 79-87. London: Pitman Publ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Flickinger</author>
</authors>
<title>Lexical Rules in the Hierarchical Lexicon. Doctoral dissertation,</title>
<date>1987</date>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="5520" citStr="Flickinger (1987)" startWordPosition="830" endWordPosition="831"> Adding the constraint (X0 head) = (X, head) to every rule of the form X0 Xi • .. Xn (with X, (1 &lt;i &lt; n) the head of the rule and assuming all head features to be collected under head) will not do, as it rules out the possibility of exceptions altogether. Shieber (1986b) therefore proposes to add this constraint conservatively, which means that, if the rule already contains conflicting information for some head feature f, the constraint is replaced by a set of constraints (X0 head f&apos;) = (X, head f&apos;), for all head features f&apos; f. • Structuring the Lexicon. Flickinger, Pollard, and Wasow (1985), Flickinger (1987), De Smedt (1990), Daelemans (1988), and others, have argued that the encoding and maintenance of the detailed lexical descriptions typical for lexicalist grammar formalisms benefits greatly from the use of (nonmonotonic) inheritance. In Flickinger, Pollard, and Wasow (1985), for instance, lexical information is organized in the form of frames, which are comparable to the templates (i.e., feature structures that may be used as part of the definition of other feature structures) of PATR-II (Shieber 1986a). A frame or specific lexical entry may inherit from more general frames. Frames can be use</context>
<context position="13573" citStr="Flickinger (1987)" startWordPosition="2132" endWordPosition="2133">operation on feature structures that relies (only) on the fact that the result should be consistent must be very restricted indeed, as more generic versions will always run into the problem that there can be several mutually exclusive solutions to solving a given unification conflict. Second, claims that the operations add conservatively, overwriting, and priority union are equivalent are unwarranted, as no definitions of these operations are available that are sufficiently explicit to determine what their result would be in moderately complex examples such as 1-3. The approach exemplified by Flickinger (1987) and others is to use a generalpurpose knowledge representation formalism to represent linguistic information and model default inheritance. Feature structures are defined as classes of some sort, which may inherit from other, more generic, classes. The inheritance strategy used says that information in the generic class is to be included in the specific class as well, as long as the specific class does not contain local information that is in conflict with the information to be inherited. Such an inheritance strategy will run into problems, however, if reentrancies are generally allowed. For </context>
</contexts>
<marker>Flickinger, 1987</marker>
<rawString>Flickinger, Daniel (1987). Lexical Rules in the Hierarchical Lexicon. Doctoral dissertation, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Flickinger</author>
<author>Carl Pollard</author>
<author>Thomas Wasow</author>
</authors>
<title>Structure-sharing in lexical representation.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>262--267</pages>
<location>Chicago, Illinois,</location>
<marker>Flickinger, Pollard, Wasow, 1985</marker>
<rawString>Flickinger, Daniel; Pollard, Carl; and Wasow, Thomas (1985). &amp;quot;Structure-sharing in lexical representation.&amp;quot; In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics. Chicago, Illinois, 262-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<date>1985</date>
<booktitle>Generalized Phrase Structure Grammar.</booktitle>
<publisher>Blackwell.</publisher>
<location>London:</location>
<contexts>
<context position="4262" citStr="Gazdar et al. 1985" startWordPosition="626" endWordPosition="629">e, all regular verbs must be marked explicitly as -INV (to prevent them from occurring in the inversion rule). Note that, in a unification-based grammar, there is no need to mark the exceptional verbs as +INV, which leads to the rather counterintuitive situation that regular verbs need to be marked extra, whereas the exceptional ones can remain underspecified. A more natural solution would be to assign all verbs the specification -INV by default (either by means of template inheritance or by means of lexical feature specification defaults as used in Generalized Phrase Structure Grammar [GPSG; Gazdar et al. 19851) and to overwrite or block this specification in the exceptional cases. The possibility of incorporating an overwrite operation in a unification-based formalism is mentioned in Shieber (1986a, p. 60). • Feature Percolation Principles. Both GPSG and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag 1987) adopt the so-called Head Feature Convention (HFC). In GPSG, the HFC is a default principle: head features will normally have identical values on mother and head, but specific rules may assign incompatible values to specific head features. In unification-based formalisms, it is impos</context>
<context position="12113" citStr="Gazdar et al. 1985" startWordPosition="1861" endWordPosition="1864"> of priority union. Thus, given two feature structures FSD (the default) and FSND (the nondefault), adding FSD to FSND conservatively would be equivalent to overwriting FSD with FSND, and to the priority union of FSND and FSD (i.e. FSND/FSD in the notation of Kaplan [1987]). However, in light of the example above, it should be clear that such a generalization is highly problematic. Other examples worth considering are 2 and 3. 1 Whether this kind of situation can occur in GPSG probably depends on whether one is willing to conclude from examples such as: S [COMP a] {[SLIBCAT all , H[COMP NIL] (Gazdar et al. 1985, p. 248) that covariation of arbitrary categories is in principle not excluded in this formalism. 1 f g : f : a g : b f : a g : all f g::ab f- : 1 b g: 1 b — ]] fg:: b I ] head: X0: head: • : head: : a. head: • : head: X0: b. head: • : 186 Gosse Bouma Feature Structures and Nonmonotonicity Example 2 FSD [f:a FSND = g:b Example 3 FSD [ f : a 1 FSND [g:b] Again, if we try to combine the two feature structures along the lines of any one of the operations mentioned above, there are at least two possible results (note that in Example 3, we could either preserve the information that features f and </context>
<context position="38840" citStr="Gazdar et al. (1985" startWordPosition="6764" endWordPosition="6767">on the nform of the subject are inherited from the embedded VP: Example 15 a. it will annoy Kim that she lost b. *Sue will annoy Kim that she lost This dependency between elements of the subcat list is encoded in the final equation, which also suppresses (or overwrites) the default value for (nform). The denotation of AUX is thus: Example 16 rest : cat: v aux: — inv : — first : [ nform:f cat : np subcat rest : empty cat : np [first : nform : rest : empty - cat : v aux: + inv : + subcat: first : 1 I 1 The nonmonotonic inheritance regime is flexible enough to allow for exceptions to exceptions. Gazdar et al. (1985, p. 65) observe that at least in some dialects of English, the auxiliary might cannot occur in inverted structures. This is expressed in the following lexical entry, in which might inherits nonmonotonically from AUX, which itself inherits nonmonotonically from VERB: Example 17 might : ( AUX !(inv) = — ). There is an important difference between the approach to nonmonotonic inheritance sketched here and the majority of inheritance-based formalisms used for Knowledge Representation, which has to do with the way in which templates are evaluated. If a template is used as part of the definition of</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan (1985). Generalized Phrase Structure Grammar. London: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
</authors>
<title>Three seductions of computational psycholinguistics.&amp;quot;</title>
<date>1987</date>
<booktitle>In Linguistic Theory and Computer Applications, edited</booktitle>
<pages>149--188</pages>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="8881" citStr="Kaplan (1987)" startWordPosition="1339" endWordPosition="1340">at describes past tense formation in English, but it is not so easy to exclude the application of this rule to irregular verbs and to describe (nonredundantly) past tense formation of these irregular verbs. Evans and Gazdar (1989a,b) present the DATR-formalism, which, among other things, contains a nonmonotonic inference system that enables an elegant account of the blocking-phenomenon just described. The examples used throughout their presentation are all drawn from inflectional morphology and illustrate once more the importance of default reasoning in this area of linguistics. • Gapping. In Kaplan (1987) it is observed that gapping constructions and other forms of nonconstituent conjunction can be analyzed in Lexical Functional Grammar (Bresnan and Kaplan, 1982) as the conjunction of two functional-structures (f-structures), one of which may be incomplete. The missing information in the incomplete f-structure can be filled in if it is merged with the complete f-structure, using an operation called priority union. Priority union of two f-structures A and B is defined as an operation that extends A with information from B that is not included (or filled in) in A. As not all information in B is </context>
</contexts>
<marker>Kaplan, 1987</marker>
<rawString>Kaplan, Ronald (1987). &amp;quot;Three seductions of computational psycholinguistics.&amp;quot; In Linguistic Theory and Computer Applications, edited by P. Whitelock, H. Somers, P. Bennett, R. Johnson, and M. McGee Wood, 149-188. London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
<author>William Rounds</author>
</authors>
<title>A logical semantics for feature structures.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>257--266</pages>
<location>New York, NY,</location>
<contexts>
<context position="16223" citStr="Kasper and Rounds (1986" startWordPosition="2535" endWordPosition="2538">active to consider an operation that subsumes the effects of the proposals so far. Default Unification, as defined below, is an attempt to provide such an operation. 187 Computational Linguistics Volume 18, Number 2 3. Feature Structures and Unification Feature structures are often depicted as matrices of attribute-value pairs where values are either atoms or feature structures themselves and, furthermore, values may be shared by different attributes in the feature structure. Feature structures can be defined using a description language, such as the one found in PATR-II (Shieber 1986a) or in Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b. Example 4 a. ( (f) = a (g f) = a (g f) = (g g) ) - f : a b. [ g :7 a ] Following the approach of Kasper and Rounds (1986; 1990), and others, we represent feature structures formally as finite (acyclic) automata (the definition below is taken from Dawar and Vijay-Shanker 1990): Definition A finite acyclic automaton A is a 7-tuple (Q, E,F, 6, qo, F, A) where: 1. Q is a nonempty finite set of states, 2. E is a countable set (the alphabet), 3. F is a countable set (the output alphabet), 4. .5 : Q x E Q is a finite partial function (the transition f</context>
</contexts>
<marker>Kasper, Rounds, 1986</marker>
<rawString>Kasper, Robert, and Rounds, William (1986). &amp;quot;A logical semantics for feature structures.&amp;quot; In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics. New York, NY, 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
<author>William Rounds</author>
</authors>
<title>The logic of unification in grammar.&amp;quot;</title>
<date>1990</date>
<journal>Linguistics and Philosophy,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>35--58</pages>
<marker>Kasper, Rounds, 1990</marker>
<rawString>Kasper, Robert, and Rounds, William (1990). &amp;quot;The logic of unification in grammar.&amp;quot; Linguistics and Philosophy, 13(1), 35-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Information-Based Syntax and Semantics, Volume 1: Fundamentals.</title>
<date>1987</date>
<booktitle>CSLI Lecture Notes 13.</booktitle>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="4578" citStr="Pollard and Sag 1987" startWordPosition="671" endWordPosition="674">s the exceptional ones can remain underspecified. A more natural solution would be to assign all verbs the specification -INV by default (either by means of template inheritance or by means of lexical feature specification defaults as used in Generalized Phrase Structure Grammar [GPSG; Gazdar et al. 19851) and to overwrite or block this specification in the exceptional cases. The possibility of incorporating an overwrite operation in a unification-based formalism is mentioned in Shieber (1986a, p. 60). • Feature Percolation Principles. Both GPSG and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag 1987) adopt the so-called Head Feature Convention (HFC). In GPSG, the HFC is a default principle: head features will normally have identical values on mother and head, but specific rules may assign incompatible values to specific head features. In unification-based formalisms, it is impossible to express this principle directly. Adding the constraint (X0 head) = (X, head) to every rule of the form X0 Xi • .. Xn (with X, (1 &lt;i &lt; n) the head of the rule and assuming all head features to be collected under head) will not do, as it rules out the possibility of exceptions altogether. Shieber (1986b) the</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl, and Sag, Ivan (1987). Information-Based Syntax and Semantics, Volume 1: Fundamentals. CSLI Lecture Notes 13. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar.</title>
<date>1986</date>
<journal>CSLI Lecture Notes</journal>
<volume>4</volume>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="1266" citStr="Shieber 1986" startWordPosition="169" endWordPosition="170"> formal semantics of the operation and demonstrate how some of the phenomena used to motivate nonmonotonic extensions of unification-based formalisms can be handled. 1. Introduction While monotonicity is often desirable from a formal and computational perspective, it is at odds with a considerable body of linguistic work. Default principles, default rules, and default feature-values can be found in many linguistic formalisms and are used prominently in work on phonology, morphology, and syntax. In spite of their great expressive power and flexibility, unification-based grammar formalisms (see Shieber 1986a, for an introduction) are in general not very successful in modeling such devices. Unification is an information-combining, monotonic, operation on feature structures, whereas the implementation of default devices typically requires some form of nonmonotonicity. In this paper, we present a nonmonotonic operation on feature structures, which enables us to implement the effects of a number of default devices used in linguistics. As the operation is defined in terms of feature structures only, an important characteristic of unification-based formalisms, namely that linguistic knowledge is encod</context>
<context position="4454" citStr="Shieber (1986" startWordPosition="655" endWordPosition="656">rbs as +INV, which leads to the rather counterintuitive situation that regular verbs need to be marked extra, whereas the exceptional ones can remain underspecified. A more natural solution would be to assign all verbs the specification -INV by default (either by means of template inheritance or by means of lexical feature specification defaults as used in Generalized Phrase Structure Grammar [GPSG; Gazdar et al. 19851) and to overwrite or block this specification in the exceptional cases. The possibility of incorporating an overwrite operation in a unification-based formalism is mentioned in Shieber (1986a, p. 60). • Feature Percolation Principles. Both GPSG and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag 1987) adopt the so-called Head Feature Convention (HFC). In GPSG, the HFC is a default principle: head features will normally have identical values on mother and head, but specific rules may assign incompatible values to specific head features. In unification-based formalisms, it is impossible to express this principle directly. Adding the constraint (X0 head) = (X, head) to every rule of the form X0 Xi • .. Xn (with X, (1 &lt;i &lt; n) the head of the rule and assuming all head fea</context>
<context position="6027" citStr="Shieber 1986" startWordPosition="905" endWordPosition="906">l head features f&apos; f. • Structuring the Lexicon. Flickinger, Pollard, and Wasow (1985), Flickinger (1987), De Smedt (1990), Daelemans (1988), and others, have argued that the encoding and maintenance of the detailed lexical descriptions typical for lexicalist grammar formalisms benefits greatly from the use of (nonmonotonic) inheritance. In Flickinger, Pollard, and Wasow (1985), for instance, lexical information is organized in the form of frames, which are comparable to the templates (i.e., feature structures that may be used as part of the definition of other feature structures) of PATR-II (Shieber 1986a). A frame or specific lexical entry may inherit from more general frames. Frames can be used to encode information economically and, perhaps more importantly, as a means to express linguistic generalizations. For instance, all properties typical of verbs are defined in the VERB-frame, and properties typical of auxiliaries are defined in the AUX-frame. The AUX-frame may inherit from the VERB-frame, thus capturing the fact that an auxiliary is a kind of verb. 184 Gosse Bouma Feature Structures and Nonmonotonicity In this approach, a mechanism that allows inheritance of information by default (</context>
<context position="7594" citStr="Shieber 1986" startWordPosition="1145" endWordPosition="1146">ss in the form of a frame, without ruling out the possibility that exceptions might exist. In unification-based formalisms, templates can play the role of frames, but as unification is used to implement inheritance, nonmonotonic inheritance is impossible. • Inflectional Morphology. In PATR-II the lexicon is a list of inflected word forms associated with feature structures. The only tools available for capturing lexical generalizations are templates (see above) and lexical rules. Lexical rules may transform the feature structure of a lexical entry. An example is the rule for agentless passive (Shieber 1986a, p. 62), which transforms the feature structure for transitive past participles into a feature structure for participles occurring in agentless passive constructions. Lexical rules can only change the feature structure of a lexical entry, not its word form, and thus, the scope of these rules is rather restricted. While the examples in Flickinger, Pollard, and Wasow (1985) and Evans and Gazdar (1989a,b) suggest that the latter restriction can be easily removed, it is not so obvious how a unification-based grammar formalism can cope with the combination of rules and exceptions typical for (inf</context>
<context position="10546" citStr="Shieber (1986" startWordPosition="1601" endWordPosition="1602">a in mind and also are rather intimately connected to peculiarities of the GPSG-formalism. What 185 Computational Linguistics Volume 18, Number 2 is particularly striking is the fact that two different conceptions of default appear to play a role: a head feature is exempt from the HFC only if this would otherwise lead to an inconsistency, whereas a feature is exempt from having the value specified in some feature specification default (among others) if this feature covaries with another feature. Overwrite and add conservatively are also highly restricted operations. From the examples given in Shieber (1986a) it seems as if overwriting can only be used to add or substitute (nonmonotonically) one atomic feature value in a given (possibly complex) feature structure (which acts as default). Add conservatively, on the other hand, is only used to add one reentrancy (as far as possible) to a given feature structure (which acts as nondefault). An additional restriction is that add conservatively is well behaved only for the kind of feature structures used in GPSG (that is, feature structures in which limited use is made of covariation or reentrancy). Consider for instance the example in (1).1 Adding th</context>
<context position="16191" citStr="Shieber 1986" startWordPosition="2531" endWordPosition="2532">nd thus it seems attractive to consider an operation that subsumes the effects of the proposals so far. Default Unification, as defined below, is an attempt to provide such an operation. 187 Computational Linguistics Volume 18, Number 2 3. Feature Structures and Unification Feature structures are often depicted as matrices of attribute-value pairs where values are either atoms or feature structures themselves and, furthermore, values may be shared by different attributes in the feature structure. Feature structures can be defined using a description language, such as the one found in PATR-II (Shieber 1986a) or in Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b. Example 4 a. ( (f) = a (g f) = a (g f) = (g g) ) - f : a b. [ g :7 a ] Following the approach of Kasper and Rounds (1986; 1990), and others, we represent feature structures formally as finite (acyclic) automata (the definition below is taken from Dawar and Vijay-Shanker 1990): Definition A finite acyclic automaton A is a 7-tuple (Q, E,F, 6, qo, F, A) where: 1. Q is a nonempty finite set of states, 2. E is a countable set (the alphabet), 3. F is a countable set (the output alphabet), 4. .5 : Q x E Q is a finite pa</context>
<context position="20363" citStr="Shieber (1986" startWordPosition="3313" endWordPosition="3314">ucture A and the nondefault feature structure B. The operation has the following characteristics: 1. It has a declarative semantics and is procedurally neutral. That is, if A = A&apos; and B = B&apos;, then (AU!B) = (A&apos;U!B&apos;). 2. It is monotonic only with respect to the nondefault argument. That is, B (ALA) is always true, but in general A E (ALJ!B) will not hold. 3. It never fails. If A is fully incompatible with B, (AU!B) = B. 4. It gives a unique result. 5. Reentrancies in the nondefault argument may be replaced by a weaker set of reentrancies if necessary (this is the add conservatively operation of Shieber (1986b)). Intuitions about default unification appear to be more clear in those cases where feature structures do not contain any reentrancies. Therefore, we will first define default unification for this case, moving to the general case in Section 4.2. Section 4.3. deals with the incorporation of add conservatively. 4.1 Default Unification without Reentrancies Subsumption suggests a straightforward definition of an operation that has properties 1-4 above. Definition Default Unification (first version) AU!B = A&apos; U B, where A&apos; is the maximal (i.e. most specific) element in the subsumption ordering s</context>
<context position="37830" citStr="Shieber 1986" startWordPosition="6578" endWordPosition="6579">mpt is made to encode some of the peculiarities of the English auxiliary system in a lexicalist grammar: Example 14 NP : ( (cat) = n (nform) = norm )• VERB : ( (cat) = v (aux) = — (iv) = — VP : ( VERB (subcat first) = NP (subcat rest) = empty AUX : ( VERB !(aux) = !(inv) = (subcat first) = VP (subcat rest first) = NP (subcat rest rest) = empty !(subcat first subcat first nform) = (subcat rest first nform) )• 197 Computational Linguistics Volume 18, Number 2 Adding the equations ! (aux) : + and !(inv) : +5 to the definition of AUX has an effect comparable to that of the overwrite-operation of (Shieber 1986a, p. 60). The AUX template inherits from VERB by default, but the equations just mentioned block inheritance of the values for (iv) and (aux). However, default unification allows us to do more. An auxiliary does not subcategorize for an ordinary NP subject, nor does it subcategorize for a complement VP that subcategorizes for an ordinary NP subject. Rather, the restrictions to be placed on the nform of the subject are inherited from the embedded VP: Example 15 a. it will annoy Kim that she lost b. *Sue will annoy Kim that she lost This dependency between elements of the subcat list is encoded</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart (1986a). An Introduction to Unification-Based Approaches to Grammar. CSLI Lecture Notes 4. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>A simple reconstruction of GPSG.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, COLING 1986.</booktitle>
<pages>211--215</pages>
<location>Bonn, Germany,</location>
<contexts>
<context position="1266" citStr="Shieber 1986" startWordPosition="169" endWordPosition="170"> formal semantics of the operation and demonstrate how some of the phenomena used to motivate nonmonotonic extensions of unification-based formalisms can be handled. 1. Introduction While monotonicity is often desirable from a formal and computational perspective, it is at odds with a considerable body of linguistic work. Default principles, default rules, and default feature-values can be found in many linguistic formalisms and are used prominently in work on phonology, morphology, and syntax. In spite of their great expressive power and flexibility, unification-based grammar formalisms (see Shieber 1986a, for an introduction) are in general not very successful in modeling such devices. Unification is an information-combining, monotonic, operation on feature structures, whereas the implementation of default devices typically requires some form of nonmonotonicity. In this paper, we present a nonmonotonic operation on feature structures, which enables us to implement the effects of a number of default devices used in linguistics. As the operation is defined in terms of feature structures only, an important characteristic of unification-based formalisms, namely that linguistic knowledge is encod</context>
<context position="4454" citStr="Shieber (1986" startWordPosition="655" endWordPosition="656">rbs as +INV, which leads to the rather counterintuitive situation that regular verbs need to be marked extra, whereas the exceptional ones can remain underspecified. A more natural solution would be to assign all verbs the specification -INV by default (either by means of template inheritance or by means of lexical feature specification defaults as used in Generalized Phrase Structure Grammar [GPSG; Gazdar et al. 19851) and to overwrite or block this specification in the exceptional cases. The possibility of incorporating an overwrite operation in a unification-based formalism is mentioned in Shieber (1986a, p. 60). • Feature Percolation Principles. Both GPSG and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag 1987) adopt the so-called Head Feature Convention (HFC). In GPSG, the HFC is a default principle: head features will normally have identical values on mother and head, but specific rules may assign incompatible values to specific head features. In unification-based formalisms, it is impossible to express this principle directly. Adding the constraint (X0 head) = (X, head) to every rule of the form X0 Xi • .. Xn (with X, (1 &lt;i &lt; n) the head of the rule and assuming all head fea</context>
<context position="6027" citStr="Shieber 1986" startWordPosition="905" endWordPosition="906">l head features f&apos; f. • Structuring the Lexicon. Flickinger, Pollard, and Wasow (1985), Flickinger (1987), De Smedt (1990), Daelemans (1988), and others, have argued that the encoding and maintenance of the detailed lexical descriptions typical for lexicalist grammar formalisms benefits greatly from the use of (nonmonotonic) inheritance. In Flickinger, Pollard, and Wasow (1985), for instance, lexical information is organized in the form of frames, which are comparable to the templates (i.e., feature structures that may be used as part of the definition of other feature structures) of PATR-II (Shieber 1986a). A frame or specific lexical entry may inherit from more general frames. Frames can be used to encode information economically and, perhaps more importantly, as a means to express linguistic generalizations. For instance, all properties typical of verbs are defined in the VERB-frame, and properties typical of auxiliaries are defined in the AUX-frame. The AUX-frame may inherit from the VERB-frame, thus capturing the fact that an auxiliary is a kind of verb. 184 Gosse Bouma Feature Structures and Nonmonotonicity In this approach, a mechanism that allows inheritance of information by default (</context>
<context position="7594" citStr="Shieber 1986" startWordPosition="1145" endWordPosition="1146">ss in the form of a frame, without ruling out the possibility that exceptions might exist. In unification-based formalisms, templates can play the role of frames, but as unification is used to implement inheritance, nonmonotonic inheritance is impossible. • Inflectional Morphology. In PATR-II the lexicon is a list of inflected word forms associated with feature structures. The only tools available for capturing lexical generalizations are templates (see above) and lexical rules. Lexical rules may transform the feature structure of a lexical entry. An example is the rule for agentless passive (Shieber 1986a, p. 62), which transforms the feature structure for transitive past participles into a feature structure for participles occurring in agentless passive constructions. Lexical rules can only change the feature structure of a lexical entry, not its word form, and thus, the scope of these rules is rather restricted. While the examples in Flickinger, Pollard, and Wasow (1985) and Evans and Gazdar (1989a,b) suggest that the latter restriction can be easily removed, it is not so obvious how a unification-based grammar formalism can cope with the combination of rules and exceptions typical for (inf</context>
<context position="10546" citStr="Shieber (1986" startWordPosition="1601" endWordPosition="1602">a in mind and also are rather intimately connected to peculiarities of the GPSG-formalism. What 185 Computational Linguistics Volume 18, Number 2 is particularly striking is the fact that two different conceptions of default appear to play a role: a head feature is exempt from the HFC only if this would otherwise lead to an inconsistency, whereas a feature is exempt from having the value specified in some feature specification default (among others) if this feature covaries with another feature. Overwrite and add conservatively are also highly restricted operations. From the examples given in Shieber (1986a) it seems as if overwriting can only be used to add or substitute (nonmonotonically) one atomic feature value in a given (possibly complex) feature structure (which acts as default). Add conservatively, on the other hand, is only used to add one reentrancy (as far as possible) to a given feature structure (which acts as nondefault). An additional restriction is that add conservatively is well behaved only for the kind of feature structures used in GPSG (that is, feature structures in which limited use is made of covariation or reentrancy). Consider for instance the example in (1).1 Adding th</context>
<context position="16191" citStr="Shieber 1986" startWordPosition="2531" endWordPosition="2532">nd thus it seems attractive to consider an operation that subsumes the effects of the proposals so far. Default Unification, as defined below, is an attempt to provide such an operation. 187 Computational Linguistics Volume 18, Number 2 3. Feature Structures and Unification Feature structures are often depicted as matrices of attribute-value pairs where values are either atoms or feature structures themselves and, furthermore, values may be shared by different attributes in the feature structure. Feature structures can be defined using a description language, such as the one found in PATR-II (Shieber 1986a) or in Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b. Example 4 a. ( (f) = a (g f) = a (g f) = (g g) ) - f : a b. [ g :7 a ] Following the approach of Kasper and Rounds (1986; 1990), and others, we represent feature structures formally as finite (acyclic) automata (the definition below is taken from Dawar and Vijay-Shanker 1990): Definition A finite acyclic automaton A is a 7-tuple (Q, E,F, 6, qo, F, A) where: 1. Q is a nonempty finite set of states, 2. E is a countable set (the alphabet), 3. F is a countable set (the output alphabet), 4. .5 : Q x E Q is a finite pa</context>
<context position="20363" citStr="Shieber (1986" startWordPosition="3313" endWordPosition="3314">ucture A and the nondefault feature structure B. The operation has the following characteristics: 1. It has a declarative semantics and is procedurally neutral. That is, if A = A&apos; and B = B&apos;, then (AU!B) = (A&apos;U!B&apos;). 2. It is monotonic only with respect to the nondefault argument. That is, B (ALA) is always true, but in general A E (ALJ!B) will not hold. 3. It never fails. If A is fully incompatible with B, (AU!B) = B. 4. It gives a unique result. 5. Reentrancies in the nondefault argument may be replaced by a weaker set of reentrancies if necessary (this is the add conservatively operation of Shieber (1986b)). Intuitions about default unification appear to be more clear in those cases where feature structures do not contain any reentrancies. Therefore, we will first define default unification for this case, moving to the general case in Section 4.2. Section 4.3. deals with the incorporation of add conservatively. 4.1 Default Unification without Reentrancies Subsumption suggests a straightforward definition of an operation that has properties 1-4 above. Definition Default Unification (first version) AU!B = A&apos; U B, where A&apos; is the maximal (i.e. most specific) element in the subsumption ordering s</context>
<context position="37830" citStr="Shieber 1986" startWordPosition="6578" endWordPosition="6579">mpt is made to encode some of the peculiarities of the English auxiliary system in a lexicalist grammar: Example 14 NP : ( (cat) = n (nform) = norm )• VERB : ( (cat) = v (aux) = — (iv) = — VP : ( VERB (subcat first) = NP (subcat rest) = empty AUX : ( VERB !(aux) = !(inv) = (subcat first) = VP (subcat rest first) = NP (subcat rest rest) = empty !(subcat first subcat first nform) = (subcat rest first nform) )• 197 Computational Linguistics Volume 18, Number 2 Adding the equations ! (aux) : + and !(inv) : +5 to the definition of AUX has an effect comparable to that of the overwrite-operation of (Shieber 1986a, p. 60). The AUX template inherits from VERB by default, but the equations just mentioned block inheritance of the values for (iv) and (aux). However, default unification allows us to do more. An auxiliary does not subcategorize for an ordinary NP subject, nor does it subcategorize for a complement VP that subcategorizes for an ordinary NP subject. Rather, the restrictions to be placed on the nform of the subject are inherited from the embedded VP: Example 15 a. it will annoy Kim that she lost b. *Sue will annoy Kim that she lost This dependency between elements of the subcat list is encoded</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart (1986b). &amp;quot;A simple reconstruction of GPSG.&amp;quot; In Proceedings, COLING 1986. Bonn, Germany, 211-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenraad De Smedt</author>
</authors>
<title>Incremental Sentence Generation. Doctoral dissertation,</title>
<date>1990</date>
<institution>Katholieke Universiteit Nijmegen,</institution>
<location>Nijmegen, The Netherlands.</location>
<marker>De Smedt, 1990</marker>
<rawString>De Smedt, Koenraad (1990). Incremental Sentence Generation. Doctoral dissertation, Katholieke Universiteit Nijmegen, Nijmegen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Touretzky</author>
</authors>
<title>The Mathematics of Inheritance Systems.</title>
<date>1986</date>
<publisher>Morgan Kaufmann.</publisher>
<location>Los Altos, CA:</location>
<contexts>
<context position="40385" citStr="Touretzky (1986)" startWordPosition="7022" endWordPosition="7023">ature Structures and Nonmonotonicity template (which is a feature structure). How this template was defined (as a set of equations or as a combination of (more general) templates, as a combination of default and nondefault information or not) is completely irrelevant to its meaning. Thus, the denotation of AUX would remain as before, if we defined it as: Example 18 AUX: ( (cat) = v (aux) = + (mo) = + ). Consequently, the denotation of might is not affected by this change in definition either. The role of classes (or frames) in inheritance-based systems, however, as described in, for instance, Touretzky (1986), is rather different. To determine the denotation of a class might that inherits from a class AUX, we not only need to know the contents of AUX, but also the classes from which AUX inherits. The latter is important for resolving multiple-inheritance conflicts. If the class might inherits from both AUX and VERB, for instance, and AUX in its turn inherits from VERB as well, information inherited from AUX must take precedence over information from VERB, as the former is more specific than the latter. In our nonmonotonic inheritance mechanism for templates, such reasoning is impossible. Adding th</context>
<context position="41681" citStr="Touretzky (1986)" startWordPosition="7244" endWordPosition="7245">ical entry) might would lead to a unification failure of the default information, and thus the definition as a whole would be considered as illega1.6 This is as it should be, we believe, given the fact that the inheritance hierarchy as such should not play a role in determining the meaning of templates. The denotation of the template AUX is the feature structure in 16 (i.e., whether it is defined as in 14 or as in 18 is irrelevant), and from that it is impossible to conclude that AUX inherits from VERB, and thus the kind of reasoning used to justify the resolution of feature conflicts used in Touretzky (1986) is not applicable in our case. 5.2 Lexical Defaults The definition of auxiliaries above is still unsatisfactory in that it predicts that auxiliaries subcategorize for verbal complements that are specified as (aux) = —. Clearly, this requirement is too strong (although it is correct for the auxiliary do). One way to solve this problem is to redefine the AUX-template as: Example 19 AUX : ( (subcat first cat) = v (subcat first subcat first) = NP (subcat first subcat rest) = empty 6 Of course, it is possible to combine incompatible default information if we impose the correct ordering explicitly.</context>
</contexts>
<marker>Touretzky, 1986</marker>
<rawString>Touretzky, David (1986). The Mathematics of Inheritance Systems. Los Altos, CA: Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>