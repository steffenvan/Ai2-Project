<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.6762795">
American Journal of Computational Linguistics Micrbfiche 81
THE FINITE STRING
</title>
<author confidence="0.648258">
VOLUME 15 NUMBER 5
</author>
<affiliation confidence="0.336186">
Releasedfor publacation March 24, 21970
</affiliation>
<bodyText confidence="0.8917722">
With this issue, David G. Hays
completes his term as Editor 0-f
AJCL and breatftes a sigh of re-
lief. Personal matter g have made
the last two issues of AJCL for
1978 excessively late. The next
issues of AJCL will appear on pa-
per; but the circumstances of the
momept Auggest that digital mag-
netic recording and direct wire
transmission will be suitable
for experimental use shortly.
AMERICAN JOURNAL OF COMPUTATIONAL
LINGUISTICS is published by the
Association for Computational
</bodyText>
<keyword confidence="0.526107272727273">
Linguistics
EDITOR, 1974-71978: David G. Hays
EDITORIAL ASSISTANT, 19-77-1978:
William L. Benzon
MANAGING EDITOR; 1977-:
Donald E. Walker, Artificial
Intelligence Center, SRI Interna-
tional, Menlo Park, CA 94025
TECHNICAL ADVISOR, 1975-:
Martin Kay, Xerox Palo Alto Re-
search Center
</keyword>
<table confidence="0.805020538461538">
Copyright C) 1979
Association for Computational Lingui$tics
2
CONTENTS
COMPUTATIONAL LINGUISTICS IN THE USSR: July 1978
Joyce Friedman . . . . • • • • • • • • 3
Astociation for Literary and Linguistic Computing BULLETIN
Volgme 6, Number 2, 1978 . . . . . . . 14
Volume 6, Number 3, 1978 . . . . . . , . - . . • . • 15
PROPERTIES OF LEXICAL RELATIONS
Yartha W. Evens and Raoul N. Smith . . . . 16
MODELS, OF THE SEMANTIC STRUCTURE OF DICTIONARIES
Kenneth C. Litkowski . . • • • • • • . 25
</table>
<sectionHeader confidence="0.638192" genericHeader="abstract">
AFIPS WASHINGTON REPORT
</sectionHeader>
<author confidence="0.494820666666667">
FebrAary 1979 • • • • • • . . 75
March 1979 . . . . . . 83
April 1979 . . 6 • 4 • • . 91
</author>
<affiliation confidence="0.181989">
3
</affiliation>
<note confidence="0.643967">
N -14 Department oE Computer ald
Computer Studies in Communicition Scrienc-!s
Formal Linguistics TUE UNIVERSIrY OF MICHICAN
July 1978 Ann Arbor, Michigan 48139
COMPUTATIONAL LINGOISTTCS IN THE USSR
Joyce Friedman
</note>
<sectionHeader confidence="0.654117" genericHeader="keywords">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.992482166666667">
As part of an official U.S (USSR
Senze &apos;xchance on Arplicatians of CImputers
in Manaoement, a subgroup on natural langaa.ge
processing visited the Soviet Union from May
29 throug&amp;quot;1 June 11, 1978. The graup met with
scientists in Moscow, Novosibirsk, Leningrad,
and Fiev. There were formal mee,tings and
presentations of technical material, Ind .lso
miny inforndl discussions. This report
presuts a view of Soviet cpmputatiinal
liPguistis which emerged fnom these
1.is-mssiofts.
</bodyText>
<note confidence="0.853111">
4
Bacground
Thp,11.S.PISSP Science Exchange on Applications of Cooputers
</note>
<tableCaption confidence="0.928766">
to Kanatieniert incluies many startasks. The exchange in natural
language-procesini is one task under ths topic &amp;quot;theoretical
foundations for software in applications in* economics and
manaaelaent&amp;quot;. The exchange in natural language processing was to
hive begun in June 1977. Howevei a schelulei tri,p by U.S.
gripntists war; cancelled at the fast minutP by the USSR side; tne
roasnn FlivPn was that there were no hotel rooms available in
woscow. rn spite of this initial disappointment
hogar ir Nnvemher 1977 when threc. SoviPt sgientists
United stAtes ft two weeks. The visitors
NarAnevani o: the Academy of Sciencet, Tomputing
Novnsih,i rst
</tableCaption>
<figure confidence="0.5505238">
the exchanTe
tne
Alexander
ntPr in
ani Victor Briabrin and Dmitri Pospelov of tne
</figure>
<tableCaption confidence="0.4452536">
1.C11PWV of riences Computing Center in Moscow. The trfp
rpnorted in this roe in tte rescheduled visit by the U.S.
riPlc.gation. It took place May 28 to June 11„. 1978.
Th m-.1thors of th-, U.S. dciegati.or were: Donald AufPnkamn,
&amp;quot;.3.F., U.S. Chnirman of the U.S./USSR Joint Working Group 34
</tableCaption>
<note confidence="0.789370583333333">
Scientific ani Techni al Cooperation in tha Application of
Computerr to lanagemnnt; Sue Boner, R.E.W.; Joyce Friedmaa,
nppartlent of Computer ani Communication Sciences, The University
of Michigan; iot,n Malhoul. Bolt Beranek and Newman, Inc.,
Cambridge; Stanley Petrir:k, Mathematics Depkrtment, 13F
T. 1. Watson Research Center, YorPtown Heights: Silly Sedelow,
DonartmPnts of Linguistils and Computer Science, University of
Kansas; ant Walter A. Sedelow, Departments af Soliology and
5
Computer ScienCe, U“versity of Kansas. The U.3. Ielegation was
accompaniel throughent the trip by R. S. Nartniyani 3f
Movosibirsk.
</note>
<subsectionHeader confidence="0.594407">
This report gr3ups toprther similar work 43ne in differelt
</subsectionHeader>
<bodyText confidence="0.804192461538462">
locations thI main patterns of the natura Language processiag
and thearem-provini ”stoms can be viewed ts based on (I)
linguistics, (2) artificial intelligence, or (3) logix, althor3h
tha distinctions ar to somP extent arbitrary. We also give in
overviPw oc the computers and programming languages avai/able f3r
W3r computational linguistics. Work on lexicography,
thesauri, and speech reF:ogni.tion was also discusped on the visit,
h&amp;quot;t is not covetel in this report.
111. iinanistilallizbased Work on Natural Lanaudae
The main roots of thft linguisti.cally-based work are the
meaning-text modP1 of Pellichuk, dependenr.y grammar, aid
transformationatt grammar. They are vari3u ly interpreted by
different systems.
</bodyText>
<subsectionHeader confidence="0.897438">
Zoya Shalyapira, Laboratory ot Machine Trtsitslatio3,-
</subsectionHeader>
<bodyText confidence="0.979050285714286">
Tnstitute voreman Languages, desctibed an Rnglish to Russtin
machine translation symtem under development since 1972 and based
primarily on the meaning-text motiel. The representation is a
*ependency tree, with wore order information, mlrphology aid
semantic/sfntactic valencies. This structure preserv.,es all tie
%urftce data but is also close tt a semantic representation 3f
the text. rher? 15 a dictionary and a grammar for each lanquacca.
</bodyText>
<page confidence="0.485667">
6
</page>
<bodyText confidence="0.932333428571429">
The grammar ruler, alre of the two forms: If &lt;structure&gt; than.
&lt;c,ndition&gt;, aria ie &lt;stuctur&gt; then &lt;transformation&gt;. Semantic
information iaclules senantic descripttons of lexical ad
marphological unfits and the semantic acceptability of word pairs.
There is a dictionary of 10,100 lexf=imes, described in terms of 10
semantic primitives The syntactic and semantic gtructures are
compatihie, so analysis goes onry aa deep as is nacessary for a
cloivw1 spn+3nce. shliyapinals group works on Ilnauistid aspecrs
Only; ther, is ao corputer implementation.
Uri Alresyan also works with the meAnipg-text model and with.
allci-ine translation as the.goal. His work is nri-marily on French
to Rus;ian translations. but he also works on Engli.sh. His
onglish grammar is sail to b the most completo evrr publishel:
th Russian grammar will 300n appear. Th&apos;. limluistic motel will
have folnr parts: morphology, deep syntax, surface syntax, aid
serirntics; however, the current reduced mo1,1 licks semantics. A
lirtionary givas for each ord its morphology, its syntactic an4
semanti6 f,a*ur,s (there are 150-syntattic features; 500 semantic.
features), the semantic criteria for possible govprninl words,
ani selectilnal Ke.strirtions. Rule scHema or &amp;quot;syntigmas&amp;quot; go fram
morpheme structure to a surface syntactic structure that is An
unortered&apos;dapendency tree. TheTP arc about 2)3 syntagulas far
Russian, each representing 2-0 rules. A syntagma allows a trae
with X over Y to be consttucted from a string containing X and Y
uniar various complex, conditions. The lexical informafion ad
thl syntagmas datPrmine the transformation from wort strihig to
surface-syntactic structure. A demp structure is then definei ay
&amp;quot;paraphrastic&amp;quot; rulas. which Convert. for examnle, strike to
</bodyText>
<page confidence="0.94038">
7
</page>
<bodyText confidence="0.954081">
deliver when the object is 1 blow. The deep structure is no
lonler languale-specitic but is universal, and serves as the
basis for transration hetween languages. Imresyan stresed tle
valde of continuing to work on the same linguistic model in order
t6 complete its development; he contrasted thin with the attltgle
of sofne current American linguists.
Th P Iakalev, of the Economics Institute is
developinl a natural languaie interface for a data base sypten.
Ts worl, has computPL sunpott and is +,1 he runnina soon ina
large factory. The natural language suLset has sentences such as
&amp;quot;w-t Is thn number of wqrkers of &lt;Type&gt; in &lt;pl&apos;ac.&amp;quot; and is
said to tv, easy fNot. economisf.s to learn. The system is based 3n
vpry recent modelsat transformatxonal grammar: Iakalev mentioned
&amp;quot;traces&amp;quot; all some of Jackendlff&apos;s theories. ¶h system Ions tram
input to d deep structure from- which it constructs a formula far
the computation of .3&gt; numerical result.
</bodyText>
<subsectionHeader confidence="0.983573">
/71 Artiflcial tnt.elligence Work in Natural Ita auale_
</subsectionHeader>
<bodyText confidence="0.6461082">
AT-baled systems are being developed at the Computing Center
of. the Acals&apos;.my of Sciences at Moscow, under tt,a direction 3f
Briabrin and at the Computing renter uf the Siberimn
Division af the Academf of Sciencez, Navasibirsk under tie
direction ac Ilmmanie-r gariniyani, in Ersholv,s.group.
</bodyText>
<subsectionHeader confidence="0.7554435">
The system demonstrated to us in Moscaw was DrLos (Dial-)gie
InformAtion Logical System). This work is heavily influehced ay
</subsectionHeader>
<bodyText confidence="0.701153">
artificial intelligence wok in the U.S. fOrilabrin&apos;spent seven
</bodyText>
<page confidence="0.424766">
8
</page>
<bodyText confidence="0.989530785714286">
months at LI.?. working with William Martin and with Carl
Hewitt.) DILOS is written in LISP and runs on ths BE5m-6 compnt-mr
in Mdscow, as well as on a PDP-11/45 at the Internatianil
Tnstitute fot Applied Systems Analysis in Laxenburg, Austria.
Th s system is intended both to test various approaches to natural
language processing and for practical applications. It =onta.ims
an ATN linquistic processor and a semantic processor based an
frames. The current applications area is attune tiakat
reservations; the demoAstration was however on 2 very small data
basr. of AI,Natmral Language Systems (including DUOS, 1US, EFL,
OWL, and larINAP). The system was able to answer simple naturil
language questions from the data nase but it was not possible
from the demonstration to get a good feelirg for the actual range
&apos;of lanbuage
</bodyText>
<subsectionHeader confidence="0.992328">
Marin&apos;yani&apos;s group in Novosibirsk has 17 psopla, in.-Adding 6
</subsectionHeader>
<bodyText confidence="0.945961846153846">
linguists and 9 mathematiciamis and programmers. Until a fsw
years ago, the work followed Mellehukts modal. This has now hean
abandoned here arid work proceeds along four lines, so far
relativelv inlebeniantly: (1) Narin&apos;yani is dewelping a formll
linguistic model which combines dependancy and constituent
structur-e in a mixed multi-level representation. Analysis
proceeds by local modification of the graph structures, expandimg
and compr2ssing T.ase frames at var ousil2vels. The linguistic
model so far in=ludes formal description of adverb groups aid
adiective groups. This formal model has now bean written up, but
so far is not implemented. (2) The semantic question-answevilg
system VOST3K-0 contains a formal model of time. Om the basis lf
tsxts of senten^es such &amp;s &apos;From the 3rd up to the 13th of Marlh
</bodyText>
<page confidence="0.308403">
9
</page>
<bodyText confidence="0.967487705882353">
Mike was in Moscow&amp;quot; it answers questions4like Where was Mike it
noon on the 17th ne March?&amp;quot;. The system is coq i&apos; SETL And was
demonstrated tu us. While the natural language fragment is still
small, even For a model of time, (e.g. nd time adverbials), tine
inferencing schame workAA sucCessfully. (3) Severil
&amp;quot;applicatianal&amp;quot; systems are being deve] ped. The first of tkeqa,
the PL-1 &amp;quot;mini&amp;quot; or &amp;quot;toy&amp;quot; system ZAPSIB-TO use essentially ho
syntactic analyvis (thlugh it relies heaVily on word order). It
has. a well-lefined subject domain, a data base of oersonnal
information, and answers questions such as &amp;quot;who ander 30 earls
more than average?&amp;quot; (Salary information is public in the USSE.k
In this vary limited sublect domai.n, th anoroazh works well,
The &amp;quot;midi&amp;quot; applicational systram is undar dmvelopment and is more
syntactically orieatad. it will cqntain a nondeterministic
bottom-up parsAr .Far a bieary context\-sensitive gramnar with
discontinuous -.IonstituAts. (4) The final subjrotip is the
programmin, language group; it has implemented 3ETL on the BES-
</bodyText>
<sectionHeader confidence="0.367715" genericHeader="introduction">
6.
</sectionHeader>
<subsectionHeader confidence="0.618973">
1.31_ Logica,based work in Natural laquagei
</subsectionHeader>
<bodyText confidence="0.98418125">
In Moscow, at VTNITI, the linguist E. B. Paducheva and the
mathematician T. 0. ttoLelskay± are developing jointly an aoproa:h
to natural language praaessinr basei on both trinsforlati,ril
grammar all first-order logic. The cnrrent Jomain is converse
theorems in geometty. The system is able to przzess- leometcy
theorems and produce their &amp;quot;convdrse theorems&amp;quot;. In this system
the semantic representation language is first-order
Algorithmi3 procedures fnr analysis and synthesis have been
</bodyText>
<page confidence="0.590696">
10
</page>
<bodyText confidence="0.984936923076923">
dev.eloped, as well as processing procedures within the logic.
The linguistic part of the metho&amp; is based on. transformational
grammar.- As is tha case with most of the Soyiet work 3n
transtemmational grammar, the deep structure uses dependency
grammar rrther thin constituent structure grammar.
transformations are originally written in the Forward directiol,
i.e. from deep to surface structure. Analysis is dome using a
&amp;quot;reversed&amp;quot; version of each transformation (not obtained
autbmattcally). While the forwari transformations are
independeni- of orier the reversal rules are strictly ordered,
for efficiency. There are 30-3 transformetions, each ezpressed
as a structural description, giver as a template, aril a
structural change, given as a sequence of elamentary operations.
</bodyText>
<subsectionHeader confidence="0.781461">
a
</subsectionHeader>
<bodyText confidence="0.968101666666667">
The work Jb developel in detail, but has no computer
implementation. Tne system is sAid to contain interestitg
solutions -tfl problems of quantification, negation, aad
coniumction re.luction. Tne authors reportel, wits SO/1e
amusement, that the description of the work was printed in 42,0,0
copies.
The current work at the University of Leningrad under 3
Tseitin, Faculty of Engineering and 4athematics, was described to
us by others as basea on loqic, but Tseitin himself took a
philosophical approach in his discussions wita us. His remarcs
were more- suggestive that i descriptive. He indicated that his
approach to natural language was bf analogy to programming
languages, usinl ma3ros as in operating systems. Ha clailmed
&amp;quot;that there is no such thing as meaning&amp;quot;, but sail that his
approach did usa procedural semantics. His previous *work on
</bodyText>
<page confidence="0.934086">
11
</page>
<tableCaption confidence="0.849408888888889">
complexity and theorem-proving Ls not related to his work 321
natnral languagy. Rowever, heid 0 argue &apos;that a natural language
system for computmrs should reflect the fact that natural
langua/e pmrformanam Ylr people does not require exp3nential timm.
TseittnIs own current wlrk is no* on natural lanauage, as he is
busy vriting an ALGOL68 implementation.
Tscitin and Liakina, formerly of the Far.dity of Philologl,
also talkei about smveral earlier natural lanauage systeas whiTh
I am unable 4-o disttnguish. They dre described in a number 3f
</tableCaption>
<bodyText confidence="0.857213722222222">
nublLcations. from 166 on. In gererai they rplay dependen-y
grammar, ani use transformations durina synta.sitic analysie.
Restriction., on the grammar are stated in the pcedi.7ate calculus
and resolution the&apos;orem-proving is used. J&apos;he goal is English to
Rus-sian translation of scionti..tic prose.
The system of J. Kapitolova, Head of the Laboratory 3f
Applied Cybeinetics at. the Institute of CyberReti=s at Kiev, is
an interactive thp-mem-Droving system fbr mathematical text..
The objertive is to be able to fill in the standard aaps in
proofs., as Ltdicatm3 by &amp;quot;Lt is obvious tftat&amp;quot; or &amp;quot;as in the pro)f
of the orevious Theorem&amp;quot;. The&apos; text Ls fitst processed manually
into a hignly stylized mathematical language. Only the formal
material, theocmms and proofs, is analyzed: disnission Ls treatmd
as comment and is ianored bj the programs- Several larae texts,
including Curtis and Reiner Alaebrair Theory. of 3roups, have bean
preproressmi. The theorem-provet is vtailored to the specific
mathematical omain. It uses resolutLon theorem-proving,
heuristi.c techniques, as well as specLal mathematical and logi.311
</bodyText>
<page confidence="0.672196">
12
</page>
<bodyText confidence="0.954358833333333">
technigues. Th.? system has been programmed and i3 about to oe
tried out on a recent thesis.. This proje.-:t is of ten years
duration, Ind has had a minimum or ii peoOle.
Tnterest in Montagne grammar, was considerable.My talk in
Moscow wits very well attendPd, and there were many gond
gnestions.rhe audience was generallf familiar with Mpntague&apos;s
w.oric an3 with recent papers the topic in Artikicial
Intelligen7.3 ani Theoretical Linguistics. The interest seeme3 to
come fnom morej g neral interest in lngic as a knowledge
reprPsentatinn natoral language systeits. Agafanov in
Novosibirsk is also inttreested in the possible applications nf
Montague gramIrte-- to programming languages.
</bodyText>
<sectionHeader confidence="0.371948" genericHeader="method">
CQ1JflflJ aCom utItt
</sectionHeader>
<bodyText confidence="0.98294125">
Computer access appearz: Lc be MUCP more difficult to obtain
for computational linguisfs in the Soviet Union.Many of tie
pepleats 1-tad no computer support, even though they were in areas
where compater testing of grammars or theories could be very
useFul. Most of the computing was on the second-generations
computer BESM-6, altl,ough thefe are more recent computers, e.g.,
the E3-E131 (Pyad), series, available. for other purposes. 0.3.
computers were op order from Hewlett-Packard: CDC, and
9urroughs.The terminals ya saw were mainly grapnics terminals
from Eastern Europe, with both Boman and Cyrillic character set-4
and seemed fine in use.
There is much interest in advanced programming languages.
</bodyText>
<page confidence="0.80286">
13
</page>
<bodyText confidence="0.949488666666667">
SETL is imp1emente3 in Novosibirsk. (This is with the aid of the
U.S./USSR Science exchange. l In Moscow, PASCAL is impiementai.
In LeningraJ, Tseit,in is impleimenting ALq01.68 tor the Ryad seris
of computers, compatible with the IBM 36q.
We diJ have oq=asion ta see some interaa.tiVa systems in
operation. Th a lAnguages were impressive, Out the programmar
support was not. Theire seamed to be few errOr liagnostics. Whan
th,re were .7rashes it was not possible to tell whic:h were due to
th computer and which: to the programs.
</bodyText>
<subsectionHeader confidence="0.846658">
7onclusion&apos;s
</subsectionHeader>
<bodyText confidence="0.989857071428571">
Work onnatural languaje processinl in tha USS!&apos; seems to be
along thre?,.! ofailr lines. The work by linguists is motivated by
machine translation. Tt relies on versions of Mel&apos;Chuk&apos;s
meaning-text model, with some type of transformations on a
aoendency base. It is charaCterized by 1 great deal _of
sophisticated devnlopment of large grammars, by large groups of
linguist, but is without- computer support. The artifiill
intelligence work is diracted toward data bas .c information
systems, is at in ea-tiler state of development, and is heavily
bascad on U.S. work. It is caaried out in :&amp;quot;.&apos;omputinl Centers aae
has good programming ani computer support. Th a logic-based work
in carried out by individuals or small groups in several
locations without f7omputeF support, and by one large group with
computers.
</bodyText>
<figure confidence="0.85297897368421">
CONTENTS 14
MACHINE-TRANSLATION OF CHINESE MATHEMATICAL
ARTICLES S Joh, L. Kong, and H.-$. Hung 111
THE CHINESE UNIVERSITY LANGUAGE TRANSLATOR P.H. Nancarrow 121
(CULT) - A REPORT
AUTOMATIC SCANSION OF SANSKRIT POETRY FOR
AUTHORSHIP CRITERIA D. Wujastyk 122
THE MIZAR-QC/6000 LOGIC INFORMATION LANGUAGE A. Trybulec 136
THE DISCOVERY OF SYNTAGMATIC AND PARADIGMATIC
CLASSES •. J.G.Wolff 141
REPORT ON A COURSE ON THE USE OF COMPUTERS IN
TEXTUAL ANALYSIS AND BIBLIOGRAPHY HELD IN THE
COMPUTER UNIT OF THE UNIVERSITY COLLEGE OF WALES
ABERYSTWYTH, 10-14 APRIL 1978 P. Sims-Williams 159
COMPARABLE COMPUTER LANGUAGES FOR LINGUISTIC AND
LITERARY DATA PROCESSING: PERFORMANCE M. Boot 161
A PARTIAL-PARSING ALGORITHM .FOR NATURAL LAN6U\GE
TEXT USING A SIMPLE GRAMMAR FOR ARCU`1ENTS ,P13. Sallis 170
WANTED — COMPUTER READABLE DICTIONARIES T.D. Crawford 177
COLLOQUIUM ON THE USE OF COMPUTERS IN\ TEXTUAL
CRITICISM: A REPORT S.M. Hockey 180
Secretary&apos;s Notes 182
Diary A 183
Advertisement (Data Bases Conference) 186
Advertisement (ICGH/4) 187
Advertisement (ALLC International Meeting, 1978) 188
Notes an Contributors 189
Editors&apos; Notes 190
News and Notes 192
Book Reviews 196
Documents Received 199
Advertisemen&apos;t .... 201
Letter to the Editor 202
Bibliography 204
ALLC Bunptin Voluille 6
/5
CONTENTS
GUEST EDITORIAL: LOOKING BACK F. de Tollfnaere 221
</figure>
<note confidence="0.5520195">
REPORTS OF MLOQUIA IN TUBINGEN D. Kottke 224
THE WORDS OCCURRING IN ENGLISH IDIOMS D.J. Wright 228
</note>
<sectionHeader confidence="0.809156" genericHeader="method">
PROPOSED CRITERIA FOR PUBLISHING
STATISTICAL RESULTS D. Ross and B. Brainerd 233
</sectionHeader>
<subsectionHeader confidence="0.394741">
ON THE TEACgING OF RUSSIAN NUMERALS
</subsectionHeader>
<bodyText confidence="0.368575">
BY USING AN ONLINE COMPUTER K1 AhMad, M. Colenso, and G. Corbett 235
</bodyText>
<sectionHeader confidence="0.815777111111111" genericHeader="method">
THE VALUE OF THE COMPUTER IN EDITING
AN &apos;OPEN TRADITION&apos; TEXT J.R.C.Martyn Z42
PROBABILITIES OF OCCURRENCE OF CHARACTERS,
CHARACTER-PAIRS, AND CHARACTER-TRIPLETS
IN ENGLISH TEXT R. Shinghal and G.T. Toussaint 245
SENTENCE LENGTH DISTRIBUTIONS IN
GREEK HEXAMETERS AND HOMER S. Michaelson, A.Q. Morton, and W.C. Wate 254
SNOBOL: THE LANGUAGE FOR LITERARY COMPUTING L.D. Burnard 268
GENERATING AND TRANSFORMING BY A COMPUTER
</sectionHeader>
<figure confidence="0.90130905">
WITHOUT A DICTIONARY U. Oman 280
THE OLD ICELANDIC IELUCIDARIcS&apos;:
A DIPLOMATIC EDITION WITH THE
HELP OF THE COMPUTER E.S. Firchow, K. (rim. &amp; ind S. Gilmour 292
Advertisements 4 302
Secrdtary&apos;s Notes 304
Editor&apos;s Notes 304
Advertisement 305
Diary 306
Notes on Contributors 308
News and Notes 309
Book Review. 318
Documents Received 320
Bibliography 322
ALLC Bulletin Volume 6
/6
Evens &amp; Smith 87
Lexicon for Q-A Spstem See Microfiche 83
Appendix II. Properties of Lexical Relations.
a. Reflexivity, Symmetry, Transitivity.
</figure>
<bodyText confidence="0.992985375">
Certain properties of lexical-semaptic relations can be very use-
ful in deductive inference. For instance, iT we know that a cheetah
is a kina or mammal ana a mammal is a kind of vertebrate then we can
deduce that a cheetah is a kind of vertebrate. Writing T for the taxonomy
relation, we can abbreviate this sentence: if cheetah T mammal and mam-
mal T vertebrate then cheetah T vertebrate. Whenever bTc and cTd, it
follaws that bTd. This fact can be described much more efficiently by
the statement that the taxonomy relation is transitive. Two other commonly
mentioned properties of relations are reflexivity and symmetry. These
properties may Apply to predicates formed from lexical entries as well
as to lexical-semantic relations.
To be precise, a relation R defined on a set S is said to be a
transitive relation if whenever b and c are R-related and also c and d
are R related then b and d stahd in a relation R also. Synonymy is a
transitive relation just as transitivity is. The preposition in behaves
in the same way. If Sam is in the kitchen and the kitchen is in the
hotel, then we know that Sam is in the hotel. The time interrelation
before behaves like this, too. If Zorro arrived before the posse did
and the posse arrived before the explosion, then we know that Zorro
arrived before the explosion.
A relation R defined on a spt S is said to have the reflexive pro-
perty if all the elements of S are R-related to themselves, that is, if
mRm is true for all members in of the set S. The synonymy relation has
this property: a word means the same thing as itself. The antonymy
</bodyText>
<subsectionHeader confidence="0.133619">
17
</subsectionHeader>
<bodyText confidence="0.787693428571429">
88
relation ANTI does not have this property. It is not true that hot
ANTI hot, for example.
A relation R defined on a set S is said to be symmetric if when-
ever.b and c are R-related then so are c and b; that is, R is symmetric
if and only if bile always implies cRb. Synonymy also has this property.
If b is synonymous with c, then c is synonymous with b. So has antonymy.
Given that hot ANTI cold, we immediately know that cad ANTI hot. Tax-
onomy is not symmetric, however. A lion is a kind of mammal, but a
mammal is not a kind of lion.
In question answering we may be just as interested in drawing nega-
tive conclusions as positive ones. Thus iip4may be important to know tHat
if bRc is true then cRb must be false. The term asymmetric is used to
describe a relation R for which bRc and cRb are never both true, at
least when b and c are different elements of the get S. Taxonomy is
asymmetric and so is the timp interrelation before. If the question
asks, &amp;quot;Did c happen before b?&amp;quot; and we know that b happened before c, we
can answer with a confident no. For want of a better term we will say
that the rel&apos; ion R is non-symmetric if it is neither symmetric or Asym-
metric. In this case bRc and cRb are sometimes both true and sometimes
not. Similarly, the term irreftexive Is used for the case in which mRm
is never true, while the term nonreflexive is used for the case in which
mRm is sometimes true and sometimes not. In the same way intransitive
is taken to mean that if bite and cRA, we can conclude that b and d are
met R-related, while nontransitive will mean that &apos;Ad is sometimes true
if bRc and cRd, but-not always.
Each lexical relation itself bas a lexical entry. The reflexivity,
symmetry, and transitivity properties of the relation are listed in this
</bodyText>
<page confidence="0.340862">
8
89
</page>
<bodyText confidence="0.936700333333333">
entry, as they are in the entries for interrelational operators and
prepositions and other lexical items for which they are relevant. There
are also lexical entries under the property names, reflexive, irreflexixe,
etc. listing the appropriate axioms. The motivation behind lexical en-
tries for properties is first of all greater generality. Secondly, it
makes it much easier to add lexical relations and to a4d other properties
which turn out to be useful.
At this stage of development there are several transitivity axioms:
For lexical relations Rel, like taxonomy
</bodyText>
<figure confidence="0.716875666666667">
b Rel c A c Re; d b Rel d
For interrelations J, like before
Eads(r(J,z1,z2)) A Holds ( I(J,Z 2,Z3)) Ho7 de (/(J,Z1,Z3))
-For prepositions Q like in or above
Hads(V(location,Z1,Pllep(Q,Z2))) Ho/d8(P(location,Z2,
Prep (Q,Z3))) Hcids(P(location,Z1,PPep(Q,Z3)))
</figure>
<bodyText confidence="0.9320198">
Intuitively these are all instances of the same concept, transitivity.
There should be some single way of expressing it. It is a defect of this
representation system that there is not.
A relation that is reflexive, symmetric, and transitive is called
an equivalence relation. The synonymy relation is an equivalence relation
since it has all three properties. If R is an equivalence relation, then
a subset consisting of all the elements which are R-related to a parti-
nntar element x by thelequivalence relation is called an equivalence class.
In an equivalence class all the elements are R-related to each other. An equi
valence relation partitions a set into equivalence classes; each element
</bodyText>
<figure confidence="0.196909">
/9
90
</figure>
<bodyText confidence="0.913226777777778">
of the set belongs to exactly one equivalence class. The synonymy re-
lation pattitions the items in the lexicon in just this way. There is
a class consisting of suspicion and all the words synonymous with sus-
picion, like mistrust and doubt. These synonymy classes are disjoint;
each word sense in the lexicon belongs to exactly one of them (cf.
Edmundson and Epstein 1972, Palmer 1976).
With this as a basis an equivalence relation of paraphrasability
between sentences can be established. Sentence S is a paraphrase of
sentence S2 if one is obtained from tilt other &apos;y substituting synonyms
</bodyText>
<subsectionHeader confidence="0.841059666666667">
for each other.1
Mr. Kennedy viewed Leidy Laura with suspicion.
Mr. Kennedy regarded Lady Laura with mistrust.
</subsectionHeader>
<bodyText confidence="0.988493">
We might also allow substitution of conversives, nominalizations, etc.
Nancy was Sally&apos;s student.
</bodyText>
<subsectionHeader confidence="0.5630565">
Sally was Nancy&apos;s teacher.
Sally taught Nancy.
</subsectionHeader>
<bodyText confidence="0.959615222222222">
The equivalence classes of this relation, each one of which is fhe set
of all paraphrases of a given sentence have a definite theoretical im-
portance and some practical significance in question answering. One
member of a class might well &apos;be part of the story; another the right
answer to -a question.
This representation system can be viewed as defining a relation P
such that Si P S2 if and only if S1 and S2 have the same representa-
tion. If the representation system is well defined, then P should
define the same equivalence classes as the paraphrasability rpiptin,
</bodyText>
<figure confidence="0.901464666666667">
20
91
b. Inverses.
</figure>
<bodyText confidence="0.981796125">
The inverse R of the relation R is the relation which &amp;quot;goes in
the opposite direction&amp;quot; from R; that is, bRc if and only if cRb. Thus,
bake T make and make T bake are two ways of saying the same thing. Both
pieces of information are stated in the lexicon. However, the lexical
entry for bake includes T mzke; the lexical entry for make includes T
hake. Why bother to say the same thing in different places? There are
two reasons for this. First of all, the inverse relation may be a re-
lation that is commonly and easily verbalized, worth naming in its own
right. This is certainly true of the CHILD relation, as in puppy CHILD
dog. Instead of asking &amp;quot;What is a baby dog called?&amp;quot;, we could ask &amp;quot;What
is a grownyp puppy tailed?&amp;quot; or &amp;quot;What does a puppy grow up to be?&amp;quot; The
second reason is that putting this information in both entries can mAke
searches easier and much faster. We may only have one half of the pair
and need the other. We may have dog and puppy. This is easy if we have
the information CHILD puppy, in the dog entry, Otherwise we might have
to search the whole lexicon. In other situations we have two words but
no direct connection between them. For example, suppose the system knows
lion T mammal and mammal T vertebrate and is then asked, &amp;quot;Is a lion a
vertebrate?&amp;quot; The connection between lion and vertebrate can be found
much more quickly if the search starts from both the vertebrate end and
the lion end of the chain at the same time, but to do this there must
be a pointer to mammal in the vertebrate entry. Another question comes
to mind. Why call the inverse relation to CHILD by the clumsy name CHILD
instead of its propei name PARENT? The ECD uses two different names for
</bodyText>
<note confidence="0.479381">
92 21
</note>
<tableCaption confidence="0.498357333333333">
a relation and its inverse (S0 and V0 are inverses, for example). If
this were done here, two versions of the appropriate axiom schemes would
be needed one in the CHILD entry and one in the PARENT entry.
</tableCaption>
<bodyText confidence="0.923817">
Since a relation R is called symmetric if bRc always implies cRb, it
follows that a symmetric relation is its own inverse. The synonymy re-
lation S and antonymy relation ANTI are both self-inverse in thin sense.
For this reason we never need the symbol ANTI, etc. ANTI&apos; is ANTI The
entry for hot includes ANTI cold, the entry for cold includes ANTI hot.
c. Unique Linkage.
Raphael (1968) has proposed a property which seems extremely useful.
He calls it unique-linkage (U). Mathematicians usually refer to such re-
lations as one-to-one. A relation R has the uniqud-linkage property if
whenever xRy then bRy is false for any bOx and xRc is false for any coy,
i.e. any object is R-related to at most one other. Raphael&apos;s example
of unique-linkage is the relation &amp;quot;just to the right of&amp;quot;. The behavior
is especially characteristic of the queuing relation, e.g. with days of
the week, Monday Q Tuesday, etc.Some relations may be uniquely linked
on one side only, e.g. mother-child is uniquely linked on the left. We
can define UL unique-linkage on the left and UR unique linkage on the
</bodyText>
<table confidence="0.849808533333334">
right. (A relation which is UR is a single-valued function. If R has
the UL property, then its inverse is a single-valued function.)
Raphael also proposed for SIR-1 (ibid, p. 101) a property which he
calls irreflexive. R is set-nonreflexive if
11) (VB c x) Oa CX) R 13)
In Che SIR model both the &apos;X is a part of Y&apos; and the &apos;X is owned by Y&apos;
22
93
relations have this property. What it pays is that every set in the
model has a minimal element with respect to the relation R. A aid pier
version of tht.s property is sufficient for our purposes.
Minimum (VXCM)-- (ley C x) (htt x) (zRy)
Condition Every nonempty subset has a minimum.
Maximum (VXGM) (VY t- X) (iZt- X) (YRZ)
Condition Every nonempty subset has a maximum.
</table>
<bodyText confidence="0.962001666666667">
The part-whole relatiori has both properties in our model. In any non-
empty subset in the model there is something in it that is not a proper
subpart of anything else in that subset, and also something that has no
proper subpart. A relation that has this property stops somewhere. It
is not reflexive and not circular. A search that goes on looking for
links of this kind will stop somewhere. The relation &apos;is an ancestor of&apos;
has this property. We will eventually run out of ancestors in one direc-
tion and descendants in the other, at least, inside a finite model.
The properties of relations are summarizpd in Table 4.
</bodyText>
<tableCaption confidence="0.994472">
Table 4. 7.roperties of Relations
</tableCaption>
<table confidence="0.9632693125">
nioperty Definition
symmetric (Nfx€M) NYQ4) (XRY -• YRX)
&apos;asymmetric (vIX,M) (V/Ye-M) (XRY-&apos; YRX)
reflexive Cc&apos;XCH) XRX
irreflexive (VX€M) (XRX)
transitive (YXEM) t. /44) (siZEM) (XRY A YRZ XRZ)
intransitive (NtXt M) M) ZL 24) (XRY A YRZ-. (XRZ ) )
uniquely linked (VXEM) E M) (XRY-&apos; Z E M) (Z RY XZ)
(XRZ Y=Z) ) )
uniquely linked
on the left CVXEM) (VY c (XRY (NIZ EM) (ZRY Z=X) )
uniquely linked
on the right (VX H) E 14) XRY (VZ EM) (XRZ Zief) )
23
94
d Partial Ordering.
</table>
<tableCaption confidence="0.359050333333333">
Any transitive relation defines a partial ordering. Several of the
lexiaal relatiohs discussed earlier are transitive; many lexical items
are transitive too. One important reason for.reprtsenting time in
</tableCaption>
<bodyText confidence="0.903754666666667">
terms of the transitive interrelaeion before is to allow one to make the
same kinds of simple deductions about time that one can make about taxon-
omy. Some transitive relations, like taxonomy, are als6 reflexive. In
this case we talk about a weak ordering. (X s Y for numbers is a weak
ordering.) Some are not reflexive, these are called strong ordering
relations. (X&lt; Y for numbers is a strong ordering.) The time relation
before is a strong ordering relation. For any weak ordering there is a
strong ordering and conversely. Starting with the taxonomy relation T,
for example, a relation T1 or &amp;quot;proper.taxonomy&amp;quot; can be defined consisting
of the pairs x and y for which xTy but x and y are different. ThenxTiy
means that x is a kind of y but different from y. If instead one starts
with # strong ordering relation before, one can define a weak relation
&amp;quot;before].&amp;quot; for which x before1 y means that either x before y or x cooccured
with y.
The queuing relation Q is not itself a partial ordering but a partial
ordering can &apos;be derived from it. Monday Q Tuesday and,. Tuesday Q Wednesday,
but it is false that Monday Q Wednesday. Queuing is an &apos;immediate successor
4plation like the relation between a natural number n and the next number
n+1. A relation Q&apos; can be defined such that xQ&apos;y if either xlpy or there
are some objects z1&apos;zzn such that xQzi, z1Qz2, ... zQy. It follows
immediately that if bQc and cQd then bQ&apos;d. Q&apos;, the &apos;successor&apos; relation,
</bodyText>
<note confidence="0.658974">
95 24
</note>
<tableCaption confidence="0.551534">
is now transitive, for if bOtc and c(Vd, then one can find a chain of
Q-related objects linking b and d just by concatenating the chain
linking c and d. Raphmul&apos;s pair of relations jright and right behave
this way. The relations &amp;quot;is a child of&amp;quot; and &amp;quot;is a descendant of&amp;quot; are
also paired in this way.
</tableCaption>
<table confidence="0.915717095238095">
MODELS OF THE SEMANTIC STRUCTURE OF 1)ICTTONARIE:3
Kenneth C. Litkowski
16729 Shea Lane
Gaithersburg, Maryland 20760
25
2
CONThNTS
Summary A
1.Introduction
7.&apos;kttitudes Toward Dictionlries
i.Previous qesearch on Dictionaries
4.Description of Dictionary Contents 1J;
&apos;,.Bagir Model
6.Expansion of the Lodel: Points as Definitions 17
7.Semantic, Structural, and Syntactic Parsing of 1)efinLtions21
8.The Ultimate MiDdel: Points as Concepts 30
9.Procedures for Finding the Primitives 33
10.RelationshiP to fforts to Represent Iknowledge in Frames 44
11.Final Remarks 47
qeferences 40
Ficures
</table>
<tableCaption confidence="0.669238636363636">
1.k typical subgrAph of the dictionary digraph usi.ng the 17
basic model
2.Subgraph of model with points representing both single 19
and multiple deanitions
3.Subgraph of expanded model with grouping of definitions 20
4.Subgraph of a model incorporating a parsing system 22
5.Basic.model, verb subgraph example subject to Rule 1 35
6.Basic model, verb subgraph example subject to Rule 2 36
7.Basic model, verb subgraph example subject to Rule 3 37
Table
1.Recognition rules for semantic components 43
</tableCaption>
<page confidence="0.521802">
17
</page>
<sectionHeader confidence="0.564475" genericHeader="method">
3
SUMMARX
</sectionHeader>
<bodyText confidence="0.97085556">
Ordinary dictionaries have not been given their due, ei-
ther as sources of material for natural language understanding
systems or as corpora that can be used to unravel the complex-
ities of meaning and how it is represented. If either of these
goals are ever to be achieved, I believe that investigators
must develop methods for extracting the semantic content of
dictionaries (or at least for transforming it into a more use-
ful form).
It is argued that definitions contain a great deal of in-
formation about the semantic characteristics which should be
attached to a lexeme. To extract or surfade such information,
it will be necessary to systematize definitions and what they
represent, prof:moiy using semantic primitives. In this paper, I
describe procedures which I have developed in an attempt to ac-
complish these objectives for the set of verbs in Webster&apos;s
Third New International AELLEnarx (W3). I describe (1) htw I
have usedthe structure of the dictionary itself in an attempt
to find semantic primitives and (2) how it appears that the
systematization must incorporate a capability for word sense
discrimination and must capture the knowledge contained in a
definition.
The body of the paper is concerned with demonstrating that
semantic information can be surfaced through a rigorous analy-
sis of dictionary definitions. The first step in this process
realgres_ a_ comppehensive_ framewok_ within wiLi_eb_ definitions can
</bodyText>
<page confidence="0.591739">
4
</page>
<bodyText confidence="0.893610846153846">
be analy7ed. In,developing this framework, we must remember
thit elch word&apos; used in 1 definition is llso detined in the aic-
tionlry, so that we mu9t be able to uncover lnd dea. &apos;oath vi-
cious circles. The framework must llso be clrable o rerresent
ing traditional notions of renerative grammar to dell with the
syntactic structure of definitions, suitable framework ar-
rears to be provided by the theory of labeled directed el-1ms
(digraphs.
Using points to represent dictionary entries And lines to
represent the relation &amp;quot;is used to define&amp;quot;, two models of the
dictionary are described. From these models and from digrarh
theory, we can conclude that there may exist Primitive units of
meaning from which all concepts in tbe dictionary can be
derived.
To determine rrimitive concepts, it i9 necessary to sub-
ject definitions to syntactic and semantic parsing in order to
identify characteristics that should be attached to each defi-
nition. Syntactic parsing such as that implemented for systemic
grammar by Winograd is the first step, semantic parser must
next be developed. It appears that definitions themselves, and
particularly definitions of prepositions (which Are used to ex-
press sense relations), will be of significant help in develop-
ing such a -Parser. Further work is necessary to develop proce-
dures for surfacing from definitions information about the con-
text which must be associated with each sense. It appears as 14
this p„arser will have more general use for ordinary discourse.
</bodyText>
<page confidence="0.555006">
29
</page>
<sectionHeader confidence="0.56932" genericHeader="method">
5
</sectionHeader>
<bodyText confidence="0.999973055555556">
These notions lead to the ultimate model of a dictionary,
where points represent concepts (which may be verbalized and
symbolized in more than one &apos;ay) and lines represent relations
(syntactic or semanttc) between concepts.
Bssed on these models, procedures for fiAding primitive
concepts are described, using the set of verbs and their defi.-
nitions from W3. Specific rules are described, based on some
elementary graph-thOretic principles, structural characteris-
tics of dictionary deTinitions, and the parsing of the defini-
tions. These rules have thus far reduced the initial set of
20,000 verbs to fewer than 4,000, with further reduction to
come as all rules are applied,
It is argued that this approach bears a strong relation-
ship to efforts to represent knowledge in frames. Although mud]
work is needed on the parser and on a computerized version of
this approach, there is some hope that the parser, if expecta-
tions are borne out, will be capable of transforming ordinary
discourse into canonical frame representations.
</bodyText>
<figure confidence="0.565095">
30
6
</figure>
<sectionHeader confidence="0.810374" genericHeader="method">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.955507">
During the past 15 years. scientists in many fields have
been buildings a reservoir of knowledge about the semantic char
acteristics of natural language. Perhaps somewhat inexplicably
-mese developments have for the most part ignored the semantic
conten1 of dictionaries, despite the fact that even a small one
contains a vast amount of material. Some attempts have been
made to dent these repositories, but the steps taken have been
tentative and have not yet borne significant fruit, perhaps be-
cause a)e sheer volume and scope of a dictionary is so over-
whelming. As a result, most studies have dealt with only a few
definitions without a comprehensive assault on the whole. While
such studies have led to many insights, it seems that the full
ulefulness of a dictionary&apos;s contents will be realized only
when a comprehensive model of its semantic structure is deovel-
</bodyText>
<sectionHeader confidence="0.52499" genericHeader="method">
oped.
</sectionHeader>
<subsectionHeader confidence="0.592098">
Any system intended to provide natural language under-
</subsectionHeader>
<bodyText confidence="0.996387888888889">
standing must necessarily include a dictionam. If any such
system is to achieve broad applicability, its dictionary must
cover a substantial part of the natural language lexicon. For
this to occur, the developers of a system must either create a
dictionary from scratch or be able to incorporate an existing
dictionary. Given the amount of effort that usually goes into
development of an ordinary dictionary, the former alternative
is rather impractical. However, little has been done toward
meeting the latter alternative; with vnat follows, I will
</bodyText>
<page confidence="0.603224">
32
7
</page>
<bodyText confidence="0.994682884615385">
describe the approach which I believe must be followed in
transforming the contents of an ordinary dictionary for use in
a true natural language system.
In order to be used in a language understanding system, a
dictionar3Os semantic contents must be systematized in a way
that the sense in which a word is being used can be identified.
Before this can be done, it is necessary to characterize what
is already contained in each definition. To do this it seems
necessary to write the meaning of each definition in terms of
semantic and syntactic primitives. My purpose in this paper is
(1) to describe how to use the dictionary itself to move toward
id61itification of the primitives, at the same time (2) showing
how this process can be used (a) to provide the capability for
discriminating among word senses (i.e. characterizing the
frames into which a given word sense will fit) and (b) to char-
acterize knowledge contained or presupposed in a definition,
Before etbarking on the description, it-is necessary tc
point out some limitations which shouild be kept in mind as the
reader proceeds. First, in trying to present an overview of my
approach, I have had to forgo describing the detailed steps
which I have followed to date. Second, even had I presented a
full description, I would still have been short of providing
sufficient details to enable computer implementation of any
procedures. Third, dince the approach presumes that concepts
represented by the lexicon are tne realizations of many as yet
unknown-frecursive functions to be discovered by stripping away
</bodyText>
<page confidence="0.673752">
32
</page>
<sectionHeader confidence="0.295797" genericHeader="method">
8
</sectionHeader>
<bodyText confidence="0.898061785714286">
one-layer at a time, results other than procedures to be used
in striPpiiig will not emerge until all layers have been re-
moved. (However, I do argue that the &amp;quot;stripping&amp;quot; procedures are
inherently useful, in that they will constitute a parser even
in the intermealaTe stages.) Fourth, since I have not ha 4 ac-
cess to a computer, which hs become essential for significant
further progress, I have been unable to determine how far the
procedures I have developed would take me, so there iLs an in-
herent uncertainty as to how much further development is needed.
Notwithstanding these livktations, I am hopeful that what is
presented will provide a satisfactof.y framework for further Jai-
veg-tigations into the contents of dictionaries. I will comment
further on these limitations and how they might be overcome at
the end of the paper.
</bodyText>
<sectionHeader confidence="0.990221" genericHeader="method">
2. ATTITUDLS TOWARD DICTIONARIES
</sectionHeader>
<subsectionHeader confidence="0.700185">
Many of the signifacant contributors to the present under-
</subsectionHeader>
<bodyText confidence="0.9986502">
standing of meaning (such as Katz and Fodor 1963, Fillmore 1968
and 1971, Chafe 1970, Jackendoff 1974, Winograd 1972, and
Sc)ank 1972) haye generally ignored dictionaries. Yet, each has
presented a formulaic structure for lexical entriLes to serve as
a basis for the creation of a new dictionary klthough their
perceptions aboutlthe nature of language are well-established,
theia, formalisms for lexical entries have not taken advantage
of the equally well-established prattices of lexicography.
The rationale underlying the development of new formalisms,
expressed in SOME cases and imDlicit in others, ids that lexical
</bodyText>
<page confidence="0.45799">
33
</page>
<sectionHeader confidence="0.345979" genericHeader="method">
9
</sectionHeader>
<bodyText confidence="0.979948576923077">
entries in dictionaries ara unsatisractory bemuse they do not
contain sufficient information. These formalisms thus require
that semantic features such as &amp;quot;animate&amp;quot; or &amp;quot;state&amp;quot; be appended,
to particular entries. While it is true that ordinary di&apos;ctio-&apos;
nary entries do not overtly identify all appropriate features,
this may be less a diSficulty inherent in definitions than the
fact that no one has developed the necessary mechanisms for
surfacing featdres from definitions. Thus, for example. &amp;quot;nurse&amp;quot;
may not have the feature &amp;quot;anlmate&amp;quot; in its definition, but
&amp;quot;nurse&amp;quot; is defined as a &amp;quot;woman&amp;quot; which is defined is a &amp;quot;person&amp;quot;
which is defined as a wbeing&amp;quot;&apos;whichl.s defined as a &amp;quot;living
thing&amp;quot;; this string seems- sufficiehb to establish &amp;quot;nurse&amp;quot; as
&amp;quot;animate&amp;quot;. In general, it seems that, if a semantic feature is
essential to the meaning of ,a particular entry, it is similarly
necessary that the feature be discoverable within the semantic
structure of a dictionary. Otherwise, there is a defect in one
or more definitions, or the dictionary contains some internal
inconsistency. (Clearly, it is beyond expectation that any pre-
sent dictionary will be free of these problems.)
The possibility of defective definitions has also gene.1.--
ated criticisms, more direct than above, on the potential use-
fulness of a dictionary. On one hand definitions are viewed as
&amp;quot;deficient in tht presentation of relevant data&amp;quot; since they
provide meanings by using &amp;quot;substitutable words (i.e. by syn..
onyms), rather than by listing distinctive featuregn (Nada
1975:172). On another handl the proliferation of meanings
</bodyText>
<note confidence="0.326407">
34
10
attached to an entry is viewed as only a case of &amp;quot;apparent
</note>
<bodyText confidence="0.999405647058823">
polysemy&amp;quot; which obscures the more general meaning of a lexeme
by the addition of &amp;quot;redundant features already determined by
the environment&amp;quot; (Bennett 1975:4-11). Both objections may have
much validity and to that extent would necessitate revisions to
individual or sets of definitions. However, neither viewpoint
is sufficient to preclude an analysis of what actually appears
in any dictionary. It is possible that a Amprehensive analysis
might more readily surface such difficulties and make their
amelioration (and the consequent improvement of definitions)
that MUnh easier.
Even though dictionaries are viewed somewhat askance by
many who study meaning, it seems that this viewpdint is influ-
enced more by the difficulty dt systematically tapping their
contents than bv any substantive objections which conclusively
establish them-as useless repositories of semantic content.
However, it is necessary to demonstrate that a systematic
appApach exists and can yield useful results.
</bodyText>
<sectionHeader confidence="0.611396" genericHeader="method">
3. PREVIOUS RESEARCH ON DICTIONARIES
</sectionHeader>
<bodyText confidence="0.955269">
Notwithstanding the foregoing direct and indirect criti-
cisms. some attempts have been made to probi the nature and
structure of dictionary definitions. A review of relevant as-
pects or two such studi‘rs will help the material presented here
stand out in sharper relief.
Olney 1968 describes the conceptual basis of many project-
ed routines for processing a machine-readable transcript of
</bodyText>
<figure confidence="0.598666615384615">
35
11
Webster&apos;s Seventh New Calle...giate Dictionary (W7). The primary
objectives of these routines were the development nf
&amp;quot;(a) rules for obtaining certain of the senses described
for W7 entries from other senses diiscribed for the
same entries or from senses described for other W7
entries from which thelfirst (at least in typical
cases) were derived morphologically; and
(b) semantic components and rules for combining them to
yield specifications of senses that cannot convenient-
ly be obtained by rules referred to in (a) above.&amp;quot;
(ibid.:6)
</figure>
<subsectionHeader confidence="0.351246">
Although these objectIves gre reasonable, they do not take ad-
</subsectionHeader>
<bodyText confidence="0.861929769230769">
vantage of the possibility that the semantic structure of a
dictionary might be a uniried whole. As a%result, any routines
that are developed seom to require the serendipitous perception
of patterns. Further, if a dictionary does have a unified se-
mantic structure, it is not clear that a rule relating meaning
to form will be relevant to .a model&apos; of the semantic structure
even though interesting results might emerge. It seems netces-
sary to have some comprehensive view that will permit UR to
know whether a particular rule is well-formed. This lack of ob-
jective criteria also imperils any analysis that selects a sub-
set of definitions for detailed analysis. The z-election of a
subset of the dictionary should, arise from well-defined a pri-
ori considerations raZher than an intuition that a particular
</bodyText>
<page confidence="0.5101615">
36
12
</page>
<bodyText confidence="0.832330230769231">
subset seems to be related. An example of this intuitive ap-
proach appears in Simmons 1975 and 1976.
Tn Quinlan 1968, the analysis of dictionary definitions
was part of a study Of semantic memory, and for that reason was
not concerned with the full development of a dictionary model.
In that study, a person determined the meaning of a concept
when he &amp;quot;looked up the &apos;patriarch&apos; word in a dictionary, then
looked up every word in each of its definitions, then looked up
every word folind-in each of those, and so on, continually
branching outward until every word he could reach by this pro-
cess had been looked up once.&amp;quot; This process was never actually
carried out because (1) not all words in a dictionary were used
in the computer files, (2) the process was terminated when a
common word was found in comparing the meanings of two words,
and (3) there was a belief that there are no primitive word
concepts. The termimation of a search rs designed was necessary
in any event since, without any restrictions, it is likely that
a large part of the d±.ctionary would have been reached on every
occasion. More importantly, Quinlan did not fully consider
what was happening when branching led to a word already encoun-
tered, namely, that a definitional circularity was thereby un-
covered Such circularities which might be vicious cir,cles,
must be treated specially (as will be shown below), and hence,
Quillian&apos;s unrestricted branching should have been modified.
Quillian also overlooked the, possibility that a Qpncept common
to two Aatriarchs is more Arimitive than either. The continued
</bodyText>
<page confidence="0.5908715">
37
13
</page>
<bodyText confidence="0.984952272727273">
comparison of more and more primitive concepts, along with re-
strictions on the outward branching, implies that primitiive
concepts actually do exist.
Based on these observations, I take, as a worki,ng hypoth-
esis, the assumption that a dictionary may be a unified whole
with underlying primitive conne1Dts.1 With this beginning, it is
necessary to articulate a model of the dictionary which will
permit an identifigation of the primitive concepts through the
application of well-defitlea rules or procedures. It is proposed
that what follows constitutes the first steps toward meeting
this objective.
</bodyText>
<sectionHeader confidence="0.807708" genericHeader="method">
4. DESCRIPTION OF DICTION4RY CONTENTS
</sectionHeader>
<subsectionHeader confidence="0.530117">
Since a dictionary contains much material, it is first
</subsectionHeader>
<bodyText confidence="0.9766902">
necessary to delineate exactly what is to be modeled:2 For thiLs
Purpose, it is assumed that the semantic content of a dictio-
nary essentially resides within its definitions, thereby ex-
cluding from formal analysis such things as the pronunciation,.
the etymology, and illuatrative examples. s presently con-
ceivea, the analysis will focus on the word being defined
(hereafter called he main entry), the definitions (including
sense numbers and letters used as delimiters), part-of-speech
1 No dictimary is likely to satisfy this assumption, which
only a theoretically desirable characteristic. The assumption
enables us to exclude the definienda from the models.
2 In the interests of space, I have glossed over A. large number
of intricacies that would have to be dealt with in arriving
at a machine-readable transcript suitable for analysis.
Several pages would be required to describe them fully.
</bodyText>
<note confidence="0.437472">
38
</note>
<page confidence="0.833152">
14
</page>
<bodyText confidence="0.980698130434783">
labels, status or usage labels, and usage notes. The manner in
which these features will be employed will be made clear as the
analysis proceeds.
The hypothesized unified nature of a dictionary arises
from the fact that definitions are expressed by words which are
also defined3 (i.e., there is no semantic metalanguage). If we
wish to understand the meaning of a given definition, then we
must first understand the meanings of its constituent wolTist
Since each constituent corresponds to a main entry, then, in
order to understand the meaning of the given definition, we
must understando the meaning of the constituent words&apos; defini-
tions. Continued repetition of the process is nothing more than
the outward branching process described by Quillian; however,
as mentioned before, we must make this branching more disci-
plined in order to deal with vicious circles and avoid unwanted
circularities.
If we are to have a fully consistent dictionary, its model
must show how each definition is related to all others. Thus,
for each definition, X, the moddl should enable ue to identify
(1) those definitions of the constituent words-of X that apply
and those that do not apply, and (2) the production rules that
generated X from these definitions. For example, in the defini-
tion of t4e noun broadcast, &amp;quot;the act of spreading abroad&amp;quot;,4 it
</bodyText>
<footnote confidence="0.826410666666667">
3 There are some exceptions to this assertion, such as proper
names,.biological caxa, and other special symbols, as pointed
out by the Journal&apos;s referee.
</footnote>
<page confidence="0.795474">
39
</page>
<bodyText confidence="0.891515777777778">
15
is necessary that the model indicate (1) which of the defini-
tions of the, act, of, spread, and abroad apply, and (2) the
production rules by which the and act and all other colloca-
tions) occur together. If this can be done for each definition
in the dictionary, and if any inconsistencies are reconciled,
then, as will be shown, it should be possible to find the prim-
itive concepts in the dictionary and to transform each defini-
tion into a canonical, fprm.
</bodyText>
<sectionHeader confidence="0.997582" genericHeader="method">
5. BASIC MODEL
</sectionHeader>
<bodyText confidence="0.9591844">
The theory of (labeled) directed graphs (digraphs)5 is
used as the formalism for the models. Digraph theory deals with
the abstract notions of &amp;quot;points&amp;quot; and &amp;quot;directed lines&amp;quot;; its
applicability to the problem before us therefore depends on how
these notions are interpreted. In thj.8 respect, it is important
to distinguish tip.e manner in which this theory is used here
from the manner in which it previously has been used in seman-
tics and linguistics. The two most common uses are (1) where
trees display phrase and syntactic structures (cf. Katz and
Fodor 1963), or (2) where directed graphs portray the sequen-
</bodyText>
<subsectionHeader confidence="0.732926">
tial generation of words in a sen,tence or phrase (cf. Simmons
</subsectionHeader>
<bodyText confidence="0.723406">
1972). In these cases and others (cf. Quillian 19-68 and Ben-
nett 1975) graphs are used primarily as a vehicle for display
</bodyText>
<sectionHeader confidence="0.8571455" genericHeader="method">
4 All definitions used in this paper are taken from Webster&apos;s
Third New International Dictionary, Ericyclopaedla Britannica,
Chicago, 1965.
5 Terminolagy for digraphs follows Harary 1965.
</sectionHeader>
<page confidence="0.466371">
40
</page>
<bodyText confidence="0.963653535714286">
16
and no results from graph theory are explicitly employed to
draw further inferences. However, as used here, graphs consti
tute an essential basis for the analysis and hence will play an
integral role in a number of assertions that are made.
in the simplest model, a point can be interpreted as rep-
resenting all the definitions appearing&apos; under a single main en-
try; the main entry word can be construed as the label for that
point. The part-of-speech labels, status or usage labels, and
usage notes are considered integral to the definitions and may
be viewed as part of a set of characteristics of the individual
definitions. A directed line from x to y will be used to repre-
sent the asymmetric relation &amp;quot;x is used to define y&amp;quot;; thus, if
the main entry x appears exactly or in an inflected form in a
definition of y, then xRy. (This does not preclude a distinct
line for yRx or xRx.) Therefore, we can establish a point for
every main entry in a dictionary and draw The appropriate di-
rected lines to form a digraph consisting of the entire dictio-
nary. (This digraph may be disconnected, but probably is not.)
An example, which is a subgraph of the dictionary digraph, is
shown in Figure 1 on the next page. Except for broadcast, only
the labels of each point are shown, but each represents all the
definitions appearing at its respective main entry. The direct-
ed line from act to broadcast corresponds to the fact that &amp;quot;act
is used to define broacicast&amp;quot;, since its token appears in &amp;quot;the
act of spreading abroad&amp;quot;, In this model, the token &amp;quot;spreading&amp;quot;
is not represented by a point, since it is not a main entry.
the act
</bodyText>
<sectionHeader confidence="0.69692" genericHeader="method">
2212.41 abroad
</sectionHeader>
<bodyText confidence="0.959787666666667">
?igure 1 A typical subgraph of the dictionary
digraph using the basic model.
Since the definition shown iLs not the only one for broadcast,
this paint has additLonal :incoming 3ines which are not shown.
The resultant digraph for even a small dictionary Ls ex-
tremely large, perhaps consisting of well over 100,000 points
and 1,000,000 lines. Clearly, such a digraph provides little
fine structure, but even so, it does have some utility. The
manner Ln which it can be used is descr bed in Section 9.
</bodyText>
<sectionHeader confidence="0.995309" genericHeader="method">
6. EXPANSION OF THIJ MODEL; POINTS 4.9 DEFINITIONS
</sectionHeader>
<bodyText confidence="0.556135833333333">
Letting each po.nt in the asic model represent all the
definitions of a main entry provides very little delineation of
subtle gradations of semantic content. As a first step toward
understanding this content, it seems worthwhile to let each
point represent only one definition. However, the basic model
will not trivially accommodAte such a specification .(Trimarily
</bodyText>
<page confidence="0.3939925">
41
17
</page>
<bodyText confidence="0.742723363636364">
broadcast (the act of
spreading abroad)
because of the interpretation given tg
thus it must first be modified.
In the basic model, the existence
points, x and y, assert z that xRy, 1.e
y&amp;quot;. Since the points represent all the
the directed line), and
of a line between two
is used to define
definitions under the
</bodyText>
<figure confidence="0.79723">
• 9 It y
42
18
</figure>
<figureCaption confidence="0.7760411">
main entries, the existence of a line arises from the simple
fact that x appears in at least one of y&apos;s definitions. If the
point y represents only one definition, say yo, there is no
difficulty in saying that xRyj. However, if WQ wish every point
to represent only one definition, then we must find the defini-
tion of x, say xl, for which x1Ry3 is true. Referrinp to the
subgraph in Figure 1, this amounts to determining, for example,
which definition of abroad. is used to define the token &amp;quot;abroad&amp;quot;
in &amp;quot;the act of spreading abroad&amp;quot;, that is, finding the i such
that &amp;quot;abroadiRthe act of spreading abroad&amp;quot; or
</figureCaption>
<bodyText confidence="0.9541659375">
&amp;quot;abroad Rbroadcast &amp;quot;
It should be intuitively clear that interpretation of
points as srngle definitions is desirable. However, there are
no a priori criteria by which the appropriate value of i can be
determined, and hence there is no immediate transformation of
the basic model into a model where each point represents one
Aefinition. Since tha.s objective is worth pursuing, it As there-
fore necessary to develop criteria or rules according to which
the desired transformation can be made.
In the application of rules that may be developed, it will
be convenient to make use of a model intermediate between the
basic one and the one with points as definitions. For thi, pur-
pose, we can combine the two models by employing a trivial -e
lation, x1Rx, wbich says that the ith definitIon of x is used
to define x; this holds for all definitions of x. The line re-
flecting xPy would remain in the model, so that the digraph
</bodyText>
<figureCaption confidence="0.990265">
Figure 2. Subgraph of model witn points representing
</figureCaption>
<bodyText confidence="0.963163125">
both single and multiple definitions.
would show both x Rxvand xRy. and x. would re a carrier, As il-
lustrated in rigure 2. In this case, the unsubscripted abroad
represents all the definitions of abroad ()Illy some of which
are shown). If and when suitable criteria establish, for ex-
ample, that abroadi, but not abroad2, abroad3,..., fits th
context of the token &amp;quot;abroad&amp;quot; in.the definition of broadcaE:,
it would then be possible to draw a line directly from abroad
to broadcast without the interm6diation of the unsubscripted
point abroad, thus eliminating.paths from -abroa42., abroaff3,..,
to broadcast%
This model thus includes the points of the basic model and
adds &apos;pints to represent each individual definition in the dic-
tionary. The lines between these points ensure that no relation
in the basic model is lost. As described in the example, It is
necessary to develop rules according to which the points repre-
</bodyText>
<page confidence="0.919011">
43
</page>
<figure confidence="0.992198533333333">
19
broadcast (the act of
spreading abroad)
abroad
the act of
spread
• • •
abroad,) abroad..4 abroadn
at (wi-Tae-ry&apos;
large) -afart)
abroad 1
(oilier a
wide area
61 4
20
</figure>
<bodyText confidence="0.945433066666667">
senting more tnan one definition can be eliminated or bypassed,
so that the only relations, xRy, that remain are such that x
and y are points which represent one definition.
It cnay happen during the application of rules that some
lines to a carrier will be eliminatqd with more than one still
remaining. In such a case, it will still be useful to modify
the digraph as much as possible. For example, if xRy in the
basic mpdel, whege x has in definitions and y has n, and xRyi in
the 9cpanded model, then Xi•ø•XmRY;je It may be that some crl-
terion indicates that, say x1,x2Ryi but that x3 .,xmftyi. When
this occurs, we can create two points xa and xb,such that
xioc2Rxa xaRyi, and x3„..,xmRxb, but with no line from xb to
as illustTated in Figure 3. The utility of this type of
broadcast
41k,
</bodyText>
<figure confidence="0.9944">
abroad abrad2
abroada
abroadb
• • • •
abroad4 abroad
abroad
- 3
</figure>
<figureCaption confidence="0.999955">
Figure 3. Subgraph of expanded%model
</figureCaption>
<bodyText confidence="0.982170333333333">
with grouping of definitions.
grouping will be demonstrated in Section 9. In any event, since
maw criteria will eventually be required in the elimdmation of
points representing two or more otafinitions. this ability to
group definitions is a necessary mechanism for modeling inter-
medite descriptions of the dictionary. (It should be noted
</bodyText>
<page confidence="0.735573">
45
</page>
<bodyText confidence="0.968157925925926">
21
here that all such paints will not be eliminated; those that re-
main will indicate an essential ambiguity in the dictionary;
this is further discurased in Section 8.)
7. SEMANTIC) STRUCTURAL, AND SYNTACTIC PARSING OF DEFINITIONS
The basic and expanded models, exampled in Figures 1, 2,
and 3, do not portray any of the meaning of the dictionary, but
rather indicate where particular relationships exist. In fact,
these two models portraSr only the relation &amp;quot;is used to define&amp;quot;
as if there is no other relation between definitions. This ap-
proach does not capture some very important elements that go to
make up a definition.
Instead of being analyzed directly into its ultimate con-
stituents, a6 in Figures 1 and 2, the definition, &amp;quot;the act of
spreading abroad&amp;quot;, should first be broken down into subnhrases
and then into its ultimate oqnstituents, n&apos;S in Figure 4, shown
on the next page. A-desirable property of the new points is
that they have the syntactical structure ol derinitions: Thus,
&amp;quot;the act&amp;quot; and &amp;quot;spreading abfbad&amp;quot; have the form of noun defini-
tionsr-&amp;quot;spread abroad&amp;quot; has the form of a verb definition; and
&amp;quot;of spreading abroad&amp;quot; (not shown, but feasible under a ditfer-
ent parsing) has the form of an adjective definition. This
would eliminate such combinations as &amp;quot;act al&amp;quot; or &amp;quot;of the. The
poinws represenvIng phrase constituents of a definition thus
have the form of definitions, but lack a label.
The absence or presence of a label seems to make no dif-
ference in understapding the definition reprectgorted. In fact,
</bodyText>
<figure confidence="0.943156769230769">
46
22
broadbast
(the act of spreading abroad)
(the act) (spreading abroad)
(spread abroad)
abroada
the act of sR/ead (over a
wide irea)
(at large)
al5oadI abroad,
(over a (at „,
wide area large)
</figure>
<figureCaption confidence="0.999189">
Figure 4. Sutigraph of a model inc)rporatIng a parsing system.
</figureCaption>
<bodyText confidence="0.998935461538462">
it seems valid to represent idelnttcally worded definitions or
phrase constituents, regardless )f the number of main entries
under which they appear, by a single point with multiple labels.
Thus, if each of the main entries disperse, scatter, and dia-
tribute has a definition verbalized as &amp;quot;spread abroad&amp;quot;, these
three words can be labels of the point &amp;quot;spread abroad&amp;quot; in Fig-
ure 4. ouch a construction has no effect on the analysis of the
definition &apos;the act of spreading abroad&amp;quot; or &amp;quot;spread abroed&amp;quot; as
showr in Figure 4, and similarly, the analysis there would have
no effect on any analysis involv ng disperse, scatter, or
tribute. Since thsre is a large number of instances where du-
plicate wording appears in a dictionary, the approach given
here would effect a substantial reduction in the size of the
</bodyText>
<page confidence="0.555652">
47
</page>
<sectionHeader confidence="0.175272" genericHeader="method">
23
</sectionHeader>
<bodyText confidence="0.981905">
digraph. (This is not to say that the words plasm, scatter,
and distribute ha Ns the same meaning, but rather that in some
instances these words can express the same concept.)
The definition, X, &amp;quot;the act of spreading Abroad&amp;quot; Is es-
sentially an entity unto itself. The definitions of its compo-
neat words have similar independence. However, like atoms in
molecules, we need to identify those forces which hold the com-
ponents together and which endow the whole with whatever char-
ecteristics it has. The deEinitions of the component words may
require several words for their expression. but they are sym-
bolized by one mord in the definition X, even so the symbol
and the definition both represent the same entity, which has
certain characteristics enabling it to be acted upon by certain
forces. These characteristics are the semantic, Structural, and
syntactic properties of definitions, and the forces are the
production rules by which the entities (i.e.,. the component def-
initions or their symbols) are brought together. A definition
may be viewed as the realization of such rules operating on the
chAractezistics of other definitions. The nerculean task before
us is to build a parsing system or recognition grammar which
will articulate the etaracteristics to be attached to each def-
inition and which will capture the production rules necessary
to portray the relationships between definitions. The remainder
of this section will present my ideas on how to approach this
task.
</bodyText>
<page confidence="0.523288">
48
24
</page>
<bodyText confidence="0.927387538461539">
The process which I have used ive finding primitives en-
tails showing that one definition is derived from another
thereby excluding the former as a candidate for being primi-
tive. Such a demonstration of a derivational relationship re-
quires a parser. Each pattern which I observe lactween defini-
tions helps to exclude fuTther definitions and simultaneously
becomes part oi* the parser. As a result, identification of the
charatteristics to be attached to each definition does not have
to be accomplished all at once; as will ba.come clear below, our
purposes can be served as the components of the parser are de-
lineated. Thus, success does not rprvii-pg, full articulation of
the parser before any parsing is initiated. The following rep-
resents general observations about the form of the parser as it
has emerged thus fal.
The first set of characteristics would result from the
syntactic parsing of each definition. The purpose of this step
would be simply to establish the syntactic pattern of each def-
inition. The output of this step would be similar to that gen-
erated by Winograd (1972) in his parser. The &apos;dictionary&apos; for
the parser would be the very lictionary we are analyzing, al-
though only the main entry, its inflectional forms, and its
part-of-speech label would be used in this step. Ambiguous
parsings and failures would be kicked out; the failures in
particular, would provide an excellent source for refining the
parser used by Winograd. Clearly, this step is not trivial, and
it might even be argued that it is beyond the state-of-the-art.
</bodyText>
<page confidence="0.503676">
49
25
</page>
<bodyText confidence="0.97157037037037">
However, by using a corpus as large as a dictionary and by
kicking out failures and ambiguities, I believe that this step
will significant3V advance the state-of-the-art
The second set of characteristics would be determined from
a semantic parsing of the definitions, that is an attempt to
identify the cabes and semantic components present within each
definition. For this study I have found the following dis-
tinction to be useful: A case is a semantic entity which is not
intrinsic to the meaning of a word, e.g. that someone is an
agent of an action, whereas a component is an intrinsic pftri of
the meaning, e.g. a human being is animate It is necessary to
articulate recognition rules for determining that a particular
case or semantic component is present The little that hg&apos;s been
done te develop such rules has been based primarily on synta4c-
tic structures or a priori assertions that a given case or com-
ponent is present. Despite the reccgnized deficiencies of dic-
tionaries, I believe that it is possfille to&apos;bring much greater
rigor to such rules with evidence gleaned directly flora the
definitions. For example, cut has a definition, &amp;quot;penetrate with
an instrument&amp;quot;; this definition Would be parsed as having the
instrument case. (Note also that this definition makes the in-
strument case intrinsic to cut.) However, in most IIRAAA, it
will be necessary to examine the definitions of the ,constituent
words. For example, the verb knife has the definition, &amp;quot;cut
with a knife&amp;quot;; although it is quite obvious in this instance
that a knife is an instrument, rigor demands that we go to its
50
26
aufinitions where we find, &amp;quot;a simple instrument ...&amp;quot;. A great
leal of analysis may ultimately be required to discern the in-
trinsic characteristics to be attached to a definition, but I
belve that many of these can come from the dictionary itself
rather than from intuition.
Although the number of cases and components discussed in
the literature is nut very large, the number of ways lin which
they may be expressed, at least in English, is significantly
larger. In addit on, there is Si £11 a large amount of ambiguity,
i.e„ not every form specifically indi-cates the presence of a
particular case. For example, a def ntkon, &amp;quot;act with haste&amp;quot;
does not indicate that &amp;quot;haste&amp;quot; in an instrument: rather, &amp;quot;with
haste&amp;quot; expresses a manner of acting. Unraveling all these nu-
ances requires a great deal! of effort. Kowever, it appears that
a partictlarly good source of help in this endeavor might be
found in the definitions of prepositicds (which are used pri-
marily to indicate sense relations).
Bennett 1975 found t possible to express the meaning of
spatial and temporal pre.pos tions (a hIgh percentage of all
prepositions) with only 25 components. However, in Webster&apos;s,
the number of their definitions is at least two orderS of mag
natudes higher. The difference seems to lie in the &amp;quot;apparent
polysemy&amp;quot; whlch, as Bennett says, arises from the inclusion in
prepositional definitions of &amp;quot;redundant features already deter-
mined by the environment&amp;quot;. In other words, many prepositional
definitions contain information about the context surrounding
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.736480666666667">Journal of Computational Linguistics 81 THE FINITE STRING VOLUME 15 NUMBER 5 publacation March 24, With this issue, David G. Hays completes his term as Editor 0-f</note>
<abstract confidence="0.9924875">AJCL and breatftes a sigh of relief. Personal matter g have made the last two issues of AJCL for 1978 excessively late. The next issues of AJCL will appear on pabut the circumstances of Auggest digital magnetic recording and direct wire transmission will be suitable for experimental use shortly.</abstract>
<note confidence="0.856285555555556">AMERICAN JOURNAL OF COMPUTATIONAL LINGUISTICS is published by the Association for Computational Linguistics EDITOR, 1974-71978: David G. Hays EDITORIAL ASSISTANT, 19-77-1978: William L. Benzon 1977-: Donald E. Walker, Artificial</note>
<affiliation confidence="0.975737">Center, SRI Interna-</affiliation>
<address confidence="0.994921">tional, Menlo Park, CA 94025</address>
<note confidence="0.475124">TECHNICAL ADVISOR, 1975-:</note>
<author confidence="0.960928">Martin Kay</author>
<author confidence="0.960928">Xerox Palo Alto Re-</author>
<affiliation confidence="0.883853">search Center</affiliation>
<note confidence="0.795756444444444">Copyright C) 1979 Association for Computational Lingui$tics 2 CONTENTS LINGUISTICS IN THE USSR: 1978 Joyce Friedman . . . . • • • • • • • • 3 Astociation for Literary and Linguistic Computing BULLETIN Volgme 6, Number 2, 1978 . . . . . . . 14 6, Number . . . . . . , . - . . • . • 15</note>
<title confidence="0.927426">PROPERTIES OF LEXICAL RELATIONS</title>
<author confidence="0.947643">Yartha W Evens</author>
<author confidence="0.947643">Raoul N Smith</author>
<affiliation confidence="0.916346">OF THE SEMANTIC STRUCTURE OF</affiliation>
<address confidence="0.5396712">C. Litkowski . . • • • • . 25 WASHINGTON FebrAary 1979 • • • • • • . . 75 March 1979 . . . . . . 83 1979 . . • 4 • • 91</address>
<note confidence="0.769224666666667">3 N -14 Department oE Computer ald Communicition Scrienc-!s TUE UNIVERSIrY OF MICHICAN Ann Arbor, Michigan 48139 Computer Studies in Formal Linguistics</note>
<date confidence="0.992839">July 1978</date>
<title confidence="0.840927">COMPUTATIONAL LINGOISTTCS IN THE USSR</title>
<author confidence="0.999483">Joyce Friedman</author>
<abstract confidence="0.98863175">As part of an official U.S (USSR on Arplicatians of CImputers in Manaoement, a subgroup on natural langaa.ge processing visited the Soviet Union from May 29 throug&amp;quot;1 June 11, 1978. The graup met with scientists in Moscow, Novosibirsk, Leningrad, Fiev. There were formal and presentations of technical material, Ind .lso miny inforndl discussions. This report a view cpmputatiinal emerged fnom these 1.is-mssiofts. 4 Bacground Thp,11.S.PISSP Science Exchange on Applications of Cooputers to Kanatieniert incluies many startasks. The exchange in natural language-procesini is one task under ths topic &amp;quot;theoretical for software in applications economics manaaelaent&amp;quot;. The exchange in natural language processing was to hive begun in June 1977. Howevei a schelulei tri,p by gripntists war; cancelled at the fast minutP by the USSR side; tne roasnn FlivPn was that there were no hotel rooms available in woscow. rn spite of this initial disappointment hogar ir Nnvemher 1977 when threc. SoviPt sgientists United stAtes ft two weeks. The visitors NarAnevani o: the Academy of Sciencet, Tomputing rst the exchanTe tne Alexander ntPr in ani Victor Briabrin and Dmitri Pospelov of tne riences Computing Center in Moscow. The rpnorted in this roe in tte rescheduled visit by the U.S. riPlc.gation. It took place May 28 to June 11„. 1978.</abstract>
<title confidence="0.253071">of U.S. dciegati.or were: Donald AufPnkamn, U.S. Chnirman of the U.S./USSR Joint Working Group Scientific ani Techni al Cooperation in tha Application of</title>
<author confidence="0.289283">Computerr to lanagemnnt</author>
<author confidence="0.289283">Sue Boner</author>
<author confidence="0.289283">Joyce Friedmaa R E W</author>
<affiliation confidence="0.996795">nppartlent of Computer ani Communication Sciences, The University</affiliation>
<address confidence="0.75173">Michigan; Malhoul. Bolt Beranek and Newman, Inc., Cambridge; Stanley Petrir:k, Mathematics Depkrtment, 13F</address>
<abstract confidence="0.991828314516129">T. 1. Watson Research Center, YorPtown Heights: Silly Sedelow, DonartmPnts of Linguistils and Computer Science, University of ant Walter Sedelow, Departments Soliology and 5 Computer ScienCe, U“versity of Kansas. The U.3. Ielegation was accompaniel throughent the trip by R. S. Nartniyani Movosibirsk. report gr3ups toprther similar work differelt thI main of the natura Language thearem-provini can viewed ts based on (I) linguistics, (2) artificial intelligence, or (3) logix, distinctions ar to somP extent arbitrary. We also give overviPw oc the computers and programming languages avai/able f3r linguistics. Work on and reF:ogni.tion was also discusped on the visit, not covetel in this report. Work on Lanaudae main roots of work are the meaning-text modP1 of Pellichuk, dependenr.y grammar, aid transformationatt grammar. They are vari3u ly interpreted different systems. Zoya Shalyapira, Laboratory ot Machine Trtsitslatio3,voreman Languages, desctibed an Rnglish to machine translation symtem under development since 1972 and based primarily on the meaning-text motiel. The representation is a *ependency tree, with wore order information, mlrphology aid semantic/sfntactic valencies. This structure preserv.,es all data is also close a semantic 3f text. rher? 15 dictionary and a grammar for each lanquacca. 6 grammar ruler, of the two forms: If &lt;structure&gt; than. aria then &lt;transformation&gt;. information iaclules senantic descripttons of lexical ad and the semantic acceptability of word pairs. There is a dictionary of 10,100 lexf=imes, described in terms of 10 semantic primitives The syntactic and semantic gtructures so analysis goes onry aa deep as is nacessary for spn+3nce. shliyapinals works on Ilnauistid aspecrs is ao corputer implementation. Uri Alresyan also works with the meAnipg-text model and with. allci-ine translation as the.goal. His work is nri-marily on French to Rus;ian translations. but he also works on Engli.sh. His onglish grammar is sail to b the most completo evrr publishel: grammar will Th&apos;. limluistic motel folnr parts: morphology, deep syntax, aid however, the current reduced licks A lirtionary givas for each ord its morphology, its syntactic an4 (there are 150-syntattic features; 500 semantic. criteria for possible govprninl words, ani selectilnal Ke.strirtions. Rule scHema or &amp;quot;syntigmas&amp;quot; go fram structure to a surface syntactic structure that is tree. about 2)3 syntagulas far each representing A syntagma allows a trae over Y to be consttucted from a string containing X and Y uniar various complex, conditions. The lexical informafion ad thl syntagmas datPrmine the transformation from wort strihig to surface-syntactic structure. A demp structure is then definei ay rulas. which Convert. for examnle, striketo 7 deliverwhen the object is 1 blow. The deep structure is no lonler languale-specitic but is universal, and serves as the basis for transration hetween languages. Imresyan stresed tle valde of continuing to work on the same linguistic model in order t6 complete its development; he contrasted thin with the attltgle current American linguists. of the Economics Institute is developinl a natural languaie interface for a data base sypten. has computPL sunpott and is runnina soon ina large factory. The natural language suLset has sentences such as Is number of wqrkers of &lt;pl&apos;ac.&amp;quot; and to easy economisf.s to learn. The system is based vpry recent modelsat transformatxonal grammar: Iakalev mentioned &amp;quot;traces&amp;quot; all some of Jackendlff&apos;s theories. ¶h system Ions tram to structure fromwhich it constructs a formula far computation of numerical result. Artiflcial tnt.elligence Work in Natural auale_ AT-baled systems are being developed at the Computing Center the Acals&apos;.my of Sciences at under tt,a direction 3f Briabrin and at the Computing renter uf the Siberimn Division af the Academf of Sciencez, Navasibirsk under tie ac Ilmmanie-r gariniyani, in system demonstrated to us in Moscaw was InformAtion Logical System). This work is heavily influehced ay intelligence wok in fOrilabrin&apos;spent seven 8 months at LI.?. working with William Martin and with Carl DILOS is written and runs on ths BE5m-6 in Mdscow, as well as on a PDP-11/45 at the Internatianil Tnstitute fot Applied Systems Analysis in Laxenburg, Austria. Th s system is intended both to test various approaches to natural language processing and for practical applications. It =onta.ims an ATN linquistic processor and a semantic processor based an frames. The current applications area is attune the demoAstration was however on data basr. of AI,Natmral Language Systems (including DUOS, 1US, EFL, OWL, and larINAP). The system was able to answer simple naturil language questions from the data nase but it was not possible from the demonstration to get a good feelirg for the actual range &apos;of lanbuage Marin&apos;yani&apos;s group in Novosibirsk has 17 psopla, in.-Adding 6 and 9 programmers. Until a years ago, the work followed Mellehukts modal. This has now hean here proceeds along four lines, so relativelv inlebeniantly: (1) Narin&apos;yani is dewelping a formll linguistic model which combines dependancy and constituent structur-e in a mixed multi-level representation. proceeds by local modification of the graph structures, expandimg and compr2ssing T.ase frames at var ousil2vels. The linguistic so far in=ludes formal description groups aid groups. This formal model has now bean written so far is not implemented. (2) The semantic system VOST3K-0 contains a formal model of time. Om the basis lf tsxts of senten^es such &amp;s &apos;From the 3rd up to the 13th of Marlh 9 was in Moscow&amp;quot; it answers Where was Mike it noon on the 17th ne March?&amp;quot;. The system is coq i&apos; SETL And While the natural language fragment is still even For a model of time, (e.g. nd time adverbials), (3) systems are being deve] ped. The first of PL-1 &amp;quot;mini&amp;quot; or &amp;quot;toy&amp;quot; system essentially syntactic analyvis (thlugh it relies heaVily on word order). It has. a well-lefined subject domain, a data base of oersonnal information, and answers questions such as &amp;quot;who ander 30 earls more than average?&amp;quot; (Salary information is public in the this vary limited sublect th works well, The &amp;quot;midi&amp;quot; applicational systram is undar dmvelopment and is more syntactically orieatad. it will cqntain a nondeterministic parsAr .Far a gramnar with (4) The final subjrotip is the programmin, language group; it has implemented 3ETL on the BES- 6. work in Natural In Moscow, at VTNITI, the linguist E. B. Paducheva and the mathematician T. 0. ttoLelskay± are developing jointly an aoproa:h to natural language praaessinr basei on both trinsforlati,ril grammar all first-order logic. The cnrrent Jomain is converse theorems in geometty. The system is able to przzessleometcy and produce their theorems&amp;quot;. In this system the semantic representation language is first-order procedures and synthesis have been dev.eloped, as well as processing procedures within the logic. The linguistic part of the metho&amp; is based on. transformational As is tha case with most of the Soyiet work transtemmational grammar, the deep structure uses dependency grammar rrther thin constituent structure grammar. transformations are originally written in the Forward directiol, i.e. from deep to surface structure. Analysis is dome using a &amp;quot;reversed&amp;quot; version of each transformation (not obtained autbmattcally). While the forwari transformations are of orier the reversal rules are strictly for efficiency. There are 30-3 transformetions, each ezpressed as a structural description, giver as a template, aril a structural change, given as a sequence of elamentary operations. a work in detail, but has no computer implementation. Tne system is sAid to contain interestitg solutions -tfl problems of quantification, negation, aad re.luction. Tne authors reportel, wits amusement, that the description of the work was printed in 42,0,0 copies. The current work at the University of Leningrad under 3 Tseitin, Faculty of Engineering and 4athematics, was described to us by others as basea on loqic, but Tseitin himself took a philosophical approach in his discussions wita us. His remarcs were moresuggestive that i descriptive. He indicated that approach to natural language was bf analogy to programming languages, usinl ma3ros as in operating systems. Ha there is no such thing that his approach did usa procedural semantics. His previous *work on 11 complexity and theorem-proving Ls not related to his work 321 natnral languagy. Rowever, heid 0 argue &apos;that a natural language system for computmrs should reflect the fact that natural langua/e pmrformanam Ylr people does not require exp3nential timm. TseittnIs own current wlrk is no* on natural lanauage, as he is busy vriting an ALGOL68 implementation. Tscitin and Liakina, formerly of the Far.dity of Philologl, also talkei about smveral earlier natural lanauage systeas whiTh am unable disttnguish. They dre described in a number 3f from 166 on. In gererai they rplay grammar, ani use transformations durina synta.sitic analysie. Restriction., on the grammar are stated in the pcedi.7ate calculus and resolution the&apos;orem-proving is used. J&apos;he goal is English to translation of prose. The system of J. Kapitolova, Head of the Laboratory 3f Applied Cybeinetics at. the Institute of CyberReti=s at Kiev, is an interactive thp-mem-Droving system fbr mathematical text.. The objertive is to be able to fill in the standard aaps in as Ltdicatm3 by &amp;quot;Lt is obvious &amp;quot;as in the pro)f of the orevious Theorem&amp;quot;. The&apos; text Ls fitst processed manually into a hignly stylized mathematical language. Only the material, theocmms and proofs, is analyzed: disnission Ls treatmd comment and bj the programs- Several larae texts, Curtis and Reiner Alaebrairof 3roups, have bean preproressmi. The theorem-provet is vtailored to the specific omain. It techniques, as well as specLal mathematical and 12 Th.? system has been programmed and about to oe out on a recent thesis.. This is of ten years duration, Ind has had a minimum or ii peoOle. in Montagne was considerable.My talk in wits very well attendPd, and there were gnestions.rhe audience was generallf familiar with Mpntague&apos;s an3 with recent papers the topic in ani Linguistics.The interest seeme3 to come fnom morej g neral interest in lngic as a knowledge reprPsentatinn natoral language systeits. Agafanov in Novosibirsk is also inttreested in the possible applications nf Montague gramIrte-to programming languages. access appearz: Lc be difficult to obtain for computational linguisfs in the Soviet Union.Many of tie no computer support, even though they were in areas where compater testing of grammars or theories could be very of the computing was on the thefe are more recent computers, e.g., the E3-E131 (Pyad), series, available. for other purposes. computers were op order from Hewlett-Packard: CDC, and 9urroughs.The terminals ya saw were mainly grapnics terminals Eastern Europe, with both Boman and Cyrillic character and seemed fine in use. There is much interest in advanced programming languages. 13 SETL is imp1emente3 in Novosibirsk. (This is with the aid of the U.S./USSR Science exchange. l In Moscow, PASCAL is impiementai. LeningraJ, is impleimenting ALq01.68 tor the Ryad computers, compatible with the diJ have oq=asion ta see some systems operation. Th a lAnguages were impressive, Out the was not. Theire seamed liagnostics. Whan were .7rashes it was not possible to tell whic:h were due to th computer and which: to the programs. 7onclusion&apos;s processinl in tha USS!&apos; seems to be ofailr lines. The work by linguists is motivated by machine translation. Tt relies on versions of Mel&apos;Chuk&apos;s meaning-text model, with some type of transformations on a base. It is charaCterized by deal _of sophisticated devnlopment of large grammars, by large groups of linguist, but is withoutcomputer support. The artifiill intelligence work is diracted toward data bas .c information is at in state of development, heavily bascad on U.S. work. It is caaried out in :&amp;quot;.&apos;omputinl Centers has good programming ani computer support. Th a logic-based work carried out by individuals or small groups locations without f7omputeF support, and by one large group with computers.</abstract>
<note confidence="0.666359142857143">MACHINE-TRANSLATION OF CHINESE MATHEMATICAL ARTICLES S Joh, L. Kong, and H.-$. Hung 111 THE CHINESE UNIVERSITY LANGUAGE TRANSLATOR P.H. Nancarrow 121 (CULT) - A REPORT AUTOMATIC SCANSION OF SANSKRIT POETRY FOR AUTHORSHIP CRITERIA D. Wujastyk 122 MIZAR-QC/6000 LOGIC LANGUAGE A. Trybulec 136</note>
<title confidence="0.792441">THE DISCOVERY OF SYNTAGMATIC AND PARADIGMATIC</title>
<author confidence="0.484296">CLASSES</author>
<affiliation confidence="0.846236666666667">REPORT ON A COURSE ON THE USE OF COMPUTERS IN TEXTUAL ANALYSIS AND BIBLIOGRAPHY HELD IN THE COMPUTER UNIT OF THE UNIVERSITY COLLEGE OF WALES</affiliation>
<address confidence="0.887254">ABERYSTWYTH, 10-14 APRIL 1978 P. Sims-Williams 159</address>
<title confidence="0.400341">COMPUTER LANGUAGES FOR AND</title>
<author confidence="0.899382">PROCESSING M Boot</author>
<note confidence="0.934634222222222">PARTIAL-PARSING ALGORITHM.FOR LAN6U\GE USING A SIMPLE GRAMMAR ARCU`1ENTS ,P13. Sallis — READABLE T.D. 177 ON THE USE OF COMPUTERS IN\ CRITICISM: A REPORT S.M. Hockey 180 Notes (Data Conference) (ICGH/4) (ALLC International Meeting, 1978)</note>
<title confidence="0.974035666666667">an Contributors Notes and Notes</title>
<note confidence="0.8863824">Reviews .... Letter to the Editor 202 Bibliography 204 Bunptin 6 /5 CONTENTS GUEST EDITORIAL: LOOKING BACK F. de Tollfnaere 221 OF MLOQUIA IN TUBINGEN D. Kottke WORDS OCCURRING IN ENGLISH IDIOMS D.J. 228</note>
<title confidence="0.823009">PROPOSED CRITERIA FOR PUBLISHING</title>
<author confidence="0.74691">RESULTS D Ross B</author>
<note confidence="0.830840333333333">THE TEACgING OF RUSSIAN USING AN ONLINE COMPUTER K1 AhMad, M. Colenso, and G. Corbett THE VALUE OF THE COMPUTER IN EDITING &apos;OPEN TRADITION&apos; TEXT Z42 OF OCCURRENCE ENGLISH TEXT R. Shinghal and 245 SENTENCE LENGTH DISTRIBUTIONS IN GREEK HEXAMETERS AND HOMER S. Michaelson, A.Q. Morton, and W.C. Wate 254 SNOBOL: THE LANGUAGE FOR LITERARY COMPUTING L.D. Burnard 268</note>
<title confidence="0.77395325">GENERATING AND TRANSFORMING BY A COMPUTER WITHOUT A DICTIONARY U. Oman 280 OLD ICELANDIC A DIPLOMATIC EDITION WITH THE</title>
<note confidence="0.847281058823529">OF THE COMPUTER Firchow, K. (rim. &amp; ind S. Gilmour Advertisements 4 302 Secrdtary&apos;s Notes 304 Editor&apos;s Notes 304 Advertisement 305 Diary 306 Notes on Contributors 308 and Notes Book Review. 318 Documents Received 320 Bibliography 322 Bulletin 6 /6 &amp; Smith for Q-A SpstemSee Microfiche 83 Appendix II. Properties of Lexical Relations. a. Reflexivity, Symmetry, Transitivity.</note>
<abstract confidence="0.997100273148148">Certain properties of lexical-semaptic relations can be very useful in deductive inference. For instance, iT we know that a cheetah is a kina or mammal ana a mammal is a kind of vertebrate then we can deduce that a cheetah is a kind of vertebrate. Writing T for the taxonomy relation, we can abbreviate this sentence: if cheetah T mammal and mam- T vertebrate then cheetah T bTc and cTd, it follaws that bTd. This fact can be described much more efficiently by the statement that the taxonomy relation is transitive. Two other commonly properties of relations are reflexivity and These properties may Apply to predicates formed from lexical entries as well to lexical-semantic be precise, relation R defined on a set S is said to be a if whenever b and c are R-related and also c and d are R related then b and d stahd in a relation R also. Synonymy is a transitive relation just as transitivity is. The preposition in behaves in the same way. If Sam is in the kitchen and the kitchen is in the hotel, then we know that Sam is in the hotel. The time interrelation like this, too. If Zorro arrived before the posse did and the posse arrived before the explosion, then we know that Zorro arrived before the explosion. relation R defined on a spt S is said to have the property if all the elements of S are R-related to themselves, that is, if mRm is true for all members in of the set S. The synonymy relation has this property: a word means the same thing as itself. The antonymy 17 88 ANTI does not have this property. It is not true that example. relation R defined on a set S is said to be if whenever.b and c are R-related then so are c and b; that is, R is symmetric if and only if bile always implies cRb. Synonymy also has this property. If b is synonymous with c, then c is synonymous with b. So has antonymy. that know that Taxonomy is not symmetric, however. A lion is a kind of mammal, but a mammal is not a kind of lion. In question answering we may be just as interested in drawing negative conclusions as positive ones. Thus iip4may be important to know tHat is true then cRb must be false. The term used to describe a relation R for which bRc and cRb are never both true, at least when b and c are different elements of the get S. Taxonomy is and so is the timp the question asks, &amp;quot;Did c happen before b?&amp;quot; and we know that b happened before c, we can answer with a confident no. For want of a better term we will say the rel&apos; ion R is it is neither symmetric or Asymthis case bRc and cRb are sometimes both true and sometimes Similarly, the term used for the case in which mRm never true, while the used for the case in which is sometimes true and sometimes not. In the same way is taken to mean that if bite and cRA, we can conclude that b and d are R-related, while mean that &apos;Ad is sometimes true if bRc and cRd, but-not always. lexical relation itself bas a lexical entry. reflexivity, symmetry, and transitivity properties of the relation are listed in this 8 89 entry, as they are in the entries for interrelational operators and prepositions and other lexical items for which they are relevant. There also lexical entries under the property irreflexixe, etc. listing the appropriate axioms. The motivation behind lexical entries for properties is first of all greater generality. Secondly, it makes it much easier to add lexical relations and to a4d other properties which turn out to be useful. At this stage of development there are several transitivity axioms: For lexical relations Rel, like taxonomy b Rel c A c Re; d b Rel d interrelations J, like ( I(J,Z Ho7 de prepositions Q like in or Intuitively these are all instances of the same concept, transitivity. There should be some single way of expressing it. It is a defect of this representation system that there is not. A relation that is reflexive, symmetric, and transitive is called an equivalence relation. The synonymy relation is an equivalence relation since it has all three properties. If R is an equivalence relation, then subset consisting of all the elements which are R-related to a partinntar element x by thelequivalence relation is called an equivalence class. In an equivalence class all the elements are R-related to each other. An equi valence relation partitions a set into equivalence classes; each element /9 90 of the set belongs to exactly one equivalence class. The synonymy relation pattitions the items in the lexicon in just this way. There is class consisting of all the words synonymous with sussynonymy classes are disjoint; each word sense in the lexicon belongs to exactly one of them (cf. Edmundson and Epstein 1972, Palmer 1976). With this as a basis an equivalence relation of paraphrasability between sentences can be established. Sentence S is a paraphrase of one is obtained from tilt other &apos;y substituting synonyms Mr. Kennedy viewed Leidy Laura with suspicion. Mr. Kennedy regarded Lady Laura with mistrust. might also allow substitution of conversives, etc. Nancy was Sally&apos;s student. Sally was Nancy&apos;s teacher. Sally taught Nancy. The equivalence classes of this relation, each one of which is fhe set of all paraphrases of a given sentence have a definite theoretical imand some in question answering. One member of a class might well &apos;be part of the story; another the right answer to -a question. This representation system can be viewed as defining a relation P that P S2 if and only if and S2 have the same representa- If the representation well defined, then P should the same equivalence the paraphrasability 20 91 The inverse R of the relation R is the relation which &amp;quot;goes in the opposite direction&amp;quot; from R; that is, bRc if and only if cRb. Thus, are ways of saying the same thing. Both pieces of information are stated in the lexicon. However, the lexical for T lexical entry for make includes T bother to say the same thing in different places? There are two reasons for this. First of all, the inverse relation may be a relation that is commonly and easily verbalized, worth naming in its own right. This is certainly true of the CHILD relation, as in puppy CHILD of asking &amp;quot;What is a baby dog called?&amp;quot;, we could ask &amp;quot;What is a grownyp puppy tailed?&amp;quot; or &amp;quot;What does a puppy grow up to be?&amp;quot; The second reason is that putting this information in both entries can mAke searches easier and much faster. We may only have one half of the pair need the other. We may have puppy. This is easy if we have information CHILD in the Otherwise we might have to search the whole lexicon. In other situations we have two words but no direct connection between them. For example, suppose the system knows mammal T is then asked, &amp;quot;Is a lion a The connection between be found more quickly if the search starts from both the and of the chain at the same time, but to do this there must a pointer to mammal in the Another question comes to mind. Why call the inverse relation to CHILD by the clumsy name CHILD instead of its propei name PARENT? The ECD uses two different names for relation and its inverse and are inverses, for example). If this were done here, two versions of the appropriate axiom schemes would be needed one in the CHILD entry and one in the PARENT entry. Since a relation R is called symmetric if bRc always implies cRb, it follows that a symmetric relation is its own inverse. The synonymy relation S and antonymy relation ANTI are both self-inverse in thin sense. this reason we never need the symbol ANTI, etc. ANTI&apos; is for ANTI entry for ANTI Raphael (1968) has proposed a property which seems extremely useful. calls it Mathematicians usually refer to such reas A relation R has the uniqud-linkage property if whenever xRy then bRy is false for any bOx and xRc is false for any coy, i.e. any object is R-related to at most one other. Raphael&apos;s example is the relation &amp;quot;just to the right of&amp;quot;. The behavior is especially characteristic of the queuing relation, e.g. with days of the week, Monday Q Tuesday, etc.Some relations may be uniquely linked one side only, e.g. uniquely linked on the left. We define UL unique-linkage on the left and unique linkage on the (A relation which is a single-valued function. If R has then its inverse is a single-valued function.) Raphael also proposed for SIR-1 (ibid, p. 101) a property which he calls irreflexive. R is set-nonreflexive if (VB c x) CX) 13) In Che SIR model both the &apos;X is a part of Y&apos; and the &apos;X is owned by Y&apos; 22 93 relations have this property. What it pays is that every set in the model has a minimal element with respect to the relation R. A aid pier version of tht.s property is sufficient for our purposes. (htt x) Condition Every nonempty subset has a minimum. Maximum (VXGM) (VY t- X) (iZt- X) (YRZ) Condition Every nonempty subset has a maximum. The part-whole relatiori has both properties in our model. In any nonempty subset in the model there is something in it that is not a proper subpart of anything else in that subset, and also something that has no proper subpart. A relation that has this property stops somewhere. It not reflexive and circular. A search that goes on looking for links of this kind will stop somewhere. The relation &apos;is an ancestor of&apos; this property. We will run out of ancestors in direction and descendants in the other, at least, inside a finite model. The properties of relations are summarizpd in Table 4. Table 4. 7.roperties of Relations nioperty Definition (Nfx€M) (XRY -• YRX) &apos;asymmetric reflexive (V/Ye-M) YRX) irreflexive (VX€M) (XRX) transitive (YXEM) t. /44) (siZEM) (XRY A YRZ XRZ) intransitive M) ZL 24) (XRY A YRZ-. ) uniquely linked (VXEM) (XRY-&apos; M) RY XZ) (XRZ ) uniquely linked on the left CVXEM) (VY c (XRY (NIZ EM) (ZRY uniquely linked on the right E 14) XRY (XRZ 23 94 d Partial Ordering. Any transitive relation defines a partial ordering. Several of the lexiaal relatiohs discussed earlier are transitive; many lexical items are transitive too. One important reason for.reprtsenting time in of the transitive interrelaeion is allow one to make the of simple deductions about time that one can make about taxonomy. Some transitive relations, like taxonomy, are als6 reflexive. In case we talk about a ordering. (X s for numbers is a weak Some are not reflexive, these are called relations. (X&lt; Y for numbers is a strong ordering.) The time relation a strong ordering relation. For any weak ordering there is a strong ordering and conversely. Starting with the taxonomy relation T, a relation T1 or &amp;quot;proper.taxonomy&amp;quot; can be defined consisting of the pairs x and y for which xTy but x and y are different. ThenxTiy means that x is a kind of y but different from y. If instead one starts for x before1 y means that either x before y or x cooccured with y. The queuing relation Q is not itself a partial ordering but a partial ordering can &apos;be derived from it. Monday Q Tuesday and,. Tuesday Q Wednesday, it is false that Monday Q Queuing is an &apos;immediate successor 4plation like the relation between a natural number n and the next number n+1. A relation Q&apos; can be defined such that xQ&apos;y if either xlpy or there some objects that ... zQy. It follows immediately that if bQc and cQd then bQ&apos;d. Q&apos;, the &apos;successor&apos; relation, now transitive, for if and c(Vd, then one can find a chain of Q-related objects linking b and d just by concatenating the chain linking c and d. Raphmul&apos;s pair of relations jright and right behave this way. The relations &amp;quot;is a child of&amp;quot; and &amp;quot;is a descendant of&amp;quot; are also paired in this way.</abstract>
<note confidence="0.475473">MODELS OF THE SEMANTIC STRUCTURE OF 1)ICTTONARIE:3</note>
<author confidence="0.915109">Kenneth C Litkowski</author>
<address confidence="0.689144666666667">16729 Shea Lane Gaithersburg, Maryland 20760 25</address>
<email confidence="0.411931">2</email>
<intro confidence="0.698398">CONThNTS</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>