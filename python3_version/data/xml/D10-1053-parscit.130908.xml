<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000092">
<title confidence="0.999299">
Hierarchical Phrase-based Translation Grammars Extracted from
Alignment Posterior Probabilities
</title>
<author confidence="0.984854">
Adri`a de Gispert, Juan Pino, William Byrne
</author>
<affiliation confidence="0.993779">
Machine Intelligence Laboratory
Department of Engineering, University of Cambridge
</affiliation>
<address confidence="0.890977">
Trumpington Street, CB2 1PZ, U.K.
</address>
<email confidence="0.997391">
{ad465|jmp84|wjb31}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.998587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99961496">
We report on investigations into hierarchi-
cal phrase-based translation grammars based
on rules extracted from posterior distributions
over alignments of the parallel text. Rather
than restrict rule extraction to a single align-
ment, such as Viterbi, we instead extract rules
based on posterior distributions provided by
the HMM word-to-word alignment model. We
define translation grammars progressively by
adding classes of rules to a basic phrase-based
system. We assess these grammars in terms
of their expressive power, measured by their
ability to align the parallel text from which
their rules are extracted, and the quality of the
translations they yield. In Chinese-to-English
translation, we find that rule extraction from
posteriors gives translation improvements. We
also find that grammars with rules with only
one nonterminal, when extracted from posteri-
ors, can outperform more complex grammars
extracted from Viterbi alignments. Finally, we
show that the best way to exploit source-to-
target and target-to-source alignment models
is to build two separate systems and combine
their output translation lattices.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937219512195">
Current practice in hierarchical phrase-based trans-
lation extracts regular phrases and hierarchical rules
from word-aligned parallel text. Alignment models
estimated over the parallel text are used to generate
these alignments, but these models are then typically
used no further in rule extraction. This is less than
ideal because these alignment models, even if they
are not suitable for direct use in translation, can still
provide a great deal of useful information beyond a
single best estimate of the alignment of the parallel
text. Our aim is to use alignment models to generate
the statistics needed to build translation grammars.
The challenge in doing so is to extend the current
procedures, which are geared towards the use of a
single alignment, to make more of what can be pro-
vided by alignment models. The goal is to extract a
richer and more robust set of translation rules.
There are two aspects to hierarchical phrase-based
translation grammars which concern us. The first
is expressive power, which we take as the ability
to generate known reference translations from sen-
tences in the source language. This is determined
by the degree of phrase movements and the trans-
lations allowed by the rules of the grammar. For a
grammar with given types of rules, larger rule sets
will yield greater expressive power. This motivates
studies of grammars based on the rules which are ex-
tracted and the movement the grammar allows. The
second aspect is of course translation accuracy. If
the expressive power is adequate, then the desire is
that the grammar assigns a high score to a correct
translation.
We use posterior probabilities over parallel data to
address both of these aspects. These posteriors allow
us to build larger rule sets with improved transla-
tion accuracy. Ideally, for a sentence pair we wish to
consider all possible alignments between all possi-
ble source and target phrases within these sentences.
Given a grammar allowing certain types of move-
ment, we would then extract all possible parses that
are consistent with any alignments of these phrases.
</bodyText>
<page confidence="0.976913">
545
</page>
<note confidence="0.817574">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545–554,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936">
To make this approach feasible, we consider only
phrase-to-phrase alignments with a high posterior
probability under the alignment models. In this way,
the alignment model probabilities guide rule extrac-
tion.
The paper is organized as follows. Section 2 re-
views related work on using posteriors to extract
phrases, as well as other approaches that tightly in-
tegrate word alignment and rule extraction. Sec-
tion 3 describes rule extraction based on word and
phrase posterior distributions provided by the HMM
word-to-word alignment model. In Section 4 we de-
fine translation grammars progressively by adding
classes of rules to a basic phrase-based system, mo-
tivating each rule type by the phrase movement it is
intended to achieve. In Section 5 we assess these
grammars in terms of their expressive power and the
quality of the translations they yield in Chinese-to-
English, showing that rule extraction from posteriors
gives translation improvements. We also find that
the best way to exploit source-to-target and target-
to-source alignment models is to build two sepa-
rate systems and combine their output translation
lattices. Section 6 presents the main conclusions of
this work.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971914893617">
Some authors have previously addressed the limita-
tion caused by decoupling word alignment models
from grammar extraction. For instance Venugopal
et al. (2008) extract rules from n-best lists of align-
ments for a syntax-augmented hierarchical system.
Alignment n-best lists are also used in Liu et al.
(2009) to create a structure called weighted align-
ment matrices that approximates word-to-word link
posterior probabilities, from which phrases are ex-
tracted for a phrase-based system. Alignment pos-
teriors have been used before for extracting phrases
in non-hierarchical phrase-based translation (Venu-
gopal et al., 2003; Kumar et al., 2007; Deng and
Byrne, 2008).
In order to simplify hierarchical phrase-based
grammars and make translation feasible with rela-
tively large parallel corpora, some authors discuss
the need for various filters during rule extraction
(Chiang, 2007). In particular Lopez (2008) enforces
a minimum span of two words per nonterminal,
Zollmann et al. (2008) use a minimum count thresh-
old for all rules, and Iglesias et al. (2009) propose
a finer-grained filtering strategy based on rule pat-
terns. Other approaches include insisting that target-
side rules are well-formed dependency trees (Shen et
al., 2008).
We also note approaches to tighter coupling be-
tween translation grammars and alignments. Marcu
and Wong (2002) describe a joint-probability
phrase-based model for alignment, but the approach
is limited due to excessive complexity as Viterbi
inference becomes NP-hard (DeNero and Klein,
2008). More recently, Saers et al. (2009) report
improvement on a phrase-based system where word
alignment has been trained with an inversion trans-
duction grammar (ITG) rather than IBM models.
Pauls et al. (2010) also use an ITG to directly align
phrases to nodes in a string-to-tree model. Bayesian
methods have been recently developed to induce a
grammar directly from an unaligned parallel corpus
(Blunsom et al., 2008; Blunsom et al., 2009). Fi-
nally, Cmejrek et al. (2009) extract rules directly
from bilingual chart parses of the parallel corpus
without using word alignments. We take a differ-
ent approach in that we aim to start with very strong
word alignment models and use them to guide gram-
mar extraction.
</bodyText>
<sectionHeader confidence="0.9117145" genericHeader="method">
3 Rule Extraction from Alignment
Posteriors
</sectionHeader>
<bodyText confidence="0.999928466666667">
The goal of rule extraction is to generate a set of
good-quality translation rules from a parallel cor-
pus. Rules are of the form X→(&apos;y,α,—) , where
&apos;y, α E {X U T}+ are the source and target sides of
the rule, T denotes the set of terminals (words) and
— is a bijective function1 relating source and target
nonterminals X of each rule (Chiang, 2007). For
each &apos;y, the probability over translations α is set by
relative frequency over the extracted examples from
the corpus.
We take a general approach to rule extraction, as
described by the following procedure. For simplic-
ity we discuss the extraction of regular phrases, that
is, rules of the form X→(w,w), where w E {T}+.
Section 3.3 extends this procedure to rules with non-
</bodyText>
<footnote confidence="0.961679">
1This function is defined if there are at least two nontermi-
nals, and for clarity of presentation will be omitted in this paper
</footnote>
<page confidence="0.997667">
546
</page>
<bodyText confidence="0.995350130434783">
terminal symbols.
Given a sentence pair (fJ1 , eI1), the extraction al-
gorithm traverses the source sentence and, for each
sequence of terminals fj2
j1 , it considers all possible
target-side sequences ei2
i1 as translation candidates.
Each target-side sequence that satisfies the align-
ment constraints CA is ranked by the function fR.
For practical reasons, a set of selection criteria CS is
then applied to these ranked candidates and defines
the set of translations of the source sequence that are
extracted as rules. Each extracted rule is assigned a
count fC.
In this section we will explore variations of this
rule extraction procedure involving alternative def-
initions of the ranking and counting functions, fR
and fC, based on probabilities over alignment mod-
els.
Common practice (Koehn et al., 2003) takes a set
of word alignment links L and defines the alignment
constraints CA so that there is a consistency between
the links in the (fj2
</bodyText>
<equation confidence="0.9920785">
j1 , ei2
i1) phrase pair. This is ex-
pressed by V(j, i) E L : (j E [j1, j2] n i E [i1, i2]) V
(j E� [j1, j2] n i E� [i1, i2]). If these constraints
are met, then alignment probabilities are ignored and
fR = fC = 1. We call this extraction Viterbi-based,
</equation>
<bodyText confidence="0.999524833333333">
as the set of alignment links is generally obtained
after applying a symmetrization heuristic to source-
to-target and target-to-source Viterbi alignments.
In the following section we depart from this ap-
proach and apply novel functions to rank and count
target-side translations according to their quality in
the context of each parallel sentence, as defined by
the word alignment models. We also depart from
common practice in that we do not use a set of links
as alignment constraints. We thus find an increase
in the number of extracted rules, and consequently
better relative frequency estimates over translations.
</bodyText>
<subsectionHeader confidence="0.999291">
3.1 Ranking and Counting Functions
</subsectionHeader>
<bodyText confidence="0.999994333333333">
We describe two alternative approaches to modify
the functions fR and fC so that they incorporate the
probabilities provided by the alignment models.
</bodyText>
<subsectionHeader confidence="0.8152685">
3.1.1 Word-to-word Alignment Posterior
Probabilities
</subsectionHeader>
<bodyText confidence="0.999658555555556">
Word-to-word alignment posterior probabilities
p(lji|fJ1 , eI1) express how likely it is that the words
in source position j and target position i are aligned
given a sentence pair. These posteriors can be effi-
ciently computed for Model 1, Model 2 and HMM,
as described in (Brown et al., 1993; Venugopal et al.,
2003; Deng and Byrne, 2008).
We will use these posteriors in functions to
score phrase pairs. For a simple non-disjoint case
</bodyText>
<equation confidence="0.9125448">
(fj2
j1 , ei2
i1) we use:
p(lji|fJ1 , eI1) (1)
i2 − i1 + 1
</equation>
<bodyText confidence="0.9998569">
which is very similar to the score used for lexical
features in many systems (Koehn, 2010), with the
link posteriors for the sentence pair playing the role
of the Model 1 translation table.
For a particular source phrase, Equation 1 is not
a proper conditional probability distribution over all
phrases in the target sentence. Therefore it cannot be
used as such without further normalization. Indeed
we find that this distribution is too sharp and over-
emphasises short phrases, so we use fC = 1. How-
ever, it does allow us to rank target phrases as pos-
sible translations. In contrast to the common extrac-
tion procedure described in the previous section, the
ranking approach described here can lead to a much
more exhaustive extraction unless selection criteria
are applied. These we describe in Section 3.2.
We note that Equation 1 can be computed us-
ing link posteriors provided by alignment models
trained on either source-to-target or target-to-source
translation directions.
</bodyText>
<subsectionHeader confidence="0.947241">
3.1.2 Phrase-to-phrase Alignment Posterior
Probabilities
</subsectionHeader>
<bodyText confidence="0.996636461538462">
Rather than limit ourselves to word-to-word
link posteriors we can define alignment proba-
bility distributions over phrase alignments. We
do this by defining the set of alignments A as
A(j1, j2, i1, i2) = jaJ 1 : aj E [i1, i2] iff j E
[j1, j2]}, where aj is the random process that de-
scribes word-to-word alignments. These are the
alignments from which the phrase pair (fj2
j1 , ei2
i1)
would be extracted.
The posterior probability of these alignments
given the sentence pair is defined as follows:
</bodyText>
<equation confidence="0.965232818181818">
p(A |eI1, fJ1 ) = Ea�1 EA p(�1,JJ1|I1)
Ea1 p(f1 ,a1 |e1)
fR(fj2
j1 , ei2
i1) =
j2 i2
�
i=i1
ri
j=j1
(2)
</equation>
<page confidence="0.971606">
547
</page>
<table confidence="0.9818094">
G0 G1 G2 G3
5—*(X,X) X—*(w X,X w) X—*(w X,X w) X—*(w X,X w)
5—*(5 X,5 X) X—*(X w,w X) X—*(X w,w X) X—*(X w,w X)
X—*(w,w) X—*(w X,w X) X—*(w X,w X)
X—*(w X w,w X w)
</table>
<tableCaption confidence="0.737766">
Table 1: Hierarchical phrase-based grammars containing different types of rules. The grammar expressivity is greater
as more types of rules are included. In addition to the rules shown in the respective columns, G1, G2 and G3 also
contain the rules of G0.
</tableCaption>
<bodyText confidence="0.999980142857143">
With IBM models 1 and 2, the numerator and de-
nominator in Equation 2 can be computed in terms
of posterior link probabilities (Deng, 2005). With
the HMM model, the denominator is computed us-
ing the forward algorithm while the numerator can
be computed using a modified forward algorithm
(Deng, 2005).
These phrase posteriors directly define a proba-
bility distribution over the alignments of translation
candidates, so we use them both for ranking and
scoring extracted rules, that is fR = fC = p. This
approach assigns a fractional count to each extracted
rule, which allows finer estimation of the forward
and backward translation probability distributions.
</bodyText>
<subsectionHeader confidence="0.9932535">
3.2 Alignment Constraints and Selection
Criteria
</subsectionHeader>
<bodyText confidence="0.9422598">
In order to keep this process computationally
tractable, some extraction constraints are needed. In
order to extract a phrase pair (fj2
j1 , ei2 ), we define
i1
</bodyText>
<listItem confidence="0.9658761">
the following:
• CA requires at least one pair of positions (j, i) :
(j E [j1, j2] n i E [i1, i2]) with word-to-word
link posterior probability p(lji|f&apos;1 , ei) &gt; 0.5,
and that there is no pair of positions (j, i) : (j E
[j1,j2]ni E� [i1,i2])V(j E� [j1,j2]ni E [i1,i2])
with p(lji|f1 , ei) &gt; 0.5
• Cs allows only the k best translation candidates
to be extracted. We use k = 3 for regular
phrases, and k = 2 for hierarchical rules.
</listItem>
<bodyText confidence="0.9997872">
Note that we do not discard rules according to
their scores fC at this point (unlike Liu et al.
(2009)), since we prefer to add all phrases from
all sentence pairs before carrying out such filtering
steps.
Once all rules over the entire collection of paral-
lel sentences have been extracted, we require each
rule to occur at least nobs times and with a forward
translation probability p(α|-y) &gt; 0.01 to be used for
translation.
</bodyText>
<subsectionHeader confidence="0.999902">
3.3 Extraction of Rules with Nonterminals
</subsectionHeader>
<bodyText confidence="0.999692785714286">
Extending the procedure previously described to
the case of more complex hierarchical rules includ-
ing one or even two nonterminals is conceptually
straightforward. It merely requires that we traverse
the source and target sentences and consider possi-
bly disjoint phrase pairs. Optionally, the alignment
constraints can also be extended to apply on the non-
terminal X.
Equation 1 is then only modified in the limits
of the product and summation, whereas Equation
2 remains unchanged, as long as the set of valid
alignments A is redefined. For example, for a rule
of the form X—*(w X w,w X w), we use A �
A(j1, j2; j3, j4; i1, i2; i3, i4).
</bodyText>
<sectionHeader confidence="0.9939745" genericHeader="method">
4 Hierarchical Translation Grammar
Definition
</sectionHeader>
<bodyText confidence="0.999818461538462">
In this section we define the hierarchical phrase-
based synchronous grammars we use for translation
experiments. Each grammar is defined by the type of
hierarchical rules it contains. The rule type can be
obtained by replacing every sequence of terminals
by a single symbol ‘w’, thus ignoring the identity of
the words, but capturing its generalized structure and
the kind of reordering it encodes (this was defined as
rule pattern in Iglesias et al. (2009)).
A monotonic phrase-based translation grammar
G0 can be defined as shown in the left-most col-
umn of Table 1; it includes all regular phrases, repre-
sented by the rule type X—*(w,w), and the two glue
</bodyText>
<page confidence="0.97695">
548
</page>
<table confidence="0.985051">
(G0) R1: S--+(X,X)
(G0) R2: X--+(s2 s3,t2)
(G1) R3: X--+(s1 X,X t3)
(G1) R4: X--+(X s4,t1 X)
(G2) R5: X--+(s1 X,t7 X)
(G3) R6: X--+(s1 X s4,t5 X t6)
</table>
<figureCaption confidence="0.983168">
Figure 1: Example of a hierarchical translation grammar and two parsing trees following alternative rule derivations
for the input sentence 81828384.
</figureCaption>
<bodyText confidence="0.9996005">
rules that allow concatenation. Our approach is now
simple: we extend this grammar by successively in-
corporating sets of hierarchical rules. The goal is to
obtain a grammar with few rule types but which is
capable of generating a rich set of translation candi-
dates for a given input sentence.
With this in mind, we define the following three
grammars, also summarized in Table 1:
</bodyText>
<equation confidence="0.470815">
�
• G1 := G0
1 X—*(w X,X w) , X—*(X w,w X) }. This
</equation>
<bodyText confidence="0.995931666666667">
incorporates reordering capabilities with two
rule types that place the unique nonterminal
in an opposite position in each language; we
call these ’phrase swap rules’. Since all non-
terminals are of the same category X, nested
reordering is possible. However, this needs to
happen consecutively, i.e. a swap must apply
after a swap, or the rule is concatenated with
the glue rule.
</bodyText>
<listItem confidence="0.978703">
• G2 := G1 U1 X—*(w X,w X) }. This
adds monotonic concatenation capabilities to
the previous translation grammar. The glue rule
already allows rule concatenation. However, it
does so at the 5 category, that is, it concate-
nates phrases and rules after they have been re-
ordered, in order to complete a sentence. With
this new rule type, G2 allows phrase/rule con-
catenation before reordering with another hier-
archical rule. Therefore, nested reordering does
not require successive swaps anymore.
• G3 := G2 U1 X—*(w X w,w X w) }. This
adds single nonterminal rules with disjoint ter-
minal sequences, which can encode a mono-
</listItem>
<bodyText confidence="0.999599290322581">
tonic or reordered relationship between them,
depending on what their alignment was in the
parallel corpus. Although one could expect the
movement captured by this phrase-disjoint rule
type to be also present in G2 (via two swaps or
one concatenation plus one swap), the terminal
sequences w may differ.
Figure 1 shows an example set of rules indicat-
ing to which of the previous grammars each rule be-
longs, and shows three translation candidates as gen-
erated by grammars G1 (left-most tree), G2 (mid-
dle tree) and G3 (right-most tree). Note that the
middle tree cannot be generated with G1 as it re-
quires monotonic concatenation before reordering
with rule R4.
The more rule types a hierarchical grammar con-
tains, the more different rule derivations and the
greater the search space of alternative translation
candidates. This is also connected to how many
rules are extracted per rule type. Ideally we would
like the grammar to be able to generate the correct
translation of a given input sentence, without over-
generating too many other candidates, as that makes
the translation task more difficult.
We will make use of the parallel data in measuring
the ability of a grammar to generate correct transla-
tions. By extracting rules from a parallel sentence,
we translate them and observe whether the transla-
tion grammar is able to produce the parallel target
translation. In Section 5.1 we evaluate this for a
Chinese-to-English task.
</bodyText>
<page confidence="0.991795">
549
</page>
<subsectionHeader confidence="0.991536">
4.1 Reducing Grammar Redundancy
</subsectionHeader>
<bodyText confidence="0.999994307692308">
Let us discuss grammar G2 in more detail. As de-
scribed in the previous section, the motivation for in-
cluding rule type X→(w X,w X) is that the gram-
mar be able to carry out monotonic concatenation
before applying another hierarchical rule with re-
ordering. This movement is permitted by this rule
type, but the use of a single nonterminal category X
also allows the grammar to apply the concatenation
after reordering, that is, immediately before the glue
rule is applied. This creates significant redundancy
in rule derivations, as this rule type is allowed to act
as a glue rule. For example, given an input sentence
s1s2 and the following simple grammar:
</bodyText>
<equation confidence="0.9979628">
R°: S--+(X,X)
R1: S--+(S X,S X)
R2: X--+(s1,t1)
R3: X--+(s2,t2)
R4: X--+(s1 X,t1 X)
</equation>
<bodyText confidence="0.995256625">
two derivations are possible: R2,R0,R3,R1 and
R3,R4,R0, and the translation result is identical.
To avoid this situation we introduce a nonterminal
M in the left-hand side of monotonic concatenation
rules of G2. All rules are allowed to use nontermi-
nals X and M in their right-hand side, except the
glue rules, which can only take X. In the context of
our example, R4 is substituted by:
</bodyText>
<equation confidence="0.4909565">
R4a: M--+(s1 X,t1 X)
R4b: M--+(s1 M,t1 M)
</equation>
<bodyText confidence="0.990664666666667">
so that only the first derivation is possible:
R2,R0,R3,R1, because applying R3,R4a yields a non-
terminal M that cannot be taken by the glue rule R0.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999979">
We report experiments in Chinese-to-English trans-
lation. Our system is trained on a subset of the
GALE 2008 evaluation parallel text;2 this is approx-
imately 50M words per language. We report trans-
lation results on a development set tune-nw and a
test set test-nw1. These contain translations pro-
duced by the GALE program and portions of the
newswire sections of MT02 through MT06. They
contain 1,755 sentences and 1,671 sentences respec-
tively. Results are also reported on a smaller held-
</bodyText>
<footnote confidence="0.991124333333333">
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
</footnote>
<figureCaption confidence="0.986901">
Figure 2: Percentage of parallel sentences successfully
aligned for various extraction methods and grammars.
</figureCaption>
<bodyText confidence="0.99959475">
out test set test-nw2, containing 60% of the NIST
newswire portion of MT06, that is, 369 sentences.
The parallel texts for both language pairs are
aligned using MTTK (Deng and Byrne, 2008). For
decoding we use HiFST, a lattice-based decoder im-
plemented with Weighted Finite State Transducers
(de Gispert et al., 2010). Likelihood-based search
pruning is applied if the number of states in the
lattice associated with each CYK grid cell exceeds
10,000, otherwise the entire search space is ex-
plored. The language model is a 4-gram language
model estimated over the English side of the paral-
lel text and the AFP and Xinhua portions of the En-
glish Gigaword Fourth Edition (LDC2009T13), in-
terpolated with a zero-cutoff stupid-backoff (Brants
et al., 2007) 5-gram estimated using 6.6B words of
English newswire text. In tuning the systems, stan-
dard MERT (Och, 2003) iterative parameter estima-
tion under IBM BLEU3 is performed on the devel-
opment sets.
</bodyText>
<subsectionHeader confidence="0.999217">
5.1 Measuring Expressive Power
</subsectionHeader>
<bodyText confidence="0.999881714285714">
We measure the expressive power of the grammars
described in the previous section by running the
translation system in alignment mode (de Gispert
et al., 2010) over the parallel corpus. Conceptually,
this is equivalent to replacing the language model by
the target sentence and seeing if the system is able to
find any candidate. Here the weights assigned to the
</bodyText>
<figure confidence="0.946117909090909">
3See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
80
70
60
50
40
30
G0
G1
G2
G3
</figure>
<page confidence="0.983364">
550
</page>
<table confidence="0.999749846153846">
Grammar Extraction # Rules tune-nw test-nw1 test-nw2
BLEU
time prune BLEU BLEU
GH V-union 979149 3.7 0.3 35.1 35.6 37.6
V-union 613962 0.4 0.0 33.6 34.6 36.4
G1 WP-st 920183 0.9 0.0 34.3 34.8 37.5
PP-st 893542 1.4 0.0 34.4 35.1 37.7
V-union 734994 1.0 0.0 34.5 35.4 37.2
G2 WP-st 1132386 5.8 0.5 35.1 36.0 37.7
PP-st 1238235 7.8 0.7 35.5 36.4 38.2
V-union 966828 1.2 0.0 34.9 35.3 37.0
G3 WP-st 2680712 8.3 1.1 35.1 36.2 37.9
PP-st 5002168 10.7 2.6 35.5 36.4 38.5
</table>
<tableCaption confidence="0.9801505">
Table 2: Chinese-to-English translation results with alternative grammars and extraction methods (lower-cased BLEU
shown). Time (secs/word) and prune (times/word) measurements done on tune-nw set.
</tableCaption>
<bodyText confidence="0.992164833333333">
rules are irrelevant, as only the ability of the gram-
mar to create a desired hypothesis is important.
We compare the percentage of target sentences
that can be successfully produced by grammars G0,
G1, G2 and G3 for the following extraction meth-
ods:
</bodyText>
<listItem confidence="0.9604127">
• Viterbi (V). This is the standard extraction
method based on a set of alignment links. We
distinguish four cases, depending on the model
used to obtain the set of links: source-to-
target (V-st), target-to-source (V-ts), and two
common symmetrization strategies: union (V-
union) and grow-diag-final (V-gdf), described
in (Koehn et al., 2003).
• Word Posteriors (WP). The extraction method
is based on word alignment posteriors de-
scribed in Section 3.1.1. These rules can be ob-
tained either from the posteriors of the source-
to-target (WP-st) or the target-to-source (WP-
ts) alignment models. We apply the alignment
constraints and selection criteria described in
Section 3.2. We do not report alignment per-
centages when using phrase posteriors (as de-
scribed in Section 3.1.2) as they are roughly
identical to the WP case.
• Finally, in both cases, we also report results
</listItem>
<bodyText confidence="0.94786212">
when merging the extracted rules in both direc-
tions into a single rule set (V-merge and WP-
merge).
Figure 2 shows the results obtained for a random
selection of 10,000 parallel corpus sentences. As ex-
pected, we can see that for any extraction method,
the percentage of aligned sentences increases when
switching from G0 to G1, G2 and G3. Posterior-
based extraction is shown to outperform standard
methods based on a Viterbi set of alignments for
nearly all grammars. The highest alignment percent-
ages are obtained when merging rules obtained un-
der models trained in each direction (WP-merge),
approximately reaching 80% for grammar G3.
The maximum rule span in alignment was al-
lowed to be 15 words, so as to be similar to transla-
tion, where the maximum rule span is 10 words. Re-
laxing this in alignment to 30 words yields approxi-
mately 90% coverage for WP-merge under G3.
We note that if alignment constraints CA and se-
lection criteria CS were not applied, that is k = oo,
then alignment percentages would be 100% even
for G0, but the extracted grammar would include
many noisy rules with poor generalization power
and would suffer from overgeneration.
</bodyText>
<subsectionHeader confidence="0.999547">
5.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.994241333333333">
In this section we investigate the translation perfor-
mance of each hierarchical grammar, as defined by
rules obtained from three rule extraction methods:
</bodyText>
<listItem confidence="0.918160333333333">
• Viterbi union (V-union). Standard rule extrac-
tion from the union of the source-to-target and
target-to-source alignment link sets.
</listItem>
<page confidence="0.927684">
551
</page>
<listItem confidence="0.999315285714286">
• Word Posteriors (WP-st). Extraction based
on word posteriors as described in Section
3.1.1. The posteriors are provided by the
source-to-target alignment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 2.
• Phrase Posteriors (PP-st). Extraction based
</listItem>
<bodyText confidence="0.983204985294118">
on phrase alignment posteriors, as described
in Section 3.1.2, with fractional counts pro-
portional to the phrase probability under the
source-to-target alignment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 0.2.
Table 2 reports the translation results, as well as
the number of extracted rules in each case. It also
shows the following decoding statistics as measured
on the tune-nw set: decoding time in seconds per in-
put word, and number of instances of search pruning
(described in Section 5) per input word.
As a contrast, we extract rules according to the
heuristics introduced in (Chiang, 2007) and apply
the filters described in (Iglesias et al., 2009) to gen-
erate a standard hierarchical phrase-based grammar
GH. This uses rules with up to two nonadjacent non-
terminals, but excludes identical rule types such as
X→(w X,w X) or X→(w X1 w X2,w X1 w X2),
which were reported to cause computational difficul-
ties without a clear improvement in translation (Igle-
sias et al., 2009).
Grammar expressivity. As expected, for the stan-
dard extraction method (see rows entitled V-union),
grammar G1 is shown to underperform all other
grammars due to its structural limitations. On the
other hand, grammar G2 obtains much better scores,
nearly generating the same translation quality as
the baseline grammar GH. Finally, G3 does not
prove able to outperform G2, which suggests that
the phrase-disjoint rules with one nonterminal are
redundant for the translation grammar.
Rule extraction method. For all grammars, we
find that the proposed extraction methods based on
alignment posteriors outperform standard Viterbi-
based extraction, with improvements ranging from
0.5 to 1.1 BLEU points for test-nw1 (depending on
the grammar) and from 1.0 to 1.5 for test-nw2. In
all cases, the use of phrase posteriors PP is the best
option. Interestingly, we find that G2 extracted with
WP and PP methods outperforms the more complex
GH grammar as obtained from Viterbi alignments.
Rule set statistics. For grammar G2 evaluated
on the tune-nw set, standard Viterbi-based extrac-
tion produces 0.7M rules, whereas the WP and PP
extraction methods yield 1.1M and 1.2M rules re-
spectively. We further analyse the sets of rules
X→(-y,α,∼) in terms of the number of distinct
source and target sequences -y and α which are ex-
tracted. Viterbi extraction yields 82k distinct source
sequences whereas the WP and PP methods yield
116k and 146k sequences, respectively. In terms
of the average number of target sequences for each
source sequence, Viterbi extraction yields an aver-
age of 8.7 while WP and PP yield 9.7 and 8.4 rules
on average. This shows that method PP yields wider
coverage but with sharper forward rule translation
probability distributions than method WP, as the av-
erage number of translations per rule is determined
by the p(α|-y) &gt; 0.01 threshold mentioned in Sec-
tion 3.2.
Decoding time and pruning in search. In connec-
tion to the previous comments, we find an increased
need for search pruning, and subsequently slower
decoding speed, as the search space grows larger
with methods WP and PP. A larger search space is
created by the larger rule sets, which allows the sys-
tem to generate new hypotheses of better quality.
</bodyText>
<subsectionHeader confidence="0.994414">
5.3 Rule Concatenation in Grammar G2
</subsectionHeader>
<bodyText confidence="0.999773166666667">
In Section 4.1 we described a strategy to reduce
grammar redundancy by introducing an additional
nonterminal M for monotonic concatenation rules.
We find that without this distinction among nonter-
minals, search pruning and decoding time are in-
creased by a factor of 1.5, and there is a slight degra-
dation in BLEU (∼0.2) as more search errors are in-
troduced.
Another relevant aspect of this grammar is the ac-
tual rule type selected for monotonic concatenation.
We described using type X→(w X,w X) (con-
catenation on the right), but one could also include
X→(X w,X w) (concatenation on the left), or both,
for the same purpose. We evaluated the three alter-
natives and found that scores are identical when ei-
ther including right or left concatenation types, but
including both is harmful for performance, as the
need to prune and decoding time increase by a fac-
</bodyText>
<page confidence="0.991511">
552
</page>
<table confidence="0.941258764705882">
tor of 5 and 4, respectively, and we observe again a
slight degradation in performance.
Rule Extraction tune-nw test-nw1 test-nw2
V-st 34.7 35.6 37.5
V-ts 34.0 34.8 36.6
V-union 34.5 35.4 37.2
V-gdf 34.4 35.3 37.1
WP-st 35.1 36.0 37.7
WP-ts 34.5 35.0 37.0
PP-st 35.5 36.4 38.2
PP-ts 34.8 35.3 37.2
PP-merge 35.5 36.4 38.4
PP-merge-MERT 35.5 36.4 38.3
LMBR(V-st) 35.0 35.8 38.4
LMBR(V-st,V-ts) 35.5 36.3 38.9
LMBR(PP-st) 36.1 36.8 38.8
LMBR(PP-st,PP-ts) 36.4 36.9 39.3
</table>
<tableCaption confidence="0.94808">
Table 3: Translation results under grammar G2 with indi-
vidual rule sets, merged rule sets, and rescoring and sys-
tem combination with lattice-based MBR (lower-cased
BLEU shown)
</tableCaption>
<subsectionHeader confidence="0.993464">
5.4 Symmetrizing Alignments of Parallel Text
</subsectionHeader>
<bodyText confidence="0.999969204545455">
In this section we investigate extraction from align-
ments (and posterior distributions) over parallel text
which are generated using alignment models trained
in the source-to-target (st) and target-to-source (ts)
directions. Our motivation is that symmetrization
strategies have been reported to be beneficial for
Viterbi extraction methods (Och and Ney, 2003;
Koehn et al., 2003).
Results are shown in Table 3 for grammar G2. We
find that rules extracted under the source-to-target
alignment models (V-st, WP-st and PP-st) consis-
tently perform better than the V-ts, WP-ts and PP-
ts cases. Also, for Viterbi extraction we find that the
source-to-target V-st case performs better than any
of the symmetrization strategies, which contradicts
previous findings for non-hierarchical phrase-based
systems(Koehn et al., 2003).
We use the PP rule extraction method to extract
two sets of rules, under the st and ts alignment mod-
els respectively. We now investigate two ways of
merging these sets into a single grammar for trans-
lation. The first strategy is PP-merge and merges
both rule sets by assigning to each rule the maximum
count assigned by either alignment model. We then
extend the previous strategy by adding three binary
feature functions to the system, indicating whether
the rule was extracted under the ’st’ model, the ’ts’
model or both. The motivation is that MERT can
weight rules differently according to the alignment
model they were extracted from. However, we do
not find any improvement with either strategy.
Finally, we use linearised lattice minimum Bayes-
risk decoding (Tromble et al., 2008; Blackwood et
al., 2010) to combine translation lattices (de Gis-
pert et al., 2010) as produced by rules extracted
under each alignment direction (see rows named
LMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)). Gains
are consistent when comparing this to applying
LMBR to each of the best individual systems (rows
named LMBR(V-st) and LMBR(PP-st)). Overall,
the best-performing strategy is to extract two sets of
translation rules under the phrase pair posteriors in
each translation direction, and then to perform trans-
lation twice and merge the results.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999864421052632">
Rule extraction based on alignment posterior proba-
bilities can generate larger rule sets. This results in
grammars with more expressive power, as measured
by the ability to align parallel sentences. Assign-
ing counts equal to phrase posteriors produces bet-
ter estimation of rule translation probabilities. This
results in improved translation scores as the search
space grows.
This more exhaustive rule extraction method per-
mits a grammar simplification, as expressed by the
phrase movement allowed by its rules. In particular
a simple grammar with rules of only one nontermi-
nal is shown to outperform a more complex gram-
mar built on rules extracted from Viterbi alignments.
Finally, we find that the best way to exploit align-
ment models trained in each translation direction is
to extract two rule sets based on alignment posteri-
ors, translate the input independently with each rule
set and combine translation output lattices.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993942">
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
</bodyText>
<page confidence="0.996397">
553
</page>
<bodyText confidence="0.491696">
Agency, Contract No. HR0011- 06-C-0022.
</bodyText>
<sectionHeader confidence="0.977076" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99995689">
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010. Efficient Path Counting Transducers
for Minimum Bayes-Risk Decoding of Statistical Ma-
chine Translation Lattices. In Proceedings of ACL,
Short Papers, pages 27–32.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Ad-
vances in Neural Information Processing Systems, vol-
ume 21, pages 161–168.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the ACL, pages
782–790.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858–867.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263–311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient Bilin-
gual Chart Parsing. In Proceedings of IWSLT, pages
136–143.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, Short Papers, pages 25–28.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494–507.
Yonggang Deng. 2005. Bitext Alignment for Statisti-
cal Machine Translation. Ph.D. thesis, Johns Hopkins
University.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the EACL, pages 380–388.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 48–54.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang Macherey.
2007. Improving word alignment with bridge lan-
guages. In Proceedings ofEMNLP-CoNLL, pages 42–
50.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP, pages 1017–
1026.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505–512.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP, pages 133–139.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160–167.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proceedings of
the HLT-NAACL, pages 118–126.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from stochastic
inversion transduction grammars. In Proceedings of
the HLT-NAACL Workshop on Syntax and Structure in
Statistical Translation, pages 28–36.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-HLT, pages 577–585.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620–629.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings ofACL, pages 319–
326.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in mt training. In Proceedings
ofAMTA, pages 192–201.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145–1152.
</reference>
<page confidence="0.998715">
554
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.394919">
<title confidence="0.992334">Hierarchical Phrase-based Translation Grammars Extracted Alignment Posterior Probabilities</title>
<author confidence="0.874454">Adri`a de_Gispert</author>
<author confidence="0.874454">Juan Pino</author>
<author confidence="0.874454">William Machine Intelligence</author>
<affiliation confidence="0.912312">Department of Engineering, University of</affiliation>
<note confidence="0.569491">Trumpington Street, CB2 1PZ,</note>
<abstract confidence="0.999829461538462">We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignment model. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteriors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL, Short Papers,</booktitle>
<pages>27--32</pages>
<marker>Blackwood, de Gispert, Byrne, 2010</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, and William Byrne. 2010. Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices. In Proceedings of ACL, Short Papers, pages 27–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian Synchronous Grammar Induction.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>21</volume>
<pages>161--168</pages>
<contexts>
<context position="6849" citStr="Blunsom et al., 2008" startWordPosition="1043" endWordPosition="1046">. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of good-quality translation rules from a parallel corpus. Rules are of the form X→(&apos;y,α,—) , where &apos;y, α E {X U T}+ are the source and target sides of the rule, T denotes the set of terminals (words) and — is a b</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. Bayesian Synchronous Grammar Induction. In Advances in Neural Information Processing Systems, volume 21, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>782--790</pages>
<contexts>
<context position="6872" citStr="Blunsom et al., 2009" startWordPosition="1047" endWordPosition="1050">) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of good-quality translation rules from a parallel corpus. Rules are of the form X→(&apos;y,α,—) , where &apos;y, α E {X U T}+ are the source and target sides of the rule, T denotes the set of terminals (words) and — is a bijective function1 rela</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the ACL, pages 782–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="21806" citStr="Brants et al., 2007" startWordPosition="3578" endWordPosition="3581">guage pairs are aligned using MTTK (Deng and Byrne, 2008). For decoding we use HiFST, a lattice-based decoder implemented with Weighted Finite State Transducers (de Gispert et al., 2010). Likelihood-based search pruning is applied if the number of states in the lattice associated with each CYK grid cell exceeds 10,000, otherwise the entire search space is explored. The language model is a 4-gram language model estimated over the English side of the parallel text and the AFP and Xinhua portions of the English Gigaword Fourth Edition (LDC2009T13), interpolated with a zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram estimated using 6.6B words of English newswire text. In tuning the systems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU3 is performed on the development sets. 5.1 Measuring Expressive Power We measure the expressive power of the grammars described in the previous section by running the translation system in alignment mode (de Gispert et al., 2010) over the parallel corpus. Conceptually, this is equivalent to replacing the language model by the target sentence and seeing if the system is able to find any candidate. Here the weights assigned to the 3See ftp://</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP-ACL, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10395" citStr="Brown et al., 1993" startWordPosition="1646" endWordPosition="1649">xtracted rules, and consequently better relative frequency estimates over translations. 3.1 Ranking and Counting Functions We describe two alternative approaches to modify the functions fR and fC so that they incorporate the probabilities provided by the alignment models. 3.1.1 Word-to-word Alignment Posterior Probabilities Word-to-word alignment posterior probabilities p(lji|fJ1 , eI1) express how likely it is that the words in source position j and target position i are aligned given a sentence pair. These posteriors can be efficiently computed for Model 1, Model 2 and HMM, as described in (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2008). We will use these posteriors in functions to score phrase pairs. For a simple non-disjoint case (fj2 j1 , ei2 i1) we use: p(lji|fJ1 , eI1) (1) i2 − i1 + 1 which is very similar to the score used for lexical features in many systems (Koehn, 2010), with the link posteriors for the sentence pair playing the role of the Model 1 translation table. For a particular source phrase, Equation 1 is not a proper conditional probability distribution over all phrases in the target sentence. Therefore it cannot be used as such without further normalization. In</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5786" citStr="Chiang, 2007" startWordPosition="879" endWordPosition="880">t lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inf</context>
<context position="7537" citStr="Chiang, 2007" startWordPosition="1168" endWordPosition="1169">tly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of good-quality translation rules from a parallel corpus. Rules are of the form X→(&apos;y,α,—) , where &apos;y, α E {X U T}+ are the source and target sides of the rule, T denotes the set of terminals (words) and — is a bijective function1 relating source and target nonterminals X of each rule (Chiang, 2007). For each &apos;y, the probability over translations α is set by relative frequency over the extracted examples from the corpus. We take a general approach to rule extraction, as described by the following procedure. For simplicity we discuss the extraction of regular phrases, that is, rules of the form X→(w,w), where w E {T}+. Section 3.3 extends this procedure to rules with non1This function is defined if there are at least two nonterminals, and for clarity of presentation will be omitted in this paper 546 terminal symbols. Given a sentence pair (fJ1 , eI1), the extraction algorithm traverses th</context>
<context position="26677" citStr="Chiang, 2007" startWordPosition="4371" endWordPosition="4372"> described in Section 3.1.2, with fractional counts proportional to the phrase probability under the source-to-target alignment model. Alignment constraints and selection criteria of Section 3.2 are applied, with nobs = 0.2. Table 2 reports the translation results, as well as the number of extracted rules in each case. It also shows the following decoding statistics as measured on the tune-nw set: decoding time in seconds per input word, and number of instances of search pruning (described in Section 5) per input word. As a contrast, we extract rules according to the heuristics introduced in (Chiang, 2007) and apply the filters described in (Iglesias et al., 2009) to generate a standard hierarchical phrase-based grammar GH. This uses rules with up to two nonadjacent nonterminals, but excludes identical rule types such as X→(w X,w X) or X→(w X1 w X2,w X1 w X2), which were reported to cause computational difficulties without a clear improvement in translation (Iglesias et al., 2009). Grammar expressivity. As expected, for the standard extraction method (see rows entitled V-union), grammar G1 is shown to underperform all other grammars due to its structural limitations. On the other hand, grammar </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
<author>Bing Xiang</author>
</authors>
<title>Enriching SCFG Rules Directly From Efficient Bilingual Chart Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of IWSLT,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="6904" citStr="Cmejrek et al. (2009)" startWordPosition="1053" endWordPosition="1056">hrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of good-quality translation rules from a parallel corpus. Rules are of the form X→(&apos;y,α,—) , where &apos;y, α E {X U T}+ are the source and target sides of the rule, T denotes the set of terminals (words) and — is a bijective function1 relating source and target nontermin</context>
</contexts>
<marker>Cmejrek, Zhou, Xiang, 2009</marker>
<rawString>Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009. Enriching SCFG Rules Directly From Efficient Bilingual Chart Parsing. In Proceedings of IWSLT, pages 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite state transducers and shallow-n grammars.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchical phrase-based translation with weighted finite state transducers and shallow-n grammars. Computational Linguistics, 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proceedings of ACLHLT, Short Papers,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="6433" citStr="DeNero and Klein, 2008" startWordPosition="976" endWordPosition="979">008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different app</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of ACLHLT, Short Papers, pages 25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="5571" citStr="Deng and Byrne, 2008" startWordPosition="847" endWordPosition="850">e limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to ti</context>
<context position="10442" citStr="Deng and Byrne, 2008" startWordPosition="1654" endWordPosition="1657">tive frequency estimates over translations. 3.1 Ranking and Counting Functions We describe two alternative approaches to modify the functions fR and fC so that they incorporate the probabilities provided by the alignment models. 3.1.1 Word-to-word Alignment Posterior Probabilities Word-to-word alignment posterior probabilities p(lji|fJ1 , eI1) express how likely it is that the words in source position j and target position i are aligned given a sentence pair. These posteriors can be efficiently computed for Model 1, Model 2 and HMM, as described in (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2008). We will use these posteriors in functions to score phrase pairs. For a simple non-disjoint case (fj2 j1 , ei2 i1) we use: p(lji|fJ1 , eI1) (1) i2 − i1 + 1 which is very similar to the score used for lexical features in many systems (Koehn, 2010), with the link posteriors for the sentence pair playing the role of the Model 1 translation table. For a particular source phrase, Equation 1 is not a proper conditional probability distribution over all phrases in the target sentence. Therefore it cannot be used as such without further normalization. Indeed we find that this distribution is too shar</context>
<context position="21243" citStr="Deng and Byrne, 2008" startWordPosition="3487" endWordPosition="3490">am and portions of the newswire sections of MT02 through MT06. They contain 1,755 sentences and 1,671 sentences respectively. Results are also reported on a smaller held2See http://projects.ldc.upenn.edu/gale/data/catalog.html. We excluded the UN material and the LDC2002E18, LDC2004T08, LDC2007E08 and CUDonga collections. Figure 2: Percentage of parallel sentences successfully aligned for various extraction methods and grammars. out test set test-nw2, containing 60% of the NIST newswire portion of MT06, that is, 369 sentences. The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne, 2008). For decoding we use HiFST, a lattice-based decoder implemented with Weighted Finite State Transducers (de Gispert et al., 2010). Likelihood-based search pruning is applied if the number of states in the lattice associated with each CYK grid cell exceeds 10,000, otherwise the entire search space is explored. The language model is a 4-gram language model estimated over the English side of the parallel text and the AFP and Xinhua portions of the English Gigaword Fourth Edition (LDC2009T13), interpolated with a zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram estimated using 6.6B words of</context>
</contexts>
<marker>Deng, Byrne, 2008</marker>
<rawString>Yonggang Deng and William Byrne. 2008. HMM word and phrase alignment for statistical machine translation. IEEE Transactions on Audio, Speech, and Language Processing, 16(3):494–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
</authors>
<title>Bitext Alignment for Statistical Machine Translation.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="12800" citStr="Deng, 2005" startWordPosition="2071" endWordPosition="2072">) fR(fj2 j1 , ei2 i1) = j2 i2 � i=i1 ri j=j1 (2) 547 G0 G1 G2 G3 5—*(X,X) X—*(w X,X w) X—*(w X,X w) X—*(w X,X w) 5—*(5 X,5 X) X—*(X w,w X) X—*(X w,w X) X—*(X w,w X) X—*(w,w) X—*(w X,w X) X—*(w X,w X) X—*(w X w,w X w) Table 1: Hierarchical phrase-based grammars containing different types of rules. The grammar expressivity is greater as more types of rules are included. In addition to the rules shown in the respective columns, G1, G2 and G3 also contain the rules of G0. With IBM models 1 and 2, the numerator and denominator in Equation 2 can be computed in terms of posterior link probabilities (Deng, 2005). With the HMM model, the denominator is computed using the forward algorithm while the numerator can be computed using a modified forward algorithm (Deng, 2005). These phrase posteriors directly define a probability distribution over the alignments of translation candidates, so we use them both for ranking and scoring extracted rules, that is fR = fC = p. This approach assigns a fractional count to each extracted rule, which allows finer estimation of the forward and backward translation probability distributions. 3.2 Alignment Constraints and Selection Criteria In order to keep this process </context>
</contexts>
<marker>Deng, 2005</marker>
<rawString>Yonggang Deng. 2005. Bitext Alignment for Statistical Machine Translation. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>380--388</pages>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of the EACL, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="8854" citStr="Koehn et al., 2003" startWordPosition="1384" endWordPosition="1387">de sequences ei2 i1 as translation candidates. Each target-side sequence that satisfies the alignment constraints CA is ranked by the function fR. For practical reasons, a set of selection criteria CS is then applied to these ranked candidates and defines the set of translations of the source sequence that are extracted as rules. Each extracted rule is assigned a count fC. In this section we will explore variations of this rule extraction procedure involving alternative definitions of the ranking and counting functions, fR and fC, based on probabilities over alignment models. Common practice (Koehn et al., 2003) takes a set of word alignment links L and defines the alignment constraints CA so that there is a consistency between the links in the (fj2 j1 , ei2 i1) phrase pair. This is expressed by V(j, i) E L : (j E [j1, j2] n i E [i1, i2]) V (j E� [j1, j2] n i E� [i1, i2]). If these constraints are met, then alignment probabilities are ignored and fR = fC = 1. We call this extraction Viterbi-based, as the set of alignment links is generally obtained after applying a symmetrization heuristic to sourceto-target and target-to-source Viterbi alignments. In the following section we depart from this approac</context>
<context position="23739" citStr="Koehn et al., 2003" startWordPosition="3893" endWordPosition="3896">rements done on tune-nw set. rules are irrelevant, as only the ability of the grammar to create a desired hypothesis is important. We compare the percentage of target sentences that can be successfully produced by grammars G0, G1, G2 and G3 for the following extraction methods: • Viterbi (V). This is the standard extraction method based on a set of alignment links. We distinguish four cases, depending on the model used to obtain the set of links: source-totarget (V-st), target-to-source (V-ts), and two common symmetrization strategies: union (Vunion) and grow-diag-final (V-gdf), described in (Koehn et al., 2003). • Word Posteriors (WP). The extraction method is based on word alignment posteriors described in Section 3.1.1. These rules can be obtained either from the posteriors of the sourceto-target (WP-st) or the target-to-source (WPts) alignment models. We apply the alignment constraints and selection criteria described in Section 3.2. We do not report alignment percentages when using phrase posteriors (as described in Section 3.1.2) as they are roughly identical to the WP case. • Finally, in both cases, we also report results when merging the extracted rules in both directions into a single rule s</context>
<context position="31253" citStr="Koehn et al., 2003" startWordPosition="5113" endWordPosition="5116">s) 36.4 36.9 39.3 Table 3: Translation results under grammar G2 with individual rule sets, merged rule sets, and rescoring and system combination with lattice-based MBR (lower-cased BLEU shown) 5.4 Symmetrizing Alignments of Parallel Text In this section we investigate extraction from alignments (and posterior distributions) over parallel text which are generated using alignment models trained in the source-to-target (st) and target-to-source (ts) directions. Our motivation is that symmetrization strategies have been reported to be beneficial for Viterbi extraction methods (Och and Ney, 2003; Koehn et al., 2003). Results are shown in Table 3 for grammar G2. We find that rules extracted under the source-to-target alignment models (V-st, WP-st and PP-st) consistently perform better than the V-ts, WP-ts and PPts cases. Also, for Viterbi extraction we find that the source-to-target V-st case performs better than any of the symmetrization strategies, which contradicts previous findings for non-hierarchical phrase-based systems(Koehn et al., 2003). We use the PP rule extraction method to extract two sets of rules, under the st and ts alignment models respectively. We now investigate two ways of merging the</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10689" citStr="Koehn, 2010" startWordPosition="1704" endWordPosition="1705">t Posterior Probabilities Word-to-word alignment posterior probabilities p(lji|fJ1 , eI1) express how likely it is that the words in source position j and target position i are aligned given a sentence pair. These posteriors can be efficiently computed for Model 1, Model 2 and HMM, as described in (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2008). We will use these posteriors in functions to score phrase pairs. For a simple non-disjoint case (fj2 j1 , ei2 i1) we use: p(lji|fJ1 , eI1) (1) i2 − i1 + 1 which is very similar to the score used for lexical features in many systems (Koehn, 2010), with the link posteriors for the sentence pair playing the role of the Model 1 translation table. For a particular source phrase, Equation 1 is not a proper conditional probability distribution over all phrases in the target sentence. Therefore it cannot be used as such without further normalization. Indeed we find that this distribution is too sharp and overemphasises short phrases, so we use fC = 1. However, it does allow us to rank target phrases as possible translations. In contrast to the common extraction procedure described in the previous section, the ranking approach described here </context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Improving word alignment with bridge languages.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL,</booktitle>
<pages>42--50</pages>
<contexts>
<context position="5548" citStr="Kumar et al., 2007" startWordPosition="843" endWordPosition="846">viously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We als</context>
</contexts>
<marker>Kumar, Och, Macherey, 2007</marker>
<rawString>Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2007. Improving word alignment with bridge languages. In Proceedings ofEMNLP-CoNLL, pages 42– 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Tian Xia</author>
<author>Xinyan Xiao</author>
<author>Qun Liu</author>
</authors>
<title>Weighted alignment matrices for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1017--1026</pages>
<contexts>
<context position="5215" citStr="Liu et al. (2009)" startWordPosition="795" endWordPosition="798">t rule extraction from posteriors gives translation improvements. We also find that the best way to exploit source-to-target and targetto-source alignment models is to build two separate systems and combine their output translation lattices. Section 6 presents the main conclusions of this work. 2 Related Work Some authors have previously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) </context>
<context position="14064" citStr="Liu et al. (2009)" startWordPosition="2290" endWordPosition="2293">traints are needed. In order to extract a phrase pair (fj2 j1 , ei2 ), we define i1 the following: • CA requires at least one pair of positions (j, i) : (j E [j1, j2] n i E [i1, i2]) with word-to-word link posterior probability p(lji|f&apos;1 , ei) &gt; 0.5, and that there is no pair of positions (j, i) : (j E [j1,j2]ni E� [i1,i2])V(j E� [j1,j2]ni E [i1,i2]) with p(lji|f1 , ei) &gt; 0.5 • Cs allows only the k best translation candidates to be extracted. We use k = 3 for regular phrases, and k = 2 for hierarchical rules. Note that we do not discard rules according to their scores fC at this point (unlike Liu et al. (2009)), since we prefer to add all phrases from all sentence pairs before carrying out such filtering steps. Once all rules over the entire collection of parallel sentences have been extracted, we require each rule to occur at least nobs times and with a forward translation probability p(α|-y) &gt; 0.01 to be used for translation. 3.3 Extraction of Rules with Nonterminals Extending the procedure previously described to the case of more complex hierarchical rules including one or even two nonterminals is conceptually straightforward. It merely requires that we traverse the source and target sentences a</context>
</contexts>
<marker>Liu, Xia, Xiao, Liu, 2009</marker>
<rawString>Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009. Weighted alignment matrices for statistical machine translation. In Proceedings of EMNLP, pages 1017– 1026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="5814" citStr="Lopez (2008)" startWordPosition="883" endWordPosition="884">et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNe</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Tera-scale translation models via pattern matching. In Proceedings of COLING, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="6252" citStr="Marcu and Wong (2002)" startWordPosition="951" endWordPosition="954">d make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; B</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of EMNLP, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="31232" citStr="Och and Ney, 2003" startWordPosition="5109" endWordPosition="5112">8.8 LMBR(PP-st,PP-ts) 36.4 36.9 39.3 Table 3: Translation results under grammar G2 with individual rule sets, merged rule sets, and rescoring and system combination with lattice-based MBR (lower-cased BLEU shown) 5.4 Symmetrizing Alignments of Parallel Text In this section we investigate extraction from alignments (and posterior distributions) over parallel text which are generated using alignment models trained in the source-to-target (st) and target-to-source (ts) directions. Our motivation is that symmetrization strategies have been reported to be beneficial for Viterbi extraction methods (Och and Ney, 2003; Koehn et al., 2003). Results are shown in Table 3 for grammar G2. We find that rules extracted under the source-to-target alignment models (V-st, WP-st and PP-st) consistently perform better than the V-ts, WP-ts and PPts cases. Also, for Viterbi extraction we find that the source-to-target V-st case performs better than any of the symmetrization strategies, which contradicts previous findings for non-hierarchical phrase-based systems(Koehn et al., 2003). We use the PP rule extraction method to extract two sets of rules, under the st and ts alignment models respectively. We now investigate tw</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="21915" citStr="Och, 2003" startWordPosition="3598" endWordPosition="3599">ed with Weighted Finite State Transducers (de Gispert et al., 2010). Likelihood-based search pruning is applied if the number of states in the lattice associated with each CYK grid cell exceeds 10,000, otherwise the entire search space is explored. The language model is a 4-gram language model estimated over the English side of the parallel text and the AFP and Xinhua portions of the English Gigaword Fourth Edition (LDC2009T13), interpolated with a zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram estimated using 6.6B words of English newswire text. In tuning the systems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU3 is performed on the development sets. 5.1 Measuring Expressive Power We measure the expressive power of the grammars described in the previous section by running the translation system in alignment mode (de Gispert et al., 2010) over the parallel corpus. Conceptually, this is equivalent to replacing the language model by the target sentence and seeing if the system is able to find any candidate. Here the weights assigned to the 3See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl 80 70 60 50 40 30 G0 G1 G2 G3 550 Grammar Extraction # Rules </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Unsupervised syntactic alignment with inversion transduction grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the HLT-NAACL,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="6640" citStr="Pauls et al. (2010)" startWordPosition="1009" endWordPosition="1012">patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of </context>
</contexts>
<marker>Pauls, Klein, Chiang, Knight, 2010</marker>
<rawString>Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Proceedings of the HLT-NAACL, pages 118–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrasebased translation via word alignments from stochastic inversion transduction grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>28--36</pages>
<marker>Saers, Wu, 2009</marker>
<rawString>Markus Saers and Dekai Wu. 2009. Improving phrasebased translation via word alignments from stochastic inversion transduction grammars. In Proceedings of the HLT-NAACL Workshop on Syntax and Structure in Statistical Translation, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="6140" citStr="Shen et al., 2008" startWordPosition="934" endWordPosition="937">, 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="32479" citStr="Tromble et al., 2008" startWordPosition="5310" endWordPosition="5313">s into a single grammar for translation. The first strategy is PP-merge and merges both rule sets by assigning to each rule the maximum count assigned by either alignment model. We then extend the previous strategy by adding three binary feature functions to the system, indicating whether the rule was extracted under the ’st’ model, the ’ts’ model or both. The motivation is that MERT can weight rules differently according to the alignment model they were extracted from. However, we do not find any improvement with either strategy. Finally, we use linearised lattice minimum Bayesrisk decoding (Tromble et al., 2008; Blackwood et al., 2010) to combine translation lattices (de Gispert et al., 2010) as produced by rules extracted under each alignment direction (see rows named LMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)). Gains are consistent when comparing this to applying LMBR to each of the best individual systems (rows named LMBR(V-st) and LMBR(PP-st)). Overall, the best-performing strategy is to extract two sets of translation rules under the phrase pair posteriors in each translation direction, and then to perform translation twice and merge the results. 6 Conclusion Rule extraction based on alignment poste</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proceedings of EMNLP, pages 620–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Effective phrase translation extraction from alignment models.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="5528" citStr="Venugopal et al., 2003" startWordPosition="838" endWordPosition="842">rk Some authors have previously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen e</context>
<context position="10419" citStr="Venugopal et al., 2003" startWordPosition="1650" endWordPosition="1653">consequently better relative frequency estimates over translations. 3.1 Ranking and Counting Functions We describe two alternative approaches to modify the functions fR and fC so that they incorporate the probabilities provided by the alignment models. 3.1.1 Word-to-word Alignment Posterior Probabilities Word-to-word alignment posterior probabilities p(lji|fJ1 , eI1) express how likely it is that the words in source position j and target position i are aligned given a sentence pair. These posteriors can be efficiently computed for Model 1, Model 2 and HMM, as described in (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2008). We will use these posteriors in functions to score phrase pairs. For a simple non-disjoint case (fj2 j1 , ei2 i1) we use: p(lji|fJ1 , eI1) (1) i2 − i1 + 1 which is very similar to the score used for lexical features in many systems (Koehn, 2010), with the link posteriors for the sentence pair playing the role of the Model 1 translation table. For a particular source phrase, Equation 1 is not a proper conditional probability distribution over all phrases in the target sentence. Therefore it cannot be used as such without further normalization. Indeed we find that this d</context>
</contexts>
<marker>Venugopal, Vogel, Waibel, 2003</marker>
<rawString>Ashish Venugopal, Stephan Vogel, and Alex Waibel. 2003. Effective phrase translation extraction from alignment models. In Proceedings ofACL, pages 319– 326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Wider pipelines: N-best alignments and parses in mt training.</title>
<date>2008</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="5067" citStr="Venugopal et al. (2008)" startWordPosition="771" endWordPosition="774"> In Section 5 we assess these grammars in terms of their expressive power and the quality of the translations they yield in Chinese-toEnglish, showing that rule extraction from posteriors gives translation improvements. We also find that the best way to exploit source-to-target and targetto-source alignment models is to build two separate systems and combine their output translation lattices. Section 6 presents the main conclusions of this work. 2 Related Work Some authors have previously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with rel</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2008</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2008. Wider pipelines: N-best alignments and parses in mt training. In Proceedings ofAMTA, pages 192–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz J Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1145--1152</pages>
<contexts>
<context position="5891" citStr="Zollmann et al. (2008)" startWordPosition="894" endWordPosition="897">ces that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Franz J. Och, and Jay Ponte. 2008. A systematic comparison of phrasebased, hierarchical and syntax-augmented statistical MT. In Proceedings of COLING, pages 1145–1152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>