<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.657666">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.920005">
The 1981 ACM Annual Conference will be held at
the Bonaventure Hotel in Los Angeles, California,
November 9-11, 1981. [See AJCL 6:3-4, pg. 194.]
For further information contact:
</bodyText>
<note confidence="0.734274">
Mrs. A.C. Toni Shelter
</note>
<address confidence="0.312522">
Xerox Corporation, A3-49
701 South Aviation Boulevard
El Segundo, California 90245
(213) 679-4511 x1968
</address>
<bodyText confidence="0.956637142857143">
The ACL is sponsoring three sessions on
&amp;quot;Computer Modeling of Linguistic Theory&amp;quot; in con-
junction with the Annual Meeting of the Linguistic
Society of America which will be held in New York
City at the Grand Hyatt Hotel, December 28-30,
1981. [See AJCL 7:1, pg. 49.] For further informa-
tion contact:
</bodyText>
<figure confidence="0.2231145">
Stan Petrick
IBM T.J. Watson Research Center
P.O. Box 218
Yorktown Heights, New York 10598
or: Terry Langendoen
CUNY Graduate Center
33 West 42nd Street
New York, New York 10036
</figure>
<title confidence="0.365192333333333">
Abstracts of Current Literature
The State-of-the-Art in Natural Language
Understanding
</title>
<author confidence="0.678956">
David L. Waltz
</author>
<affiliation confidence="0.660087">
Coordinated Science Laboratory
University of Illinois
Urbana, Illinois 61801
</affiliation>
<subsubsectionHeader confidence="0.377778">
Working Paper WP-27, Dec. 1980, 35 pages.
</subsubsectionHeader>
<bodyText confidence="0.999649333333334">
Research in computer understanding of natural
language has led to the construction of programs
which can handle a number of different types of lan-
guage, including questions about the contents of data
bases, stories and news articles, dialogues, and scene
descriptions. This research draws on and has in turn
had an affect on many other research areas, including
software engineering, linguistics, psychology, philoso-
phy, and knowledge representation. This paper pro-
vides a brief history and overview of the field, along
with examples and explanations of the operation of
several natural language understanding programs. The
limitations of our current technology are discussed,
and assessments are given of the most promising cur-
rent research directions.
</bodyText>
<note confidence="0.369437">
Evaluation of Natural Language Processors
Harry R. Tennant
Coordinated Science Laboratory
University of Illinois
Urbana, Illinois 61801
</note>
<subsectionHeader confidence="0.423049">
Technical Report T-103, Nov. 1980, 247 pages.
</subsectionHeader>
<bodyText confidence="0.999871">
Despite a large amount of research on developing
natural language understanding problems, little work
has been done on evaluating their performance or
potential. The evaluations that have been done have
been unsystematic and incomplete. This has led to
uncertainty and confusion over the accomplishments of
natural language processing research.
The lack of evaluation can be primarily attributed
to the difficulty of the problem. The desired behavior
of natural language processors has not been clearly
specified. Partial progress toward the eventual goals
for natural language processors has not been delineat-
ed, much less measured.
This thesis attempts to clarify some of the difficul-
ties behind evaluating the performance of natural lan-
guage processors. It also proposes an evaluation me-
thod that is designed to be systematic and thorough.
The method relies on considering a natural language
processor from three viewpoints in the light of several
taxonomies of issues relevant to natural language proc-
essing. Finally, an evaluation is described of
PLANES, a natural language database query system.
</bodyText>
<subsectionHeader confidence="0.7070035">
A Program Conversing In Portuguese
Providing a Library Service
Helder Coelho
Centro de Informatica
</subsectionHeader>
<bodyText confidence="0.282887">
Laboratorio Nacional de Engenharia Civil
101, Av. do Brasil
</bodyText>
<sectionHeader confidence="0.346692" genericHeader="method">
1799 Lisboa Codex, PORTUGAL
</sectionHeader>
<subsectionHeader confidence="0.326567">
Edinburgh Ph.D. Thesis, Dec. 1979.
</subsectionHeader>
<bodyText confidence="0.999944555555556">
TUGA is a program which converses in Portuguese
to provide a library service covering the field of Artifi-
cial Intelligence. The objective of designing TUGA
was the development of a feasible method for consult-
ing and creating data bases in natural Portuguese. The
resulting program allows dialogues where the program
and its users behave in the way humans normally do in
a dialogue setting. The program can answer, and
question, in pre-defined scenarios. Users can ques-
tion, answer and issue commands in a natural and
convenient way, without bothering excessively with
the form of the dialogues and sentences.
The original contributions of this work are: the
treatment of dialogues, the adaptation of Colmerauer&apos;s
natural language framework to Portuguese, the partic-
ular method for evaluating the logical structures in-
volved in Colmerauer&apos;s framework, and the library
service application itself. The program is implemented
</bodyText>
<page confidence="0.509529">
126 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
</page>
<note confidence="0.898254">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.998532333333333">
in PROLOG, a simple and powerful programming
language essentially identical in syntax and semantics
to a subset of predicate calculus in clausal form.
</bodyText>
<subsectionHeader confidence="0.795618">
Semantic Grammar and Meaning
Representation Language in a Natural
Language Question and Answer System
</subsectionHeader>
<author confidence="0.725684">
C. Rathke, B. Sonntag, and W. Schopper
</author>
<subsectionHeader confidence="0.624645">
Institut far Informatik
Universitat Stuttgart
D-7000 Stuttgart 1, WEST GERMANY
</subsectionHeader>
<bodyText confidence="0.97093025">
Angew. Inf. 22, 4 (April 1980), 155-157 (In German).
We describe a natural language question-answering
system. Questions can be put to the system in written
natural language about the world of German soccer
teams and the people employed by them. A so-called
semantic grammar is introduced which is especially
designed to cover a large set of possible questions
about the world mentioned. An expression of a formal
Meaning Representation Language (MRL) is produced
by the system. Applied to the database, which is or-
ganized in a semantic net, it generates the appropriate
answer.
</bodyText>
<sectionHeader confidence="0.5232075" genericHeader="method">
A D-LADDER User&apos;s Guide
Daniel Sagalowiez
</sectionHeader>
<subsectionHeader confidence="0.5803585">
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<bodyText confidence="0.980323368421053">
Technical Note 224, Aug. 1980.
D-LADDER (DIAMOND-based language Access to
Distributed Data with Error Recovery) is a computer
system designed to provide answers to questions posed
at the terminal in a subset of natural language infor-
mation. The system accepts natural-language ques-
tions about the data. For each question D-LADDER
plans a sequence of appropriate queries to the data
base management system, determines on which ma-
chines the queries are to be processed, establishes
links to those machines over the ARPANET, monitors
the processing of the queries and recovers from certain
errors in execution, and prepares a relevant answer to
the original text.
This user&apos;s guide is intended for the person who
knows how to log in to the host operating system, as
well as how to enter and edit a line of text. It does
not explain how D-LADDER works, but rather how to
use it on a demonstration basis.
</bodyText>
<subsectionHeader confidence="0.986685714285714">
Interpreting Discourse: Coherence and the
Analysis of Ethnographic Interviews
Michael Agar and Jerry Hobbs
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<bodyText confidence="0.989668583333333">
Technical Note 225, Aug. 1980.
Practioners of ethnography, in seeking to discover
and describe complex patterns of behavior, face a
number of serious problems. First, the patterns should
be described in as formal a fashion as possible, and
yet the formalism that ethnographers have availed
themselves of are simply inadequate to the task. Sec-
ondly, data such as ethnographic interviews constitutes
the most common way of discovering a culture, but
there is a dearth of formal methods for going from a
text to the cultural presuppositions that underlie it.
Thirdly, it is difficult to know in ethnographic inter-
views how much of what is said is a reflection of the
culture, how much is the speaker&apos;s personal interpreta-
tion, and how much is due to the interview situation
itself. Finally, ethnographers face the problem of how
to deal in depth with large volumes of ethnographic
data.
Artificial intelligence can be viewed in large part as
the investigation of complex formalisms. Heretofore,
these have been applied primarily in simple domains.
In this paper we attempt to use Al formalism as a
formal language of description for the complex conver-
sational behavior that occurs in ethnographic inter-
views. We thus address the first of the ethnographer&apos;s
problems by exploring the use of formalisms that begin
to be adequate to the task. Moreover, work on dis-
course analysis in the Al framework has sought to
characterize the structure of texts in terms of the goals
and beliefs of the speaker. It thus suggests methods of
using the structure of the text to force the explicitation
of the underlying belief system, addressing the second
of the ethnographer&apos;s problems. Finally, our work has
confronted us with the last two problems, and the
approach we are taking has suggested tentative ideas
for dealing with these problems.
</bodyText>
<subsectionHeader confidence="0.984781833333333">
Generalization and Memory in an
Integrated Understanding System
Michael Lebowitz
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.538724">
Research Report 186, Oct. 1980.
</subsubsectionHeader>
<bodyText confidence="0.98699205882353">
Generalization and memory are part of natural lan-
guage understanding. As people read stories describ-
ing various situations, they are able to recall similar
episodes from memory and use them as a basis to form
generalizations about the way such situations normally
American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 127
The FINITE STRING Newsletter Abstracts of Current Literature
occur. This thesis describes an integrated system for
language understanding, IPP (Integrated Partial Par-
ser), that encompasses the ability to generalize and
record information in long-term memory as well as
conceptual analysis.
IPP is a program that learns about the world by
reading stories taken from newspapers and the UPI
news wire, adding information from these stories to
memory, and making generalizations that describe
specific situations. It uses the generalizations that it
has made to help in understanding future stories.
As it reads stories, IPP adds them to its permanent
memory. If it locates similar stories in memory as it
does this, then it attempts to make generalizations that
describe the similarities among the events. Such gen-
eralizations form the basis for organizing events in
memory and understanding later stories. IPP also
includes a procedure for confirming generalizations as
further stories are read.
In order to analyze the text that it reads, IPP
makes extensive use of top-down, predictive process-
ing. As it processes a story, IPP accesses memory in
an attempt to identify generalizations describing ster-
eotypical situations that can provide predictions to be
used in understanding. Such use of memory to provide
top-down context results in a robust and efficient un-
derstanding system.
</bodyText>
<subsectionHeader confidence="0.945574857142857">
Retrieval and Organizational Strategies in
Conceptual Memory: A Computer Model
Janet Lynne Kolodner
Department of Computer Science
Yale University
New Haven, Connecticut 06520
Research Report 187, Nov. 1980.
</subsectionHeader>
<bodyText confidence="0.999706153846154">
People effortlessly recall past events and episodes in
their lives many times in the course of a normal day.
A reasonable goal in the design of computer programs
is to construct a memory with that same capability.
To facilitate human-like retrieval of events from a
computer memory, we must first specify a reasonable
memory organization. We must then design updating
and retrieval processes to build up and access that
information. This thesis will present such a theory,
and will describe a computer program called CYRUS
which implements that theory.
CYRUS (Computerized Yale Retrieval and Updat-
ing System) stores and retrieves episodes in the lives
of Secretaries of State Cyrus Vance and Edmund
Muskie. When new events are added to its memory,
CYRUS integrates them into memory along with the
events it already knows about. CYRUS can then an-
swer questions posed to it in English about the events
it stores.
The algorithms and memory organization used in
CYRUS have been developed by examining the way
people answer questions requiring extensive memory
search. Its reconstructive processes include instantia-
tion strategies, which construct and elaborate on con-
texts for search, and search strategies, which direct
construction.
Reconstructive processes require a vast store of
generalized knowledge in order to be applied. Recon-
structive retrieval implies a memory organization
which organizes both generalized information about
different types of events and distinguishing features of
particular events. CYRUS&apos; memory is self-organizing.
When given a new fact about Vance or Muskie, it
integrates the new event into its already-existing mem-
ory organization. In the process, it updates its gener-
alized information and indexes the new event in the
appropriate places. CYRUS can be seen as both a
model of human memory and an intelligent informa-
tion retrieval system.
</bodyText>
<subsectionHeader confidence="0.9259418">
Memory, Meaning, and Syntax
Roger C. Schank and Lawrence Birnbaum
Computer Science Department
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<bodyText confidence="0.96537">
Research Report 189, Nov. 1980.
This paper explores the role of syntax in computa-
tional theories of natural language. We discuss the
integrated processing hypothesis, which contends that
meaning and world knowledge play a crucial part in
language understanding even at the earliest points in
the process. The hypothesis implies that syntactic
knowledge plays no privileged role in language proc-
essing. Computer models of language analysis are
discussed in relation to the overall theory.
</bodyText>
<subsectionHeader confidence="0.965198833333333">
Word Expert Parsing: A Theory of Distributed
Word-Based Natural Language Understanding
Steven Small
Department of Computer Science
University of Maryland
College Park, Maryland 20742
</subsectionHeader>
<bodyText confidence="0.981283326086956">
Technical Report TR-954, Sept. 1980.
People have an incredible facility for organizing and
selecting word senses in arriving at the intended mean-
ing of sentences in context. The process takes place
with such unnoticed and subconscious ease that it has
been often overlooked in the study of language. To
those building computer programs to understand lan-
guage, however, this phenomenon represents a central
problem. While many syntactic constructions and
conceptual relations can be described through systems
of rewrite rules, the word sense selection problem
remains. The reason for this lies in the incompatibility
128 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
The FINITE STRING Newsletter Abstracts of Current Literature
of the sense discrimination problem and the rule-based
problem-solving method.
The computational theory of Word Expert Parsing
approaches natural language understanding as a non-
uniform distributed process of interacting words. An
expert process for each word actively pursues its in-
tended meaning in the context of other word experts
and real-world knowledge. The theory perceives un-
derstanding as a behavior of memory interactions, and
the computer model emphasizes process rather than
output structures.
The Lexical Interaction Language (LIL) formalizes
the interactions among individual word experts, and
the Sense Discrimination Language (SDL) specifies
their actions to determine intended word senses in
context. These languages constitute the formal theory
of Word Expert Parsing, and permit the representation
of all linguistic knowledge in terms of active word-
based distributed agents. An existing computer pro-
gram translates word experts represented in these lan-
guages into executable processes that interact to coop-
eratively analyze sentences.
The thesis makes a number of claims about the
processes of natural language comprehension and its
computational realization. A formal theory is devel-
oped and a computer model constructed to provide
evidence to support those claims. Word Expert Pars-
ing explains the understanding of sentences containing
highly ambiguous words and complex structures. The
distributed word-based approach is advanced as a
framework for a full-scale theory of discourse compre-
hension.
</bodyText>
<subsectionHeader confidence="0.920995833333333">
Understanding English Descriptions of Programs
Allan Ramsay
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
</subsectionHeader>
<subsubsectionHeader confidence="0.579619">
DAI Research Paper No. 140, 1980, 4 pages.
</subsubsectionHeader>
<bodyText confidence="0.998338">
A considerable amount of work has been done on
verifying that computer programs fit their specifica-
tions. However, providing formal specifications is
itself a difficult and tedious task, so that programs are
generally only documented incompletely and impre-
cisely. This paper presents a computer system which
accepts English descriptions of procedures and relates
them to LISP programs that are supposed to imple-
ment them. This system is intended to illustrate how
&amp;quot;informal&amp;quot; techniques may be used to provide a rough
analysis of a program for which incomplete specifica-
tions are provided.
</bodyText>
<subsectionHeader confidence="0.9051845">
Parsing English Text
Allan Ramsay
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
</subsectionHeader>
<subsubsectionHeader confidence="0.569958">
DAI Research Paper No. 139, 1980, 5 pages.
</subsubsectionHeader>
<bodyText confidence="0.999856285714286">
This paper presents a technique for parsing English
text according to a grammar specified as a set of rew-
rite rules. The paper describes a compact way of rep-
resenting such a grammar and presents a program
which uses this representation to parse text without
backtracking and without repeating work that it has
already done.
</bodyText>
<subsectionHeader confidence="0.989733">
Using Determinism to Predict Garden Paths
Rob Milne
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
</subsectionHeader>
<bodyText confidence="0.522581">
Edinburgh EH1 2QL SCOTLAND
</bodyText>
<subsubsectionHeader confidence="0.494712">
DAI Research Paper No. 142, 1980, 6 pages.
</subsubsectionHeader>
<bodyText confidence="0.999802923076923">
I am interested in making a psychologically valid
model of human natural language understanding, and
especially in a processing model for predicting when a
sentence will be a garden path. While extending the
Marcus deterministic parser to include noun-noun
modification, several counter examples to Marcus&apos;
garden path prediction were found. In this paper I
proposed that when people encounter an ambiguous
situation that may lead to a garden path, they use se-
mantics to decide rather than look ahead. I will pres-
ent an extension to the garden path prediction mecha-
nism of Marcus&apos; parser to account for this and several
experiments to test this theory.
</bodyText>
<subsectionHeader confidence="0.9977315">
Parsing Against Lexical Ambiguity
Rob Milne
Department of Artificial Intelligence
University of Edinburgh
</subsectionHeader>
<bodyText confidence="0.468619">
Edinburgh EH1 2QL SCOTLAND
</bodyText>
<subsubsectionHeader confidence="0.578956">
DAI Research Paper No. 144, 1980, 8 pages.
</subsubsectionHeader>
<bodyText confidence="0.999319555555556">
Marcus&apos; original deterministic parsing included al-
most no part-cf-speech ambiguity. In this paper, the
addition of part-of-speech ambiguity to a deterministic
parser written in PROLOG is described. To handle
this ambiguity, it was necessary to add no special
mechanisms to the parser. Instead the grammar rules
were made to enforce agreement, and reject ungram-
matical sentences. The resulting system is very effec-
tive and covers many examples of ambiguity.
</bodyText>
<table confidence="0.509188363636364">
American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 129
The FINITE STRING Newsletter Abstracts of Current Literature
Predictive Analysis in Sentence Comprehension:
A Computer Simulation Model for Surface
Structure Parsing
C.P. Whaley
Communications Psychology (Dept. 3Z11)
Bell-Northern Research
P.O. Box 3511
Station C
Ottawa, Ontario KIY 4H7 CANADA
</table>
<subsubsectionHeader confidence="0.413588">
Int. J. Man-Mach. Studies 13, 3 (Oct. 1980), 259-294.
</subsubsectionHeader>
<bodyText confidence="0.999805307692308">
Through numerous models have been proposed by
linguists and computer scientists for the parsing of
natural language, Kimball (1973) has outlined one of
the few that takes into account the operational limita-
tions of the human perceiver. His predictive analysis
model is intuitively appealing and is supported by em-
pirical research (e.g. Whaley, 1979). This paper pres-
ents the parsing principles suggested by Kimball, and
describes a computer simulation which incorporates
them. The parsing accuracy of the model is demon-
strated and discussed for various sentence types. Fi-
nally, extensions to the existing model are considered
including the quantification of the model&apos;s predictions.
</bodyText>
<note confidence="0.7969085">
The Use of Artificial Intelligence Techniques in
Computer-Assisted Instruction: An Overview
</note>
<author confidence="0.271424">
Alice Gable and Carl V. Page
</author>
<affiliation confidence="0.562046333333333">
Department of Computer Science
Michigan State University
East Lansing, Michigan 48824
</affiliation>
<subsubsectionHeader confidence="0.623728">
Int. J. Man-Mach. Studies 12, 3 (April 1980), 259-282.
</subsubsectionHeader>
<bodyText confidence="0.999401714285714">
One of the major goals of research in Artificial
Intelligence is the representation of knowledge so that
a computer can solve problems or communicate in a
manner which exhibits &amp;quot;common sense&amp;quot;. Few com-
puter programs, including those for education, possess
behavior which approaches any facet of the constella-
tion of human skills and knowledge which are impre-
cisely called &amp;quot;common sense&amp;quot;. However, the revolu-
tionary decline in hardware costs now makes it possi-
ble to consider economically viable, sophisticated de-
signs for computer-aided instruction systems possess-
ing some of the common sense attributes of a human
tutor.
In this survey we examine, in depth, techniques
from Artificial Intelligence that can be used to endow
a Computer-Aided Instruction system with approxima-
tions of some of the desirable qualities of a human
tutor. We consider both techniques which have been
proved in prototype systems for Computer-Aided In-
struction and some techniques which were originally
developed for other purposes.
</bodyText>
<subsectionHeader confidence="0.4224335">
The Structure of The Merriam-Webster
Pocket Dictionary
Robert A. Amsler
Department of Computer Sciences
University of Texas
Austin, Texas 78712
</subsectionHeader>
<bodyText confidence="0.977596486842106">
Technical Report TR-164, Dec. 1980.
The dissertation has as its purpose the exploration
and discussion of the structure of a machine-readable
copy of an ordinary pocket dictionary with particular
attention to the utility of the information contained
therein for future application in computational linguis-
tics, ethnosemantics, and information science. This
structure was first explored by hand using two
concordance-like printouts of the Merriam-Webster
Pocket Dictionary&apos;s contents prepared from magnetic
tapes produced by John Olney at System Development
Corporation.
Initially the goal of the analyses was the determina-
tion of whether useful semantic information could be
derived from dictionary definitions. Once it was de-
termined by several hand analyses that such data did
provide a new source of information about the lexicon,
the larger goal of assembling a complete taxonomy of
the words in the dictionary was conceived and under-
taken.
A hand analysis of the rich semantic information
contained in the dictionary for verbs defined in terms
of &amp;quot;move&amp;quot; is presented in Chapter 2. This componen-
tial analysis of a set of definitions revealed the poten-
tial of dictionaries for use as the basis of numerous
additional studies of high-frequency verbs based upon
their usage in defining more specific verbs.
Chapter 3 presents the results of a hand analysis of
the taxonomy of &amp;quot;vehicle&amp;quot; terms developed from a
large sample of definitions based upon the word
&amp;quot;vehicle&amp;quot; and its descendants. This study revealed
that indeed the dictionary did contain large, coherent,
and computationally useful information in a
taxonomic-like organization that could be revealed by
connecting together definitions on the basis of their
defining terms.
Chapter 4 deals with the steps involved in design-
ing, loading, and selectively accessing large databases
containing all the definitions of nouns, verbs, and ad-
jectives contained in the dictionary. Statistical infor-
mation on the frequencies and nature of the part of
speech data contained in the dictionary is given in
chapter 4 and appendix 3. A &amp;quot;word sense meaning&amp;quot;
representation for uniquely specifying dictionary sen-
ses and the problems of semantic ambiguity in lexical
usage are also illustrated.
In chapter 5, statistics on the frequency of diction-
ary defining vocabulary and measures of semantic
ambiguity are given. Section 5.2 contains an extensive
130 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
The FINITE STRING Newsletter Abstracts of Current Literature
discussion of the steps taken to perform semantic di-
sambiguation on dictionary definition terms along with
further statistics on the frequencies of such disambigu-
ated words in the dictionary. Appendix 4 gives part of
a disambiguation protocol session transcribed from a
tape-recording and demonstrating the nature of the
human disambiguation task carried out on the diction-
ary. Finally, chapter 5 concludes with a discussion of
the methods used to computationally assemble and
enumerate the complete taxonomic structure of the
dictionary&apos;s noun and verb definitions.
Chapter 6 discusses some of the findings about the
nature of the dictionary&apos;s taxonomies of nouns and
verbs. It discusses the manners in which dictionary
taxonomies, ultimately terminate in primitive root con-
cepts, relationships to other taxonomies, case argu-
ment relattpns to verbs, and in partitives and collec-
tives.
Finally, chapter 7 concludes with a discussion of a
possible means of automating the analysis procedures
of chapter 4 and 5 to perform fully automatic parsing
and disambiguation of dictionary entries. This discus-
sion also demonstrates the application of the proposed
disambiguation technique to natural language process-
ing and computational linguistics in general.
</bodyText>
<table confidence="0.2966565">
The Automated Dictionary
Mark S. Fox, Donald J. Bebel, and Alice C. Parker
Carnegie-Mellon University
Schenley Park
Pittsburgh, Pennsylvania 15213
Computer 13, 7 (July 1980), 35-48.
</table>
<bodyText confidence="0.999890684210526">
An automated dictionary is a computer-based de-
vice that holds all or part of a dictionary and allows
access to and the display of entries. The
&amp;quot;automation&amp;quot; of a dictionary raises several issues:
What information should be stored? How does the
automated dictionary&apos;s size determine the style of
usage? What is the interface between the user and the
dictionary? What are the automated dictionary&apos;s phys-
ical components? Each issue provides a rich set of
alternatives from which to choose. Part of the design
problem of an automated dictionary is picking feasible
points in this multidimensional design space.
The purpose of this article is to present the results
of some preliminary studies conducted at the request
of the National Institute of Education. We determine
the capability of an automated dictionary system built
using present technology and estimate present and
future costs based on trends in the cost and function-
ality of digital electronics.
</bodyText>
<subsectionHeader confidence="0.808913833333333">
Computer Programs for Detecting and
Correcting Spelling Errors
James L. Peterson
Department of Computer Sciences
University of Texas
Austin, Texas 78712
</subsectionHeader>
<bodyText confidence="0.9892703">
Comm. ACM 23, 12 (Dec. 1980), 676-687.
With the increase in word and text processing com-
puter systems, programs which check and correct
spelling will become more and more common. This
paper investigates the basic structure of several such
existing programs and their approaches to solving the
problems which arise when this type of program is
created. The basic framework and background neces-
sary to write a spelling checker or corrector are pro-
vided.
</bodyText>
<subsectionHeader confidence="0.695380333333333">
Recognition of Spoken Spelled Names
for Directory Assistance Using
Speaker-Independent Templates
</subsectionHeader>
<bodyText confidence="0.365542">
A.E. Rosenberg, L.R. Rabiner, and J.G. Wilpon
</bodyText>
<subsectionHeader confidence="0.672010666666667">
Bell Telephone Laboratories, Inc.
600 Mountain Avenue
Murray Hill, New Jersey 07974
</subsectionHeader>
<bodyText confidence="0.981234">
Bell Syst. Tech. J. 59, 4 (April 1980), 571-592.
In a recent paper, Rosenberg and Schmidt demon-
strated the applicability of a speaker-trained, isolated
word speech recognizer to the problem of automatic
directory assistance. Input to the system was in the
form of a string of letters which spelled the last name
and initials of an individual for whom a directory list-
ing was required. Rosenberg and Schmidt found that,
even though the recognition rate for individual letters
was rather low (approximately 80 percent), the rate at
which the correct directory listing was found was high-
er (approximately 95 percent). In this paper, we ex-
tend these results to include the case of speaker-
independent recognition of letters. We show that
overall performance in the speaker-independent mode
is comparable to performance in a speaker-dependent
mode and examine various factors important for oper-
ation in a speaker-independent mode, such as charac-
teristics of the reference templates, choice of decision
rule, and threshold parameters. For the most part, the
overall system is remarkably robust to the parameters
of the recognizer. For the best choice of these param-
eters, a 95-percent correct string rate is obtained,
comparable to the performance in a speaker-dependent
mode.
</bodyText>
<table confidence="0.71725275">
American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 131
The FINITE STRING Newsletter Abstracts of Current Literature
A Minimum-Distance Search Technique and Its
Application to Automatic Directory Assistance
B. Aldefeld, S.E. Levinson, and T.G. Szymanski
Bell Telephone Laboratories, Inc.
600 Mountain Hill
Murray Hill, New Jersey 07974
</table>
<subsubsectionHeader confidence="0.586486">
Bell Syst. Tech. J. 59, 8 (Oct. 1980), 1343-1356.
</subsubsectionHeader>
<bodyText confidence="0.999996888888889">
This paper describes a new search procedure and its
application to the problem of obtaining telephone di-
rectory information from spoken spelled input. The
method obtains its speed from using the concept of
equivalence classes, with names classified according to
their letter-by-letter acoustic similarity. It derives its
accuracy from the use of a minimum-distance criterion
for selecting answers. The search procedure finds the
name with the minimum distance, usually after only a
small fraction of the directory file has been examined.
Using an acoustic analyzer with an 80 percent correct
recognition rate for individual letters, a 98.6 percent
correct recognition rate for names was achieved when
the method was applied to a directory of 18,000 en-
tries. On the average, only 1.2 percent of the directo-
ry had to be examined for each query. With an input
recognition rate of 71 percent for letters, the respec-
tive figures were 97.2 percent and 2.8 percent.
</bodyText>
<subsectionHeader confidence="0.88482775">
Synthesis by Rule of Prosodic Features in
Word Concatenation Synthesis
S. J. Young
Control Systems Centre
University of Manchester
Institute of Science and Technology
Manchester, M80 1QD, ENGLAND
F. Fallside
Engineering Department
Cambridge University
Corn Exchange Street
Cambridge, CB2 3QG, ENGLAND
</subsectionHeader>
<subsubsectionHeader confidence="0.711761">
Int. J. Man-Mach. Studies 12, 4 (April 1980), 241-258.
</subsubsectionHeader>
<bodyText confidence="0.999768941176471">
The quality of speech obtainable by the technique
of Word Concatenation Synthesis depends crucially on
the accuracy of the pitch and timing contours which
need to be computed for each utterance synthesized.
A method for the synthesis-by-rule of these prosodic
features is described for utterances for which the only
information available is a syntactic phrase marker and
the lexical structure of each component word.
Rules and algorithms are presented for the determi-
nation of word group boundaries and the placement of
stress, accent and nuclei. A timing contour algorithm
is described which implements the prominence re-
quired for stressed syllables and also generates the
stress-timed rhythm of natural English. A correspond-
ing pitch contour algorithm computes appropriate into-
nation patterns using a minimal set of three tone
groups; fall, rise and fall-rise.
</bodyText>
<note confidence="0.4573685">
The Linguistic Reason Why the
Computer Will Never Think
</note>
<table confidence="0.689535">
John R. Hammen
P.O. Box 12931
Seattle, Washington 98111
SIGLASH Newsletter 13, 4 (Dec. 1980), 8-16.
</table>
<bodyText confidence="0.99978719047619">
This paper draws a comparison between the respec-
tive lowest denominators of computer and human lan-
guage, both of which are the respective mediums of
computer programming and human reasoning. The
lowest denominator of the former is Boolean Algebra,
which is a logic defined by the 3 operators: And, Or,
and Not. Underlying human language is the system of
logic called semantic structure. The Boolean operators
— And, Or, and Not — exist in semantic structure.
This paper shows that the 3 determiners: Definite,
Indefinite, and All-Inclusive; the 5 moods: Indicative,
Imperative, Interrogative, Conditional, and Emphatic;
the 3 tenses: Past, Present, and Future; and the 8 cas-
es: Agent, Possessive, Source, Direction, Instrument,
Experiencer, Location, and Object function just as
much like logic operators as do And, Or, and Not.
This means that semantic structure is defined by 22
operators (19 more than Boolean Algebra). The
author contends that because of this difference in
numbers of operators the computer will never be
programmed to think in a human fashion.
</bodyText>
<subsectionHeader confidence="0.698775">
The Natural Language of Interactive Systems
</subsectionHeader>
<author confidence="0.690656">
Henry Ledgard, Andrew Singer, and William Seymour
</author>
<affiliation confidence="0.436858">
Computer and Information Science Department
University of Massachusetts
Amherst, Massachusetts 01003
</affiliation>
<subsectionHeader confidence="0.82347375">
John A. Whiteside
Digital Equipment Corporation
146 Main Street
Maynard, Massachusetts 01754
</subsectionHeader>
<bodyText confidence="0.991225588235294">
Comm. ACM 23, 10 (Oct. 1980), 556-563.
The work reported here stems from our deep belief
that improved human engineering can add significantly
to the acceptance and use of computer technology. In
particular, this report describes an experiment to test
the hypothesis that certain features of natural language
provide a useful guide for the human engineering of
interactive command languages. The goal was to es-
tablish that a syntax employing familiar, descriptive,
everyday words and well-formed English phrases con-
tributes to a language that can be easily and effective-
ly used. Users with varying degrees of interactive
computing experience used two versions of an interac-
tive text editor; one with an English-based command
syntax in the sense described above, the other with a
more notational syntax. Performance differences
strongly favored the English-based editor.
</bodyText>
<page confidence="0.53044">
132 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
</page>
<table confidence="0.305173916666667">
The FINITE STRING Newsletter Abstracts of Current Literature
Intensional Concepts in Propositional
Semantic Networks
Anthony S. Maida
Rochester Institute of Technology
Rochester, New York
Stuart C. Shapiro
Computer Science Department
State University of New York at Buffalo
4230 Ridge Lea Road
Amherst, New York 14226
Technical Report 171, Feb. 1981.
</table>
<bodyText confidence="0.999981157894737">
One of the major characteristics of semantic net-
works as a representation of knowledge is what we
term the uniqueness principle: that each concept repre-
sented in the network is represented by a unique node.
We show that this principle entails that at least some
nodes represent intensional, rather than extensional,
objects. By taking the position that a semantic net-
work is a model of the conceptual organization of a
cognitive agent rather than of the world, we argue
further that nodes can represent only intensional ob-
jects. To allow for coreferential terms, we include the
representation of a proposition that asserts the exten-
sional equivalence of two intensional concepts. This
approach is shown to lead to elegant solutions to three
apparently unrelated problems in the representation of
knowledge. Our discussion brings to bear relevant
literature in analytic philosophy and experimental psy-
chology and leads us to suggest two new psychology
experiments.
</bodyText>
<subsectionHeader confidence="0.9668385">
COCCI: A Deductive Semantic Network
Program for Solving Microbiology Unknowns
Stuart C. Shapiro
Department of Computer Science
State University of New York at Buffalo
4230 Ridge Lea Road
</subsectionHeader>
<bodyText confidence="0.552991">
Amherst, New York 14226
</bodyText>
<subsectionHeader confidence="0.841586">
Technical Report 173, March 1981.
</subsectionHeader>
<bodyText confidence="0.999935176470588">
&amp;quot;You have been given a culture of one of nine coc-
ci. Identify it.&amp;quot; COCCI is a program to solve this
problem. To identify the unknown, COCCI requests a
human assistant to perform tests and make observa-
tions and report the results to it. COCCI consists of
14 specific facts and 17 deduction rules stored in
SNePS, a general purpose deductive semantic network
processing system, and a small ATN grammar for pars-
ing and for generating English from the semantic net-
work. All of COCCI&apos;s reasoning and interaction with
humans is driven by the general purpose bi-directional
inference sub-system of SNePS. This paper describes
COCCI as an illustration of deductive semantic net-
works in general and SNePS in particular. Specific
points covered include structure sharing, procedural
attachment, generation grammars and the structure of
deduction rules.
</bodyText>
<subsectionHeader confidence="0.600534">
Bi-Directional Inference
</subsectionHeader>
<bodyText confidence="0.5031995">
Joao P. Martins, Donald P. McKay
and Stuart C. Shapiro
</bodyText>
<subsectionHeader confidence="0.861816666666667">
Department of Computer Science
State University of New York at Buffalo
4230 Ridge Lea Road
</subsectionHeader>
<bodyText confidence="0.654549">
Amherst, New York 74226
</bodyText>
<subsectionHeader confidence="0.833831">
Technical Report 174, March 1981.
</subsectionHeader>
<bodyText confidence="0.99959125">
In this paper we present a brief overview of the
SNePS deduction system and show through an exam-
ple the interaction between forward and backward
inference. This interaction — resulting in a class of
inference termed bi-directional inference — enables an
easy and elegant way of performing resource-limited
searches and the possibility of performing resource-
limited deduction in a natural and simple way. Fur-
thermore, bi-directional inference focus a system&apos;s
attention towards the interests of the user and can cut
down the fan-out of pure forward or pure backward
chaining.
</bodyText>
<subsectionHeader confidence="0.986461666666667">
A Belief Revision System Based on Relevance
Logic and Heterarchical Contexts
Joao P. Martins and Stuart C. Shapiro
Department of Computer Science
State University of New York at Buffalo
4230 Ridge Lea Road
</subsectionHeader>
<bodyText confidence="0.872122">
Amherst, New York 14226
</bodyText>
<subsectionHeader confidence="0.710074">
Technical Report 175, March 1981.
</subsectionHeader>
<bodyText confidence="0.999924166666667">
This paper describes the underlying theory of a
Belief Revision System based on Relevance Logic and
Heterarchical Contexts. In our system each statement
is indexed by the set of basic (i.e., non-derived) as-
sumptions used in its derivation and by the set of basic
assumptions with which it is incompatible. A context
is a set of basic assumptions and contains all the state-
ments whose first index is a subset of the context and
whose second index is disjoint from the context. This
allows straightforward switching between contexts and
the possibility of efficiently performing hypothetical
reasoning.
</bodyText>
<subsectionHeader confidence="0.962713285714286">
Interacting in Natural Language with Artificial
Systems: The Donau Project
Giovanni Guida and Marco Somalvico
Milan Polytechnic Artificial Intelligence Project
Istituto di Elettrotecnica ed Elettronica
Politecnico di Milano
Milan, ITALY
</subsectionHeader>
<bodyText confidence="0.95538321875">
Inform. Systems 5, 4 (1980), 333-344.
This paper is intended to propose a new methodo-
logical approach to the conception and development of
natural language understanding systems. This new
contribution is supported by the design, implementa-
tion, and experimentation of DONAU: a general pur-
pose domain-oriented natural language understanding
American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 133
The FINITE STRING Newsletter Abstracts of Current Literature
system developed and presently running at the Milan
Polytechnic Artificial Intelligence Project. The system
is based on a two-level modular architecture intended
to overcome the lack of flexibility and generality often
pointed out in many existing systems, and to facilitate
the exchange of results and actual experiences be-
tween different projects. The horizontal level allows
an independent and parallel development of the single
segments of the system (syntactic analyser, semantic
analyser, information extractor, legality controller).
The vertical level ensures the possibility of changing
(enlarging or redefining) the definition of the semantic
domain on which each particular version of the system
is oriented and specialized in a simple, incremental,
and user-oriented way. In the paper the general archi-
tecture of the system and the mode of operation of
each segment are illustrated in detail. Linguistic mod-
els, knowledge representation, and parsing algorithms
are described and illustrated by means of selected
examples. Performance evaluations of the system in
the application version on data base inquiry are re-
ported and discussed. Promising directions for future
research are presented in the conclusions.
</bodyText>
<subsectionHeader confidence="0.897473666666667">
Using the Data Base as a Semantic Component
to Aid in the Parsing of Natural Language
Data Base Queries
Larry R. Harris
Mathematics Department
Dartmouth College
</subsectionHeader>
<bodyText confidence="0.985725225806452">
Hanover, New Hampshire 03755
J. Cybern. 10, 1-3 (Jan.-March 1980), 77-96.
Many of the recent advances in artificial intelli-
gence (Al) have been brought about through the use
of domain specific knowledge. In this same spirit, this
paper presents an approach to understanding natural
language data base queries that employs the use of
domain specific knowledge to aid the parsing process.
The unique aspect of the data base query environment
relative to other Al problem domains is that the re-
quired body of knowledge already exists in the form of
the data base being queried. Thus, there is an impor-
tant benefit of making use of this knowledge in the
form in which it is maintained by the data base man-
agement system, since the very difficult problems of
gathering and representing this information can be
circumvented.
The paper describes the problems encountered in
the parsing of data base queries that can be solved by
the semantic use of the data base, as well as a precise
description of how these problems can be solved by
existing data base systems. The proposed use of the
data base as a semantic component has been a pro-
mary component of the design of a high performance
natural language data base query system called RO-
BOT, that has been successfully installed at several
commercial installations. Since this system is a physi-
cal realization of the proposed methodology, a brief
description of the system and its experiences in the
field are given as evidence of the feasibility of this
approach.
</bodyText>
<subsectionHeader confidence="0.813488428571429">
Natural Language Access to Medical Text
Donald E. Walker and Jerry R. Hobbs
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
Technical Note 240, March 1981.
</subsectionHeader>
<bodyText confidence="0.99998525">
This paper describes research on the development
of a methodology for representing the information in
texts and of procedures for relating the linguistic
structure of a request to the corresponding representa-
tions. The work is being done in the context of a
prototype system that will allow physicians and other
health professionals to access information in a compu-
terized textbook of hepatitis through natural language
dialogues. The interpretation of natural language quer-
ies is derived from DIAMOND/DIAGRAM, a linguis-
tically motivated, domain-independent natural lan-
guage interface developed at SRI. A text access com-
ponent is being developed that uses representations of
the propositional content of text passages and of the
hierarchical structure of the text as a whole to retrieve
relevant information.
</bodyText>
<subsectionHeader confidence="0.771648769230769">
An Efficient Easily Adaptable System
For Interpreting Natural Language Queries
David H.D. Warren
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
Fernando C.N. Pereira
CAAS Studies, Department of Architecture
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
DAI Research Paper No. 155, 1981, 18 pages.
</subsectionHeader>
<bodyText confidence="0.999983">
This paper gives an overall account of a prototype
natural language question answering system, called
Chat-80. Chat-80 has been designed to be both effi-
cient and easily adaptable to a variety of applications.
The system is implemented entirely in PROLOG, a
programming language based on logic. With the aid of
a logic-based grammar formalism called extraposition
grammars, Chat-80 translates English questions into
the PROLOG subset of logic. The resulting logical
expression is then transformed by a planning algorithm
into efficient PROLOG, cf. &amp;quot;query optimisation&amp;quot; in a
relational database. Finally the PROLOG form is
executed to yield the answer. On a domain of world
geography, most questions within the English subset
</bodyText>
<page confidence="0.608927">
134 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
</page>
<note confidence="0.602913">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.765567">
are answered in well under one second, including rela-
tively complex queries.
A Computer Written Language Lab
</bodyText>
<subsectionHeader confidence="0.7264634">
Mike Sharpies
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
Edinburgh EH1 2QL SCOTLAND
</subsectionHeader>
<subsubsectionHeader confidence="0.592172">
DAI Research Paper No. 134, 1980, 9 pages.
</subsubsectionHeader>
<bodyText confidence="0.931664">
A group of Edinburgh primary school children are
using a prototype computer written language lab, de-
signed to help them understand language and develop
their writing style. In a more sophisticated version the
language lab can become a resource for teachers, lin-
guists, indeed anyone interested in creating or investi-
gating language.
A Computer Based Language Workshop
</bodyText>
<subsectionHeader confidence="0.82161575">
Mike Sharpies
Department of Artificial Intelligence
University of Edinburgh
Forrest Hill
</subsectionHeader>
<bodyText confidence="0.462573">
Edinburgh EH1 2QL SCOTLAND
</bodyText>
<subsubsectionHeader confidence="0.483401">
DAI Research Paper No. 135, 1980, 11 pages.
</subsubsectionHeader>
<bodyText confidence="0.999963428571429">
The paper describes a project to implement and
evaluate a computer written language workshop. The
pilot phase, now completed, provides pupils with a
small range of language manipulation programs. The
second phase will offer children a more extensive
range of computer based language aids for the explor-
ation of written style.
</bodyText>
<subsectionHeader confidence="0.9039138">
Reading, Looking and Learning
Christine Urquhart
38 Shipton Road
Sutton Coldfield
W. Midlands B721 NR, ENGLAND
</subsectionHeader>
<subsubsectionHeader confidence="0.87981">
Journal of Inform. Sci. 1 (1980), 333-344.
</subsubsectionHeader>
<bodyText confidence="0.9996580625">
The survey of aspects of reading research discusses
reading styles, &apos;readability&apos;, and the influence of pres-
entation on reading and comprehension. Considera-
tion of these leads to the fundamental problems of
how the mind and eye interact in visual processing of
text. Some research on the psychology of reading is
reviewed, and certain psycholinguistic models of the
reading process are discussed. Text structure and
content appear to influence comprehension and learn-
ing, and relevant research on this, and the interaction
of the reader with the text, is outlined. Reading stra-
tegies are discussed, and the possibilities of altering
reading behaviour by increasing reading speed or im-
proving learning patterns are reviewed. Possible impli-
cations for information science of reading research are
mentioned.
</bodyText>
<subsectionHeader confidence="0.831607">
Expert Systems
Donald Michie
</subsectionHeader>
<reference confidence="0.7777864">
Machine Intelligence Research Unit
University of Edinburgh
Hope Park Square, Meadow Lane
Edinburgh EH8 9NW, SCOTLAND
Computer Journal 23, 4 (Nov. 1980), 369-376.
A central feature of &apos;expert systems&apos; in chemistry,
molecular geology, medicine, plant pathology, robotics,
chess and other applications is the man-machine com-
munication of descriptive concepts in the form of pat-
terns. &apos;Humanisation&apos; of machine-made representa-
tions must find a place in future designs.
A Linguistic Approach to Decisionmaking
with Fuzzy Sets
Richard M. Tong
Advanced Information and Decision Systems
201 San Antonio Circle
Suite 286
Mountain View, California 94040
Piero P. Bonissone
Corporate Research and Development Department
General Electric Company
P.O. Box 43
Building 37-579
Schenectady, New York 12301
IEEE Trans. Sys. Man Cyb. 10, 11 (Nov. 1980), 716-722.
</reference>
<bodyText confidence="0.99995225">
A technique for making linguistic decisions is pres-
ented. Fuzzy sets are assumed to be an appropriate
way of dealing with uncertainty, and it is therefore
concluded that decisions taken on the basis of such
information must themselves be fuzzy. It is inappro-
priate then to present the decision in numerical form;
a statement in natural language is much better. For
brevity only a single-stage multiattribute decision
problem is considered. Solutions to such problems are
shown using ideas in linguistic approximation and truth
qualification. An extensive example illuminates the
basic ideas and techniques.
</bodyText>
<subsectionHeader confidence="0.85523">
Natural Language Programming and
Natural Programming Languages
Larry H. Reeker
Department of Computer Science
University of Queensland
St. Lucia, Brisbane
Queensland, AUSTRALIA 4067
</subsectionHeader>
<bodyText confidence="0.904639615384616">
Aust. Comput. J. 12, 3 (Aug. 1980), 89-92.
The idea of &amp;quot;natural language programming&amp;quot; has a
good deal of superficial attractiveness, but it also rais-
es a number of questions. The most immediate of
these is what one means by &amp;quot;natural language pro-
gramming&amp;quot;, since the use of unfettered, everyday lan-
guage is not feasible, for a number of reasons. Two
approaches to moving toward the goal of more natural
programming languages using natural language are
American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 135
The FINITE STRING Newsletter Abstracts of Current Literature
discussed, and the second, or &amp;quot;bottom up&amp;quot; approach is
advocated.
</bodyText>
<reference confidence="0.940178">
Natural Language Programming:
Styles, Strategies and Contrasts
Lance A. Miller
Computer Sciences Department
IBM Thomas J. Watson Research Center
Yorktown Heights, New York 10598
IBM Systems Journal 20, 2 (1981), 184-215.
</reference>
<bodyText confidence="0.992222428571429">
College students who were not familiar with com-
puters were asked to produce written natural language
procedural instructions as directions for others to fol-
low. These directions were solutions for six file-
manipulation problems that also could reasonably be
solved by writing computer programs. The written
texts were examined from five points of view: solu-
tion correctness, preferences of expression, contextual
referencing, word usage, and formal programming
languages. The results provide insight both on the
manner in which people express computer-like proce-
dures &amp;quot;naturally&amp;quot; and on what features programming
languages should include if they are to be made more
&amp;quot;natural-like.&amp;quot;
</bodyText>
<reference confidence="0.967651285714286">
Probabilistic Languages: A Review
and Some Open Questions
C.S. Wetherell
Bell Telephone Laboratories, Inc.
600 Mountain Avenue
Murray Hill, New Jersey 07974
Computing Surveys, 12, 4 (Dec. 1980), 361-379.
</reference>
<bodyText confidence="0.996490916666667">
Context-free languages are commonly used to de-
scribe the structure of programming languages. How-
ever many interesting problems involve not just a
language&apos;s structure but also the actual usage of the
language. Adding a notion of probability to ordinary
grammars gives rise to probabilistic context-free gram-
mars. Interesting in their own right because of some
pretty theorems, probabilistic context-free languages
can be applied to the analysis of programming lan-
guages, automatic parsers, and error correctors. A
complete outline of the theory is presented with exam-
ples. Some open questions are posed.
</bodyText>
<reference confidence="0.936532916666667">
Compiler Testing Using a Sentence Generator
A. Celentano, S. Crespi Reghizzi, P. Della Vigna and
C. Ghezzi
Istituto di Elettrotecnica ed Elettronica
Politecnico di Milano
Piazza L. da Vinci 32
1-20133 Milano ITALY
G. Granata and F. Savoretti
Ing. C. Olivetti &amp; C. S.p.A.
Via Jervis 77
Ivrea ITALY
Softw. Pract. Exper. 10, 11 (Nov. 1980), 897-918.
</reference>
<bodyText confidence="0.998439428571429">
A system for assisting in the testing phase of compi-
lers is described. The definition of the language to be
compiled drives an automatic sentence generator. The
language is described by an extended BNF grammar
which can be augmented by actions to ensure contex-
tual congruence, e.g. between definition and use of
identifiers. For deep control of the structure of the
produced sample the grammar can be described by
step-wise refinements: the generator is iteratively
applied to each level of refinement, producing at last
compilable, complete programs. The implementation
is described and some experimental results are report-
ed concerning PLZ, MINIPL and some other lan-
guages.
</bodyText>
<reference confidence="0.605897666666667">
Soft Display Key for Kanji Input
Jouko J. Seppanen
Helsinki University of Technology
Computing Centre
021 Espoo 15, FINLAND
Research Report 17, 1980.
</reference>
<bodyText confidence="0.999913">
The concept of a soft display key as applied to input
of large character sets or vocabularies such as Kanji,
the ancient Chinese ideographic script, is discussed.
The Japanese orthography and the necessity of using
Kanji characters in data terminals are explained. Prob-
lems arising from the number and complexity of Kanji
symbols for the manufacture and use of keyboard
devices are stated. A review is made of devices and
methods presently used or suggested. The feasibility
of the soft display key is then demonstrated. Some
requirements for the design and implementation of a
soft display keyboard for Kanji are considered. In
conclusion, implications to man/computer interface
design, human factors engineering and hardware unifi-
cation and standardization are stated.
</bodyText>
<page confidence="0.874836">
136 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.855894">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<note confidence="0.84375125">ACM Annual Conference be held at the Bonaventure Hotel in Los Angeles, California, 9-11, 1981. [See 6:3-4, 194.] For further information contact:</note>
<author confidence="0.731018">A C Toni Shelter</author>
<address confidence="0.726609333333333">Xerox Corporation, A3-49 701 South Aviation Boulevard El Segundo, California 90245</address>
<phone confidence="0.91825">(213) 679-4511 x1968</phone>
<title confidence="0.4068565">The ACL is sponsoring three sessions on &amp;quot;Computer Modeling of Linguistic Theory&amp;quot; in conwith the Meeting of the Linguistic of America will be held in New York</title>
<note confidence="0.845899666666667">City at the Grand Hyatt Hotel, December 28-30, [See 7:1, 49.] For further information contact:</note>
<author confidence="0.999406">Stan Petrick</author>
<affiliation confidence="0.999946">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.958149">218 Yorktown Heights, New York 10598</address>
<author confidence="0.871488">or Terry Langendoen</author>
<affiliation confidence="0.99712">CUNY Graduate Center</affiliation>
<address confidence="0.972185">33 West 42nd Street New York, New York 10036</address>
<title confidence="0.956080666666667">Abstracts of Current Literature The State-of-the-Art in Natural Language Understanding</title>
<author confidence="0.999992">David L Waltz</author>
<affiliation confidence="0.999984">Coordinated Science Laboratory University of Illinois</affiliation>
<address confidence="0.999783">Urbana, Illinois 61801</address>
<abstract confidence="0.932892333333333">Working Paper WP-27, Dec. 1980, 35 pages. Research in computer understanding of natural language has led to the construction of programs which can handle a number of different types of language, including questions about the contents of data bases, stories and news articles, dialogues, and scene descriptions. This research draws on and has in turn had an affect on many other research areas, including software engineering, linguistics, psychology, philosophy, and knowledge representation. This paper provides a brief history and overview of the field, along with examples and explanations of the operation of several natural language understanding programs. The limitations of our current technology are discussed, and assessments are given of the most promising current research directions. Evaluation of Natural Language Processors R.</abstract>
<affiliation confidence="0.999948">Coordinated Science Laboratory University of Illinois</affiliation>
<address confidence="0.999877">Urbana, Illinois 61801</address>
<abstract confidence="0.996199826086957">Technical Report T-103, Nov. 1980, 247 pages. Despite a large amount of research on developing natural language understanding problems, little work has been done on evaluating their performance or potential. The evaluations that have been done have been unsystematic and incomplete. This has led to uncertainty and confusion over the accomplishments of natural language processing research. The lack of evaluation can be primarily attributed to the difficulty of the problem. The desired behavior of natural language processors has not been clearly specified. Partial progress toward the eventual goals for natural language processors has not been delineated, much less measured. This thesis attempts to clarify some of the difficulties behind evaluating the performance of natural language processors. It also proposes an evaluation method that is designed to be systematic and thorough. The method relies on considering a natural language processor from three viewpoints in the light of several of issues relevant to natural language processing. Finally, an evaluation is described PLANES, a natural language database query system.</abstract>
<title confidence="0.9832605">A Program Conversing In Portuguese Providing a Library Service</title>
<author confidence="0.996508">Helder Coelho</author>
<affiliation confidence="0.9872265">Centro de Informatica Laboratorio Nacional de Engenharia Civil</affiliation>
<address confidence="0.9973985">101, Av. do Brasil 1799 Lisboa Codex, PORTUGAL</address>
<abstract confidence="0.932501">Edinburgh Ph.D. Thesis, Dec. 1979. TUGA is a program which converses in Portuguese to provide a library service covering the field of Artificial Intelligence. The objective of designing TUGA was the development of a feasible method for consulting and creating data bases in natural Portuguese. The resulting program allows dialogues where the program and its users behave in the way humans normally do in a dialogue setting. The program can answer, and question, in pre-defined scenarios. Users can question, answer and issue commands in a natural and convenient way, without bothering excessively with the form of the dialogues and sentences. The original contributions of this work are: the treatment of dialogues, the adaptation of Colmerauer&apos;s natural language framework to Portuguese, the particular method for evaluating the logical structures involved in Colmerauer&apos;s framework, and the library service application itself. The program is implemented Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 The FINITE STRING Newsletter Abstracts of Current Literature in PROLOG, a simple and powerful programming language essentially identical in syntax and semantics to a subset of predicate calculus in clausal form.</abstract>
<title confidence="0.99433">Semantic Grammar and Meaning Representation Language in a Natural Language Question and Answer System</title>
<author confidence="0.999925">C Rathke</author>
<author confidence="0.999925">B Sonntag</author>
<author confidence="0.999925">W Schopper</author>
<affiliation confidence="0.993898">Institut far Informatik Universitat Stuttgart</affiliation>
<address confidence="0.995674">D-7000 Stuttgart 1, WEST GERMANY</address>
<abstract confidence="0.988983583333333">Angew. Inf. 22, 4 (April 1980), 155-157 (In German). We describe a natural language question-answering system. Questions can be put to the system in written natural language about the world of German soccer teams and the people employed by them. A so-called semantic grammar is introduced which is especially designed to cover a large set of possible questions about the world mentioned. An expression of a formal Meaning Representation Language (MRL) is produced by the system. Applied to the database, which is organized in a semantic net, it generates the appropriate answer.</abstract>
<title confidence="0.972737">User&apos;s Guide</title>
<author confidence="0.999652">Daniel Sagalowiez</author>
<affiliation confidence="0.99975">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997871">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.963542947368421">Technical Note 224, Aug. 1980. D-LADDER (DIAMOND-based language Access to Distributed Data with Error Recovery) is a computer system designed to provide answers to questions posed at the terminal in a subset of natural language information. The system accepts natural-language quesabout the data. For each question plans a sequence of appropriate queries to the data base management system, determines on which machines the queries are to be processed, establishes links to those machines over the ARPANET, monitors the processing of the queries and recovers from certain errors in execution, and prepares a relevant answer to the original text. This user&apos;s guide is intended for the person who knows how to log in to the host operating system, as well as how to enter and edit a line of text. It does not explain how D-LADDER works, but rather how to use it on a demonstration basis.</abstract>
<title confidence="0.9106015">Interpreting Discourse: Coherence and the Analysis of Ethnographic Interviews</title>
<author confidence="0.998964">Michael Agar</author>
<author confidence="0.998964">Jerry Hobbs</author>
<affiliation confidence="0.9997335">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9978955">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.992386027777778">Technical Note 225, Aug. 1980. Practioners of ethnography, in seeking to discover and describe complex patterns of behavior, face a number of serious problems. First, the patterns should be described in as formal a fashion as possible, and yet the formalism that ethnographers have availed themselves of are simply inadequate to the task. Secondly, data such as ethnographic interviews constitutes the most common way of discovering a culture, but there is a dearth of formal methods for going from a text to the cultural presuppositions that underlie it. Thirdly, it is difficult to know in ethnographic interviews how much of what is said is a reflection of the culture, how much is the speaker&apos;s personal interpretation, and how much is due to the interview situation itself. Finally, ethnographers face the problem of how to deal in depth with large volumes of ethnographic data. Artificial intelligence can be viewed in large part as the investigation of complex formalisms. Heretofore, these have been applied primarily in simple domains. In this paper we attempt to use Al formalism as a formal language of description for the complex conversational behavior that occurs in ethnographic interviews. We thus address the first of the ethnographer&apos;s problems by exploring the use of formalisms that begin to be adequate to the task. Moreover, work on discourse analysis in the Al framework has sought to characterize the structure of texts in terms of the goals and beliefs of the speaker. It thus suggests methods of using the structure of the text to force the explicitation of the underlying belief system, addressing the second of the ethnographer&apos;s problems. Finally, our work has confronted us with the last two problems, and the approach we are taking has suggested tentative ideas for dealing with these problems.</abstract>
<title confidence="0.975052">Generalization and Memory in an Integrated Understanding System</title>
<author confidence="0.999997">Michael Lebowitz</author>
<affiliation confidence="0.9821795">Department of Computer Science Yale University</affiliation>
<address confidence="0.992384">New Haven, Connecticut 06520</address>
<note confidence="0.755958">Research Report 186, Oct. 1980.</note>
<abstract confidence="0.966519911764706">Generalization and memory are part of natural language understanding. As people read stories describing various situations, they are able to recall similar episodes from memory and use them as a basis to form generalizations about the way such situations normally Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 The FINITE STRING Newsletter Abstracts of Current Literature occur. This thesis describes an integrated system for language understanding, IPP (Integrated Partial Parser), that encompasses the ability to generalize and record information in long-term memory as well as conceptual analysis. IPP is a program that learns about the world by reading stories taken from newspapers and the UPI news wire, adding information from these stories to memory, and making generalizations that describe specific situations. It uses the generalizations that it has made to help in understanding future stories. As it reads stories, IPP adds them to its permanent memory. If it locates similar stories in memory as it does this, then it attempts to make generalizations that describe the similarities among the events. Such generalizations form the basis for organizing events in memory and understanding later stories. IPP also includes a procedure for confirming generalizations as further stories are read. In order to analyze the text that it reads, IPP makes extensive use of top-down, predictive processing. As it processes a story, IPP accesses memory in an attempt to identify generalizations describing stereotypical situations that can provide predictions to be used in understanding. Such use of memory to provide top-down context results in a robust and efficient understanding system.</abstract>
<title confidence="0.694985">Retrieval and Organizational Strategies in Conceptual Memory: A Computer Model</title>
<author confidence="0.991354">Janet Lynne Kolodner</author>
<affiliation confidence="0.9821055">Department of Computer Science Yale University</affiliation>
<address confidence="0.992125">New Haven, Connecticut 06520</address>
<note confidence="0.573427">Research Report 187, Nov. 1980. People effortlessly recall past events and episodes in</note>
<abstract confidence="0.996310710526316">their lives many times in the course of a normal day. A reasonable goal in the design of computer programs is to construct a memory with that same capability. To facilitate human-like retrieval of events from a computer memory, we must first specify a reasonable memory organization. We must then design updating and retrieval processes to build up and access that information. This thesis will present such a theory, and will describe a computer program called CYRUS which implements that theory. CYRUS (Computerized Yale Retrieval and Updating System) stores and retrieves episodes in the lives of Secretaries of State Cyrus Vance and Edmund Muskie. When new events are added to its memory, CYRUS integrates them into memory along with the events it already knows about. CYRUS can then answer questions posed to it in English about the events it stores. The algorithms and memory organization used in CYRUS have been developed by examining the way people answer questions requiring extensive memory Its reconstructive processes include instantiastrategies, construct and elaborate on confor search, and strategies, direct construction. Reconstructive processes require a vast store of generalized knowledge in order to be applied. Reconstructive retrieval implies a memory organization which organizes both generalized information about different types of events and distinguishing features of particular events. CYRUS&apos; memory is self-organizing. When given a new fact about Vance or Muskie, it integrates the new event into its already-existing memory organization. In the process, it updates its generalized information and indexes the new event in the appropriate places. CYRUS can be seen as both a model of human memory and an intelligent information retrieval system.</abstract>
<title confidence="0.961816">Memory, Meaning, and Syntax</title>
<author confidence="0.999236">Roger C Schank</author>
<author confidence="0.999236">Lawrence Birnbaum</author>
<affiliation confidence="0.9925335">Computer Science Department Yale University</affiliation>
<address confidence="0.992729">New Haven, Connecticut 06520</address>
<note confidence="0.735113">Research Report 189, Nov. 1980. This paper explores the role of syntax in computa-</note>
<abstract confidence="0.985795625">tional theories of natural language. We discuss the processing hypothesis, contends that meaning and world knowledge play a crucial part in language understanding even at the earliest points in the process. The hypothesis implies that syntactic knowledge plays no privileged role in language processing. Computer models of language analysis are discussed in relation to the overall theory.</abstract>
<title confidence="0.960243">Word Expert Parsing: A Theory of Distributed Word-Based Natural Language Understanding</title>
<author confidence="0.99998">Steven Small</author>
<affiliation confidence="0.999987">Department of Computer Science University of Maryland</affiliation>
<address confidence="0.998979">College Park, Maryland 20742</address>
<abstract confidence="0.985025130434783">Technical Report TR-954, Sept. 1980. People have an incredible facility for organizing and selecting word senses in arriving at the intended meaning of sentences in context. The process takes place with such unnoticed and subconscious ease that it has been often overlooked in the study of language. To those building computer programs to understand language, however, this phenomenon represents a central problem. While many syntactic constructions and conceptual relations can be described through systems of rewrite rules, the word sense selection problem remains. The reason for this lies in the incompatibility Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 The FINITE STRING Newsletter Abstracts of Current Literature of the sense discrimination problem and the rule-based problem-solving method. The computational theory of Word Expert Parsing approaches natural language understanding as a nonuniform distributed process of interacting words. An expert process for each word actively pursues its intended meaning in the context of other word experts and real-world knowledge. The theory perceives understanding as a behavior of memory interactions, and the computer model emphasizes process rather than output structures. The Lexical Interaction Language (LIL) formalizes the interactions among individual word experts, and the Sense Discrimination Language (SDL) specifies their actions to determine intended word senses in context. These languages constitute the formal theory of Word Expert Parsing, and permit the representation of all linguistic knowledge in terms of active wordbased distributed agents. An existing computer program translates word experts represented in these languages into executable processes that interact to cooperatively analyze sentences. The thesis makes a number of claims about the processes of natural language comprehension and its computational realization. A formal theory is developed and a computer model constructed to provide evidence to support those claims. Word Expert Parsing explains the understanding of sentences containing highly ambiguous words and complex structures. The distributed word-based approach is advanced as a framework for a full-scale theory of discourse comprehension.</abstract>
<title confidence="0.996147">Understanding English Descriptions of Programs</title>
<author confidence="0.999978">Allan Ramsay</author>
<affiliation confidence="0.999775">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.7599995">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.978285538461539">DAI Research Paper No. 140, 1980, 4 pages. A considerable amount of work has been done on verifying that computer programs fit their specifications. However, providing formal specifications is itself a difficult and tedious task, so that programs are generally only documented incompletely and imprecisely. This paper presents a computer system which accepts English descriptions of procedures and relates to LISP programs that are supposed to imple- This system is intended to illustrate how &amp;quot;informal&amp;quot; techniques may be used to provide a rough analysis of a program for which incomplete specifications are provided.</abstract>
<title confidence="0.976981">Parsing English Text</title>
<author confidence="0.999969">Allan Ramsay</author>
<affiliation confidence="0.9997805">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.76435">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.98525975">DAI Research Paper No. 139, 1980, 5 pages. This paper presents a technique for parsing English text according to a grammar specified as a set of rewrite rules. The paper describes a compact way of representing such a grammar and presents a program which uses this representation to parse text without backtracking and without repeating work that it has already done.</abstract>
<title confidence="0.99751">Using Determinism to Predict Garden Paths</title>
<author confidence="0.999966">Rob Milne</author>
<affiliation confidence="0.999777">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.7635065">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.990853">DAI Research Paper No. 142, 1980, 6 pages. I am interested in making a psychologically valid model of human natural language understanding, and especially in a processing model for predicting when a sentence will be a garden path. While extending the Marcus deterministic parser to include noun-noun modification, several counter examples to Marcus&apos; garden path prediction were found. In this paper I proposed that when people encounter an ambiguous situation that may lead to a garden path, they use semantics to decide rather than look ahead. I will present an extension to the garden path prediction mechanism of Marcus&apos; parser to account for this and several experiments to test this theory.</abstract>
<title confidence="0.991393">Parsing Against Lexical Ambiguity</title>
<author confidence="0.999986">Rob Milne</author>
<affiliation confidence="0.999953">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.978636">Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.9676436">DAI Research Paper No. 144, 1980, 8 pages. Marcus&apos; original deterministic parsing included almost no part-cf-speech ambiguity. In this paper, the addition of part-of-speech ambiguity to a deterministic parser written in PROLOG is described. To handle this ambiguity, it was necessary to add no special mechanisms to the parser. Instead the grammar rules were made to enforce agreement, and reject ungrammatical sentences. The resulting system is very effective and covers many examples of ambiguity.</abstract>
<note confidence="0.905519">Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981</note>
<title confidence="0.992142">The FINITE STRING Newsletter Abstracts of Current Literature Predictive Analysis in Sentence Comprehension: A Computer Simulation Model for Surface Structure Parsing</title>
<affiliation confidence="0.526693">Communications Psychology (Dept. 3Z11) Bell-Northern Research</affiliation>
<address confidence="0.726154333333333">P.O. Box 3511 Station C Ottawa, Ontario KIY 4H7 CANADA</address>
<abstract confidence="0.956028">Int. J. Man-Mach. Studies 13, 3 (Oct. 1980), 259-294. Through numerous models have been proposed by linguists and computer scientists for the parsing of natural language, Kimball (1973) has outlined one of the few that takes into account the operational limitaof the human perceiver. His analysis model is intuitively appealing and is supported by empirical research (e.g. Whaley, 1979). This paper presents the parsing principles suggested by Kimball, and describes a computer simulation which incorporates them. The parsing accuracy of the model is demonstrated and discussed for various sentence types. Finally, extensions to the existing model are considered including the quantification of the model&apos;s predictions.</abstract>
<title confidence="0.8327715">The Use of Artificial Intelligence Techniques in Computer-Assisted Instruction: An Overview</title>
<author confidence="0.998176">Alice Gable</author>
<author confidence="0.998176">Carl V Page</author>
<affiliation confidence="0.999827">Department of Computer Science Michigan State University</affiliation>
<address confidence="0.998427">East Lansing, Michigan 48824</address>
<abstract confidence="0.9858715">Int. J. Man-Mach. Studies 12, 3 (April 1980), 259-282. One of the major goals of research in Artificial Intelligence is the representation of knowledge so that a computer can solve problems or communicate in a manner which exhibits &amp;quot;common sense&amp;quot;. Few computer programs, including those for education, possess behavior which approaches any facet of the constellation of human skills and knowledge which are imprecisely called &amp;quot;common sense&amp;quot;. However, the revolutionary decline in hardware costs now makes it possible to consider economically viable, sophisticated designs for computer-aided instruction systems possessing some of the common sense attributes of a human tutor. In this survey we examine, in depth, techniques from Artificial Intelligence that can be used to endow a Computer-Aided Instruction system with approximations of some of the desirable qualities of a human tutor. We consider both techniques which have been proved in prototype systems for Computer-Aided Instruction and some techniques which were originally developed for other purposes.</abstract>
<title confidence="0.9287405">The Structure of The Merriam-Webster Pocket Dictionary</title>
<author confidence="0.999935">Robert A Amsler</author>
<affiliation confidence="0.999939">Department of Computer Sciences University of Texas</affiliation>
<address confidence="0.997857">Austin, Texas 78712</address>
<abstract confidence="0.987512907894737">Technical Report TR-164, Dec. 1980. The dissertation has as its purpose the exploration and discussion of the structure of a machine-readable copy of an ordinary pocket dictionary with particular attention to the utility of the information contained therein for future application in computational linguistics, ethnosemantics, and information science. This structure was first explored by hand using two printouts of the Dictionary&apos;s prepared from magnetic tapes produced by John Olney at System Development Corporation. Initially the goal of the analyses was the determination of whether useful semantic information could be derived from dictionary definitions. Once it was determined by several hand analyses that such data did provide a new source of information about the lexicon, the larger goal of assembling a complete taxonomy of the words in the dictionary was conceived and undertaken. A hand analysis of the rich semantic information contained in the dictionary for verbs defined in terms of &amp;quot;move&amp;quot; is presented in Chapter 2. This componential analysis of a set of definitions revealed the potential of dictionaries for use as the basis of numerous additional studies of high-frequency verbs based upon their usage in defining more specific verbs. Chapter 3 presents the results of a hand analysis of the taxonomy of &amp;quot;vehicle&amp;quot; terms developed from a large sample of definitions based upon the word &amp;quot;vehicle&amp;quot; and its descendants. This study revealed that indeed the dictionary did contain large, coherent, and computationally useful information in a taxonomic-like organization that could be revealed by connecting together definitions on the basis of their defining terms. Chapter 4 deals with the steps involved in designing, loading, and selectively accessing large databases containing all the definitions of nouns, verbs, and adjectives contained in the dictionary. Statistical information on the frequencies and nature of the part of speech data contained in the dictionary is given in chapter 4 and appendix 3. A &amp;quot;word sense meaning&amp;quot; representation for uniquely specifying dictionary senses and the problems of semantic ambiguity in lexical usage are also illustrated. In chapter 5, statistics on the frequency of dictionary defining vocabulary and measures of semantic ambiguity are given. Section 5.2 contains an extensive Journal of Computational Linguistics, Volume 7, Number 2, April-June The FINITE STRING Newsletter Abstracts of Current Literature discussion of the steps taken to perform semantic disambiguation on dictionary definition terms along with further statistics on the frequencies of such disambiguated words in the dictionary. Appendix 4 gives part of a disambiguation protocol session transcribed from a tape-recording and demonstrating the nature of the human disambiguation task carried out on the dictionary. Finally, chapter 5 concludes with a discussion of the methods used to computationally assemble and enumerate the complete taxonomic structure of the dictionary&apos;s noun and verb definitions. Chapter 6 discusses some of the findings about the nature of the dictionary&apos;s taxonomies of nouns and verbs. It discusses the manners in which dictionary taxonomies, ultimately terminate in primitive root concepts, relationships to other taxonomies, case argument relattpns to verbs, and in partitives and collectives. Finally, chapter 7 concludes with a discussion of a possible means of automating the analysis procedures of chapter 4 and 5 to perform fully automatic parsing and disambiguation of dictionary entries. This discussion also demonstrates the application of the proposed disambiguation technique to natural language processing and computational linguistics in general.</abstract>
<title confidence="0.983273">The Automated Dictionary</title>
<author confidence="0.999953">Mark S Fox</author>
<author confidence="0.999953">Donald J Bebel</author>
<author confidence="0.999953">Alice C Parker</author>
<affiliation confidence="0.999902">Carnegie-Mellon University</affiliation>
<address confidence="0.887912">Schenley Park Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.87124">Computer 13, 7 (July 1980), 35-48. An automated dictionary is a computer-based de-</note>
<abstract confidence="0.989547388888889">vice that holds all or part of a dictionary and allows access to and the display of entries. &amp;quot;automation&amp;quot; of a dictionary raises several issues: What information should be stored? How does the automated dictionary&apos;s size determine the style of usage? What is the interface between the user and the dictionary? What are the automated dictionary&apos;s physical components? Each issue provides a rich set of alternatives from which to choose. Part of the design problem of an automated dictionary is picking feasible points in this multidimensional design space. The purpose of this article is to present the results of some preliminary studies conducted at the request of the National Institute of Education. We determine the capability of an automated dictionary system built using present technology and estimate present and future costs based on trends in the cost and functionality of digital electronics.</abstract>
<title confidence="0.900774">for Detecting and Correcting Spelling Errors</title>
<author confidence="0.999982">James L Peterson</author>
<affiliation confidence="0.99993">Department of Computer Sciences University of Texas</affiliation>
<address confidence="0.998516">Austin, Texas 78712</address>
<abstract confidence="0.8978533">Comm. ACM 23, 12 (Dec. 1980), 676-687. With the increase in word and text processing computer systems, programs which check and correct spelling will become more and more common. This paper investigates the basic structure of several such existing programs and their approaches to solving the problems which arise when this type of program is created. The basic framework and background necessary to write a spelling checker or corrector are provided.</abstract>
<title confidence="0.994278">Recognition of Spoken Spelled Names for Directory Assistance Using Speaker-Independent Templates</title>
<author confidence="0.997102">L R Rabiner</author>
<author confidence="0.997102">J G Wilpon</author>
<affiliation confidence="0.999012">Bell Telephone Laboratories, Inc.</affiliation>
<address confidence="0.9989455">600 Mountain Avenue Murray Hill, New Jersey 07974</address>
<abstract confidence="0.98594232">Bell Syst. Tech. J. 59, 4 (April 1980), 571-592. In a recent paper, Rosenberg and Schmidt demonstrated the applicability of a speaker-trained, isolated word speech recognizer to the problem of automatic directory assistance. Input to the system was in the form of a string of letters which spelled the last name and initials of an individual for whom a directory listing was required. Rosenberg and Schmidt found that, even though the recognition rate for individual letters was rather low (approximately 80 percent), the rate at which the correct directory listing was found was higher (approximately 95 percent). In this paper, we extend these results to include the case of speakerindependent recognition of letters. We show that overall performance in the speaker-independent mode is comparable to performance in a speaker-dependent mode and examine various factors important for operation in a speaker-independent mode, such as characteristics of the reference templates, choice of decision rule, and threshold parameters. For the most part, the overall system is remarkably robust to the parameters of the recognizer. For the best choice of these parameters, a 95-percent correct string rate is obtained, comparable to the performance in a speaker-dependent mode.</abstract>
<note confidence="0.895431">Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981</note>
<title confidence="0.988914666666667">The FINITE STRING Newsletter Abstracts of Current Literature A Minimum-Distance Search Technique and Its Application to Automatic Directory Assistance</title>
<author confidence="0.999734">S E Levinson</author>
<author confidence="0.999734">T G Szymanski</author>
<affiliation confidence="0.999899">Bell Telephone Laboratories, Inc.</affiliation>
<address confidence="0.99454">600 Mountain Hill Murray Hill, New Jersey 07974</address>
<abstract confidence="0.96555647368421">Bell Syst. Tech. J. 59, 8 (Oct. 1980), 1343-1356. This paper describes a new search procedure and its application to the problem of obtaining telephone directory information from spoken spelled input. The method obtains its speed from using the concept of equivalence classes, with names classified according to their letter-by-letter acoustic similarity. It derives its accuracy from the use of a minimum-distance criterion for selecting answers. The search procedure finds the name with the minimum distance, usually after only a small fraction of the directory file has been examined. Using an acoustic analyzer with an 80 percent correct recognition rate for individual letters, a 98.6 percent correct recognition rate for names was achieved when the method was applied to a directory of 18,000 entries. On the average, only 1.2 percent of the directory had to be examined for each query. With an input recognition rate of 71 percent for letters, the respective figures were 97.2 percent and 2.8 percent.</abstract>
<title confidence="0.817279">Synthesis by Rule of Prosodic Features in Word Concatenation Synthesis</title>
<author confidence="0.999917">S J Young</author>
<affiliation confidence="0.999546333333333">Control Systems Centre University of Manchester Institute of Science and Technology</affiliation>
<address confidence="0.999856">Manchester, M80 1QD, ENGLAND</address>
<author confidence="0.971221">F Fallside</author>
<affiliation confidence="0.997414">Engineering Department Cambridge University</affiliation>
<address confidence="0.9631705">Corn Exchange Street Cambridge, CB2 3QG, ENGLAND</address>
<abstract confidence="0.990092777777778">Int. J. Man-Mach. Studies 12, 4 (April 1980), 241-258. The quality of speech obtainable by the technique of Word Concatenation Synthesis depends crucially on the accuracy of the pitch and timing contours which need to be computed for each utterance synthesized. A method for the synthesis-by-rule of these prosodic features is described for utterances for which the only information available is a syntactic phrase marker and the lexical structure of each component word. Rules and algorithms are presented for the determination of word group boundaries and the placement of stress, accent and nuclei. A timing contour algorithm is described which implements the prominence required for stressed syllables and also generates the stress-timed rhythm of natural English. A corresponding pitch contour algorithm computes appropriate intonation patterns using a minimal set of three tone groups; fall, rise and fall-rise.</abstract>
<title confidence="0.700207">The Linguistic Reason Why the Computer Will Never Think</title>
<author confidence="0.998672">John R Hammen</author>
<address confidence="0.996008">P.O. Box 12931 Seattle, Washington 98111</address>
<abstract confidence="0.9799502">SIGLASH Newsletter 13, 4 (Dec. 1980), 8-16. This paper draws a comparison between the respective lowest denominators of computer and human language, both of which are the respective mediums of computer programming and human reasoning. The lowest denominator of the former is Boolean Algebra, is a logic defined by the 3 operators: Or, human language is the system of logic called semantic structure. The Boolean operators And, Or, exist in semantic structure.</abstract>
<note confidence="0.5614638">paper shows that the 3 determiners: 5 moods: Interrogative, Conditional, 3 tenses: Present, the 8 cas- Possessive, Source, Direction, Instrument,</note>
<abstract confidence="0.884643714285714">Location, just as like logic operators as do Or, This means that semantic structure is defined by 22 operators (19 more than Boolean Algebra). The author contends that because of this difference in numbers of operators the computer will never be programmed to think in a human fashion.</abstract>
<title confidence="0.999517">The Natural Language of Interactive Systems</title>
<author confidence="0.999805">Henry Ledgard</author>
<author confidence="0.999805">Andrew Singer</author>
<author confidence="0.999805">William Seymour</author>
<affiliation confidence="0.9992905">Computer and Information Science Department University of Massachusetts</affiliation>
<address confidence="0.683552">Massachusetts</address>
<author confidence="0.999603">John A Whiteside</author>
<affiliation confidence="0.999884">Digital Equipment Corporation</affiliation>
<address confidence="0.998199">146 Main Street Maynard, Massachusetts 01754</address>
<note confidence="0.713091">Comm. ACM 23, 10 (Oct. 1980), 556-563. The work reported here stems from our deep belief</note>
<abstract confidence="0.9916768">that improved human engineering can add significantly to the acceptance and use of computer technology. In particular, this report describes an experiment to test the hypothesis that certain features of natural language provide a useful guide for the human engineering of interactive command languages. The goal was to establish that a syntax employing familiar, descriptive, everyday words and well-formed English phrases contributes to a language that can be easily and effectively used. Users with varying degrees of interactive computing experience used two versions of an interactive text editor; one with an English-based command syntax in the sense described above, the other with a more notational syntax. Performance differences strongly favored the English-based editor.</abstract>
<note confidence="0.923533">Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981</note>
<title confidence="0.9866435">The FINITE STRING Newsletter Abstracts of Current Literature Intensional Concepts in Propositional</title>
<author confidence="0.99972">Anthony S Maida</author>
<affiliation confidence="0.999709">Rochester Institute of Technology</affiliation>
<address confidence="0.999669">Rochester, New York</address>
<author confidence="0.999695">Stuart C Shapiro</author>
<affiliation confidence="0.99969">Computer Science Department State University of New York at Buffalo</affiliation>
<address confidence="0.9992305">4230 Ridge Lea Road Amherst, New York 14226</address>
<abstract confidence="0.97668975">Technical Report 171, Feb. 1981. One of the major characteristics of semantic networks as a representation of knowledge is what we the principle: each concept represented in the network is represented by a unique node. We show that this principle entails that at least some nodes represent intensional, rather than extensional, objects. By taking the position that a semantic network is a model of the conceptual organization of a cognitive agent rather than of the world, we argue further that nodes can represent only intensional objects. To allow for coreferential terms, we include the representation of a proposition that asserts the extensional equivalence of two intensional concepts. This approach is shown to lead to elegant solutions to three apparently unrelated problems in the representation of knowledge. Our discussion brings to bear relevant literature in analytic philosophy and experimental psychology and leads us to suggest two new psychology experiments.</abstract>
<title confidence="0.9942025">COCCI: A Deductive Semantic Network Program for Solving Microbiology Unknowns</title>
<author confidence="0.999992">Stuart C Shapiro</author>
<affiliation confidence="0.9992395">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.998479">4230 Ridge Lea Road Amherst, New York 14226</address>
<pubnum confidence="0.280114">Technical Report 173, March 1981.</pubnum>
<abstract confidence="0.979553235294118">amp;quot;You have been given a culture of one of nine cocci. Identify it.&amp;quot; COCCI is a program to solve this problem. To identify the unknown, COCCI requests a human assistant to perform tests and make observations and report the results to it. COCCI consists of 14 specific facts and 17 deduction rules stored in SNePS, a general purpose deductive semantic network processing system, and a small ATN grammar for parsing and for generating English from the semantic network. All of COCCI&apos;s reasoning and interaction with humans is driven by the general purpose bi-directional inference sub-system of SNePS. This paper describes COCCI as an illustration of deductive semantic networks in general and SNePS in particular. Specific points covered include structure sharing, procedural attachment, generation grammars and the structure of deduction rules.</abstract>
<title confidence="0.697219">Bi-Directional Inference</title>
<author confidence="0.9976435">Joao P Martins</author>
<author confidence="0.9976435">Donald P McKay</author>
<author confidence="0.9976435">Stuart C Shapiro</author>
<affiliation confidence="0.9992395">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.998961">4230 Ridge Lea Road Amherst, New York 74226</address>
<abstract confidence="0.947000384615385">Technical Report 174, March 1981. In this paper we present a brief overview of the SNePS deduction system and show through an example the interaction between forward and backward inference. This interaction — resulting in a class of inference termed bi-directional inference — enables an easy and elegant way of performing resource-limited searches and the possibility of performing resourcelimited deduction in a natural and simple way. Furthermore, bi-directional inference focus a system&apos;s attention towards the interests of the user and can cut down the fan-out of pure forward or pure backward chaining.</abstract>
<title confidence="0.957628">A Belief Revision System Based on Relevance Logic and Heterarchical Contexts</title>
<author confidence="0.999978">Joao P Martins</author>
<author confidence="0.999978">Stuart C Shapiro</author>
<affiliation confidence="0.9992395">Department of Computer Science State University of New York at Buffalo</affiliation>
<address confidence="0.9982725">4230 Ridge Lea Road Amherst, New York 14226</address>
<abstract confidence="0.875548846153846">Technical Report 175, March 1981. This paper describes the underlying theory of a Belief Revision System based on Relevance Logic and Heterarchical Contexts. In our system each statement is indexed by the set of basic (i.e., non-derived) assumptions used in its derivation and by the set of basic assumptions with which it is incompatible. A context is a set of basic assumptions and contains all the statements whose first index is a subset of the context and whose second index is disjoint from the context. This allows straightforward switching between contexts and the possibility of efficiently performing hypothetical reasoning.</abstract>
<title confidence="0.9160405">Interacting in Natural Language with Artificial Systems: The Donau Project</title>
<author confidence="0.999086">Giovanni Guida</author>
<author confidence="0.999086">Marco Somalvico</author>
<affiliation confidence="0.963085666666667">Milan Polytechnic Artificial Intelligence Project Istituto di Elettrotecnica ed Elettronica Politecnico di Milano</affiliation>
<address confidence="0.970084">Milan, ITALY</address>
<abstract confidence="0.98681296875">Inform. Systems 5, 4 (1980), 333-344. This paper is intended to propose a new methodological approach to the conception and development of natural language understanding systems. This new contribution is supported by the design, implementation, and experimentation of DONAU: a general purpose domain-oriented natural language understanding Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 The FINITE STRING Newsletter Abstracts of Current Literature system developed and presently running at the Milan Polytechnic Artificial Intelligence Project. The system is based on a two-level modular architecture intended to overcome the lack of flexibility and generality often pointed out in many existing systems, and to facilitate the exchange of results and actual experiences between different projects. The horizontal level allows an independent and parallel development of the single segments of the system (syntactic analyser, semantic analyser, information extractor, legality controller). The vertical level ensures the possibility of changing (enlarging or redefining) the definition of the semantic domain on which each particular version of the system is oriented and specialized in a simple, incremental, and user-oriented way. In the paper the general architecture of the system and the mode of operation of each segment are illustrated in detail. Linguistic models, knowledge representation, and parsing algorithms are described and illustrated by means of selected examples. Performance evaluations of the system in the application version on data base inquiry are reported and discussed. Promising directions for future research are presented in the conclusions.</abstract>
<title confidence="0.957825">Using the Data Base as a Semantic Component to Aid in the Parsing of Natural Language Data Base Queries</title>
<author confidence="0.999862">Larry R Harris</author>
<affiliation confidence="0.999604">Mathematics Department Dartmouth College</affiliation>
<address confidence="0.999199">Hanover, New Hampshire 03755</address>
<abstract confidence="0.993594466666667">J. Cybern. 10, 1-3 (Jan.-March 1980), 77-96. Many of the recent advances in artificial intelligence (Al) have been brought about through the use of domain specific knowledge. In this same spirit, this paper presents an approach to understanding natural language data base queries that employs the use of domain specific knowledge to aid the parsing process. The unique aspect of the data base query environment relative to other Al problem domains is that the required body of knowledge already exists in the form of the data base being queried. Thus, there is an important benefit of making use of this knowledge in the form in which it is maintained by the data base management system, since the very difficult problems of gathering and representing this information can be circumvented. The paper describes the problems encountered in the parsing of data base queries that can be solved by the semantic use of the data base, as well as a precise description of how these problems can be solved by existing data base systems. The proposed use of the data base as a semantic component has been a promary component of the design of a high performance natural language data base query system called RO- BOT, that has been successfully installed at several commercial installations. Since this system is a physical realization of the proposed methodology, a brief description of the system and its experiences in the field are given as evidence of the feasibility of this approach.</abstract>
<title confidence="0.954494">Natural Language Access to Medical Text</title>
<author confidence="0.990143">Donald E Walker</author>
<author confidence="0.990143">Jerry R Hobbs</author>
<affiliation confidence="0.9997355">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997897">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.988291647058824">Technical Note 240, March 1981. This paper describes research on the development of a methodology for representing the information in texts and of procedures for relating the linguistic structure of a request to the corresponding representations. The work is being done in the context of a prototype system that will allow physicians and other health professionals to access information in a computerized textbook of hepatitis through natural language dialogues. The interpretation of natural language queries is derived from DIAMOND/DIAGRAM, a linguistically motivated, domain-independent natural language interface developed at SRI. A text access component is being developed that uses representations of the propositional content of text passages and of the hierarchical structure of the text as a whole to retrieve relevant information.</abstract>
<title confidence="0.9976255">An Efficient Easily Adaptable System Interpreting Natural Language</title>
<author confidence="0.999981">David H D Warren</author>
<affiliation confidence="0.999691">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.7674265">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<author confidence="0.989804">Fernando C N Pereira</author>
<affiliation confidence="0.997088">CAAS Studies, Department of Architecture University of Edinburgh</affiliation>
<address confidence="0.765386">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.983884210526316">DAI Research Paper No. 155, 1981, 18 pages. This paper gives an overall account of a prototype natural language question answering system, called Chat-80. Chat-80 has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in PROLOG, a programming language based on logic. With the aid of a logic-based grammar formalism called extraposition grammars, Chat-80 translates English questions into the PROLOG subset of logic. The resulting logical expression is then transformed by a planning algorithm into efficient PROLOG, cf. &amp;quot;query optimisation&amp;quot; in a database. Finally the is executed to yield the answer. On a domain of world geography, most questions within the English subset Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 The FINITE STRING Newsletter Abstracts of Current Literature are answered in well under one second, including relatively complex queries.</abstract>
<title confidence="0.968728">A Computer Written Language Lab</title>
<author confidence="0.996433">Mike Sharpies</author>
<affiliation confidence="0.999727">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.762524">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.97241175">DAI Research Paper No. 134, 1980, 9 pages. A group of Edinburgh primary school children are using a prototype computer written language lab, designed to help them understand language and develop their writing style. In a more sophisticated version the language lab can become a resource for teachers, linguists, indeed anyone interested in creating or investigating language.</abstract>
<title confidence="0.978439">A Computer Based Language Workshop</title>
<author confidence="0.998023">Mike Sharpies</author>
<affiliation confidence="0.999738">Department of Artificial Intelligence University of Edinburgh</affiliation>
<address confidence="0.7657505">Forrest Hill Edinburgh EH1 2QL SCOTLAND</address>
<abstract confidence="0.99500975">DAI Research Paper No. 135, 1980, 11 pages. The paper describes a project to implement and evaluate a computer written language workshop. The pilot phase, now completed, provides pupils with a small range of language manipulation programs. The second phase will offer children a more extensive range of computer based language aids for the exploration of written style.</abstract>
<note confidence="0.559311714285714">Reading, Looking and Learning Christine Urquhart 38 Shipton Road Sutton Coldfield W. Midlands B721 NR, ENGLAND Journal of Inform. Sci. 1 (1980), 333-344. The survey of aspects of reading research discusses</note>
<abstract confidence="0.9977584">reading styles, &apos;readability&apos;, and the influence of presentation on reading and comprehension. Consideration of these leads to the fundamental problems of how the mind and eye interact in visual processing of text. Some research on the psychology of reading is reviewed, and certain psycholinguistic models of the reading process are discussed. Text structure and content appear to influence comprehension and learning, and relevant research on this, and the interaction of the reader with the text, is outlined. Reading strategies are discussed, and the possibilities of altering reading behaviour by increasing reading speed or improving learning patterns are reviewed. Possible implications for information science of reading research are mentioned.</abstract>
<affiliation confidence="0.8458175">Expert Systems Machine Intelligence Research Unit University of Edinburgh Hope Park Square, Meadow Lane</affiliation>
<address confidence="0.820752">Edinburgh EH8 9NW, SCOTLAND</address>
<abstract confidence="0.851495">Computer Journal 23, 4 (Nov. 1980), 369-376. A central feature of &apos;expert systems&apos; in chemistry, molecular geology, medicine, plant pathology, robotics, chess and other applications is the man-machine communication of descriptive concepts in the form of patterns. &apos;Humanisation&apos; of machine-made representations must find a place in future designs.</abstract>
<title confidence="0.989491">A Linguistic Approach to Decisionmaking with Fuzzy Sets</title>
<author confidence="0.999997">Richard M Tong</author>
<affiliation confidence="0.991608">Advanced Information and Decision Systems</affiliation>
<address confidence="0.967080666666667">201 San Antonio Circle Suite 286 View, California</address>
<author confidence="0.998247">Piero P Bonissone</author>
<affiliation confidence="0.964582">Corporate Research and Development Department General Electric Company</affiliation>
<address confidence="0.960972333333333">P.O. Box 43 Building 37-579 Schenectady, New York 12301</address>
<note confidence="0.8322005">IEEE Trans. Sys. Man Cyb. 10, 11 (Nov. 1980), 716-722. A technique for making linguistic decisions is pres-</note>
<abstract confidence="0.998518272727273">ented. Fuzzy sets are assumed to be an appropriate way of dealing with uncertainty, and it is therefore concluded that decisions taken on the basis of such information must themselves be fuzzy. It is inappropriate then to present the decision in numerical form; a statement in natural language is much better. For brevity only a single-stage multiattribute decision problem is considered. Solutions to such problems are shown using ideas in linguistic approximation and truth qualification. An extensive example illuminates the basic ideas and techniques.</abstract>
<title confidence="0.9724215">Natural Language Programming and Natural Programming Languages</title>
<author confidence="0.999985">Larry H Reeker</author>
<affiliation confidence="0.999879">Department of Computer Science University of Queensland</affiliation>
<address confidence="0.989776">St. Lucia, Brisbane Queensland, AUSTRALIA 4067</address>
<abstract confidence="0.972154461538462">Aust. Comput. J. 12, 3 (Aug. 1980), 89-92. The idea of &amp;quot;natural language programming&amp;quot; has a good deal of superficial attractiveness, but it also raises a number of questions. The most immediate of these is what one means by &amp;quot;natural language programming&amp;quot;, since the use of unfettered, everyday language is not feasible, for a number of reasons. Two approaches to moving toward the goal of more natural programming languages using natural language are Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 The FINITE STRING Newsletter Abstracts of Current Literature discussed, and the second, or &amp;quot;bottom up&amp;quot; approach is advocated.</abstract>
<title confidence="0.9891485">Natural Language Programming: Styles, Strategies and Contrasts</title>
<author confidence="0.999757">Lance A Miller</author>
<affiliation confidence="0.9993785">Computer Sciences Department IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.964936">Yorktown Heights, New York 10598</address>
<abstract confidence="0.9458698">IBM Systems Journal 20, 2 (1981), 184-215. College students who were not familiar with computers were asked to produce written natural language procedural instructions as directions for others to follow. These directions were solutions for six filemanipulation problems that also could reasonably be solved by writing computer programs. The written texts were examined from five points of view: solution correctness, preferences of expression, contextual referencing, word usage, and formal programming languages. The results provide insight both on the manner in which people express computer-like procedures &amp;quot;naturally&amp;quot; and on what features programming languages should include if they are to be made more &amp;quot;natural-like.&amp;quot;</abstract>
<title confidence="0.471416">Probabilistic Languages: A Review</title>
<author confidence="0.433525">Some Open Questions</author>
<affiliation confidence="0.995992">Bell Telephone Laboratories, Inc.</affiliation>
<address confidence="0.998274">600 Mountain Avenue Murray Hill, New Jersey 07974</address>
<abstract confidence="0.958051230769231">Computing Surveys, 12, 4 (Dec. 1980), 361-379. Context-free languages are commonly used to describe the structure of programming languages. However many interesting problems involve not just a language&apos;s structure but also the actual usage of the language. Adding a notion of probability to ordinary grammars gives rise to probabilistic context-free grammars. Interesting in their own right because of some pretty theorems, probabilistic context-free languages can be applied to the analysis of programming languages, automatic parsers, and error correctors. A complete outline of the theory is presented with examples. Some open questions are posed.</abstract>
<title confidence="0.975814">Compiler Testing Using a Sentence Generator</title>
<author confidence="0.7706755">A Celentano</author>
<author confidence="0.7706755">S Crespi Reghizzi</author>
<author confidence="0.7706755">P Della Vigna</author>
<author confidence="0.7706755">C Ghezzi</author>
<affiliation confidence="0.948597">Istituto di Elettrotecnica ed Elettronica Politecnico di Milano</affiliation>
<address confidence="0.987136">Piazza L. da Vinci 32 1-20133 Milano ITALY</address>
<author confidence="0.556025666666667">C Olivetti</author>
<author confidence="0.556025666666667">C S p A Via Jervis</author>
<affiliation confidence="0.618638">Ivrea ITALY</affiliation>
<abstract confidence="0.951629866666667">Softw. Pract. Exper. 10, 11 (Nov. 1980), 897-918. A system for assisting in the testing phase of compilers is described. The definition of the language to be compiled drives an automatic sentence generator. The language is described by an extended BNF grammar which can be augmented by actions to ensure contextual congruence, e.g. between definition and use of identifiers. For deep control of the structure of the produced sample the grammar can be described by step-wise refinements: the generator is iteratively applied to each level of refinement, producing at last compilable, complete programs. The implementation is described and some experimental results are reported concerning PLZ, MINIPL and some other languages.</abstract>
<title confidence="0.720625">Soft Display Key for Kanji Input</title>
<author confidence="0.999949">Jouko J Seppanen</author>
<affiliation confidence="0.9968265">Helsinki University of Technology Computing Centre</affiliation>
<address confidence="0.993825">021 Espoo 15, FINLAND</address>
<note confidence="0.5344425">Research Report 17, 1980. The concept of a soft display key as applied to input</note>
<abstract confidence="0.993856142857143">of large character sets or vocabularies such as Kanji, the ancient Chinese ideographic script, is discussed. The Japanese orthography and the necessity of using Kanji characters in data terminals are explained. Problems arising from the number and complexity of Kanji symbols for the manufacture and use of keyboard devices are stated. A review is made of devices and methods presently used or suggested. The feasibility of the soft display key is then demonstrated. Some requirements for the design and implementation of a soft display keyboard for Kanji are considered. In conclusion, implications to man/computer interface design, human factors engineering and hardware unification and standardization are stated.</abstract>
<note confidence="0.665682">Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<institution>Machine Intelligence Research Unit University of Edinburgh</institution>
<marker></marker>
<rawString>Machine Intelligence Research Unit University of Edinburgh</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hope Park Square</author>
</authors>
<title>Meadow Lane Edinburgh EH8 9NW,</title>
<date>1980</date>
<journal>SCOTLAND Computer Journal</journal>
<volume>23</volume>
<pages>369--376</pages>
<marker>Square, 1980</marker>
<rawString>Hope Park Square, Meadow Lane Edinburgh EH8 9NW, SCOTLAND Computer Journal 23, 4 (Nov. 1980), 369-376.</rawString>
</citation>
<citation valid="false">
<title>A central feature of &apos;expert systems&apos; in chemistry, molecular geology, medicine, plant pathology, robotics, chess and other applications is the man-machine communication of descriptive concepts in the form of patterns. &apos;Humanisation&apos; of machine-made representations must find a place in future designs. A Linguistic Approach to Decisionmaking with Fuzzy Sets</title>
<marker></marker>
<rawString>A central feature of &apos;expert systems&apos; in chemistry, molecular geology, medicine, plant pathology, robotics, chess and other applications is the man-machine communication of descriptive concepts in the form of patterns. &apos;Humanisation&apos; of machine-made representations must find a place in future designs. A Linguistic Approach to Decisionmaking with Fuzzy Sets</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Richard</author>
</authors>
<title>Tong Advanced Information and Decision Systems 201 San Antonio Circle Suite 286</title>
<marker>Richard, </marker>
<rawString>Richard M. Tong Advanced Information and Decision Systems 201 San Antonio Circle Suite 286</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mountain View</author>
</authors>
<booktitle>California 94040 Piero P. Bonissone Corporate Research and Development Department General Electric Company</booktitle>
<marker>View, </marker>
<rawString>Mountain View, California 94040 Piero P. Bonissone Corporate Research and Development Department General Electric Company</rawString>
</citation>
<citation valid="false">
<authors>
<author>P O Box</author>
</authors>
<title>43 Building 37-579</title>
<pages>12301</pages>
<location>Schenectady, New York</location>
<marker>Box, </marker>
<rawString>P.O. Box 43 Building 37-579 Schenectady, New York 12301</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sys Man Cyb</author>
</authors>
<date>1980</date>
<booktitle>Natural Language Programming: Styles, Strategies and Contrasts</booktitle>
<volume>10</volume>
<pages>11</pages>
<marker>Cyb, 1980</marker>
<rawString>IEEE Trans. Sys. Man Cyb. 10, 11 (Nov. 1980), 716-722. Natural Language Programming: Styles, Strategies and Contrasts</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Lance</author>
</authors>
<title>Miller Computer Sciences Department IBM Thomas J. Watson Research Center Yorktown Heights,</title>
<pages>10598</pages>
<location>New York</location>
<marker>Lance, </marker>
<rawString>Lance A. Miller Computer Sciences Department IBM Thomas J. Watson Research Center Yorktown Heights, New York 10598</rawString>
</citation>
<citation valid="true">
<date>1981</date>
<journal>IBM Systems Journal</journal>
<booktitle>Probabilistic Languages: A Review and Some Open Questions C.S. Wetherell Bell Telephone Laboratories, Inc. 600 Mountain Avenue</booktitle>
<volume>20</volume>
<pages>184--215</pages>
<marker>1981</marker>
<rawString>IBM Systems Journal 20, 2 (1981), 184-215. Probabilistic Languages: A Review and Some Open Questions C.S. Wetherell Bell Telephone Laboratories, Inc. 600 Mountain Avenue</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murray Hill</author>
</authors>
<date>0797</date>
<journal>Computing Surveys,</journal>
<volume>12</volume>
<pages>361--379</pages>
<location>New Jersey</location>
<marker>Hill, 0797</marker>
<rawString>Murray Hill, New Jersey 07974 Computing Surveys, 12, 4 (Dec. 1980), 361-379.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Celentano</author>
<author>S Crespi Reghizzi</author>
<author>P Della Vigna</author>
<author>C Ghezzi</author>
</authors>
<title>Compiler Testing Using a Sentence Generator</title>
<marker>Celentano, Reghizzi, Vigna, Ghezzi, </marker>
<rawString>Compiler Testing Using a Sentence Generator A. Celentano, S. Crespi Reghizzi, P. Della Vigna and C. Ghezzi</rawString>
</citation>
<citation valid="false">
<authors>
<author>ITALY G Granata</author>
<author>F Savoretti</author>
</authors>
<title>Istituto di Elettrotecnica ed Elettronica Politecnico di Milano Piazza L. da</title>
<journal>Vinci</journal>
<volume>32</volume>
<pages>1--20133</pages>
<location>Milano</location>
<marker>Granata, Savoretti, </marker>
<rawString>Istituto di Elettrotecnica ed Elettronica Politecnico di Milano Piazza L. da Vinci 32 1-20133 Milano ITALY G. Granata and F. Savoretti</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Olivetti</author>
<author>C S p A</author>
</authors>
<journal>Via Jervis</journal>
<volume>77</volume>
<marker>Olivetti, A, </marker>
<rawString>Ing. C. Olivetti &amp; C. S.p.A. Via Jervis 77</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pract</author>
</authors>
<date>1980</date>
<journal>Exper.</journal>
<volume>10</volume>
<pages>11</pages>
<marker>Pract, 1980</marker>
<rawString>Ivrea ITALY Softw. Pract. Exper. 10, 11 (Nov. 1980), 897-918.</rawString>
</citation>
<citation valid="true">
<date></date>
<journal>Research Report</journal>
<booktitle>Computing Centre 021 Espoo 15,</booktitle>
<volume>17</volume>
<institution>Soft Display Key for Kanji Input Jouko J. Seppanen Helsinki University of Technology</institution>
<marker></marker>
<rawString>Soft Display Key for Kanji Input Jouko J. Seppanen Helsinki University of Technology Computing Centre 021 Espoo 15, FINLAND Research Report 17, 1980.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>