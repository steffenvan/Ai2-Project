<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.941867">
Variational Inference for Adaptor Grammars
</title>
<author confidence="0.986988">
Shay B. Cohen
</author>
<affiliation confidence="0.887505666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.991189">
scohen@cs.cmu.edu
</email>
<author confidence="0.995711">
David M. Blei
</author>
<affiliation confidence="0.977722">
Computer Science Department
Princeton University
</affiliation>
<address confidence="0.693838">
Princeton, NJ 08540, USA
</address>
<email confidence="0.994317">
blei@cs.princeton.edu
</email>
<author confidence="0.957881">
Noah A. Smith
</author>
<affiliation confidence="0.886299666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.993422">
nasmith@cs.cmu.edu
</email>
<sectionHeader confidence="0.994676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99932345">
Adaptor grammars extend probabilistic
context-free grammars to define prior dis-
tributions over trees with “rich get richer”
dynamics. Inference for adaptor grammars
seeks to find parse trees for raw text. This
paper describes a variational inference al-
gorithm for adaptor grammars, providing
an alternative to Markov chain Monte Carlo
methods. To derive this method, we develop
a stick-breaking representation of adaptor
grammars, a representation that enables us
to define adaptor grammars with recursion.
We report experimental results on a word
segmentation task, showing that variational
inference performs comparably to MCMC.
Further, we show a significant speed-up when
parallelizing the algorithm. Finally, we report
promising results for a new application for
adaptor grammars, dependency grammar
induction.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919090909091">
Recent research in unsupervised learning for NLP
focuses on Bayesian methods for probabilistic gram-
mars (Goldwater and Griffiths, 2007; Toutanova and
Johnson, 2007; Johnson et al., 2007). Such meth-
ods have been made more flexible with nonparamet-
ric Bayesian (NP Bayes) methods, such as Dirichlet
process mixture models (Antoniak, 1974; Pitman,
2002). One line of research uses NP Bayes meth-
ods on whole tree structures, in the form of adaptor
grammars (Johnson et al., 2006; Johnson, 2008b;
Johnson, 2008a; Johnson and Goldwater, 2009), in
order to identify recurrent subtree patterns.
Adaptor grammars provide a flexible distribu-
tion over parse trees that has more structure than
a traditional context-free grammar. Adaptor gram-
mars are used via posterior inference, the compu-
tational problem of determining the posterior distri-
bution of parse trees given a set of observed sen-
tences. Current posterior inference algorithms for
adaptor grammars are based on MCMC sampling
methods (Robert and Casella, 2005). MCMC meth-
ods are theoretically guaranteed to converge to the
true posterior, but come at great expense: they are
notoriously slow to converge, especially with com-
plex hidden structures such as syntactic trees. John-
son (2008b) comments on this, and suggests the use
of variational inference as a possible remedy.
Variational inference provides a deterministic al-
ternative to sampling. It was introduced for Dirich-
let process mixtures by Blei and Jordan (2005) and
applied to infinite grammars by Liang et al. (2007).
With NP Bayes models, variational methods are
based on the stick-breaking representation (Sethu-
raman, 1994). Devising a stick-breaking represen-
tation is a central challenge to using variational in-
ference in this setting.
The rest of this paper is organized as follows. In
§2 we describe a stick-breaking representation of
adaptor grammars, which enables variational infer-
ence (§3) and a well-defined incorporation of recur-
sion into adaptor grammars. In §4 we give an em-
pirical comparison of the algorithm to MCMC in-
ference and describe a novel application of adaptor
grammars to unsupervised dependency parsing.
</bodyText>
<sectionHeader confidence="0.984405" genericHeader="method">
2 Adaptor Grammars
</sectionHeader>
<bodyText confidence="0.9999145">
We review adaptor grammars and develop a stick-
breaking representation of the tree distribution.
</bodyText>
<subsectionHeader confidence="0.996436">
2.1 Definition of Adaptor Grammars
</subsectionHeader>
<bodyText confidence="0.999654555555555">
Adaptor grammars capture syntactic regularities in
sentences by placing a nonparametric prior over the
distribution of syntactic trees that underlie them.
The model exhibits “rich get richer” dynamics: once
a tree is generated, it is more likely to reappear.
Adaptor grammars were developed by Johnson et
al. (2006). An adaptor grammar is a tuple A =
(G, M, a, b, α), which contains: (i) a context-free
grammar G = (W, N, R, 5) where W is the set of
</bodyText>
<page confidence="0.9711">
564
</page>
<note confidence="0.752491">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 564–572,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998309733333333">
terminals, N is the set of nonterminals, R is a set of
production rules, and S E N is the start symbol—we
denote by RA the subset of R with left-hand side A;
(ii) a set of adapted nonterminals, M C N; and (iii)
parameters a, b and α, which are described below.
An adaptor grammar assumes the following gen-
erative process of trees. First, the multinomial dis-
tributions 0 for a PCFG based on G are drawn
from Dirichlet distributions. Specifically, multino-
mial 0A — Dir(αA) where α is collection of Dirich-
let parameters, indexed by A E N.
Trees are then generated top-down starting with
S. Any non-adapted nonterminal A E N \ M is
expanded by drawing a rule from RA. There are
two ways to expand A E M:
</bodyText>
<listItem confidence="0.878207">
1. With probability (nz — bA)/(nA + aA) we ex-
pand A to subtree z (a tree rooted at A with a
yield in W*), where nz is the number of times
the tree z was previously generated and nA is the
total number of subtrees (tokens) previously gen-
erated root being A. We denote by a the concen-
tration parameters and b the discount parameters,
both indexed by A E M. We have aA &gt; 0 and
bA E [0, 11.
2. With probability (aA + kAbA)/(nA + aA), A is
expanded as in a PCFG by a draw from 0A over
RA, where kA is the number of subtrees (types)
previously generated with root A.
</listItem>
<bodyText confidence="0.992321717391304">
For the expansion of adapted nonterminals, this
process can be explained using the Chinese restau-
rant process (CRP) metaphor: a “customer” (cor-
responding to a partially generated tree) enters a
“restaurant” (corresponding to a nonterminal) and
selects a “table” (corresponding to a subtree) to at-
tach to the partially generated tree. If she is the first
customer at the table, the PCFG (G, 0) produces the
new table’s associated “dish” (a subtree).1
When adaptor grammars are defined using the
CRP, the PCFG G has to be non-recursive with re-
1We note that our construction deviates from the strict def-
inition of adaptor grammars (Johnson et al., 2006): (i) in our
construction, we assume (as prior work does in practice) that
the adaptors in A = (G, M, a, b, α) follow the Pitman-Yor
(PY) process (Pitman and Yor, 1997), though in general other
stochastic processes might be used; and (ii) we place a sym-
metric Dirichlet over the parameters of the PCFG, 9, whereas
Johnson et al. used a fixed PCFG for the definition (though they
experimented with a Dirichlet prior).
spect to the adapted nonterminals. More precisely,
for A E N, denote by Reachable(G, A) all the non-
terminals that can be reached from A using a partial
derivation from G. Then we restrict G such that
for all A E M, we have A E/ Reachable(G, A).
Without this restriction, we might end up in a sit-
uation where the generative process is ill-defined:
in the CRP terminology, a customer could enter a
restaurant and select a table whose dish is still in
the process of being selected.2 In the more general
form of adaptor grammars with arbitrary adaptors,
the problem amounts to mutually dependent defini-
tions of distributions which rely on the others to be
defined. We return to this problem in §3.1.
Inference The inference problem is to compute
the posterior distribution of parse trees given ob-
served sentences x = (x1, ... , x.). Typically, in-
ference with adaptor grammars is done with Gibbs
sampling. Johnson et al. (2006) use an embedded
Metropolis-Hastings sampler (Robert and Casella,
2005) inside a Gibbs sampler. The proposal distribu-
tion is a PCFG, resembling a tree substitution gram-
mar (TSG; Joshi, 2003). The sampler of Johnson et
al. is based on the representation of the PY process
as a distribution over partitions of integers. This rep-
resentation is not amenable to variational inference.
</bodyText>
<subsectionHeader confidence="0.999281">
2.2 Stick-Breaking Representation
</subsectionHeader>
<bodyText confidence="0.999714461538461">
To develop a variational inference algorithm for
adaptor grammars, we require an alternative repre-
sentation of the model in §2.1. The CRP-based def-
inition implicitly marginalizes out a random distri-
bution over trees. For variational inference, we con-
struct that distribution.
We first review the Dirichlet process and its stick-
breaking representation. The Dirichlet process de-
fines a distribution over distributions. Samples from
the Dirichlet process tend to deviate from a base
distribution depending on a concentration parame-
ter. Let G — DP(G0, a) be a distribution sampled
from the Dirichlet process with base distribution G0
</bodyText>
<footnote confidence="0.996346142857143">
2Consider the simple grammar with rules { S --+ S S, S --+ a
}. Assume that a customer enters the restaurant for S. She sits
at a table, and selects a dish, a subtree, which starts with the rule
S --+ S S. Perhaps the first child S is expanded by S --+ a. For
the second child S, it is possible to re-enter the “S restaurant”
and choose the first table, where the “dish” subtree is still being
generated.
</footnote>
<page confidence="0.998131">
565
</page>
<bodyText confidence="0.9922158125">
and concentration parameter a. The distribution G
is discrete, which means it puts positive mass on a
countable number of atoms drawn from G0. Re-
peated draws from G exhibit the “clustering prop-
erty,” which means that they will be assigned to the
same value with positive probability. Thus, they ex-
hibit a partition structure. Marginalizing out G, the
distribution of that partition structure is given by a
CRP with parameter a (Pitman, 2002).
The stick-breaking process gives a constructive
definition of G (Sethuraman, 1994). With the stick-
breaking process (for the PY process), we first sam-
ple “stick lengths” π ∼ GEM(a, b) (in the case of
Dirichlet process, we have b = 0). The GEM par-
titions the interval [0, 1] into countably many seg-
ments. First, draw vi ∼ Beta(1 − b, a + ib) for
</bodyText>
<equation confidence="0.9952935">
jli�1
i ∈ {1, ...}. Then, define 7ri °= vi j=1(1 − vj).
</equation>
<bodyText confidence="0.5412395">
In addition, we also sample infinitely many “atoms”
independently zi ∼ G0. Define G as:
</bodyText>
<equation confidence="0.998296">
G(z) = E&apos;1 7riS(zi, z) (1)
</equation>
<bodyText confidence="0.993919733333333">
where S(zi, z) is 1 if zi = z and 0 otherwise. This
random variable is drawn from a Pitman-Yor pro-
cess. Notice the discreteness of G is laid bare in the
stick-breaking construction.
With the stick-breaking representation in hand,
we turn to a constructive definition of the distri-
bution over trees given by an adaptor grammar.
Let A1, ... , AK be an enumeration of the nonter-
minals in M which satisfies: i ≤ j ⇒ Aj ∈�
Reachable(G, Ai). (That this exists follows from
the assumption about the lack of recursiveness of
adapted nonterminals.) Let Yield(z) be the yield of
a tree derivation z. The process that generates ob-
served sentences x = hx1, ... , xni from the adaptor
grammar A = hG, M, a, b, αi is as follows:
</bodyText>
<listItem confidence="0.870827095238095">
1. For each A ∈ N, draw θA ∼ Dir(αA).
2. For A from A1 to AK, define GA as follows:
(a) Draw πA  |aA, bA ∼ GEM(aA, bA).
(b) For i ∈ {1, ...}, grow a tree zA,i as follows:
i. Draw A → B1 ... Bn from RA.
ii. zA,i = A
�� � ���
B1 · · · Bn
iii. While Yield(zA,i) has nonterminals:
A. Choose an unexpanded nonterminal B
from yield of zA,i.
B. If B ∈ M, expand B according to GB
(defined on previous iterations of step 2).
C. If B ∈ N \ M, expand B with a rule from
RB according to Mult(θB).
(c) For i ∈ {1, ...}, define GA(zA,i) = 7rA,i
3. For i ∈ {1, ... , n} draw zi as follows:
(a) If 5 ∈ M, draw zi  |GS ∼ GS.
(b) If 5 ∈� M, draw zi as in 2(b) (omitted for
space).
4. Set xi = Yield(zi) for i ∈ {1, ... , n}.
</listItem>
<bodyText confidence="0.999577153846154">
Here, there are four collections of hidden variables:
the PCFG multinomials θ = {θA  |A ∈ N}, the
stick length proportions v = {vA  |A ∈ M} where
vA = hvA,1, vA,2, ...i, the adapted nonterminals’
subtrees zA = {zA,i  |A ∈ M; i ∈ {1, ...}} and
the derivations z1:n = z1, ... , zn. The symbol z
refers to the collection of {zA  |A ∈ M}, and z1:n
refers to the derivations of the data x.
Note that the distribution in 2(c) is defined with
the GEM distribution, as mentioned earlier. It is a
sample from the Pitman-Yor process (or the Dirich-
let process), which is later used in 3(a) to sample
trees for an adapted non-terminal.
</bodyText>
<sectionHeader confidence="0.987148" genericHeader="method">
3 Variational Inference
</sectionHeader>
<bodyText confidence="0.999527">
Variational inference is a deterministic alternative
to MCMC, which casts posterior inference as an
optimization problem (Jordan et al., 1999; Wain-
wright and Jordan, 2008). The optimized function
is a bound on the marginal likelihood of the obser-
vations, which is expressed in terms of a so-called
“variational distribution” over the hidden variables.
When the bound is tightened, that distribution is
close to the posterior of interest. Variational meth-
ods tend to converge faster than MCMC, and can be
more easily parallelized over multiple processors in
a framework such as MapReduce (Dean and Ghe-
mawat, 2004).
The variational bound on the likelihood of the
data is:
</bodyText>
<equation confidence="0.949687166666667">
log p(x  |a, α) ≥ H(q) + � Eq[log p(vA  |aA)]
AEM
+ � Eq[log p(θA  |αA)]
AEM
+ � Eq[logp(zA  |v, θ)] + Eq[logp(z  |vA)]
AEM
</equation>
<page confidence="0.972009">
566
</page>
<bodyText confidence="0.998906294117647">
Expectations are taken with respect to the variational
distribution q(v, θ, z) and H(q) is its entropy.
Before tightening the bound, we define the func-
tional form of the variational distribution. We use
the mean-field distribution in which all of the hid-
den variables are independent and governed by in-
dividual variational parameters. (Note that in the
true posterior, the hidden variables are highly cou-
pled.) To account for the infinite collection of ran-
dom variables, for which we cannot define a varia-
tional distribution, we use the truncated stick distri-
bution (Blei and Jordan, 2005). Hence, we assume
that, for all A ∈ M, there is some value NA such
that q(vA,NA = 1) = 1. The assigned probability to
parse trees in the stick will be 0 for i &gt; NA, so we
can ignore zA,i for i &gt; NA. This leads to a factor-
ized variational distribution:
</bodyText>
<equation confidence="0.9982156">
q(v, θ, z) = (2)
!
Y
q(vA,i) × q(zA,i) × q(zi)
i=1
</equation>
<bodyText confidence="0.982326769230769">
the grammaton G(A, sA,i) and for q(zi
observed sentence. We parametrize the grammaton
ized weighted grammars. Choosing such distribu-
tions is motivated by their ability to make the varia-
tional bound tight (similar to Cohen et al., 2008, and
Cohen and Smith, 2009). In practice we do not have
to use rewrite rules for all strings t
s in the gram-
maton. It suffices to add rewrite rules only for the
strings t = sA,i that have some grammaton attached
to them, G(A,
The variational distribution above yields a vari-
ational inference algorithm for approximating the
</bodyText>
<equation confidence="0.955321529411765">
|φA)
|φ).
§3.3)
∈
sA
hsA,1,
sA,NAi.
|φA)
|φ)
φA
φ)
⊆
sA,i).
γA,i,τA,φA
φ
d α. Let r be a PCFG rule. Let
�
</equation>
<bodyText confidence="0.9809463125">
567 Using a grammaton, we define the distributions
q(zA,i
and q(zi
This requires a pre-
processing step (described in detail in
that de-
fines, for each A
M, a list of strings
=
... ,
Then, for q(zA,i
we use
f(r, sB,k) = Ee(zk Os,k)1f(ri zk)1, where f(ri zk)
counts the number of times that rule r is applied in
the derivation zk. Let A → Q denote a rule from
use the grammaton G(A, xi) where xi is the ith
</bodyText>
<equation confidence="0.797987">
tity A
r, sB,k) is computed using the
we
</equation>
<bodyText confidence="0.851923411764706">
with weights
(or
for each rule in the gramma-
ton. This makes the variational distributions over the
trees for strings s (and trees for x) globally normal-
inside-outside (IO) algorithm. Fig. 1 gives the vari
-
ational inference updates.
posterior by estimating
and
it-
eratively, given a fixed set of hyperparameters
a, ban
rameters (a, b and
with respect to expected suffi-
cient statistics under the variational distribution. We
use Newton-Raphson for each (Boyd and Van
</bodyText>
<equation confidence="0.8280435">
α)
den-
</equation>
<bodyText confidence="0.84406">
berghe, 2004); Fig. 2 gives the objectives.
</bodyText>
<subsectionHeader confidence="0.828733">
3.1 Note about Recursive Grammars
</subsectionHeader>
<equation confidence="0.955094333333333">
YNA
i=1
q(θA)
Y
AEM
n
RB!∪ [ [ {Bi →
A→B1...B�ERAiE{i BiEM}
t
[
RA∪BEU
 |t ⊆ s}.
</equation>
<bodyText confidence="0.942751470588235">
It is natural to define the variational distributions
over
and v to be Dirichlet distributions with pa-
rameters
and Beta distributions with parameters
respectively. The two distributions over trees,
q(zA,i) and q(zi), are more problematic. For ex-
ample, with q(zi
we need to take into ac-
count different subtrees that could be generated by
the model and use them with the proper probabilities
in the variational distribution q(zi
We follow
and extend the idea from Johnson et al. (2006) and
use grammatons for these distributions. Gramma-
tons are
inspired by the grammar
</bodyText>
<figure confidence="0.985404228571428">
G.
For two strings in s, t
we use
to mean that t is a substring of s. In that case, a
grammaton is defined as follows:
Definition 1. Let A =
M, a, b,
be an adap-
tor grammar with G =
N, R,
Let s be a fi-
nite string over the alphabet of G and A
N.
be the set of nonterminals &apos;Ll °= Reachable(G, A)
(N \
θ
τA
γA,i,
|φ),
 |φ).
“mini-grammars,”
∈W*,
“t⊆s”
hG,
αi
hW,
5i.
∈
Let &apos;LL
∩
). The grammaton G(A, s) is the context-
free grammar with the start symbol A and the rules
G. The quan
Variational EM We use variational EM to fit the
hyperparameters. Variational EM is an EM algo-
</figure>
<bodyText confidence="0.98954204">
rithm where the E step is replaced by variational in-
ference (Fig. 1). The M-step optimizes the hyperpa-
With recursive grammars, the stick-breaking pro-
cess representation gives probability mass to events
which are ill-defined. In step 2(iii)(c) of the stick-
breaking representation, we assign nonzero proba-
bility to an event in which we choose to expand the
current tree using a subtree with the same index that
we are currently still expan
ding (see footnote 2). In
short, with recursive grammars, we can get “loops”
inside the trees.
We would still like to use recursion in the cases
which are not ill-defined. In the case of recur-
sive grammars, there is no problem with the stick-
breaking representation and the order by which we
enumerate the nonterminals. This is true because the
stick-breaking process separates allocating the prob-
abilities for each index in the stick and allocating the
atoms for each index in the stick.
Our variational distributions give probability 0 to
any event which is ill-defined in the sense men-
tioned above. Optimizing the variational bound in
this case is equivalent to optimizing the same vari-
ational bound with a model p0 that (i) starts with p,
</bodyText>
<equation confidence="0.909876714285714">
(ii) assigns probability 0 to ill-defined events, and
(iii) renormalizes:
Proposition 2. Let p(x, z) be a probability distri-
bution, where z ∈ Z, and let S ⊂ Z. Let Q = {q |
q(z) = 0, ∀z ∈ S}, a set of distributions. Then:
argmax Eq[log p(x, z)] = argmax Eq[log p0(x,z)]
q∈Q q
</equation>
<bodyText confidence="0.942748727272727">
where p0(x, z) is a probability distribution defined
as p0(x, z) = p(x, z)/ EI∈S p(x, z) for z ∈ S and
0 otherwise.
For this reason, our variational approximation al-
lows the use of recursive grammars. The use of re-
cursive grammars with MCMC methods is problem-
atic, since it has no corresponding probabilistic in-
terpretation, enabled by zeroing events that are ill-
defined in the variational distribution. There is no
underlying model such as p0, and thus the inference
algorithm is invalid.
</bodyText>
<subsectionHeader confidence="0.99944">
3.2 Time Complexity
</subsectionHeader>
<bodyText confidence="0.9998265">
The algorithm in Johnson et al. (2006) works by
sampling from a PCFG containing rewrite rules that
rewrite to a whole tree fragment. This requires
a procedure that uses the inside-outside algorithm.
Despite the grammar being bigger (because of the
rewrite rules to a string), the asymptotic complexity
of the IO algorithm stays O(|N|2|xi|3 + |N|3|xi|2)
where |xi |is the length of the ith sentence.3
</bodyText>
<footnote confidence="0.941258">
3This analysis is true for CNF grammars augmented with
rules rewriting to a whole string, like those used in our study.
</footnote>
<equation confidence="0.999823636363637">
y1A,i = 1 − bA + EB∈M Ek &amp;quot;1 f (A → sA,i, sB,k)
y2A,i = aA + ibA
+ Ej=1 EB∈M ENB
k =1 f (A → sA,j, sB,k)
TA,A→β = EB∈M ENB
k=1 f (A → N, sB,k)
/ 1 / 1 2
OA,A→sa,: = ΨlyA,i) −(( ΨlyA,i + yA,i)
+ Ej=1 (Ψ(y2A,i) − Ψ(y1A,i + y2A,i))
(E �
�A,A→β = Ψ(TA,A→β) − Ψ β TA,A→β
</equation>
<figureCaption confidence="0.997553">
Figure 1: Updates for variational inference with adaptor
grammars. Ψ is the digamma function.
</figureCaption>
<bodyText confidence="0.9999645">
Our algorithm requires running the IO algorithm
for each yield in the variational distribution, for each
nonterminal, and for each sentence. However, IO
runs with much smaller grammars coming from the
grammatons. The cost of running the IO algorithm
on the yields in the sticks for A ∈ M can be taken
into account parsing a string that appears in the cor-
pus with the full grammars. This leads to an asymp-
totic complexity of O(|N|2|xi|3 + |N|3|xi|2) for the
ith sentence in the corpus each iteration.
Asymptotically, both sampling and variational
EM behave the same. However, there are different
constants that hide in these asymptotic runtimes: the
number of iterations that the algorithm takes to con-
verge (for which variational EM generally has an ad-
vantage over sampling) and the number of additional
rewrite rules that rewrite to a string representing a
tree (for which MCMC has a relative advantage, be-
cause it does not use a fixed set of strings; instead,
the size of the grammars it uses grow as sampling
proceeds). In §4, we see that variational EM and
sampling methods are similar in the time it takes to
complete because of a trade-off between these two
constants. Simple parallelization, however, which
is possible only with variational inference, provides
significant speed-ups.4
</bodyText>
<subsectionHeader confidence="0.954659">
3.3 Heuristics for Variational Inference
</subsectionHeader>
<bodyText confidence="0.998690333333333">
For the variational approximation from §3, we need
to decide on a set of strings, sA,i (for A ∈ M and
i ∈ {1, ... , NA}) to define the grammatons in the
</bodyText>
<footnote confidence="0.54513575">
4Newman et al. (2009) show how to parallelize sampling al-
gorithms, but in general, parallelizing these algorithms is more
complicated than parallelizing variational algorithms and re-
quires further approximation.
</footnote>
<page confidence="0.967228">
568
</page>
<equation confidence="0.98924925">
�E �E ��
maxαA log P(|RA|αA) − |RA |log P(αA) + (αA − 1) A→-∈RA �(TA→-) − � A→-∈RA TA→-
maxaA 1 NA aA ((l//yA,i) −�(//yA,i + yA,i)) + log P(aA + 1 + ibA) − log P(ibA + aA)
maxbA ENA ibA (&apos;Fly2A,i) − &apos;FlyA,i + yA,i)) + log P(aA + 1 + ibA) − log P(1 − bA) − log P(ibA + aA)
</equation>
<figureCaption confidence="0.99166">
Figure 2: Variational M-step updates. P is the gamma function.
</figureCaption>
<bodyText confidence="0.997639875">
nonparametric stick. Any set of strings will give
a valid approximation, but to make the variational
approximation as accurate as possible, we require
that: (i) the strings in the set must be likely to be
generated using the adaptor grammar as constituents
headed by the relevant nonterminal, and (ii) strings
that are more likely to be generated should be asso-
ciated with a lower index in the stick. The reason for
the second requirement is the exponential decay of
coefficients as the index increases.
We show that a simple heuristic leads to an order
over the strings generated by the adaptor grammars
that yields an accurate variational estimation. We
begin with a weighted context-free grammar Gheur
that has the same rules as in G, only the weight for
all of its rules is 1. We then compute the quantity:
</bodyText>
<equation confidence="0.9306835">
I
&apos;F-Gheur[fi(z; A, s)] − p log |s|
</equation>
<bodyText confidence="0.960359916666667">
(3)
where fi(z; A, s) is a function computing the count
of constituents headed by A with yield s in the tree
z for the sentence xi. This quantity can be com-
puted by using the IO algorithm on Gheur. The term
p log |s |is subtracted to avoid preference for shorter
constituents, similar to Mochihashi et al. (2009).
While computing c(A, s) using the IO algorithm,
we sort the set of all substrings of s according to
their expected counts (aggregated over all strings s).
Then, we use the top NA strings in the sorted list for
the grammatons of A.5
</bodyText>
<subsectionHeader confidence="0.950408">
3.4 Decoding
</subsectionHeader>
<bodyText confidence="0.999903">
The variational inference algorithm gives a distribu-
tions over parameters and hidden structures (through
the grammatons). We experiment with two com-
monly used decoding methods: Viterbi decoding
</bodyText>
<footnote confidence="0.9850874">
5The requirement to select NA in advance is strict. We ex-
perimented with dynamic expansions of the stick, in the spirit
of Kurihara et al. (2006) and Wang and Blei (2009), but we did
not achieve better performance and it had an adverse effect on
runtime. For completeness, we give these results in §4.
</footnote>
<bodyText confidence="0.995488">
and minimum Bayes risk decoding (MBR; Good-
man, 1996).
To parse a string with Viterbi (or MBR) decoding,
we find the tree with highest score for the gramma-
ton which is attached to that string. For all rules
which rewrite to strings in the resulting tree, we
again perform Viterbi (or MBR) decoding recur-
sively using other grammatons.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999901666666667">
We describe experiments with variational inference
for adaptor grammars for word segmentation and de-
pendency grammar induction.
</bodyText>
<subsectionHeader confidence="0.998892">
4.1 Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999854">
We follow the experimental setting of Johnson and
Goldwater (2009), who present state-of-the-art re-
sults for inference with adaptor grammars using
Gibbs sampling on a segmentation problem. We
use the standard Brent corpus (Brent and Cartwright,
1996), which includes 9,790 unsegmented phone-
mic representations of utterances of child-directed
speech from the Bernstein-Ratner (1987) corpus.
Johnson and Goldwater (2009) test three gram-
mars for this segmentation task. The first grammar
is a character unigram grammar (GUnigram). The
second grammar is a grammar that takes into con-
sideration collocations (GColloc) which includes the
rules { Sentence —* Colloc, Sentence —* Colloc Sen-
tence, Colloc —* Word+, Word —* Char+ }. The
third grammar incorporates more prior knowledge
about the syllabic structure of English (GSyllable).
GUnigram and GSyllable can be found in Johnson
and Goldwater (2009). Once an utterance is parsed,
Word constituents denote segments.
The value of p (penalty term for string length) had
little effect on our results and was fixed at p = −0.2.
When NA (number of strings used in the variational
distributions) is fixed, we use NA = 15,000. We re-
port results using Viterbi and MBR decoding. John-
son and Goldwater (2009) experimented with two
</bodyText>
<equation confidence="0.9946612">
1
c(A, s) =
n
n
i=1
</equation>
<page confidence="0.997466">
569
</page>
<table confidence="0.993614909090909">
this paper J&amp;G 2009
grammar model Vit. MBR SA MM
Dir 0.49 0.84 0.57 0.54
PY 0.49 0.84 0.81 0.75
PY+inc 0.42 0.59 - -
Dir 0.40 0.86 0.75 0.72
PY 0.40 0.86 0.83 0.86
PY+inc 0.43 0.60 - -
Dir 0.77 0.83 0.84 0.84
PY 0.77 0.83 0.89 0.88
PY+inc 0.75 0.76 - -
</table>
<tableCaption confidence="0.998237">
Table 1: F1 performance for word segmentation on the
</tableCaption>
<bodyText confidence="0.72141">
Brent corpus. Dir. stands for Dirichlet Process adaptor
(b = 0), PY stands for Pitman-Yor adaptor (b optimized),
and PY+inc. stands for Pitman-Yor with iteratively in-
creasing NA for A E M (see footnote 5). J&amp;G 2009 are
the results adapted from Johnson and Goldwater (2009);
SA is sample average decoding, and MM is maximum
marginal decoding.
</bodyText>
<figure confidence="0.9669403">
● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ●
● ●
● ●
● ● ● ●
● ●
● ●
● ●
2000 4000 6000 8000 10000 12000 14000
Truncated stick length
</figure>
<figureCaption confidence="0.9991485">
Figure 3: F1 performance of GUnigr,,. as influenced by
the length of the stick, NWord.
</figureCaption>
<bodyText confidence="0.999972627118644">
decoding methods, sample average (SA) and maxi-
mal marginal decoding (MM), which are closely re-
lated to Viterbi and MBR, respectively. With MM,
we marginalize the tree structure, rather than the
word segmentation induced, similar to MBR decod-
ing. With SA, we compute the probability of a whole
tree, by averaging its count in the samples, an ap-
proximation to finding the tree with highest proba-
bility, like Viterbi.
Table 1 gives the results for our experiments. No-
tice that the results for the Pitman-Yor process and
the Dirichlet process are similar. When inspecting
the learned parameters, we noticed that the discount
parameters (b) learned by the variational inference
algorithm for the Pitman-Yor process are very close
to 0. In this case, the Pitman-Yor process is reduced
to the Dirichlet process.
Similar to Johnson and Goldwater’s comparisons,
we see superior performance when using minimum
Bayes risk over Viterbi decoding. Further notice that
the variational inference algorithm obtains signifi-
cantly superior performance for simpler grammars
than Johnson et al., while performance using the syl-
lable grammar is lower. The results also suggest that
it is better to decide ahead on the set of strings avail-
able in the sticks, instead of working gradually and
increase the size of the sticks as described in foot-
note 5. We believe that the reason is that the varia-
tional inference algorithm settles in a trajectory that
uses fewer strings, then fails to exploit the strings
that are added to the stick later. Given that select-
ing NA in advance is advantageous, we may inquire
if choosing NA to be too large can lead to degraded
performance, because of fragmention of the gram-
mar. Fig. 3 suggests it is not the case, and per-
formance stays steady after NA reaches a certain
value.
One of the advantages of variational approxima-
tion over sampling methods is the ability to run
for fewer iterations. For example, with GUnigram
convergence typically takes 40 iterations with vari-
ational inference, while Johnson and Goldwater
(2009) ran their sampler for 2,000 iterations, for
which 1,000 were for burning in. The inside-outside
algorithm dominates the iteration’s runtime, both
for sampling and variational EM. Each iteration
with sampling, however, takes less time, despite the
asymptotic analysis in §3.2, because of different im-
plementations and the different number of rules that
rewrite to a string. We now give a comparison of
clock time for GUnigram for variational inference
and sampling as described in Johnson and Goldwa-
ter (2009).6 Replicating the experiment in Johnson
and Goldwater (first row in Table 1) took 2 hours
and 11 minutes. With the variational approximation,
we had the following: (i) the preprocessing (§3.3)
step took 114 seconds; (ii) each iteration took ap-
proximately 204 seconds, with convergence after 40
iterations, leading to 8,160 seconds of pure varia-
</bodyText>
<footnote confidence="0.999515">
6We used the code and data available at http://www.
cog.brown.edu/˜mj/Software.htm. The machine
used for this comparison is a 64-bit machine with 2.6GHz CPU,
4MB of cache memory and 8GB of RAM.
</footnote>
<figure confidence="0.57545625">
F1 score 80
75
70
65
</figure>
<page confidence="0.991277">
570
</page>
<bodyText confidence="0.999837866666667">
tional EM processing; (iii) parsing took another 952
seconds. The total time is 2 hours and 34 minutes.
At first glance it seems that variational inference
is slower than MCMC sampling. However, note that
the cost of the grammar preprocessing step is amor-
tized over all experiments with the specific gram-
mar, and the E-step with variational inference can be
parallelized, while sampling requires an update of a
global set of parameters after each tree update. We
ran our algorithm on a cluster of 20 1.86GHz CPUs
and achieved a significant speed-up: preprocessing
took 34 seconds, each variational EM iteration took
43 seconds and parsing took 208 seconds. The total
time was 47 minutes, which is 2.8 times faster than
sampling.
</bodyText>
<subsectionHeader confidence="0.974293">
4.2 Dependency Grammar Induction
</subsectionHeader>
<bodyText confidence="0.999971677419355">
We conclude our experiments with preliminary re-
sults for unsupervised syntax learning. This is a new
application of adaptor grammars, which have so far
been used in segmentation (Johnson and Goldwater,
2009) and named entity recognition (Elsner et al.,
2009).
The grammar we use is the dependency model
with valence (DMV Klein and Manning, 2004) rep-
resented as a probabilistic context-free grammar,
GDMV (Smith, 2006). We note that GDMV is re-
cursive; this is not a problem (§3.1).
We used part-of-speech sequences from the Wall
Street Journal Penn Treebank (Marcus et al., 1993),
stripped of words and punctuation. We follow stan-
dard parsing conventions and train on sections 2–
21 and test on section 23 (while using sentences of
length 10 or less). Because of the unsupervised na-
ture of the problem, we report results on the training
set, in addition to the test set.
The nonterminals that we adapted correspond to
nonterminals that define noun constituents. We then
use the preprocessing step defined in §3.3 with a uni-
form grammar and take the top 3,000 strings for each
nonterminal of a noun constituent.
The results are in Table 4.2. We report attach-
ment accuracy, the fraction of parent-child relation-
ships that the algorithm classified correctly. Notice
that the results are not very different for Viterbi and
MBR decoding, unlike the case with word segmen-
tation. It seems like the DMV grammar, applied
to this task, is more robust to changes in decod-
</bodyText>
<table confidence="0.997644428571429">
model Vit. MBR
non-Bayesian 48.2 48.3
Dirichlet prior 48.3 48.6
Adaptor grammar 54.0 153.7
non-Bayesian 45.8 46.1
Dirichlet prior 45.9 46.1
Adaptor grammar 48.3 50.2
</table>
<tableCaption confidence="0.991227">
Table 2: Attachment accuracy for different models for
dependency grammar induction. Bold marks best overall
accuracy per evaluation set, and † marks figures that are
not significantly worse (binomial sign test, p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.995089">
ing mechanism. Adaptor grammars improve perfor-
mance over classic EM and variational EM with a
Dirichlet prior significantly.
We note that adaptor grammars are not limited to
a selection of a Dirichlet distribution as a prior for
the grammar rules. Our variational inference algo-
rithm, for example, can be extended to use the lo-
gistic normal prior instead of the Dirichlet, shown
successful by Cohen and Smith (2009).7
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999961846153846">
We described a variational inference algorithm for
adaptor grammars based on a stick-breaking process
representation, which solves a problem with adaptor
grammars and recursive PCFGs. We tested it for a
segmentation task, and showed results which are ei-
ther comparable or an imporvement of state of the
art. We showed that significant speed-ups can be
obtained using parallelization of the algorithm. We
also tested the algorithm on a novel task for adap-
tor grammars, dependency grammar induction. We
showed that an improvement can be obtained using
adaptor grammars over non-Bayesian and paramet-
ric baselines.
</bodyText>
<sectionHeader confidence="0.999081" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.6167313">
The authors would like to thank the anonymous review-
ers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson,
and Chong Wang for their useful feedback and com-
ments. This work was supported by the following grants:
ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF
IIS-0836431 and IIS-0915187 to Smith.
7The performance of Cohen and Smith (2009), like the per-
formance of Headden et al. (2009), is greater than what we re-
port, but those developments are orthogonal to the contributions
of this paper.
</reference>
<figure confidence="0.800567">
train
test
</figure>
<page confidence="0.987091">
571
</page>
<sectionHeader confidence="0.99285" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849434343435">
C. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152–1174.
N. Bernstein-Ratner. 1987. The phonology of parent
child speech. Children’s Language, 6.
D. Blei and M. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Journal of Bayesian Anal-
ysis, 1(1):121–144.
S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
tion. Cambridge Press.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 6:93–125.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Proc. of
OSDI.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
M. Johnson and S. Goldwater. 2009. Improving nonpa-
rameteric Bayesian inference experiments on unsuper-
vised word segmentation with adaptor grammars. In
Proc. of NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2008a. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proceedings
of the Tenth Meeting ofACL Special Interest Group on
Computational Morphology and Phonology.
M. Johnson. 2008b. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183–
233.
A. Joshi. 2003. Tree adjoining grammars. In R. Mitkov,
editor, The Oxford Handbook of Computational Lin-
guistics, pages 483–501. Oxford University Press.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara, M. Welling, and N. A. Vlassis. 2006. Ac-
celerated variational Dirichlet process mixtures. In
NIPS.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313–330.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2009. Distributed algorithms for topic models. Jour-
nal of Machine Learning Research, 10:1801–1828.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25(2):855–900.
J. Pitman. 2002. Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School. Springer-
Verlag, New York, NY.
C. P. Robert and G. Casella. 2005. Monte Carlo Statisti-
cal Methods. Springer.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639–650.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. of NIPS.
M. J. Wainwright and M. I. Jordan. 2008. Graphi-
cal models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1:1–305.
C. Wang and D. M. Blei. 2009. Variational inference for
the nested Chinese restaurant process. In NIPS.
</reference>
<page confidence="0.997366">
572
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553439">
<title confidence="0.999178">Variational Inference for Adaptor Grammars</title>
<author confidence="0.999998">Shay B Cohen</author>
<affiliation confidence="0.999939">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997774">Pittsburgh, PA 15213, USA</address>
<email confidence="0.99966">scohen@cs.cmu.edu</email>
<author confidence="0.999108">M David</author>
<affiliation confidence="0.8314135">Computer Science Princeton</affiliation>
<address confidence="0.997721">Princeton, NJ 08540,</address>
<email confidence="0.999875">blei@cs.princeton.edu</email>
<author confidence="0.99994">Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997887">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999655">nasmith@cs.cmu.edu</email>
<abstract confidence="0.992267333333333">Adaptor grammars extend probabilistic context-free grammars to define prior distributions over trees with “rich get richer” dynamics. Inference for adaptor grammars seeks to find parse trees for raw text. This paper describes a variational inference algorithm for adaptor grammars, providing an alternative to Markov chain Monte Carlo methods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>The authors would like to thank the anonymous reviewers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson, and Chong Wang for their useful feedback and comments. This work was supported by the following grants:</title>
<booktitle>ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF IIS-0836431 and IIS-0915187 to Smith.</booktitle>
<marker></marker>
<rawString>The authors would like to thank the anonymous reviewers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson, and Chong Wang for their useful feedback and comments. This work was supported by the following grants: ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF IIS-0836431 and IIS-0915187 to Smith.</rawString>
</citation>
<citation valid="true">
<title>7The performance of Cohen and Smith</title>
<date>2009</date>
<contexts>
<context position="20798" citStr="(2009)" startWordPosition="3652" endWordPosition="3652">ge, because it does not use a fixed set of strings; instead, the size of the grammars it uses grow as sampling proceeds). In §4, we see that variational EM and sampling methods are similar in the time it takes to complete because of a trade-off between these two constants. Simple parallelization, however, which is possible only with variational inference, provides significant speed-ups.4 3.3 Heuristics for Variational Inference For the variational approximation from §3, we need to decide on a set of strings, sA,i (for A ∈ M and i ∈ {1, ... , NA}) to define the grammatons in the 4Newman et al. (2009) show how to parallelize sampling algorithms, but in general, parallelizing these algorithms is more complicated than parallelizing variational algorithms and requires further approximation. 568 �E �E �� maxαA log P(|RA|αA) − |RA |log P(αA) + (αA − 1) A→-∈RA �(TA→-) − � A→-∈RA TA→- maxaA 1 NA aA ((l//yA,i) −�(//yA,i + yA,i)) + log P(aA + 1 + ibA) − log P(ibA + aA) maxbA ENA ibA (&apos;Fly2A,i) − &apos;FlyA,i + yA,i)) + log P(aA + 1 + ibA) − log P(1 − bA) − log P(ibA + aA) Figure 2: Variational M-step updates. P is the gamma function. nonparametric stick. Any set of strings will give a valid approximatio</context>
<context position="22487" citStr="(2009)" startWordPosition="3959" endWordPosition="3959"> over the strings generated by the adaptor grammars that yields an accurate variational estimation. We begin with a weighted context-free grammar Gheur that has the same rules as in G, only the weight for all of its rules is 1. We then compute the quantity: I &apos;F-Gheur[fi(z; A, s)] − p log |s| (3) where fi(z; A, s) is a function computing the count of constituents headed by A with yield s in the tree z for the sentence xi. This quantity can be computed by using the IO algorithm on Gheur. The term p log |s |is subtracted to avoid preference for shorter constituents, similar to Mochihashi et al. (2009). While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s). Then, we use the top NA strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and </context>
<context position="23793" citStr="(2009)" startWordPosition="4175" endWordPosition="4175">completeness, we give these results in §4. and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem. We use the standard Brent corpus (Brent and Cartwright, 1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task. The first grammar is a character unigram grammar (GUnigram). The second grammar is a grammar that takes into consideration collocations (GColloc) which includes the rules { Sentence —* Colloc, Senten</context>
<context position="25595" citStr="(2009)" startWordPosition="4481" endWordPosition="4481">two 1 c(A, s) = n n i=1 569 this paper J&amp;G 2009 grammar model Vit. MBR SA MM Dir 0.49 0.84 0.57 0.54 PY 0.49 0.84 0.81 0.75 PY+inc 0.42 0.59 - - Dir 0.40 0.86 0.75 0.72 PY 0.40 0.86 0.83 0.86 PY+inc 0.43 0.60 - - Dir 0.77 0.83 0.84 0.84 PY 0.77 0.83 0.89 0.88 PY+inc 0.75 0.76 - - Table 1: F1 performance for word segmentation on the Brent corpus. Dir. stands for Dirichlet Process adaptor (b = 0), PY stands for Pitman-Yor adaptor (b optimized), and PY+inc. stands for Pitman-Yor with iteratively increasing NA for A E M (see footnote 5). J&amp;G 2009 are the results adapted from Johnson and Goldwater (2009); SA is sample average decoding, and MM is maximum marginal decoding. ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 2000 4000 6000 8000 10000 12000 14000 Truncated stick length Figure 3: F1 performance of GUnigr,,. as influenced by the length of the stick, NWord. decoding methods, sample average (SA) and maximal marginal decoding (MM), which are closely related to Viterbi and MBR, respectively. With MM, we marginalize the tree structure, rather than the word segmentation induced, similar to MBR decoding. With SA, we compute the probability of a whole tree, by averaging its count </context>
<context position="27909" citStr="(2009)" startWordPosition="4882" endWordPosition="4882">ses fewer strings, then fails to exploit the strings that are added to the stick later. Given that selecting NA in advance is advantageous, we may inquire if choosing NA to be too large can lead to degraded performance, because of fragmention of the grammar. Fig. 3 suggests it is not the case, and performance stays steady after NA reaches a certain value. One of the advantages of variational approximation over sampling methods is the ability to run for fewer iterations. For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in. The inside-outside algorithm dominates the iteration’s runtime, both for sampling and variational EM. Each iteration with sampling, however, takes less time, despite the asymptotic analysis in §3.2, because of different implementations and the different number of rules that rewrite to a string. We now give a comparison of clock time for GUnigram for variational inference and sampling as described in Johnson and Goldwater (2009).6 Replicating the experiment in Johnson and Goldwater (first row in Table 1) took 2 hours </context>
<context position="32006" citStr="(2009)" startWordPosition="5548" endWordPosition="5548">r different models for dependency grammar induction. Bold marks best overall accuracy per evaluation set, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). ing mechanism. Adaptor grammars improve performance over classic EM and variational EM with a Dirichlet prior significantly. We note that adaptor grammars are not limited to a selection of a Dirichlet distribution as a prior for the grammar rules. Our variational inference algorithm, for example, can be extended to use the logistic normal prior instead of the Dirichlet, shown successful by Cohen and Smith (2009).7 5 Conclusion We described a variational inference algorithm for adaptor grammars based on a stick-breaking process representation, which solves a problem with adaptor grammars and recursive PCFGs. We tested it for a segmentation task, and showed results which are either comparable or an imporvement of state of the art. We showed that significant speed-ups can be obtained using parallelization of the algorithm. We also tested the algorithm on a novel task for adaptor grammars, dependency grammar induction. We showed that an improvement can be obtained using adaptor grammars over non-Bayesian</context>
</contexts>
<marker>2009</marker>
<rawString>7The performance of Cohen and Smith (2009), like the performance of Headden et al. (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Antoniak</author>
</authors>
<title>Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="1552" citStr="Antoniak, 1974" startWordPosition="214" endWordPosition="215">mentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adapt</context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parent child speech.</title>
<date>1987</date>
<journal>Children’s Language,</journal>
<volume>6</volume>
<contexts>
<context position="24108" citStr="Bernstein-Ratner (1987)" startWordPosition="4217" endWordPosition="4218">e again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem. We use the standard Brent corpus (Brent and Cartwright, 1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task. The first grammar is a character unigram grammar (GUnigram). The second grammar is a grammar that takes into consideration collocations (GColloc) which includes the rules { Sentence —* Colloc, Sentence —* Colloc Sentence, Colloc —* Word+, Word —* Char+ }. The third grammar incorporates more prior knowledge about the syllabic structure of English (GSyllable). GUnigram and GSyllable can be found in Johnson and Goldwater (2009). Once an utterance is parsed, Word constituents denote segments. The value of p (pena</context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>N. Bernstein-Ratner. 1987. The phonology of parent child speech. Children’s Language, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>M Jordan</author>
</authors>
<title>Variational inference for Dirichlet process mixtures.</title>
<date>2005</date>
<journal>Journal of Bayesian Analysis,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2685" citStr="Blei and Jordan (2005)" startWordPosition="388" endWordPosition="391">trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees. Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy. Variational inference provides a deterministic alternative to sampling. It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al. (2007). With NP Bayes models, variational methods are based on the stick-breaking representation (Sethuraman, 1994). Devising a stick-breaking representation is a central challenge to using variational inference in this setting. The rest of this paper is organized as follows. In §2 we describe a stick-breaking representation of adaptor grammars, which enables variational inference (§3) and a well-defined incorporation of recursion into adaptor grammars. In §4 we give an empirical comparison of the algorithm to MCMC inference and describe a nove</context>
<context position="13266" citStr="Blei and Jordan, 2005" startWordPosition="2282" endWordPosition="2285">v, θ)] + Eq[logp(z |vA)] AEM 566 Expectations are taken with respect to the variational distribution q(v, θ, z) and H(q) is its entropy. Before tightening the bound, we define the functional form of the variational distribution. We use the mean-field distribution in which all of the hidden variables are independent and governed by individual variational parameters. (Note that in the true posterior, the hidden variables are highly coupled.) To account for the infinite collection of random variables, for which we cannot define a variational distribution, we use the truncated stick distribution (Blei and Jordan, 2005). Hence, we assume that, for all A ∈ M, there is some value NA such that q(vA,NA = 1) = 1. The assigned probability to parse trees in the stick will be 0 for i &gt; NA, so we can ignore zA,i for i &gt; NA. This leads to a factorized variational distribution: q(v, θ, z) = (2) ! Y q(vA,i) × q(zA,i) × q(zi) i=1 the grammaton G(A, sA,i) and for q(zi observed sentence. We parametrize the grammaton ized weighted grammars. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al., 2008, and Cohen and Smith, 2009). In practice we do not have to us</context>
</contexts>
<marker>Blei, Jordan, 2005</marker>
<rawString>D. Blei and M. Jordan. 2005. Variational inference for Dirichlet process mixtures. Journal of Bayesian Analysis, 1(1):121–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge Press.</publisher>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>S. Boyd and L. Vandenberghe. 2004. Convex Optimization. Cambridge Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
<author>T Cartwright</author>
</authors>
<title>Distributional regularity and phonotactic constraints are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>6--93</pages>
<contexts>
<context position="23977" citStr="Brent and Cartwright, 1996" startWordPosition="4199" endWordPosition="4202"> with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem. We use the standard Brent corpus (Brent and Cartwright, 1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task. The first grammar is a character unigram grammar (GUnigram). The second grammar is a grammar that takes into consideration collocations (GColloc) which includes the rules { Sentence —* Colloc, Sentence —* Colloc Sentence, Colloc —* Word+, Word —* Char+ }. The third grammar incorporates more prior knowledge about the syllabic structure of English (GSyllable). GUnigram and GSyllable</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>M. Brent and T. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition, 6:93–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="13832" citStr="Cohen and Smith, 2009" startWordPosition="2392" endWordPosition="2395">the truncated stick distribution (Blei and Jordan, 2005). Hence, we assume that, for all A ∈ M, there is some value NA such that q(vA,NA = 1) = 1. The assigned probability to parse trees in the stick will be 0 for i &gt; NA, so we can ignore zA,i for i &gt; NA. This leads to a factorized variational distribution: q(v, θ, z) = (2) ! Y q(vA,i) × q(zA,i) × q(zi) i=1 the grammaton G(A, sA,i) and for q(zi observed sentence. We parametrize the grammaton ized weighted grammars. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al., 2008, and Cohen and Smith, 2009). In practice we do not have to use rewrite rules for all strings t s in the grammaton. It suffices to add rewrite rules only for the strings t = sA,i that have some grammaton attached to them, G(A, The variational distribution above yields a variational inference algorithm for approximating the |φA) |φ). §3.3) ∈ sA hsA,1, sA,NAi. |φA) |φ) φA φ) ⊆ sA,i). γA,i,τA,φA φ d α. Let r be a PCFG rule. Let � 567 Using a grammaton, we define the distributions q(zA,i and q(zi This requires a preprocessing step (described in detail in that defines, for each A M, a list of strings = ... , Then, for q(zA,i </context>
<context position="32006" citStr="Cohen and Smith (2009)" startWordPosition="5545" endWordPosition="5548">ment accuracy for different models for dependency grammar induction. Bold marks best overall accuracy per evaluation set, and † marks figures that are not significantly worse (binomial sign test, p &lt; 0.05). ing mechanism. Adaptor grammars improve performance over classic EM and variational EM with a Dirichlet prior significantly. We note that adaptor grammars are not limited to a selection of a Dirichlet distribution as a prior for the grammar rules. Our variational inference algorithm, for example, can be extended to use the logistic normal prior instead of the Dirichlet, shown successful by Cohen and Smith (2009).7 5 Conclusion We described a variational inference algorithm for adaptor grammars based on a stick-breaking process representation, which solves a problem with adaptor grammars and recursive PCFGs. We tested it for a segmentation task, and showed results which are either comparable or an imporvement of state of the art. We showed that significant speed-ups can be obtained using parallelization of the algorithm. We also tested the algorithm on a novel task for adaptor grammars, dependency grammar induction. We showed that an improvement can be obtained using adaptor grammars over non-Bayesian</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="13804" citStr="Cohen et al., 2008" startWordPosition="2387" endWordPosition="2390">al distribution, we use the truncated stick distribution (Blei and Jordan, 2005). Hence, we assume that, for all A ∈ M, there is some value NA such that q(vA,NA = 1) = 1. The assigned probability to parse trees in the stick will be 0 for i &gt; NA, so we can ignore zA,i for i &gt; NA. This leads to a factorized variational distribution: q(v, θ, z) = (2) ! Y q(vA,i) × q(zA,i) × q(zi) i=1 the grammaton G(A, sA,i) and for q(zi observed sentence. We parametrize the grammaton ized weighted grammars. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al., 2008, and Cohen and Smith, 2009). In practice we do not have to use rewrite rules for all strings t s in the grammaton. It suffices to add rewrite rules only for the strings t = sA,i that have some grammaton attached to them, G(A, The variational distribution above yields a variational inference algorithm for approximating the |φA) |φ). §3.3) ∈ sA hsA,1, sA,NAi. |φA) |φ) φA φ) ⊆ sA,i). γA,i,τA,φA φ d α. Let r be a PCFG rule. Let � 567 Using a grammaton, we define the distributions q(zA,i and q(zi This requires a preprocessing step (described in detail in that defines, for each A M, a list of strin</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Proc. of OSDI.</booktitle>
<contexts>
<context position="12496" citStr="Dean and Ghemawat, 2004" startWordPosition="2147" endWordPosition="2151">l Inference Variational inference is a deterministic alternative to MCMC, which casts posterior inference as an optimization problem (Jordan et al., 1999; Wainwright and Jordan, 2008). The optimized function is a bound on the marginal likelihood of the observations, which is expressed in terms of a so-called “variational distribution” over the hidden variables. When the bound is tightened, that distribution is close to the posterior of interest. Variational methods tend to converge faster than MCMC, and can be more easily parallelized over multiple processors in a framework such as MapReduce (Dean and Ghemawat, 2004). The variational bound on the likelihood of the data is: log p(x |a, α) ≥ H(q) + � Eq[log p(vA |aA)] AEM + � Eq[log p(θA |αA)] AEM + � Eq[logp(zA |v, θ)] + Eq[logp(z |vA)] AEM 566 Expectations are taken with respect to the variational distribution q(v, θ, z) and H(q) is its entropy. Before tightening the bound, we define the functional form of the variational distribution. We use the mean-field distribution in which all of the hidden variables are independent and governed by individual variational parameters. (Note that in the true posterior, the hidden variables are highly coupled.) To accou</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>J. Dean and S. Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Proc. of OSDI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Structured generative models for unsupervised namedentity clustering.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="30002" citStr="Elsner et al., 2009" startWordPosition="5215" endWordPosition="5218">pdate of a global set of parameters after each tree update. We ran our algorithm on a cluster of 20 1.86GHz CPUs and achieved a significant speed-up: preprocessing took 34 seconds, each variational EM iteration took 43 seconds and parsing took 208 seconds. The total time was 47 minutes, which is 2.8 times faster than sampling. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to th</context>
</contexts>
<marker>Elsner, Charniak, Johnson, 2009</marker>
<rawString>M. Elsner, E. Charniak, and M. Johnson. 2009. Structured generative models for unsupervised namedentity clustering. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1352" citStr="Goldwater and Griffiths, 2007" startWordPosition="181" endWordPosition="184">ethods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="23282" citStr="Goodman, 1996" startWordPosition="4092" endWordPosition="4094">strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and Blei (2009), but we did not achieve better performance and it had an adverse effect on runtime. For completeness, we give these results in §4. and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sa</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACLHLT.</booktitle>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="1753" citStr="Johnson and Goldwater, 2009" startWordPosition="245" endWordPosition="248">lts for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously s</context>
<context position="23793" citStr="Johnson and Goldwater (2009)" startWordPosition="4172" endWordPosition="4175">ffect on runtime. For completeness, we give these results in §4. and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem. We use the standard Brent corpus (Brent and Cartwright, 1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task. The first grammar is a character unigram grammar (GUnigram). The second grammar is a grammar that takes into consideration collocations (GColloc) which includes the rules { Sentence —* Colloc, Senten</context>
<context position="25595" citStr="Johnson and Goldwater (2009)" startWordPosition="4478" endWordPosition="4481">09) experimented with two 1 c(A, s) = n n i=1 569 this paper J&amp;G 2009 grammar model Vit. MBR SA MM Dir 0.49 0.84 0.57 0.54 PY 0.49 0.84 0.81 0.75 PY+inc 0.42 0.59 - - Dir 0.40 0.86 0.75 0.72 PY 0.40 0.86 0.83 0.86 PY+inc 0.43 0.60 - - Dir 0.77 0.83 0.84 0.84 PY 0.77 0.83 0.89 0.88 PY+inc 0.75 0.76 - - Table 1: F1 performance for word segmentation on the Brent corpus. Dir. stands for Dirichlet Process adaptor (b = 0), PY stands for Pitman-Yor adaptor (b optimized), and PY+inc. stands for Pitman-Yor with iteratively increasing NA for A E M (see footnote 5). J&amp;G 2009 are the results adapted from Johnson and Goldwater (2009); SA is sample average decoding, and MM is maximum marginal decoding. ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 2000 4000 6000 8000 10000 12000 14000 Truncated stick length Figure 3: F1 performance of GUnigr,,. as influenced by the length of the stick, NWord. decoding methods, sample average (SA) and maximal marginal decoding (MM), which are closely related to Viterbi and MBR, respectively. With MM, we marginalize the tree structure, rather than the word segmentation induced, similar to MBR decoding. With SA, we compute the probability of a whole tree, by averaging its count </context>
<context position="27909" citStr="Johnson and Goldwater (2009)" startWordPosition="4879" endWordPosition="4882">in a trajectory that uses fewer strings, then fails to exploit the strings that are added to the stick later. Given that selecting NA in advance is advantageous, we may inquire if choosing NA to be too large can lead to degraded performance, because of fragmention of the grammar. Fig. 3 suggests it is not the case, and performance stays steady after NA reaches a certain value. One of the advantages of variational approximation over sampling methods is the ability to run for fewer iterations. For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in. The inside-outside algorithm dominates the iteration’s runtime, both for sampling and variational EM. Each iteration with sampling, however, takes less time, despite the asymptotic analysis in §3.2, because of different implementations and the different number of rules that rewrite to a string. We now give a comparison of clock time for GUnigram for variational inference and sampling as described in Johnson and Goldwater (2009).6 Replicating the experiment in Johnson and Goldwater (first row in Table 1) took 2 hours </context>
<context position="29951" citStr="Johnson and Goldwater, 2009" startWordPosition="5207" endWordPosition="5210">inference can be parallelized, while sampling requires an update of a global set of parameters after each tree update. We ran our algorithm on a cluster of 20 1.86GHz CPUs and achieved a significant speed-up: preprocessing took 34 seconds, each variational EM iteration took 43 seconds and parsing took 208 seconds. The total time was 47 minutes, which is 2.8 times faster than sampling. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we re</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>M. Johnson and S. Goldwater. 2009. Improving nonparameteric Bayesian inference experiments on unsupervised word segmentation with adaptor grammars. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models.</title>
<date>2006</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1691" citStr="Johnson et al., 2006" startWordPosition="237" endWordPosition="240">izing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the tru</context>
<context position="3820" citStr="Johnson et al. (2006)" startWordPosition="562" endWordPosition="565">we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised dependency parsing. 2 Adaptor Grammars We review adaptor grammars and develop a stickbreaking representation of the tree distribution. 2.1 Definition of Adaptor Grammars Adaptor grammars capture syntactic regularities in sentences by placing a nonparametric prior over the distribution of syntactic trees that underlie them. The model exhibits “rich get richer” dynamics: once a tree is generated, it is more likely to reappear. Adaptor grammars were developed by Johnson et al. (2006). An adaptor grammar is a tuple A = (G, M, a, b, α), which contains: (i) a context-free grammar G = (W, N, R, 5) where W is the set of 564 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 564–572, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics terminals, N is the set of nonterminals, R is a set of production rules, and S E N is the start symbol—we denote by RA the subset of R with left-hand side A; (ii) a set of adapted nonterminals, M C N; and (iii) parameters a, b and α, which are described below. An</context>
<context position="6068" citStr="Johnson et al., 2006" startWordPosition="978" endWordPosition="981">nonterminals, this process can be explained using the Chinese restaurant process (CRP) metaphor: a “customer” (corresponding to a partially generated tree) enters a “restaurant” (corresponding to a nonterminal) and selects a “table” (corresponding to a subtree) to attach to the partially generated tree. If she is the first customer at the table, the PCFG (G, 0) produces the new table’s associated “dish” (a subtree).1 When adaptor grammars are defined using the CRP, the PCFG G has to be non-recursive with re1We note that our construction deviates from the strict definition of adaptor grammars (Johnson et al., 2006): (i) in our construction, we assume (as prior work does in practice) that the adaptors in A = (G, M, a, b, α) follow the Pitman-Yor (PY) process (Pitman and Yor, 1997), though in general other stochastic processes might be used; and (ii) we place a symmetric Dirichlet over the parameters of the PCFG, 9, whereas Johnson et al. used a fixed PCFG for the definition (though they experimented with a Dirichlet prior). spect to the adapted nonterminals. More precisely, for A E N, denote by Reachable(G, A) all the nonterminals that can be reached from A using a partial derivation from G. Then we rest</context>
<context position="7408" citStr="Johnson et al. (2006)" startWordPosition="1215" endWordPosition="1218">where the generative process is ill-defined: in the CRP terminology, a customer could enter a restaurant and select a table whose dish is still in the process of being selected.2 In the more general form of adaptor grammars with arbitrary adaptors, the problem amounts to mutually dependent definitions of distributions which rely on the others to be defined. We return to this problem in §3.1. Inference The inference problem is to compute the posterior distribution of parse trees given observed sentences x = (x1, ... , x.). Typically, inference with adaptor grammars is done with Gibbs sampling. Johnson et al. (2006) use an embedded Metropolis-Hastings sampler (Robert and Casella, 2005) inside a Gibbs sampler. The proposal distribution is a PCFG, resembling a tree substitution grammar (TSG; Joshi, 2003). The sampler of Johnson et al. is based on the representation of the PY process as a distribution over partitions of integers. This representation is not amenable to variational inference. 2.2 Stick-Breaking Representation To develop a variational inference algorithm for adaptor grammars, we require an alternative representation of the model in §2.1. The CRP-based definition implicitly marginalizes out a r</context>
<context position="15792" citStr="Johnson et al. (2006)" startWordPosition="2748" endWordPosition="2751">2 gives the objectives. 3.1 Note about Recursive Grammars YNA i=1 q(θA) Y AEM n RB!∪ [ [ {Bi → A→B1...B�ERAiE{i BiEM} t [ RA∪BEU |t ⊆ s}. It is natural to define the variational distributions over and v to be Dirichlet distributions with parameters and Beta distributions with parameters respectively. The two distributions over trees, q(zA,i) and q(zi), are more problematic. For example, with q(zi we need to take into account different subtrees that could be generated by the model and use them with the proper probabilities in the variational distribution q(zi We follow and extend the idea from Johnson et al. (2006) and use grammatons for these distributions. Grammatons are inspired by the grammar G. For two strings in s, t we use to mean that t is a substring of s. In that case, a grammaton is defined as follows: Definition 1. Let A = M, a, b, be an adaptor grammar with G = N, R, Let s be a finite string over the alphabet of G and A N. be the set of nonterminals &apos;Ll °= Reachable(G, A) (N \ θ τA γA,i, |φ), |φ). “mini-grammars,” ∈W*, “t⊆s” hG, αi hW, 5i. ∈ Let &apos;LL ∩ ). The grammaton G(A, s) is the contextfree grammar with the start symbol A and the rules G. The quan Variational EM We use variational EM to</context>
<context position="18450" citStr="Johnson et al. (2006)" startWordPosition="3232" endWordPosition="3235">stributions. Then: argmax Eq[log p(x, z)] = argmax Eq[log p0(x,z)] q∈Q q where p0(x, z) is a probability distribution defined as p0(x, z) = p(x, z)/ EI∈S p(x, z) for z ∈ S and 0 otherwise. For this reason, our variational approximation allows the use of recursive grammars. The use of recursive grammars with MCMC methods is problematic, since it has no corresponding probabilistic interpretation, enabled by zeroing events that are illdefined in the variational distribution. There is no underlying model such as p0, and thus the inference algorithm is invalid. 3.2 Time Complexity The algorithm in Johnson et al. (2006) works by sampling from a PCFG containing rewrite rules that rewrite to a whole tree fragment. This requires a procedure that uses the inside-outside algorithm. Despite the grammar being bigger (because of the rewrite rules to a string), the asymptotic complexity of the IO algorithm stays O(|N|2|xi|3 + |N|3|xi|2) where |xi |is the length of the ith sentence.3 3This analysis is true for CNF grammars augmented with rules rewriting to a whole string, like those used in our study. y1A,i = 1 − bA + EB∈M Ek &amp;quot;1 f (A → sA,i, sB,k) y2A,i = aA + ibA + Ej=1 EB∈M ENB k =1 f (A → sA,j, sB,k) TA,A→β = EB∈M </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2006</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparameteric Bayesian models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T L Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1404" citStr="Johnson et al., 2007" startWordPosition="189" endWordPosition="192">epresentation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. L. Griffiths, and S. Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Unsupervised word segmentation for Sesotho using adaptor grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the Tenth Meeting ofACL Special Interest Group on Computational Morphology and Phonology.</booktitle>
<contexts>
<context position="1706" citStr="Johnson, 2008" startWordPosition="241" endWordPosition="242">inally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, bu</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>M. Johnson. 2008a. Unsupervised word segmentation for Sesotho using adaptor grammars. In Proceedings of the Tenth Meeting ofACL Special Interest Group on Computational Morphology and Phonology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1706" citStr="Johnson, 2008" startWordPosition="241" endWordPosition="242">inally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, bu</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>M. Johnson. 2008b. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Jordan</author>
<author>Z Ghahramani</author>
<author>T S Jaakola</author>
<author>L K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>2</issue>
<pages>233</pages>
<contexts>
<context position="12025" citStr="Jordan et al., 1999" startWordPosition="2072" endWordPosition="2075">, the adapted nonterminals’ subtrees zA = {zA,i |A ∈ M; i ∈ {1, ...}} and the derivations z1:n = z1, ... , zn. The symbol z refers to the collection of {zA |A ∈ M}, and z1:n refers to the derivations of the data x. Note that the distribution in 2(c) is defined with the GEM distribution, as mentioned earlier. It is a sample from the Pitman-Yor process (or the Dirichlet process), which is later used in 3(a) to sample trees for an adapted non-terminal. 3 Variational Inference Variational inference is a deterministic alternative to MCMC, which casts posterior inference as an optimization problem (Jordan et al., 1999; Wainwright and Jordan, 2008). The optimized function is a bound on the marginal likelihood of the observations, which is expressed in terms of a so-called “variational distribution” over the hidden variables. When the bound is tightened, that distribution is close to the posterior of interest. Variational methods tend to converge faster than MCMC, and can be more easily parallelized over multiple processors in a framework such as MapReduce (Dean and Ghemawat, 2004). The variational bound on the likelihood of the data is: log p(x |a, α) ≥ H(q) + � Eq[log p(vA |aA)] AEM + � Eq[log p(θA |αA)] A</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakola, Saul, 1999</marker>
<rawString>M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K. Saul. 1999. An introduction to variational methods for graphical models. Machine Learning, 37(2):183– 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>Tree adjoining grammars.</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics,</booktitle>
<pages>483--501</pages>
<editor>In R. Mitkov, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7598" citStr="Joshi, 2003" startWordPosition="1246" endWordPosition="1247">form of adaptor grammars with arbitrary adaptors, the problem amounts to mutually dependent definitions of distributions which rely on the others to be defined. We return to this problem in §3.1. Inference The inference problem is to compute the posterior distribution of parse trees given observed sentences x = (x1, ... , x.). Typically, inference with adaptor grammars is done with Gibbs sampling. Johnson et al. (2006) use an embedded Metropolis-Hastings sampler (Robert and Casella, 2005) inside a Gibbs sampler. The proposal distribution is a PCFG, resembling a tree substitution grammar (TSG; Joshi, 2003). The sampler of Johnson et al. is based on the representation of the PY process as a distribution over partitions of integers. This representation is not amenable to variational inference. 2.2 Stick-Breaking Representation To develop a variational inference algorithm for adaptor grammars, we require an alternative representation of the model in §2.1. The CRP-based definition implicitly marginalizes out a random distribution over trees. For variational inference, we construct that distribution. We first review the Dirichlet process and its stickbreaking representation. The Dirichlet process de</context>
</contexts>
<marker>Joshi, 2003</marker>
<rawString>A. Joshi. 2003. Tree adjoining grammars. In R. Mitkov, editor, The Oxford Handbook of Computational Linguistics, pages 483–501. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="30089" citStr="Klein and Manning, 2004" startWordPosition="5230" endWordPosition="5233"> a cluster of 20 1.86GHz CPUs and achieved a significant speed-up: preprocessing took 34 seconds, each variational EM iteration took 43 seconds and parsing took 208 seconds. The total time was 47 minutes, which is 2.8 times faster than sampling. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define nou</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kurihara</author>
<author>M Welling</author>
<author>N A Vlassis</author>
</authors>
<title>Accelerated variational Dirichlet process mixtures.</title>
<date>2006</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="23073" citStr="Kurihara et al. (2006)" startWordPosition="4054" endWordPosition="4057"> similar to Mochihashi et al. (2009). While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s). Then, we use the top NA strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and Blei (2009), but we did not achieve better performance and it had an adverse effect on runtime. For completeness, we give these results in §4. and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation an</context>
</contexts>
<marker>Kurihara, Welling, Vlassis, 2006</marker>
<rawString>K. Kurihara, M. Welling, and N. A. Vlassis. 2006. Accelerated variational Dirichlet process mixtures. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>S Petrov</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2741" citStr="Liang et al. (2007)" startWordPosition="398" endWordPosition="401">inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees. Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy. Variational inference provides a deterministic alternative to sampling. It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al. (2007). With NP Bayes models, variational methods are based on the stick-breaking representation (Sethuraman, 1994). Devising a stick-breaking representation is a central challenge to using variational inference in this setting. The rest of this paper is organized as follows. In §2 we describe a stick-breaking representation of adaptor grammars, which enables variational inference (§3) and a well-defined incorporation of recursion into adaptor grammars. In §4 we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised depend</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="30322" citStr="Marcus et al., 1993" startWordPosition="5268" endWordPosition="5271">ampling. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define noun constituents. We then use the preprocessing step defined in §3.3 with a uniform grammar and take the top 3,000 strings for each nonterminal of a noun constituent. The results are in Table 4.2. We report attachment accuracy, the fra</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mochihashi</author>
<author>T Yamada</author>
<author>N Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="22487" citStr="Mochihashi et al. (2009)" startWordPosition="3956" endWordPosition="3959"> leads to an order over the strings generated by the adaptor grammars that yields an accurate variational estimation. We begin with a weighted context-free grammar Gheur that has the same rules as in G, only the weight for all of its rules is 1. We then compute the quantity: I &apos;F-Gheur[fi(z; A, s)] − p log |s| (3) where fi(z; A, s) is a function computing the count of constituents headed by A with yield s in the tree z for the sentence xi. This quantity can be computed by using the IO algorithm on Gheur. The term p log |s |is subtracted to avoid preference for shorter constituents, similar to Mochihashi et al. (2009). While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s). Then, we use the top NA strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and </context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>D. Mochihashi, T. Yamada, and N. Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>A Asuncion</author>
<author>P Smyth</author>
<author>M Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--1801</pages>
<contexts>
<context position="20798" citStr="Newman et al. (2009)" startWordPosition="3649" endWordPosition="3652">lative advantage, because it does not use a fixed set of strings; instead, the size of the grammars it uses grow as sampling proceeds). In §4, we see that variational EM and sampling methods are similar in the time it takes to complete because of a trade-off between these two constants. Simple parallelization, however, which is possible only with variational inference, provides significant speed-ups.4 3.3 Heuristics for Variational Inference For the variational approximation from §3, we need to decide on a set of strings, sA,i (for A ∈ M and i ∈ {1, ... , NA}) to define the grammatons in the 4Newman et al. (2009) show how to parallelize sampling algorithms, but in general, parallelizing these algorithms is more complicated than parallelizing variational algorithms and requires further approximation. 568 �E �E �� maxαA log P(|RA|αA) − |RA |log P(αA) + (αA − 1) A→-∈RA �(TA→-) − � A→-∈RA TA→- maxaA 1 NA aA ((l//yA,i) −�(//yA,i + yA,i)) + log P(aA + 1 + ibA) − log P(ibA + aA) maxbA ENA ibA (&apos;Fly2A,i) − &apos;FlyA,i + yA,i)) + log P(aA + 1 + ibA) − log P(1 − bA) − log P(ibA + aA) Figure 2: Variational M-step updates. P is the gamma function. nonparametric stick. Any set of strings will give a valid approximatio</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>D. Newman, A. Asuncion, P. Smyth, and M. Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research, 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Annals of Probability,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="6236" citStr="Pitman and Yor, 1997" startWordPosition="1010" endWordPosition="1013">staurant” (corresponding to a nonterminal) and selects a “table” (corresponding to a subtree) to attach to the partially generated tree. If she is the first customer at the table, the PCFG (G, 0) produces the new table’s associated “dish” (a subtree).1 When adaptor grammars are defined using the CRP, the PCFG G has to be non-recursive with re1We note that our construction deviates from the strict definition of adaptor grammars (Johnson et al., 2006): (i) in our construction, we assume (as prior work does in practice) that the adaptors in A = (G, M, a, b, α) follow the Pitman-Yor (PY) process (Pitman and Yor, 1997), though in general other stochastic processes might be used; and (ii) we place a symmetric Dirichlet over the parameters of the PCFG, 9, whereas Johnson et al. used a fixed PCFG for the definition (though they experimented with a Dirichlet prior). spect to the adapted nonterminals. More precisely, for A E N, denote by Reachable(G, A) all the nonterminals that can be reached from A using a partial derivation from G. Then we restrict G such that for all A E M, we have A E/ Reachable(G, A). Without this restriction, we might end up in a situation where the generative process is ill-defined: in t</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25(2):855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
</authors>
<title>Combinatorial Stochastic Processes. Lecture Notes for St. Flour Summer School.</title>
<date>2002</date>
<publisher>SpringerVerlag,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="1567" citStr="Pitman, 2002" startWordPosition="216" endWordPosition="217">showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are</context>
<context position="9302" citStr="Pitman, 2002" startWordPosition="1532" endWordPosition="1533">panded by S --+ a. For the second child S, it is possible to re-enter the “S restaurant” and choose the first table, where the “dish” subtree is still being generated. 565 and concentration parameter a. The distribution G is discrete, which means it puts positive mass on a countable number of atoms drawn from G0. Repeated draws from G exhibit the “clustering property,” which means that they will be assigned to the same value with positive probability. Thus, they exhibit a partition structure. Marginalizing out G, the distribution of that partition structure is given by a CRP with parameter a (Pitman, 2002). The stick-breaking process gives a constructive definition of G (Sethuraman, 1994). With the stickbreaking process (for the PY process), we first sample “stick lengths” π ∼ GEM(a, b) (in the case of Dirichlet process, we have b = 0). The GEM partitions the interval [0, 1] into countably many segments. First, draw vi ∼ Beta(1 − b, a + ib) for jli�1 i ∈ {1, ...}. Then, define 7ri °= vi j=1(1 − vj). In addition, we also sample infinitely many “atoms” independently zi ∼ G0. Define G as: G(z) = E&apos;1 7riS(zi, z) (1) where S(zi, z) is 1 if zi = z and 0 otherwise. This random variable is drawn from a</context>
</contexts>
<marker>Pitman, 2002</marker>
<rawString>J. Pitman. 2002. Combinatorial Stochastic Processes. Lecture Notes for St. Flour Summer School. SpringerVerlag, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Robert</author>
<author>G Casella</author>
</authors>
<title>Monte Carlo Statistical Methods.</title>
<date>2005</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2225" citStr="Robert and Casella, 2005" startWordPosition="316" endWordPosition="319">yes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees. Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy. Variational inference provides a deterministic alternative to sampling. It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al. (2007). With NP Bayes models, variational methods are based on the stick-breaking represen</context>
<context position="7479" citStr="Robert and Casella, 2005" startWordPosition="1224" endWordPosition="1227">, a customer could enter a restaurant and select a table whose dish is still in the process of being selected.2 In the more general form of adaptor grammars with arbitrary adaptors, the problem amounts to mutually dependent definitions of distributions which rely on the others to be defined. We return to this problem in §3.1. Inference The inference problem is to compute the posterior distribution of parse trees given observed sentences x = (x1, ... , x.). Typically, inference with adaptor grammars is done with Gibbs sampling. Johnson et al. (2006) use an embedded Metropolis-Hastings sampler (Robert and Casella, 2005) inside a Gibbs sampler. The proposal distribution is a PCFG, resembling a tree substitution grammar (TSG; Joshi, 2003). The sampler of Johnson et al. is based on the representation of the PY process as a distribution over partitions of integers. This representation is not amenable to variational inference. 2.2 Stick-Breaking Representation To develop a variational inference algorithm for adaptor grammars, we require an alternative representation of the model in §2.1. The CRP-based definition implicitly marginalizes out a random distribution over trees. For variational inference, we construct </context>
</contexts>
<marker>Robert, Casella, 2005</marker>
<rawString>C. P. Robert and G. Casella. 2005. Monte Carlo Statistical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sethuraman</author>
</authors>
<title>A constructive definition of Dirichlet priors.</title>
<date>1994</date>
<journal>Statistica Sinica,</journal>
<pages>4--639</pages>
<contexts>
<context position="2850" citStr="Sethuraman, 1994" startWordPosition="414" endWordPosition="416">ethods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees. Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy. Variational inference provides a deterministic alternative to sampling. It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al. (2007). With NP Bayes models, variational methods are based on the stick-breaking representation (Sethuraman, 1994). Devising a stick-breaking representation is a central challenge to using variational inference in this setting. The rest of this paper is organized as follows. In §2 we describe a stick-breaking representation of adaptor grammars, which enables variational inference (§3) and a well-defined incorporation of recursion into adaptor grammars. In §4 we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised dependency parsing. 2 Adaptor Grammars We review adaptor grammars and develop a stickbreaking representation of the</context>
<context position="9386" citStr="Sethuraman, 1994" startWordPosition="1543" endWordPosition="1544">taurant” and choose the first table, where the “dish” subtree is still being generated. 565 and concentration parameter a. The distribution G is discrete, which means it puts positive mass on a countable number of atoms drawn from G0. Repeated draws from G exhibit the “clustering property,” which means that they will be assigned to the same value with positive probability. Thus, they exhibit a partition structure. Marginalizing out G, the distribution of that partition structure is given by a CRP with parameter a (Pitman, 2002). The stick-breaking process gives a constructive definition of G (Sethuraman, 1994). With the stickbreaking process (for the PY process), we first sample “stick lengths” π ∼ GEM(a, b) (in the case of Dirichlet process, we have b = 0). The GEM partitions the interval [0, 1] into countably many segments. First, draw vi ∼ Beta(1 − b, a + ib) for jli�1 i ∈ {1, ...}. Then, define 7ri °= vi j=1(1 − vj). In addition, we also sample infinitely many “atoms” independently zi ∼ G0. Define G as: G(z) = E&apos;1 7riS(zi, z) (1) where S(zi, z) is 1 if zi = z and 0 otherwise. This random variable is drawn from a Pitman-Yor process. Notice the discreteness of G is laid bare in the stick-breaking</context>
</contexts>
<marker>Sethuraman, 1994</marker>
<rawString>J. Sethuraman. 1994. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
</authors>
<title>Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="30161" citStr="Smith, 2006" startWordPosition="5242" endWordPosition="5243">k 34 seconds, each variational EM iteration took 43 seconds and parsing took 208 seconds. The total time was 47 minutes, which is 2.8 times faster than sampling. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define noun constituents. We then use the preprocessing step defined in §3.3 with </context>
</contexts>
<marker>Smith, 2006</marker>
<rawString>N. A. Smith. 2006. Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian LDAbased model for semi-supervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="1381" citStr="Toutanova and Johnson, 2007" startWordPosition="185" endWordPosition="188">we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior infere</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>K. Toutanova and M. Johnson. 2007. A Bayesian LDAbased model for semi-supervised part-of-speech tagging. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Wainwright</author>
<author>M I Jordan</author>
</authors>
<title>Graphical models, exponential families, and variational inference. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="12055" citStr="Wainwright and Jordan, 2008" startWordPosition="2076" endWordPosition="2080">inals’ subtrees zA = {zA,i |A ∈ M; i ∈ {1, ...}} and the derivations z1:n = z1, ... , zn. The symbol z refers to the collection of {zA |A ∈ M}, and z1:n refers to the derivations of the data x. Note that the distribution in 2(c) is defined with the GEM distribution, as mentioned earlier. It is a sample from the Pitman-Yor process (or the Dirichlet process), which is later used in 3(a) to sample trees for an adapted non-terminal. 3 Variational Inference Variational inference is a deterministic alternative to MCMC, which casts posterior inference as an optimization problem (Jordan et al., 1999; Wainwright and Jordan, 2008). The optimized function is a bound on the marginal likelihood of the observations, which is expressed in terms of a so-called “variational distribution” over the hidden variables. When the bound is tightened, that distribution is close to the posterior of interest. Variational methods tend to converge faster than MCMC, and can be more easily parallelized over multiple processors in a framework such as MapReduce (Dean and Ghemawat, 2004). The variational bound on the likelihood of the data is: log p(x |a, α) ≥ H(q) + � Eq[log p(vA |aA)] AEM + � Eq[log p(θA |αA)] AEM + � Eq[logp(zA |v, θ)] + Eq</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>M. J. Wainwright and M. I. Jordan. 2008. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1:1–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>D M Blei</author>
</authors>
<title>Variational inference for the nested Chinese restaurant process.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="23098" citStr="Wang and Blei (2009)" startWordPosition="4059" endWordPosition="4062">l. (2009). While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s). Then, we use the top NA strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and Blei (2009), but we did not achieve better performance and it had an adverse effect on runtime. For completeness, we give these results in §4. and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar indu</context>
</contexts>
<marker>Wang, Blei, 2009</marker>
<rawString>C. Wang and D. M. Blei. 2009. Variational inference for the nested Chinese restaurant process. In NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>