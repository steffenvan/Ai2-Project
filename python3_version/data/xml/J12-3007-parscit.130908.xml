<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998755">
A Scalable Distributed Syntactic, Semantic,
and Lexical Language Model
</title>
<author confidence="0.998173">
Ming Tan*
</author>
<affiliation confidence="0.990457">
Wright State University
</affiliation>
<author confidence="0.98947">
Wenli Zhou**
</author>
<affiliation confidence="0.988847">
Wright State University
</affiliation>
<author confidence="0.978535">
Lei Zhengt
</author>
<affiliation confidence="0.991566">
Wright State University
</affiliation>
<author confidence="0.979482">
Shaojun Wang$
</author>
<affiliation confidence="0.989575">
Wright State University
</affiliation>
<bodyText confidence="0.999513">
This paper presents an attempt at building a large scale distributed composite language model
that is formed by seamlessly integrating an n-gram model, a structured language model, and
probabilistic latent semantic analysis under a directed Markov random field paradigm to simul-
taneously account for local word lexical information, mid-range sentence syntactic structure,
and long-span document semantic content. The composite language model has been trained by
performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm
to improve word prediction power on corpora with up to a billion tokens and stored on a
supercomputer. The large scale distributed composite language model gives drastic perplexity
reduction over n-grams and achieves significantly better translation quality measured by the
Bleu score and “readability” of translations when applied to the task of re-ranking the N-best list
from a state-of-the-art parsing-based machine translation system.
</bodyText>
<sectionHeader confidence="0.996946" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9998022">
The Markov chain (n-gram) source models, which predict each word on the basis of the
previous n − 1 words, have been the workhorses of state-of-the-art speech recognizers
and machine translators that help to resolve acoustic or foreign language ambiguities by
placing higher probability on more likely original underlying word strings. Although
the Markov chains are efficient at encoding local word interactions, the n-gram model
</bodyText>
<note confidence="0.99410875">
* Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton
OH 45435. E-mail: tan.6®wright.edu.
** Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton
OH 45435. E-mail: zhou.23®wright.edu.
</note>
<footnote confidence="0.368205">
t Kno.e.sis Center, Wright State University, Dayton OH 45435. E-mail: lei.zheng®wright.edu.
</footnote>
<note confidence="0.712119666666667">
t Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton
OH 45435. E-mail: shaojun.wang®wright.edu.
Submission received: 10 October 2010; revised submission received: 17 October 2011; accepted for publication:
16 November 2011.
No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of
the United States Government and is therefore a work of the United States Government In accordance with
</note>
<page confidence="0.703318">
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
</page>
<note confidence="0.5795">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.99996598">
clearly ignores the rich syntactic and semantic structures that constrain natural lan-
guages. Attempting to increase the order of an n-gram to capture longer range depen-
dencies in natural language immediately runs into the curse of dimensionality (Bengio
et al. 2003). The performance of conventional n-gram technology has essentially reached
a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to
improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005;
Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen
2007) have shown that using an immense distributed computing paradigm, up to
6-grams, can be trained on up to billions and trillions of tokens, yielding consistent sys-
tem improvements because of excellent n-gram hit ratios on unseen test data, but Zhang
(2008) did not observe much improvement beyond 6-grams. As the machine translation
(MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These
approaches have resulted in small improvements in MT quality, but have not funda-
mentally solved the problem. There is a dire need for developing novel approaches to
language modeling.”
Over the past two decades, more sophisticated models have been developed that
outperform n-grams; these are mainly the syntactic language models (Della Pietra et al.
1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and
Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005)
that effectively exploit sentence-level syntactic structure of natural language, and the
topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda
2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each
of these language models only targets some specific, distinct linguistic phenomena
(Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects
of natural language regularity. A natural question we should ask is whether/how
we can construct more complex and powerful but computationally tractable language
models by integrating many existing/emerging language model components, with each
component focusing on specific linguistic phenomena like syntactic structure, semantic
topic, morphology, and pragmatics in complementary, supplementary, and coherent
ways (Bellegarda 2001, 2003).
Several techniques for combining language models have been investigated. The
most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek
and Mercer 1980; Goodman 2001), where each individual model is trained separately
and then combined by a weighted linear combination. All of the syntactic structure-
based models have used linear interpolation to combine trigrams to achieve further
improvement over using their own models alone (Charniak 2001; Chelba and Jelinek
2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out
data. Even though this technique is simple and easy to implement, it does not generally
yield very effective combinations (Rosenfeld 1996) because the linear additive form
is a strong assumption in capturing subtleties in each of the component models (see
more explanation and analysis in Section 6.2 and Appendix A). The second method
is based on maximum entropy philosophy, which became very popular in machine
learning and natural language processing communities due to the work in Berger,
Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997),
Lau et al. (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum
entropy is nothing but maximum likelihood estimation for undirected Markov random
fields (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra,
and Lafferty 1997). As stated in Wang et al. (2005b), however, there are two weaknesses
with maximum entropy approach. The first weakness is that this approach can only
model distributions over explicitly observed features, but we know there is hidden
</bodyText>
<page confidence="0.98845">
632
</page>
<note confidence="0.931978">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.99997894">
information in natural language, such as syntactic structure and semantic topic. The
second weakness is that if the statistical model is too complex it becomes intractable to
estimate model parameters; computationally very expensive Markov chain Monte Carlo
sampling methods (Mark, Miller, and Grenander 1996; Rosenfeld 2000b; Rosenfeld,
Chen, and Zhu 2001) would have to be used. One way to overcome the first hurdle
is to use a preprocessing tool to extract hidden features (e.g., Rosenfeld [1996] used
mutual information clustering method to find word pair triggers) then combine these
triggers with trigrams through a maximum conditional entropy approach to allow the
discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba
and Jelinek’s structured language model and a word clustering model to extract relevant
grammatical and semantic features, then to again combine these features with trigrams
through a maximum conditional entropy approach to form a syntactic, semantic, and
lexical language model. Wang and colleagues (Wang et al. 2005a; Wang, Schuurmans,
and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which
extends standard maximum entropy estimation by incorporating hidden dependency
structure, but still the LME wouldn’t overcome the second hurdle. The third method is
directed Markov random field (Wang et al. 2005b) that overcomes both weaknesses in
the maximum entropy approach. Wang et al. used this approach to combine trigram,
probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis
(PLSA) models; a generalized inside–outside algorithm is derived that alters the well-
known inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with
modular modification to take into account the effect of n-gram and PLSA while remain-
ing at the same cubic time complexity. When applying this to the Wall Street Journal
corpus with 40 million tokens, they achieved moderate perplexity reduction. Because
the probabilistic dependency structure in a structured language model (SLM) (Chelba
2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG,
Wang et al. (2006) studied the stochastic properties for the composite language model
that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al.
2005b) and derived another generalized inside–outside algorithm to train a composite n-
gram, SLM, and PLSA language model from a general expectation maximization (EM)
(Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition
of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized
inside–outside algorithm alters Jelinek’s inside–outside algorithm with modular modi-
fication and has the same sixth order of sentence-length time complexity. Unfortunately,
there are no experimental results reported.
In this article, we study the same composite n-gram, SLM, and PLSA model un-
der the directed MRF framework as in Wang et al. (2006). The composite n-gram/
SLM/PLSA language model under the directed MRF paradigm is first introduced in
Section 2. In Section 3, instead of using the sixth order generalized inside–outside
algorithm proposed in Wang et al. (2006), we show how to train this composite model
via an N-best list approximate EM algorithm that has linear time complexity and a
follow-up EM algorithm to improve word prediction power. We prove the convergence
of the N-best list approximate EM algorithm. To resolve the data sparseness problem,
we generalize Jelinek and Mercer’s recursive mixing scheme for Markov source (Jelinek
and Mercer 1980) to a mixture of Markov chains. To handle large-scale corpora up to a
billion tokens, we demonstrate how to implement these algorithms under a distributed
computing environment and how to store this language model on a supercomputer. In
Section 4, we describe how to use the model for testing. Related works are then summa-
rized and compared in Section 5. Because language modeling is a data-rich and feature-
rich density estimation problem, there is always a trade-off between approximate error
</bodyText>
<page confidence="0.997611">
633
</page>
<note confidence="0.799926">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.999960952380953">
and estimation error, thus in Section 6 we conduct comprehensive experiments on
corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens, and compare
perplexity results with n-grams (n = 3, 4, 5 respectively) on these three corpora under
various situations; drastic perplexity reductions are obtained. We explain why the com-
posite language models lead to better predictive capacity than linear interpolation. The
proposed composite language models are applied to the task of re-ranking the N-best
list from Hiero (Chiang 2005, 2007), a state-of-the-art parsing-based machine translation
system; we achieve significantly better translation quality measured by the Bleu score
and “readability” of translations. Finally, we draw our conclusions and propose future
work in Section 7.
The main theme of our approach is “to exploit information, be it syntactic structure
or semantic fabric, which involves a fairly high degree of cognition. This is precisely
the kind of knowledge that humans naturally and inherently use to process natural
language, so it can be reasonably conjectured to represent a key ingredient for success”
(Bellegarda 2003, p. 105). In that light, the directed MRF framework, “whose ultimate
goal is to integrate all available knowledge sources, appears most likely to harbor a
potential breakthrough. It is hoped that the on-going effort conducted in this work to
leverage such latent synergies will lead, in the not-too-distant future, to more polyva-
lent, multi-faceted, effective and tractable solutions for language modeling – this is only
beginning to scratch the surface in developing systems capable of deep understanding
of natural language” (Bellegarda 2003, p. 105).
</bodyText>
<sectionHeader confidence="0.982489" genericHeader="method">
2. The Composite n-gram/SLM/PLSA Language Model
</sectionHeader>
<bodyText confidence="0.995811">
Let X denote a set of random variables (Xτ)τ∈r taking values in a (discrete) probability
space (Xτ)τ∈r, where Γ is a finite set of states. We define a (discrete) directed Markov
random field to be a probability distribution P, which admits a recursive factorization
if there exist non-negative functions, κτ(·, ·),τ E Γ defined on Xτ x Xpa(τ), such that
Exτ κτ(xτ,xpa(τ)) = 1 and P has density
</bodyText>
<equation confidence="0.790697">
p(x) = 11 κτ(xτ, xpa(τ)) (1)
τ∈r
</equation>
<bodyText confidence="0.999039066666667">
Here pa(τ) denotes the set of parent states of τ. If the recursive factorization refers to a
graph, then we have a Bayesian network (Lauritzen 1996). Broadly speaking, however,
the recursive factorization can refer to a representation more complicated than a graph
with a fixed set of nodes and edges—for example, PCFG and SLM are examples of
directed MRFs whose parse tree structure is a random object that can’t be described
as a Bayesian network (McAllester, Collins, and Pereira 2004). A key difference be-
tween directed MRFs and undirected MRFs is that a directed MRF requires many
local normalization constraints whereas an undirected MRF has a global normalization
factor.
The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is essentially a
WORD-PREDICTOR, that is, given its entire document history, it predicts the next word
wk+1 E V based on the last n–1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 =
wk−n+2, · · · , wk and V denotes the vocabulary.
The SLM proposed in Chelba and Jelinek (1998, 2000) and Chelba (2000) uses syntac-
tic information beyond the regular n-gram models to capture sentence-level long-range
</bodyText>
<page confidence="0.995791">
634
</page>
<note confidence="0.932119">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.999549166666667">
dependencies. The SLM is based on statistical parsing techniques that allow syntactic
analysis of sentences; it assigns a probability p(W,T) to every sentence W and every
possible binary parse T. The terminals of T are the words of W with part of speech
(POS) tags, and the nodes of T are annotated with phrase headwords and non-terminal
labels. Let W be a sentence of length n words to which we have prepended the sentence
beginning marker (s) and appended the sentence end marker (/s) so that w0 =(s) and
wn+1 =(/s). Let Wk = w0, · · · , wk be the word k-prefix of the sentence (the words from
the beginning of the sentence up to the current position k) and WkTk be the word-parse
k-prefix. A word-parse k-prefix has a set of exposed heads h−m, · · · , h−1 ∈ H, with each
head being a pair (headword, non-terminal label), H = V × ONT where ONT denotes
the set of non-terminal label (NTlabel), or in the case of a root-only tree (word, POS tag)
H = V × O where O denotes the set of POS tags. The exposed heads at a given position
k in the input sentence are a function of the word-parse k-prefix.
The SLM operates left-to-right, building up the parse structure in a bottom–up
manner. At any given stage of the word generation by the SLM, the exposed headwords
are those headwords of the current partial parse which are not yet part of a higher
phrase with a head of its own. An mth order SLM (m-SLM) has three operators to
generate a sentence:
</bodyText>
<listItem confidence="0.859328">
• The WORD-PREDICTOR predicts the next word wk+1 ∈ V based on the m
most recently exposed headwords h−1
</listItem>
<equation confidence="0.6169315">
−m = h−m,··· , h−1 in the word-parse
k-prefix with probability p(wk+1|h−1
</equation>
<bodyText confidence="0.95079175">
−m), and then passes control to the
TAGGER.
• The TAGGER predicts the POS tag tk+1 ∈ O to the next word wk+1 based
on the next word wk+1 and the POS tags of the m most recently exposed
headwords h−1
−m (denoted as h−1 −m.tag = h−m.tag, · · · ,h−1.tag) in the
word-parse k-prefix with probability p(tk+1|wk+1, h−1−m.tag).
• The CONSTRUCTOR builds the partial parse Tk+1 from Tk, wk+1, and tk+1
in a series of moves ending with NULL, where a parse move a is made
with probability p(a|h−1
−m); a ∈ A={(unary, NTlabel), (adjoin-left, NTlabel),
(adjoin-right, NTlabel), NULL}. Depending on an action a = adjoin-right
or adjoin-left, the headword h−1 or h−2 is percolated up by one tree level,
the indices of the current exposed headwords h−3, h−4, · · · are increased
by 1, and these headwords together with h−1 or h−2 become the new
exposed headwords. Once the CONSTRUCTOR hits NULL, the
headword indexing and current parse structure remain as they are,
and the CONSTRUCTOR passes control to the WORD-PREDICTOR.
SLM is thus essentially a generalization of a shift-reduce parser (Aho and Ullman
1972) with adjoin corresponding to reduce and predict to shift. (See a detailed description
about SLM in Chelba and Jelinek [1998, 2000]; Chelba [2000]; Jelinek [2004]). As an
example taken from Jelinek (2004), Figure 1 shows a complete parse where SB/SE is a
distinguished POS tag for (s)/(/s) respectively, ((s),TOP) is the only allowed head, and
((/s),TOP’) is the head of any constituent that dominates (/s) but not (s). In Figure 1,
at the time just after the word as is generated, the exposed headwords are “(s) SB,
show np, has vbz.” The subsequent model actions are: “POStag as, null, predict its,
POStag its, null, predict host, POStag host, adjoin-right-np, adjoin-left-pp, adjoin-left-
pp, null, predict a, · · · .”
</bodyText>
<page confidence="0.995819">
635
</page>
<figure confidence="0.487729">
Computational Linguistics Volume 38, Number 3
</figure>
<figureCaption confidence="0.984232">
Figure 1
</figureCaption>
<bodyText confidence="0.967527">
A complete parse tree by the structured language model.
A PLSA model (Hofmann 2001) is a generative probabilistic model of word-
document co-occurrences using the bag-of-words assumption described as follows:
</bodyText>
<listItem confidence="0.99969925">
• Choose a document d with probability p(d).
• SEMANTIZER selects a semantic class g E 9 with probability p(gJd) where
9 denotes the set of topics.
• WORD-PREDICTOR picks a word w E V with probability p(wJg).
</listItem>
<bodyText confidence="0.990591739130435">
Because only one pair of (d, w) is being observed, the joint probability model is a mixture
of log-linear models with the expression p(d,w) = p(d) Eg p(wJg)p(gJd). Typically, the
number of documents and the vocabulary size are much larger than the size of latent
semantic class variables. Latent semantic class variables therefore function as bottleneck
variables to constrain word occurrences in documents.
When combining n-gram, m-SLM, and PLSA together to build a composite
generative language model under the directed MRF paradigm (Wang et al. 2005b,
2006), the composite language model is simply a complicated generative model that has
four operators: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER.
The TAGGER and CONSTRUCTOR in SLM and the SEMANTIZER in PLSA remain
unchanged; the WORD-PREDICTORs in n-gram, m-SLM, and PLSA, however, are
combined to form a stronger WORD-PREDICTOR that generates the next word, wk+1,
not only depending on the m most recently exposed headwords h−1
−m in the word-parse
k-prefix but also its n-gram history wkk−n+2 and its semantic content gk+1. The parameter
for WORD-PREDICTOR in the composite n-gram/m-SLM/PLSA language model
becomes p(wJw−n+1h−1). The resulting composite language model has an even more
mg
complex dependency structure but with more expressive power than the original
SLM. Figure 2 illustrates the structure of a composite n-gram/m-SLM/PLSA language
model.
The composite n-gram/m-SLM/PLSA language model can be formulated as a
rather complex chain-tree-table directed MRF model (Wang et al. 2006) with local
</bodyText>
<page confidence="0.998773">
636
</page>
<note confidence="0.924809">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<figureCaption confidence="0.973264">
Figure 2
</figureCaption>
<bodyText confidence="0.97142">
A composite n-gram/m-SLM/PLSA language model where the hidden information is the parse
tree T and semantic content g. The n-gram encodes local word interactions, the m-SLM models
the sentence’s syntactic structure, and the PLSA captures the document’s semantic content;
all interact together to constrain the generation of natural language. The WORD-PREDICTOR
generates the next word wk+1 with probability p(wk+1|wk k−n+2h−1
</bodyText>
<equation confidence="0.930321">
−mgk+1) instead of p(wk+1|wkk−n+2),
p(wk+1|h−1
−m), and p(wk+1|gk+1), respectively.
</equation>
<bodyText confidence="0.7456705">
normalization constraints for the parameters of each model component, WORD-
PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER. That is,
</bodyText>
<equation confidence="0.999509285714286">
p(w|w−1
−n+1h−1
−mg) = 1 (2)
p(t|wh−1 −m.tag) = 1 (3)
p(a|h−1
−m) = 1 (4)
p(g|d) = 1 (5)
</equation>
<bodyText confidence="0.999920875">
If we look at the example in Figure 1, for the composite n-gram/m-SLM/PLSA
language model there exists a SEMANTIZER’s action to choose a topic g before
any WORD-PREDICTOR’s action. Moreover, for m-SLM, its WORD-PREDICTOR
predicts the next word, such as a, based on m most recently exposed headwords
“(s)-SB, show-np, has-vp,” but for the composite model, the WORD-PREDICTOR
predicts the next word a based on m most recently exposed headwords “(s)-SB,
show-np, has-vp,” n-grams “as its host,” and a topic g. These are the only differences
between SLM and our proposed composite language model.
</bodyText>
<sectionHeader confidence="0.948272" genericHeader="method">
3. Training Algorithm
</sectionHeader>
<bodyText confidence="0.999174333333333">
For the composite n-gram/m-SLM/PLSA language model under the directed MRF
paradigm, the likelihood of a training corpus v, a collection of documents, can be
written as
</bodyText>
<equation confidence="0.756247636363636">
11        
ˆ�(v, p) =  � � � Pp(Wl, Tl, Gl|d)   p(d) (6)
d∈v l Gl Tl
�
w∈V
�
t∈O
�
a∈A
�
g∈4
</equation>
<page confidence="0.864204">
637
</page>
<note confidence="0.255603">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.9542714">
where (Wl, Tl, Gl|d) denotes the joint sequence of the lth sentence Wl with its parse struc-
ture Tl and semantic annotation string Gl in document d. This sequence is produced by
a unique sequence of model actions: WORD-PREDICTOR, TAGGER, CONSTRUCTOR,
SEMANTIZER moves; its probability is obtained by chaining the probabilities of these
moves
</bodyText>
<equation confidence="0.999286785714286">
� p(g|d)#(g,Wl,Gl,d) Il (7)
Pp(Wl,Tl,Gl|d) = h−1,···,h−mEll
gEG

w,w−1,···,w−n+1EV
H
p(w|w−1 −mg,Wl,Tl,Gl,d)
−n+1h−1
−mg)#(w−1
−n+1wh−1
tEO −m.tag,Wl,Tl,d) �
p(t|wh−1
−m.tag)#(t,wh−1
aEA
</equation>
<bodyText confidence="0.631564666666667">
where #(g, Wl, Gl, d) is the count of semantic content g in semantic annotation string
of
the lth sentence Wl in document d;
is the count of n-grams,
its m most recently exposed headwords, and semantic content g in parse Tl and semantic
annotation string Gl of the lth sentence Wl in document d;
</bodyText>
<equation confidence="0.812881916666666">
Wl, Tl, d) is the
count of tag t predicted by word w and the tags of m most recently exposed headwords
in parse tree Tl of the lth sentence Wl in document d; and finally
is the
count of constructor move a conditi
Gl
#(w−1
−n+1wh−1
−mg,Wl, Tl,Gl, d)
#(twh−1
−m.tag,
#(ah−1
−m,Wl,Tl,d)
oning on m exposed headwords h−1
−m in parse tree Tl
of the lth sentence Wl in document d.
Let rl
then G(D,p) =      
dED � � �Pp(Wl,Tl,Gl|d)    (8)
l Gl
Tl
ˆG(D,p) = G(D,p) � (p(d)) (9)
dED
Clearly, when maximizing
</equation>
<bodyText confidence="0.975015571428571">
in Equation (6), p(d) is an ancillary term that is
independent of all other data-generating parameters, it is not critical to anything that
follows; moreover, when a language model is used to find the most likely word se-
quence in machine translation and speech recognition, this term is useless. Thus, similar
to an n-gram language model, we will generally ignore this term and concentrate on
optimizing Equation (8) in the subsequent development.
The objective of maximum likelihood estimation is to maximize the likeli
</bodyText>
<equation confidence="0.933176285714286">
ˆG(D,p)
hood
G(D,p) with respect to model parameters. For a given sentence, its parse tree and
ri
p(a|h−1
−m)#(a,h−1
−m,Wl,Tl,d)
</equation>
<page confidence="0.817418">
638
</page>
<bodyText confidence="0.937385">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
semantic content are hidden and the number of parse trees grows faster than expo-
nentially with sentence length; Wang et al. (2006) have derived a generalized inside–
outside algorithm by applying the standard EM algorithm and considering the auxiliary
function
</bodyText>
<equation confidence="0.997808">
� � � � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) (10)
Q(p , p) = Gl Tl
d∈D l
</equation>
<bodyText confidence="0.999452">
The complexity of this algorithm is sixth order (sentence length), however; thus it is
computationally too expensive to be practical for a large corpus even with the use of
pruning on charts (Jelinek and Chelba 1999; Jelinek 2004).
</bodyText>
<subsectionHeader confidence="0.99783">
3.1 N-best List Approximate EM
</subsectionHeader>
<bodyText confidence="0.99867375">
Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list
approximate EM re-estimation with modular modifications to seamlessly incorporate
the effect of n-gram and PLSA components. Instead of maximizing the likelihood
L(D, p), we maximize the N-best list likelihood,
</bodyText>
<equation confidence="0.9855806">
max �        
TSN L(D,p,T�N) = ��  �
d∈D  max Pp(Wl,Tl,Gl|d)    (11)
T�l N∈T�N
l Gl Tl∈T�lN,||T�lN||=N
</equation>
<bodyText confidence="0.999750666666667">
where T&apos;lN is a set of N parse trees for sentence Wl in document d,  ||·  ||denotes the
cardinality, and T�N is a collection of T&apos;N for sentences over entire corpus D.
The N-best list approximate EM involves two steps:
</bodyText>
<listItem confidence="0.9918095">
1. N-best list search: For each sentence W in document d, find N-best
parse trees,
</listItem>
<equation confidence="0.998543666666667">
� � � �
TN l = argmax Pp(Wl,Tl,Gl|d), ||T�l N ||= N
T&apos;lNGlTl∈T�lN
</equation>
<bodyText confidence="0.909646">
and denote TN as the collection of N-best list parse trees for sentences
over entire corpus D under model parameter p.
</bodyText>
<listItem confidence="0.999085">
2. EM update: Perform one iteration (or several iterations) of the EM
algorithm to estimate model parameters that maximize N-best list
likelihood of the training corpus D,
</listItem>
<equation confidence="0.597104666666667">
� 
˜L(D,p,TN) = ri Y: � Pp(Wl, Tl, Gl|d)   
d∈D l Gl Tl∈TlN∈TN
</equation>
<page confidence="0.984409">
639
</page>
<figure confidence="0.93796">
Computational Linguistics Volume 38, Number 3
That is,
(a) E-step: Compute the auxiliary function of the N-best list likelihood
� ˜Q(p�,p,TN) = � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d)
d∈v l Gl Tl∈TlN∈TN
</figure>
<listItem confidence="0.9004825">
(b) M-step: Maximize ˜Q(p&apos;, p, TN) with respect to p&apos; to get the new
update for p.
</listItem>
<bodyText confidence="0.998159875">
Iterate steps (1) and (2) until the convergence of the N-best list likelihood.
We use Zangwill’s global convergence theorem (Zangwill 1969) to analyze the
behavior of convergence of the N-best list approximate EM.
First, we define two concepts needed for Zangwill’s global convergence theorem.
A map M is from points of © to subsets of © is called a point-to-set map on ©. It
is said to be closed at 0 if 0i → 0,0i E © and Ai → A, Ai E M(0i) implies A E M(0).
For a point-to-point map, continuity implies closedness. Then the global convergence
theorem (Zangwill 1969) states the following.
</bodyText>
<subsectionHeader confidence="0.508118">
Theorem
</subsectionHeader>
<bodyText confidence="0.999840285714286">
Let M be a point-to-set map (an algorithm) that, given a point 00 E ©, generates a
sequence {0∞i=0} through the iteration 0i+1 = M(0i). Let Q E © be the set of fixed points
of M. Suppose (i) M is closed over the complement of Q; (ii) there is a continuous
function 4) on © such that (a) if 0 E� Q, 4)(A) &gt; 4)(0) for all A E M(0), and (b) if 0 E Q,
4)(A) ≥ 4)(0) for all A E M(0).
Then all the limit points of {0i} are in Q and 4)(0i) converges monotonically to 4)(0)
for some 0 E Q.
</bodyText>
<subsectionHeader confidence="0.743118">
Proof
</subsectionHeader>
<bodyText confidence="0.999892375">
This theorem has been used by Wu (1983) to prove the convergence of a standard EM
algorithm (Dempster, Laird, and Rubin 1977). We now use this theorem to show that
the N-best list approximate EM algorithm globally converges to the stationary points
of the N-best list likelihood. We encounter one difficulty at this point, however, due to
the maximization operator in Equation (11); after each iteration the N-best list may have
been changed, therefore the set of data presented for the estimation of model parameters
may be different from the previous one. Nevertheless, we prove the convergence of the
N-best list approximate EM algorithm by checking whether it satisfies two conditions
in Zangwill’s global convergence theorem. Because the composite model is essentially
a mixture model of a curved exponential family through a complex hierarchy, there
is a closed form solution for the ˜Q(p&apos;,p,TN) function irrespective of the N-best list
parse trees, so the N-best list approximate EM algorithm is a one-to-one map. Because
˜Q(p&apos;,p,TN) is continuous in both p&apos; and p, the map is closed, thus condition (i) is
satisfied.
To check condition (ii), we need to verify that the N-best list likelihood as a function
of p satisfies the properties of 4)(0) in condition (ii). Let ˇTN and ¯TN be the two collections
</bodyText>
<page confidence="0.984706">
640
</page>
<note confidence="0.60519">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.980724">
of N-best list parse trees for sentences over entire corpus D under two model parameters
pˇ and ¯p, respectively:
</bodyText>
<equation confidence="0.98057225">
ˇTN = arg max L(D, ˇp,T&apos;N) (12)
TSN
¯TN = arg max L(D, ¯p, T&apos;N) (13)
TSN
</equation>
<bodyText confidence="0.83194">
and let p¯ be the closed form solution of maximizing ˜Q(p&apos;, ˇp, ˇTN) with respect to p&apos;, that is,
</bodyText>
<equation confidence="0.961505833333333">
Then p¯ = arg max ˜Q(p&apos;,ˇp, ˇTN) (14)
p,
max L(D, ¯p,T�N) ≥ ˜L(D, ¯p, ˇTN) (15)
TSN ≥ ˜L(D, ˇp, ˇTN) (16)
≥ max L(D, ˇp,T&apos;N) (17)
T-N
</equation>
<bodyText confidence="0.974713178571429">
The inequality in Equation (15) is strict unless ˇTN = ¯TN, which results in p¯ ∈ M( ¯p).
Using results proven by Wu (1983), we know that when pˇ is not a stationary point of the
N-best list likelihood or pˇ ∈/ M( ˇp), a L(D, ˇp,TN)— a˜Q(p&apos;, ˇp, ˇTN) ˜Q( ¯p,ˇp,ˇTN) &gt; ˜Q(ˇp, ˇp, ˇTN),
a pˇ — a pˇ =~0,
thus the inequality in Equation (16) is strict. Finally, the inequality in Equation (17) is
strict unless pˇ ∈ M( ˇp). Thus condition (ii) is satisfied.
This completes the proof that the N-best list approximate EM algorithm mono-
tonically increases the N-best list likelihood and converges in the sense of Zangwill’s
global convergence.
In the following, we formally derive the N-best list approximate EM algorithm with
linear sentence length time complexity. ■
3.1.1 N-best List Search Strategy. For each sentence W in document d, instead of scanning
all the hidden events (both allowed parse trees and semantic annotation strings) we
restrict the algorithm to operate with N-best hidden events. We find that, for each
document, a large number of topics should be pruned and only a small set of allowed
topics should be kept due to the considerations of both computational time and resource
demand, otherwise we have to use many more machines to store WORD-PREDICTOR’s
parameters.
We can either find both the N-best parses for each sentence and N-best topics for
each document simultaneously or separately. The latter is much preferred, because the
first case is much more computationally expensive.
To extract the N-best topics, we run an EM algorithm for a PLSA model on training
corpus D, then keep the N most likely topics (denoted as Gd) according to the values of
p(g|d); the rest of the topics are purged.
To extract the N-best parse trees, we adopt a synchronous, multi-stack search
strategy that is similar to the one in Chelba and Jelinek (1998, 2000) and Chelba
(2000), which involves a set of stacks storing partial parses of the most likely ones
for a given prefix Wk and the less probable parses are purged. Each stack contains
</bodyText>
<page confidence="0.993428">
641
</page>
<note confidence="0.290858">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.9970947">
hypotheses (partial parses) that have been constructed by the same number of WORD-
PREDICTOR and the same number of CONSTRUCTOR operations. The hypotheses in
each stack are ranked according to the log(Pp (Wk, Tk  |d)) score with the highest on top,
where Pp(Wk,Tk |d) = EGk Pp(Wk, Tk, Gk|d) and the Wk, Tk, Gk denote the joint sequence
of prefix Wk = w0, w1 · · · , wk with its parse structure Tk and semantic annotation string
Gk = g1, ··· ,gk, gi E 9d, i = 1, · · · , k in document d. This sequence is produced by a
unique sequence of model actions: WORD-PREDICTOR, TAGGER, CONSTRUCTOR,
and SEMANTIZER moves. Its probability is obtained by chaining the probabilities of
these moves. The value of Pp(Wk,Tk|d) is computed recursively from Pp(Wk−1,Tk−1|d)
by the following formula:
</bodyText>
<equation confidence="0.998849285714286">
 
 �Pp(Wk,Tk|d) = Pp(Wk−1,Tk−1|d) p(wk|wk−1
k−n+1h Egic
—mgk) gk|d) (18)
gk∈9d 9d p(gi|d)
p(tk|wk, h−1
−m tag)p(Tk−1,k|Wk−1Tk−1, wk, tk)
</equation>
<bodyText confidence="0.999098730769231">
where Wk−1Tk−1 is the word-parse (k − 1)-prefix; wk is the kth word predicted by
WORD-PREDICTOR; tk is the tag assigned to wk by the TAGGER; Tk−1,k is the incre-
mental parse structure that generates Tk = Tk−1||Tk−1,k when attached to Tk−1, (this
is the parse structure built on top of Tk−1 and the newly predicted word wk); the ||
notation stands for concatenation. Finally, p(Tk−1,k|Wk−1Tk−1,wk,tk) is the product of
the probabilities of a series of CONSTRUCTOR moves in Tk−1,k to form Tk. Because the
topics are pruned to 9d, the probability of the SEMANTIZER is normalized to ensure a
proper probability distribution. A stack vector consists of the ordered set of stacks con-
taining partial parses with the same number of WORD-PREDICTOR operations but a
different number of CONSTRUCTOR operations. In WORD-PREDICTOR and TAGGER
operations, some hypotheses are discarded due to the maximum number of hypotheses
that the stack can contain at any given time. In the CONSTRUCTOR operation, the
resulting hypotheses are discarded due to either finite stack size or the log-probability
threshold (the maximum tolerable difference between the log-probability score of the
top-most hypothesis and the bottom-most hypothesis at any given state of the stack).
The synchronous, multi-stack search strategy is a greedy best-first search algorithm,
one of the local heuristic search procedures that does not use future cost estimates
to guide the search and thus does not guarantee that the N-best list parse trees are a
global optimal solution (Russell and Norvig 2010). In practice, however, we find that
the N-best list approximate EM algorithm does converge within several iterations.
3.1.2 EM Update. Once we have both the N-best parse trees for each sentence in docu-
ment d and the N-best topics for document d, we derive the EM algorithm to estimate
model parameters.
Maximizing ˜Q(p&apos;, p, TN) with respect to p&apos; leads to re-estimated parameters of the
composite model, which are nothing but the following normalized conditional expected
counts:
</bodyText>
<equation confidence="0.983981222222222">
� �
p�(w|w−1
−n+1h−1 −mg) °�
d∈v l
E Pp(Tl, Gl|Wl, d)#(w−1
−n+1wh−1
−mg, Wl, Tl, Gl, d) (19)
Gl Tl∈Tl N∈TN
E
</equation>
<page confidence="0.919149">
642
</page>
<figure confidence="0.896518">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
� � � Pp(Tl|Wl, d)#(twh−1 −m.tag, Wl, Tl, d) (20)
p�(t|wh−1 l Tl∈TlN∈TN
−m.tag) ∝
d∈D
� � � Pp(Tl|Wl,d)#(ah−1
p�(a|h−1 l Tl∈TlN∈TN −m,Wl,Tl,d) (21)
−m)) ∝
d∈D
� � � E Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d) (22)
p&apos;(g|d) ∝ l Gl Tl∈TlN∈TN
d∈D
</figure>
<bodyText confidence="0.998340384615385">
In the E-step, we use Equations (19)–(22) to compute the expected count of each
model parameter over sentence Wl in document d in the training corpus D. In the full
case where the number of parse trees grows faster than exponentially with sentence
length, we use Jelinek-style recursive formulas in the generalized inside–outside algo-
rithm (Jelinek 2004) to handle the tree structure and describe the weighted forest of
possible derivations (Wang et al. 2006). In the N-best list case considered in this paper,
however, we just enumerate each parse tree in the N-best list and compute the expected
posterior count for each parse tree. For the WORD-PREDICTOR and the SEMANTIZER,
we use Equations (19) and (22) and note that there is a sum over semantic annotation se-
quence Gl where the number of possible semantic annotation sequences is exponential.
We use forward–backward recursive formulas reminiscent of those in hidden Markov
models to compute the expected counts. To be more specific, for each parse Tl ∈ TlN, we
define the forward vector αl(g|d) to be
</bodyText>
<equation confidence="0.996492777777778">
� Pp(Wlk, Tlk, wkk−n+2wk+1h−1
αlk+1(g|d) = −mg, Glk|d) (23)
Gl
k
= Pp(Wlk, Tlk, wkk−n+2wk+1h−1
−mg|d)
l l k −1 p(gk+1|d)
= Pp(Wk, Tk|d)p(wk+1|wk−n+2h−mg, d)
Egi∈Gd p(gi|d)
</equation>
<bodyText confidence="0.999553333333333">
where Wlk is the word k-prefix for sentence Wl, and Tlk is the parse for k-prefix. It is easy
to see that the forward vector αl(g|d) can be recursively computed in a forward manner
using Equation (18) as
</bodyText>
<equation confidence="0.98721125">
 
 �
αlk+1(g|d) = αlk(gk|d) p(tk|wk,h−1
−m.tag)p(Tlk−1,k|Wlk−1Tlk−1, wk, tk) (24)
gk∈Gd
p(wk+1|wk k−n+2h−1
−mg,d) p(gk+1,d)
Egi∈Gd p(gi|d)
</equation>
<bodyText confidence="0.703187625">
We define the backward vector βl(g|d) to be
� Pp(Wl k+1,·,Tl k+1,·,Gl k+1,·|wk k−n+2wk+1h−1
βlk+1(g|d) = −mg,d) (25)
l
Gk+1,·
where Wlk+1,· = wlk+2, · · · , (/s) is the subsequence after word wlk+1 in sentence Wl, Tl
k+1,·
is the incremental parse structure after the parse structure Tlk+1 of word (k + 1)-prefix
</bodyText>
<page confidence="0.990333">
643
</page>
<figure confidence="0.281301">
Computational Linguistics Volume 38, Number 3
Wlk+1 that generates parse tree Tl, Tl = Tlk+1||Tlk+1,·, and Glk+1,· = gk+2, · · ·, is the seman-
</figure>
<equation confidence="0.923672526315789">
tic subsequence in Gl relevant to Wlk+1,·. Again it is easy to see that the backward vector
βl(g|d) can be recursively computed in a backward manner as
βl
k+1(g|d) = p(tk+1|wk+1, h−1
−m.tag)p(Tl k,k+1|Wl kTl k,wk+1,tk+1) (26)
�
gk+2∈Gd
p(wk+2|wk k−n+3h−1
−mgk+2, d) p(gk+2|d)
Egi∈Gd p(gi|d)
� Pp(Wlk+2,·,Tlk+2,·,Glk+2,·|wk+1
Glk+2,· k−n+3wk+2h−1
−mgk+2,d)
= p(tk+1|wk+1, h−1
−m.tag)p(Tl k,k+1|Wl kTl k, wk+1, tk+1)
� p(wk+2|wk k−n+3h−1
gk+2∈Gd −mgk+2, d) p(gk+2|d) Egi∈Gd p(gi|d)βlk+2(gk+2|d)
Then, the expected count of w−1
−n+1wh−1
</equation>
<bodyText confidence="0.7383105">
−mg for the WORD-PREDICTOR on sentence Wl
in document d is
</bodyText>
<table confidence="0.570953454545455">
� � Pp(Tl,Gl|Wl,d)#(w−1
Gl Tl∈TlN∈TN −n+1wh−1
−mg, Wl,Tl,Gl,d) (27)
�= E Pp(Tl, Gl, Wl|d)#(w−1
Gl Tl∈TlN∈TN −n+1wh−1
−mg, Wl, Tl, Gl, d)/Pp(Wl|d)
� � αl k+1(g|d)βl k+1(g|d)δ(wk k−n+2wk+1h−1
= −mgk+1 = w−1
l k −n+1wh−1
−mg)/Pp(Wl|d)
where Pp(Wl|d) = EGl � N∈TN Pp(Tl,Wl|d), Pp(Tl, Wl|d)
</table>
<subsubsectionHeader confidence="0.828457">
Tl∈Tl N∈TN Pp(Tl, Gl, Wl|d) = �Tl∈Tl
</subsubsectionHeader>
<bodyText confidence="0.998102666666667">
is recursively computed by Equation (18) through traversing the lth parse tree Tl ∈ TNl
of sentence Wl from left to right, and δ(·) is an indicator function. The expected count
of g for the SEMANTIZER on sentence Wl in document d is
</bodyText>
<equation confidence="0.944782">
� E Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d) (28)
Gl Tl∈TlN∈TN
</equation>
<bodyText confidence="0.9005605">
� � αl k+1(g|d)βl k+1(g|d)p(wk+1|wk k−n+2h−1
= −mg)/Pp(Wl|d)
l k
For the TAGGER and the CONSTRUCTOR, we use Equations (20) and (21), and the
expected count of each event of twh−1 −m.tag and ah−1 −mover parse Tl of sentence Wl in
document d is the real count appearing in parse tree Tl of sentence Wl in document d
times the conditional distribution Pp(Tl|Wl, d) = Pp(Tl,Wl|d)/ ETl∈Tl Pp(Tl, Wl|d)—that
is, Pp(Tl|Wl, d)#(twh−1 −m.tag, Wl, Tl, d) and Pp(Tl|Wl, d)#(ah−1
−m, Wl, Tl, d), respectively.
When only SLM is considered, the expected count for each model component,
WORD-PREDICTOR, TAGGER, and CONSTRUCTOR, over parse Tl of sentence Wl in
document d is the real count that appeared in parse Tl of sentence Wl in document d
</bodyText>
<page confidence="0.978692">
644
</page>
<note confidence="0.499647">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.996681076923077">
times the posterior probability Pp(Tl|Wl, d), as is done in Chelba and Jelinek (1998, 2000)
and Chelba (2000).
In the M-step, the recursive linear interpolation scheme (Jelinek and Mercer 1980)
is used to obtain a smooth probability estimate for each model component (WORD-
PREDICTOR, TAGGER, and CONSTRUCTOR). The TAGGER and CONSTRUCTOR are
conditional probabilistic models of the type p(u|z1, · · · ,zn) where u, z1, · · · ,zn belong to
a mixed set of words, POS tags, NTtags, and CONSTRUCTOR actions (u only); and
z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one
among relative frequency estimates of different orders k = 0, · · · , n and has been ex-
plained in Chelba and Jelinek (1998, 2000) and Chelba (2000). The WORD-PREDICTOR
is, however, a conditional probabilistic model p(w|w−1
−n+1h−1−mg) where there are three
kinds of context, w−1
</bodyText>
<equation confidence="0.701784">
−n+1, h−1
</equation>
<bodyText confidence="0.868729315789474">
−m, and g—each forms a linear Markov chain. The model
has a combinatorial number of relative frequency estimates of different orders among
three linear Markov chains. We generalize Jelinek and Mercer’s (1980) original recur-
sive mixing scheme to handle the situation where the context is a mixture of Markov
chains. The factored language (FL) model (Bilmes and Kirchhoff 2003) is close to the
smoothing technique we propose here, the major difference is that FL considers all
possible combination of the context of conditional probability that can be concisely
represented by a factor graph, whereas our approach strictly respects the order of
Markov chains for word sequence and headword sequence because we believe natural
language tightly follows these orders; moreover, where FL uses a backoff technique,
we use linear interpolation.
Consider a composite trigram/2-SLM/PLSA language model. Figure 3 illustrates
a lattice formed of all possible conditional probabilistic models and relative frequency
Figure 3
Recursive linear interpolation lattice to estimate WORD-PREDICTOR p(w|w−2w−1h−2h−1g) of
the composite trigram/2-SLM/PLSA language model, where U is the vocabulary in which the
predicted random variable w takes values and p(U) denotes uniform distribution of U. The
lattice is formed by three linear Markov chains, w−2w−1, h−2h−1, and g. Starting from p(U),
each vertex is visited in a bottom–up, back to front, and right to left order.
</bodyText>
<page confidence="0.98891">
645
</page>
<note confidence="0.28458">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.995352714285714">
estimates of different orders along each of the three linear Markov chains. Each vertex
in the lattice represents a conditional probabilistic model that is a linear interpolation of
vertices having directed arcs pointing to this vertex and its relative frequency estimate;
the linear interpolation coefficients are the weights of directed arcs. For example, the
WORD-PREDICTOR p(w|w−2w−1h−2h−1g) is a linear interpolation of three conditional
probabilistic models, p(w|w−1h−2h−1g), p(w|w−2w−1h−1g), p(w|w−2w−1h−2h−1), and
their relative frequency estimate f(w|w−2w−1h−2h−1g),
</bodyText>
<equation confidence="0.9999902">
p(w|w−2w−1h−2h−1g) = λw(w−2w−1h−2h−1g) · p(w|w−1h−2h−1g) (29)
+λh(w−2w−1h−2h−1g) · p(w|w−2w−1h−1g)
+λg(w−2w−1h−2h−1g) · p(w|w−2w−1h−2h−1)
+(1 − λw(w−2w−1h−2h−1g) − λh(w−2w−1h−2h−1g)
−λg(w−2w−1h−2h−1g)) · f (w|w−2w−1h−2h−1g)
</equation>
<bodyText confidence="0.9928945">
where λw(w−2w−1h−2h−1g),λh(w−2w−1h−2h−1g), and λg(w−2w−1h−2h−1g) are non-
negative context-dependent interpolation coefficients with a sum of less than 1;
</bodyText>
<equation confidence="0.9920475">
f (w|w−2w−1h−2h−1g) = C(w−2w−1wh−2h−1g)
C(w−2w−1h−2h−1g) ; and C(w−2w−1wh−2h−1g) is the expected
</equation>
<bodyText confidence="0.8733915">
count of the event w−2w−1wh−2h−1g that is extracted from the training cor-
pus by the E-step of the N-best approximate EM algorithm, C(w−2w−1h−2h−1g) =
</bodyText>
<equation confidence="0.822749">
E
</equation>
<bodyText confidence="0.9965899375">
w∈U C(w−2w−2wh−2h−1g). The linear interpolation coefficients are grouped into
equivalence classes (tied) based on the range into which the count falls; the count ranges
for each equivalence class, “buckets,” are set such that a statistically sufficient number
of events fall within that range. In our experiments, we set the count ranges to be the
intervals of 2i, i = 0, 1, · · · ,10 (i.e., 0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, and ∞).
These “tied” interpolation weights are determined by the maximum likelihood estimate
from cross-validation data through the EM algorithm (Dempster, Laird, and Rubin 1977)
where we use a public available parser in the openNLP software1 to parse sentences in
cross-validation data, and we run LSA to extract N most likely topics for each document
in cross-validation data, then we gather joint counts for each model component, WORD-
PREDICTOR, TAGGER, CONSTRUCTOR used to determine interpolation weights.
In the M-step, assuming that the count ranges and the corresponding interpolation
values for each order are kept fixed to their initial values, the only parameters to be
re-estimated using the EM algorithm are the maximal order counts for each model
component. The interpolation scheme outlined here is then used to obtain a smooth
probability estimate for each model component.
</bodyText>
<subsectionHeader confidence="0.987803">
3.2 Follow-up EM
</subsectionHeader>
<bodyText confidence="0.9997946">
As explained in Chelba and Jelinek (2000) and Chelba (2000), for the SLM component
a large fraction of the partial parse trees that can be used for assigning probability to
the next word do not survive in the synchronous, multi-stack search strategy, thus they
are not used in the N-best approximate EM algorithm for the estimation of WORD-
PREDICTOR to improve its predictive power. To remedy this weakness, we estimate a
</bodyText>
<footnote confidence="0.943024">
1 http://www.codeproject.com/KB/recipes/englishparsing.aspx.
</footnote>
<page confidence="0.994315">
646
</page>
<note confidence="0.516647">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.9892215">
separate WORD-PREDICTOR (and SEMANTIZER) model using the partial parse trees
exploited by the synchronous, multi-stack search strategy.
First, we look at how to compute the language model probability assignment for the
word at position k+1 in the input sentence of document d when the word-parse k-prefix
WkTk is available. From the causal relationship among the parameters of the composite
n-gram/m-SLM/PLSA, we have
</bodyText>
<equation confidence="0.994480727272727">
Pp(wk+1|Wk, d) = � Pp(wk+1, Tk, gk+1|Wk, d) (30)
Tk∈Zk,gk+1∈Gd
Tk∈Zk1 ∈Gd Pp(wk+1|Wk, Tk, gk+1, d)Pp(Tk|Wk, d) p(gk+1|d)
Egi∈Gd p(gi|d)
p(wk+1|wk k−n+2h−1
−mgk+1)Pp(Tk|Wk,d) p(gk+1|d)
g;E4d I&apos; &apos; d)
fi
�=
h−m ∈Tk;Tk ∈Zk,gk+1 ∈Gd
c
</equation>
<bodyText confidence="0.996121666666667">
where Pp (Tk  |Wk, d) = ETk∈E CkPGk Pp Pp(Wk,TkTk,GkGk|d) Gk|d) to ensure a proper probability normaliza-
tion over word strings Wk; Zk is the set of all parses present in the stacks at the current
stage k during the synchronous multi-stack pruning strategy and it is a function of
the word k-prefix Wk = w0, · · · , wk; Gk = g1, · · · , gk, gi ∈ Gd, i = 1, · · · , k is the semantic
string up to k; and Pp(Wk, Tk, Gk|d) is the joint probability of word-parse k-prefix WkTk
and its semantic string Gk in a document d.
The likelihood of a training corpus D under this language model probability as-
signment that uses partial parse trees generated during the process of the synchronous,
multi-stack search strategy can be written as
</bodyText>
<equation confidence="0.99341025">
ri ri ( I
˜L(D,p) = l Pp(Wl|d) (31)
d∈D
k1) I Wk, d) and W1 is the lth sentence in document d. Again,
</equation>
<bodyText confidence="0.921496357142857">
where Pp(W1 d) = jIk Pp(w
similar to Equation (8), we ignore the ancillary term p(d) in Equation (31).
We use a second stage of parameter re-estimation for p(wk+1|wkk−n+2h−1
−mgk+1)
and p(gk+1|d) by maximizing Equation (31) to improve WORD-PREDICTOR’s
predictive power. In this case, the estimation of the WORD-PREDICTOR is for
the emission probability of a hidden Markov model with fixed transition probabil-
ities (although dependent on the position k in the input sentence) specified by the
Pp(Tk|Wk, d) p(gk+1|d)
� gi∈Gd p(gi|d) values. We use EM again. The E-step is to gather expected
joint counts C(wk k−n+2wk+1h−1
−mgk+1,d) and C(gk+1,d) of the WORD-PREDICTOR
model by accumulating each count at position k weighted by a posterior probability
Pp(Tk, gk+1|wk+1, Wk, d), namely,
</bodyText>
<equation confidence="0.495404666666667">
Pp(Tk, gk+1|wk+1, Wk, d) = p(wk+1|wkk−n+2h−1
−mgk+1)p(gk+1|d)Pp(Tk|Wk, d)
F-h−1 ∈Tk∈Zk,g∈Gd p(wk+1|wkk−n+2h−mg)p(g|d)Pp(Tk|Wk, d)
</equation>
<bodyText confidence="0.903313">
The M-step uses the same count smoothing technique as that described in the N-best
list approximate EM.
</bodyText>
<page confidence="0.974603">
647
</page>
<note confidence="0.435784">
Computational Linguistics Volume 38, Number 3
</note>
<subsectionHeader confidence="0.983456">
3.3 Distributed Architecture
</subsectionHeader>
<bodyText confidence="0.999952421052631">
When using very large corpora to train our composite language model, the data and the
parameters cannot be stored together on a single machine, so we have to resort to dis-
tributed computing. The topic of large-scale distributed language models is relatively
new, and existing work is restricted to n-grams only (Zhang, Hildebrand, and Vogel
2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007). Although all existing
research use distributed architectures that follow the client–server paradigm, the real
implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store
training corpora in suffix arrays such that one sub-corpus per server serves raw counts,
and test sentences are loaded in a client. This implies that when computing the language
model probability of a sentence in a client, all servers need to be contacted for each
n-gram request. The approach by Brants et al. (2007) follows a standard MapReduce
paradigm (Dean and Ghemawat 2004): The corpus is first divided and loaded into a
number of clients, and n-gram counts are collected at each client, then the n-gram counts
are mapped via hashing and are stored in a number of servers, resulting in exactly one
server being contacted per n-gram when computing the language model probability
of a sentence. We adopt a similar approach to Brants et al. (2007) and make it suitable
to perform iterations of the N-best list approximate EM algorithm (see Figure 4). The
corpus is divided and loaded into a number of clients. We use a publicly available
parser to parse the sentences in each client to get the initial counts for w−1
</bodyText>
<equation confidence="0.926013285714286">
−n+1wh−1
−mg
(WORD-PREDICTOR), twh−1
−m.tag (TAGGER), and ah−1
−m (CONSTRUCTOR), we finish
the Map part, and then the counts for a particular w−1
−n+1wh−1
</equation>
<bodyText confidence="0.894514888888889">
−mg at different clients are
summed up and stored in one of the servers by hashing through word w−1, headword
h−1, and its topic g. The counts for all twh−1 −m.tag and ah−1
−m at different clients are
summed up and stored in one of the servers, then we complete the Reduce part. This
is the initialization of the N-best list approximate EM step. Each client then calls the
servers for parameters to perform a synchronous multi-stack search for each sentence
to get the N-best list parse trees. Again, the expected count for a particular parameter of
w−1
</bodyText>
<equation confidence="0.933481666666667">
−n+1wh−1
−mg, twh−1
−m.tag, and ah−1
</equation>
<bodyText confidence="0.910051">
−m at the clients are computed, thus we finish the Map
part. The expected count of w−1
</bodyText>
<subsubsectionHeader confidence="0.36664">
−n+1wh−1
</subsubsectionHeader>
<bodyText confidence="0.7209895">
−mg are then summed up and stored in one of the
servers by hashing through word w−1, headword h−1, and its topic g, and the counts
for all twh−1
−m.tag and ah−1
−m at different clients are summed up and stored in one of
the servers; thus we finish the Reduce part. The SEMANTIZER has document-specific
</bodyText>
<figureCaption confidence="0.676674">
Figure 4
</figureCaption>
<footnote confidence="0.56096075">
Distributed architecture is essentially a MapReduce paradigm: Clients store partitioned data and
perform the E-step: compute expected counts; this is “Map.” Servers store parameters (counts)
for the M-step where counts of w−1
−n+1wh−1
−mg are hashed by word w−1, headword h−1, and its
topic g to evenly distribute these model parameters into servers as much as possible and counts
of twh−1
−m.tag and ah−1 −mare stored into one server; this is “Reduce.”
</footnote>
<page confidence="0.987808">
648
</page>
<note confidence="0.767269">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.99969875">
parameters, thus the EM iterative updates are performed at each of local clients. We
repeat this procedure until convergence.
Similarly, we use a distributed architecture as in Figure 4 to perform the follow-up
EM algorithm to re-estimate WORD-PREDICTOR.
</bodyText>
<sectionHeader confidence="0.919948" genericHeader="method">
4. Using the Model for Testing
</sectionHeader>
<bodyText confidence="0.998522071428571">
When a language model is used in one-pass decoders of speech recognition and
phrased-based MT systems to guide the search, the search space is organized as a prefix
tree and operates left to right, thus we need to know the language model probability at
the word level given by Equation (30) one word at a time. Because a document of the
test data is not contained in the original training corpus, to compute the language model
probability assignment for word wk+1 we use a “fold-in” heuristic approach similar
to the one used in Hofmann (2001): The parameters corresponding to SEMANTIZER,
p(g|d), are re-estimated by maximizing the probability of word subsequence seen so
far—that is, a pseudo-document ˜dk = (Wk, S), where S is the set of previous sentences of
a document in test data—while holding the other parameters fixed. Wang et al. (2005b)
use on-line gradient ascent to re-estimate these parameters. We use three methods, one-
step on-line EM, on-line EM with fixed learning rate, and batch EM, to re-estimate these
parameters. Both one-step on-line EM and on-line EM with fixed learning rate use
Equation (32) with γ set to 1
</bodyText>
<equation confidence="0.9138114">
 |˜dk|+1 and a constant 0.2, respectively.
� −mg)p(g |˜dk−1)Pp(Tk−1|Wk−1, ˜dk−1)
h−1
−m∈Tk−1;Tk−1∈Zk−1 p(wk|wk−1
k−n+1h−1
� � −mg)p(g |˜dk−1)Pp(Tk−1|Wk−1, ˜dk−1)
g∈Gd h−1
−m∈Tk−1;Tk−1∈Zk−1 p(wk|wk−1
k−n+1h−1
+(1 − γ)p(g|˜dk−1) (32)
The batch EM is the standard EM algorithm where we repeat the iterative procedure
�d∈D #(d) p(g|d)
�gi∈Gd p(gi|d)
until convergence. The initial values are set to , where for the topics
|D|
</equation>
<bodyText confidence="0.999601307692308">
that are purged we just plug in 0 for p(g|d). #(d) is the number of words in document
d,d ∈ D, and |D |= Ed#(d) denotes the size of training corpus (which is the total
number of words in the entire training corpus).
When we use Equation (30) to compute perplexity, the system only uses information
coming from previous words to generate a topic distribution, which then is used to
predict the next word, so the sum over all next words is 1.
We find that the perplexity results are sensitive to these three methods and the initial
values. For example, for batch EM, if we set initial values to be those obtained by using
˜dk−1 = (Wk−1,S) and trained by batch
EM, we obtain worse perplexity results. Table 8 in Section 6.2 gives perplexity results
that use these three methods to re-estimate the parameters of the SEMANTIZER, where
the on-line EM with fixed learning rate not only has the cheapest computational cost
but also leads to the highest perplexity reductions.
</bodyText>
<sectionHeader confidence="0.999224" genericHeader="method">
5. Related Work
</sectionHeader>
<bodyText confidence="0.981954">
Besides the work by Wang et al. (2005b, 2006) that was discussed in the Introduction,
the closest work to ours is that by Khudanpur and Wu (2000) where the authors used
</bodyText>
<equation confidence="0.6287705">
p(g|˜dk) = γ
the pseudo-document up to the previous word
</equation>
<page confidence="0.927068">
649
</page>
<note confidence="0.453938">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.999694739130435">
SLM and a word clustering model to extract relevant grammatical and semantic fea-
tures, then integrated these features with n-grams by a maximum conditional entropy
approach. Our composite language model is a generative model, all features play impor-
tant roles in the EM iterations to allow maximal order events for WORD-PREDICTOR to
appear; in Khudanpur and Wu (2000), however, the counts for all events are fixed after
feature extraction from SLM and word clustering and no new maximal order events
for WORD-PREDICTOR are possibly extracted, this potentially hinders the predictive
power of WORD-PREDICTOR. Moreover, the training algorithm in Khudanpur and Wu
is computationally expensive. Both methods use the first-stage N-best list approximate
EM to extract headwords, thus the complexity is at the same order at this stage; at
second stage, however, where we use the follow-up EM, they use the maximum en-
tropy approach. The maximum entropy approach is more expensive, mainly in feature
expectation and normalization as well as optimization (such as iterative scaling or the
quasi Newton method); ours is quite simple, which is expected relative to frequency
estimates with proper smoothing.
The highest reported perplexity reductions are those by Goodman (2001), where the
author examines the techniques of caching, clustering, higher-order n-grams, skipping
models, and sentence-mixture models in various combinations (mainly linear interpola-
tion). The author compares to the baseline of a Katz smoothed trigram with no count cutoffs.
On a small training corpus with 100k tokens, a 50% perplexity reduction (1 bit improve-
ment) is obtained. On a larger corpus with 284 million tokens without punctuation,
the improvement declines to 38%; we assume that this improvement shrinks to 30%
when compared with 4-gram as the baseline.
</bodyText>
<sectionHeader confidence="0.996419" genericHeader="method">
6. Experimental Results
</sectionHeader>
<bodyText confidence="0.99993825">
In this section, we first explain the experimental set-up for our experiments, we then
show comprehensive perplexity results in various situations, and we end by reporting
the results when we apply the composite language model to the task of re-ranking the
N-best list from a state-of-the-art parsing-based machine translation system.
</bodyText>
<subsectionHeader confidence="0.928228">
6.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999978058823529">
In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba
and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been
trained on relatively small data sets. There is the impression that complex language
models only lead to better results than n-grams on small training corpora. For example,
Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can
take advantage of longer-distance information than n-grams, which suggests that they
might do a better job at language modeling/word prediction. It turns out that if we
have a very large amount of training data, a 4-gram or 5-gram is nonetheless still the
best way to do language modeling.” To verify whether this is true, we have trained our
language models using three different training sets: one has 44 million tokens, another
has 230 million tokens, and the third has 1.3 billion tokens. An independent test set
with 354k tokens is chosen. The independent check data set used to determine the
linear interpolation coefficients has 1.7 million tokens for the 44 million token training
corpus, and 13.7 million tokens for both the 230 million and 1.3 billion token training
corpora. All these data sets are taken from the LDC English Gigaword corpus with non-
verbalized punctuation and we remove all punctuation. Table 1 provides the detailed in-
formation on how these data sets were chosen from the LDC English Gigaword corpus.
</bodyText>
<page confidence="0.996117">
650
</page>
<note confidence="0.969421">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<tableCaption confidence="0.999346">
Table 1
</tableCaption>
<table confidence="0.961880684210526">
The corpora used in our experiments.
1.3 BILLION TOKEN TRAINING CORPUS
AFP 19940512.0003 - 19961015.0568
AFW 19941111.0001 - 19960414.0652
NYT 19940701.0001 - 19950131.0483
NYT 19950401.0001 - 20040909.0063
XIN 19970901.0001 - 20041125.0119
230 MILLION TOKEN TRAINING CORPUS
AFP 19940622.0336 - 19961031.0797
APW 19941111.0001 - 19960419.0765
NYT 19940701.0001 - 19941130.0405
44 MILLION TOKEN TRAINING CORPUS
AFP 19940601.0001 - 19950721.0137
13.7 MILLION TOKEN CHECK CORPUS
NYT 19950201.0001 - 19950331.0494
1.7 MILLION TOKEN CHECK CORPUS
AFP 19940512.0003 - 19940531.0197
354K TOKEN TEST CORPUS
CNA 20041101.0006 - 20041217.0009
</table>
<bodyText confidence="0.8230868">
These are selected from the LDC English Gigaword corpus. AFP = Agence France-Presse;
AFW = Associated Press Worldstream; NYT = New York Times; XIN = Xinhua News Agency;
and CNA = Central News Agency of Taiwan denote the sections of the LDC English Gigaword
corpus.
The vocabulary sizes in all three cases are:
</bodyText>
<listItem confidence="0.999872857142857">
• word (also WORD-PREDICTOR operation) vocabulary: 60k, open—
all words outside the vocabulary are mapped to the (unk) token, these
60k words are chosen from the most frequently occurring words in the
44 million token corpus;
• POS tag (also TAGGER operation) vocabulary: 69, closed;
• non-terminal tag vocabulary: 54, closed;
• CONSTRUCTOR operation vocabulary: 157, closed.
</listItem>
<bodyText confidence="0.954552666666667">
The out-of-vocabulary (OOV) rate on the 44 million, 230 million, 1.3 billion token
training corpora is 0.6%, 0.9%, and 1.2%, respectively. The OOV rate on the 1.7 million
and 13.7 million token check corpora is 0.6% and 1.3%, respectively. The OOV rate on
</bodyText>
<page confidence="0.990575">
651
</page>
<table confidence="0.577044">
Computational Linguistics Volume 38, Number 3
</table>
<tableCaption confidence="0.989824">
Table 2
</tableCaption>
<table confidence="0.848769166666667">
Statistics about the number of types of n-grams (n = 3, 4, 5) on the 44 million, 230 million, and
1.3 billion token corpora.
n=3 n=4 n=5
44 M 14,302,355 23,833,023 29,068,173
230 M 51,115,539 94,617,433 120,978,281
1.3 B 224,767,319 481,645,099 660,599,586
</table>
<bodyText confidence="0.998032875">
the 354k token test corpus is 2.0%. Table 2 lists the statistics about the number of types
of n-grams on these three corpora.
Similar to SLM (Chelba 2000; Chelba and Jelinek 2000), after the parse under-
goes headword percolation and binarization, each model component of WORD-
PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed
sentences. We use the openNLP software2 to parse a large number of sentences in the
LDC English Gigaword corpus to generate an automatic treebank, which has a slightly
different word-tokenization than that of the manual treebank such as the Penn Treebank
used in Chelba and Jelinek (2000) and Chelba (2000). For the 44 and 230 million token
corpora, all sentences are automatically parsed and used to initialize model parameters,
whereas for the 1.3 billion token corpus, we parse the sentences from a portion of the
corpus that contains 230 million tokens, then use them to initialize model parameters.
The parser at openNLP is trained on the Penn Treebank, which has only one million
tokens, and there is a mismatch between the Penn Treebank and the LDC English
Gigaword corpus. Nevertheless, experimental results show that this approach is effec-
tive to provide initial values of model parameters.
</bodyText>
<subsectionHeader confidence="0.998832">
6.2 Perplexity Results
</subsectionHeader>
<bodyText confidence="0.999975166666667">
Table 3 gives the perplexity results (Bahl et al. 1977) of n-grams (n = 3, 4, and 5) using
linear interpolation and Kneser-Ney (1995) smoothing when the training corpus has
44 million, 230 million, and 1.3 billion tokens, respectively. We have implemented a
distributed n-gram with linear interpolation smoothing, but we don’t have distributed
n-grams with Kneser-Ney smoothing implemented by us. Instead, we use the SRI
Language Modeling Toolkit to obtain perplexity results of n-grams with Kneser-Ney
smoothing for the 44 million and 230 million token corpora using a single machine that
has 20G memory at the Ohio Supercomputer center. We are not able to compute per-
plexity results of n-grams with Kneser-Ney smoothing on the 1.3 billion token corpus,
thus we leave these results blank in Table 3. From the results in Table 3, we decided to
use a linearly smoothed trigram as the baseline model for the 44 million token corpus,
a linearly smoothed 4-gram as the baseline model for the 230 million token corpus, and
a linearly smoothed 5-gram as the baseline model for the 1.3 billion token corpus.
As we mentioned in Section 3.1.1, we can keep only a small set of topics due to
the considerations of computational time and resource demand. Table 4 shows the
perplexity results and computation time of composite n-gram/PLSA language models
that are trained on the three corpora when the pre-defined number of total topics is 200,
but different numbers of most-likely topics are kept for each document in PLSA; the
</bodyText>
<footnote confidence="0.895561">
2 http://www.codeproject.com/KB/recipes/englishparsing.aspx.
</footnote>
<page confidence="0.984572">
652
</page>
<note confidence="0.970103">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<tableCaption confidence="0.99897">
Table 3
</tableCaption>
<table confidence="0.931058785714286">
Perplexity results of n-grams (n = 3, 4, and 5) using linear interpolation and Kneser-Ney
smoothing when training set is a 44 million, 230 million, or 1.3 billion token corpus, respectively.
44 M LINEAR KNESER-NEY
n=3 262 244
n=4 258 235
n=5 260 235
230 M LINEAR KNESER-NEY
n=3 217 195
n=4 200 183
n=5 201 183
1.3 B LINEAR KNESER-NEY
n=3 161 —
n=4 141 —
n=5 138 —
</table>
<tableCaption confidence="0.999419">
Table 4
</tableCaption>
<figureCaption confidence="0.441935333333333">
Perplexity (ppl) results and time consumed of the composite n-gram/PLSA language model
trained on three corpora when different numbers of most-likely topics are kept for each
document in PLSA.
</figureCaption>
<figure confidence="0.989202">
CORPUS n # OF PPL TIME # OF # OF # OF TYPES
TOPICS (HOURS) SERVERS CLIENTS OF ww−1
−n+1g
44M 3 5 196 0.5 40 100 120.1M
3 10 194 1.0 40 100 218.6M
3 20 190 2.7 80 100 537.8M
3 50 189 6.3 80 100 1.123B
3 100 189 11.2 80 100 1.616B
3 200 188 19.3 80 100 2.280B
230M 4 5 146 25.6 280 100 0.681B
1.3B 5 2 111 26.5 400 100 1.790B
5 5 102 75.0 400 100 4.391B
</figure>
<bodyText confidence="0.999783416666667">
rest are pruned. For the composite 5-gram/PLSA model trained on the 1.3 billion token
corpus, 400 cores have to be used to keep the top five most likely topics. For the
composite trigram/PLSA model trained on the 44M token corpus, the computation
time increases drastically, with less than 5% percent perplexity improvement. In the
following experiments, therefore, we keep the top five topics for each document from
a total of 200 topics—all other 195 topics are pruned.
All composite language models are first trained by performing the N-best list
approximate EM algorithm until convergence, then the EM algorithm for a second
stage of parameter re-estimation for WORD-PREDICTOR and SEMANTIZER until
convergence. We fix the size of topics in the PLSA to be 200 and then prune to 5 in
the experiments, where the unpruned 5 topics in general account for 70% probability
in p(g1d). Table 5 shows comprehensive perplexity results for a variety of different
</bodyText>
<page confidence="0.85412">
653
654
</page>
<table confidence="0.450906">
Computational Linguistics Volume 38, Number 3
</table>
<tableCaption confidence="0.997499">
Table 5
</tableCaption>
<table confidence="0.955376263157895">
Perplexity results for various language models on test corpora, where + denotes linear combination, / denotes composite model; n denotes the order
of the n-gram, and m denotes the order of the SLM; the topic nodes are pruned from 200 to 5.
44M REDUCTION 230M REDUCTION 1.3B REDUCTION
n=3,m=2 n=4,m=3 n=5,m=4
262 200 138
244 6.9% 183 8.5% — —
279 −6.5% 190 5.0% 137 0.0%
825 −214.9% 812 −306.0% 773 −460.0%
247 5.7% 184 8.0% 129 6.5%
235 10.3% 179 10.5% 128 7.2%
222 15.3% 175 12.5% 123 10.9%
243 7.3% 171 14.5% (125) 9.4%
196 25.2% 146 27.0% 102 26.1%
198 24.4% 140 30.0% (103) 25.4%
183 30.2% 140 30.0% (93) 32.6%
183 30.2% 139 30.5% (94) 31.9%
184 29.8% 137 31.5% (91) 34.1%
180 31.3% 130 35.0% — —
176 32.8% — — — —
</table>
<figure confidence="0.9773164375">
LANGUAGE MODEL
BASELINE n-GRAM (LINEAR)
n-GRAM (KNESER-NEY)
m-SLM
PLSA
n-GRAM+m-SLM
n-GRAM+PLSA
n-GRAM+m-SLM+PLSA
n-GRAM/m-SLM
n-GRAM/PLSA
m-SLM/PLSA
n-GRAM/PLSA+m-SLM/PLSA
n-GRAM/m-SLM+m-SLM/PLSA
n-GRAM/m-SLM+n-GRAM/PLSA
n-GRAM/m-SLM+n-GRAM/PLSA+m-SLM/PLSA
n-GRAM/m-SLM/PLSA
</figure>
<bodyText confidence="0.984057184210526">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
models such as composite n-gram/m-SLM, n-gram/PLSA, m-SLM/PLSA, their linear
combinations, and so on, where we use on-line EM with a fixed learning rate to re-
estimate the parameters of the SEMANTIZER of test document. The m-SLM performs
competitively with its counterpart n-gram (n = m+1) on large scale corpus. Table 6
lists the statistics about the number of types in the predictor of the m-SLMs on these
three corpora, where for the 230 million token and 1.3 billion token corpora we cut
off the fractional expected counts that are less than a predefined threshold of 0.005, to
significantly reduce the number of the predictor’s types by 70%.
In Table 5, for the composite n-gram/m-SLM model (n = 3, m = 2 and n = 4, m = 3)
trained on 44 million tokens and 230 million tokens, we cut off its fractional expected
counts that are less than a threshold 0.005; this significantly reduces the number of the
predictor’s types by 85%. When we train the composite language on the 1.3 billion token
corpus, we have to both aggressively prune the parameters of WORD-PREDICTOR and
shrink the order of n-gram and m-SLM in order to store them in a supercomputer having
1,000 cores. In particular, for the composite 5-gram/4-SLM model, its size is too big
to store, thus we use its approximation, a linear combination of 5-gram/2-SLM and
2-gram/4-SLM. For the 5-gram/2-SLM or 2-gram/4-SLM, again we cut off its fractional
expected counts that are less than a threshold 0.005, which significantly reduces the
number of the predictor’s types by 85%. For the composite 4-SLM/PLSA model, we cut
off its fractional expected counts that are less than a threshold 0.002, again this signifi-
cantly reduces the number of predictor’s types by 85%. For the composite 4-SLM/PLSA
model or its linear combination with models, we ignore all the tags and use only the
words in the four headwords. We have checked that the conditional language model
(Equation [30]) sums to 1 for large randomly selected conditional events. The compos-
ite n-gram/m-SLM/PLSA model gives significant perplexity reductions over baseline
n-grams (n = 3, 4, 5) and m-SLMs (m = 2, 3, 4). The majority of gains comes from the
PLSA component, but when adding the SLM component into the n-gram/PLSA, there
is a further 10% relative perplexity reduction.
Table 7 shows how large the composite 5-gram/PLSA, 5-gram/2-SLM (or
2-gram/4-SLM), and 4-SLM/PLSA models are when trained by the 1.3 billion token
corpus after aggressive pruning. The total minimum number of servers used to store
the parameters of the predictor for the composite 5-gram/PLSA, 5-gram/2-SLM (or
2-gram/4-SLM), and 4-SLM/PLSA models is, respectively, 400, 240, 400, and the num-
ber of clients to store the partitioned data of the 1.3 billion token corpus is 100 for these
three composite language models. There is no way to store the parameters of the linear
combination of the composite 5-gram/PLSA, 5-gram/2-SLM (or 2-gram/4-SLM), and
4-SLM/PLSA models in our currently available supercomputer resources.
</bodyText>
<tableCaption confidence="0.727159">
Table 6
</tableCaption>
<bodyText confidence="0.96388075">
Statistics about the number of types in the predictor of the m-SLMs (m = 2, 3, 4) on the
44 million, 230 million, and 1.3 billion token corpora. For the 230 million and 1.3 billion token
corpora, fractional expected counts that are less than a threshold are pruned to significantly
reduce the number of m-SLM (m=3, 4) predictor’s types by 70%.
</bodyText>
<table confidence="0.673586833333333">
m=2 m=3 m=4
44 M 189,002,525 269,685,833 318,174,025
230 M 267,507,672 1,154,020,346 1,417,977,184
1.3 B 946,683,807 1,342,323,444 1,849,882,215
655
Computational Linguistics Volume 38, Number 3
</table>
<tableCaption confidence="0.994771">
Table 7
</tableCaption>
<bodyText confidence="0.96861275">
Counts of the types in the predictor of the 5-gram/PLSA, 5-gram/2-SLM (or 2-gram/4-SLM),
and 4-SLM/PLSA models when trained on the 1.3B corpus. Fractional expected counts that are
less than a threshold are pruned; this significantly reduces the number of predictor’s types
by 85%.
</bodyText>
<equation confidence="0.980446545454545">
COMPOSITE TYPES # OF # OF # OF
MODEL OF TYPES SERVERS CLIENTS
5-GRAM/PLSA w−1
−4wg 4.39 B 400 100
5-GRAM/2-SLM w−1
−4wh−1
−2 2.01B 240 100
2-GRAM/4-SLM w−1wh−1
−4
4-SLM/PLSA wh−1
−4g 4.88 B 400 100
</equation>
<bodyText confidence="0.999959305555556">
Appendix A shows an example of sentence probability that is provided by 5-gram,
5-gram/PLSA, and 5-gram/4-SLM+5-gram/PLSA models, respectively; these language
models are trained using the 1.3 billion tokens corpus. The example demonstrates that
our composite model is able to extract topic information and grammatical structure to
improve word prediction for natural language.
Table 8 shows the perplexity results for composite n-gram/PLSA and n-gram/
m-SLM/PLSA language models when three methods are used to re-estimate the pa-
rameters of the SEMANTIZER of test document; we use superscript 1, 2, and 3 to
denote that during testing we used one step on-line EM, on-line EM with fixed learning
rate, and batch EM, respectively. The on-line EM with fixed learning rate gives the
best perplexity results as well as the least computation time. Again, when we train
the composite language on the 1.3 billion token corpus, we have to shrink the order
of the n-gram and m-SLM in order to store them in a supercomputer having 1,000 cores.
For the composite 4-SLM/PLSA model or its linear combination with models, we ignore
all the tags and use only the words in the four headwords. For the composite 5-gram/
4-SLM model or its linear combination with models, we in fact use its approximation,
a linear combination of the 5-gram/2-SLM and 2-gram/4-SLM models.
To better explain and analyze our model, we mark the perplexity results for the
40 million token corpus in Table 5 on the vertices in Figure 3 to reveal many insights.
The baseline trigram result is given by the vertex p(w|w−2w−1), the 2-SLM result is
given by the vertex p(w|h−2h−1), the PLSA result is given by the vertex p(w|g), the
trigram/2-SLM result is given by the vertex p(w|w−2w−1h−2h−1), the trigram/PLSA
result is given by the vertex p(w|w−2w−1g), and the trigram/2-SLM/PLSA is given by
the vertex p(w|w−2w−1h−2h−1g). The trigram+2-SLM result is given by a linear combi-
nation of vertices p(w|w−2w−1) and p(w|h−2h−1); the trigram+PLSA result is given by a
linear combination of vertices p(w|w−2w−1) and p(w|g); and the trigram+2-SLM+PLSA
result is given by a linear combination of vertices p(w|w−2w−1), p(w|h−2h−1), and
p(w|g). The trigram/PLSA+2-SLM/PLSA result is given by a linear combination of
vertices p(w|w−2w−1g) and p(w|h−2h−1g), and so on. The trigram/PLSA+trigram/
2-SLM+2-SLM/PLSA result is given by a linear combination of vertices p(w|w−2w−1g),
p(w|w−2w−1h−2h−1g), and p(w|h−2h−1g). The composite trigram/2-SLM/PLSA lan-
guage model is more powerful and expressive than the linear combination of tri-
gram, 2-SLM, and PLSA for two reasons. First, valuable relative frequency estimates
such as f(w|w−2w−1h−2h−1g),f(w|w−2w−1h−2h−1), and so forth, are encoded into the
composite language model, as seen from Figure 3. As long as there are events such
as w−2w−1wh−2h−1g, and so on, that occur explicitly or implicitly in the training
</bodyText>
<page confidence="0.998224">
656
</page>
<note confidence="0.878624">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<tableCaption confidence="0.72801325">
Table 8
Perplexity results for the composite n-gram/PLSA and n-gram/m-SLM/PLSA language models on the test corpus, where + denotes linear
combination, / denotes composite model; n is the order of the n-gram and m is the order of the SLM, and superscripts 1, 2, 3 denote using one-step
on-line EM, on-line EM with fixed learning rate, and batch EM during testing, respectively.
</tableCaption>
<table confidence="0.990497555555555">
44M REDUCTION 230M REDUCTION 1.3B REDUCTION
n=3,m=2 n=4,m=3 n=5,m=4
262 200 138
202 22.9% 150 25.0% 107 22.5%
192 26.7% 142 29.0% (97) 29.1%
196 25.2% 146 27.0% 102 26.1%
184 29.8% 137 31.5% 34.1%
201 23.3% 148 26.0% 104 24.6%
189 27.9% 140 30.0% 33.3%
</table>
<figure confidence="0.9599558">
LANGUAGE MODEL
n-GRAM (LINEAR)
n-GRAM/PLSA1
n-GRAM/m-SLM+n-GRAM/PLSA1
n-GRAM/PLSA2
n-GRAM/m-SLM+n-GRAM/PLSA2
n-GRAM/PLSA3
n-GRAM/m-SLM+n-GRAM/PLSA3
657
Computational Linguistics Volume 38, Number 3
</figure>
<bodyText confidence="0.99964588">
corpus, the composite trigram/2-SLM/PLSA will take them into account to improve
the prediction power for test data, whereas a linear combination of trigram, 2-SLM,
and PLSA just neglects a large amount of this valuable information. The second rea-
son is that the weights used in a simple linear combination are context-independent,
thus more restricted. Similarly, the composite trigram/2-SLM/PLSA language model
is more powerful and expressive than a linear combination of pairwise composite
language models (e.g., trigram/2-SLM, trigram/PLSA, and 2-SLM/PLSA), since the
composite trigram/2-SLM/PLSA can take advantage of the relative frequency estimate
f(w|w−2w−1h−2h−1g),f(w|w−2w−1h−1g), and f(w|w−1h−2h−1g). The improvement in
this case shrinks, however, because pairwise composite language models use some valu-
able lower order relative frequency estimates such as f (w|w−2w−1g), and so forth. Stated
another way, each vertex of the lattice in Figure 3 is an expert of WORD-PREDICTOR
that is proficient in making a prediction based on the context represented at the vertex; it
predicts words based on the information provided by a committee consisting of experts
from parent vertices as well as the relative frequency estimate it extracts. These experts
are hierarchically organized, with the WORD-PREDICTOR of the composite trigram/
2-SLM/PLSA (i.e., p(w|w−2w−1h−2h−1g)) overseeing all available information to make
the most powerful prediction.
Finally, we conducted experiments where we fixed the size of the training data
and increased the complexity of our language models. Because available resources are
limited, preventing us from considering complex language models that are trained on
the 1.3 billion token corpus, we considered complex language models trained on the
44 million token corpus instead. Table 9 shows the perplexity results. We can see that as
we increase the order for n-gram and m-SLM from n = 3 and m = 2 to n = 4 and m = 3,
the composite language models become better and have up to 5% perplexity reductions;
when we increase the order for n-gram and m-SLM to n = 5 and m = 4, however, the
composite language models become worse and slightly overfit the data even if we use
linear interpolation smoothing, and there are no further perplexity reductions.
To summarize, as a sub-problem for MT and speech recognition under the source-
channel paradigm (Jelinek 2009), language modeling is a data-rich and feature-rich
density estimation problem with Kullback-Leibler divergence as a cost function, and
there is always a trade-off between approximation error and estimation error (Barron
and Sheu 1991), reminiscent of the “bias-variance” trade-off for a regression problem
with a quadratic cost function (Hastie, Tibshirani, and Friedman 2009). Figure 5 explains
the perplexity results in Tables 3 and 5 from a model selection point of view.
Let pˆ denote the true (but unknown) distribution of natural language, its infor-
mation projection to n-grams is the minimum Kullback-Leibler divergence from pˆ to
n-grams (Amari and Nagaoka 2000; Wang, Greiner, and Wang 2009) and is denoted as
ˆpn, n = 3, 4, 5, 6. Let p˜ denote the empirical distribution of natural language—in partic-
ular, ˜pM denotes the empirical distribution for a million token corpus, ˜pB denotes the
empirical distribution for a billion token corpus, and ˜pT denotes the empirical distri-
bution for a trillion token corpus. The information projection of ˜pM to trigram is p3M, to
4-gram is p4M, and to 5-gram is p5M. The distance between pˆ and ˆpn(n = 3, 4, 5, 6), D( ˆp, ˆpn),
is the approximation error when using n-gram to represent ˆp, that is, the best the n-gram
can do when abundant data are available. The distance between ˜pnM and ˆpn, n = 3, 4, 5,
D( ˜pnM, ˆpn), is the estimation error when only the million token corpus is available. The
Pythagorean theorem states that the distance between pˆ and ˜pM, D( ˆp, ˜pM), is the sum
of the approximation error and the estimation error (Barron and Sheu 1991; Amari and
Nagaoka 2000; Wang, Greiner, and Wang 2009). In language modeling research, because
pˆ is unknown, the distance between pˆ and pnM, n = 3,4 is approximately computed
</bodyText>
<page confidence="0.998744">
658
</page>
<tableCaption confidence="0.993988">
Table 9
</tableCaption>
<table confidence="0.988278947368421">
Perplexity results for various language models on test corpora, where + denotes linear combination, / denotes composite model; n denotes the order
of the n-gram, and m denotes the order of the SLM; the topic nodes are pruned from 200 to 5.
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
44M REDUCTION 44M REDUCTION 44M REDUCTION
n=3,m=2 n=4,m=3 n=5,m=4
262 258 260
244 6.9% 235 8.9% 235 9.6%
279 −6.5% 254 1.6% 254 2.3%
247 5.7% 233 9.7% 234 10.0%
235 10.3% 230 10.9% 231 11.2%
222 15.3% 220 14.7% 221 15.0%
243 7.3% 232 10.1% 235 9.6%
196 25.2% 189 26.7% 193 25.8%
198 24.4% 190 26.4% 192 26.2%
183 30.2% 179 30.6% 178 31.5%
183 30.2% 178 31.0% 180 30.8%
184 29.8% 176 31.8% 178 31.5%
180 31.3% 173 33.0% 173 33.5%
176 32.8% 169 34.5% 171 34.2%
</table>
<figure confidence="0.979162235294118">
LANGUAGE MODEL
BASELINE n-GRAM (LINEAR)
n-GRAM (KNESER-NEY)
m-SLM
n-GRAM+m-SLM
n-GRAM+PLSA
n-GRAM+m-SLM+PLSA
n-GRAM/m-SLM
n-GRAM/PLSA
m-SLM/PLSA
n-GRAM/PLSA+m-SLM/PLSA
n-GRAM/m-SLM+m-SLM/PLSA
n-GRAM/m-SLM+n-GRAM/PLSA
n-GRAM/m-SLM+n-GRAM/PLSA+m-SLM/PLSA
n-GRAM/m-SLM/PLSA
659
Computational Linguistics Volume 38, Number 3
</figure>
<figureCaption confidence="0.989121">
Figure 5
</figureCaption>
<bodyText confidence="0.9997965">
Language modeling is a data-rich and feature-rich density estimation problem. The information
projection from true distribution and empirical distribution to n-grams is unique, and the
information projection from true distribution and empirical distribution to composite language
models might be local optimal. There is a trade-off between approximation error and estimation
error for composite language models.
by the perplexity result using test data. By the Glivenko-Cantelli theorem (Vapnik
1998), we know that the empirical distribution p˜ converges to the true distribution ˆp;
similarly, the information projection of empirical distribution on an n-gram converges
to the information projection on an n-gram of true distribution (i.e., the estimation error
shrinks to 0). In the same vein, we can define the information projection of pˆ or p˜ to the
composite language models and the corresponding approximate error and estimation
error, and so forth. In this case, the Pythagorean theorem breaks down due to the non-
convexity of the set of composite language models. As noted by Dr. Ciprian Chelba
in our private communication on March 20th, 2010, “When playing with large data,
the model capacity is an important factor to language model performance: The supply
of more data needs to be matched by demand on the model side. A simple way to
achieve this in n-grams is to increase the order n as much as the data will allow. This
of course implies that the computational aspects of storing and serving such models
are solved and that it is not a constraint” (see also Chelba et al. 2010). This is also
true for our composite language models as justified from the results in Tables 5 and
9: The composite n-gram/m-SLM/PLSA language model has rich features, thus has
smaller approximation error than the n-gram, m-SLM, PLSA, or any composite model
of two, or their linear combinations. Table 5 shows that the information projection of
the empirical distribution for the million and billion token corpora, ˜pM and ˜pB on the
composite n-gram/m-SLM/PLSA language model, is closer to the true distribution ˆp.
This is reflected approximately by the perplexity results on test data.
</bodyText>
<subsectionHeader confidence="0.999517">
6.3 Re-ranking Machine Translation Results
</subsectionHeader>
<bodyText confidence="0.997403">
We have applied our composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA1 lan-
guage model that is trained by a 1.3 billion word corpus for the task of re-ranking the
N-best list in statistical MT. We used the same two 1,000-best lists that were used
</bodyText>
<page confidence="0.99649">
660
</page>
<note confidence="0.982678">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<tableCaption confidence="0.997427">
Table 10
</tableCaption>
<table confidence="0.900658875">
10-fold cross-validation Bleu score results for the task of re-ranking the 1,000-best list generated
on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set.
SYSTEM MODEL MEAN (%) 95% CI (%)
BASELINE 31.75 0.22
5-GRAM 32.53 0.24
5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 0.24
5-GRAM/PLSA1 33.01 0.24
5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 33.32 0.25
</table>
<bodyText confidence="0.999314">
by Zhang and colleagues (Zhang, Hildebrand, and Vogel 2006; Zhang 2008; Zhang
et al. 2011). The first list was generated on 919 sentences of 100 documents from
the MT03 Chinese–English evaluation set, and the second was generated on 191 sen-
tences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero
(Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a
trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and
Martin 2008) on a 200 million token corpus. Each translation has 11 features and
language model is one of them. We substitute our language model and use MERT
(Och 2003) to optimize the Bleu score (Papineni et al. 2002). We conduct two ex-
periments on these two data sets. In the first experiment, we partition the first data
set that consists of 100 documents into ten pieces; each piece consists of 10 docu-
ments, nine pieces are used as training data to optimize the Bleu score (Papineni
et al. 2002) by MERT (Och 2003), and the remaining single piece is used to re-rank
the 1,000-best list and obtain the Bleu score. The cross-validation process is then
repeated 10 times (the folds), with each of the 10 pieces used exactly once as the
validation data. The 10 results from the folds then can be averaged (or otherwise
combined) to produce a single estimation for Bleu score. The mean and variance of
the Bleu score are calculated with each different LM. We assume that the score follows
Student’s t-distribution and we compute the 95% confidence interval according to
mean and variance. Table 10 shows the Bleu scores through 10-fold cross-validation.
The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA1 language model gives
1.57 percentage point Bleu score improvement over the baseline and 0.79 percentage
point Bleu score improvement over the 5-gram. We are not able to further improve
Bleu score when we use either the 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA2
or 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA3. This is because there is not much
diversity on the 1,000-best list, and essentially only 20 ∼ 30 distinct sentences are in the
1,000-best list.
In the second experiment, we used the first data set as training data to optimize the
Bleu score by MERT, then the second data set is used to re-rank the 1,000-best list and
obtain the Bleu score. To obtain the confidence interval of the Bleu score, we resort to
the bootstrap resampling described by Koehn (2004). We randomly select 10 re-ranked
documents from the 20 re-ranked documents in the second data set with replacement.
We draw the translation results of the 10 documents and compute the Bleu score. We
repeat this procedure 1,000 times. When we compute the 95% confidence interval, we
drop the top 25 and bottom 25 Bleu scores, and only consider the range of 26th to 975th
Bleu scores. Table 11 shows the Bleu scores. These statistics are computed with different
language models, but on the same chosen test sets. The 5-gram gives 0.51 percent-
age point Bleu score improvement over the baseline. The composite 5-gram/2-SLM+
</bodyText>
<page confidence="0.994656">
661
</page>
<note confidence="0.448922">
Computational Linguistics Volume 38, Number 3
</note>
<tableCaption confidence="0.992347">
Table 11
</tableCaption>
<table confidence="0.937189125">
Bleu score results for the task of re-ranking the 1,000-best list generated on 191 sentences of 20
documents from the MT04 Chinese–English evaluation set.
SYSTEM MODEL MEAN (%) 95% CI (%)
BASELINE 27.59 0.31
5-GRAM 28.10 0.32
5-GRAM/2-SLM+2-GRAM/4-SLM 28.34 0.32
5-GRAM/PLSA1 28.53 0.31
5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 28.78 0.31
</table>
<bodyText confidence="0.999331">
2-gram/4-SLM+5-gram/PLSA1 language model gives 1.19 percentage point Bleu score
improvement over the baseline and 0.68 percentage point Bleu score improvement over
the 5-gram.
Chiang (2007) studied the performance of machine translation on Hiero, the Bleu
score is 33.31% when n-gram is used to re-rank the N-best list; the Bleu score becomes
significantly higher (37.09%) when the n-gram is embedded directly into Hiero’s one
pass decoder, however. This is because there is not much diversity in the N-best list. It
is expected that putting our composite language into a one-pass decoder should result
in much improved Bleu scores.
Besides reporting the Bleu scores, we look at the “readability” of translations,
similar to the study conducted by Charniak, Knight, and Yamada (2003). The trans-
lations are sorted into four groups: good/bad syntax crossed with good/bad mean-
ing by human judges (see Table 12). We find that many more sentences are perfect,
many more are grammatically correct, and many more are semantically correct. The
syntactic language model (Charniak et al. 2003) only improves translations to have
good grammar, but does not improve translations to preserve meaning. The composite
5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA1 language model improves both signif-
icantly. Bear in mind that Charniak et al. (2003) integrated Charniak’s language model
with the syntax-based translation model proposed by Yamada and Knight (2001) to
rescore a tree-to-string translation forest, whereas we use only our language model
for N-best list re-ranking. Also, the same study (Charniak et al. 2003) found that the
outputs produced using the n-grams received higher scores from Bleu; ours did not. The
difference between human judgments and Bleu scores indicates that closer agreement
may be possible by incorporating syntactic structure and semantic information into the
Bleu score evaluation. For example, semantically similar words like insure and ensure as
in Bleu paper (Papineni et al. 2002) should be substituted in the formula, and there is a
weight to measure the goodness of syntactic structure. This modification will lead to a
better metric and such information can be provided by our composite language models.
</bodyText>
<tableCaption confidence="0.995065">
Table 12
</tableCaption>
<table confidence="0.917546333333333">
Results of “readability” evaluation on 919 translated sentences of 100 documents. P = perfect;
S = only semantically correct; G = only grammatically correct; W = wrong.
SYSTEM MODEL P S G W
BASELINE 95 398 20 406
5-GRAM 122 406 24 367
5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 151 425 33 310
</table>
<page confidence="0.993254">
662
</page>
<note confidence="0.834143">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<bodyText confidence="0.984700319148936">
In Appendix B, we give examples of “perfect” sentences, “only semantically
correct” sentences, and “only grammatically correct” sentences.
7. Conclusion and Future Work
We have built a powerful large-scale distributed composite language model which inte-
grates well-known n-gram, SLM, and PLSA models under the directed MRF paradigm.
The composite language model has been trained by performing a convergent N-best list
approximate EM algorithm and a follow-up EM algorithm to improve word prediction
power on corpora up to a billion tokens, and stored on a supercomputer. We have
achieved drastic perplexity reductions and obtained significantly better translation
quality measured by the Bleu score and “readability” of translations in the task of
re-ranking the N-best list from a state-of-the-art parsing-based MT system. As far as
we know, this is the first work building a complex large-scale distributed language
model with a principled approach that simultaneously exploits syntactic, semantic,
and lexical regularities and is still more powerful than n-grams trained on a very
large corpus with up to a billion tokens. It is reasonable to conjecture that compos-
ite language models can achieve drastic perplexity reduction and significantly better
translation quality than n-gram when trained on Web-scale corpora that have trillions
of tokens.
As stated in Wang et al. (2010, p. 45), “Since Banko and Brill’s pioneering work
almost a decade ago (Banko and Brill 2001), it has been widely observed that the effec-
tiveness of statistical natural language processing (NLP) techniques is highly suscepti-
ble to the data size used to develop them. As empirical studies have repeatedly shown
that simple algorithms can often outperform their more complicated counterparts in
wide varieties of NLP applications with large data sets, many have come to believe that
it is the size of data, not the sophistication of the algorithms, that ultimately play the cen-
tral role in modern NLP (Norvig 2008).” It is true that ‘the more the data, the better the
result,’ a dictum recently reiterated in a somewhat stronger form in Halevy, Norvig, and
Pereira (2009), but care needs to be taken here. As we explained in the last paragraph of
Section 6.2, after we increase the size of data, we should also increase the complexity
of the model in order to achieve best results. For language modeling in particular,
because the expressive power of simple n-grams is rather limited, it is worthwhile
to exploit latent semantic information and syntactic structure that constrain the gen-
eration of natural language; this usually involves designing sophisticated algorithms.
Of course, this implies that it takes a huge amount of resources to perform the com-
putation. As cloud computing becomes the dominant platform for data management
and information processing as utility computing, this will become feasible, affordable,
and cheap.
The development of the large-scale distributed composite language model is in
its infancy; we are planning to deepen our research and push this research in its limit.
Specifically, we plan to integrate more advanced topic language models such as LDA
(Blei, Ng, and Jordan 2003) and resort to a hierarchical non-parametric Bayesian model
(Teh 2006; Teh and Jordan 2010) for smoothing fractional counts due to latent variables
to handle the sparse data problem in Kneser-Ney’s sense in a principled manner,
thus constructing a family of large-scale distributed composite lexical, syntactic, and
semantic language models. Finally we will put this family of composite language
models into a phrased-based machine translation decoder (Koehn, Och, and Marcu
2003) that produces a lattice of alternative translations/transcriptions or a syntax-based
</bodyText>
<page confidence="0.995723">
663
</page>
<note confidence="0.591811">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.993899">
decoder (Chiang 2005, 2007) that produces a forest of alternatives (such integration
would, in the exact case, reside in an extremely difficult complexity class, probably
PSPACE-complete) to significantly improve the performance of the state-of-the-art
machine translation systems.
</bodyText>
<sectionHeader confidence="0.895979" genericHeader="method">
Appendix A: An Example of Sentence Probability
</sectionHeader>
<bodyText confidence="0.988472916666667">
We chose a document from the LDC English Gigaword corpus to show how
sentence probability varies when computed by 5-gram, 5-gram/PLSA, and 5-gram/
PLSA+4-SLM/PLSA. The document tag is (XIN ENG 20041126 0168.story). This
document’s perplexity computed by 5-gram, 5-gram+PLSA, 5-gram+4-SLM+PLSA,
5-gram/PLSA, and 5-gram/PLSA+4-SLM/PLSA that are trained using 1.3 billion
tokens corpus is 97, 93, 83, 71, and 64, respectively. We show the first four sentences
below.
(s) cpc initiates education campaign to strengthen members ’ wavering convictions (/s)
(s) by zhao lei (/s) (s) beijing nov. ’nmbr xinhua the communist party of china cpc has decided
to launch a mass internal educational campaign from january next year to prevent its members
from wavering in their convictions (/s) (s) the decision aiming to keep the nature of the party
members intact was made at the meeting of the political bureau of the cpc central committee on
this oct. ’nmbr the cpc ’s top power organ (/s) � � � � � �
We then list the word conditional probabilities given its document history for the
fourth sentence. The first line is the fourth sentence; the second line (a) denotes the
natural log value of the conditional word probabilities given its document history
computed by 5-gram; the third line (b) denotes the natural log value of the conditional
word probabilities given its document history computed by 5-gram+PLSA; the fourth
line (c) denotes the natural log value of the conditional word probabilities given its
document history computed by 5-gram+PLSA+4-SLM; the fifth line (d) denotes the
natural log value of the conditional word probabilities given its document history
computed by 5-gram/PLSA; and the sixth line (e) denotes the natural log value of the
conditional word probabilities given its document history computed by 5-gram/PLSA+
4-SLM/PLSA.
</bodyText>
<table confidence="0.9825610625">
the decision aiming to keep the nature of the
−2.00317 −5.99654 −14.9793 −0.852055 −4.68269 −1.49193 −9.84554 −0.526566 −0.671103
−2.05502 −6.08843 −13.2655 −0.950885 −4.78594 −1.56474 −9.81423 −0.6258 −0.761926
−2.05416 −6.07556 −13.3486 −0.871798 −4.69523 −1.57311 −9.99731 −0.897362 −0.829652
−1.72696 −5.65359 −14.2013 −0.99068 −5.43248 −1.65002 −7.6 −0.612751 −0.731037
−1.80167 −5.73861 −14.5548 −0.893825 −5.05692 −1.60568 −7.92909 −0.751419 −0.755122
party members intact was made at the meeting of
−6.52337 −5.93013 −14.992 −5.5802 −5.91863 −3.47798 −1.0155 −3.77026 −3.11882
−6.48382 −6.00924 −13.8132 −5.57218 −5.98123 −3.56856 −1.1003 −3.87003 −3.14354
−6.48696 −5.81026 −8.11845 −3.04638 −2.21191 −2.80501 −1.12155 −3.85156 −2.3551
−3.46383 −5.03999 −15.242 −5.27819 −4.73655 −3.03394 −0.69443 −3.23709 −3.40986
−3.80075 −5.16911 −8.52597 −3.38567 −2.54778 −2.74127 −0.790644 −3.36195 −2.64652
the political bureau of the cpc central committee
−0.619712 −5.91994 −1.36559 −0.17816 −0.217888 −1.55966 −0.282506 −0.110539
−0.710967 −5.96757 −1.47083 −0.278998 −0.313708 −1.66454 −0.387673 −0.215632
−0.636643 −6.0839 −1.43513 −0.6519 −0.634246 −2.10113 −0.504145 −0.216812
−0.475928 −4.13345 −0.527685 −0.226433 −0.204276 −1.55903 −0.379722 −0.147238
−0.475442 −4.43649 −0.702968 −0.427385 −0.388118 −1.79781 −0.42272 −0.136813
664
Tan et al. this A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
on oct. ’nmbr the cpc ’s top power
−4.33953 −7.02792 −10.7495 −0.0380615 −3.87067 −9.93617 −3.54366 −4.19702 −7.6261
−4.37441 −6.88172 −10.6397 −0.141938 −3.65821 −8.81816 −3.60823 −4.29886 −7.64586
−3.57338 −6.86285 −10.9656 −0.131813 −3.8662 −8.85551 −3.42688 −4.28615 −7.82392
−4.61674 −6.49064 −13.0595 −0.255452 −3.73302 −5.55244 −3.60481 −3.97708 −7.85289
−3.85647 −6.61406 −12.5666 −0.178075 −3.92356 −5.90511 −3.46416 −4.03158 −7.91198
organ (/s)
−5.97561 −2.62716
−6.08022 −2.67444
−6.01553 −2.65078
−4.84265 −2.76932
−5.05393 −2.70787
</table>
<bodyText confidence="0.999749958333333">
The conditional probability of the word(s) party or political bureau given document
history computed by 5-gram/PLSA or 5-gram/PLSA+4-SLM/PLSA is significantly
boosted due to the appearance of semantic related words such as cpc and communist
party in the previous sentences, this clearly shows that the composite language mod-
els (5-gram/PLSA and 5-gram/PLSA+4-SLM/PLSA) trigger long-span document-level
discourse topics to influence word prediction. In contrast, there is no effect when using
linear combination models (i.e., 5-gram+PLSA and 5-gram+4-SLM+PLSA). Similarly,
the conditional probability of the words was made (or the word intact) given docu-
ment history computed by 5-gram/PLSA+4-SLM/PLSA is significantly boosted due
the appearance of the grammatical headword decision (or keep) in the same sentence,
this clearly shows that the composite language model (5-gram/PLSA+4-SLM/PLSA)
exploits sentence level syntactic structure to influence word prediction. In this case, the
n-gram has to increase its order to 11 or 8. The linear combination model 5-gram+4-
SLM+PLSA is quite effective, although it has negative impact for the prediction of
function words such as of the after the word(s) natural or political bureau.
Table 13 shows the statistics when n-grams are the same as the SLM’s WORD-
PREDICTOR in the most likely parse structure of each sentence in training corpora.
Whenever the n-grams are not the same as SLM’s WORD-PREDICTOR, the SLM com-
ponent will be effective to furnish sentence-level long-range grammatical information.
This example and Table 13 clearly demonstrate that an n-gram alone is not able to
achieve a similar effect to. SLM and PLSA even using Web-scale data, and the directed
MRF paradigm effectively synergizes n-gram, m-SLM, and PLSA in a complementary,
supplementary, and coherent way to form a powerful language model for word predic-
tion of natural language.
</bodyText>
<tableCaption confidence="0.846979">
Table 13
</tableCaption>
<bodyText confidence="0.67942">
Statistics when n-grams are the same as SLM’s WORD-PREDICTOR in the most likely parse
structure of each sentence in training corpora.
</bodyText>
<table confidence="0.948942166666667">
CORPUS w−1 w−1 w−1
−2 = h−1 −3 = h−1 −4 = h−1
−2 −3 −4
44 M 57% 46% 38%
230 M 59% 46% 38%
1.3 B 55% 48% 43%
</table>
<page confidence="0.824334">
665
</page>
<note confidence="0.527677">
Computational Linguistics Volume 38, Number 3
</note>
<sectionHeader confidence="0.893226" genericHeader="method">
Appendix B: Examples of Translation Results
</sectionHeader>
<bodyText confidence="0.971752833333333">
In the following, we give examples of “perfect” sentences, “only semantically cor-
rect” sentences, and “only grammatically correct” sentences, where the digit numbers
are the sentence number in the N-best list from Hiero (a) denotes the reference sentence,
(b) denotes the result provided by the composite language model, and (c) denotes the
result provided by 5-gram.
A few examples of “perfect” sentences provided by the composite language model:
</bodyText>
<figure confidence="0.87203624">
—512—
a. Sri Lanka’s Prime Minister Calls on the People to Work together for Permanent Peace
b. Sri Lanka prime minister called on national common efforts to achieve lasting peace
c. Sri Lanka prime minister called on the national common achieve lasting peace
—54—
a. Wilner said the maximum penalty for securities fraud is 10 years imprisonment. However,
the sentence is expected to be “significantly shorter” under the plea deal.
b. wiener, said securities fraud charges could be sentenced to 10 years’ imprisonment, according
to pleaded guilty mitigation, the sentence is “shorten”.
c. wiener, sentenced to 10 years’ imprisonment maximum securities fraud charges, according to
pleaded guilty mitigation, the sentence is “shorten”.
—206—
a. He said at a press conference in Doha, capital of Qarta, that if the United States “attacks Iraq,
it may trigger a global disaster.”
b. his press conference in doha, capital of qatar, said “if the united states attacks iraq, it will
trigger a world disaster”.
c. his press conference in doha, capital of qatar, said that the united states attacks iraq, “if it will
trigger a world disaster”.
—249—
a. Some Areas in Northwest Australia Face floods
b. floods in some areas in the northwest australia
c. australia northwest part offloods
A few examples of “only grammatically correct” sentences provided by the com-
posite language model:
—458—
</figure>
<reference confidence="0.63089475">
a. Sutiyoso said that gardens and flower beds would reduce the impression that the US embassy
is a fort.
b. szudy about woven said that garden landscape could reduce the us embassy to a fortress.
c. szudy over so that garden landscape can reduce the u.s. embassy to a fortress.
</reference>
<page confidence="0.98273">
666
</page>
<note confidence="0.634335">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<figure confidence="0.984414428571429">
—676—
a. He said that during last Christmas and the New Year, mainland tourists’ spending accounted
for 30
b. during christmas last year, he said, the mainland visitors spending will account for a three to
four percent of the kaneyuki business and become the major consumer of the industry.
c. last year, he said, mainland visitors during the christmas spending for the kaneyuki 3 to
4 percent of the business, has become the major consumption.
A few examples of “only semantically correct” sentences provided by the composite
language model:
—507—
a. The famous historic city of Cologne also narrowly escaped the disaster in the heavy rains.
b. cologne, a famous historical city also escaped unscathed in the heavy rain.
c. cologne, a famous historical city in heavy rain, escaped unscathed.
—416—
</figure>
<bodyText confidence="0.780470166666667">
a. However, he insisted on the timetable laid down by Bush. That is UN only has “weeks but not
months” to try to disarm Iraq peacefully and it would be military action thereafter.
b. however, he insists the bush timetable, the united nations is “weeks rather than months” to
urge iraq to the peace disarm, then we will take military action.
c. however, he insists that the bush timetable, the only “weeks rather than months” to urge iraq
to the peace disarm, she went on to take military action.
</bodyText>
<figure confidence="0.799779916666667">
—787—
a. France circulated its proposals in the form of “a non-paper.”
b. france is to distribute their proposals in the form of “non - paper.”
c. france is the form of “non - paper” distribute their proposals.
—313—
a. In China, three-quarters of the 1.3 billion population were reported to have celebrated the New
Year by watching television.
b. 1.3 billion population in china, according to reports, 3 / 4 is to watch tv celebrate lunar
new year.
c. 1.3 billion population in china, according to reports, 3 / 4 is to celebrate televisions.
667
Computational Linguistics Volume 38, Number 3
</figure>
<sectionHeader confidence="0.988496" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99919925">
We would like to dedicate this work to the
memory of Fred Jelinek, who passed away
while we were finalizing this manuscript.
Fred Jelinek laid the foundation for modern
speech recognition and text translation
technology. His work has greatly influenced
us. This research is supported by the
National Science Foundation under grant IIS
RI-small 0812483, a Google research award,
and Air Force Office of Scientific Research
under grant FA9550-10-1-0335. We would
like to thank the Ohio Supercomputer Center
for an allocation of computing time to make
this research possible; Ciprian Chelba for
providing the SLM code, answering many
questions regarding SLM, and consulting on
various aspects of the work; Ying Zhang and
Philip Resnik for providing the 1,000-best list
from Hiero for re-ranking in machine
translation; Peng Xu for suggesting to look at
the conditional probability of a word given
its document history to make the perplexity
result much more convincing. Finally we
would also like to thank the reviewers, who
made a number of invaluable suggestions
about the writing of the paper and pointed
out many weaknesses in our original
manuscript.
</bodyText>
<sectionHeader confidence="0.998702" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994827517647059">
Aho, A. and J. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling,
Volume 1: Parsing. Prentice-Hall,
Upper Saddle River, NJ.
Amari, S. and H. Nagaoka. 2000. Methods
of Information Geometry. Translations
of Mathematical Monographs; v. 191,
American Mathematical Society,
Providence, RI.
Bahl, L., J. Baker, F. Jelinek, and R. Mercer.
1977. Perplexity: A measure of difficulty
of speech recognition tasks. 94th Meeting
of the Acoustical Society of America, 62:S63,
Supplement 1, Miami, FL.
Baker, J. 1979. Trainable grammars for
speech recognition. The 97th Meeting of
the Acoustical Society of America, 547–550,
Cambridge, MA.
Banko, M. and E. Brill. 2001. Mitigating
the paucity-of-data problem: Exploring
the effect of training corpus size on
classifier performance for natural
language processing. Proceedings of the
1st International Conference on Human
Language Technology Research (HLT), 1–5,
Strodsburg, PA.
Barron, A. and C. Sheu. 1991. Approximation
of density functions by sequences of
exponential families. Annals of Statistics,
19:1347–1369.
Bellegarda, J. 2000. Exploiting latent
semantic information in statistical
language modeling. Proceedings of IEEE,
88(8):1279–1296.
Bellegarda, J. 2001. Robustness in
statistical language modeling: Review
and perspectives. In J. Junqua and
G. van Noods, editors, Robustness in
Language and Speech Technology, pages
101–121. Kluwer Academic Publishers,
Dordrecht.
Bellegarda, J. 2003. Statistical language
model adaptation: Review and
perspectives. Speech Communication,
42:93–108.
Benedi, J. and J. S´anchez. 2005. Estimation of
stochastic context-free grammars and their
use as language models. Computer Speech
and Language, 19(3):249–274.
Bengio, Y., R. Ducharme, P. Vincent, and
C. Jauvin. 2003. A neural probabilistic
language model. Journal of Machine
Learning Research, 3:1137–1155.
Berger, A., S. Della Pietra, and V. Della Pietra.
1996. A maximum entropy approach
to natural language processing.
Computational Linguistics, 22(1):39–71.
Bilmes, J. and K. Kirchhoff. 2003. Factored
language models and generalized parallel
backoff. Proceedings of the North American
Chapter of the Association for Computational
Linguistics - Human Language Technologies
(NAACL-HLT), 4–6, Edmonton, Alberta,
Canada.
Blei, D., A. Ng, and M. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Brants, T., A. Popat, P. Xu, F. Och, and
J. Dean. 2007. Large language models in
machine translation. The 2007 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), 858–867, Prague,
Czech Republic.
Charniak, E. 2001. Immediate-head
parsing for language models. The
39th Annual Conference on Association
of Computational Linguistics (ACL),
124–131, Toulouse, France.
Charniak, E., K. Knight, and K. Yamada.
2003. Syntax-based language models for
statistical machine translation. MT Summit
IX, International Association for Machine
Translation, 40–46, New Orleans, LA.
Chelba, C. 2000. Exploiting Syntactic
Structure for Natural Language Modeling.
</reference>
<page confidence="0.997197">
668
</page>
<note confidence="0.975982">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<reference confidence="0.998711669491525">
Ph.D. dissertation, The Johns Hopkins
University, Baltimore, MD.
Chelba, C. and F. Jelinek. 1998. Exploiting
syntactic structure for language modeling.
The 36th Annual Conference on Association of
Computational Linguistics (ACL), 225–231,
Montreal, Quebec, Canada.
Chelba, C. and F. Jelinek. 2000. Structured
language modeling. Computer Speech and
Language, 14(4):283–332.
Chelba, C., J. Schalkwyk, T. Brants, V. Ha,
B. Harb, W. Neveitt, C. Parada, and
P. Xu. 2010. Query language modeling for
voice search. Proceedings of the 2010 IEEE
Workshop on Spoken Language Technology
(SLT), 127–132, Berkeley, CA.
Chen, S. and J. Goodman. 1999. An
empirical study of smoothing techniques
for language modeling. Computer Speech
and Language,13(4): 319–358.
Chiang, D. 2005. A hierarchical phrase-based
model for statistical machine translation.
The 43th Annual Conference on Association of
Computational Linguistics (ACL), 263–270,
Ann Arbor, MI.
Chiang, D. 2007. Hierarchical phrase-based
translation. Computational Linguistics,
33(2):201–228.
Dean, J. and S. Ghemawat. 2004. MapReduce:
Simplified data processing on large
clusters. The Sixth Symposium on Operating
Systems Design and Implementation (OSDI),
137–150, San Francisco, CA.
Della Pietra, S., V. Della Pietra, J. Gillett,
J. Lafferty, H. Printz, and L. Ures.1994.
Inference and estimation of a long-range
trigram model. Second International
Colloquium on Grammatical Inference
and Applications (ICGI), pages 78–92,
Springer-Verlag, Alicante, Spain.
Della Pietra, S., V. Della Pietra and J. Lafferty.
1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380–393.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood estimation from
incomplete data via the EM algorithm.
Journal of Royal Statistical Society, 39:1–38.
Emami, A., K. Papineni, and J. Sorensen.
2007. Large-scale distributed language
modeling. The 32nd IEEE International
Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pages 37–40,
Honolulu, HI.
Gildea, D. and T. Hofmann. 1999.
Topic-based language models using
EM. The 6th European Conference on
Speech Communication and Technology
(EUROSPEECH), pages 2167–2170.
Goodman, J. 2001. A bit of progress in
language modeling. Computer Speech
and Language, 15(4):403–434.
Halevy, A., P. Norvig, and F. Pereira. 2009.
The unreasonable effectiveness of data.
IEEE Intelligent Systems, 24(2):8–12.
Hastie, T., R. Tibshirani and J. Friedman.
2009. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction,
2nd edition. Springer, Berlin.
Hofmann, T. 2001. Unsupervised learning
by probabilistic latent semantic analysis.
Machine Learning, 42(1):177–196.
Jelinek, F. 1991. Up from trigrams!
The struggle for improved language
models. Second European Conference on
Speech Communication and Technology
(EUROSPEECH), pages 1037–1040,
Genove, Italy.
Jelinek, F. 1998. Statistical Methods for Speech
Recognition. MIT Press, Cambridge, MA.
Jelinek, F. 2004. Stochastic analysis of
structured language modeling. In M.
Johnson, S. Khudanpur, M. Ostendorf,
and R. Rosenfeld, editors, Mathematical
Foundations of Speech and Language
Processing, pages 37–72, Springer-Verlag,
Berlin.
Jelinek, F. 2009. The dawn of statistical
ASR and MT. Computational Linguistics,
35(4):483–494.
Jelinek, F. and C. Chelba. 1999. Putting
language into language modeling.
Sixth European Conference on Speech
Communication and Technology
(EUROSPEECH), Keynote Paper 1,
Budapest, Hungary.
Jelinek, F. and R. Mercer. 1980. Interpolated
estimation of Markov source parameters
from sparse data. In E. Gelsema and
L. Kanal, editors, Pattern Recognition in
Practice, pages 381–397. North Holland
Publishers, Amsterdam.
Jurafsky, D. and J. Martin. 2008. Speech
and Language Processing, 2nd edition.
Prentice Hall, Upper Saddle River, NJ.
Khudanpur, S. and J. Wu. 2000. Maximum
entropy techniques for exploiting
syntactic, semantic and collocational
dependencies in language modeling.
Computer Speech and Language,
14(4):355–372.
Kneser, R. and H. Ney. 1995. Improved
backing-off for m-gram language
modeling. The 20th IEEE International
Conference on Acoustics, Speech, and Signal
Processing (ICASSP), 181–184, Detroit, MI.
Koehn, P. 2004. Statistical significance tests
for machine translation evaluation. The
</reference>
<page confidence="0.958635">
669
</page>
<reference confidence="0.994636268907563">
Computational Linguistics Volume 38, Number 3
2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 388–395, Barcelona, Spain.
Koehn, P., F. Och, and D. Marcu. 2003.
Statistical phrase-based translation.
The Human Language Technology
Conference (HLT), pages 48–54,
Edmonton, Alberta, Canada.
Lari, K. and S. Young. 1990. The estimation
of stochastic context-free grammars using
the inside-outside algorithm. Computer
Speech and Language, 4:35–56.
Lau, R., R. Rosenfeld, and S. Roukos.
1993. Trigger-based language models:
A maximum entropy approach. The 18th
IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),
II:45–48, Minneapolis, MN.
Lauritzen, S. 1996. Graphical Models. Oxford
University Press.
Lavie, A., D. Yarowsky, K. Knight,
C. Callison-Burch, N. Habash, and
T. Mitamura. 2006. MINDS Workshops
Machine Translation Working
Group Final Report. Available at
http://www-nlpir.nist.gov/MINDS/
FINAL/MT.web.pdf.
Lin, J. and C. Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan and
Claypool Publishers.
Mark, K., M. Miller, and U. Grenander.
1996. Constrained stochastic language
models, In S. Levinson and L. Shepp,
editors, Image Models and Their Speech
Model Cousins, pages 131–137,
Springer-Verlag, Berlin.
McAllester, D., M. Collins, and F. Pereira.
2004. Case-factor diagrams for structured
probabilistic modeling. Proceedings of the
20th Conference on Uncertainty in Artificial
Intelligence (UAI), pages 382–391, Banff,
Canada.
Norvig, P. 2008. Statistical learning as the
ultimate agile development tool. ACM
17th Conference on Information and
Knowledge Management (CIKM)
Industry Event, Napa Valley, CA.
Och, F. 2003. Minimum error rate training
in statistical machine translation. The
41th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 160–167, Sapporo, Japan.
Och, F. 2005. Statistical machine
translation: Foundations and recent
advances. Presentation at MT-Summit.
http://www.mt-archive.info/
MTS-2005-och.pdf
Papineni, K., S. Roukos, T. Ward, and
W. Zhu. 2002. BLEU: a method for
automatic evaluation of machine
translation. The 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 311–318, Philadelphia, PA.
Pereira, F. 2000. Formal grammar and
information theory: Together again?
Philosophical Transactions of the Royal
Society: Mathematical, Physical and
Engineering Sciences, 358(1769):1239–1253.
Roark, B. 2001. Probabilistic top–down
parsing and language modeling.
Computational Linguistics, 27(2):249–276.
Rosenfeld, R. 1996. A maximum entropy
approach to adaptive statistical language
modeling. Computer Speech and Language,
10(2):187–228.
Rosenfeld, R. 2000a. Two decades of
statistical language modeling: Where
do we go from here? Proceedings of IEEE,
88(8):1270–1278.
Rosenfeld, R. 2000b. Incorporating
linguistic structure into statistical
language models. Philosophical
Transactions of the Royal Society:
Mathematical, Physical and Engineering
Sciences, 358(1769):1311–1324.
Rosenfeld, R., S. Chen, and X. Zhu. 2001.
Whole-sentence exponential language
models: A vehicle for linguistic-statistical
integration. Computer Speech and Language,
15(1): 55–73.
Russell, S. and P. Norvig. 2010. Artificial
Intelligence: A Modern Approach,
3rd edition. Prentice Hall, Upper
Saddle River, NJ.
Saul, L. and F. Pereira. 1997. Aggregate
and mixed-order Markov models for
statistical language processing. The
Second Conference on Empirical Methods
in Natural Language Processing (EMNLP),
81–89, Providence, RI.
Teh, Y. 2006. A hierarchical Bayesian
language model based on Pitman-Yor
processes. The 44th Annual Conference of
the Association of Computational Linguistics
(ACL), 985–992, Sydney, Australia.
Teh, Y. and M. Jordan. 2010. Hierarchical
Bayesian nonparametric models with
applications. In N. Hjort, C. Holmes,
P. Mueller, and S. Walker, editors,
Bayesian Nonparametrics: Principles
and Practice, pages 158–207,
Cambridge University Press.
Van Uytsel, D. and D. Compernolle. 2005.
Language modeling with probabilistic
left corner parsing. Computer Speech and
Language, 19(2):171–204.
Vapnik, V. 1998. Statistical Learning Theory.
Springer, Berlin.
</reference>
<page confidence="0.989411">
670
</page>
<note confidence="0.886738">
Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model
</note>
<reference confidence="0.999799475">
Wallach, H. 2006. Topic modeling: Beyond
bag-of-words. The 23rd International
Conference on Machine Learning (ICML),
977–984, Pittsburgh, PA.
Wang, S., R. Greiner, and S. Wang. 2009.
Consistency and generalization bounds
for maximum entropy density estimation.
Manuscript.
Wang, W. and M. Harper. 2002. The
SuperARV language model: Investigating
the effectiveness of tightly integrating
multiple knowledge sources. The 2002
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 238–247, Philadelphia, PA.
Wang, S., D. Schuurmans, F. Peng, and
Y. Zhao. 2005a. Combining statistical
language models via the latent maximum
entropy principle. Machine Learning
Journal: Special Issue on Learning in Speech
and Language Technologies, 60:229–250.
Wang, S., D. Schuurmans, and Y. Zhao.
2012. The latent maximum entropy
principle. ACM Transactions on Knowledge
Discovery from Data (TKDD) to appear.
In Press.
Wang, K., C. Thrasher, E. Viegas, X. Li,
and P. Hsu. 2010. An overview of
Microsoft web N-gram corpus and
applications. Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human
Language Technologies (NAACL-HLT):
Demonstration Session, pages 45–48,
Los Angeles, CA.
Wang, S., S. Wang, L. Cheng, R. Greiner,
and D. Schuurmans. 2006. Stochastic
analysis of lexical and semantic
enhanced structural language model.
The 8th International Colloquium on
Grammatical Inference (ICGI), pages 97–111,
Tokyo, Japan.
Wang, S., S. Wang, R. Greiner, D.
Schuurmans, and L. Cheng. 2005b.
Exploiting syntactic, semantic and
lexical regularities in language modeling
via directed Markov random fields.
The 22nd International Conference on
Machine Learning (ICML), 953–960,
Bonn, Germany.
Wu, C. 1983. On the convergence properties
of the EM algorithm. Annals of Statistics,
11:95–103.
Yamada, K. and K. Knight. 2001.
A syntax-based statistical translation
model. Proceedings of the 39th Annual
Conference of the Association of
Computational Linguistics (ACL),
1067–1074, Toulouse, France.
Zangwill, W. 1969. Nonlinear Programming:
A Unified Approach. Prentice-Hall,
Upper Saddle River, NJ.
Zhang, Y. 2008. Structured Language
Models for Statistical Machine Translation.
Ph.D. dissertation, Carnegie Mellon
University, Pittsburgh, PA.
Zhang, Y., A. Hildebrand, and S. Vogel.
2006. Distributed language modeling for
N-best list re-ranking. The 2006 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), 216–223, Sydney,
Australia.
Zhang, Y., S. Vogel, A. Emami, K. Papineni,
J. Sorensen, and J. Quinn. 2011. Distributed
language modeling. In Joseph Olive,
Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language
Processing and Machine Translation: DARPA
Global Autonomous Language Exploitation,
Chapter 2.5.1, 252–270, Springer.
</reference>
<page confidence="0.998541">
671
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.612033">
<title confidence="0.972411">A Scalable Distributed Syntactic, Semantic,</title>
<author confidence="0.646712">Lexical Language Model</author>
<affiliation confidence="0.994515">Wright State University Wright State University Wright State University Wright State University</affiliation>
<abstract confidence="0.997317636363636">This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and “readability” of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>a</author>
</authors>
<title>Sutiyoso said that gardens and flower beds would reduce the impression that the US embassy is a fort. b. szudy about woven said that garden landscape could reduce the us embassy to a fortress. c. szudy over so that garden landscape can reduce the u.s. embassy to a fortress.</title>
<marker>a, </marker>
<rawString>a. Sutiyoso said that gardens and flower beds would reduce the impression that the US embassy is a fort. b. szudy about woven said that garden landscape could reduce the us embassy to a fortress. c. szudy over so that garden landscape can reduce the u.s. embassy to a fortress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aho</author>
<author>J Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing. Prentice-Hall, Upper Saddle River, NJ.</booktitle>
<contexts>
<context position="17126" citStr="Aho and Ullman 1972" startWordPosition="2648" endWordPosition="2651">ty p(a|h−1 −m); a ∈ A={(unary, NTlabel), (adjoin-left, NTlabel), (adjoin-right, NTlabel), NULL}. Depending on an action a = adjoin-right or adjoin-left, the headword h−1 or h−2 is percolated up by one tree level, the indices of the current exposed headwords h−3, h−4, · · · are increased by 1, and these headwords together with h−1 or h−2 become the new exposed headwords. Once the CONSTRUCTOR hits NULL, the headword indexing and current parse structure remain as they are, and the CONSTRUCTOR passes control to the WORD-PREDICTOR. SLM is thus essentially a generalization of a shift-reduce parser (Aho and Ullman 1972) with adjoin corresponding to reduce and predict to shift. (See a detailed description about SLM in Chelba and Jelinek [1998, 2000]; Chelba [2000]; Jelinek [2004]). As an example taken from Jelinek (2004), Figure 1 shows a complete parse where SB/SE is a distinguished POS tag for (s)/(/s) respectively, ((s),TOP) is the only allowed head, and ((/s),TOP’) is the head of any constituent that dominates (/s) but not (s). In Figure 1, at the time just after the word as is generated, the exposed headwords are “(s) SB, show np, has vbz.” The subsequent model actions are: “POStag as, null, predict its,</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, A. and J. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing. Prentice-Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Amari</author>
<author>H Nagaoka</author>
</authors>
<title>Methods of Information Geometry.</title>
<date>2000</date>
<journal>Translations of Mathematical Monographs;</journal>
<volume>191</volume>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="75247" citStr="Amari and Nagaoka 2000" startWordPosition="12107" endWordPosition="12110">ich density estimation problem with Kullback-Leibler divergence as a cost function, and there is always a trade-off between approximation error and estimation error (Barron and Sheu 1991), reminiscent of the “bias-variance” trade-off for a regression problem with a quadratic cost function (Hastie, Tibshirani, and Friedman 2009). Figure 5 explains the perplexity results in Tables 3 and 5 from a model selection point of view. Let pˆ denote the true (but unknown) distribution of natural language, its information projection to n-grams is the minimum Kullback-Leibler divergence from pˆ to n-grams (Amari and Nagaoka 2000; Wang, Greiner, and Wang 2009) and is denoted as ˆpn, n = 3, 4, 5, 6. Let p˜ denote the empirical distribution of natural language—in particular, ˜pM denotes the empirical distribution for a million token corpus, ˜pB denotes the empirical distribution for a billion token corpus, and ˜pT denotes the empirical distribution for a trillion token corpus. The information projection of ˜pM to trigram is p3M, to 4-gram is p4M, and to 5-gram is p5M. The distance between pˆ and ˆpn(n = 3, 4, 5, 6), D( ˆp, ˆpn), is the approximation error when using n-gram to represent ˆp, that is, the best the n-gram c</context>
</contexts>
<marker>Amari, Nagaoka, 2000</marker>
<rawString>Amari, S. and H. Nagaoka. 2000. Methods of Information Geometry. Translations of Mathematical Monographs; v. 191, American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>J Baker</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Perplexity: A measure of difficulty of speech recognition tasks.</title>
<date>1977</date>
<booktitle>94th Meeting of the Acoustical Society of America, 62:S63, Supplement 1,</booktitle>
<location>Miami, FL.</location>
<contexts>
<context position="59688" citStr="Bahl et al. 1977" startWordPosition="9612" endWordPosition="9615">es are automatically parsed and used to initialize model parameters, whereas for the 1.3 billion token corpus, we parse the sentences from a portion of the corpus that contains 230 million tokens, then use them to initialize model parameters. The parser at openNLP is trained on the Penn Treebank, which has only one million tokens, and there is a mismatch between the Penn Treebank and the LDC English Gigaword corpus. Nevertheless, experimental results show that this approach is effective to provide initial values of model parameters. 6.2 Perplexity Results Table 3 gives the perplexity results (Bahl et al. 1977) of n-grams (n = 3, 4, and 5) using linear interpolation and Kneser-Ney (1995) smoothing when the training corpus has 44 million, 230 million, and 1.3 billion tokens, respectively. We have implemented a distributed n-gram with linear interpolation smoothing, but we don’t have distributed n-grams with Kneser-Ney smoothing implemented by us. Instead, we use the SRI Language Modeling Toolkit to obtain perplexity results of n-grams with Kneser-Ney smoothing for the 44 million and 230 million token corpora using a single machine that has 20G memory at the Ohio Supercomputer center. We are not able </context>
</contexts>
<marker>Bahl, Baker, Jelinek, Mercer, 1977</marker>
<rawString>Bahl, L., J. Baker, F. Jelinek, and R. Mercer. 1977. Perplexity: A measure of difficulty of speech recognition tasks. 94th Meeting of the Acoustical Society of America, 62:S63, Supplement 1, Miami, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>The 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="8512" citStr="Baker 1979" startWordPosition="1238" endWordPosition="1239">proposed the latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn’t overcome the second hurdle. The third method is directed Markov random field (Wang et al. 2005b) that overcomes both weaknesses in the maximum entropy approach. Wang et al. used this approach to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the wellknown inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framewo</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, J. 1979. Trainable grammars for speech recognition. The 97th Meeting of the Acoustical Society of America, 547–550, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing.</title>
<date>2001</date>
<booktitle>Proceedings of the 1st International Conference on Human Language Technology Research (HLT), 1–5,</booktitle>
<location>Strodsburg, PA.</location>
<contexts>
<context position="87969" citStr="Banko and Brill 2001" startWordPosition="14109" endWordPosition="14112">e first work building a complex large-scale distributed language model with a principled approach that simultaneously exploits syntactic, semantic, and lexical regularities and is still more powerful than n-grams trained on a very large corpus with up to a billion tokens. It is reasonable to conjecture that composite language models can achieve drastic perplexity reduction and significantly better translation quality than n-gram when trained on Web-scale corpora that have trillions of tokens. As stated in Wang et al. (2010, p. 45), “Since Banko and Brill’s pioneering work almost a decade ago (Banko and Brill 2001), it has been widely observed that the effectiveness of statistical natural language processing (NLP) techniques is highly susceptible to the data size used to develop them. As empirical studies have repeatedly shown that simple algorithms can often outperform their more complicated counterparts in wide varieties of NLP applications with large data sets, many have come to believe that it is the size of data, not the sophistication of the algorithms, that ultimately play the central role in modern NLP (Norvig 2008).” It is true that ‘the more the data, the better the result,’ a dictum recently </context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, M. and E. Brill. 2001. Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing. Proceedings of the 1st International Conference on Human Language Technology Research (HLT), 1–5, Strodsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Barron</author>
<author>C Sheu</author>
</authors>
<title>Approximation of density functions by sequences of exponential families.</title>
<date>1991</date>
<journal>Annals of Statistics,</journal>
<pages>19--1347</pages>
<contexts>
<context position="74812" citStr="Barron and Sheu 1991" startWordPosition="12040" endWordPosition="12043"> up to 5% perplexity reductions; when we increase the order for n-gram and m-SLM to n = 5 and m = 4, however, the composite language models become worse and slightly overfit the data even if we use linear interpolation smoothing, and there are no further perplexity reductions. To summarize, as a sub-problem for MT and speech recognition under the sourcechannel paradigm (Jelinek 2009), language modeling is a data-rich and feature-rich density estimation problem with Kullback-Leibler divergence as a cost function, and there is always a trade-off between approximation error and estimation error (Barron and Sheu 1991), reminiscent of the “bias-variance” trade-off for a regression problem with a quadratic cost function (Hastie, Tibshirani, and Friedman 2009). Figure 5 explains the perplexity results in Tables 3 and 5 from a model selection point of view. Let pˆ denote the true (but unknown) distribution of natural language, its information projection to n-grams is the minimum Kullback-Leibler divergence from pˆ to n-grams (Amari and Nagaoka 2000; Wang, Greiner, and Wang 2009) and is denoted as ˆpn, n = 3, 4, 5, 6. Let p˜ denote the empirical distribution of natural language—in particular, ˜pM denotes the em</context>
<context position="76189" citStr="Barron and Sheu 1991" startWordPosition="12276" endWordPosition="12279">n for a trillion token corpus. The information projection of ˜pM to trigram is p3M, to 4-gram is p4M, and to 5-gram is p5M. The distance between pˆ and ˆpn(n = 3, 4, 5, 6), D( ˆp, ˆpn), is the approximation error when using n-gram to represent ˆp, that is, the best the n-gram can do when abundant data are available. The distance between ˜pnM and ˆpn, n = 3, 4, 5, D( ˜pnM, ˆpn), is the estimation error when only the million token corpus is available. The Pythagorean theorem states that the distance between pˆ and ˜pM, D( ˆp, ˜pM), is the sum of the approximation error and the estimation error (Barron and Sheu 1991; Amari and Nagaoka 2000; Wang, Greiner, and Wang 2009). In language modeling research, because pˆ is unknown, the distance between pˆ and pnM, n = 3,4 is approximately computed 658 Table 9 Perplexity results for various language models on test corpora, where + denotes linear combination, / denotes composite model; n denotes the order of the n-gram, and m denotes the order of the SLM; the topic nodes are pruned from 200 to 5. Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model 44M REDUCTION 44M REDUCTION 44M REDUCTION n=3,m=2 n=4,m=3 n=5,m=4 262 258 260 244 6.9% 2</context>
</contexts>
<marker>Barron, Sheu, 1991</marker>
<rawString>Barron, A. and C. Sheu. 1991. Approximation of density functions by sequences of exponential families. Annals of Statistics, 19:1347–1369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling.</title>
<date>2000</date>
<booktitle>Proceedings of IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="4350" citStr="Bellegarda 2000" startWordPosition="632" endWordPosition="633">undamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morpho</context>
<context position="54982" citStr="Bellegarda 2000" startWordPosition="8869" endWordPosition="8870">orpus with 284 million tokens without punctuation, the improvement declines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a 4-gram or 5-gram</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Bellegarda, J. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of IEEE, 88(8):1279–1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Robustness in statistical language modeling: Review and perspectives.</title>
<date>2001</date>
<booktitle>Robustness in Language and Speech Technology,</booktitle>
<pages>101--121</pages>
<editor>In J. Junqua and G. van Noods, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="5038" citStr="Bellegarda 2001" startWordPosition="719" endWordPosition="720">ly, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though th</context>
</contexts>
<marker>Bellegarda, 2001</marker>
<rawString>Bellegarda, J. 2001. Robustness in statistical language modeling: Review and perspectives. In J. Junqua and G. van Noods, editors, Robustness in Language and Speech Technology, pages 101–121. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Statistical language model adaptation: Review and perspectives.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>42--93</pages>
<contexts>
<context position="12141" citStr="Bellegarda 2003" startWordPosition="1794" endWordPosition="1795">, 2007), a state-of-the-art parsing-based machine translation system; we achieve significantly better translation quality measured by the Bleu score and “readability” of translations. Finally, we draw our conclusions and propose future work in Section 7. The main theme of our approach is “to exploit information, be it syntactic structure or semantic fabric, which involves a fairly high degree of cognition. This is precisely the kind of knowledge that humans naturally and inherently use to process natural language, so it can be reasonably conjectured to represent a key ingredient for success” (Bellegarda 2003, p. 105). In that light, the directed MRF framework, “whose ultimate goal is to integrate all available knowledge sources, appears most likely to harbor a potential breakthrough. It is hoped that the on-going effort conducted in this work to leverage such latent synergies will lead, in the not-too-distant future, to more polyvalent, multi-faceted, effective and tractable solutions for language modeling – this is only beginning to scratch the surface in developing systems capable of deep understanding of natural language” (Bellegarda 2003, p. 105). 2. The Composite n-gram/SLM/PLSA Language Mod</context>
</contexts>
<marker>Bellegarda, 2003</marker>
<rawString>Bellegarda, J. 2003. Statistical language model adaptation: Review and perspectives. Speech Communication, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Benedi</author>
<author>J S´anchez</author>
</authors>
<title>Estimation of stochastic context-free grammars and their use as language models.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>3</issue>
<marker>Benedi, S´anchez, 2005</marker>
<rawString>Benedi, J. and J. S´anchez. 2005. Estimation of stochastic context-free grammars and their use as language models. Computer Speech and Language, 19(3):249–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2906" citStr="Bengio et al. 2003" startWordPosition="411" endWordPosition="414">ovember 2011. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Bengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A., S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT), 4–6,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="39479" citStr="Bilmes and Kirchhoff 2003" startWordPosition="6406" endWordPosition="6409">different orders k = 0, · · · , n and has been explained in Chelba and Jelinek (1998, 2000) and Chelba (2000). The WORD-PREDICTOR is, however, a conditional probabilistic model p(w|w−1 −n+1h−1−mg) where there are three kinds of context, w−1 −n+1, h−1 −m, and g—each forms a linear Markov chain. The model has a combinatorial number of relative frequency estimates of different orders among three linear Markov chains. We generalize Jelinek and Mercer’s (1980) original recursive mixing scheme to handle the situation where the context is a mixture of Markov chains. The factored language (FL) model (Bilmes and Kirchhoff 2003) is close to the smoothing technique we propose here, the major difference is that FL considers all possible combination of the context of conditional probability that can be concisely represented by a factor graph, whereas our approach strictly respects the order of Markov chains for word sequence and headword sequence because we believe natural language tightly follows these orders; moreover, where FL uses a backoff technique, we use linear interpolation. Consider a composite trigram/2-SLM/PLSA language model. Figure 3 illustrates a lattice formed of all possible conditional probabilistic mo</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Bilmes, J. and K. Kirchhoff. 2003. Factored language models and generalized parallel backoff. Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT), 4–6, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D., A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Popat</author>
<author>P Xu</author>
<author>F Och</author>
<author>J Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>The 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), 858–867,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3205" citStr="Brants et al. 2007" startWordPosition="455" endWordPosition="458">. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing n</context>
<context position="46655" citStr="Brants et al. 2007" startWordPosition="7506" endWordPosition="7509">)Pp(Tk|Wk, d) F-h−1 ∈Tk∈Zk,g∈Gd p(wk+1|wkk−n+2h−mg)p(g|d)Pp(Tk|Wk, d) The M-step uses the same count smoothing technique as that described in the N-best list approximate EM. 647 Computational Linguistics Volume 38, Number 3 3.3 Distributed Architecture When using very large corpora to train our composite language model, the data and the parameters cannot be stored together on a single machine, so we have to resort to distributed computing. The topic of large-scale distributed language models is relatively new, and existing work is restricted to n-grams only (Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007). Although all existing research use distributed architectures that follow the client–server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts, and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each n-gram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Brants, T., A. Popat, P. Xu, F. Och, and J. Dean. 2007. Large language models in machine translation. The 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), 858–867, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>The 39th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>124--131</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="4067" citStr="Charniak 2001" startWordPosition="592" endWordPosition="593">atios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should </context>
<context position="5513" citStr="Charniak 2001" startWordPosition="788" endWordPosition="789">a like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, De</context>
<context position="55035" citStr="Charniak 2001" startWordPosition="8877" endWordPosition="8878">improvement declines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a 4-gram or 5-gram is nonetheless still the best way to do language mod</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Charniak, E. 2001. Immediate-head parsing for language models. The 39th Annual Conference on Association of Computational Linguistics (ACL), 124–131, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>MT Summit IX, International Association for Machine Translation,</booktitle>
<pages>40--46</pages>
<location>New Orleans, LA.</location>
<contexts>
<context position="84971" citStr="Charniak et al. 2003" startWordPosition="13654" endWordPosition="13657">use there is not much diversity in the N-best list. It is expected that putting our composite language into a one-pass decoder should result in much improved Bleu scores. Besides reporting the Bleu scores, we look at the “readability” of translations, similar to the study conducted by Charniak, Knight, and Yamada (2003). The translations are sorted into four groups: good/bad syntax crossed with good/bad meaning by human judges (see Table 12). We find that many more sentences are perfect, many more are grammatically correct, and many more are semantically correct. The syntactic language model (Charniak et al. 2003) only improves translations to have good grammar, but does not improve translations to preserve meaning. The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA1 language model improves both significantly. Bear in mind that Charniak et al. (2003) integrated Charniak’s language model with the syntax-based translation model proposed by Yamada and Knight (2001) to rescore a tree-to-string translation forest, whereas we use only our language model for N-best list re-ranking. Also, the same study (Charniak et al. 2003) found that the outputs produced using the n-grams received higher scores from Bleu; </context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Charniak, E., K. Knight, and K. Yamada. 2003. Syntax-based language models for statistical machine translation. MT Summit IX, International Association for Machine Translation, 40–46, New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<institution>The Johns Hopkins University,</institution>
<location>Baltimore, MD.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="4027" citStr="Chelba 2000" startWordPosition="586" endWordPosition="587">ents because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language r</context>
<context position="5551" citStr="Chelba 2000" startWordPosition="794" endWordPosition="795">ic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), D</context>
<context position="8881" citStr="Chelba 2000" startWordPosition="1295" endWordPosition="1296">to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the wellknown inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outsi</context>
<context position="14224" citStr="Chelba (2000)" startWordPosition="2132" endWordPosition="2133"> network (McAllester, Collins, and Pereira 2004). A key difference between directed MRFs and undirected MRFs is that a directed MRF requires many local normalization constraints whereas an undirected MRF has a global normalization factor. The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is essentially a WORD-PREDICTOR, that is, given its entire document history, it predicts the next word wk+1 E V based on the last n–1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, · · · , wk and V denotes the vocabulary. The SLM proposed in Chelba and Jelinek (1998, 2000) and Chelba (2000) uses syntactic information beyond the regular n-gram models to capture sentence-level long-range 634 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model dependencies. The SLM is based on statistical parsing techniques that allow syntactic analysis of sentences; it assigns a probability p(W,T) to every sentence W and every possible binary parse T. The terminals of T are the words of W with part of speech (POS) tags, and the nodes of T are annotated with phrase headwords and non-terminal labels. Let W be a sentence of length n words to which we have prepended the s</context>
<context position="24394" citStr="Chelba 2000" startWordPosition="3831" endWordPosition="3832">parse trees grows faster than exponentially with sentence length; Wang et al. (2006) have derived a generalized inside– outside algorithm by applying the standard EM algorithm and considering the auxiliary function � � � � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) (10) Q(p , p) = Gl Tl d∈D l The complexity of this algorithm is sixth order (sentence length), however; thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba 1999; Jelinek 2004). 3.1 N-best List Approximate EM Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, max �         TSN L(D,p,T�N) = ��  � d∈D  max Pp(Wl,Tl,Gl|d)    (11) T�l N∈T�N l Gl Tl∈T�lN,||T�lN||=N where T&apos;lN is a set of N parse trees for sentence Wl in document d, ||· ||denotes the cardinality, and T�N is a collection of T&apos;N for sentences over entire corpus D. The N-best list approximate EM involves two steps: 1. N-best list search: For e</context>
<context position="30557" citStr="Chelba (2000)" startWordPosition="4926" endWordPosition="4927">’s parameters. We can either find both the N-best parses for each sentence and N-best topics for each document simultaneously or separately. The latter is much preferred, because the first case is much more computationally expensive. To extract the N-best topics, we run an EM algorithm for a PLSA model on training corpus D, then keep the N most likely topics (denoted as Gd) according to the values of p(g|d); the rest of the topics are purged. To extract the N-best parse trees, we adopt a synchronous, multi-stack search strategy that is similar to the one in Chelba and Jelinek (1998, 2000) and Chelba (2000), which involves a set of stacks storing partial parses of the most likely ones for a given prefix Wk and the less probable parses are purged. Each stack contains 641 Computational Linguistics Volume 38, Number 3 hypotheses (partial parses) that have been constructed by the same number of WORDPREDICTOR and the same number of CONSTRUCTOR operations. The hypotheses in each stack are ranked according to the log(Pp (Wk, Tk |d)) score with the highest on top, where Pp(Wk,Tk |d) = EGk Pp(Wk, Tk, Gk|d) and the Wk, Tk, Gk denote the joint sequence of prefix Wk = w0, w1 · · · , wk with its parse struct</context>
<context position="38318" citStr="Chelba (2000)" startWordPosition="6213" endWordPosition="6214"> d times the conditional distribution Pp(Tl|Wl, d) = Pp(Tl,Wl|d)/ ETl∈Tl Pp(Tl, Wl|d)—that is, Pp(Tl|Wl, d)#(twh−1 −m.tag, Wl, Tl, d) and Pp(Tl|Wl, d)#(ah−1 −m, Wl, Tl, d), respectively. When only SLM is considered, the expected count for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR, over parse Tl of sentence Wl in document d is the real count that appeared in parse Tl of sentence Wl in document d 644 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model times the posterior probability Pp(Tl|Wl, d), as is done in Chelba and Jelinek (1998, 2000) and Chelba (2000). In the M-step, the recursive linear interpolation scheme (Jelinek and Mercer 1980) is used to obtain a smooth probability estimate for each model component (WORDPREDICTOR, TAGGER, and CONSTRUCTOR). The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1, · · · ,zn) where u, z1, · · · ,zn belong to a mixed set of words, POS tags, NTtags, and CONSTRUCTOR actions (u only); and z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0, · · · , n and has been explained in Chelb</context>
<context position="43208" citStr="Chelba (2000)" startWordPosition="6949" endWordPosition="6950">t in cross-validation data, then we gather joint counts for each model component, WORDPREDICTOR, TAGGER, CONSTRUCTOR used to determine interpolation weights. In the M-step, assuming that the count ranges and the corresponding interpolation values for each order are kept fixed to their initial values, the only parameters to be re-estimated using the EM algorithm are the maximal order counts for each model component. The interpolation scheme outlined here is then used to obtain a smooth probability estimate for each model component. 3.2 Follow-up EM As explained in Chelba and Jelinek (2000) and Chelba (2000), for the SLM component a large fraction of the partial parse trees that can be used for assigning probability to the next word do not survive in the synchronous, multi-stack search strategy, thus they are not used in the N-best approximate EM algorithm for the estimation of WORDPREDICTOR to improve its predictive power. To remedy this weakness, we estimate a 1 http://www.codeproject.com/KB/recipes/englishparsing.aspx. 646 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model separate WORD-PREDICTOR (and SEMANTIZER) model using the partial parse trees exploited by t</context>
<context position="54995" citStr="Chelba 2000" startWordPosition="8871" endWordPosition="8872">llion tokens without punctuation, the improvement declines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a 4-gram or 5-gram is nonethele</context>
<context position="58518" citStr="Chelba 2000" startWordPosition="9426" endWordPosition="9427">, and 1.2%, respectively. The OOV rate on the 1.7 million and 13.7 million token check corpora is 0.6% and 1.3%, respectively. The OOV rate on 651 Computational Linguistics Volume 38, Number 3 Table 2 Statistics about the number of types of n-grams (n = 3, 4, 5) on the 44 million, 230 million, and 1.3 billion token corpora. n=3 n=4 n=5 44 M 14,302,355 23,833,023 29,068,173 230 M 51,115,539 94,617,433 120,978,281 1.3 B 224,767,319 481,645,099 660,599,586 the 354k token test corpus is 2.0%. Table 2 lists the statistics about the number of types of n-grams on these three corpora. Similar to SLM (Chelba 2000; Chelba and Jelinek 2000), after the parse undergoes headword percolation and binarization, each model component of WORDPREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the openNLP software2 to parse a large number of sentences in the LDC English Gigaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Penn Treebank used in Chelba and Jelinek (2000) and Chelba (2000). For the 44 and 230 million token corpora, all sentences are automatically parsed and used to initial</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Chelba, C. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. dissertation, The Johns Hopkins University, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>The 36th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>225--231</pages>
<location>Montreal, Quebec, Canada.</location>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Chelba, C. and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. The 36th Annual Conference on Association of Computational Linguistics (ACL), 225–231, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="4052" citStr="Chelba and Jelinek 2000" startWordPosition="588" endWordPosition="591">of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural ques</context>
<context position="5538" citStr="Chelba and Jelinek 2000" startWordPosition="790" endWordPosition="793">c structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pie</context>
<context position="8907" citStr="Chelba and Jelinek 2000" startWordPosition="1297" endWordPosition="1300">igram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the wellknown inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outside algorithm alters Jeline</context>
<context position="43190" citStr="Chelba and Jelinek (2000)" startWordPosition="6944" endWordPosition="6947">likely topics for each document in cross-validation data, then we gather joint counts for each model component, WORDPREDICTOR, TAGGER, CONSTRUCTOR used to determine interpolation weights. In the M-step, assuming that the count ranges and the corresponding interpolation values for each order are kept fixed to their initial values, the only parameters to be re-estimated using the EM algorithm are the maximal order counts for each model component. The interpolation scheme outlined here is then used to obtain a smooth probability estimate for each model component. 3.2 Follow-up EM As explained in Chelba and Jelinek (2000) and Chelba (2000), for the SLM component a large fraction of the partial parse trees that can be used for assigning probability to the next word do not survive in the synchronous, multi-stack search strategy, thus they are not used in the N-best approximate EM algorithm for the estimation of WORDPREDICTOR to improve its predictive power. To remedy this weakness, we estimate a 1 http://www.codeproject.com/KB/recipes/englishparsing.aspx. 646 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model separate WORD-PREDICTOR (and SEMANTIZER) model using the partial parse tr</context>
<context position="55020" citStr="Chelba and Jelinek 2000" startWordPosition="8873" endWordPosition="8876">without punctuation, the improvement declines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a 4-gram or 5-gram is nonetheless still the best way to </context>
<context position="58544" citStr="Chelba and Jelinek 2000" startWordPosition="9428" endWordPosition="9431">espectively. The OOV rate on the 1.7 million and 13.7 million token check corpora is 0.6% and 1.3%, respectively. The OOV rate on 651 Computational Linguistics Volume 38, Number 3 Table 2 Statistics about the number of types of n-grams (n = 3, 4, 5) on the 44 million, 230 million, and 1.3 billion token corpora. n=3 n=4 n=5 44 M 14,302,355 23,833,023 29,068,173 230 M 51,115,539 94,617,433 120,978,281 1.3 B 224,767,319 481,645,099 660,599,586 the 354k token test corpus is 2.0%. Table 2 lists the statistics about the number of types of n-grams on these three corpora. Similar to SLM (Chelba 2000; Chelba and Jelinek 2000), after the parse undergoes headword percolation and binarization, each model component of WORDPREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the openNLP software2 to parse a large number of sentences in the LDC English Gigaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Penn Treebank used in Chelba and Jelinek (2000) and Chelba (2000). For the 44 and 230 million token corpora, all sentences are automatically parsed and used to initialize model parameters, wher</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Chelba, C. and F. Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>J Schalkwyk</author>
<author>T Brants</author>
<author>V Ha</author>
<author>B Harb</author>
<author>W Neveitt</author>
<author>C Parada</author>
<author>P Xu</author>
</authors>
<title>Query language modeling for voice search.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 IEEE Workshop on Spoken Language Technology (SLT),</booktitle>
<pages>127--132</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="79082" citStr="Chelba et al. 2010" startWordPosition="12734" endWordPosition="12737">he Pythagorean theorem breaks down due to the nonconvexity of the set of composite language models. As noted by Dr. Ciprian Chelba in our private communication on March 20th, 2010, “When playing with large data, the model capacity is an important factor to language model performance: The supply of more data needs to be matched by demand on the model side. A simple way to achieve this in n-grams is to increase the order n as much as the data will allow. This of course implies that the computational aspects of storing and serving such models are solved and that it is not a constraint” (see also Chelba et al. 2010). This is also true for our composite language models as justified from the results in Tables 5 and 9: The composite n-gram/m-SLM/PLSA language model has rich features, thus has smaller approximation error than the n-gram, m-SLM, PLSA, or any composite model of two, or their linear combinations. Table 5 shows that the information projection of the empirical distribution for the million and billion token corpora, ˜pM and ˜pB on the composite n-gram/m-SLM/PLSA language model, is closer to the true distribution ˆp. This is reflected approximately by the perplexity results on test data. 6.3 Re-ran</context>
</contexts>
<marker>Chelba, Schalkwyk, Brants, Ha, Harb, Neveitt, Parada, Xu, 2010</marker>
<rawString>Chelba, C., J. Schalkwyk, T. Brants, V. Ha, B. Harb, W. Neveitt, C. Parada, and P. Xu. 2010. Query language modeling for voice search. Proceedings of the 2010 IEEE Workshop on Spoken Language Technology (SLT), 127–132, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and</journal>
<volume>13</volume>
<issue>4</issue>
<pages>319--358</pages>
<contexts>
<context position="5196" citStr="Chen and Goodman 1999" startWordPosition="739" endWordPosition="742">and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a s</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Chen, S. and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language,13(4): 319–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>The 43th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="11526" citStr="Chiang 2005" startWordPosition="1703" endWordPosition="1704">approximate error 633 Computational Linguistics Volume 38, Number 3 and estimation error, thus in Section 6 we conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens, and compare perplexity results with n-grams (n = 3, 4, 5 respectively) on these three corpora under various situations; drastic perplexity reductions are obtained. We explain why the composite language models lead to better predictive capacity than linear interpolation. The proposed composite language models are applied to the task of re-ranking the N-best list from Hiero (Chiang 2005, 2007), a state-of-the-art parsing-based machine translation system; we achieve significantly better translation quality measured by the Bleu score and “readability” of translations. Finally, we draw our conclusions and propose future work in Section 7. The main theme of our approach is “to exploit information, be it syntactic structure or semantic fabric, which involves a fairly high degree of cognition. This is precisely the kind of knowledge that humans naturally and inherently use to process natural language, so it can be reasonably conjectured to represent a key ingredient for success” (</context>
<context position="90322" citStr="Chiang 2005" startWordPosition="14476" endWordPosition="14477">ical non-parametric Bayesian model (Teh 2006; Teh and Jordan 2010) for smoothing fractional counts due to latent variables to handle the sparse data problem in Kneser-Ney’s sense in a principled manner, thus constructing a family of large-scale distributed composite lexical, syntactic, and semantic language models. Finally we will put this family of composite language models into a phrased-based machine translation decoder (Koehn, Och, and Marcu 2003) that produces a lattice of alternative translations/transcriptions or a syntax-based 663 Computational Linguistics Volume 38, Number 3 decoder (Chiang 2005, 2007) that produces a forest of alternatives (such integration would, in the exact case, reside in an extremely difficult complexity class, probably PSPACE-complete) to significantly improve the performance of the state-of-the-art machine translation systems. Appendix A: An Example of Sentence Probability We chose a document from the LDC English Gigaword corpus to show how sentence probability varies when computed by 5-gram, 5-gram/PLSA, and 5-gram/ PLSA+4-SLM/PLSA. The document tag is (XIN ENG 20041126 0168.story). This document’s perplexity computed by 5-gram, 5-gram+PLSA, 5-gram+4-SLM+PLS</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, D. 2005. A hierarchical phrase-based model for statistical machine translation. The 43th Annual Conference on Association of Computational Linguistics (ACL), 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="80753" citStr="Chiang 2007" startWordPosition="12990" endWordPosition="12991">ist generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set. SYSTEM MODEL MEAN (%) 95% CI (%) BASELINE 31.75 0.22 5-GRAM 32.53 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 0.24 5-GRAM/PLSA1 33.01 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 33.32 0.25 by Zhang and colleagues (Zhang, Hildebrand, and Vogel 2006; Zhang 2008; Zhang et al. 2011). The first list was generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set, and the second was generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero (Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and Martin 2008) on a 200 million token corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och 2003) to optimize the Bleu score (Papineni et al. 2002). We conduct two experiments on these two data sets. In the first experiment, we partition the first data set that consists of 100 documents into ten pieces; each piece consists of 10 documents, nine pieces are used as training d</context>
<context position="84077" citStr="Chiang (2007)" startWordPosition="13513" endWordPosition="13514">The composite 5-gram/2-SLM+ 661 Computational Linguistics Volume 38, Number 3 Table 11 Bleu score results for the task of re-ranking the 1,000-best list generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set. SYSTEM MODEL MEAN (%) 95% CI (%) BASELINE 27.59 0.31 5-GRAM 28.10 0.32 5-GRAM/2-SLM+2-GRAM/4-SLM 28.34 0.32 5-GRAM/PLSA1 28.53 0.31 5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 28.78 0.31 2-gram/4-SLM+5-gram/PLSA1 language model gives 1.19 percentage point Bleu score improvement over the baseline and 0.68 percentage point Bleu score improvement over the 5-gram. Chiang (2007) studied the performance of machine translation on Hiero, the Bleu score is 33.31% when n-gram is used to re-rank the N-best list; the Bleu score becomes significantly higher (37.09%) when the n-gram is embedded directly into Hiero’s one pass decoder, however. This is because there is not much diversity in the N-best list. It is expected that putting our composite language into a one-pass decoder should result in much improved Bleu scores. Besides reporting the Bleu scores, we look at the “readability” of translations, similar to the study conducted by Charniak, Knight, and Yamada (2003). The </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, D. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>The Sixth Symposium on Operating Systems Design and Implementation (OSDI), 137–150,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="47273" citStr="Dean and Ghemawat 2004" startWordPosition="7603" endWordPosition="7606"> 2007; Emami, Papineni, and Sorensen 2007). Although all existing research use distributed architectures that follow the client–server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts, and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each n-gram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean and Ghemawat 2004): The corpus is first divided and loaded into a number of clients, and n-gram counts are collected at each client, then the n-gram counts are mapped via hashing and are stored in a number of servers, resulting in exactly one server being contacted per n-gram when computing the language model probability of a sentence. We adopt a similar approach to Brants et al. (2007) and make it suitable to perform iterations of the N-best list approximate EM algorithm (see Figure 4). The corpus is divided and loaded into a number of clients. We use a publicly available parser to parse the sentences in each </context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Dean, J. and S. Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. The Sixth Symposium on Operating Systems Design and Implementation (OSDI), 137–150, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>V Della Pietra S</author>
<author>J Gillett</author>
<author>J Lafferty</author>
<author>H Printz</author>
<author>L Ures 1994</author>
</authors>
<title>Inference and estimation of a long-range trigram model.</title>
<date></date>
<booktitle>Second International Colloquium on Grammatical Inference and Applications (ICGI),</booktitle>
<pages>78--92</pages>
<publisher>Springer-Verlag,</publisher>
<location>Alicante,</location>
<contexts>
<context position="4014" citStr="Pietra et al. 1994" startWordPosition="582" endWordPosition="585">tent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natur</context>
</contexts>
<marker>Pietra, S, Gillett, Lafferty, Printz, 1994, </marker>
<rawString>Della Pietra, S., V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures.1994. Inference and estimation of a long-range trigram model. Second International Colloquium on Grammatical Inference and Applications (ICGI), pages 78–92, Springer-Verlag, Alicante, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>V Della Pietra S</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Pietra, S, Lafferty, 1997</marker>
<rawString>Della Pietra, S., V. Della Pietra and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A., N. Laird, and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of Royal Statistical Society, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Emami</author>
<author>K Papineni</author>
<author>J Sorensen</author>
</authors>
<title>Large-scale distributed language modeling.</title>
<date>2007</date>
<booktitle>The 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>37--40</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="46886" citStr="Emami et al. (2007)" startWordPosition="7539" endWordPosition="7542">stributed Architecture When using very large corpora to train our composite language model, the data and the parameters cannot be stored together on a single machine, so we have to resort to distributed computing. The topic of large-scale distributed language models is relatively new, and existing work is restricted to n-grams only (Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007). Although all existing research use distributed architectures that follow the client–server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts, and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each n-gram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean and Ghemawat 2004): The corpus is first divided and loaded into a number of clients, and n-gram counts are collected at each client, then the n-gram counts are mapped via hashing and are stored in a number of servers, resulting in </context>
</contexts>
<marker>Emami, Papineni, Sorensen, 2007</marker>
<rawString>Emami, A., K. Papineni, and J. Sorensen. 2007. Large-scale distributed language modeling. The 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 37–40, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>T Hofmann</author>
</authors>
<title>Topic-based language models using EM.</title>
<date>1999</date>
<booktitle>The 6th European Conference on Speech Communication and Technology (EUROSPEECH),</booktitle>
<pages>2167--2170</pages>
<contexts>
<context position="4333" citStr="Gildea and Hofmann 1999" startWordPosition="628" endWordPosition="631">T quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, seman</context>
<context position="54965" citStr="Gildea and Hofmann 1999" startWordPosition="8865" endWordPosition="8868">s obtained. On a larger corpus with 284 million tokens without punctuation, the improvement declines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>Gildea, D. and T. Hofmann. 1999. Topic-based language models using EM. The 6th European Conference on Speech Communication and Technology (EUROSPEECH), pages 2167–2170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="5236" citStr="Goodman 2001" startWordPosition="747" endWordPosition="748">ge regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties</context>
<context position="53974" citStr="Goodman (2001)" startWordPosition="8718" endWordPosition="8719">r and Wu is computationally expensive. Both methods use the first-stage N-best list approximate EM to extract headwords, thus the complexity is at the same order at this stage; at second stage, however, where we use the follow-up EM, they use the maximum entropy approach. The maximum entropy approach is more expensive, mainly in feature expectation and normalization as well as optimization (such as iterative scaling or the quasi Newton method); ours is quite simple, which is expected relative to frequency estimates with proper smoothing. The highest reported perplexity reductions are those by Goodman (2001), where the author examines the techniques of caching, clustering, higher-order n-grams, skipping models, and sentence-mixture models in various combinations (mainly linear interpolation). The author compares to the baseline of a Katz smoothed trigram with no count cutoffs. On a small training corpus with 100k tokens, a 50% perplexity reduction (1 bit improvement) is obtained. On a larger corpus with 284 million tokens without punctuation, the improvement declines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this s</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Goodman, J. 2001. A bit of progress in language modeling. Computer Speech and Language, 15(4):403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Halevy</author>
<author>P Norvig</author>
<author>F Pereira</author>
</authors>
<title>The unreasonable effectiveness of data.</title>
<date>2009</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Halevy, Norvig, Pereira, 2009</marker>
<rawString>Halevy, A., P. Norvig, and F. Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2):8–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd edition.</title>
<date>2009</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Hastie, T., R. Tibshirani and J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd edition. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="17988" citStr="Hofmann 2001" startWordPosition="2792" endWordPosition="2793"> is a distinguished POS tag for (s)/(/s) respectively, ((s),TOP) is the only allowed head, and ((/s),TOP’) is the head of any constituent that dominates (/s) but not (s). In Figure 1, at the time just after the word as is generated, the exposed headwords are “(s) SB, show np, has vbz.” The subsequent model actions are: “POStag as, null, predict its, POStag its, null, predict host, POStag host, adjoin-right-np, adjoin-left-pp, adjoin-leftpp, null, predict a, · · · .” 635 Computational Linguistics Volume 38, Number 3 Figure 1 A complete parse tree by the structured language model. A PLSA model (Hofmann 2001) is a generative probabilistic model of worddocument co-occurrences using the bag-of-words assumption described as follows: • Choose a document d with probability p(d). • SEMANTIZER selects a semantic class g E 9 with probability p(gJd) where 9 denotes the set of topics. • WORD-PREDICTOR picks a word w E V with probability p(wJg). Because only one pair of (d, w) is being observed, the joint probability model is a mixture of log-linear models with the expression p(d,w) = p(d) Eg p(wJg)p(gJd). Typically, the number of documents and the vocabulary size are much larger than the size of latent sema</context>
<context position="50436" citStr="Hofmann (2001)" startWordPosition="8145" endWordPosition="8146">lgorithm to re-estimate WORD-PREDICTOR. 4. Using the Model for Testing When a language model is used in one-pass decoders of speech recognition and phrased-based MT systems to guide the search, the search space is organized as a prefix tree and operates left to right, thus we need to know the language model probability at the word level given by Equation (30) one word at a time. Because a document of the test data is not contained in the original training corpus, to compute the language model probability assignment for word wk+1 we use a “fold-in” heuristic approach similar to the one used in Hofmann (2001): The parameters corresponding to SEMANTIZER, p(g|d), are re-estimated by maximizing the probability of word subsequence seen so far—that is, a pseudo-document ˜dk = (Wk, S), where S is the set of previous sentences of a document in test data—while holding the other parameters fixed. Wang et al. (2005b) use on-line gradient ascent to re-estimate these parameters. We use three methods, onestep on-line EM, on-line EM with fixed learning rate, and batch EM, to re-estimate these parameters. Both one-step on-line EM and on-line EM with fixed learning rate use Equation (32) with γ set to 1 |˜dk|+1 a</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Hofmann, T. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Up from trigrams! The struggle for improved language models.</title>
<date>1991</date>
<booktitle>Second European Conference on Speech Communication and Technology (EUROSPEECH),</booktitle>
<pages>1037--1040</pages>
<location>Genove, Italy.</location>
<contexts>
<context position="3097" citStr="Jelinek 1991" startWordPosition="440" endWordPosition="441">rnment In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small impr</context>
</contexts>
<marker>Jelinek, 1991</marker>
<rawString>Jelinek, F. 1991. Up from trigrams! The struggle for improved language models. Second European Conference on Speech Communication and Technology (EUROSPEECH), pages 1037–1040, Genove, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13874" citStr="Jelinek 1998" startWordPosition="2072" endWordPosition="2073">a graph, then we have a Bayesian network (Lauritzen 1996). Broadly speaking, however, the recursive factorization can refer to a representation more complicated than a graph with a fixed set of nodes and edges—for example, PCFG and SLM are examples of directed MRFs whose parse tree structure is a random object that can’t be described as a Bayesian network (McAllester, Collins, and Pereira 2004). A key difference between directed MRFs and undirected MRFs is that a directed MRF requires many local normalization constraints whereas an undirected MRF has a global normalization factor. The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is essentially a WORD-PREDICTOR, that is, given its entire document history, it predicts the next word wk+1 E V based on the last n–1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, · · · , wk and V denotes the vocabulary. The SLM proposed in Chelba and Jelinek (1998, 2000) and Chelba (2000) uses syntactic information beyond the regular n-gram models to capture sentence-level long-range 634 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model dependencies. The SLM is based on statistical parsing techniques t</context>
<context position="24374" citStr="Jelinek 1998" startWordPosition="3828" endWordPosition="3829">n and the number of parse trees grows faster than exponentially with sentence length; Wang et al. (2006) have derived a generalized inside– outside algorithm by applying the standard EM algorithm and considering the auxiliary function � � � � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) (10) Q(p , p) = Gl Tl d∈D l The complexity of this algorithm is sixth order (sentence length), however; thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba 1999; Jelinek 2004). 3.1 N-best List Approximate EM Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, max �         TSN L(D,p,T�N) = ��  � d∈D  max Pp(Wl,Tl,Gl|d)    (11) T�l N∈T�N l Gl Tl∈T�lN,||T�lN||=N where T&apos;lN is a set of N parse trees for sentence Wl in document d, ||· ||denotes the cardinality, and T�N is a collection of T&apos;N for sentences over entire corpus D. The N-best list approximate EM involves two steps: 1. N-bes</context>
<context position="30532" citStr="Jelinek (1998" startWordPosition="4922" endWordPosition="4923">s to store WORD-PREDICTOR’s parameters. We can either find both the N-best parses for each sentence and N-best topics for each document simultaneously or separately. The latter is much preferred, because the first case is much more computationally expensive. To extract the N-best topics, we run an EM algorithm for a PLSA model on training corpus D, then keep the N most likely topics (denoted as Gd) according to the values of p(g|d); the rest of the topics are purged. To extract the N-best parse trees, we adopt a synchronous, multi-stack search strategy that is similar to the one in Chelba and Jelinek (1998, 2000) and Chelba (2000), which involves a set of stacks storing partial parses of the most likely ones for a given prefix Wk and the less probable parses are purged. Each stack contains 641 Computational Linguistics Volume 38, Number 3 hypotheses (partial parses) that have been constructed by the same number of WORDPREDICTOR and the same number of CONSTRUCTOR operations. The hypotheses in each stack are ranked according to the log(Pp (Wk, Tk |d)) score with the highest on top, where Pp(Wk,Tk |d) = EGk Pp(Wk, Tk, Gk|d) and the Wk, Tk, Gk denote the joint sequence of prefix Wk = w0, w1 · · · ,</context>
<context position="38293" citStr="Jelinek (1998" startWordPosition="6209" endWordPosition="6210">f sentence Wl in document d times the conditional distribution Pp(Tl|Wl, d) = Pp(Tl,Wl|d)/ ETl∈Tl Pp(Tl, Wl|d)—that is, Pp(Tl|Wl, d)#(twh−1 −m.tag, Wl, Tl, d) and Pp(Tl|Wl, d)#(ah−1 −m, Wl, Tl, d), respectively. When only SLM is considered, the expected count for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR, over parse Tl of sentence Wl in document d is the real count that appeared in parse Tl of sentence Wl in document d 644 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model times the posterior probability Pp(Tl|Wl, d), as is done in Chelba and Jelinek (1998, 2000) and Chelba (2000). In the M-step, the recursive linear interpolation scheme (Jelinek and Mercer 1980) is used to obtain a smooth probability estimate for each model component (WORDPREDICTOR, TAGGER, and CONSTRUCTOR). The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1, · · · ,zn) where u, z1, · · · ,zn belong to a mixed set of words, POS tags, NTtags, and CONSTRUCTOR actions (u only); and z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0, · · · , n and ha</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>Jelinek, F. 1998. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Stochastic analysis of structured language modeling.</title>
<date>2004</date>
<booktitle>Mathematical Foundations of Speech and Language Processing,</booktitle>
<pages>37--72</pages>
<editor>In M. Johnson, S. Khudanpur, M. Ostendorf, and R. Rosenfeld, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="4115" citStr="Jelinek 2004" startWordPosition="600" endWordPosition="601">ot observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex</context>
<context position="9444" citStr="Jelinek 2004" startWordPosition="1381" endWordPosition="1382"> in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outside algorithm alters Jelinek’s inside–outside algorithm with modular modification and has the same sixth order of sentence-length time complexity. Unfortunately, there are no experimental results reported. In this article, we study the same composite n-gram, SLM, and PLSA model under the directed MRF framework as in Wang et al. (2006). The composite n-gram/ SLM/PLSA language model under the directed MRF paradigm is first introduced in Section 2. In Section 3, instead of using the sixth order generalized inside–outside algorithm proposed in Wang et al. (2006)</context>
<context position="17330" citStr="Jelinek (2004)" startWordPosition="2682" endWordPosition="2683">l, the indices of the current exposed headwords h−3, h−4, · · · are increased by 1, and these headwords together with h−1 or h−2 become the new exposed headwords. Once the CONSTRUCTOR hits NULL, the headword indexing and current parse structure remain as they are, and the CONSTRUCTOR passes control to the WORD-PREDICTOR. SLM is thus essentially a generalization of a shift-reduce parser (Aho and Ullman 1972) with adjoin corresponding to reduce and predict to shift. (See a detailed description about SLM in Chelba and Jelinek [1998, 2000]; Chelba [2000]; Jelinek [2004]). As an example taken from Jelinek (2004), Figure 1 shows a complete parse where SB/SE is a distinguished POS tag for (s)/(/s) respectively, ((s),TOP) is the only allowed head, and ((/s),TOP’) is the head of any constituent that dominates (/s) but not (s). In Figure 1, at the time just after the word as is generated, the exposed headwords are “(s) SB, show np, has vbz.” The subsequent model actions are: “POStag as, null, predict its, POStag its, null, predict host, POStag host, adjoin-right-np, adjoin-left-pp, adjoin-leftpp, null, predict a, · · · .” 635 Computational Linguistics Volume 38, Number 3 Figure 1 A complete parse tree by </context>
<context position="24302" citStr="Jelinek 2004" startWordPosition="3816" endWordPosition="3817">yntactic, Semantic, and Lexical Language Model semantic content are hidden and the number of parse trees grows faster than exponentially with sentence length; Wang et al. (2006) have derived a generalized inside– outside algorithm by applying the standard EM algorithm and considering the auxiliary function � � � � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) (10) Q(p , p) = Gl Tl d∈D l The complexity of this algorithm is sixth order (sentence length), however; thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba 1999; Jelinek 2004). 3.1 N-best List Approximate EM Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, max �         TSN L(D,p,T�N) = ��  � d∈D  max Pp(Wl,Tl,Gl|d)    (11) T�l N∈T�N l Gl Tl∈T�lN,||T�lN||=N where T&apos;lN is a set of N parse trees for sentence Wl in document d, ||· ||denotes the cardinality, and T�N is a collection of T&apos;N for sentences over enti</context>
<context position="34508" citStr="Jelinek 2004" startWordPosition="5583" endWordPosition="5584">nd Lexical Language Model � � � Pp(Tl|Wl, d)#(twh−1 −m.tag, Wl, Tl, d) (20) p�(t|wh−1 l Tl∈TlN∈TN −m.tag) ∝ d∈D � � � Pp(Tl|Wl,d)#(ah−1 p�(a|h−1 l Tl∈TlN∈TN −m,Wl,Tl,d) (21) −m)) ∝ d∈D � � � E Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d) (22) p&apos;(g|d) ∝ l Gl Tl∈TlN∈TN d∈D In the E-step, we use Equations (19)–(22) to compute the expected count of each model parameter over sentence Wl in document d in the training corpus D. In the full case where the number of parse trees grows faster than exponentially with sentence length, we use Jelinek-style recursive formulas in the generalized inside–outside algorithm (Jelinek 2004) to handle the tree structure and describe the weighted forest of possible derivations (Wang et al. 2006). In the N-best list case considered in this paper, however, we just enumerate each parse tree in the N-best list and compute the expected posterior count for each parse tree. For the WORD-PREDICTOR and the SEMANTIZER, we use Equations (19) and (22) and note that there is a sum over semantic annotation sequence Gl where the number of possible semantic annotation sequences is exponential. We use forward–backward recursive formulas reminiscent of those in hidden Markov models to compute the e</context>
</contexts>
<marker>Jelinek, 2004</marker>
<rawString>Jelinek, F. 2004. Stochastic analysis of structured language modeling. In M. Johnson, S. Khudanpur, M. Ostendorf, and R. Rosenfeld, editors, Mathematical Foundations of Speech and Language Processing, pages 37–72, Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>The dawn of statistical</title>
<date>2009</date>
<journal>ASR and MT. Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="74577" citStr="Jelinek 2009" startWordPosition="12009" endWordPosition="12010"> 44 million token corpus instead. Table 9 shows the perplexity results. We can see that as we increase the order for n-gram and m-SLM from n = 3 and m = 2 to n = 4 and m = 3, the composite language models become better and have up to 5% perplexity reductions; when we increase the order for n-gram and m-SLM to n = 5 and m = 4, however, the composite language models become worse and slightly overfit the data even if we use linear interpolation smoothing, and there are no further perplexity reductions. To summarize, as a sub-problem for MT and speech recognition under the sourcechannel paradigm (Jelinek 2009), language modeling is a data-rich and feature-rich density estimation problem with Kullback-Leibler divergence as a cost function, and there is always a trade-off between approximation error and estimation error (Barron and Sheu 1991), reminiscent of the “bias-variance” trade-off for a regression problem with a quadratic cost function (Hastie, Tibshirani, and Friedman 2009). Figure 5 explains the perplexity results in Tables 3 and 5 from a model selection point of view. Let pˆ denote the true (but unknown) distribution of natural language, its information projection to n-grams is the minimum </context>
</contexts>
<marker>Jelinek, 2009</marker>
<rawString>Jelinek, F. 2009. The dawn of statistical ASR and MT. Computational Linguistics, 35(4):483–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>C Chelba</author>
</authors>
<title>Putting language into language modeling.</title>
<date>1999</date>
<booktitle>Sixth European Conference on Speech Communication and Technology (EUROSPEECH), Keynote Paper 1,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="3123" citStr="Jelinek and Chelba 1999" startWordPosition="442" endWordPosition="445">rdance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, bu</context>
<context position="24287" citStr="Jelinek and Chelba 1999" startWordPosition="3812" endWordPosition="3815"> A Scalable Distributed Syntactic, Semantic, and Lexical Language Model semantic content are hidden and the number of parse trees grows faster than exponentially with sentence length; Wang et al. (2006) have derived a generalized inside– outside algorithm by applying the standard EM algorithm and considering the auxiliary function � � � � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) (10) Q(p , p) = Gl Tl d∈D l The complexity of this algorithm is sixth order (sentence length), however; thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba 1999; Jelinek 2004). 3.1 N-best List Approximate EM Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, max �         TSN L(D,p,T�N) = ��  � d∈D  max Pp(Wl,Tl,Gl|d)    (11) T�l N∈T�N l Gl Tl∈T�lN,||T�lN||=N where T&apos;lN is a set of N parse trees for sentence Wl in document d, ||· ||denotes the cardinality, and T�N is a collection of T&apos;N for sent</context>
</contexts>
<marker>Jelinek, Chelba, 1999</marker>
<rawString>Jelinek, F. and C. Chelba. 1999. Putting language into language modeling. Sixth European Conference on Speech Communication and Technology (EUROSPEECH), Keynote Paper 1, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data. In</title>
<date>1980</date>
<booktitle>Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<editor>E. Gelsema and L. Kanal, editors,</editor>
<publisher>North Holland Publishers,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="5221" citStr="Jelinek and Mercer 1980" startWordPosition="743" endWordPosition="746">aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in captu</context>
<context position="10439" citStr="Jelinek and Mercer 1980" startWordPosition="1531" endWordPosition="1534">). The composite n-gram/ SLM/PLSA language model under the directed MRF paradigm is first introduced in Section 2. In Section 3, instead of using the sixth order generalized inside–outside algorithm proposed in Wang et al. (2006), we show how to train this composite model via an N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power. We prove the convergence of the N-best list approximate EM algorithm. To resolve the data sparseness problem, we generalize Jelinek and Mercer’s recursive mixing scheme for Markov source (Jelinek and Mercer 1980) to a mixture of Markov chains. To handle large-scale corpora up to a billion tokens, we demonstrate how to implement these algorithms under a distributed computing environment and how to store this language model on a supercomputer. In Section 4, we describe how to use the model for testing. Related works are then summarized and compared in Section 5. Because language modeling is a data-rich and featurerich density estimation problem, there is always a trade-off between approximate error 633 Computational Linguistics Volume 38, Number 3 and estimation error, thus in Section 6 we conduct compr</context>
<context position="38402" citStr="Jelinek and Mercer 1980" startWordPosition="6223" endWordPosition="6226">Pp(Tl, Wl|d)—that is, Pp(Tl|Wl, d)#(twh−1 −m.tag, Wl, Tl, d) and Pp(Tl|Wl, d)#(ah−1 −m, Wl, Tl, d), respectively. When only SLM is considered, the expected count for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR, over parse Tl of sentence Wl in document d is the real count that appeared in parse Tl of sentence Wl in document d 644 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model times the posterior probability Pp(Tl|Wl, d), as is done in Chelba and Jelinek (1998, 2000) and Chelba (2000). In the M-step, the recursive linear interpolation scheme (Jelinek and Mercer 1980) is used to obtain a smooth probability estimate for each model component (WORDPREDICTOR, TAGGER, and CONSTRUCTOR). The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1, · · · ,zn) where u, z1, · · · ,zn belong to a mixed set of words, POS tags, NTtags, and CONSTRUCTOR actions (u only); and z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0, · · · , n and has been explained in Chelba and Jelinek (1998, 2000) and Chelba (2000). The WORD-PREDICTOR is, however, a cond</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, F. and R. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In E. Gelsema and L. Kanal, editors, Pattern Recognition in Practice, pages 381–397. North Holland Publishers, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J Martin</author>
</authors>
<title>Speech and Language Processing, 2nd edition. Prentice Hall, Upper Saddle River,</title>
<date>2008</date>
<location>NJ.</location>
<contexts>
<context position="13901" citStr="Jurafsky and Martin 2008" startWordPosition="2074" endWordPosition="2077">we have a Bayesian network (Lauritzen 1996). Broadly speaking, however, the recursive factorization can refer to a representation more complicated than a graph with a fixed set of nodes and edges—for example, PCFG and SLM are examples of directed MRFs whose parse tree structure is a random object that can’t be described as a Bayesian network (McAllester, Collins, and Pereira 2004). A key difference between directed MRFs and undirected MRFs is that a directed MRF requires many local normalization constraints whereas an undirected MRF has a global normalization factor. The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is essentially a WORD-PREDICTOR, that is, given its entire document history, it predicts the next word wk+1 E V based on the last n–1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, · · · , wk and V denotes the vocabulary. The SLM proposed in Chelba and Jelinek (1998, 2000) and Chelba (2000) uses syntactic information beyond the regular n-gram models to capture sentence-level long-range 634 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model dependencies. The SLM is based on statistical parsing techniques that allow syntactic analysi</context>
<context position="55286" citStr="Jurafsky and Martin (2008" startWordPosition="8914" endWordPosition="8917">rehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a 4-gram or 5-gram is nonetheless still the best way to do language modeling.” To verify whether this is true, we have trained our language models using three different training sets: one has 44 million tokens, another has 230 million tokens, and the third has 1.3 billion tokens. An independent test set with 354k tokens </context>
<context position="80918" citStr="Jurafsky and Martin 2008" startWordPosition="13009" endWordPosition="13012">M 32.53 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 0.24 5-GRAM/PLSA1 33.01 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 33.32 0.25 by Zhang and colleagues (Zhang, Hildebrand, and Vogel 2006; Zhang 2008; Zhang et al. 2011). The first list was generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set, and the second was generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero (Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and Martin 2008) on a 200 million token corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och 2003) to optimize the Bleu score (Papineni et al. 2002). We conduct two experiments on these two data sets. In the first experiment, we partition the first data set that consists of 100 documents into ten pieces; each piece consists of 10 documents, nine pieces are used as training data to optimize the Bleu score (Papineni et al. 2002) by MERT (Och 2003), and the remaining single piece is used to re-rank the 1,000-best list and obtain the Bleu s</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Jurafsky, D. and J. Martin. 2008. Speech and Language Processing, 2nd edition. Prentice Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khudanpur</author>
<author>J Wu</author>
</authors>
<title>Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="7531" citStr="Khudanpur and Wu (2000)" startWordPosition="1096" endWordPosition="1099">atistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods (Mark, Miller, and Grenander 1996; Rosenfeld 2000b; Rosenfeld, Chen, and Zhu 2001) would have to be used. One way to overcome the first hurdle is to use a preprocessing tool to extract hidden features (e.g., Rosenfeld [1996] used mutual information clustering method to find word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba and Jelinek’s structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigrams through a maximum conditional entropy approach to form a syntactic, semantic, and lexical language model. Wang and colleagues (Wang et al. 2005a; Wang, Schuurmans, and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn’t overcome the second hurdle. The third method is direc</context>
<context position="52577" citStr="Khudanpur and Wu (2000)" startWordPosition="8501" endWordPosition="8504">methods and the initial values. For example, for batch EM, if we set initial values to be those obtained by using ˜dk−1 = (Wk−1,S) and trained by batch EM, we obtain worse perplexity results. Table 8 in Section 6.2 gives perplexity results that use these three methods to re-estimate the parameters of the SEMANTIZER, where the on-line EM with fixed learning rate not only has the cheapest computational cost but also leads to the highest perplexity reductions. 5. Related Work Besides the work by Wang et al. (2005b, 2006) that was discussed in the Introduction, the closest work to ours is that by Khudanpur and Wu (2000) where the authors used p(g|˜dk) = γ the pseudo-document up to the previous word 649 Computational Linguistics Volume 38, Number 3 SLM and a word clustering model to extract relevant grammatical and semantic features, then integrated these features with n-grams by a maximum conditional entropy approach. Our composite language model is a generative model, all features play important roles in the EM iterations to allow maximal order events for WORD-PREDICTOR to appear; in Khudanpur and Wu (2000), however, the counts for all events are fixed after feature extraction from SLM and word clustering a</context>
</contexts>
<marker>Khudanpur, Wu, 2000</marker>
<rawString>Khudanpur, S. and J. Wu. 2000. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech and Language, 14(4):355–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>The 20th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<marker>Kneser, Ney, 1995</marker>
<rawString>Kneser, R. and H. Ney. 1995. Improved backing-off for m-gram language modeling. The 20th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 181–184, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>The Computational Linguistics Volume 38, Number</booktitle>
<volume>3</volume>
<contexts>
<context position="82871" citStr="Koehn (2004)" startWordPosition="13330" endWordPosition="13331">am. We are not able to further improve Bleu score when we use either the 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA2 or 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA3. This is because there is not much diversity on the 1,000-best list, and essentially only 20 ∼ 30 distinct sentences are in the 1,000-best list. In the second experiment, we used the first data set as training data to optimize the Bleu score by MERT, then the second data set is used to re-rank the 1,000-best list and obtain the Bleu score. To obtain the confidence interval of the Bleu score, we resort to the bootstrap resampling described by Koehn (2004). We randomly select 10 re-ranked documents from the 20 re-ranked documents in the second data set with replacement. We draw the translation results of the 10 documents and compute the Bleu score. We repeat this procedure 1,000 times. When we compute the 95% confidence interval, we drop the top 25 and bottom 25 Bleu scores, and only consider the range of 26th to 975th Bleu scores. Table 11 shows the Bleu scores. These statistics are computed with different language models, but on the same chosen test sets. The 5-gram gives 0.51 percentage point Bleu score improvement over the baseline. The com</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, P. 2004. Statistical significance tests for machine translation evaluation. The Computational Linguistics Volume 38, Number 3</rawString>
</citation>
<citation valid="true">
<date>2004</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="17288" citStr="[2004]" startWordPosition="2676" endWordPosition="2676"> is percolated up by one tree level, the indices of the current exposed headwords h−3, h−4, · · · are increased by 1, and these headwords together with h−1 or h−2 become the new exposed headwords. Once the CONSTRUCTOR hits NULL, the headword indexing and current parse structure remain as they are, and the CONSTRUCTOR passes control to the WORD-PREDICTOR. SLM is thus essentially a generalization of a shift-reduce parser (Aho and Ullman 1972) with adjoin corresponding to reduce and predict to shift. (See a detailed description about SLM in Chelba and Jelinek [1998, 2000]; Chelba [2000]; Jelinek [2004]). As an example taken from Jelinek (2004), Figure 1 shows a complete parse where SB/SE is a distinguished POS tag for (s)/(/s) respectively, ((s),TOP) is the only allowed head, and ((/s),TOP’) is the head of any constituent that dominates (/s) but not (s). In Figure 1, at the time just after the word as is generated, the exposed headwords are “(s) SB, show np, has vbz.” The subsequent model actions are: “POStag as, null, predict its, POStag its, null, predict host, POStag host, adjoin-right-np, adjoin-left-pp, adjoin-leftpp, null, predict a, · · · .” 635 Computational Linguistics Volume 38, N</context>
</contexts>
<marker>2004</marker>
<rawString>2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>The Human Language Technology Conference (HLT),</booktitle>
<pages>48--54</pages>
<location>Edmonton, Alberta, Canada.</location>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., F. Och, and D. Marcu. 2003. Statistical phrase-based translation. The Human Language Technology Conference (HLT), pages 48–54, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="8534" citStr="Lari and Young 1990" startWordPosition="1240" endWordPosition="1243"> latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn’t overcome the second hurdle. The third method is directed Markov random field (Wang et al. 2005b) that overcomes both weaknesses in the maximum entropy approach. Wang et al. used this approach to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the wellknown inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b)</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Lari, K. and S. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
<author>R Rosenfeld</author>
<author>S Roukos</author>
</authors>
<title>Trigger-based language models: A maximum entropy approach.</title>
<date>1993</date>
<booktitle>The 18th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), II:45–48,</booktitle>
<location>Minneapolis, MN.</location>
<contexts>
<context position="6216" citStr="Lau et al. (1993)" startWordPosition="897" endWordPosition="900">ned using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997), Lau et al. (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum entropy is nothing but maximum likelihood estimation for undirected Markov random fields (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997). As stated in Wang et al. (2005b), however, there are two weaknesses with maximum entropy approach. The first weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden 632 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model information in natural lan</context>
</contexts>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>Lau, R., R. Rosenfeld, and S. Roukos. 1993. Trigger-based language models: A maximum entropy approach. The 18th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), II:45–48, Minneapolis, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lauritzen</author>
</authors>
<title>Graphical Models.</title>
<date>1996</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="13319" citStr="Lauritzen 1996" startWordPosition="1985" endWordPosition="1986">omposite n-gram/SLM/PLSA Language Model Let X denote a set of random variables (Xτ)τ∈r taking values in a (discrete) probability space (Xτ)τ∈r, where Γ is a finite set of states. We define a (discrete) directed Markov random field to be a probability distribution P, which admits a recursive factorization if there exist non-negative functions, κτ(·, ·),τ E Γ defined on Xτ x Xpa(τ), such that Exτ κτ(xτ,xpa(τ)) = 1 and P has density p(x) = 11 κτ(xτ, xpa(τ)) (1) τ∈r Here pa(τ) denotes the set of parent states of τ. If the recursive factorization refers to a graph, then we have a Bayesian network (Lauritzen 1996). Broadly speaking, however, the recursive factorization can refer to a representation more complicated than a graph with a fixed set of nodes and edges—for example, PCFG and SLM are examples of directed MRFs whose parse tree structure is a random object that can’t be described as a Bayesian network (McAllester, Collins, and Pereira 2004). A key difference between directed MRFs and undirected MRFs is that a directed MRF requires many local normalization constraints whereas an undirected MRF has a global normalization factor. The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is</context>
</contexts>
<marker>Lauritzen, 1996</marker>
<rawString>Lauritzen, S. 1996. Graphical Models. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>D Yarowsky</author>
<author>K Knight</author>
<author>C Callison-Burch</author>
<author>N Habash</author>
<author>T Mitamura</author>
</authors>
<title>MINDS Workshops Machine Translation Working Group Final Report. Available at http://www-nlpir.nist.gov/MINDS/ FINAL/MT.web.pdf.</title>
<date>2006</date>
<contexts>
<context position="3641" citStr="Lavie et al. 2006" startWordPosition="524" endWordPosition="527">nd it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of nat</context>
</contexts>
<marker>Lavie, Yarowsky, Knight, Callison-Burch, Habash, Mitamura, 2006</marker>
<rawString>Lavie, A., D. Yarowsky, K. Knight, C. Callison-Burch, N. Habash, and T. Mitamura. 2006. MINDS Workshops Machine Translation Working Group Final Report. Available at http://www-nlpir.nist.gov/MINDS/ FINAL/MT.web.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>C Dyer</author>
</authors>
<date>2010</date>
<booktitle>Data-Intensive Text Processing with MapReduce.</booktitle>
<publisher>Morgan and Claypool Publishers.</publisher>
<marker>Lin, Dyer, 2010</marker>
<rawString>Lin, J. and C. Dyer. 2010. Data-Intensive Text Processing with MapReduce. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mark</author>
<author>M Miller</author>
<author>U Grenander</author>
</authors>
<title>Constrained stochastic language models,</title>
<date>1996</date>
<booktitle>Image Models and Their Speech Model Cousins,</booktitle>
<pages>131--137</pages>
<editor>In S. Levinson and L. Shepp, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Mark, Miller, Grenander, 1996</marker>
<rawString>Mark, K., M. Miller, and U. Grenander. 1996. Constrained stochastic language models, In S. Levinson and L. Shepp, editors, Image Models and Their Speech Model Cousins, pages 131–137, Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
<author>M Collins</author>
<author>F Pereira</author>
</authors>
<title>Case-factor diagrams for structured probabilistic modeling.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>382--391</pages>
<location>Banff, Canada.</location>
<marker>McAllester, Collins, Pereira, 2004</marker>
<rawString>McAllester, D., M. Collins, and F. Pereira. 2004. Case-factor diagrams for structured probabilistic modeling. Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI), pages 382–391, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Norvig</author>
</authors>
<title>Statistical learning as the ultimate agile development tool.</title>
<date>2008</date>
<booktitle>ACM 17th Conference on Information and Knowledge Management (CIKM) Industry Event,</booktitle>
<location>Napa Valley, CA.</location>
<contexts>
<context position="88488" citStr="Norvig 2008" startWordPosition="14195" endWordPosition="14196">, p. 45), “Since Banko and Brill’s pioneering work almost a decade ago (Banko and Brill 2001), it has been widely observed that the effectiveness of statistical natural language processing (NLP) techniques is highly susceptible to the data size used to develop them. As empirical studies have repeatedly shown that simple algorithms can often outperform their more complicated counterparts in wide varieties of NLP applications with large data sets, many have come to believe that it is the size of data, not the sophistication of the algorithms, that ultimately play the central role in modern NLP (Norvig 2008).” It is true that ‘the more the data, the better the result,’ a dictum recently reiterated in a somewhat stronger form in Halevy, Norvig, and Pereira (2009), but care needs to be taken here. As we explained in the last paragraph of Section 6.2, after we increase the size of data, we should also increase the complexity of the model in order to achieve best results. For language modeling in particular, because the expressive power of simple n-grams is rather limited, it is worthwhile to exploit latent semantic information and syntactic structure that constrain the generation of natural language</context>
</contexts>
<marker>Norvig, 2008</marker>
<rawString>Norvig, P. 2008. Statistical learning as the ultimate agile development tool. ACM 17th Conference on Information and Knowledge Management (CIKM) Industry Event, Napa Valley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>The 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="81074" citStr="Och 2003" startWordPosition="13039" endWordPosition="13040">Vogel 2006; Zhang 2008; Zhang et al. 2011). The first list was generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set, and the second was generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero (Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and Martin 2008) on a 200 million token corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och 2003) to optimize the Bleu score (Papineni et al. 2002). We conduct two experiments on these two data sets. In the first experiment, we partition the first data set that consists of 100 documents into ten pieces; each piece consists of 10 documents, nine pieces are used as training data to optimize the Bleu score (Papineni et al. 2002) by MERT (Och 2003), and the remaining single piece is used to re-rank the 1,000-best list and obtain the Bleu score. The cross-validation process is then repeated 10 times (the folds), with each of the 10 pieces used exactly once as the validation data. The 10 result</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. 2003. Minimum error rate training in statistical machine translation. The 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical machine translation: Foundations and recent advances. Presentation at MT-Summit.</title>
<date>2005</date>
<note>http://www.mt-archive.info/ MTS-2005-och.pdf</note>
<contexts>
<context position="3150" citStr="Och 2005" startWordPosition="448" endWordPosition="449">tection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally so</context>
</contexts>
<marker>Och, 2005</marker>
<rawString>Och, F. 2005. Statistical machine translation: Foundations and recent advances. Presentation at MT-Summit. http://www.mt-archive.info/ MTS-2005-och.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>The 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="81124" citStr="Papineni et al. 2002" startWordPosition="13046" endWordPosition="13049">011). The first list was generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set, and the second was generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero (Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and Martin 2008) on a 200 million token corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och 2003) to optimize the Bleu score (Papineni et al. 2002). We conduct two experiments on these two data sets. In the first experiment, we partition the first data set that consists of 100 documents into ten pieces; each piece consists of 10 documents, nine pieces are used as training data to optimize the Bleu score (Papineni et al. 2002) by MERT (Och 2003), and the remaining single piece is used to re-rank the 1,000-best list and obtain the Bleu score. The cross-validation process is then repeated 10 times (the folds), with each of the 10 pieces used exactly once as the validation data. The 10 results from the folds then can be averaged (or otherwis</context>
<context position="85884" citStr="Papineni et al. 2002" startWordPosition="13786" endWordPosition="13789">tax-based translation model proposed by Yamada and Knight (2001) to rescore a tree-to-string translation forest, whereas we use only our language model for N-best list re-ranking. Also, the same study (Charniak et al. 2003) found that the outputs produced using the n-grams received higher scores from Bleu; ours did not. The difference between human judgments and Bleu scores indicates that closer agreement may be possible by incorporating syntactic structure and semantic information into the Bleu score evaluation. For example, semantically similar words like insure and ensure as in Bleu paper (Papineni et al. 2002) should be substituted in the formula, and there is a weight to measure the goodness of syntactic structure. This modification will lead to a better metric and such information can be provided by our composite language models. Table 12 Results of “readability” evaluation on 919 translated sentences of 100 documents. P = perfect; S = only semantically correct; G = only grammatically correct; W = wrong. SYSTEM MODEL P S G W BASELINE 95 398 20 406 5-GRAM 122 406 24 367 5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 151 425 33 310 662 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Lang</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. The 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Formal grammar and information theory: Together again?</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences,</journal>
<volume>358</volume>
<issue>1769</issue>
<contexts>
<context position="4528" citStr="Pereira 2000" startWordPosition="654" endWordPosition="655"> that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most</context>
</contexts>
<marker>Pereira, 2000</marker>
<rawString>Pereira, F. 2000. Formal grammar and information theory: Together again? Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences, 358(1769):1239–1253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top–down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="4079" citStr="Roark 2001" startWordPosition="594" endWordPosition="595"> test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is wheth</context>
<context position="5564" citStr="Roark 2001" startWordPosition="796" endWordPosition="797">y, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, </context>
<context position="55048" citStr="Roark 2001" startWordPosition="8879" endWordPosition="8880">lines to 38%; we assume that this improvement shrinks to 30% when compared with 4-gram as the baseline. 6. Experimental Results In this section, we first explain the experimental set-up for our experiments, we then show comprehensive perplexity results in various situations, and we end by reporting the results when we apply the composite language model to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 6.1 Experimental Set-up In previous work (Gildea and Hofmann 1999; Bellegarda 2000; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001), all complex language models have been trained on relatively small data sets. There is the impression that complex language models only lead to better results than n-grams on small training corpora. For example, Jurafsky and Martin (2008, page 482), state, “We said earlier that statistical parsers can take advantage of longer-distance information than n-grams, which suggests that they might do a better job at language modeling/word prediction. It turns out that if we have a very large amount of training data, a 4-gram or 5-gram is nonetheless still the best way to do language modeling.” To ve</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, B. 2001. Probabilistic top–down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="5756" citStr="Rosenfeld 1996" startWordPosition="826" endWordPosition="827">d method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997), Lau et al. (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum entropy is nothing but maximum likelihood estimation for undirected Markov</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, R. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10(2):187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="3007" citStr="Rosenfeld 2000" startWordPosition="426" endWordPosition="427">n Employee of the United States Government and is therefore a work of the United States Government In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in the</context>
<context position="4544" citStr="Rosenfeld 2000" startWordPosition="656" endWordPosition="657">rm n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used m</context>
<context position="7114" citStr="Rosenfeld 2000" startWordPosition="1033" endWordPosition="1034">05b), however, there are two weaknesses with maximum entropy approach. The first weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden 632 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model information in natural language, such as syntactic structure and semantic topic. The second weakness is that if the statistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods (Mark, Miller, and Grenander 1996; Rosenfeld 2000b; Rosenfeld, Chen, and Zhu 2001) would have to be used. One way to overcome the first hurdle is to use a preprocessing tool to extract hidden features (e.g., Rosenfeld [1996] used mutual information clustering method to find word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba and Jelinek’s structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigr</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Rosenfeld, R. 2000a. Two decades of statistical language modeling: Where do we go from here? Proceedings of IEEE, 88(8):1270–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Incorporating linguistic structure into statistical language models.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences,</journal>
<volume>358</volume>
<issue>1769</issue>
<contexts>
<context position="3007" citStr="Rosenfeld 2000" startWordPosition="426" endWordPosition="427">n Employee of the United States Government and is therefore a work of the United States Government In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in the</context>
<context position="4544" citStr="Rosenfeld 2000" startWordPosition="656" endWordPosition="657">rm n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used m</context>
<context position="7114" citStr="Rosenfeld 2000" startWordPosition="1033" endWordPosition="1034">05b), however, there are two weaknesses with maximum entropy approach. The first weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden 632 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model information in natural language, such as syntactic structure and semantic topic. The second weakness is that if the statistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods (Mark, Miller, and Grenander 1996; Rosenfeld 2000b; Rosenfeld, Chen, and Zhu 2001) would have to be used. One way to overcome the first hurdle is to use a preprocessing tool to extract hidden features (e.g., Rosenfeld [1996] used mutual information clustering method to find word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba and Jelinek’s structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigr</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Rosenfeld, R. 2000b. Incorporating linguistic structure into statistical language models. Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences, 358(1769):1311–1324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
<author>S Chen</author>
<author>X Zhu</author>
</authors>
<title>Whole-sentence exponential language models: A vehicle for linguistic-statistical integration.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>1</issue>
<pages>55--73</pages>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Rosenfeld, R., S. Chen, and X. Zhu. 2001. Whole-sentence exponential language models: A vehicle for linguistic-statistical integration. Computer Speech and Language, 15(1): 55–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Russell</author>
<author>P Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach, 3rd edition. Prentice Hall, Upper Saddle River,</title>
<date>2010</date>
<location>NJ.</location>
<contexts>
<context position="33245" citStr="Russell and Norvig 2010" startWordPosition="5366" endWordPosition="5369">ain at any given time. In the CONSTRUCTOR operation, the resulting hypotheses are discarded due to either finite stack size or the log-probability threshold (the maximum tolerable difference between the log-probability score of the top-most hypothesis and the bottom-most hypothesis at any given state of the stack). The synchronous, multi-stack search strategy is a greedy best-first search algorithm, one of the local heuristic search procedures that does not use future cost estimates to guide the search and thus does not guarantee that the N-best list parse trees are a global optimal solution (Russell and Norvig 2010). In practice, however, we find that the N-best list approximate EM algorithm does converge within several iterations. 3.1.2 EM Update. Once we have both the N-best parse trees for each sentence in document d and the N-best topics for document d, we derive the EM algorithm to estimate model parameters. Maximizing ˜Q(p&apos;, p, TN) with respect to p&apos; leads to re-estimated parameters of the composite model, which are nothing but the following normalized conditional expected counts: � � p�(w|w−1 −n+1h−1 −mg) °� d∈v l E Pp(Tl, Gl|Wl, d)#(w−1 −n+1wh−1 −mg, Wl, Tl, Gl, d) (19) Gl Tl∈Tl N∈TN E 642 Tan et</context>
</contexts>
<marker>Russell, Norvig, 2010</marker>
<rawString>Russell, S. and P. Norvig. 2010. Artificial Intelligence: A Modern Approach, 3rd edition. Prentice Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Saul</author>
<author>F Pereira</author>
</authors>
<title>Aggregate and mixed-order Markov models for statistical language processing.</title>
<date>1997</date>
<booktitle>The Second Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>81–89, Providence, RI.</location>
<contexts>
<context position="4308" citStr="Saul and Pereira 1997" startWordPosition="624" endWordPosition="627">small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like s</context>
</contexts>
<marker>Saul, Pereira, 1997</marker>
<rawString>Saul, L. and F. Pereira. 1997. Aggregate and mixed-order Markov models for statistical language processing. The Second Conference on Empirical Methods in Natural Language Processing (EMNLP), 81–89, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>The 44th Annual Conference of the Association of Computational Linguistics (ACL),</booktitle>
<location>985–992, Sydney, Australia.</location>
<contexts>
<context position="89755" citStr="Teh 2006" startWordPosition="14396" endWordPosition="14397"> Of course, this implies that it takes a huge amount of resources to perform the computation. As cloud computing becomes the dominant platform for data management and information processing as utility computing, this will become feasible, affordable, and cheap. The development of the large-scale distributed composite language model is in its infancy; we are planning to deepen our research and push this research in its limit. Specifically, we plan to integrate more advanced topic language models such as LDA (Blei, Ng, and Jordan 2003) and resort to a hierarchical non-parametric Bayesian model (Teh 2006; Teh and Jordan 2010) for smoothing fractional counts due to latent variables to handle the sparse data problem in Kneser-Ney’s sense in a principled manner, thus constructing a family of large-scale distributed composite lexical, syntactic, and semantic language models. Finally we will put this family of composite language models into a phrased-based machine translation decoder (Koehn, Och, and Marcu 2003) that produces a lattice of alternative translations/transcriptions or a syntax-based 663 Computational Linguistics Volume 38, Number 3 decoder (Chiang 2005, 2007) that produces a forest of</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Teh, Y. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. The 44th Annual Conference of the Association of Computational Linguistics (ACL), 985–992, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Teh</author>
<author>M Jordan</author>
</authors>
<title>Hierarchical Bayesian nonparametric models with applications.</title>
<date>2010</date>
<booktitle>Bayesian Nonparametrics: Principles and Practice,</booktitle>
<pages>158--207</pages>
<editor>In N. Hjort, C. Holmes, P. Mueller, and S. Walker, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="89777" citStr="Teh and Jordan 2010" startWordPosition="14398" endWordPosition="14401">, this implies that it takes a huge amount of resources to perform the computation. As cloud computing becomes the dominant platform for data management and information processing as utility computing, this will become feasible, affordable, and cheap. The development of the large-scale distributed composite language model is in its infancy; we are planning to deepen our research and push this research in its limit. Specifically, we plan to integrate more advanced topic language models such as LDA (Blei, Ng, and Jordan 2003) and resort to a hierarchical non-parametric Bayesian model (Teh 2006; Teh and Jordan 2010) for smoothing fractional counts due to latent variables to handle the sparse data problem in Kneser-Ney’s sense in a principled manner, thus constructing a family of large-scale distributed composite lexical, syntactic, and semantic language models. Finally we will put this family of composite language models into a phrased-based machine translation decoder (Koehn, Och, and Marcu 2003) that produces a lattice of alternative translations/transcriptions or a syntax-based 663 Computational Linguistics Volume 38, Number 3 decoder (Chiang 2005, 2007) that produces a forest of alternatives (such in</context>
</contexts>
<marker>Teh, Jordan, 2010</marker>
<rawString>Teh, Y. and M. Jordan. 2010. Hierarchical Bayesian nonparametric models with applications. In N. Hjort, C. Holmes, P. Mueller, and S. Walker, editors, Bayesian Nonparametrics: Principles and Practice, pages 158–207, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Van Uytsel</author>
<author>D Compernolle</author>
</authors>
<title>Language modeling with probabilistic left corner parsing. Computer Speech and Language,</title>
<date>2005</date>
<marker>Van Uytsel, Compernolle, 2005</marker>
<rawString>Van Uytsel, D. and D. Compernolle. 2005. Language modeling with probabilistic left corner parsing. Computer Speech and Language, 19(2):171–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="77990" citStr="Vapnik 1998" startWordPosition="12550" endWordPosition="12551">-GRAM/m-SLM+n-GRAM/PLSA+m-SLM/PLSA n-GRAM/m-SLM/PLSA 659 Computational Linguistics Volume 38, Number 3 Figure 5 Language modeling is a data-rich and feature-rich density estimation problem. The information projection from true distribution and empirical distribution to n-grams is unique, and the information projection from true distribution and empirical distribution to composite language models might be local optimal. There is a trade-off between approximation error and estimation error for composite language models. by the perplexity result using test data. By the Glivenko-Cantelli theorem (Vapnik 1998), we know that the empirical distribution p˜ converges to the true distribution ˆp; similarly, the information projection of empirical distribution on an n-gram converges to the information projection on an n-gram of true distribution (i.e., the estimation error shrinks to 0). In the same vein, we can define the information projection of pˆ or p˜ to the composite language models and the corresponding approximate error and estimation error, and so forth. In this case, the Pythagorean theorem breaks down due to the nonconvexity of the set of composite language models. As noted by Dr. Ciprian Che</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vapnik, V. 1998. Statistical Learning Theory. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
</authors>
<title>Topic modeling: Beyond bag-of-words.</title>
<date>2006</date>
<booktitle>The 23rd International Conference on Machine Learning (ICML),</booktitle>
<pages>977--984</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4365" citStr="Wallach 2006" startWordPosition="634" endWordPosition="635">ed the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on specific linguistic phenomena like syntactic structure, semantic topic, morphology, and pragm</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Wallach, H. 2006. Topic modeling: Beyond bag-of-words. The 23rd International Conference on Machine Learning (ICML), 977–984, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>R Greiner</author>
<author>S Wang</author>
</authors>
<title>Consistency and generalization bounds for maximum entropy density estimation.</title>
<date>2009</date>
<tech>Manuscript.</tech>
<marker>Wang, Greiner, Wang, 2009</marker>
<rawString>Wang, S., R. Greiner, and S. Wang. 2009. Consistency and generalization bounds for maximum entropy density estimation. Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>M Harper</author>
</authors>
<title>The SuperARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>The 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>238--247</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4101" citStr="Wang and Harper 2002" startWordPosition="596" endWordPosition="599">but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Benediand S´anchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some specific, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construc</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wang, W. and M. Harper. 2002. The SuperARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources. The 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 238–247, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>D Schuurmans</author>
<author>F Peng</author>
<author>Y Zhao</author>
</authors>
<title>Combining statistical language models via the latent maximum entropy principle.</title>
<date>2005</date>
<booktitle>Machine Learning Journal: Special Issue on Learning in Speech and Language Technologies,</booktitle>
<pages>60--229</pages>
<contexts>
<context position="6502" citStr="Wang et al. (2005" startWordPosition="942" endWordPosition="945">ation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997), Lau et al. (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum entropy is nothing but maximum likelihood estimation for undirected Markov random fields (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997). As stated in Wang et al. (2005b), however, there are two weaknesses with maximum entropy approach. The first weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden 632 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model information in natural language, such as syntactic structure and semantic topic. The second weakness is that if the statistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods (Mark, Miller, and Grenander 1996; Ro</context>
<context position="7861" citStr="Wang et al. 2005" startWordPosition="1145" endWordPosition="1148">ract hidden features (e.g., Rosenfeld [1996] used mutual information clustering method to find word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba and Jelinek’s structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigrams through a maximum conditional entropy approach to form a syntactic, semantic, and lexical language model. Wang and colleagues (Wang et al. 2005a; Wang, Schuurmans, and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn’t overcome the second hurdle. The third method is directed Markov random field (Wang et al. 2005b) that overcomes both weaknesses in the maximum entropy approach. Wang et al. used this approach to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the well</context>
<context position="9132" citStr="Wang et al. 2005" startWordPosition="1335" endWordPosition="1338">ri and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outside algorithm alters Jelinek’s inside–outside algorithm with modular modification and has the same sixth order of sentence-length time complexity. Unfortunately, there are no experimental results reported. In this article, we study the same composite n</context>
<context position="18873" citStr="Wang et al. 2005" startWordPosition="2930" endWordPosition="2933">s. • WORD-PREDICTOR picks a word w E V with probability p(wJg). Because only one pair of (d, w) is being observed, the joint probability model is a mixture of log-linear models with the expression p(d,w) = p(d) Eg p(wJg)p(gJd). Typically, the number of documents and the vocabulary size are much larger than the size of latent semantic class variables. Latent semantic class variables therefore function as bottleneck variables to constrain word occurrences in documents. When combining n-gram, m-SLM, and PLSA together to build a composite generative language model under the directed MRF paradigm (Wang et al. 2005b, 2006), the composite language model is simply a complicated generative model that has four operators: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER. The TAGGER and CONSTRUCTOR in SLM and the SEMANTIZER in PLSA remain unchanged; the WORD-PREDICTORs in n-gram, m-SLM, and PLSA, however, are combined to form a stronger WORD-PREDICTOR that generates the next word, wk+1, not only depending on the m most recently exposed headwords h−1 −m in the word-parse k-prefix but also its n-gram history wkk−n+2 and its semantic content gk+1. The parameter for WORD-PREDICTOR in the composite n-gram/m-SLM</context>
<context position="50738" citStr="Wang et al. (2005" startWordPosition="8191" endWordPosition="8194">age model probability at the word level given by Equation (30) one word at a time. Because a document of the test data is not contained in the original training corpus, to compute the language model probability assignment for word wk+1 we use a “fold-in” heuristic approach similar to the one used in Hofmann (2001): The parameters corresponding to SEMANTIZER, p(g|d), are re-estimated by maximizing the probability of word subsequence seen so far—that is, a pseudo-document ˜dk = (Wk, S), where S is the set of previous sentences of a document in test data—while holding the other parameters fixed. Wang et al. (2005b) use on-line gradient ascent to re-estimate these parameters. We use three methods, onestep on-line EM, on-line EM with fixed learning rate, and batch EM, to re-estimate these parameters. Both one-step on-line EM and on-line EM with fixed learning rate use Equation (32) with γ set to 1 |˜dk|+1 and a constant 0.2, respectively. � −mg)p(g |˜dk−1)Pp(Tk−1|Wk−1, ˜dk−1) h−1 −m∈Tk−1;Tk−1∈Zk−1 p(wk|wk−1 k−n+1h−1 � � −mg)p(g |˜dk−1)Pp(Tk−1|Wk−1, ˜dk−1) g∈Gd h−1 −m∈Tk−1;Tk−1∈Zk−1 p(wk|wk−1 k−n+1h−1 +(1 − γ)p(g|˜dk−1) (32) The batch EM is the standard EM algorithm where we repeat the iterative procedur</context>
<context position="52469" citStr="Wang et al. (2005" startWordPosition="8482" endWordPosition="8485">so the sum over all next words is 1. We find that the perplexity results are sensitive to these three methods and the initial values. For example, for batch EM, if we set initial values to be those obtained by using ˜dk−1 = (Wk−1,S) and trained by batch EM, we obtain worse perplexity results. Table 8 in Section 6.2 gives perplexity results that use these three methods to re-estimate the parameters of the SEMANTIZER, where the on-line EM with fixed learning rate not only has the cheapest computational cost but also leads to the highest perplexity reductions. 5. Related Work Besides the work by Wang et al. (2005b, 2006) that was discussed in the Introduction, the closest work to ours is that by Khudanpur and Wu (2000) where the authors used p(g|˜dk) = γ the pseudo-document up to the previous word 649 Computational Linguistics Volume 38, Number 3 SLM and a word clustering model to extract relevant grammatical and semantic features, then integrated these features with n-grams by a maximum conditional entropy approach. Our composite language model is a generative model, all features play important roles in the EM iterations to allow maximal order events for WORD-PREDICTOR to appear; in Khudanpur and Wu </context>
</contexts>
<marker>Wang, Schuurmans, Peng, Zhao, 2005</marker>
<rawString>Wang, S., D. Schuurmans, F. Peng, and Y. Zhao. 2005a. Combining statistical language models via the latent maximum entropy principle. Machine Learning Journal: Special Issue on Learning in Speech and Language Technologies, 60:229–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>D Schuurmans</author>
<author>Y Zhao</author>
</authors>
<title>The latent maximum entropy principle.</title>
<date>2012</date>
<journal>ACM Transactions on Knowledge</journal>
<publisher>In Press.</publisher>
<marker>Wang, Schuurmans, Zhao, 2012</marker>
<rawString>Wang, S., D. Schuurmans, and Y. Zhao. 2012. The latent maximum entropy principle. ACM Transactions on Knowledge Discovery from Data (TKDD) to appear. In Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wang</author>
<author>C Thrasher</author>
<author>E Viegas</author>
<author>X Li</author>
<author>P Hsu</author>
</authors>
<title>An overview of Microsoft web N-gram corpus and applications.</title>
<date>2010</date>
<booktitle>Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT): Demonstration Session,</booktitle>
<pages>45--48</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="87876" citStr="Wang et al. (2010" startWordPosition="14093" endWordPosition="14096">-best list from a state-of-the-art parsing-based MT system. As far as we know, this is the first work building a complex large-scale distributed language model with a principled approach that simultaneously exploits syntactic, semantic, and lexical regularities and is still more powerful than n-grams trained on a very large corpus with up to a billion tokens. It is reasonable to conjecture that composite language models can achieve drastic perplexity reduction and significantly better translation quality than n-gram when trained on Web-scale corpora that have trillions of tokens. As stated in Wang et al. (2010, p. 45), “Since Banko and Brill’s pioneering work almost a decade ago (Banko and Brill 2001), it has been widely observed that the effectiveness of statistical natural language processing (NLP) techniques is highly susceptible to the data size used to develop them. As empirical studies have repeatedly shown that simple algorithms can often outperform their more complicated counterparts in wide varieties of NLP applications with large data sets, many have come to believe that it is the size of data, not the sophistication of the algorithms, that ultimately play the central role in modern NLP (</context>
</contexts>
<marker>Wang, Thrasher, Viegas, Li, Hsu, 2010</marker>
<rawString>Wang, K., C. Thrasher, E. Viegas, X. Li, and P. Hsu. 2010. An overview of Microsoft web N-gram corpus and applications. Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT): Demonstration Session, pages 45–48, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>S Wang</author>
<author>L Cheng</author>
<author>R Greiner</author>
<author>D Schuurmans</author>
</authors>
<title>Stochastic analysis of lexical and semantic enhanced structural language model.</title>
<date>2006</date>
<booktitle>The 8th International Colloquium on Grammatical Inference (ICGI),</booktitle>
<pages>97--111</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="8976" citStr="Wang et al. (2006)" startWordPosition="1311" endWordPosition="1314">semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the wellknown inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outside algorithm alters Jelinek’s inside–outside algorithm with modular modification and has the sa</context>
<context position="19882" citStr="Wang et al. 2006" startWordPosition="3076" endWordPosition="3079">ing on the m most recently exposed headwords h−1 −m in the word-parse k-prefix but also its n-gram history wkk−n+2 and its semantic content gk+1. The parameter for WORD-PREDICTOR in the composite n-gram/m-SLM/PLSA language model becomes p(wJw−n+1h−1). The resulting composite language model has an even more mg complex dependency structure but with more expressive power than the original SLM. Figure 2 illustrates the structure of a composite n-gram/m-SLM/PLSA language model. The composite n-gram/m-SLM/PLSA language model can be formulated as a rather complex chain-tree-table directed MRF model (Wang et al. 2006) with local 636 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model Figure 2 A composite n-gram/m-SLM/PLSA language model where the hidden information is the parse tree T and semantic content g. The n-gram encodes local word interactions, the m-SLM models the sentence’s syntactic structure, and the PLSA captures the document’s semantic content; all interact together to constrain the generation of natural language. The WORD-PREDICTOR generates the next word wk+1 with probability p(wk+1|wk k−n+2h−1 −mgk+1) instead of p(wk+1|wkk−n+2), p(wk+1|h−1 −m), and p(wk+1|gk+1)</context>
<context position="23866" citStr="Wang et al. (2006)" startWordPosition="3738" endWordPosition="3741">n and speech recognition, this term is useless. Thus, similar to an n-gram language model, we will generally ignore this term and concentrate on optimizing Equation (8) in the subsequent development. The objective of maximum likelihood estimation is to maximize the likeli ˆG(D,p) hood G(D,p) with respect to model parameters. For a given sentence, its parse tree and ri p(a|h−1 −m)#(a,h−1 −m,Wl,Tl,d) 638 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model semantic content are hidden and the number of parse trees grows faster than exponentially with sentence length; Wang et al. (2006) have derived a generalized inside– outside algorithm by applying the standard EM algorithm and considering the auxiliary function � � � � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) (10) Q(p , p) = Gl Tl d∈D l The complexity of this algorithm is sixth order (sentence length), however; thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba 1999; Jelinek 2004). 3.1 N-best List Approximate EM Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list approximate EM re-estimation with modular modi</context>
<context position="34613" citStr="Wang et al. 2006" startWordPosition="5598" endWordPosition="5601">tag) ∝ d∈D � � � Pp(Tl|Wl,d)#(ah−1 p�(a|h−1 l Tl∈TlN∈TN −m,Wl,Tl,d) (21) −m)) ∝ d∈D � � � E Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d) (22) p&apos;(g|d) ∝ l Gl Tl∈TlN∈TN d∈D In the E-step, we use Equations (19)–(22) to compute the expected count of each model parameter over sentence Wl in document d in the training corpus D. In the full case where the number of parse trees grows faster than exponentially with sentence length, we use Jelinek-style recursive formulas in the generalized inside–outside algorithm (Jelinek 2004) to handle the tree structure and describe the weighted forest of possible derivations (Wang et al. 2006). In the N-best list case considered in this paper, however, we just enumerate each parse tree in the N-best list and compute the expected posterior count for each parse tree. For the WORD-PREDICTOR and the SEMANTIZER, we use Equations (19) and (22) and note that there is a sum over semantic annotation sequence Gl where the number of possible semantic annotation sequences is exponential. We use forward–backward recursive formulas reminiscent of those in hidden Markov models to compute the expected counts. To be more specific, for each parse Tl ∈ TlN, we define the forward vector αl(g|d) to be </context>
</contexts>
<marker>Wang, Wang, Cheng, Greiner, Schuurmans, 2006</marker>
<rawString>Wang, S., S. Wang, L. Cheng, R. Greiner, and D. Schuurmans. 2006. Stochastic analysis of lexical and semantic enhanced structural language model. The 8th International Colloquium on Grammatical Inference (ICGI), pages 97–111, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>S Wang</author>
<author>R Greiner</author>
<author>D Schuurmans</author>
<author>L Cheng</author>
</authors>
<title>Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields.</title>
<date>2005</date>
<booktitle>The 22nd International Conference on Machine Learning (ICML),</booktitle>
<location>953–960, Bonn, Germany.</location>
<contexts>
<context position="6502" citStr="Wang et al. (2005" startWordPosition="942" endWordPosition="945">ation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997), Lau et al. (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum entropy is nothing but maximum likelihood estimation for undirected Markov random fields (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997). As stated in Wang et al. (2005b), however, there are two weaknesses with maximum entropy approach. The first weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden 632 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model information in natural language, such as syntactic structure and semantic topic. The second weakness is that if the statistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods (Mark, Miller, and Grenander 1996; Ro</context>
<context position="7861" citStr="Wang et al. 2005" startWordPosition="1145" endWordPosition="1148">ract hidden features (e.g., Rosenfeld [1996] used mutual information clustering method to find word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to influence word prediction; Khudanpur and Wu (2000) used Chelba and Jelinek’s structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigrams through a maximum conditional entropy approach to form a syntactic, semantic, and lexical language model. Wang and colleagues (Wang et al. 2005a; Wang, Schuurmans, and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn’t overcome the second hurdle. The third method is directed Markov random field (Wang et al. 2005b) that overcomes both weaknesses in the maximum entropy approach. Wang et al. used this approach to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the well</context>
<context position="9132" citStr="Wang et al. 2005" startWordPosition="1335" endWordPosition="1338">ri and Young 1990) with modular modification to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outside algorithm alters Jelinek’s inside–outside algorithm with modular modification and has the same sixth order of sentence-length time complexity. Unfortunately, there are no experimental results reported. In this article, we study the same composite n</context>
<context position="18873" citStr="Wang et al. 2005" startWordPosition="2930" endWordPosition="2933">s. • WORD-PREDICTOR picks a word w E V with probability p(wJg). Because only one pair of (d, w) is being observed, the joint probability model is a mixture of log-linear models with the expression p(d,w) = p(d) Eg p(wJg)p(gJd). Typically, the number of documents and the vocabulary size are much larger than the size of latent semantic class variables. Latent semantic class variables therefore function as bottleneck variables to constrain word occurrences in documents. When combining n-gram, m-SLM, and PLSA together to build a composite generative language model under the directed MRF paradigm (Wang et al. 2005b, 2006), the composite language model is simply a complicated generative model that has four operators: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER. The TAGGER and CONSTRUCTOR in SLM and the SEMANTIZER in PLSA remain unchanged; the WORD-PREDICTORs in n-gram, m-SLM, and PLSA, however, are combined to form a stronger WORD-PREDICTOR that generates the next word, wk+1, not only depending on the m most recently exposed headwords h−1 −m in the word-parse k-prefix but also its n-gram history wkk−n+2 and its semantic content gk+1. The parameter for WORD-PREDICTOR in the composite n-gram/m-SLM</context>
<context position="50738" citStr="Wang et al. (2005" startWordPosition="8191" endWordPosition="8194">age model probability at the word level given by Equation (30) one word at a time. Because a document of the test data is not contained in the original training corpus, to compute the language model probability assignment for word wk+1 we use a “fold-in” heuristic approach similar to the one used in Hofmann (2001): The parameters corresponding to SEMANTIZER, p(g|d), are re-estimated by maximizing the probability of word subsequence seen so far—that is, a pseudo-document ˜dk = (Wk, S), where S is the set of previous sentences of a document in test data—while holding the other parameters fixed. Wang et al. (2005b) use on-line gradient ascent to re-estimate these parameters. We use three methods, onestep on-line EM, on-line EM with fixed learning rate, and batch EM, to re-estimate these parameters. Both one-step on-line EM and on-line EM with fixed learning rate use Equation (32) with γ set to 1 |˜dk|+1 and a constant 0.2, respectively. � −mg)p(g |˜dk−1)Pp(Tk−1|Wk−1, ˜dk−1) h−1 −m∈Tk−1;Tk−1∈Zk−1 p(wk|wk−1 k−n+1h−1 � � −mg)p(g |˜dk−1)Pp(Tk−1|Wk−1, ˜dk−1) g∈Gd h−1 −m∈Tk−1;Tk−1∈Zk−1 p(wk|wk−1 k−n+1h−1 +(1 − γ)p(g|˜dk−1) (32) The batch EM is the standard EM algorithm where we repeat the iterative procedur</context>
<context position="52469" citStr="Wang et al. (2005" startWordPosition="8482" endWordPosition="8485">so the sum over all next words is 1. We find that the perplexity results are sensitive to these three methods and the initial values. For example, for batch EM, if we set initial values to be those obtained by using ˜dk−1 = (Wk−1,S) and trained by batch EM, we obtain worse perplexity results. Table 8 in Section 6.2 gives perplexity results that use these three methods to re-estimate the parameters of the SEMANTIZER, where the on-line EM with fixed learning rate not only has the cheapest computational cost but also leads to the highest perplexity reductions. 5. Related Work Besides the work by Wang et al. (2005b, 2006) that was discussed in the Introduction, the closest work to ours is that by Khudanpur and Wu (2000) where the authors used p(g|˜dk) = γ the pseudo-document up to the previous word 649 Computational Linguistics Volume 38, Number 3 SLM and a word clustering model to extract relevant grammatical and semantic features, then integrated these features with n-grams by a maximum conditional entropy approach. Our composite language model is a generative model, all features play important roles in the EM iterations to allow maximal order events for WORD-PREDICTOR to appear; in Khudanpur and Wu </context>
</contexts>
<marker>Wang, Wang, Greiner, Schuurmans, Cheng, 2005</marker>
<rawString>Wang, S., S. Wang, R. Greiner, D. Schuurmans, and L. Cheng. 2005b. Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields. The 22nd International Conference on Machine Learning (ICML), 953–960, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wu</author>
</authors>
<title>On the convergence properties of the EM algorithm.</title>
<date>1983</date>
<journal>Annals of Statistics,</journal>
<pages>11--95</pages>
<contexts>
<context position="26899" citStr="Wu (1983)" startWordPosition="4301" endWordPosition="4302">hen the global convergence theorem (Zangwill 1969) states the following. Theorem Let M be a point-to-set map (an algorithm) that, given a point 00 E ©, generates a sequence {0∞i=0} through the iteration 0i+1 = M(0i). Let Q E © be the set of fixed points of M. Suppose (i) M is closed over the complement of Q; (ii) there is a continuous function 4) on © such that (a) if 0 E� Q, 4)(A) &gt; 4)(0) for all A E M(0), and (b) if 0 E Q, 4)(A) ≥ 4)(0) for all A E M(0). Then all the limit points of {0i} are in Q and 4)(0i) converges monotonically to 4)(0) for some 0 E Q. Proof This theorem has been used by Wu (1983) to prove the convergence of a standard EM algorithm (Dempster, Laird, and Rubin 1977). We now use this theorem to show that the N-best list approximate EM algorithm globally converges to the stationary points of the N-best list likelihood. We encounter one difficulty at this point, however, due to the maximization operator in Equation (11); after each iteration the N-best list may have been changed, therefore the set of data presented for the estimation of model parameters may be different from the previous one. Nevertheless, we prove the convergence of the N-best list approximate EM algorith</context>
<context position="28799" citStr="Wu (1983)" startWordPosition="4628" endWordPosition="4629">alable Distributed Syntactic, Semantic, and Lexical Language Model of N-best list parse trees for sentences over entire corpus D under two model parameters pˇ and ¯p, respectively: ˇTN = arg max L(D, ˇp,T&apos;N) (12) TSN ¯TN = arg max L(D, ¯p, T&apos;N) (13) TSN and let p¯ be the closed form solution of maximizing ˜Q(p&apos;, ˇp, ˇTN) with respect to p&apos;, that is, Then p¯ = arg max ˜Q(p&apos;,ˇp, ˇTN) (14) p, max L(D, ¯p,T�N) ≥ ˜L(D, ¯p, ˇTN) (15) TSN ≥ ˜L(D, ˇp, ˇTN) (16) ≥ max L(D, ˇp,T&apos;N) (17) T-N The inequality in Equation (15) is strict unless ˇTN = ¯TN, which results in p¯ ∈ M( ¯p). Using results proven by Wu (1983), we know that when pˇ is not a stationary point of the N-best list likelihood or pˇ ∈/ M( ˇp), a L(D, ˇp,TN)— a˜Q(p&apos;, ˇp, ˇTN) ˜Q( ¯p,ˇp,ˇTN) &gt; ˜Q(ˇp, ˇp, ˇTN), a pˇ — a pˇ =~0, thus the inequality in Equation (16) is strict. Finally, the inequality in Equation (17) is strict unless pˇ ∈ M( ˇp). Thus condition (ii) is satisfied. This completes the proof that the N-best list approximate EM algorithm monotonically increases the N-best list likelihood and converges in the sense of Zangwill’s global convergence. In the following, we formally derive the N-best list approximate EM algorithm with li</context>
</contexts>
<marker>Wu, 1983</marker>
<rawString>Wu, C. 1983. On the convergence properties of the EM algorithm. Annals of Statistics, 11:95–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual Conference of the Association of Computational Linguistics (ACL),</booktitle>
<pages>1067--1074</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="85327" citStr="Yamada and Knight (2001)" startWordPosition="13701" endWordPosition="13704">to four groups: good/bad syntax crossed with good/bad meaning by human judges (see Table 12). We find that many more sentences are perfect, many more are grammatically correct, and many more are semantically correct. The syntactic language model (Charniak et al. 2003) only improves translations to have good grammar, but does not improve translations to preserve meaning. The composite 5-gram/2-SLM+2-gram/4-SLM+5-gram/PLSA1 language model improves both significantly. Bear in mind that Charniak et al. (2003) integrated Charniak’s language model with the syntax-based translation model proposed by Yamada and Knight (2001) to rescore a tree-to-string translation forest, whereas we use only our language model for N-best list re-ranking. Also, the same study (Charniak et al. 2003) found that the outputs produced using the n-grams received higher scores from Bleu; ours did not. The difference between human judgments and Bleu scores indicates that closer agreement may be possible by incorporating syntactic structure and semantic information into the Bleu score evaluation. For example, semantically similar words like insure and ensure as in Bleu paper (Papineni et al. 2002) should be substituted in the formula, and </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, K. and K. Knight. 2001. A syntax-based statistical translation model. Proceedings of the 39th Annual Conference of the Association of Computational Linguistics (ACL), 1067–1074, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zangwill</author>
</authors>
<title>Nonlinear Programming: A Unified Approach. Prentice-Hall, Upper Saddle River,</title>
<date>1969</date>
<location>NJ.</location>
<contexts>
<context position="25911" citStr="Zangwill 1969" startWordPosition="4101" endWordPosition="4102">the EM algorithm to estimate model parameters that maximize N-best list likelihood of the training corpus D, �  ˜L(D,p,TN) = ri Y: � Pp(Wl, Tl, Gl|d)    d∈D l Gl Tl∈TlN∈TN 639 Computational Linguistics Volume 38, Number 3 That is, (a) E-step: Compute the auxiliary function of the N-best list likelihood � ˜Q(p�,p,TN) = � Pp(Tl, Gl|Wl, d) log Pp,(Wl, Tl, Gl|d) d∈v l Gl Tl∈TlN∈TN (b) M-step: Maximize ˜Q(p&apos;, p, TN) with respect to p&apos; to get the new update for p. Iterate steps (1) and (2) until the convergence of the N-best list likelihood. We use Zangwill’s global convergence theorem (Zangwill 1969) to analyze the behavior of convergence of the N-best list approximate EM. First, we define two concepts needed for Zangwill’s global convergence theorem. A map M is from points of © to subsets of © is called a point-to-set map on ©. It is said to be closed at 0 if 0i → 0,0i E © and Ai → A, Ai E M(0i) implies A E M(0). For a point-to-point map, continuity implies closedness. Then the global convergence theorem (Zangwill 1969) states the following. Theorem Let M be a point-to-set map (an algorithm) that, given a point 00 E ©, generates a sequence {0∞i=0} through the iteration 0i+1 = M(0i). Let </context>
</contexts>
<marker>Zangwill, 1969</marker>
<rawString>Zangwill, W. 1969. Nonlinear Programming: A Unified Approach. Prentice-Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
</authors>
<title>Structured Language Models for Statistical Machine Translation.</title>
<date>2008</date>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="3021" citStr="Zhang 2008" startWordPosition="428" endWordPosition="429"> United States Government and is therefore a work of the United States Government In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 38, Number 3 clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difficult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their final repor</context>
<context position="80487" citStr="Zhang 2008" startWordPosition="12946" endWordPosition="12947">best list in statistical MT. We used the same two 1,000-best lists that were used 660 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model Table 10 10-fold cross-validation Bleu score results for the task of re-ranking the 1,000-best list generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set. SYSTEM MODEL MEAN (%) 95% CI (%) BASELINE 31.75 0.22 5-GRAM 32.53 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 0.24 5-GRAM/PLSA1 33.01 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 33.32 0.25 by Zhang and colleagues (Zhang, Hildebrand, and Vogel 2006; Zhang 2008; Zhang et al. 2011). The first list was generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set, and the second was generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero (Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and Martin 2008) on a 200 million token corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och 2003) to optimize </context>
</contexts>
<marker>Zhang, 2008</marker>
<rawString>Zhang, Y. 2008. Structured Language Models for Statistical Machine Translation. Ph.D. dissertation, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>A Hildebrand</author>
<author>S Vogel</author>
</authors>
<title>Distributed language modeling for N-best list re-ranking.</title>
<date>2006</date>
<booktitle>The 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>216--223</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="46862" citStr="Zhang et al. (2006)" startWordPosition="7534" endWordPosition="7537">lume 38, Number 3 3.3 Distributed Architecture When using very large corpora to train our composite language model, the data and the parameters cannot be stored together on a single machine, so we have to resort to distributed computing. The topic of large-scale distributed language models is relatively new, and existing work is restricted to n-grams only (Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007). Although all existing research use distributed architectures that follow the client–server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts, and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each n-gram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean and Ghemawat 2004): The corpus is first divided and loaded into a number of clients, and n-gram counts are collected at each client, then the n-gram counts are mapped via hashing and are stored in a number o</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Zhang, Y., A. Hildebrand, and S. Vogel. 2006. Distributed language modeling for N-best list re-ranking. The 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), 216–223, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
<author>A Emami</author>
<author>K Papineni</author>
<author>J Sorensen</author>
<author>J Quinn</author>
</authors>
<title>Distributed language modeling.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation, Chapter 2.5.1,</booktitle>
<pages>252--270</pages>
<editor>In Joseph Olive, Caitlin Christianson, and John McCary, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="80507" citStr="Zhang et al. 2011" startWordPosition="12948" endWordPosition="12951"> statistical MT. We used the same two 1,000-best lists that were used 660 Tan et al. A Scalable Distributed Syntactic, Semantic, and Lexical Language Model Table 10 10-fold cross-validation Bleu score results for the task of re-ranking the 1,000-best list generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set. SYSTEM MODEL MEAN (%) 95% CI (%) BASELINE 31.75 0.22 5-GRAM 32.53 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 0.24 5-GRAM/PLSA1 33.01 0.24 5-GRAM/2-SLM+2-GRAM/4-SLM+5-GRAM/PLSA1 33.32 0.25 by Zhang and colleagues (Zhang, Hildebrand, and Vogel 2006; Zhang 2008; Zhang et al. 2011). The first list was generated on 919 sentences of 100 documents from the MT03 Chinese–English evaluation set, and the second was generated on 191 sentences of 20 documents from the MT04 Chinese–English evaluation set, both by Hiero (Chiang 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Jurafsky and Martin 2008) on a 200 million token corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och 2003) to optimize the Bleu score (Papi</context>
</contexts>
<marker>Zhang, Vogel, Emami, Papineni, Sorensen, Quinn, 2011</marker>
<rawString>Zhang, Y., S. Vogel, A. Emami, K. Papineni, J. Sorensen, and J. Quinn. 2011. Distributed language modeling. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation, Chapter 2.5.1, 252–270, Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>