<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000123">
<title confidence="0.992346">
SRL-based Verb Selection for ESL
</title>
<author confidence="0.996135">
1,2Xiaohua Liu, 3Bo Han*, 4Kuan Li*, 5Stephan Hyeonjun Stiller and 2Ming Zhou
</author>
<affiliation confidence="0.998025666666667">
1School of Computer Science and Technology
Harbin Institute of Technology
2Microsoft Research Asia
3Department of Computer Science and Software Engineering
The University of Melbourne
4College of Computer Science
Chongqing University
5Computer Science Department
Stanford University
</affiliation>
<email confidence="0.902539">
{xiaoliu, mingzhou, v-kuli}@microsoft.com
b.han@pgrad.unimelb.edu.au
sstiller@stanford.edu
</email>
<sectionHeader confidence="0.993849" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997024235294118">
In this paper we develop an approach to tackle
the problem of verb selection for learners of
English as a second language (ESL) by using
features from the output of Semantic Role La-
beling (SRL). Unlike existing approaches to
verb selection that use local features such as
n-grams, our approach exploits semantic fea-
tures which explicitly model the usage context
of the verb. The verb choice highly depends
on its usage context which is not consistently
captured by local features. We then combine
these semantic features with other local fea-
tures under the generalized perceptron learn-
ing framework. Experiments on both in-
domain and out-of-domain corpora show that
our approach outperforms the baseline and
achieves state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.998108" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761285714286">
Verbs in English convey actions or states of being.
In addition, they also communicate sentiments and
imply circumstances, e.g., in “He got [gained] the
scholarship after three interviews.”, the verb
“gained” may indicate that the “scholarship” was
competitive and required the agent’s efforts; in
contrast, “got” sounds neutral and less descriptive.
</bodyText>
<note confidence="0.4978935">
* This work has been done while the author was visiting Mi-
crosoft Research Asia.
</note>
<bodyText confidence="0.99915128125">
Since verbs carry multiple important functions,
misusing them can be misleading, e.g., the native
speaker could be confused when reading “I like
looking [reading] books”. Unfortunately, accord-
ing to (Gui and Yang, 2002; Yi et al., 2008), more
than 30% of the errors in the Chinese Learner Eng-
lish Corpus (CLEC) are verb choice errors. Hence,
it is useful to develop an approach to automatically
detect and correct verb selection errors made by
ESL learners.
However, verb selection is a challenging task
because verbs often exhibit a variety of usages and
each usage depends on a particular context, which
can hardly be adequately described by convention-
al n-gram features. For instance, both “made” and
“received” can complete “I have __ a telephone
call.”, where the usage context can be represented
as “made/received a telephone call”; however, in
“I have __ a telephone call from my boss”, the
prepositional phrase “from my boss” becomes a
critical part of the context, which now cannot be
described by n-gram features, resulting in only
“received” being suitable.
Some researchers (Tetreault and Chodorow,
2008) exploited syntactic information and n-gram
features to represent verb usage context. Yi et al.
(2008) introduced an unsupervised web-based
proofing method for correcting verb-noun colloca-
tion errors. Brockett et al. (2006) employed phrasal
Statistical Machine Translation (SMT) techniques
to correct countability errors. None of their meth-
ods incorporated semantic information.
</bodyText>
<note confidence="0.768941333333333">
1068
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998098811320755">
Unlike the other papers, we derive features from
the output of an SRL (Màrquez, 2009) system to
explicitly model verb usage context. SRL is gener-
ally understood as the task of identifying the argu-
ments of a given verb and assigning them semantic
labels describing the roles they play. For example,
given a sentence “I want to watch TV tonight” and
the target predicate “watch”, the output of SRL
will be something like “I [A0] want to watch [tar-
get predicate] TV [A1] tonight [AM-TMP].”,
meaning that the action “watch” is conducted by
the agent “I”, on the patient “TV”, and the action
happens “tonight”.
We believe that SRL results are excellent fea-
tures for characterizing verb usage context for
three reasons: (i) Intuitively, the predicate-
argument structures generated by SRL systems
capture major relationships between a verb and its
contextual participants and consequently largely
determine whether or not the verb usage is proper.
For example, in “I want to watch a match tonight.”,
“match” is the patient of “watch”, and “watch ...
match” forms a collocation, suggesting “watch” is
appropriately used. (ii) Predicate-argument struc-
tures abstract away syntactic differences in sen-
tences with similar meanings, and therefore can
potentially filter out lots of noise from the usage
context. For example, consider “I want to watch a
football match on TV tonight”: if “match” is suc-
cessfully identified as the agent of “watch”,
“watch ... football”, which is unrelated to the us-
age of “watch” in this case, can be easily excluded
from the usage context. (iii) Research on SRL has
made great achievements, including human-
annotated training corpora and state-of-the-art sys-
tems, which can be directly leveraged.
Taking an English sentence as input, our method
first generates correction candidates by replacing
each verb with verbs in its pre-defined confusion
set; then for every candidate, it extracts SRL-
derived features; finally our method scores every
candidate using a linear function trained by the
generalized perceptron learning algorithm (Collins,
2002) and selects the best candidate as output.
Experimental results show that SRL-derived fea-
tures are effective in verb selection, but we also
observe that noise in SRL output adversely in-
creases feature space dimensions and the number
of false suggestions. To alleviate this issue, we use
local features, e.g., n-gram-related features, and
achieve state-of-the-art performance when all fea-
tures are integrated.
Our contributions can be summarized as follows:
</bodyText>
<listItem confidence="0.999159">
1. We propose to exploit SRL-derived fea-
tures to explicitly model verb usage con-
text.
2. We propose to use the generalized percep-
tron framework to integrate SRL-derived
(and other) features and achieve state-of-
the-art performance on both in-domain and
out-of-domain test sets.
</listItem>
<bodyText confidence="0.999839571428571">
Our paper is organized as follows: In the next
section, we introduce related work. In Section 3,
we describe our method. Experimental results and
analysis on both in-domain and out-of-domain cor-
pora are presented in Section 4. Finally, we con-
clude our paper with a discussion of future work in
Section 5.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999848535714286">
SRL results are used in various tasks. Moldovan et
al. (2004) classify the semantic relations of noun
phrases based on SRL. Ye and Baldwin (2006)
apply semantic role–related information to verb
sense disambiguation. Narayanan and Harabagiu
(2004) use semantic role structures for question
answering. Surdeanu et al. (2003) employ predi-
cate-argument structures for information extrac-
tion.
However, in the context of ESL error detection
and correction, little study has been carried out on
clearly exploiting semantic information. Brockett
et al. (2006) propose the use of the phrasal statisti-
cal machine translation (SMT) technique to identi-
fy and correct ESL errors. They devise several
heuristic rules to generate synthetic data from a
high-quality newswire corpus and then use the syn-
thetic data together with their original counterparts
for SMT training. The SMT approach on the artifi-
cial data set achieves encouraging results for cor-
recting countability errors. Yi et al. (2008) use web
frequency counts to identify and correct determiner
and verb-noun collocation errors. Compared with
these methods, our approach explicitly models
verb usage context by leveraging the SRL output.
The SRL-based semantic features are integrated,
along with the local features, into the generalized
perceptron model.
</bodyText>
<figure confidence="0.536243">
1069
3 Our Approach tures derived from the semantic roles listed in Ta-
ble 2.
</figure>
<bodyText confidence="0.999884545454545">
Our method can be regarded as a pipeline consist-
ing of three steps. Given as input an English sen-
tence written by ESL learners, the system first
checks every verb and generates correction candi-
dates by replacing each verb with its confusion set.
Then a feature vector that represents verb usage
context is derived from the outputs of an SRL sys-
tem and then multiplied with the feature weight
vector trained by the generalized perceptron. Final-
ly, the candidate with the highest score is selected
as the output.
</bodyText>
<subsectionHeader confidence="0.994956">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.999976">
We formulate the task as a process of generating
and then selecting correction candidates:
</bodyText>
<equation confidence="0.95453">
(1)
</equation>
<bodyText confidence="0.996279">
Here denotes the input sentence for proofing,
GEav(s) is the set of correction candidates, and
Score(s) is the linear model trained by the percep-
tron learning algorithm, which will be discussed in
section 3.4.
We call every target verb in a checkpoint.
For example, “sees” is a checkpoint in “Jane sees
TV every day.”. Correction candidates are generat-
ed by replacing each checkpoint with its confu-
sions. Table 1 shows a sentence with one
checkpoint and the corresponding correction can-
didates.
</bodyText>
<table confidence="0.872357">
Input Jane sees TV every day.
Candidates Jane watches TV every day.
Jane looks TV every day.
�
</table>
<tableCaption confidence="0.998773">
Table 1. Correction candidate list.
</tableCaption>
<bodyText confidence="0.983252125">
One state-of-the-art SRL system (Riedel and
Meza-Ruiz, 2008) is then utilized to extract predi-
cate-argument structures for each verb in the input,
as illustrated in Table 2.
Semantic features are generated by combining
the predicate with each of its arguments; e.g.,
“watches_A0_Jane”, “sees_A0_Jane”, “watch-
es_A1_TV” and “sees_A1_TV” are semantic fea-
</bodyText>
<table confidence="0.998324285714286">
Sentence Semantic roles
Jane sees TV every day Predicate: sees;
A0: Jane;
A1: TV;
Jane watches TV every Predicate: watches;
day A0: Jane;
A1: TV;
</table>
<tableCaption confidence="0.999863">
Table 2. Examples of SRL outputs.
</tableCaption>
<bodyText confidence="0.9983915">
At the training stage, each sentence is labeled by
the SRL system. Each correction candidate is
represented as a feature vector , where
d is the total number of features. The feature
</bodyText>
<equation confidence="0.928699333333333">
weight vector is denoted as , andScore(s)
is computed as follows:
Score(s) = (D (s) • w (2)
</equation>
<bodyText confidence="0.993675666666667">
Finally, is applied to each candidate,
and , the one with the highest score, is selected
as the output, as shown in Table 3.
</bodyText>
<table confidence="0.9617294">
Correction candidate Score
s* Jane watches TV every day. 10.8
Jane looks TV every day. 0.8
Jane reads TV every day. 0.2
... �
</table>
<tableCaption confidence="0.998305">
Table 3. Correction candidate scoring.
</tableCaption>
<bodyText confidence="0.999850571428571">
In the above framework, the basic idea is to
generate correction candidates with the help of pre-
defined confusion sets and apply the global linear
model to each candidate to compute the degree of
its fitness to the usage context that is represented
as features derived from SRL results.
To make our idea practical, we need to solve the
following three subtasks: (i) generating the confu-
sion set that includes possible replacements for a
given verb; (ii) representing the context with se-
mantic features and other complementary features;
and (iii) training the feature weight. We will de-
scribe our solutions to those subtasks in the rest of
this section.
</bodyText>
<table confidence="0.858416">
1070
I have made[opened] an American bank account in Boston .
[A0] [Predicate] [A1] [AM-LOC]
</table>
<tableCaption confidence="0.999054">
Table 4. An example of SRL output.
</tableCaption>
<subsectionHeader confidence="0.999221">
3.2 Generation of Verb Confusion Sets
</subsectionHeader>
<bodyText confidence="0.999896230769231">
Verb confusion sets are used to generate correction
candidates. Due to the great number of verbs and
their diversified usages, manually collecting all
verb confusions in all scenarios is prohibitively
time-consuming. To focus on the study of the ef-
fectiveness of semantic role features, we restrict
our research scope to correcting verb selection er-
rors made by Chinese ESL learners and select fifty
representative verbs which are among the most
frequent ones and account for more than 50% of
ESL verb errors in the CLEC data set. For every
selected verb we manually compile a confusion set
using the following data sources:
</bodyText>
<listItem confidence="0.994105809523809">
1. Encarta treasures. We extract all the syno-
nyms of verbs from the Microsoft Encarta Diction-
ary, and this forms the major source for our
confusion sets.
2. English-Chinese Dictionaries. ESL learners
may get interference from their mother tongue (Liu
et al., 2000). For example, some Chinese people
mistakenly say “see newspaper”, partially because
the translation of “see” co-occurs with “newspa-
per” in Chinese. Therefore English verbs in the
dictionary sharing more than two Chinese mean-
ings are collected. For example, “see” and “read”
are in a confusion set because they share the mean-
ings of both “看” (“to see”, “to read”) and “领会”
(“to grasp”) in Chinese.
3. An SMT translation table. We extract para-
phrasing verb expressions from a phrasal SMT
translation table learnt from parallel corpora (Och
and Ney, 2004). This may help us use the implicit
semantics of verbs that SMT can capture but a dic-
tionary cannot, such as the fact that the verb
</listItem>
<bodyText confidence="0.9959154">
Note that verbs in any confusion set that we are
not interested in are dropped, and that the verb it-
self is included in its own confusion set. We leave
it to our future work to automatically construct
verb confusions.
</bodyText>
<subsectionHeader confidence="0.995556">
3.3 Verb Usage Context Features
</subsectionHeader>
<bodyText confidence="0.99984275">
The verb usage context1 refers to its surrounding
text, which influences the way one understands the
expression. Intuitively, verb usage context can take
the form of a collocation, e.g., “watch ... TV” in “I
saw [watched] TV yesterday.” ; it can also simply
be idioms, e.g., we say “kick one’s habit” instead
of “remove one’s habit”.
We use features derived from the SRL output to
represent verb usage context. The SRL system ac-
cepts a sentence as input and outputs all arguments
and the semantic roles they play for every verb in
the sentence. For instance, given the sentence “I
have opened an American bank account in Bos-
ton.” and the predicate “opened”, the output of
SRL is listed in Table 4, where A0 and A1 are two
core roles, representing the agent and patient of an
action, respectively, and other roles starting with
“AM-”are adjunct roles, e.g., AM-LOC indicates
the location of an action. Predicate-argument struc-
tures keep the key participants of a given verb
while dropping other unrelated words from its us-
age context. For instance, in “My teacher said Chi-
nese is not easy to learn.”, the SRL system
recognizes that “Chinese” is not the A1-argument
of “said”. So “say _ Chinese”, which is irrelevant
to the usage of said, is not extracted as a feature.
The SRL system, however, may output
erroneous predicate-argument structures, which
negatively affect the performance of verb
selection. For instance, for the sentence “He
hasn’t done anything but take [make] a lot of
money”, “lot” is incorrectly identified as the patient
of “take”, making it hard to select “make” as the
proper verb even though “make money” forms a
sound collocation. To tackle this issue, we use
local textual features, namely features related to n-
gram, chunk and chunk headword, as shown in
Table 5. Back-off features are generated by
replacing the word with its POS tag to alleviate
data sparseness.
</bodyText>
<table confidence="0.97986715">
1 http://en.wikipedia.org/wiki/Context_(language_use)
1071
Local: trigrams
have_opened
have_opened_a
opened_an_American
PRP_VBP_opened
VBP_opened_DT
opened_DT_JJ
Local: chunk
have_opened
opened_an_American_investment_bank
_account
PRP_opened
opened_NN
Semantic: SRL derived features
A0_I_opened
opened_A1_account
opened_AM-LOC_in
...
</table>
<tableCaption confidence="0.999731">
Table 5. An example of feature set.
</tableCaption>
<subsectionHeader confidence="0.974492">
3.4 Perceptron Learning
</subsectionHeader>
<bodyText confidence="0.9844706">
We choose the generalized perceptron algorithm as
our training method because of its easy implemen-
tation and its capability of incorporating various
features. However, there are still two concerns
about this perceptron learning approach: its inef-
fectiveness in dealing with inseparable samples
and its ignorance of weight normalization that po-
tentially limits its ability to generalize. In section
4.4 we show that the training error rate drops sig-
nificantly to a very low level after several rounds
of training, suggesting that the correct candidates
can almost be separated from others. We also ob-
serve that our method performs well on an out-of-
domain test corpus, indicating the good generaliza-
tion ability of this method. We leave it to our fu-
ture work to replace perceptron learning with other
models like Support Vector Machines (Vapnik,
1995).
In Figure 1, s; is the ith correct sentence within
the training data. T and N represent the number of
training iterations and training examples, respec-
tively. GElv(s&apos;) is the function that outputs all the
possible corrections for the input sentence with
each checkpoint substituted by one of its confu-
sions, as described in Section 3.1. We observe that
the generated candidates sometimes contain rea-
sonable outputs for the verb selection task, which
should be removed. For instance, in “... reporters
could not take [make] notes or tape the conversa-
tion”, both “take” and “make” are suitable verbs in
this context. To fix this issue, we trained a trigram
language model using SRILM (Stolcke, 2002) on
LDC data2, and calculated the logarithms of the
language model score for the original sentence and
its artificial manipulations. We only kept manipu-
lations with a language model score that is t lower
than that of the original sentence. We experimen-
tally set t = 5.
Inputs: training examples , i=1...N
Initialization:w=0
</bodyText>
<figure confidence="0.9797198">
Algorithm:
For r= 1.. T, i= 1..N
Calculateo=argmax:.(D(s) •i
If
Outputs:
</figure>
<figureCaption confidence="0.9977115">
Figure 1. The perceptron algorithm, adapted from Co-
lins (2002).
</figureCaption>
<bodyText confidence="0.99323925">
(D in Figure 1 is the feature extraction function.
(D(o) and are vectors extracted from the out-
put and oracle, respectively. A vector field is filled
with 1 if the corresponding feature exists, or 0 oth-
erwise; is the feature weight vector, where posi-
tive elements suggest that the corresponding
features support the hypothesis that the candidate
is correct.
The training process is to update w , when the
output differs from the oracle. For example, when
o is “I want to look TV” and is “I want to watch
TV”, will be updated.
We use the averaged Perceptron algorithm (Col-
lins, 2002) to alleviate overfitting on the training
data. The averaged perceptron weight vector is
defined as
</bodyText>
<equation confidence="0.9973815">
 i r
=  w ,
 1
TN
(3)
i=1..N,r=1..T
</equation>
<bodyText confidence="0.990952">
where , is the weight vector immediately af-
ter the ith sentence in the rth iteration.
</bodyText>
<equation confidence="0.596895">
2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-
logId=LDC2005T12
1072
</equation>
<sectionHeader confidence="0.998204" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999806">
In this section, we compare our approach with the
SMT-based approach. Furthermore, we study the
contribution of predicate-argument-related
features, and the performances on verbs with
varying distance to their arguments.
</bodyText>
<subsectionHeader confidence="0.986563">
4.1 Experiment Preparation
</subsectionHeader>
<bodyText confidence="0.999996942857143">
The training corpus for perceptron learning was
taken from LDC2005T12. We randomly selected
newswires containing target verbs from the New
York Times as the training data. We then used the
OpenNEP package3to extract sentences from the
newswire text and to parse them into the corre-
sponding tokens, POS tags, and chunks. The SRL
system is built according to Riedel and Meza-Ruiz
(2008), using the CoNLL-2008 shared task data for
training. We assume that the newswire data is of
high quality and free of linguistic errors, and final-
ly we gathered 20000 sentences that contain any of
the target verbs we were focusing on. We experi-
mentally set the number of training rounds to T =
50.
We constructed two sets of testing data for in-
domain and out-of-domain test purposes, respec-
tively. To construct the in-domain test data, we
first collected all the sentences that contain any of
the verbs we were interested in from the previous
unused LDC dataset; then we replaced any target
verb in our list with a verb in its confusion set;
next, we used the language-model-based pruning
strategy described in 3.4 to drop possibly correct
manipulations from the test data; and finally we
randomly sampled 5000 sentences for testing.
To build the out-of-domain test dataset, we
gathered 186 samples that contained errors related
to the verbs we were interested in from English
blogs written by Chinese and from the CLEC cor-
pus, which were then corrected by an English na-
tive speaker. Furthermore, for every error
involving the verbs in our target list, both the verb
and the word that determines the error are marked
by the English native speaker.
</bodyText>
<subsectionHeader confidence="0.978608">
4.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999715666666667">
We built up a phrasal SMT system with the word
re-ordering feature disabled, since our task only
concerns the substitution of the target verb. To
</bodyText>
<footnote confidence="0.459206">
3 http://opennlp.sourceforge.net/
</footnote>
<bodyText confidence="0.999087">
construct the training corpus, we followed the idea
in Brockett et al. (2006), and applied a similar
strategy described in section 3.4 to the SRL sys-
tem’s training data to generate aligned pairs.
</bodyText>
<subsectionHeader confidence="0.998814">
4.3 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999799235294118">
We employed the following metrics adapted from
(Yi et al., 2008): revised precision (RP), recall of
the correction (RC) and false alarm (FA).
RP reflects how many outputs are correct usag-
es. The output is regarded as a correct suggestion if
and only if it is exactly the same as the answer.
Paraphrasing scenarios, for example, the case that
the output is “take notes” and the answer is “make
notes”, are counted as errors.
RC indicates how many erroneous sentences are
corrected among all the errors. It measures the sys-
tem’s coverage of verb selection errors.
FA is related to the cases where a correct verb is
mistakenly replaced by an inappropriate one. The-
se false suggestions are likely to disturb or even
annoy users, and thus should be avoided as much
as possible.
</bodyText>
<subsectionHeader confidence="0.975684">
4.4 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999878133333333">
The training error curves of perceptron learning
with different feature sets are shown in Figure 2.
They drop to a low error rate and then stabilize
after a few number of training rounds, indicating
that most of the cases are linearly separable and
that perceptron learning is applicable to the verb
selection task.
We conducted feature selection by dropping fea-
tures that occur less than N times. Here N was ex-
perimentally set to 5. We observe that, after feature
selection, some useful features such as
“watch_A1_TV” and “see_A1_TV” were kept, but
some noisy features like “Jane_A0_sees” and
“Jane_A0_watches” were removed, suggesting the
effectiveness of this feature selection approach.
</bodyText>
<page confidence="0.645093">
1073
</page>
<figureCaption confidence="0.999231">
Figure 2. Training error curves of the perceptron.
</figureCaption>
<bodyText confidence="0.999984552631579">
We tested the baseline and our approach on the
in-domain and out-of-domain corpora. The results
are shown in Table 7 and 8, respectively.
In the in-domain test, the SMT-based approach
has the highest false alarm rate, though its output
with word insertions or deletions is not considered
wrong if the substituted verb is correct. Our ap-
proach, regardless of what feature sets are used,
outperforms the SMT-based approach in terms of
all metrics, showing the effectiveness of percep-
tron learning for the verb selection task. Under the
perceptron learning framework, we can see that the
system using only SRL-related features has higher
revised precision and recall of correction, but also
a slightly higher false alarm rate than the system
based on only local features. When local features
and SRL-derived features are integrated together,
the state-of-the-art performance is achieved with a
5% increase in recall, and minor changes in preci-
sion and false alarm.
In the out-of-domain test, the SMT-based ap-
proach performs much better than in the in-domain
test, especially in terms of false alarm rate, indicat-
ing the SMT-based approach may favor short sen-
tences. However, its recall drops greatly. We ob-
serve similar performance differences between the
systems with different feature sets under the same
perceptron learning framework, reaffirming the
usefulness of the SRL-based features for verb se-
lection.
We also conducted significance test. The results
confirm that the improvements (SRL+Local vs.
SMT-based) are statistically significant (p-value &lt;
0.001) for both the open-domain and the in-domain
experiments.
Furthermore, we studied the performance of our
system on verbs with varying distance to their ar-
guments on the out-of-domain test corpus.
</bodyText>
<table confidence="0.999879125">
Local d&lt;=2 2&lt;d&lt;=4 d&gt;4
RP 64.3% 60.3% 59.4%
RC 34.6% 33.1% 28.9%
FA 3.0% 6.3% 5.0%
SRL d&lt;=2 2&lt;d&lt;=4 d&gt;4
RP 65.1% 60.1% 62.1%
RC 40.3% 34.0% 36.9%
FA 5.0% 6.7% 6.3%
</table>
<tableCaption confidence="0.9956065">
Table 9. Performance on verbs with different distance to
their arguments on out-of-domain test data.
</tableCaption>
<bodyText confidence="0.961224">
Table 9 shows that the system with only SRL-
derived features performs significantly better than
the system with only local features on the verb
whose usage depends on a distant argument, i.e.,
one where the number of words between the predi-
cate and the argument is larger than 4. To under-
stand the reason, consider the following sentence:
</bodyText>
<table confidence="0.888166285714286">
“It&apos;s raining outside. Please wear[take] the
black raincoat with you.”
SMT-based Our method
SRL Local SRL + Local
RP 48.4% 64.5% 62.2% 66.4%
RC 23.5% 40.2% 32.9% 46.4%
FA 13.3% 5.6% 4.2% 6.8%
</table>
<tableCaption confidence="0.998699">
Table 7. In-domain test results.
</tableCaption>
<table confidence="0.9994654">
SMT-based Our method
SRL Local SRL + Local
RP 50.7% 64.0% 62.6% 65.5%
RC 13.5% 39.0% 33.3% 44.0%
FA 6.1% 5.5% 4.0% 6.5%
</table>
<tableCaption confidence="0.999894">
Table 8. Out-of-domain test results.
</tableCaption>
<page confidence="0.814854">
1074
</page>
<bodyText confidence="0.999966615384616">
Intuitively, “wear” and “take” seem to fill the
blank well, since they both form a collocation with
“raincoat”; however, when “with [AM-MNR] you”
is considered as part of the context, “wear” no
longer fits it and “take” wins. In this case, the long-
distance feature devised from AM-MNR helps se-
lect the suitable verb, while the trigram features
cannot because they cannot represent the long dis-
tance verb usage context.
We also find some typical cases that are beyond
the reach of the SRL-derived features. For instance,
consider “Everyone doubts [suspects] that Tom is
a spy.”. Both of the verbs can be followed by a
clause. However, the SRL system regards “is”, the
predicate of the clause, as the patient, resulting in
features like “doubt_A1_is” and “suspect_A1_is”,
which capture nothing about verb usage context.
However, if we consider the whole clause “sus-
pect_Tom is a spy” as the patient, this could result
in a very sparse feature that would be filtered. In
the future, we will combine word-level and phrase-
level SRL systems to address this problem.
Besides its incapability of handling verb selec-
tion errors involving clauses, the SRL-derived fea-
tures fail to work when verb selection depends on
deep meanings that cannot be captured by current
shallow predicate-argument structures. For exam-
ple, in “He was wandering in the park, spending
[killing] his time watching the children playing.”,
though “spending” and “killing” fit the syntactic
structure and collocation agreement, and express
the meaning “to allocate some time doing some-
thing”, the word “wandering” suggests that “kill-
ing” may be more appropriate. Current SRL
systems cannot represent the semantic connection
between two predicates and thus are helpless for
this case. We argue that the performance of our
system can be improved along with the progress of
SRL.
</bodyText>
<sectionHeader confidence="0.997825" genericHeader="evaluation">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999962875">
Verb selection is challenging because verb usage
highly depends on the usage context, which is hard
to capture and represent. In this paper, we propose
to utilize the output of an SRL system to explicitly
model verb usage context. We also propose to use
the generalized perceptron learning framework to
integrate SRL-derived features with other features.
Experimental results show that our method outper-
forms the SMT-based system and achieves state-
of-the-art performance when SRL-related features
and other local features are integrated. We also
show that, for cases where the particular verb us-
age mainly depends on its distant arguments, a sys-
tem with only SRL-derived features performs
much better than the system with only local fea-
tures.
In the future, we plan to automatically construct
confusion sets, expand our approach to more verbs
and test our approach on a larger size of real data.
We will try to combine the outputs of several SRL
systems to make our system more robust. We also
plan to further validate the effectiveness of the
SRL-derived features under other learning methods
like SVMs.
</bodyText>
<sectionHeader confidence="0.939553" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9998385">
We thank the anonymous reviewers for their valu-
able comments. We also thank Changning Huang,
Yunbo Cao, Dongdong Zhang, Henry Li and Mu
Li for helpful discussions.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998454988888889">
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994.
Countability and number in Japanese to English ma-
chine translation. Proc. of the 15th conference on
Computational Linguistics, pages 32-38.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL errors using phrasal SMT
techniques. Proc. of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational
Linguistics, pages 249-256.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: theory and experiments
with perceptron algorithms. Proc. of the ACL-02
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1-8.
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic
Grammar Checking for Second Language Learners –
the Use of Prepositions. Proc. of NoDaliDa.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitrtiy Belen-
ko, and Lucy Vanderwende. 2008. Using Contextual
Speller Techniques and Language Modeling for ESL
Error Correction. Proc. of the International Joint
Conference on Natural Language Processing.
Shichun Gui and Huizhong Yang. 2002. Chinese Learn-
er English Corpus. Shanghai Foreign Languages Ed-
ucation Press, Shanghai, China.
1075
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. Proc. of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational
Linguistics, pages 519-525.
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. Proc. of the 46th Annual Meeting
on Association for Computational Linguistics, pages
174-182.
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and
Changning Huang. 2000. PENS: A Machine-aided
English Writing System for Chinese Users. Proc. of
the 38th Annual Meeting on Association for Compu-
tational Linguistics, pages 529-536.
Lluís Màrquez. 2009. Semantic Role Labeling Past,
Present and Future, Tutorial of ACL-IJCNLP 2009.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Proc. of the
HLT-NAACL Workshop on Computational Lexical
Semantics, pages 60-67.
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. Proc. of the
20th International Conference on Computational
Linguistics, pages 693-701.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Journal of Computational Linguistics, 30(4),
pages 417-449.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with Markov Logic. Proc. of
the Twelfth Conference on Computational Natural
Language Learning, pages 193-197.
Andreas Stolcke. 2002. SRILM -- An Extensible Lan-
guage Modeling Toolkit. Proc. of International Con-
ference on Spoken Language Processing, pages: 901-
904.
Mihai Surdeanu, Lluis Màrquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, page 105-151.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. Proc. of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 8-15.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. Proc. of the 22nd international Conference on
Computational Linguistics, pages 865-872.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, New York.
Patrick Ye and Timothy Baldwin. 2006. Verb Sense
Disambiguation Using Selectional Preferences
Extracted with a State-of-the-art Semantic Role
Labeler. Proc. of the Australasian Language
Technology Workshop, pages 141-148.
Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A
Web-based English Proofing System for English as a
Second Language Users. Proc. of International Joint
Conference on Natural Language Processing, pages
619-624.
</reference>
<page confidence="0.834571">
1076
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.300725">
<title confidence="0.996383">SRL-based Verb Selection for ESL</title>
<author confidence="0.997205">Hyeonjun Stiller Liu</author>
<affiliation confidence="0.877957777777778">of Computer Science and Harbin Institute of Research of Computer Science and Software The University of of Computer Chongqing Science Stanford</affiliation>
<email confidence="0.9884375">{xiaoliu,mingzhou,sstiller@stanford.edu</email>
<abstract confidence="0.999221166666667">In this paper we develop an approach to tackle the problem of verb selection for learners of English as a second language (ESL) by using features from the output of Semantic Role Labeling (SRL). Unlike existing approaches to verb selection that use local features such as n-grams, our approach exploits semantic features which explicitly model the usage context of the verb. The verb choice highly depends on its usage context which is not consistently captured by local features. We then combine these semantic features with other local features under the generalized perceptron learning framework. Experiments on both indomain and out-of-domain corpora show that our approach outperforms the baseline and achieves state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Kentaro Ogura</author>
<author>Satoru Ikehara</author>
</authors>
<title>Countability and number in Japanese to English machine translation.</title>
<date>1994</date>
<booktitle>Proc. of the 15th conference on Computational Linguistics,</booktitle>
<pages>32--38</pages>
<marker>Bond, Ogura, Ikehara, 1994</marker>
<rawString>Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. Countability and number in Japanese to English machine translation. Proc. of the 15th conference on Computational Linguistics, pages 32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>Proc. of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="3026" citStr="Brockett et al. (2006)" startWordPosition="449" endWordPosition="452">n complete “I have __ a telephone call.”, where the usage context can be represented as “made/received a telephone call”; however, in “I have __ a telephone call from my boss”, the prepositional phrase “from my boss” becomes a critical part of the context, which now cannot be described by n-gram features, resulting in only “received” being suitable. Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. Yi et al. (2008) introduced an unsupervised web-based proofing method for correcting verb-noun collocation errors. Brockett et al. (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. None of their methods incorporated semantic information. 1068 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Unlike the other papers, we derive features from the output of an SRL (Màrquez, 2009) system to explicitly model verb usage context. SRL is generally understood as the task of identifying the arguments of a given verb and assigning them sema</context>
<context position="7056" citStr="Brockett et al. (2006)" startWordPosition="1070" endWordPosition="1073">ur paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SMT training. The SMT approach on the artificial data set achieves encouraging results for correcting countability errors. Yi et al. (2008) use web frequency counts to identify and correct determiner and verb-noun collocation errors. Compared with these methods, our approach explicitly models verb usage co</context>
<context position="20217" citStr="Brockett et al. (2006)" startWordPosition="3217" endWordPosition="3220">es that contained errors related to the verbs we were interested in from English blogs written by Chinese and from the CLEC corpus, which were then corrected by an English native speaker. Furthermore, for every error involving the verbs in our target list, both the verb and the word that determines the error are marked by the English native speaker. 4.2 Baseline We built up a phrasal SMT system with the word re-ordering feature disabled, since our task only concerns the substitution of the target verb. To 3 http://opennlp.sourceforge.net/ construct the training corpus, we followed the idea in Brockett et al. (2006), and applied a similar strategy described in section 3.4 to the SRL system’s training data to generate aligned pairs. 4.3 Evaluation Metric We employed the following metrics adapted from (Yi et al., 2008): revised precision (RP), recall of the correction (RC) and false alarm (FA). RP reflects how many outputs are correct usages. The output is regarded as a correct suggestion if and only if it is exactly the same as the answer. Paraphrasing scenarios, for example, the case that the output is “take notes” and the answer is “make notes”, are counted as errors. RC indicates how many erroneous sen</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL errors using phrasal SMT techniques. Proc. of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting on Association for Computational Linguistics, pages 249-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>Proc. of the ACL-02 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5451" citStr="Collins, 2002" startWordPosition="825" endWordPosition="826">”, which is unrelated to the usage of “watch” in this case, can be easily excluded from the usage context. (iii) Research on SRL has made great achievements, including humanannotated training corpora and state-of-the-art systems, which can be directly leveraged. Taking an English sentence as input, our method first generates correction candidates by replacing each verb with verbs in its pre-defined confusion set; then for every candidate, it extracts SRLderived features; finally our method scores every candidate using a linear function trained by the generalized perceptron learning algorithm (Collins, 2002) and selects the best candidate as output. Experimental results show that SRL-derived features are effective in verb selection, but we also observe that noise in SRL output adversely increases feature space dimensions and the number of false suggestions. To alleviate this issue, we use local features, e.g., n-gram-related features, and achieve state-of-the-art performance when all features are integrated. Our contributions can be summarized as follows: 1. We propose to exploit SRL-derived features to explicitly model verb usage context. 2. We propose to use the generalized perceptron framework</context>
<context position="17748" citStr="Collins, 2002" startWordPosition="2818" endWordPosition="2820">hm, adapted from Colins (2002). (D in Figure 1 is the feature extraction function. (D(o) and are vectors extracted from the output and oracle, respectively. A vector field is filled with 1 if the corresponding feature exists, or 0 otherwise; is the feature weight vector, where positive elements suggest that the corresponding features support the hypothesis that the candidate is correct. The training process is to update w , when the output differs from the oracle. For example, when o is “I want to look TV” and is “I want to watch TV”, will be updated. We use the averaged Perceptron algorithm (Collins, 2002) to alleviate overfitting on the training data. The averaged perceptron weight vector is defined as  i r =  w ,  1 TN (3) i=1..N,r=1..T where , is the weight vector immediately after the ith sentence in the rth iteration. 2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?catalogId=LDC2005T12 1072 4 Experiments In this section, we compare our approach with the SMT-based approach. Furthermore, we study the contribution of predicate-argument-related features, and the performances on verbs with varying distance to their arguments. 4.1 Experiment Preparation The training corpus for perceptro</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. Proc. of the ACL-02 Conference on Empirical Methods in Natural Language Processing, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Eeg-Olofsson</author>
<author>Ola Knutsson</author>
</authors>
<title>Automatic Grammar Checking for Second Language Learners – the Use of Prepositions.</title>
<date>2003</date>
<booktitle>Proc. of NoDaliDa.</booktitle>
<marker>Eeg-Olofsson, Knutsson, 2003</marker>
<rawString>Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic Grammar Checking for Second Language Learners – the Use of Prepositions. Proc. of NoDaliDa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alexandre Klementiev</author>
<author>William B Dolan</author>
<author>Dmitrtiy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<booktitle>Proc. of the International Joint Conference on Natural Language Processing.</booktitle>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alexandre Klementiev, William B. Dolan, Dmitrtiy Belenko, and Lucy Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. Proc. of the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shichun Gui</author>
<author>Huizhong Yang</author>
</authors>
<title>Chinese Learner English Corpus. Shanghai Foreign Languages</title>
<date>2002</date>
<publisher>Education Press,</publisher>
<location>Shanghai, China.</location>
<contexts>
<context position="1905" citStr="Gui and Yang, 2002" startWordPosition="272" endWordPosition="275">or states of being. In addition, they also communicate sentiments and imply circumstances, e.g., in “He got [gained] the scholarship after three interviews.”, the verb “gained” may indicate that the “scholarship” was competitive and required the agent’s efforts; in contrast, “got” sounds neutral and less descriptive. * This work has been done while the author was visiting Microsoft Research Asia. Since verbs carry multiple important functions, misusing them can be misleading, e.g., the native speaker could be confused when reading “I like looking [reading] books”. Unfortunately, according to (Gui and Yang, 2002; Yi et al., 2008), more than 30% of the errors in the Chinese Learner English Corpus (CLEC) are verb choice errors. Hence, it is useful to develop an approach to automatically detect and correct verb selection errors made by ESL learners. However, verb selection is a challenging task because verbs often exhibit a variety of usages and each usage depends on a particular context, which can hardly be adequately described by conventional n-gram features. For instance, both “made” and “received” can complete “I have __ a telephone call.”, where the usage context can be represented as “made/receive</context>
</contexts>
<marker>Gui, Yang, 2002</marker>
<rawString>Shichun Gui and Huizhong Yang. 2002. Chinese Learner English Corpus. Shanghai Foreign Languages Education Press, Shanghai, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia E Heine</author>
</authors>
<title>Definiteness predictions for Japanese noun phrases.</title>
<date>1998</date>
<booktitle>Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>519--525</pages>
<marker>Heine, 1998</marker>
<rawString>Julia E. Heine. 1998. Definiteness predictions for Japanese noun phrases. Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 519-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Correcting misuse of verb forms.</title>
<date>2008</date>
<booktitle>Proc. of the 46th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>174--182</pages>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008. Correcting misuse of verb forms. Proc. of the 46th Annual Meeting on Association for Computational Linguistics, pages 174-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Ming Zhou</author>
<author>Jianfeng Gao</author>
</authors>
<title>Endong Xun and Changning Huang.</title>
<date>2000</date>
<booktitle>Proc. of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<contexts>
<context position="11939" citStr="Liu et al., 2000" startWordPosition="1874" endWordPosition="1877">role features, we restrict our research scope to correcting verb selection errors made by Chinese ESL learners and select fifty representative verbs which are among the most frequent ones and account for more than 50% of ESL verb errors in the CLEC data set. For every selected verb we manually compile a confusion set using the following data sources: 1. Encarta treasures. We extract all the synonyms of verbs from the Microsoft Encarta Dictionary, and this forms the major source for our confusion sets. 2. English-Chinese Dictionaries. ESL learners may get interference from their mother tongue (Liu et al., 2000). For example, some Chinese people mistakenly say “see newspaper”, partially because the translation of “see” co-occurs with “newspaper” in Chinese. Therefore English verbs in the dictionary sharing more than two Chinese meanings are collected. For example, “see” and “read” are in a confusion set because they share the meanings of both “看” (“to see”, “to read”) and “领会” (“to grasp”) in Chinese. 3. An SMT translation table. We extract paraphrasing verb expressions from a phrasal SMT translation table learnt from parallel corpora (Och and Ney, 2004). This may help us use the implicit semantics o</context>
</contexts>
<marker>Liu, Zhou, Gao, 2000</marker>
<rawString>Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and Changning Huang. 2000. PENS: A Machine-aided English Writing System for Chinese Users. Proc. of the 38th Annual Meeting on Association for Computational Linguistics, pages 529-536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Màrquez</author>
</authors>
<title>Semantic Role Labeling Past, Present and Future,</title>
<date>2009</date>
<journal>Tutorial of ACL-IJCNLP</journal>
<contexts>
<context position="3470" citStr="Màrquez, 2009" startWordPosition="511" endWordPosition="512">to represent verb usage context. Yi et al. (2008) introduced an unsupervised web-based proofing method for correcting verb-noun collocation errors. Brockett et al. (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. None of their methods incorporated semantic information. 1068 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Unlike the other papers, we derive features from the output of an SRL (Màrquez, 2009) system to explicitly model verb usage context. SRL is generally understood as the task of identifying the arguments of a given verb and assigning them semantic labels describing the roles they play. For example, given a sentence “I want to watch TV tonight” and the target predicate “watch”, the output of SRL will be something like “I [A0] want to watch [target predicate] TV [A1] tonight [AM-TMP].”, meaning that the action “watch” is conducted by the agent “I”, on the patient “TV”, and the action happens “tonight”. We believe that SRL results are excellent features for characterizing verb usag</context>
</contexts>
<marker>Màrquez, 2009</marker>
<rawString>Lluís Màrquez. 2009. Semantic Role Labeling Past, Present and Future, Tutorial of ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>Proc. of the HLT-NAACL Workshop on Computational Lexical Semantics,</booktitle>
<pages>60--67</pages>
<contexts>
<context position="6566" citStr="Moldovan et al. (2004)" startWordPosition="1000" endWordPosition="1003"> features to explicitly model verb usage context. 2. We propose to use the generalized perceptron framework to integrate SRL-derived (and other) features and achieve state-ofthe-art performance on both in-domain and out-of-domain test sets. Our paper is organized as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL er</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe and Roxana Girju. 2004. Models for the semantic classification of noun phrases. Proc. of the HLT-NAACL Workshop on Computational Lexical Semantics, pages 60-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>Proc. of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>693--701</pages>
<contexts>
<context position="6751" citStr="Narayanan and Harabagiu (2004)" startWordPosition="1026" endWordPosition="1029">-art performance on both in-domain and out-of-domain test sets. Our paper is organized as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SM</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question answering based on semantic structures. Proc. of the 20th International Conference on Computational Linguistics, pages 693-701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>417--449</pages>
<contexts>
<context position="12492" citStr="Och and Ney, 2004" startWordPosition="1964" endWordPosition="1967"> may get interference from their mother tongue (Liu et al., 2000). For example, some Chinese people mistakenly say “see newspaper”, partially because the translation of “see” co-occurs with “newspaper” in Chinese. Therefore English verbs in the dictionary sharing more than two Chinese meanings are collected. For example, “see” and “read” are in a confusion set because they share the meanings of both “看” (“to see”, “to read”) and “领会” (“to grasp”) in Chinese. 3. An SMT translation table. We extract paraphrasing verb expressions from a phrasal SMT translation table learnt from parallel corpora (Och and Ney, 2004). This may help us use the implicit semantics of verbs that SMT can capture but a dictionary cannot, such as the fact that the verb Note that verbs in any confusion set that we are not interested in are dropped, and that the verb itself is included in its own confusion set. We leave it to our future work to automatically construct verb confusions. 3.3 Verb Usage Context Features The verb usage context1 refers to its surrounding text, which influences the way one understands the expression. Intuitively, verb usage context can take the form of a collocation, e.g., “watch ... TV” in “I saw [watch</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Journal of Computational Linguistics, 30(4), pages 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Ivan Meza-Ruiz</author>
</authors>
<title>Collective semantic role labelling with Markov Logic.</title>
<date>2008</date>
<booktitle>Proc. of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>193--197</pages>
<contexts>
<context position="9195" citStr="Riedel and Meza-Ruiz, 2008" startWordPosition="1416" endWordPosition="1419">e set of correction candidates, and Score(s) is the linear model trained by the perceptron learning algorithm, which will be discussed in section 3.4. We call every target verb in a checkpoint. For example, “sees” is a checkpoint in “Jane sees TV every day.”. Correction candidates are generated by replacing each checkpoint with its confusions. Table 1 shows a sentence with one checkpoint and the corresponding correction candidates. Input Jane sees TV every day. Candidates Jane watches TV every day. Jane looks TV every day. � Table 1. Correction candidate list. One state-of-the-art SRL system (Riedel and Meza-Ruiz, 2008) is then utilized to extract predicate-argument structures for each verb in the input, as illustrated in Table 2. Semantic features are generated by combining the predicate with each of its arguments; e.g., “watches_A0_Jane”, “sees_A0_Jane”, “watches_A1_TV” and “sees_A1_TV” are semantic feaSentence Semantic roles Jane sees TV every day Predicate: sees; A0: Jane; A1: TV; Jane watches TV every Predicate: watches; day A0: Jane; A1: TV; Table 2. Examples of SRL outputs. At the training stage, each sentence is labeled by the SRL system. Each correction candidate is represented as a feature vector ,</context>
<context position="18699" citStr="Riedel and Meza-Ruiz (2008)" startWordPosition="2962" endWordPosition="2965">is section, we compare our approach with the SMT-based approach. Furthermore, we study the contribution of predicate-argument-related features, and the performances on verbs with varying distance to their arguments. 4.1 Experiment Preparation The training corpus for perceptron learning was taken from LDC2005T12. We randomly selected newswires containing target verbs from the New York Times as the training data. We then used the OpenNEP package3to extract sentences from the newswire text and to parse them into the corresponding tokens, POS tags, and chunks. The SRL system is built according to Riedel and Meza-Ruiz (2008), using the CoNLL-2008 shared task data for training. We assume that the newswire data is of high quality and free of linguistic errors, and finally we gathered 20000 sentences that contain any of the target verbs we were focusing on. We experimentally set the number of training rounds to T = 50. We constructed two sets of testing data for indomain and out-of-domain test purposes, respectively. To construct the in-domain test data, we first collected all the sentences that contain any of the verbs we were interested in from the previous unused LDC dataset; then we replaced any target verb in o</context>
</contexts>
<marker>Riedel, Meza-Ruiz, 2008</marker>
<rawString>Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective semantic role labelling with Markov Logic. Proc. of the Twelfth Conference on Computational Natural Language Learning, pages 193-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM -- An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>Proc. of International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="16705" citStr="Stolcke, 2002" startWordPosition="2642" endWordPosition="2643">sent the number of training iterations and training examples, respectively. GElv(s&apos;) is the function that outputs all the possible corrections for the input sentence with each checkpoint substituted by one of its confusions, as described in Section 3.1. We observe that the generated candidates sometimes contain reasonable outputs for the verb selection task, which should be removed. For instance, in “... reporters could not take [make] notes or tape the conversation”, both “take” and “make” are suitable verbs in this context. To fix this issue, we trained a trigram language model using SRILM (Stolcke, 2002) on LDC data2, and calculated the logarithms of the language model score for the original sentence and its artificial manipulations. We only kept manipulations with a language model score that is t lower than that of the original sentence. We experimentally set t = 5. Inputs: training examples , i=1...N Initialization:w=0 Algorithm: For r= 1.. T, i= 1..N Calculateo=argmax:.(D(s) •i If Outputs: Figure 1. The perceptron algorithm, adapted from Colins (2002). (D in Figure 1 is the feature extraction function. (D(o) and are vectors extracted from the output and oracle, respectively. A vector field</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM -- An Extensible Language Modeling Toolkit. Proc. of International Conference on Spoken Language Processing, pages: 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Lluis Màrquez</author>
<author>Xavier Carreras</author>
<author>Pere R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>105--151</pages>
<marker>Surdeanu, Màrquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Lluis Màrquez, Xavier Carreras, and Pere R. Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research, page 105-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>Proc. of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="6827" citStr="Surdeanu et al. (2003)" startWordPosition="1037" endWordPosition="1040">ed as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SMT training. The SMT approach on the artificial data set achieves encouraging</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. Proc. of the 41st Annual Meeting on Association for Computational Linguistics, pages 8-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection</title>
<date>2008</date>
<booktitle>in ESL writing. Proc. of the 22nd international Conference on Computational Linguistics,</booktitle>
<pages>865--872</pages>
<contexts>
<context position="2803" citStr="Tetreault and Chodorow, 2008" startWordPosition="418" endWordPosition="421">challenging task because verbs often exhibit a variety of usages and each usage depends on a particular context, which can hardly be adequately described by conventional n-gram features. For instance, both “made” and “received” can complete “I have __ a telephone call.”, where the usage context can be represented as “made/received a telephone call”; however, in “I have __ a telephone call from my boss”, the prepositional phrase “from my boss” becomes a critical part of the context, which now cannot be described by n-gram features, resulting in only “received” being suitable. Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. Yi et al. (2008) introduced an unsupervised web-based proofing method for correcting verb-noun collocation errors. Brockett et al. (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. None of their methods incorporated semantic information. 1068 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Unlike the other p</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. Proc. of the 22nd international Conference on Computational Linguistics, pages 865-872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="16006" citStr="Vapnik, 1995" startWordPosition="2527" endWordPosition="2528">: its ineffectiveness in dealing with inseparable samples and its ignorance of weight normalization that potentially limits its ability to generalize. In section 4.4 we show that the training error rate drops significantly to a very low level after several rounds of training, suggesting that the correct candidates can almost be separated from others. We also observe that our method performs well on an out-ofdomain test corpus, indicating the good generalization ability of this method. We leave it to our future work to replace perceptron learning with other models like Support Vector Machines (Vapnik, 1995). In Figure 1, s; is the ith correct sentence within the training data. T and N represent the number of training iterations and training examples, respectively. GElv(s&apos;) is the function that outputs all the possible corrections for the input sentence with each checkpoint substituted by one of its confusions, as described in Section 3.1. We observe that the generated candidates sometimes contain reasonable outputs for the verb selection task, which should be removed. For instance, in “... reporters could not take [make] notes or tape the conversation”, both “take” and “make” are suitable verbs </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Verb Sense Disambiguation Using Selectional Preferences Extracted with a State-of-the-art Semantic Role Labeler.</title>
<date>2006</date>
<booktitle>Proc. of the Australasian Language Technology Workshop,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="6650" citStr="Ye and Baldwin (2006)" startWordPosition="1014" endWordPosition="1017">d perceptron framework to integrate SRL-derived (and other) features and achieve state-ofthe-art performance on both in-domain and out-of-domain test sets. Our paper is organized as follows: In the next section, we introduce related work. In Section 3, we describe our method. Experimental results and analysis on both in-domain and out-of-domain corpora are presented in Section 4. Finally, we conclude our paper with a discussion of future work in Section 5. 2 Related Work SRL results are used in various tasks. Moldovan et al. (2004) classify the semantic relations of noun phrases based on SRL. Ye and Baldwin (2006) apply semantic role–related information to verb sense disambiguation. Narayanan and Harabagiu (2004) use semantic role structures for question answering. Surdeanu et al. (2003) employ predicate-argument structures for information extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-qua</context>
</contexts>
<marker>Ye, Baldwin, 2006</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2006. Verb Sense Disambiguation Using Selectional Preferences Extracted with a State-of-the-art Semantic Role Labeler. Proc. of the Australasian Language Technology Workshop, pages 141-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Yi</author>
<author>Jianfeng Gao</author>
<author>William B Dolan</author>
</authors>
<title>A Web-based English Proofing System for English as a Second Language Users.</title>
<date>2008</date>
<booktitle>Proc. of International Joint Conference on Natural Language Processing,</booktitle>
<pages>619--624</pages>
<contexts>
<context position="1923" citStr="Yi et al., 2008" startWordPosition="276" endWordPosition="279">In addition, they also communicate sentiments and imply circumstances, e.g., in “He got [gained] the scholarship after three interviews.”, the verb “gained” may indicate that the “scholarship” was competitive and required the agent’s efforts; in contrast, “got” sounds neutral and less descriptive. * This work has been done while the author was visiting Microsoft Research Asia. Since verbs carry multiple important functions, misusing them can be misleading, e.g., the native speaker could be confused when reading “I like looking [reading] books”. Unfortunately, according to (Gui and Yang, 2002; Yi et al., 2008), more than 30% of the errors in the Chinese Learner English Corpus (CLEC) are verb choice errors. Hence, it is useful to develop an approach to automatically detect and correct verb selection errors made by ESL learners. However, verb selection is a challenging task because verbs often exhibit a variety of usages and each usage depends on a particular context, which can hardly be adequately described by conventional n-gram features. For instance, both “made” and “received” can complete “I have __ a telephone call.”, where the usage context can be represented as “made/received a telephone call</context>
<context position="7488" citStr="Yi et al. (2008)" startWordPosition="1139" endWordPosition="1142">ormation extraction. However, in the context of ESL error detection and correction, little study has been carried out on clearly exploiting semantic information. Brockett et al. (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. They devise several heuristic rules to generate synthetic data from a high-quality newswire corpus and then use the synthetic data together with their original counterparts for SMT training. The SMT approach on the artificial data set achieves encouraging results for correcting countability errors. Yi et al. (2008) use web frequency counts to identify and correct determiner and verb-noun collocation errors. Compared with these methods, our approach explicitly models verb usage context by leveraging the SRL output. The SRL-based semantic features are integrated, along with the local features, into the generalized perceptron model. 1069 3 Our Approach tures derived from the semantic roles listed in Table 2. Our method can be regarded as a pipeline consisting of three steps. Given as input an English sentence written by ESL learners, the system first checks every verb and generates correction candidates by</context>
<context position="20422" citStr="Yi et al., 2008" startWordPosition="3251" endWordPosition="3254">error involving the verbs in our target list, both the verb and the word that determines the error are marked by the English native speaker. 4.2 Baseline We built up a phrasal SMT system with the word re-ordering feature disabled, since our task only concerns the substitution of the target verb. To 3 http://opennlp.sourceforge.net/ construct the training corpus, we followed the idea in Brockett et al. (2006), and applied a similar strategy described in section 3.4 to the SRL system’s training data to generate aligned pairs. 4.3 Evaluation Metric We employed the following metrics adapted from (Yi et al., 2008): revised precision (RP), recall of the correction (RC) and false alarm (FA). RP reflects how many outputs are correct usages. The output is regarded as a correct suggestion if and only if it is exactly the same as the answer. Paraphrasing scenarios, for example, the case that the output is “take notes” and the answer is “make notes”, are counted as errors. RC indicates how many erroneous sentences are corrected among all the errors. It measures the system’s coverage of verb selection errors. FA is related to the cases where a correct verb is mistakenly replaced by an inappropriate one. These </context>
</contexts>
<marker>Yi, Gao, Dolan, 2008</marker>
<rawString>Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A Web-based English Proofing System for English as a Second Language Users. Proc. of International Joint Conference on Natural Language Processing, pages 619-624.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>