<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000230">
<title confidence="0.9993655">
Context-based Message Expansion for Disentanglement
of Interleaved Text Conversations
</title>
<author confidence="0.997731">
Lidan Wang
</author>
<affiliation confidence="0.998076">
Computer Science Dept./UMIACS
University of Maryland, College Park
</affiliation>
<address confidence="0.946953">
College Park, MD 20742
</address>
<email confidence="0.999273">
lidan@cs.umd.edu
</email>
<author confidence="0.920401">
Douglas W. Oard
</author>
<affiliation confidence="0.990854">
College of Information Studies/UMIACS
and HLT Center of Excellence
University of Maryland, College Park
</affiliation>
<address confidence="0.947156">
College Park, MD 20742
</address>
<email confidence="0.999491">
oard@umd.edu
</email>
<sectionHeader confidence="0.995667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999281333333334">
Computational processing of text exchanged
in interactive venues in which participants en-
gage in simultaneous conversations can bene-
fit from techniques for automatically grouping
overlapping sequences of messages into sepa-
rate conversations, a problem known as “dis-
entanglement.” While previous methods ex-
ploit both lexical and non-lexical information
that exists in conversations for this task, the
inter-dependency between the meaning of a
message and its temporal and social contexts
is largely ignored. Our approach exploits con-
textual properties (both explicit and hidden)
to probabilistically expand each message to
provide a more accurate message representa-
tion. Extensive experimental evaluations show
our approach outperforms the best previously
known technique.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999122106382979">
Conversational media such as the text messages
found in Internet Relay Chat presents both new op-
portunities and new challenges. Among the chal-
lenges are that individual messages are often quite
short, for the reason that conversational participants
are able to assemble the required context over the
course of a conversation. A natural consequence of
this is that many tasks that we would like to perform
on conversational media (e.g., search, summariza-
tion, or automated response) would benefit from re-
assembly of individual messages into complete con-
versations. This task has been studied extensively in
the context of email (where it is often referred to as
“threading”) (Yeh et al., 2006). The extensive meta-
data associated with email and the relatively rich
content of some email messages makes email some-
what of a special case in the broad set of conversa-
tion recovery tasks, however. At the opposite ex-
treme, conversation “threading” in multi-party spo-
ken interactions (e.g., meetings) would be a com-
pelling application, but the word error rate of current
automated transcription techniques somewhat limits
access to the lexical evidence that we know is use-
ful for this task. The recent interest in identifying
individual conversations from online-discussions, a
task that some refer to as “disentanglement,” there-
fore seems to be something of a middle ground in
the research space: computationally tractable, repre-
sentative to some degree of a broader class of prob-
lems, and directly useful as a pre-processing step for
a range of important applications.
One way to think of this task is as a clustering
problem—we seek to partition the messages into a
set of disjoint clusters, where each cluster represents
a conversation among a set of participants on a topic.
This formulation raises the natural question of how
we should design a similarity measure. Since the
messages are often too short to be meaningful by
themselves, techniques based solely on lexical over-
lap (e.g., inner products of term vectors weighted
by some function of term frequency, document fre-
quency and message length) are unlikely to be suc-
cessful. For instance, consider the multi-party ex-
change in Figure 1, in which a single message may
not convey much about the topic without consider-
ing what has been said before, and who said it.
Fortunately for us, additional sources of evidence
</bodyText>
<page confidence="0.945293">
200
</page>
<note confidence="0.959240888888889">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 200–208,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
(18323 Ricardo) is there a way to emulate input for a
program listening on a COM port?
(18911 Azzie) Ricardo: Hello there, how is it going?
(18939 Ricardo) pretty good, just at the office, about to
leave. How are you?
(18970 Azzie) well, end of semester work, what could
be better?
</note>
<figureCaption confidence="0.4246244">
(18980 Josephina) if it&apos;s just reading from /dev/ttyS0 or
something you could somehow get it to just read from a
named pipe instead
(19034 Ricardo) Josephina: I might just have to end up
modifying the entire program...
(19045 Ricardo) so it can read from a different input
stream
Figure 1: An example of the text message stream. The
number before each author’s name denotes the time-
stamp of the message.
</figureCaption>
<bodyText confidence="0.9998308125">
are available. As we describe below, messages
are strongly correlated both temporally (i.e., across
time) and socially (i.e,, across participants). For
example, in our running example in Figure 1, Ri-
cardo’s message (19045 Ricardo) “so it can read
from a different input stream” elaborates on his
previous message (19034 Ricardo) to Josephina.
Messages that are close in time and from the
same speaker can share related meanings. Simi-
larly, we see that Ricardo’s messages to Josephina
(19034 Ricardo and 19045 Ricardo) are responses
to earlier comments made by Josephina (18980
Josephina), and that fact is signaled by Ricardo in-
voking Josephena’s name. This is an example of
social correlation: lexicalized references to identity
can also provide useful evidence. If we take so-
cial and temporal context into account, we should be
able to do better at recognizing conversations than
we could using lexical overlap alone.
In recent years, several approaches have been de-
veloped for detecting conversational threads in dy-
namic text streams (Elsner et al., 2008; Shen et
al., 2006; Wang et al., 2008). Although they use
both lexical and non-lexical information (e.g., time,
name mentions in message) for this task, they have
ignored the temporal and social contexts a message
appears in, which provide valuable cues for inter-
preting the message. Correlation clustering used in
a two-step approach (Elsner et al., 2008) exploits
message contexts to some degree, but its perfor-
mance is largely limited by the classifier used in the
first-step which computes message similarity with-
out considering the temporal and social contexts of
each message.
Our approach exploits contextual properties (both
explicit and hidden) to probabilistically expand each
message to provide a more accurate message rep-
resentation. The new representation leads to a much
improved performance for conversation disentangle-
ment. We note that this is a general approach and can
be applied to the representation of non-chat data that
exhibits temporal and social correlations as well.
The results that we obtain with this technique are
close to the limit of what we can measure using
present test collections and evaluation measures. To
the best of our knowledge, our work is the first to
apply document expansion to the conversation dis-
entanglement problem.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99940525">
Previous work in conversation disentanglement
(i.e. thread detection) has shown the conven-
tional lexical-based clustering is not suitable for text
streams because messages are often too short and
incomplete. They focus on using discourse/chat-
specific features to bias the lexical-based message
similarity (Elsner et al., 2008; Shen et al., 2006;
Wang et al., 2008). These features provide the
means to link messages that may not have sufficient
lexical overlap but are nevertheless likely to be top-
ically related. However, our work is different from
them in several aspects:
</bodyText>
<listItem confidence="0.982293">
(1) They treat individual messages as the basic ele-
ments for clustering, and ignore the social and tem-
poral contexts of the messages. In our work, each
message is probabilistically expanded using reliable
information from its contexts and the expanded mes-
sages are the basic elements for clustering.
(2) Messages have different amount of explicit infor-
mation. For example, messages that initiate conver-
sations may have more name mentions than subse-
quent messages (i.e. for establishing conversations).
Previous work only uses what are explicitly present
in each message, and clusters may be erroneously
assigned for messages that lack enough explicit in-
</listItem>
<page confidence="0.997805">
201
</page>
<bodyText confidence="0.998721875">
formation. Our work exploits both explicit and im-
plicit context for each message due to how we define
contexts (Section 3.2.1).
(3) Most work imposes a fixed window size for clus-
tering and it may break up long conversations or may
not be fine-grained enough for short conversations.
Given each message, we use an exponential decay
model to naturally encode time effect and assign dif-
ferential weights to messages in its contexts.
Another thread of related work is document ex-
pansion. It was previously studied in (Singhal et al.,
1999) in the context of the speech retrieval, helping
to overcome limitations in the transcription accuracy
by selecting additional terms from lexically simi-
lar (text) documents. Document expansion has also
been applied to cross-language retrieval in (Levow
et al., 2005), in that case to overcome limitations
in translation resources. The technique has recently
been re-visited (Tao et al., 2006; Kurland et al.,
2004; Liu et al., 2004) in the language modeling
framework, where lexically related documents are
used to enlarge the sample space for a document
to improve the accuracy of the estimated document
language model. However, these lexical-based ap-
proaches are less well suited to conversational in-
teraction, because conversational messages are often
short, they therefore may not overlap sufficiently in
words with other messages to provide a useful basis
for expansion. Our technique can be viewed as an
extension of these previous methods to text streams.
Our work is also related to text segmentation (Ji
et al., 2003) and meeting segmentation (Malioutov
et al., 2006; Malioutov et al., 2007; Galley et al.,
2003; Eisenstein et al., 2008). Text segmentation
identifies boundaries of topic changes in long text
documents, but we form threads of messages from
streams consisting of short messages. Meeting con-
versations are not as highly interleaving as chat con-
versations, where participants can create a new con-
versation at any time.
</bodyText>
<sectionHeader confidence="0.986435" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9999495">
This section describes our technique for clustering
messages into threads based on the lexical similar-
ity of documents that have been expanded based on
social and temporal evidence.
</bodyText>
<subsectionHeader confidence="0.99868">
3.1 Context-Free Message Model
</subsectionHeader>
<bodyText confidence="0.99995776">
To represent the semantic information of messages
and threads (clusters of messages), most of the prior
approaches build a document representation on each
message alone (using word features and time-stamp
and/or discourse features found in the message). We
call such a model a context-free message model.
Most commonly, a message is represented as a vec-
tor (Salton, 1989). Each dimension corresponds to
a separate term. If a term occurs in the message,
its value in the vector is non-zero. Several dif-
ferent ways of computing these values, known as
term weights, have been developed. One of the best
known schemes is tf-idf weighting.
However, in conversational text, a context-free
model cannot fully capture the semantics of mes-
sages. The meaning of a message is highly depen-
dent on other messages in its context. For example,
in our running example in Figure 1, to fully interpret
the message 19045 Ricardo, we need to first read
his previous message (19034 Ricardo) to Josephina.
Further, messages on the same topic may have lit-
tle or no overlap in words (Figure 1), and the mes-
sages between participants are highly interactive and
are often too short and incomplete to fully capture a
topic on their own.
</bodyText>
<subsectionHeader confidence="0.998177">
3.2 Context-Sensitive Message Model
</subsectionHeader>
<bodyText confidence="0.9999946875">
Our main idea is to exploit the temporal and so-
cial aspects of the conversations to build a context-
sensitive document model for each message. We
do this by first identifying the temporal and social
contexts for each message, then probabilistically ex-
panding the content of each message with selected
messages in each context. As we have seen, a mes-
sage’s contexts provide valuable cues for interpret-
ing the message. Finally, we cluster the messages
into distinct conversations based on their new repre-
sentation models.
We present the formal definitions of each context
and discuss how to model them in Section 3.2.1. In
Section 3.2.2, we show how to efficiently identify
the related messages in each context, and how to use
them to expand our representation of the message.
</bodyText>
<subsubsectionHeader confidence="0.605128">
3.2.1 Social and Temporal Contexts
</subsubsectionHeader>
<bodyText confidence="0.982281">
Social contexts: we define two kinds of social con-
texts: author context and conversational context. We
</bodyText>
<page confidence="0.962741">
202
</page>
<figure confidence="0.982350892857143">
Time diff. between messages from same author
Time difference between name mention
Time difference between message pairs
1
0.8
0.6
0.4
0.2
0
−400 −200 0 200 400
1
0.8
0.6
0.4
0.2
0
−400 −200 0 200 400
Probability in same thread
Probability in same thread
Probability in same thread
0.8
0.6
0.4
0.2
0
−400 −200 0 200 400
1
(i) (ii) (iii)
</figure>
<figureCaption confidence="0.992056666666667">
Figure 2: (i) Relationship between messages from the same author (ii) Relationship between messages that mention
each other’s authors, and (iii) All pairs of messages as a function of time. Estimation is based on training data used in
experiments.
</figureCaption>
<bodyText confidence="0.9465845">
explain them in detail below.
Author context: the author context of a message
m, denoted by CA(m), is the set of other messages
written by m’s author am:
</bodyText>
<equation confidence="0.995111">
CA(m) = {miJami = am,m =� mi}
</equation>
<bodyText confidence="0.999986588235294">
Further, because of the nature of human conversa-
tions, we would be less surprised to find messages
from the same person belonging to the same conver-
sation if they are close in time rather than far apart.
This is illustrated in Figure 2(i) 1, which shows the
probability that a pair of messages written by the
same person belong to the same conversation as a
function of the time difference between them. Not
surprisingly, messages in m’s author context have
probabilities which are influenced by their temporal
proximity to m.
We use a normal distribution (Figure 2(i)) to en-
code the notion of author context. Given two mes-
sages mi and mj written by the same author, each
with time-stamp ti and tj, respectively, the proba-
bility that mj is topically related to mi given their
time difference d = tj − ti is:
</bodyText>
<equation confidence="0.956273">
Pa(d) = N(µa, σ2a) = 1 e
σa √2π
</equation>
<bodyText confidence="0.998502333333333">
The exponential decay helps to limit the influence
from temporally remote messages. For message mi,
this distribution models the uncertainty that mes-
sages in its author context (i.e. other messages mj
from the same author) belong to the same conver-
sation by assigning assigning a high value to mj if
</bodyText>
<footnote confidence="0.9520875">
1Gaussian kernels shown for illustration purpose in Figure 2
are un-normalized.
</footnote>
<bodyText confidence="0.993734">
tj − ti is small. The mean µa is chosen to be zero so
that the curve is centered at each message. The vari-
ance can be readily estimated from training data.
Conversational context: the second kind of so-
cial context is the conversational context, which is
constructed from name mentions. As pointed out by
previous linguistic studies of discourse, especially
analysis of multi-party conversation (ONeill et al.,
2003), one key difference between multi-party con-
versation and typical two-party conversation is the
frequency with which participants mention each oth-
ers’ names. Name mentioning is hypothesized as a
strategy for participants to compensate for the lack
of cues normally present in face-to-face dialogue
(ONeill et al., 2003; Elsner et al., 2008). Although
infrequent, name mentions (such as Azzie’s com-
ments to Ricardo in Figure 1) provide a means for
linking two speakers and their messages.
The conversational context of m, CC(m), is de-
fined to be the set of all messages written by peo-
ple whose names are mentioned in any of am’s mes-
sages (where am is the author of m), or who mention
am in their messages. Let Ma denote all messages
written by author a. The conversational context of
m is:
</bodyText>
<equation confidence="0.868951">
CC(m) = {ba MaJmention(am, a)}
U {ba MaJmention(a, am)}
</equation>
<bodyText confidence="0.9661594">
where mention(am, a) = true if author am men-
tions a in any of am’s messages. Mention(a, am)
is similarly defined.
Discussion: From the definition, mj is included in
mi’s conversational context if the author of mi men-
</bodyText>
<equation confidence="0.9634595">
(d−ma)2
− 2σ2a
</equation>
<page confidence="0.993212">
203
</page>
<bodyText confidence="0.999938018518519">
tions the author of mj in any of mi’s messages, or
vice versa. For instance, the conversational con-
text for Ricardo’s message (19034 Ricardo) in Fig-
ure 1 includes the messages from Josephina (18980
Josephina) due to the mentioning of Josephina in
his message. However, it may well be the case that
mi does not contain any name mentions, e.g. Ri-
cardo’s message to Azzie (18939 Ricardo). In this
case, if Ricardo is being mentioned by another au-
thor (here Azzie asks Ricardo a question by start-
ing with his name in 18939 Azzie), message (18939
Ricardo)’s conversational context will contain all of
Azzie’s messages (18911 and 18970 Azzie) accord-
ing to the above definition. This intuitively captures
the implicit question-answer patterns in conversa-
tional speech: Ricardo’s subsequent answer is a re-
sponse to Azzie’s comments, hence they are in each
other’s conversational context.
Our definition also accounts for another source of
implicit context. In interactive conversations name
mention is a tool for getting people’s attention and
starting a conversation. Once a participant ai estab-
lishes a conversation with aj (such that ai may men-
tion aj’s name in an initial message mp to aj), ai
may stop mentioning aj’s name in subsequent mes-
sages (mq) to aj. This is illustrated in Ricardo’s last
message to Josephina in Figure 1. Our definition
accounts for the conversation continuity between aj
and ai by including messages from aj in the conver-
sational context of subsequent messages mq from ai
(note mq may or may not mention aj). For instance,
message 19045 Ricardo continues the conversation
with Josephina from 19034 Ricardo, message 19045
Ricardo thus has Josephina’s messages as part of its
conversational context.
In general, a person can participate in multiple
conversations over time, but as time goes on the
topic of interest may shift and the person may start
talking to other people. So the messages in the con-
versational context of mi due to earlier discussions
with other people should be assigned a lower con-
fidence value for mi. For example, five hours later
Ricardo may still be active, but it is unlikely he still
chats with Josephina on the same topic, so the ear-
lier messages by Josephina should receive a small
confidence value in the conversational context of Ri-
cardo’s later messages. We illustrate this idea in Fig-
ure 2(ii). It shows the probability that message mj,
where mj E CC(mi), belongs to the same thread
as mi, given their time difference tj − ti. This
is encoded with a normal probability distribution,
N(pc, Qc) where pc = 0 and variance is estimated
from training data. Let d = tj − ti, the probability
they are topically related given mj E CC(mi) is:
</bodyText>
<equation confidence="0.870603666666667">
1
Pc(d) = e
Qc.\/2π
Temporal context: temporal context for message
m, CT (m), refers to all other messages:
CT(m) = M \ m
</equation>
<bodyText confidence="0.994765">
where M denotes the entire set of messages. The
intuition is that nearby messages to m can provide
further evidence to the semantics of m. This is illus-
trated in Figure 2(iii). From the viewpoint of doc-
ument smoothing, this can also be regarded as us-
ing temporally nearby messages to smooth the rep-
resentation of m. So given mi, we again model its
temporal context by fitting a normal probability dis-
tribution N(pt, Qt), so that if mj E CT(mi) and
d = tj − ti, the probability that mj is topically re-
lated to mi is:
</bodyText>
<equation confidence="0.931424666666667">
1
Pt(d) = e
Qt�/2π
</equation>
<subsectionHeader confidence="0.968785">
3.2.2 Constructing Expanded Messages
</subsectionHeader>
<bodyText confidence="0.983677647058823">
We have shown how to use the social and tem-
poral aspects of conversational text to identify and
model the contexts of each message, and how to
assign confidence values to messages in its con-
texts. We now show how to use a message’s con-
texts and their associated messages to probabilisti-
cally expand the given message. We hypothesize
that the expanded message provides a more accurate
message representation and that this improved repre-
sentation can lead to improved accuracy for conver-
sation disentanglement. We will test this hypothesis
in the experiment section.
Each message m is represented as a vector of es-
timated term counts. We expand m using the nor-
malized messages in its contexts. For the expanded
message m&apos; of m we estimate the term counts as a
linear mixture of term counts from each message in
</bodyText>
<equation confidence="0.7696978">
2σ2c
d2
−
− d2
2σ2t
</equation>
<page confidence="0.993607">
204
</page>
<table confidence="0.988310454545455">
Min Mean Max
Number of Conversations 50.00 81.33 128.00
Avg. Conv. Length 6.20 10.60 16.00
Avg. Conv. Density 2.53 2.75 2.92
E Pa(dji) x c(w, mj) Table 1: Statistics on the IRC chat transcript data (Elsner
+ AA Pt(dji) x c(w, mj)I et al., 2008). The reported values are based on annota-
mjECA(m) tions from six different annotations for the 800 lines of
E chat transcript.
+ AT
mjECT (m)
each context:
</table>
<equation confidence="0.711477666666667">
c(w, m&apos;) = αc(w, m) + (1 − α){
AC E Pc(dji) x c(w, mj)
mjECC(m)
</equation>
<bodyText confidence="0.999616739130435">
These parameter values are tuned on training data:
α controls how much relative weight we give to lex-
ical content of m (0.45 in our experiments), and
AC, AA and AT are the relative weights assigned
to the conversational, author and temporal contexts
(0.6, 0.3, and 0.1 in our experiments, respectively).
A context with large variance in its normal density
graph should receive a small A value. This is be-
cause a large variance in context k implies more un-
certainty on a message mj being topically related to
m while mj is in the context k of m. In Figure 2,
the conversational context (Figure 2(ii)) has the min-
imum variance among all contexts, hence, it is more
accurate for linking messages related in topic and it
is assigned a higher A value (0.6), while the tempo-
ral context has the lowest A value (0.1). Finally, for
a message mj in context k of mi, Pk(dji) indicates
how strongly we believe mj is topically related to
mi, given their time difference dji.
Because of the exponential decays of the normal
densities that model contexts k, messages in a con-
text will contribute differentially to mi. Temporally
distant messages will have a very low density.
</bodyText>
<subsectionHeader confidence="0.999255">
3.3 Single-Pass Clustering
</subsectionHeader>
<bodyText confidence="0.999843">
The expanded messages are the basic elements for
clustering. The cosine is used to measure similarity:
</bodyText>
<equation confidence="0.464802">
c(w, mi)c(w, mj)
IImiIIIImjII
</equation>
<bodyText confidence="0.995850363636363">
Single-pass clustering is then performed: treat the
first message as a single-message cluster T; for each
remaining message m compute VT:
sim(m, T) = maxmiET sim(mi, m)
For the thread T that maximizes sim(m,T), if
sim(m, T) &gt; tsim, where tsim is a threshold (0.7 in
our experiments) empirically estimated from train-
ing data, add m to T; else, start a new cluster con-
taining only m. The time complexity of this algo-
rithm is O(n2), which is tractable for problems of
moderate size.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999972476190476">
The collection used in the experiments consists of
real text streams produced in Internet Relay Chat,
created by (Elsner et al., 2008) and annotated inde-
pendently by six annotators. As an upper (human)
baseline for each of the three measures reported be-
low, we report the average agreement between all
pairs of annotators (i.e., treating one annotator as
truth and another as a “system”). For our experi-
ment results, we report the average across all anno-
tators of the agreement between our system and each
annotator.
The test collection also contains both a develop-
ment set and an evaluation set. We used the devel-
opment set to approximate the normal densities used
in our context models and the evaluation set to ob-
tain the results reported below. Some statistics for
the 800 annotated messages in the chat transcript of
the evaluation collection are shown in Table 1. As
that table shows, the average number of active con-
versation at a given time is 2.75, which makes thread
detection a non-trivial task.
</bodyText>
<subsectionHeader confidence="0.992882">
4.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9915102">
We conduct comparisons using three commonly
used evaluation measures for the thread detection
task. As a measure of the systems ability to group
related messages we report the F-measure (Shen et
al., 2006):
</bodyText>
<equation confidence="0.951315">
maxj(F(i, j))
Esim(mi, mj) =
w
EF =
i
</equation>
<page confidence="0.728864666666667">
ni
n
205
</page>
<bodyText confidence="0.999982642857143">
where i is a ground-truth conversation with length
ni, and n is the length of entire transcript. F(i, j)
is the harmonic mean of recall (fraction of the mes-
sages in the i also present in j) and precision (frac-
tion of messages in j also present in i), and F is
a weighted sum over all ground-truth conversations
(i.e., F is microaveraged).
Two other evaluation measures are “one-to-one
accuracy” and “local agreement” (Elsner et al.,
2008). “One-to-one accuracy” measures how well
we extract whole conversations intact (e.g., as might
be required for summarization). It is computed by
finding the max-weight bipartite matching between
the set of detected threads and the set of real threads,
where weight is defined in terms of percentage over-
laps for each ground truth and detected thread pair.
Some applications (e.g., real-time monitoring)
may not require that we look at entire conversations
ar once; in this case a “local agreement” measure
might make more sense. “loc3” between system and
human annotations as the average (over all possible
sets of three consecutive messages) of whether those
3 consecutive messages are assigned consistently by
the ground truth and the system. For example, if
both the ground truth and the system cluster the first
and third messages together and place the second
message in a different cluster, then agreement would
be recorded.
</bodyText>
<subsectionHeader confidence="0.999177">
4.2 Methods Used in Comparison
</subsectionHeader>
<bodyText confidence="0.975061923076923">
We compare with the following methods:
Elsner et al. 2008 (best previously known tech-
nique): Message similarity is computed with lexical
and discourse features, but without document
expansion.
Blocks of k: Every consecutive group of k messages
is a conversation.
Pause of k: Every pause of k seconds or more
separate two conversations.
Speaker: Each speaker’s messages are treated as a
single conversation.
All different: Each utterance is a separate thread.
All same: The entire transcript is one conversation.
</bodyText>
<sectionHeader confidence="0.790385" genericHeader="evaluation">
4.3 Results
</sectionHeader>
<figureCaption confidence="0.988772">
Figure 3 compares the effectiveness of different
schemes in terms of the F measure. We show results
Figure 3: F measure. The dotted line represents inter-
annotator agreement.
</figureCaption>
<bodyText confidence="0.99997228125">
from the best baseline, Elsner and our technique
(which we call the Context model). The average F
between human annotators is shown with the dotted
line at 0.55; we would expect this to be an upper
bound for any model. Our method substantially out-
performs the other methods, with a 24% improve-
ment over Elsner and 48% improvement over the
best baseline (speaker). Viewed another way, our
system achieves 98% of human performance, while
Elsner and the best baseline achieve 79% and 66% of
that bound, respectively. From this, we can conclude
that our Context model is quite effective at cluster-
ing messages from same conversation together.
To illustrate the impact of conversation length,
we binned the lengths of ground-truth conversations
from a single assessor into bins of size 5 (i.e., 3–7
messages, 8–12 messages, ...; there were no ground
truth bins of size 1 or 2). Figure 4 plots the approx-
imated microaveraged F at the center value of each
bin (i.e., the F for each ground truth cluster, scaled
by the number of messages in the cluster). These
fine-grained values provide insight into the contri-
bution of conversations of different sizes to the over-
all microaveraged F. The Context model performs
well for every conversation length, but particularly
so for conversations containing 35 or more messages
as shown by the widened gap in that region. Long
conversations usually have richer social and tempo-
ral contexts for each message. The context model
can benefit more from drawing evidences from these
sources and using them to expand the message, thus
makes it possible to group messages of the same
</bodyText>
<page confidence="0.99785">
206
</page>
<figureCaption confidence="0.99846975">
Figure 4: Dependence of F on ground-truth conversation
size, in number of messages.
Figure 5: One-to-one measure. The dotted line represents
inter-annotator agreement.
</figureCaption>
<bodyText confidence="0.990865631578947">
conversation together. The other two methods that
ignore contextual properties do not do well in com-
parison.
To measure how well we extract whole conversa-
tions intact, Figure 5 shows the results in terms of
the one-to-one measure, where each real conversa-
tion is matched up with a distinct detected conversa-
tion thread. It is computed by max-weight bipartite
matching such that the total message overlap is max-
imized between the sets of detected threads and real
threads. The average by this measure between hu-
man annotators is 0.53. In this case, the proposed
context model achieves an 14% increase over El-
sner and 32% increase over the best baseline, and
it is within 88% of human performance. This fairly
clearly indicates that our Context model can disen-
tangle interleaved conversations relatively well.
Finally, Figure 6 presents the results for “local-3”
to evaluate the system’s ability to do local annota-
</bodyText>
<figureCaption confidence="0.9911715">
Figure 6: Local-3 measure. The dotted line represents
inter-annotator agreement.
</figureCaption>
<bodyText confidence="0.99992625">
tions. The difference between the best baseline and
maximum upper bound is small, implying limited
room for potential improvement by any non-baseline
techniques. Our result again compares favorably
with the previously reported result and the best base-
line, although with a smaller margin of 20% over the
best baseline and 3% over Elsner as a result of the
relatively high baseline for this measure.
</bodyText>
<sectionHeader confidence="0.990529" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999998210526316">
We have presented an approach that exploits contex-
tual properties to probabilistically expand each mes-
sage to provide a more accurate message represen-
tation for dynamic conversations. It is a general ap-
proach and can be applied to the representation of
non-chat data that exhibits temporal and social cor-
relations as well. For conversation disentanglement,
it outperforms the best previously known technique.
Our work raises three important questions: (1) to
what extent is the single test collection that we have
used representative of the broad range of “text chat”
applications?, (2) to what extent do the measures we
have reported correlate to effective performance of
downstream tasks such as summarization or auto-
mated response?, and (3) can we re-conceptualize
the formalized problem in a way that would result
in greater inter-annotator agreement, and hence pro-
vide scope for further refinements in our technique.
These problems will be the focus of our future work.
</bodyText>
<page confidence="0.996271">
207
</page>
<sectionHeader confidence="0.995613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999783768421053">
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? A Corpus and Algorithm for Conversa-
tion Disentanglement. In ACL 2008: Proceedings of
the 46th Annual Meeting on Association for Compu-
tational Linguistics, pages 834-842, Columbus, OH,
USA. Association for Computational Linguistics.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread Detection in Dynamic Text Mes-
sage Streams. In SIGIR 2006: Proceedings of the
29th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 35-42, Seattle, WA, USA. Association for Com-
puting Machinery.
Yi-Chia Wang, Mahesh Joshi, William Cohen, and Car-
olyn Rose. 2008. Recovering Implicit Thread Struc-
ture in Newsgroup Style Conversations. In ICWSM
2008: Proceedings of the 2nd International Confer-
ence on Weblogs and Social Media, pages 152-160,
Seattle, WA, USA. Association for the Advancement
of Artificial Intelligence.
Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXi-
ang Zhai. 2006. Language Model Information Re-
trieval with Document Expansion. In HLT-NAACL
2006: Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of the
ACL, pages 407-414, New York, NY, USA. Associa-
tion for Computational Linguistics.
Oren Kurland and Lillian Lee. 2004. Corpus Structure,
Language Models, and AdHoc Information Retrieval.
In SIGIR 2004: Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 194-201,
Sheffield, UK. Association for Computing Machinery.
Xiaoyong Liu and W Croft. 2004. Cluster-based Re-
trieval Using Language Models. In SIGIR 2004: Pro-
ceedings of the 27th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 186-193, Sheffield, UK. Associa-
tion for Computing Machinery.
Amit Singhal and Fernando Pereira. 1999. Document
Expansion for Speech Retrieval. In SIGIR 1999: Pro-
ceedings of the 22nd annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 34-41, Berkeley, CA, USA. Asso-
ciation for Computing Machinery.
Xiang Ji and Hongyuan Zha 2003. Domain-Independent
Text Segmentation using Anisotropic Diffusion and
Dynamic Programming. In SIGIR 2003: Proceedings
of the 26th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 322-329, Toronto, Canada. Association
for Computing Machinery.
Michel Galley, Kathleen McKeown, Eric Lussier, and
Hongyan Jing. 2003. Discourse Segmentation of
Multi-Party Conversation. In ACL 2003: Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 562-569, Sapporo,
Japan. Association for Computational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
Unsupervised Topic Segmentation. In EMNLP 2008:
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 334-
343, Honolulu, Hawaii, USA. Association for Compu-
tational Linguistics.
Igor Malioutov and Regina Barzilay 2006. Minimum-
Cut Model for Spoken Lecture Segmentation. In ACL
2006: Proceedings of the 44rd Annual Meeting of the
Association for Computational Linguistics, pages 25-
32, Sydney, Australia. Association for Computational
Linguistics.
Igor Malioutov, Alex Park, Regina Barzilay, and James
Glass. 2007. Making Sense of Sound: Unsuper-
vised Topic Segmentation over Acoustic Input. In ACL
2007: Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 504-
511, Prague, Czech Republic. Association for Compu-
tational Linguistics.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email Thread
Reassembly Using Similarity Matching. In CEAS
2006: The 3rd Conference on Email and Anti-Spam,
pages 64-71, Mountain View, CA, USA.
Jacki ONeill and David Martin. 2003. Text Chat in Ac-
tion. In ACM SIGGROUP 2003: Proceedings of the
2003 International ACM SIGGROUP Conference on
Supporting Group Work, pages 40-49, New York, NY,
USA. ACM Press.
Gerard Salton. 1989. Automatic Text Processing: the
Transformation, Analysis and Retrieval of Information
by Computer. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA, 1989.
Gina-Anne Levow, Douglas Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-language
information retrieval. In Information Processing and
Management Special Issue: Cross-Language Informa-
tion Retrieval, 41(3): 523-547.
</reference>
<page confidence="0.997779">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882510">
<title confidence="0.9986875">Context-based Message Expansion for of Interleaved Text Conversations</title>
<author confidence="0.960181">Lidan</author>
<affiliation confidence="0.9997995">Computer Science University of Maryland, College</affiliation>
<address confidence="0.98118">College Park, MD</address>
<email confidence="0.999481">lidan@cs.umd.edu</email>
<author confidence="0.97522">W Douglas</author>
<affiliation confidence="0.998510666666667">College of Information and HLT Center of University of Maryland, College</affiliation>
<address confidence="0.982892">College Park, MD</address>
<email confidence="0.999163">oard@umd.edu</email>
<abstract confidence="0.999219578947368">Computational processing of text exchanged in interactive venues in which participants engage in simultaneous conversations can benefit from techniques for automatically grouping overlapping sequences of messages into separate conversations, a problem known as “disentanglement.” While previous methods exploit both lexical and non-lexical information that exists in conversations for this task, the inter-dependency between the meaning of a message and its temporal and social contexts is largely ignored. Our approach exploits contextual properties (both explicit and hidden) to probabilistically expand each message to provide a more accurate message representation. Extensive experimental evaluations show our approach outperforms the best previously known technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You talking to me? A Corpus and Algorithm for Conversation Disentanglement. In</title>
<date>2008</date>
<booktitle>ACL 2008: Proceedings of the 46th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>834--842</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH, USA.</location>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You talking to me? A Corpus and Algorithm for Conversation Disentanglement. In ACL 2008: Proceedings of the 46th Annual Meeting on Association for Computational Linguistics, pages 834-842, Columbus, OH, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Qiang Yang</author>
<author>Jian-Tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Thread Detection in Dynamic Text Message Streams.</title>
<date>2006</date>
<booktitle>In SIGIR 2006: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>35--42</pages>
<publisher>Association for Computing Machinery.</publisher>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="5482" citStr="Shen et al., 2006" startWordPosition="856" endWordPosition="859">’s messages to Josephina (19034 Ricardo and 19045 Ricardo) are responses to earlier comments made by Josephina (18980 Josephina), and that fact is signaled by Ricardo invoking Josephena’s name. This is an example of social correlation: lexicalized references to identity can also provide useful evidence. If we take social and temporal context into account, we should be able to do better at recognizing conversations than we could using lexical overlap alone. In recent years, several approaches have been developed for detecting conversational threads in dynamic text streams (Elsner et al., 2008; Shen et al., 2006; Wang et al., 2008). Although they use both lexical and non-lexical information (e.g., time, name mentions in message) for this task, they have ignored the temporal and social contexts a message appears in, which provide valuable cues for interpreting the message. Correlation clustering used in a two-step approach (Elsner et al., 2008) exploits message contexts to some degree, but its performance is largely limited by the classifier used in the first-step which computes message similarity without considering the temporal and social contexts of each message. Our approach exploits contextual pr</context>
<context position="7091" citStr="Shen et al., 2006" startWordPosition="1104" endWordPosition="1107"> we obtain with this technique are close to the limit of what we can measure using present test collections and evaluation measures. To the best of our knowledge, our work is the first to apply document expansion to the conversation disentanglement problem. 2 Related Work Previous work in conversation disentanglement (i.e. thread detection) has shown the conventional lexical-based clustering is not suitable for text streams because messages are often too short and incomplete. They focus on using discourse/chatspecific features to bias the lexical-based message similarity (Elsner et al., 2008; Shen et al., 2006; Wang et al., 2008). These features provide the means to link messages that may not have sufficient lexical overlap but are nevertheless likely to be topically related. However, our work is different from them in several aspects: (1) They treat individual messages as the basic elements for clustering, and ignore the social and temporal contexts of the messages. In our work, each message is probabilistically expanded using reliable information from its contexts and the expanded messages are the basic elements for clustering. (2) Messages have different amount of explicit information. For examp</context>
<context position="23554" citStr="Shen et al., 2006" startWordPosition="3898" endWordPosition="3901">nt set to approximate the normal densities used in our context models and the evaluation set to obtain the results reported below. Some statistics for the 800 annotated messages in the chat transcript of the evaluation collection are shown in Table 1. As that table shows, the average number of active conversation at a given time is 2.75, which makes thread detection a non-trivial task. 4.1 Evaluation Measures We conduct comparisons using three commonly used evaluation measures for the thread detection task. As a measure of the systems ability to group related messages we report the F-measure (Shen et al., 2006): maxj(F(i, j)) Esim(mi, mj) = w EF = i ni n 205 where i is a ground-truth conversation with length ni, and n is the length of entire transcript. F(i, j) is the harmonic mean of recall (fraction of the messages in the i also present in j) and precision (fraction of messages in j also present in i), and F is a weighted sum over all ground-truth conversations (i.e., F is microaveraged). Two other evaluation measures are “one-to-one accuracy” and “local agreement” (Elsner et al., 2008). “One-to-one accuracy” measures how well we extract whole conversations intact (e.g., as might be required for s</context>
</contexts>
<marker>Shen, Yang, Sun, Chen, 2006</marker>
<rawString>Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen. 2006. Thread Detection in Dynamic Text Message Streams. In SIGIR 2006: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35-42, Seattle, WA, USA. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Chia Wang</author>
<author>Mahesh Joshi</author>
<author>William Cohen</author>
<author>Carolyn Rose</author>
</authors>
<title>Recovering Implicit Thread Structure in Newsgroup Style Conversations.</title>
<date>2008</date>
<booktitle>In ICWSM 2008: Proceedings of the 2nd International Conference on Weblogs and Social Media,</booktitle>
<pages>152--160</pages>
<publisher>Association</publisher>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="5502" citStr="Wang et al., 2008" startWordPosition="860" endWordPosition="863">phina (19034 Ricardo and 19045 Ricardo) are responses to earlier comments made by Josephina (18980 Josephina), and that fact is signaled by Ricardo invoking Josephena’s name. This is an example of social correlation: lexicalized references to identity can also provide useful evidence. If we take social and temporal context into account, we should be able to do better at recognizing conversations than we could using lexical overlap alone. In recent years, several approaches have been developed for detecting conversational threads in dynamic text streams (Elsner et al., 2008; Shen et al., 2006; Wang et al., 2008). Although they use both lexical and non-lexical information (e.g., time, name mentions in message) for this task, they have ignored the temporal and social contexts a message appears in, which provide valuable cues for interpreting the message. Correlation clustering used in a two-step approach (Elsner et al., 2008) exploits message contexts to some degree, but its performance is largely limited by the classifier used in the first-step which computes message similarity without considering the temporal and social contexts of each message. Our approach exploits contextual properties (both expli</context>
<context position="7111" citStr="Wang et al., 2008" startWordPosition="1108" endWordPosition="1111">s technique are close to the limit of what we can measure using present test collections and evaluation measures. To the best of our knowledge, our work is the first to apply document expansion to the conversation disentanglement problem. 2 Related Work Previous work in conversation disentanglement (i.e. thread detection) has shown the conventional lexical-based clustering is not suitable for text streams because messages are often too short and incomplete. They focus on using discourse/chatspecific features to bias the lexical-based message similarity (Elsner et al., 2008; Shen et al., 2006; Wang et al., 2008). These features provide the means to link messages that may not have sufficient lexical overlap but are nevertheless likely to be topically related. However, our work is different from them in several aspects: (1) They treat individual messages as the basic elements for clustering, and ignore the social and temporal contexts of the messages. In our work, each message is probabilistically expanded using reliable information from its contexts and the expanded messages are the basic elements for clustering. (2) Messages have different amount of explicit information. For example, messages that in</context>
</contexts>
<marker>Wang, Joshi, Cohen, Rose, 2008</marker>
<rawString>Yi-Chia Wang, Mahesh Joshi, William Cohen, and Carolyn Rose. 2008. Recovering Implicit Thread Structure in Newsgroup Style Conversations. In ICWSM 2008: Proceedings of the 2nd International Conference on Weblogs and Social Media, pages 152-160, Seattle, WA, USA. Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>Xuanhui Wang</author>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Language Model Information Retrieval with Document Expansion.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 2006: Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>407--414</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York, NY, USA.</location>
<contexts>
<context position="8905" citStr="Tao et al., 2006" startWordPosition="1392" endWordPosition="1395">an exponential decay model to naturally encode time effect and assign differential weights to messages in its contexts. Another thread of related work is document expansion. It was previously studied in (Singhal et al., 1999) in the context of the speech retrieval, helping to overcome limitations in the transcription accuracy by selecting additional terms from lexically similar (text) documents. Document expansion has also been applied to cross-language retrieval in (Levow et al., 2005), in that case to overcome limitations in translation resources. The technique has recently been re-visited (Tao et al., 2006; Kurland et al., 2004; Liu et al., 2004) in the language modeling framework, where lexically related documents are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for expansion. Our technique can be viewed as an extension of these previous methods to text streams. Our work is also related to </context>
</contexts>
<marker>Tao, Wang, Mei, Zhai, 2006</marker>
<rawString>Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 2006. Language Model Information Retrieval with Document Expansion. In HLT-NAACL 2006: Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407-414, New York, NY, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Kurland</author>
<author>Lillian Lee</author>
</authors>
<title>Corpus Structure, Language Models, and AdHoc Information Retrieval.</title>
<date>2004</date>
<marker>Kurland, Lee, 2004</marker>
<rawString>Oren Kurland and Lillian Lee. 2004. Corpus Structure, Language Models, and AdHoc Information Retrieval.</rawString>
</citation>
<citation valid="false">
<booktitle>In SIGIR 2004: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>194--201</pages>
<publisher>Association for Computing Machinery.</publisher>
<location>Sheffield, UK.</location>
<marker></marker>
<rawString>In SIGIR 2004: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194-201, Sheffield, UK. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyong Liu</author>
<author>W Croft</author>
</authors>
<title>Cluster-based Retrieval Using Language Models.</title>
<date>2004</date>
<booktitle>In SIGIR 2004: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>186--193</pages>
<publisher>Association for Computing Machinery.</publisher>
<location>Sheffield, UK.</location>
<marker>Liu, Croft, 2004</marker>
<rawString>Xiaoyong Liu and W Croft. 2004. Cluster-based Retrieval Using Language Models. In SIGIR 2004: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186-193, Sheffield, UK. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Fernando Pereira</author>
</authors>
<title>Document Expansion for Speech Retrieval. In</title>
<date>1999</date>
<booktitle>SIGIR 1999: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>34--41</pages>
<publisher>Association for Computing Machinery.</publisher>
<location>Berkeley, CA, USA.</location>
<marker>Singhal, Pereira, 1999</marker>
<rawString>Amit Singhal and Fernando Pereira. 1999. Document Expansion for Speech Retrieval. In SIGIR 1999: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 34-41, Berkeley, CA, USA. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Ji</author>
<author>Hongyuan Zha</author>
</authors>
<title>Domain-Independent Text Segmentation using Anisotropic Diffusion and Dynamic Programming.</title>
<date>2003</date>
<booktitle>In SIGIR 2003: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>322--329</pages>
<publisher>Association for Computing Machinery.</publisher>
<location>Toronto, Canada.</location>
<marker>Ji, Zha, 2003</marker>
<rawString>Xiang Ji and Hongyuan Zha 2003. Domain-Independent Text Segmentation using Anisotropic Diffusion and Dynamic Programming. In SIGIR 2003: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 322-329, Toronto, Canada. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric Lussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse Segmentation of Multi-Party Conversation.</title>
<date>2003</date>
<booktitle>In ACL 2003: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9634" citStr="Galley et al., 2003" startWordPosition="1508" endWordPosition="1511">s are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for expansion. Our technique can be viewed as an extension of these previous methods to text streams. Our work is also related to text segmentation (Ji et al., 2003) and meeting segmentation (Malioutov et al., 2006; Malioutov et al., 2007; Galley et al., 2003; Eisenstein et al., 2008). Text segmentation identifies boundaries of topic changes in long text documents, but we form threads of messages from streams consisting of short messages. Meeting conversations are not as highly interleaving as chat conversations, where participants can create a new conversation at any time. 3 Method This section describes our technique for clustering messages into threads based on the lexical similarity of documents that have been expanded based on social and temporal evidence. 3.1 Context-Free Message Model To represent the semantic information of messages and th</context>
</contexts>
<marker>Galley, McKeown, Lussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric Lussier, and Hongyan Jing. 2003. Discourse Segmentation of Multi-Party Conversation. In ACL 2003: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 562-569, Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian Unsupervised Topic Segmentation. In</title>
<date>2008</date>
<booktitle>EMNLP 2008: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>334--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii, USA.</location>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian Unsupervised Topic Segmentation. In EMNLP 2008: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 334-343, Honolulu, Hawaii, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>MinimumCut Model for Spoken Lecture Segmentation.</title>
<date>2006</date>
<booktitle>In ACL 2006: Proceedings of the 44rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia.</location>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay 2006. MinimumCut Model for Spoken Lecture Segmentation. In ACL 2006: Proceedings of the 44rd Annual Meeting of the Association for Computational Linguistics, pages 25-32, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Alex Park</author>
<author>Regina Barzilay</author>
<author>James Glass</author>
</authors>
<title>Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input. In</title>
<date>2007</date>
<booktitle>ACL 2007: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>504--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9613" citStr="Malioutov et al., 2007" startWordPosition="1504" endWordPosition="1507">xically related documents are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for expansion. Our technique can be viewed as an extension of these previous methods to text streams. Our work is also related to text segmentation (Ji et al., 2003) and meeting segmentation (Malioutov et al., 2006; Malioutov et al., 2007; Galley et al., 2003; Eisenstein et al., 2008). Text segmentation identifies boundaries of topic changes in long text documents, but we form threads of messages from streams consisting of short messages. Meeting conversations are not as highly interleaving as chat conversations, where participants can create a new conversation at any time. 3 Method This section describes our technique for clustering messages into threads based on the lexical similarity of documents that have been expanded based on social and temporal evidence. 3.1 Context-Free Message Model To represent the semantic informati</context>
</contexts>
<marker>Malioutov, Park, Barzilay, Glass, 2007</marker>
<rawString>Igor Malioutov, Alex Park, Regina Barzilay, and James Glass. 2007. Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input. In ACL 2007: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504-511, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Yuan Yeh</author>
<author>Aaron Harnly</author>
</authors>
<title>Email Thread Reassembly Using Similarity Matching.</title>
<date>2006</date>
<booktitle>In CEAS 2006: The 3rd Conference on Email and Anti-Spam,</booktitle>
<pages>64--71</pages>
<location>Mountain View, CA, USA.</location>
<marker>Yeh, Harnly, 2006</marker>
<rawString>Jen-Yuan Yeh and Aaron Harnly. 2006. Email Thread Reassembly Using Similarity Matching. In CEAS 2006: The 3rd Conference on Email and Anti-Spam, pages 64-71, Mountain View, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacki ONeill</author>
<author>David Martin</author>
</authors>
<title>Text Chat in Action.</title>
<date>2003</date>
<booktitle>In ACM SIGGROUP 2003: Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work,</booktitle>
<pages>40--49</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<marker>ONeill, Martin, 2003</marker>
<rawString>Jacki ONeill and David Martin. 2003. Text Chat in Action. In ACM SIGGROUP 2003: Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work, pages 40-49, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing: the Transformation, Analysis and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley Longman Publishing Co., Inc.,</publisher>
<location>Boston, MA, USA,</location>
<contexts>
<context position="10549" citStr="Salton, 1989" startWordPosition="1652" endWordPosition="1653">sation at any time. 3 Method This section describes our technique for clustering messages into threads based on the lexical similarity of documents that have been expanded based on social and temporal evidence. 3.1 Context-Free Message Model To represent the semantic information of messages and threads (clusters of messages), most of the prior approaches build a document representation on each message alone (using word features and time-stamp and/or discourse features found in the message). We call such a model a context-free message model. Most commonly, a message is represented as a vector (Salton, 1989). Each dimension corresponds to a separate term. If a term occurs in the message, its value in the vector is non-zero. Several different ways of computing these values, known as term weights, have been developed. One of the best known schemes is tf-idf weighting. However, in conversational text, a context-free model cannot fully capture the semantics of messages. The meaning of a message is highly dependent on other messages in its context. For example, in our running example in Figure 1, to fully interpret the message 19045 Ricardo, we need to first read his previous message (19034 Ricardo) t</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. 1989. Automatic Text Processing: the Transformation, Analysis and Retrieval of Information by Computer. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Douglas Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Dictionary-based techniques for cross-language information retrieval.</title>
<date>2005</date>
<booktitle>In Information Processing and Management Special Issue: Cross-Language Information Retrieval,</booktitle>
<volume>41</volume>
<issue>3</issue>
<pages>523--547</pages>
<contexts>
<context position="8780" citStr="Levow et al., 2005" startWordPosition="1373" endWordPosition="1376">ng and it may break up long conversations or may not be fine-grained enough for short conversations. Given each message, we use an exponential decay model to naturally encode time effect and assign differential weights to messages in its contexts. Another thread of related work is document expansion. It was previously studied in (Singhal et al., 1999) in the context of the speech retrieval, helping to overcome limitations in the transcription accuracy by selecting additional terms from lexically similar (text) documents. Document expansion has also been applied to cross-language retrieval in (Levow et al., 2005), in that case to overcome limitations in translation resources. The technique has recently been re-visited (Tao et al., 2006; Kurland et al., 2004; Liu et al., 2004) in the language modeling framework, where lexically related documents are used to enlarge the sample space for a document to improve the accuracy of the estimated document language model. However, these lexical-based approaches are less well suited to conversational interaction, because conversational messages are often short, they therefore may not overlap sufficiently in words with other messages to provide a useful basis for e</context>
</contexts>
<marker>Levow, Oard, Resnik, 2005</marker>
<rawString>Gina-Anne Levow, Douglas Oard, and Philip Resnik. 2005. Dictionary-based techniques for cross-language information retrieval. In Information Processing and Management Special Issue: Cross-Language Information Retrieval, 41(3): 523-547.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>