<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.106194">
<title confidence="0.9979605">
The DCU Discourse Parser for Connective, Argument Identification and
Explicit Sense Classification
</title>
<author confidence="0.992645">
Longyue Wang, Chris Hokamp, Tsuyoshi Okita, Xiaojun Zhang, Qun Liu
</author>
<affiliation confidence="0.988384">
ADAPT Centre
School of Computing, Dublin City University
</affiliation>
<address confidence="0.550006">
Glasnevin, Dublin 9, Ireland
</address>
<email confidence="0.984931">
{lwang, chokamp, tokita, xzhang, qliu}@computing.dcu.ie
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999762857142857">
This paper describes our submission to
the CoNLL-2015 shared task on discourse
parsing. We factor the pipeline into sub-
components which are then used to form
the final sequential architecture. Focusing
on achieving good performance when in-
ferring explicit discourse relations, we ap-
ply maximum entropy and recurrent neu-
ral networks to different sub-tasks such
as connective identification, argument ex-
traction, and sense classification. The our
final system achieves 16.51%, 12.73% and
11.15% overall F1 scores on the dev, WSJ
and blind test sets, respectively.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937">
The task of discourse parsing is generally con-
ceived as a pipeline of steps, corresponding to: i)
locating explicit discourse connectives, ii) identi-
fying the spans of text that serve as the two argu-
ments for each discourse connective, and iii) pre-
dicting the sense for both explicit and implicit re-
lations. Understanding such discourse information
is clearly an important component of natural lan-
guage understanding that impacts a wide range of
downstream natural language applications.
Since Penn Discourse Treebank was released,
a number of data driven approaches have been
proposed to deal with different challenging sub-
tasks of discourse parsing. As explicit arguments
may be intra-sentential or inter-sentential, Lin et
al. (2012), Xu et al. (2012), Stepanov and Ric-
cardi (2012) propose to employ argument posi-
tion classification as heuristic and then apply sepa-
rated models for argument extraction. Ghosh et al.
(2011) regarded argument extraction as a token-
level sequence labeling task, applying conditional
random fields (CRFs) to label each token in a sen-
tence. Following on this work, Ghosh et al. (2012)
designed many global features to help distinguish
Argument1 and Argument2 within the same sen-
tence. Lin et al. (2014) formulated the task as
finding the nodes in the constituent parse that are
Argument1 or Argument2. However, the perfor-
mance of this approach is heavily dependent upon
the quality of the input parse trees. The different
characteristic of implicit and explicit discourse re-
lations are another important consideration. Lin et
al. (2009) apply three feature classes: the con-
stituent parse, the dependency parse and word-
pair features for implicit relation classification.
Rutherford and Xue (2014) exploit Brown cluster
pairs to represent discourse relations in naturally
occurring text. Considering the whole task, Lin
et al. (2014) introduce a pipeline framework in-
cluding several sub-tasks (connective classifier, ar-
gument labeler, explicit classifier and non-explicit
classifier) to handle both explicit and non-explicit
relations based on the PDTB corpus using maxi-
mum entropy.
In our work, we design the framework of our
system based on Lin et al. (2014). The task is
split the into seven components: connective clas-
sifier, argument positions classifier, three argu-
ment extractors, explicit sense classifier and im-
plicit sense classifier. We approach argument ex-
traction as a sequence labelling task, employing
recurrent neural network (RNN) to classify each
candidate token. We use distributional representa-
tions via word embeddings to decrease the out-of-
vocabulary words (OOVs) problem which result
from the scarcity of training data. After a post-
precessing step which resolves label conflicts, we
extract the spans of arguments. For other com-
ponents, we use a classification via maximum en-
tropy, and explore diverse features. In this sys-
tem, we mainly focus on explicit relations, thus
we only apply a simple majority function for the
non-explicit component.
The remainder of this paper is organized as fol-
</bodyText>
<page confidence="0.997953">
89
</page>
<note confidence="0.8332025">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 89–94,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9980422">
lows: Section 2 describes the framework and each
component of our proposed system. Then we dis-
cuss the results, including the official results and
post-task results, in Section 3. Finally, we sum-
marize our conclusions in Section 4.
</bodyText>
<sectionHeader confidence="0.886919" genericHeader="method">
2 Proposed System
</sectionHeader>
<bodyText confidence="0.999979136363637">
The framework of our system is shown in Figure 1.
In the first step, the connective classifier is used to
identify connectives according to the occurrences
of the predefined connectives. Once a candidate
is labelled as a connective, an explicit relation is
created. The next step is then to find the argument
positions (arg1 and arg2) for each explicit rela-
tion. Here we use a classifier to label two cases:
1, arg1 and arg2 are in the same sentence (SS),
or 2, arg1 and arg2 are not in the same sentence
(OT). Then we train and apply different argument
extraction models for these two cases. After la-
belling the argument span, we use a sense classi-
fication component to classify them to predefined
sense types.
After processing the explicit relations, the non-
explicit part extracts all the adjacent sentence pairs
which are not explicit relations and then infers im-
plicit relations. As we mainly focus on explicit
relations, in this part, we only apply a simple ma-
jority function to give all candidate pairs the same
results.
</bodyText>
<subsectionHeader confidence="0.994327">
2.1 Connective Classifier
</subsectionHeader>
<bodyText confidence="0.999979580645161">
As words which can be discourse connectives do
not always function as discourse connectives, we
need to identify if an instance of a connective
candidate is a functional connective each time it
occurs. Pilter and Nenkova (2009) showed that
syntactic features extracted from constituent parse
trees are very useful in disambiguating discourse
connectives from other functions. Lin et al. (2014)
tackled this problem by first using the connec-
tive list to identify the candidates and then us-
ing a combination of simple POS-based features
and tree-based features, an approach which also
achieved good performance. To model the syntac-
tic relation, they also propose a path feature, which
is the combined tags of sub-tree nodes from con-
nective to the root. Compressed path means the
adjacent identical tags are combined (e.g., -NP-
NP- is combined into -NP-).
Based on above work, we extract the 99 types of
connectives defined in the PDTB training corpus.
As shown in Table 1, we use three feature classes:
lexical, syntactic and others. Especially, we em-
ploy the position of connection as a new feature
(i.e., beginning or not), because we observe that
the candidates occurring at the beginning are al-
ways the connectives. Then a ME model is applied
to classify each connective candidate as a connec-
tive or not. After exploring 14 features and com-
binations, we finally found that the feature set {2-
10,13, 14} which yields the best performance on
dev set. The final score is shown in Section 3.
</bodyText>
<subsectionHeader confidence="0.999428">
2.2 Argument Position Classification
</subsectionHeader>
<bodyText confidence="0.999918214285714">
arg2 is the argument with which the connective is
syntactically associated, and its position is fixed
once we have located the connective from the pre-
vious component (Section 2.1). Thus, the chal-
lenging step for this task is to identify the location
of arg1.
Prasad et al. (2008) show that arg1 may be lo-
cated in various positions to the connective, such
as within the same sentence (SS), before (PS),
or after (FS) the sentence containing the connec-
tive. Furthermore, arg1 may be adjacent or non-
adjacent with connective sentence. arg1 may also
contain one or more sentences. Table 2 shows the
statistics of each of above scenarios.
</bodyText>
<table confidence="0.9998542">
Relative Position 1 Sent n Sents
SS 60.38% -
FS 0.01% 0.03%
PS 27.93% 1.89%
Other Scenarios 9.79%
</table>
<tableCaption confidence="0.98999">
Table 2: Statistics of arg1’s Positions. (Percent-
</tableCaption>
<bodyText confidence="0.967514454545455">
age (%) is computed as the number of the scenario
divided by the total relations; n&gt;1)
As SS and PS constitute 90.20% of all explicit
relations, our system mainly focus on these two
cases. Therefore, we use a argument position clas-
sifier to classify a relation as SS or PS. In our ex-
periment, we compared 17 features and their com-
binations, which are shown in Table 3. Finally, we
use the feature set {1-3, 5, 7, 9, 11-14, 17} since
it achieves the highest accuracy (97.78%) on dev
set.
</bodyText>
<subsectionHeader confidence="0.998562">
2.3 Argument Extraction
</subsectionHeader>
<bodyText confidence="0.999886">
One of the key problems in discourse parsing is
the task of extraction of argument spans of dis-
course relation. In the light of the recent success
</bodyText>
<page confidence="0.993119">
90
</page>
<figureCaption confidence="0.998658">
Figure 1: Framework of Our System
</figureCaption>
<table confidence="0.989014323529412">
Type ID Features
1 Connective Word
2 Connective POS
3 1st Previous Word of Connective
4 1st Next Word of Connective
Lexical Features 5 1st Previous Word + Connective Word
6 Connective Word + 1st Next Word
7 1st Previous POS + Connective POS
8 Connective POS + 1st Next POS
9 1st Previous Word + Connective Word + 1st Next Word
10 1st Previous POS + Connective POS + 1st Next POS
11 Path of Connective to the Root
Syntactic Features 12 Path of Connective’s Parent to the Root
13 Compressed Path of Connective’s Parent to the Root
Others 14 Low-Cased Connective Word
Table 1: Features for Connective Classification
Type ID Features
1 Connective Word
2 Connective POS
3 1st Previous Word of Connective
4 1st Next Word of Connective
5 1st Previous POS of Connective
6 1st Next POS of Connective
7 1st Previous Word + Connective Word
Lexical Features 8 Connective Word + 1st Next Word
9 1st Previous POS + Connective POS
10 Connective POS + 1st Next POS
11 2nd Previous POS of Connective
12 2nd Previous Word of Connective
13 2nd Previous POS + Connective POS
14 2nd Previous Word + Connective Word
15 1st Previous Word + Connective Word + 1st Next Word
16 1st Previous POS + Connective POS + 1st Next POS
Others 17 Position of Connective
</table>
<tableCaption confidence="0.961399">
Table 3: Features for Argument Position Classification
</tableCaption>
<page confidence="0.998857">
91
</page>
<bodyText confidence="0.999914609756098">
of applying deep neural network technologies in
natural language processing, we carried out an in-
vestigation of the use of recurrent neural network
(RNN) for this difficult task (Mesnil et al., 2013;
Raymond and Riccardi, 2007).
After determining the likely position of arg1,
we split the explicit relations into two sets: SS
and OT. We apply token-level sequence labeling
approach with the separate models for arguments
of intra-sentential and inter-sentential explicit dis-
course relations (Ghosh et al. 2011; Stepanov and
Riccardi, 2012). As shown in Figure 1, we apply
two components to deal with these two cases. Be-
sides, in OT, we also train separated models to deal
with Arg1 and Arg2 extraction.
Since for sequence labeling we use IOBE (In-
side, Out, Begin, End) notation as the labels for
both Arg1 and Arg2. For example, the set of
classes for the SS case is {arg1-B, arg1-I, arg1-
E, arg2-B, arg2-I, arg2-E and None}. The sets
of classes for OT are {arg1-B, arg1-I, arg1-E and
None} and {arg2-B, arg2-I, arg2-E and None}.
As input features, we use the word embeddings
for Arg1 and Arg2 in order to infer the argument
labels. We use RNNs to learn a word embedding
on the part of training data. As the official scorer
will give points only when the whole argument
span is right, we employ this scorer to calculate the
performance in each iteration of training. Further-
more, we compare the performance with different
parameters: number of context windows, hidden
layers, iterations and word embeddings. Finally,
we set number of context windows as 5, hidden
layers as 300, iterations as 10 and word embed-
dings as 100 to achieve the highest performance.
Besides, we only extract the relations in the
corresponding scenario as the training data, thus
OOVs may harm the models. We use distri-
butional representations via word embeddings to
alleviate the problem, which results from the
scarcity of training data.
</bodyText>
<subsectionHeader confidence="0.989863">
2.4 Explicit Sense Classification
</subsectionHeader>
<bodyText confidence="0.997067692307692">
One method that has previously been employed
to resolve the ambiguity in discourse connectives
is to build a classifier with some very simple
features. They are the connective (one or more
words), the connectives POS, and the connective +
its previous word (Lin et al., 2014). This approach
achieves an F1 score of 86.77, which is quite im-
pressive compared the human agreement score of
84%.
Therefore, for this component, we still employ
the similar feature set, which is shown in Table
4. Finally, we apply the feature set {1-3, 5-6} to
obtain the best scores on dev set.
</bodyText>
<subsectionHeader confidence="0.971079">
2.5 Non-Explicit Relations
</subsectionHeader>
<bodyText confidence="0.999758111111111">
The non-explicit relation includes Implicit , Al-
tLex, EntRel and NoRel relations.
The non-explicit relations are annotated for all
adjacent sentence pairs within paragraphs. If there
is already an explicit relation from the previous
step between two adjacent sentences, they are ex-
empt from this step. In our system, we just apply
a majority classifier, labeling all non-explicit rela-
tion candidates as EntRel.
</bodyText>
<sectionHeader confidence="0.995375" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.997421">
3.1 System Setup
</subsectionHeader>
<bodyText confidence="0.9999565">
All available training data, development set, test
sets from CoNLL 2015 (LDC2015E21)1 are used
in this study. Besides, we use the Skip-gram Neu-
ral Word Embeddings2 for RNNs. All the used
syntactic information are automatically predicted
by the Berkeley Parser3.
We use Maxent toolkit4 for the ME method.
And we apply Theano5 (Bastien et al., 2012;
Bergstra et al., 2010) for the RNNs. We use the
Python programming language to develop all the
compontents and divided each component into two
parts: one is training which is processed in our
CPU and GPU servers and the other is decoding
which is run on TIRA server6.
</bodyText>
<subsectionHeader confidence="0.999308">
3.2 Official Results
</subsectionHeader>
<bodyText confidence="0.999959727272727">
The official results are shown in Table 5. The per-
formance of connective classifier is around 80%,
which is not good enough. There are two reasons:
1, we skip some separated connectives such as ei-
ther or, neither nor etc. and 2, the current fea-
ture set missed some syntactic information. For
argument extraction, the reasonable scores show
our proposed method can really work for this part.
However, it does not work well for OT case, be-
cause the span is always located the whole sen-
tence. It may be helpful by adding structure fea-
</bodyText>
<footnote confidence="0.999991666666667">
1Available at https://www.ldc.upenn.edu
2Available at https://code.google.com/p/word2vec
3Description at http://www.cs.brandeis.edu/ clp/conll15st/rules.html
4Available at https://github.com/lzhang10/maxent
5Available at http://deeplearning.net/tutorial/rnnslu.html
6Available at http://www.tira.io
</footnote>
<page confidence="0.977532">
92
</page>
<table confidence="0.999486142857143">
Type of Feature ID Features
1 Connective Word
2 Connective POS
Lexical Features 3 Connective + 1st Previous Word
4 Connective + 2st Previous Word
5 Connective + 1st Previous POS
Others 6 Low-Cased Connective
</table>
<tableCaption confidence="0.969543">
Table 4: Features for Explicit Sense Classification.
</tableCaption>
<bodyText confidence="0.999520888888889">
tures into RNNs. The sense classifier is the worst
component, which only obtained about 8% F1
scores. It is because 1, the errors from previous
components are propagated, which is also the lim-
itation of the pipeline architecture; 2, we apply
a simple non-explicit component and miss a lot
implicit relations, which result in the low recall.
On the whole, our system can still be improved in
many ways.
</bodyText>
<sectionHeader confidence="0.998208" genericHeader="conclusions">
4 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.9999881">
This paper describes the discourse parsing sys-
tem we implemented for the CoNLL-2015 shared
task. We build a pipeline system which focuses
on achieving good performance when inferring ex-
plicit discourse relations. We apply maximum en-
tropy and recurrent neural networks to different
sub-tasks.
This is our ongoing work, and we will keep on
improving the system by employing novel neural
network methods.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999783">
This work is supported by the Science Founda-
tion of Ireland (SFI) CNGL project (Grant No.:
12/CE/I2267), and partly supported by the DCU-
Huawei Joint Project (Grant No.:201504032) and
the Open Projects Program of National Laboratory
of Patter Recognition (Grant No.: 201407353),
and also by the European Commission FP7 EX-
PERT project.
</bodyText>
<sectionHeader confidence="0.998432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997921905660377">
Ziheng Lin, Hwee Tou Ng and Min-Yen Kan. 2014.
A PDTB-styled End-to-End Discourse Parser, vol-
ume 1-34. Natural Language Engineering.
Sucheta Ghosh, Richard Johansson and Sara Tonelli.
2011. Shallow Discourse Parsing with Conditional
Random Fields. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Sucheta Ghosh, Giuseppe Riccardi and Richard Jo-
hansson. 2012. Global Features for Shallow Dis-
course Parsing, 150-159. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on
Discourse and Dialogue. Association for Computa-
tional Linguistics
Emily Pitler and Ani Nenkova. 2009. Global Fea-
tures for Shallow Discourse Parsing, 150-159. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers. Association for Computational Lin-
guistics.
Attapol T. Rutherford and Nianwen Xue. 2014.
Discovering Implicit Discourse Relations Through
Brown Cluster Pair Representation and Coreference
Patterns, 645. In Proceedings of EACL 2014.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, Bonnie Web-
ber. 2008. The Penn Discourse TreeBank 2.0. In
Proceedings of the LREC.
Grgoire Mesnil, Xiaodong He, Li Deng and Yoshua
Bengio. 2013. Investigation of Recurrent-Neural-
Network Architectures and Learning Methods for
Spoken Language Understanding. In Proceedings
of the Interspeech 2013.
Christian Raymond and Giuseppe Riccardi. 2007.
Generative and discriminative algorithms for spo-
ken language understanding. In Proceedings of the
Interspeech 2007.
Frdric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, David Warde-Farley, Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. In Proceedings of the Python for Scien-
tific Computing Conference (SciPy).
James Bergstra, Olivier Breuleux, Frdric Bastien, Pas-
cal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, Yoshua
Bengio. 2010. Theano: a CPU and GPU math ex-
pression compiler. In Proceedings of the Python for
Scientific Computing Conference (SciPy).
Evgeny A. Stepanov and Giuseppe Riccardi. 2013.
Comparative evaluation of argument extraction al-
gorithms in discourse relation parsing. In Proceed-
ings of 13th International Conference on Parsing
Technologies (IWPT 2013).
</reference>
<page confidence="0.998754">
93
</page>
<table confidence="0.99972525">
Dev Set Test Set Blind Set
Components Precision Recall F1 Precision Recall F1 Precision Recall F1
Connectives 0.9010 0.8162 0.8565 0.9040 0.8570 0.8799 0.8487 0.7464 0.7943
Arg1 0.3437 0.4770 0.3995 0.3100 0.4384 0.3632 0.2794 0.3755 0.3204
Arg2 0.3778 0.5244 0.4392 0.3559 0.5034 0.4170 0.3489 0.4690 0.4001
Arg1 &amp; Arg2 0.2559 0.3552 0.2975 0.2174 0.3074 0.2546 0.1926 0.2589 0.2209
Sense 0.3194 0.1080 0.0938 0.2257 0.1124 0.0849 0.0905 0.0701 0.0481
Overall 0.1420 0.1971 0.1651 0.1087 0.1537 0.1273 0.0972 0.1307 0.1115
</table>
<tableCaption confidence="0.984129">
Table 5: Official Results.
</tableCaption>
<reference confidence="0.9642764">
Xu Ming, Zhu Qiao and Zhou Guo Dong. 2012. A
Unified Framework for Discourse Argument Identi-
fication via Shallow Semantic Parsing. In Proceed-
ings of 24th International Conference on Computa-
tional Linguistics.
</reference>
<page confidence="0.99955">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738370">
<title confidence="0.9986465">The DCU Discourse Parser for Connective, Argument Identification and Explicit Sense Classification</title>
<author confidence="0.993009">Longyue Wang</author>
<author confidence="0.993009">Chris Hokamp</author>
<author confidence="0.993009">Tsuyoshi Okita</author>
<author confidence="0.993009">Xiaojun Zhang</author>
<author confidence="0.993009">Qun</author>
<affiliation confidence="0.9513245">ADAPT School of Computing, Dublin City</affiliation>
<address confidence="0.967738">Glasnevin, Dublin 9,</address>
<email confidence="0.88733">chokamp,tokita,xzhang,</email>
<abstract confidence="0.994647866666667">This paper describes our submission to the CoNLL-2015 shared task on discourse parsing. We factor the pipeline into subcomponents which are then used to form the final sequential architecture. Focusing on achieving good performance when inferring explicit discourse relations, we apply maximum entropy and recurrent neural networks to different sub-tasks such as connective identification, argument extraction, and sense classification. The our final system achieves 16.51%, 12.73% and 11.15% overall F1 scores on the dev, WSJ and blind test sets, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled End-to-End Discourse Parser, volume 1-34. Natural Language Engineering.</title>
<date>2014</date>
<contexts>
<context position="2127" citStr="Lin et al. (2014)" startWordPosition="317" endWordPosition="320">ging subtasks of discourse parsing. As explicit arguments may be intra-sentential or inter-sentential, Lin et al. (2012), Xu et al. (2012), Stepanov and Riccardi (2012) propose to employ argument position classification as heuristic and then apply separated models for argument extraction. Ghosh et al. (2011) regarded argument extraction as a tokenlevel sequence labeling task, applying conditional random fields (CRFs) to label each token in a sentence. Following on this work, Ghosh et al. (2012) designed many global features to help distinguish Argument1 and Argument2 within the same sentence. Lin et al. (2014) formulated the task as finding the nodes in the constituent parse that are Argument1 or Argument2. However, the performance of this approach is heavily dependent upon the quality of the input parse trees. The different characteristic of implicit and explicit discourse relations are another important consideration. Lin et al. (2009) apply three feature classes: the constituent parse, the dependency parse and wordpair features for implicit relation classification. Rutherford and Xue (2014) exploit Brown cluster pairs to represent discourse relations in naturally occurring text. Considering the </context>
<context position="5831" citStr="Lin et al. (2014)" startWordPosition="900" endWordPosition="903">explicit relations and then infers implicit relations. As we mainly focus on explicit relations, in this part, we only apply a simple majority function to give all candidate pairs the same results. 2.1 Connective Classifier As words which can be discourse connectives do not always function as discourse connectives, we need to identify if an instance of a connective candidate is a functional connective each time it occurs. Pilter and Nenkova (2009) showed that syntactic features extracted from constituent parse trees are very useful in disambiguating discourse connectives from other functions. Lin et al. (2014) tackled this problem by first using the connective list to identify the candidates and then using a combination of simple POS-based features and tree-based features, an approach which also achieved good performance. To model the syntactic relation, they also propose a path feature, which is the combined tags of sub-tree nodes from connective to the root. Compressed path means the adjacent identical tags are combined (e.g., -NPNP- is combined into -NP-). Based on above work, we extract the 99 types of connectives defined in the PDTB training corpus. As shown in Table 1, we use three feature cl</context>
<context position="11956" citStr="Lin et al., 2014" startWordPosition="1955" endWordPosition="1958">beddings as 100 to achieve the highest performance. Besides, we only extract the relations in the corresponding scenario as the training data, thus OOVs may harm the models. We use distributional representations via word embeddings to alleviate the problem, which results from the scarcity of training data. 2.4 Explicit Sense Classification One method that has previously been employed to resolve the ambiguity in discourse connectives is to build a classifier with some very simple features. They are the connective (one or more words), the connectives POS, and the connective + its previous word (Lin et al., 2014). This approach achieves an F1 score of 86.77, which is quite impressive compared the human agreement score of 84%. Therefore, for this component, we still employ the similar feature set, which is shown in Table 4. Finally, we apply the feature set {1-3, 5-6} to obtain the best scores on dev set. 2.5 Non-Explicit Relations The non-explicit relation includes Implicit , AltLex, EntRel and NoRel relations. The non-explicit relations are annotated for all adjacent sentence pairs within paragraphs. If there is already an explicit relation from the previous step between two adjacent sentences, they </context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng and Min-Yen Kan. 2014. A PDTB-styled End-to-End Discourse Parser, volume 1-34. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow Discourse Parsing with Conditional Random Fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="1819" citStr="Ghosh et al. (2011)" startWordPosition="267" endWordPosition="270">. Understanding such discourse information is clearly an important component of natural language understanding that impacts a wide range of downstream natural language applications. Since Penn Discourse Treebank was released, a number of data driven approaches have been proposed to deal with different challenging subtasks of discourse parsing. As explicit arguments may be intra-sentential or inter-sentential, Lin et al. (2012), Xu et al. (2012), Stepanov and Riccardi (2012) propose to employ argument position classification as heuristic and then apply separated models for argument extraction. Ghosh et al. (2011) regarded argument extraction as a tokenlevel sequence labeling task, applying conditional random fields (CRFs) to label each token in a sentence. Following on this work, Ghosh et al. (2012) designed many global features to help distinguish Argument1 and Argument2 within the same sentence. Lin et al. (2014) formulated the task as finding the nodes in the constituent parse that are Argument1 or Argument2. However, the performance of this approach is heavily dependent upon the quality of the input parse trees. The different characteristic of implicit and explicit discourse relations are another </context>
<context position="10241" citStr="Ghosh et al. 2011" startWordPosition="1664" endWordPosition="1667">S + 1st Next POS Others 17 Position of Connective Table 3: Features for Argument Position Classification 91 of applying deep neural network technologies in natural language processing, we carried out an investigation of the use of recurrent neural network (RNN) for this difficult task (Mesnil et al., 2013; Raymond and Riccardi, 2007). After determining the likely position of arg1, we split the explicit relations into two sets: SS and OT. We apply token-level sequence labeling approach with the separate models for arguments of intra-sentential and inter-sentential explicit discourse relations (Ghosh et al. 2011; Stepanov and Riccardi, 2012). As shown in Figure 1, we apply two components to deal with these two cases. Besides, in OT, we also train separated models to deal with Arg1 and Arg2 extraction. Since for sequence labeling we use IOBE (Inside, Out, Begin, End) notation as the labels for both Arg1 and Arg2. For example, the set of classes for the SS case is {arg1-B, arg1-I, arg1- E, arg2-B, arg2-I, arg2-E and None}. The sets of classes for OT are {arg1-B, arg1-I, arg1-E and None} and {arg2-B, arg2-I, arg2-E and None}. As input features, we use the word embeddings for Arg1 and Arg2 in order to in</context>
</contexts>
<marker>Ghosh, Johansson, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson and Sara Tonelli. 2011. Shallow Discourse Parsing with Conditional Random Fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Giuseppe Riccardi</author>
<author>Richard Johansson</author>
</authors>
<title>Global Features for Shallow Discourse Parsing,</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</booktitle>
<contexts>
<context position="2009" citStr="Ghosh et al. (2012)" startWordPosition="298" endWordPosition="301">nn Discourse Treebank was released, a number of data driven approaches have been proposed to deal with different challenging subtasks of discourse parsing. As explicit arguments may be intra-sentential or inter-sentential, Lin et al. (2012), Xu et al. (2012), Stepanov and Riccardi (2012) propose to employ argument position classification as heuristic and then apply separated models for argument extraction. Ghosh et al. (2011) regarded argument extraction as a tokenlevel sequence labeling task, applying conditional random fields (CRFs) to label each token in a sentence. Following on this work, Ghosh et al. (2012) designed many global features to help distinguish Argument1 and Argument2 within the same sentence. Lin et al. (2014) formulated the task as finding the nodes in the constituent parse that are Argument1 or Argument2. However, the performance of this approach is heavily dependent upon the quality of the input parse trees. The different characteristic of implicit and explicit discourse relations are another important consideration. Lin et al. (2009) apply three feature classes: the constituent parse, the dependency parse and wordpair features for implicit relation classification. Rutherford and</context>
</contexts>
<marker>Ghosh, Riccardi, Johansson, 2012</marker>
<rawString>Sucheta Ghosh, Giuseppe Riccardi and Richard Johansson. 2012. Global Features for Shallow Discourse Parsing, 150-159. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Global Features for Shallow Discourse Parsing,</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Association for Computational Linguistics.</booktitle>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Global Features for Shallow Discourse Parsing, 150-159. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attapol T Rutherford</author>
<author>Nianwen Xue</author>
</authors>
<title>Discovering Implicit Discourse Relations Through Brown Cluster Pair Representation and Coreference Patterns, 645.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="2620" citStr="Rutherford and Xue (2014)" startWordPosition="392" endWordPosition="395"> et al. (2012) designed many global features to help distinguish Argument1 and Argument2 within the same sentence. Lin et al. (2014) formulated the task as finding the nodes in the constituent parse that are Argument1 or Argument2. However, the performance of this approach is heavily dependent upon the quality of the input parse trees. The different characteristic of implicit and explicit discourse relations are another important consideration. Lin et al. (2009) apply three feature classes: the constituent parse, the dependency parse and wordpair features for implicit relation classification. Rutherford and Xue (2014) exploit Brown cluster pairs to represent discourse relations in naturally occurring text. Considering the whole task, Lin et al. (2014) introduce a pipeline framework including several sub-tasks (connective classifier, argument labeler, explicit classifier and non-explicit classifier) to handle both explicit and non-explicit relations based on the PDTB corpus using maximum entropy. In our work, we design the framework of our system based on Lin et al. (2014). The task is split the into seven components: connective classifier, argument positions classifier, three argument extractors, explicit </context>
</contexts>
<marker>Rutherford, Xue, 2014</marker>
<rawString>Attapol T. Rutherford and Nianwen Xue. 2014. Discovering Implicit Discourse Relations Through Brown Cluster Pair Representation and Coreference Patterns, 645. In Proceedings of EACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC.</booktitle>
<contexts>
<context position="7244" citStr="Prasad et al. (2008)" startWordPosition="1140" endWordPosition="1143">ng are always the connectives. Then a ME model is applied to classify each connective candidate as a connective or not. After exploring 14 features and combinations, we finally found that the feature set {2- 10,13, 14} which yields the best performance on dev set. The final score is shown in Section 3. 2.2 Argument Position Classification arg2 is the argument with which the connective is syntactically associated, and its position is fixed once we have located the connective from the previous component (Section 2.1). Thus, the challenging step for this task is to identify the location of arg1. Prasad et al. (2008) show that arg1 may be located in various positions to the connective, such as within the same sentence (SS), before (PS), or after (FS) the sentence containing the connective. Furthermore, arg1 may be adjacent or nonadjacent with connective sentence. arg1 may also contain one or more sentences. Table 2 shows the statistics of each of above scenarios. Relative Position 1 Sent n Sents SS 60.38% - FS 0.01% 0.03% PS 27.93% 1.89% Other Scenarios 9.79% Table 2: Statistics of arg1’s Positions. (Percentage (%) is computed as the number of the scenario divided by the total relations; n&gt;1) As SS and PS</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grgoire Mesnil</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Yoshua Bengio</author>
</authors>
<title>Investigation of Recurrent-NeuralNetwork Architectures and Learning Methods for Spoken Language Understanding.</title>
<date>2013</date>
<booktitle>In Proceedings of the Interspeech</booktitle>
<contexts>
<context position="9930" citStr="Mesnil et al., 2013" startWordPosition="1618" endWordPosition="1621">ext Word 9 1st Previous POS + Connective POS 10 Connective POS + 1st Next POS 11 2nd Previous POS of Connective 12 2nd Previous Word of Connective 13 2nd Previous POS + Connective POS 14 2nd Previous Word + Connective Word 15 1st Previous Word + Connective Word + 1st Next Word 16 1st Previous POS + Connective POS + 1st Next POS Others 17 Position of Connective Table 3: Features for Argument Position Classification 91 of applying deep neural network technologies in natural language processing, we carried out an investigation of the use of recurrent neural network (RNN) for this difficult task (Mesnil et al., 2013; Raymond and Riccardi, 2007). After determining the likely position of arg1, we split the explicit relations into two sets: SS and OT. We apply token-level sequence labeling approach with the separate models for arguments of intra-sentential and inter-sentential explicit discourse relations (Ghosh et al. 2011; Stepanov and Riccardi, 2012). As shown in Figure 1, we apply two components to deal with these two cases. Besides, in OT, we also train separated models to deal with Arg1 and Arg2 extraction. Since for sequence labeling we use IOBE (Inside, Out, Begin, End) notation as the labels for bo</context>
</contexts>
<marker>Mesnil, He, Deng, Bengio, 2013</marker>
<rawString>Grgoire Mesnil, Xiaodong He, Li Deng and Yoshua Bengio. 2013. Investigation of Recurrent-NeuralNetwork Architectures and Learning Methods for Spoken Language Understanding. In Proceedings of the Interspeech 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Raymond</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Generative and discriminative algorithms for spoken language understanding.</title>
<date>2007</date>
<booktitle>In Proceedings of the Interspeech</booktitle>
<contexts>
<context position="9959" citStr="Raymond and Riccardi, 2007" startWordPosition="1622" endWordPosition="1625">us POS + Connective POS 10 Connective POS + 1st Next POS 11 2nd Previous POS of Connective 12 2nd Previous Word of Connective 13 2nd Previous POS + Connective POS 14 2nd Previous Word + Connective Word 15 1st Previous Word + Connective Word + 1st Next Word 16 1st Previous POS + Connective POS + 1st Next POS Others 17 Position of Connective Table 3: Features for Argument Position Classification 91 of applying deep neural network technologies in natural language processing, we carried out an investigation of the use of recurrent neural network (RNN) for this difficult task (Mesnil et al., 2013; Raymond and Riccardi, 2007). After determining the likely position of arg1, we split the explicit relations into two sets: SS and OT. We apply token-level sequence labeling approach with the separate models for arguments of intra-sentential and inter-sentential explicit discourse relations (Ghosh et al. 2011; Stepanov and Riccardi, 2012). As shown in Figure 1, we apply two components to deal with these two cases. Besides, in OT, we also train separated models to deal with Arg1 and Arg2 extraction. Since for sequence labeling we use IOBE (Inside, Out, Begin, End) notation as the labels for both Arg1 and Arg2. For example</context>
</contexts>
<marker>Raymond, Riccardi, 2007</marker>
<rawString>Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language understanding. In Proceedings of the Interspeech 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frdric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>James Bergstra</author>
<author>Ian Goodfellow</author>
<author>Arnaud Bergeron</author>
<author>Nicolas Bouchard</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: new features and speed improvements.</title>
<date>2012</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy).</booktitle>
<contexts>
<context position="13081" citStr="Bastien et al., 2012" startWordPosition="2137" endWordPosition="2140">re is already an explicit relation from the previous step between two adjacent sentences, they are exempt from this step. In our system, we just apply a majority classifier, labeling all non-explicit relation candidates as EntRel. 3 Experiments and Results 3.1 System Setup All available training data, development set, test sets from CoNLL 2015 (LDC2015E21)1 are used in this study. Besides, we use the Skip-gram Neural Word Embeddings2 for RNNs. All the used syntactic information are automatically predicted by the Berkeley Parser3. We use Maxent toolkit4 for the ME method. And we apply Theano5 (Bastien et al., 2012; Bergstra et al., 2010) for the RNNs. We use the Python programming language to develop all the compontents and divided each component into two parts: one is training which is processed in our CPU and GPU servers and the other is decoding which is run on TIRA server6. 3.2 Official Results The official results are shown in Table 5. The performance of connective classifier is around 80%, which is not good enough. There are two reasons: 1, we skip some separated connectives such as either or, neither nor etc. and 2, the current feature set missed some syntactic information. For argument extracti</context>
</contexts>
<marker>Bastien, Lamblin, Pascanu, Bergstra, Goodfellow, Bergeron, Bouchard, Warde-Farley, Bengio, 2012</marker>
<rawString>Frdric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio. 2012. Theano: new features and speed improvements. In Proceedings of the Python for Scientific Computing Conference (SciPy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Frdric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy).</booktitle>
<contexts>
<context position="13105" citStr="Bergstra et al., 2010" startWordPosition="2141" endWordPosition="2144">cit relation from the previous step between two adjacent sentences, they are exempt from this step. In our system, we just apply a majority classifier, labeling all non-explicit relation candidates as EntRel. 3 Experiments and Results 3.1 System Setup All available training data, development set, test sets from CoNLL 2015 (LDC2015E21)1 are used in this study. Besides, we use the Skip-gram Neural Word Embeddings2 for RNNs. All the used syntactic information are automatically predicted by the Berkeley Parser3. We use Maxent toolkit4 for the ME method. And we apply Theano5 (Bastien et al., 2012; Bergstra et al., 2010) for the RNNs. We use the Python programming language to develop all the compontents and divided each component into two parts: one is training which is processed in our CPU and GPU servers and the other is decoding which is run on TIRA server6. 3.2 Official Results The official results are shown in Table 5. The performance of connective classifier is around 80%, which is not good enough. There are two reasons: 1, we skip some separated connectives such as either or, neither nor etc. and 2, the current feature set missed some syntactic information. For argument extraction, the reasonable score</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Frdric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny A Stepanov</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Comparative evaluation of argument extraction algorithms in discourse relation parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of 13th International Conference on Parsing Technologies (IWPT</booktitle>
<marker>Stepanov, Riccardi, 2013</marker>
<rawString>Evgeny A. Stepanov and Giuseppe Riccardi. 2013. Comparative evaluation of argument extraction algorithms in discourse relation parsing. In Proceedings of 13th International Conference on Parsing Technologies (IWPT 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Ming</author>
<author>Zhu Qiao</author>
<author>Zhou Guo Dong</author>
</authors>
<title>A Unified Framework for Discourse Argument Identification via Shallow Semantic Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of 24th International Conference on Computational Linguistics.</booktitle>
<marker>Ming, Qiao, Dong, 2012</marker>
<rawString>Xu Ming, Zhu Qiao and Zhou Guo Dong. 2012. A Unified Framework for Discourse Argument Identification via Shallow Semantic Parsing. In Proceedings of 24th International Conference on Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>