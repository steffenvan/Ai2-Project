<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.99843">
Semi-Supervised Lexicon Mining from Parenthetical Expressions
in Monolingual Web Pages
</title>
<author confidence="0.993878">
Xianchao Wu† Naoaki Okazaki† Jun’ichi Tsujii†‡
</author>
<affiliation confidence="0.974962">
†Computer Science, Graduate School of Information Science and Technology, University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
‡School of Computer Science, University of Manchester
National Centre for Text Mining (NaCTeM)
</affiliation>
<address confidence="0.603411">
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
</address>
<email confidence="0.982904">
{wxc, okazaki, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.994737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931842105263">
This paper presents a semi-supervised learn-
ing framework for mining Chinese-English
lexicons from large amount of Chinese Web
pages. The issue is motivated by the ob-
servation that many Chinese neologisms are
accompanied by their English translations in
the form of parenthesis. We classify par-
enthetical translations into bilingual abbrevi-
ations, transliterations, and translations. A
frequency-based term recognition approach is
applied for extracting bilingual abbreviations.
A self-training algorithm is proposed for min-
ing transliteration and translation lexicons. In
which, we employ available lexicons in terms
of morpheme levels, i.e., phoneme correspon-
dences in transliteration and grapheme (e.g.,
suffix, stem, and prefix) correspondences in
translation. The experimental results verified
the effectiveness of our approaches.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945476190476">
Bilingual lexicons, as lexical or phrasal parallel
corpora, are widely used in applications of multi-
lingual language processing, such as statistical ma-
chine translation (SMT) and cross-lingual informa-
tion retrieval. However, it is a time-consuming task
for constructing large-scale bilingual lexicons by
hand. There are many facts cumber the manual de-
velopment of bilingual lexicons, such as the contin-
uous emergence of neologisms (e.g., new technical
terms, personal names, abbreviations, etc.), the dif-
ficulty of keeping up with the neologisms for lexi-
cographers, etc. In order to turn the facts to a better
way, one of the simplest strategies is to automati-
cally mine large-scale lexicons from corpora such as
the daily updated Web.
Generally, there are two kinds of corpora used
for automatic lexicon mining. One is the purely
monolingual corpora, wherein frequency-based
expectation-maximization (EM, refer to (Dempster
et al., 1977)) algorithms and cognate clues play a
central role (Koehn and Knight, 2002). Haghighi
et al. (2008) presented a generative model based
on canonical correlation analysis, in which monolin-
gual features such as the context and orthographic
substrings of words were taken into account. The
other is multilingual parallel and comparable cor-
pora (e.g., Wikipedia1), wherein features such as co-
occurrence frequency and context are popularly em-
ployed (Cheng et al., 2004; Shao and Ng, 2004; Cao
et al., 2007; Lin et al., 2008).
In this paper, we focus on a special type of com-
parable corpus, parenthetical translations. The issue
is motivated by the observation that Web pages and
technical papers written in Asian languages (e.g.,
Chinese, Japanese) sometimes annotate named enti-
ties or technical terms with their translations in En-
glish inside a pair of parentheses. This is considered
to be a traditional way to annotate new terms, per-
sonal names or other named entities with their En-
glish translations expressed in brackets. Formally,
a parenthetical translation can be expressed by the
following pattern,
</bodyText>
<equation confidence="0.591347">
f1 f2 ... fJ (e1 e2 ... eI). (1)
</equation>
<bodyText confidence="0.8697718">
Here, f1 f2 ... fJ(fJ1 ), the pre-parenthesis text, de-
notes the word sequence of some language other
than English; and e1 e2 ... eI(eI1), the in-parenthesis
text, denotes the word sequence of English. We sep-
arate parenthetical translations into three categories:
</bodyText>
<footnote confidence="0.995819">
1http://en.wikipedia.org/wiki/Main Page
</footnote>
<page confidence="0.95274">
424
</page>
<note confidence="0.9156785">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 424–432,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<table confidence="0.992764">
Type Examples with translations in italic
Abbreviation 54f,11 k* =LK ANTI ik (GCOS)
to Global Climate Observing System (GCOS)
Transliteration ;4 4 &apos;411 +ajK- 4+j,&apos;f-(Shipton-Tilman)
brand will be among Shipton-Tilman (Shipton-Tilman)
Translation R9j ;k��f,11 *MA(Cancelbots)
time bomb, Cancelbots (Cancelbots)
Mixture (Bradford University)
the English Bradford University (Bradford University)
that holds lessons in Hongkong
&apos;4 -*A- moi* 0 -1FCQx11 -&apos;+wA4+ k*
</table>
<tableCaption confidence="0.850871">
Table 1: Parenthetical translation categories and exam-
ples extracted from Chinese Web pages. Mixture stands
for the mixture of translation (University) and translitera-
tion (Bradford). ‘11’ denotes the left boundary of f1 .
</tableCaption>
<bodyText confidence="0.999879418181818">
bilingual abbreviation, transliteration, and transla-
tion. Table 1 illustrates examples of these categories.
We address several characteristics of parenthetical
translations that differ from traditional comparable
corpora. The first is that they only appear in mono-
lingual Web pages or documents, and the context
information of ei is unknown. Second, frequency
and word number of el are frequently small. This
is because parenthetical translations are only used
when the authors thought that f1 contained some
neologism(s) which deserved further explanation in
another popular language (e.g., English). Thus, tra-
ditional context based approaches are not applicable
and frequency based approaches may yield low re-
call while with high precision. Furthermore, cog-
nate clues such as orthographic features are not ap-
plicable between language pairs such as English and
Chinese.
Parenthetical translation mining faces the follow-
ing issues. First, we need to distinguish paren-
thetical translations from parenthetical expressions,
since parenthesis has many functions (e.g., defining
abbreviations, elaborations, ellipsis, citations, anno-
tations, etc.) other than translation. Second, the
left boundary (denoted as 11 in Table 1) of the pre-
parenthesis text need to be determined to get rid of
the unrelated words. Third, we need further distin-
guish different translation types, such as bilingual
abbreviation, the mixture of translation and translit-
eration, as shown in Table 1.
In order to deal with these problems, supervised
(Cao et al., 2007) and unsupervised (Li et al., 2008)
methods have been proposed. However, supervised
approaches are restricted by the quality and quantity
of manually constructed training data, and unsuper-
vised approaches are totally frequency-based with-
out using any semantic clues. In contrast, we pro-
pose a semi-supervised framework for mining par-
enthetical translations. We apply a monolingual ab-
breviation extraction approach to bilingual abbrevia-
tion extraction. We construct an English-syllable to
Chinese-pinyin transliteration model which is self-
trained using phonemic similarity measurements.
We further employ our cascaded translation model
(Wu et al., 2008) which is self-trained based on
morpheme-level translation similarity.
This paper is organized as follows. We briefly
review the related work in the next section. Our
system framework and self-training algorithm is de-
scribed in Section 3. Bilingual abbreviation ex-
traction, self-trained transliteration models and cas-
caded translation models are described in Section 4,
5, and 6, respectively. In Section 7, we evaluate our
mined lexicons by Wikipedia. We conclude in Sec-
tion 8 finally.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999171521739131">
Numerous researchers have proposed a variety of
automatic approaches to mine lexicons from the
Web pages or other large-scale corpora. Shao and
Ng (2004) presented a method to mine new transla-
tions from Chinese and English news documents of
the same period from different news agencies, com-
bining both transliteration and context information.
Kuo et al. (2006) used active learning and unsu-
pervised learning for mining transliteration lexicon
from the Web pages, in which an EM process was
used for estimating the phonetic similarities between
English syllables and Chinese characters.
Cao et al. (2007) split parenthetical translation
mining task into two parts, transliteration detection
and translation detection. They employed a translit-
eration lexicon for constructing a grapheme-based
transliteration model and annotated boundaries man-
ually to train a classifier. Lin et al. (2008) applied
a frequency-based word alignment approach, Com-
petitive Link (Melanmed, 2000), to determine the
outer boundary (Section 7).
On the other hand, there have been many semi-
supervised approaches in numerous applications
</bodyText>
<page confidence="0.998468">
425
</page>
<figure confidence="0.983213583333333">
Chinese Web pages
Parenthetical expression extraction{C(E)}
S-MSRSeg
Chinese word segmentation{c...(e...)}
(Lin et al., 2008)
Heuristic filtering{c...(e...)}
Section 4
Bilingual abbreviation mining
Section 5
Transliteration lexicon mining
Section 6
Translation lexicon mining
</figure>
<figureCaption confidence="0.6781165">
Figure 1: The system framework of mining lexicons from
Chinese Web pages.
(Zhu, 2007), such as self-training in word sense
disambiguation (Yarowsky, 2005) and parsing (Mc-
Closky et al., 2008). In this paper, we apply self-
training to a new topic, lexicon mining.
</figureCaption>
<sectionHeader confidence="0.9748245" genericHeader="method">
3 System Framework and Self-Training
Algorithm
</sectionHeader>
<bodyText confidence="0.997389714285714">
Figure 1 illustrates our system framework for min-
ing lexicons from Chinese Web pages. First, par-
enthetical expressions matching Pattern 1 are ex-
tracted. Then, pre-parenthetical Chinese sequences
are segmented into word sequences by S-MSRSeg2
(Gao et al., 2006). The initial parenthetical transla-
tion corpus is constructed by applying the heuristic
rules defined in (Lin et al., 2008)3. Based on this
corpus, we mine three lexicons step by step, a bilin-
gual abbreviation lexicon, a transliteration lexicon,
and a translation lexicon. The abbreviation candi-
dates are extracted firstly by using a heuristic rule
(Section 4.1). Then, the transliteration candidates
are selected by employing a transliteration model
(Section 5.1). Specially, fJ1 (eI1) is taken as a translit-
eration candidate only if a word ei in eI1 can be
transliterated. In addition, a transliteration candidate
will also be considered as a translation candidate if
not all ei can be transliterated (refer to the mixture
example in Table1). Finally, after abbreviation filter-
ing and transliteration filtering, the remaining candi-
</bodyText>
<footnote confidence="0.983071">
2http://research.microsoft.com/research/downloads/details/
7a2bb7ee-35e6-40d7-a3f1-0b743a56b424/details.aspx
3e.g., fJ1 is predominantly in Chinese and eI1 is predomi-
nantly in English
</footnote>
<construct confidence="0.711827">
Algorithm 1 self-training algorithm
Require: L, U = {fJ1 (eI1)}, T, M &gt;L, (labeled) train-
ing set; U, (unlabeled) candidate set; T, test set; M, the
transliteration or translation model.
</construct>
<listItem confidence="0.780324133333333">
1: Lexicon = {} &gt; new mined lexicon
2: repeat
3: N = {} &gt; new mined lexicon during one iteration
4: train M on L
5: evaluate M on T
6: for fJ1 (eI1) E U do
7: topN = {C&apos;|decode eI1 by M}
8: N = N U {(c, eI1)|c E fJ1 n
3C&apos; E topN s.t. similarity{c, C&apos;} &gt; B}
9: end for
10: U = U − N
11: L = unified(L U N)
12: Lexicon = unified(Lexicon U N)
13: until |N |&lt; 2
14: return Lexicon &gt; the output
</listItem>
<bodyText confidence="0.992924033333333">
dates are used for translation lexicon mining.
Algorithm 1 addresses the self-training algorithm
for lexicon mining. The main part is a loop from
Line 2 to Line 13. A given seed lexicon is taken
as labeled data and is split into training and testing
sets (L and T). U={fJ1 (eI1)}, stands for the (unla-
beled) parenthetical expression set. Initially, a trans-
lation/transliteration model (M) is trained on L and
evaluated on T (Line 4 and 5). Then, the English
phrase eI1 of each unlabeled entry is decoded by M,
and the top-N outputs are stored in set toff (Line
7-8). A similarity function on c (a word substring
of fJ1 ) and a top-N output C&apos; is employed to make
the decision of classification: the pair (c, eI1) will be
selected as a new entry if the similarity between c
and C&apos; is no smaller than a threshold value 0 (Line
8). After processing each entry in U, the new mined
lexicon N is deleted from U and unified with the
current training set L as the new training set (Line
10 and 11). Also, N is added to the final lexicon
(Line 12). When |N |is lower than a threshold, the
loop stops. Finally, the algorithm returns the mined
lexicon.
One of the open problems in Algorithm 1 is how
to append new mined entries into the existing seed
lexicon, considering they have different distribu-
tions. One way is to design and estimate a weight
function on the frequency of new mined entries. For
simplicity, we use a deficient strategy that takes the
weights of all new mined entries to be one.
</bodyText>
<page confidence="0.998376">
426
</page>
<sectionHeader confidence="0.983012" genericHeader="method">
4 Bilingual Abbreviation Extraction
</sectionHeader>
<subsectionHeader confidence="0.966237">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.998644857142857">
The method that we use for extracting a bilingual
abbreviation lexicon from parenthetical expressions
is inspired by (Okzaki and Ananiadou, 2006). They
used a term recognition approach to build a monolin-
gual abbreviation dictionary from the Medical Liter-
ature Analysis and Retrieval System Online (MED-
LINE) abstracts, wherein acronym definitions (e.g.,
ADM is short for adriamycin, adrenomedullin, etc.)
are abundant. They reported 99% precision and 82-
95% recall. Through locating a textual fragment
with an acronym and its expanded form in pattern
long form (short form), (2)
they defined a heuristic formula to compute the long-
form likelihood LH(c) for a candidate c:
</bodyText>
<equation confidence="0.822344">
freq(t)
freq(t) x
&amp;ET, freq(t).
(3)
</equation>
<bodyText confidence="0.984827833333333">
Here, c is a long-form candidate; freq(c) denotes the
frequency of co-occurrence of c with a short-form;
and Tc is a set of nested long-form candidates, each
of which consists of a preceding word followed by
the candidate c. Obviously, for t E Tc, Equation 3
can be explained as:
</bodyText>
<equation confidence="0.995517">
LH(c) = freq(c) − ]E[freq(t)]. (4)
</equation>
<bodyText confidence="0.999733428571428">
In this paper, we apply their method on the task
of bilingual abbreviation lexicon extraction. Now,
the long-form is a Chinese word sequence and the
short-form is an English acronym. We filter the par-
enthetical expressions in the Web pages with several
heuristic rules to meet the form of pattern 2 and to
save the computing time:
</bodyText>
<listItem confidence="0.970781090909091">
• the short-form (eI1) should contain only one En-
glish word (I = 1), and all letters in which
should be capital;
• similar with (Lin et al., 2008), the pre-
parenthesis text is trimmed with: |c |&gt; 10 x
|eI1 |+ 6 when |eI1 |G 6, and |c |&gt; 2 x |eI1 |+ 6,
otherwise. |c |and |eI1 |are measured in bytes.
We further trim the remaining pre-parenthesis
text by punctuations other than hyphens and
dots, i.e., the right most punctuation and its left
subsequence are discarded.
</listItem>
<table confidence="0.9966632">
No. Chinese long-form candidates LH T/F
1 肿瘤 相关 抗原 172.5 T
Tumor-Associated Antigen
2 硫 代 乙酰 胺 79.9 T
thioacetamide
3 胺 33.8 F
amine
抗原 24.5 F
antigen
5 相关 抗原 21.2 F
associated antigen
的 肿瘤 相关 抗原 F
s Tumor-Associated Antigen
4 总 氨基酸 T
total amino acid
</table>
<figure confidence="0.3886005">
6
16.5
Table 2: Top-7 Chinese long-form candidates for the En-
glish acronym TAA, according to the LH score.
7
16.2
</figure>
<subsectionHeader confidence="0.703135">
4.2 Experiment
</subsectionHeader>
<bodyText confidence="0.999595321428571">
We used SogouT Internet Corpus Version 2.04,
which contains about 13 billion original Web pages
(mainly Chinese) in the form of 252 gigabyte .txt
files. In addition, we used 55 gigabyte (.txt for-
mat) Peking University Chinese Paper Corpus. We
constructed a partially parallel corpus in the form
of Pattern 1 from the union of the two corpora us-
ing the heuristic rules defined in (Lin et al., 2008).
We gained a partially parallel corpus which contains
12,444,264 entries.
We extracted 107,856 distinct English acronyms.
Limiting LH score &gt; 1.0 in Equation 3, we gained
2,020,012 Chinese long-form candidates for the
107,856 English acronyms. Table 2 illustrates the
top-7 Chinese long-form candidates of the English
acronym TAA. Three candidates are correct (T) long-
forms while the other 4 are wrong (F). Wrong can-
didates from No. 3 to 5 are all subsequences of the
correct candidate No. 1. No. 6 includes No. 1 while
with a Chinese functional word de in the left most
side. These error types can be easily tackled with
some filtering patterns, such as ‘remove the left most
functional word in the long-form candidates’, ‘only
keep the relatively longer candidates with larger LH
score’, etc.
Since there does not yet exists a common eval-
uation data set for the bilingual abbreviation lexi-
con, we manually evaluated a small sample of it.
</bodyText>
<footnote confidence="0.519864">
4http://www.sogou.com/labs/dl/t.html
</footnote>
<equation confidence="0.976775333333333">
�
LH(c) = freq(c) −
tET,
</equation>
<page confidence="0.983549">
427
</page>
<bodyText confidence="0.999977166666667">
Of the 107,856 English acronyms, we randomly se-
lected 200 English acronyms and their top-1 Chi-
nese long-form candidates for manually evaluating.
We found, 92 candidates were correct including 3
transliteration examples. Of the 108 wrong candi-
dates, 96 candidates included the correct long-form
with some redundant words on the left side (i.e., c =
(word)+ correct long-form), the other 12 candidates
missed some words of the correct long-form or had
some redundant words right before the left paren-
thesis (i.e., c = (word)* correct long-form (word)+
or c = (word)* subsequence of correct long-form
word)*). We classified the redundant word right be-
fore the correct long-form of each of the 96 candi-
dates, de occupied 32, noun occupied 7, verb occu-
pied 18, prepositions and conjunctions occupied the
remaining ones.
In total, the abbreviation translation accuracy is
44.5%. We improved the accuracy to 60.5% with
an additional de filtering pattern. According to for-
mer mentioned error analysis, the accuracy may fur-
ther be improved if a Chinese part-of-speech tagger
is employed and the non-nominal words in the long-
form are removed beforehand.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="method">
5 Self-Training for Transliteration Models
</sectionHeader>
<bodyText confidence="0.999972428571429">
In this section, we first describe and compare three
transliteration models. Then, we select and train the
best model following Algorithm 1 for lexicon min-
ing. We investigate two things, the scalability of the
self-trained model given different amount of initial
training data, and the performance of several strate-
gies for selecting new training samples.
</bodyText>
<subsectionHeader confidence="0.995019">
5.1 Model description
</subsectionHeader>
<bodyText confidence="0.999961333333334">
We construct and compare three forward translit-
eration models, a phoneme-based model (English
phonemes to Chinese pinyins), a grapheme-based
model (English syllables to Chinese characters)
and a hybrid model (English syllables to Chinese
pinyins). Similar models have been compared in
(Oh et al., 2006) for English-to-Korean and English-
to-Japanese transliteration. All the three models are
phrase-based, i.e., adjacent phonemes or graphemes
are allowable to form phrase-level transliteration
units. Building the correspondences on phrase
level can effectively tackle the missing or redundant
phoneme/grapheme problem during transliteration.
For example, when Aamodt is transliterated into a
m¯o t`e5, a and d are missing. The problem can be
easily solved when taking Aa and dt as single units
for transliterating.
Making use of Moses (Koehn et al., 2007), a
phrase-based SMT system, Matthews (2007) has
shown that the performance was comparable to re-
cent state-of-the-art work (Jiang et al., 2007) in
English-to-Chinese personal name transliteration.
Matthews (2007) took transliteration as translation
at the surface level. Inspired by his idea, we also
implemented our transliteration models employing
Moses. The main difference is that, while Matthews
(2007) tokenized the English names into individual
letters before training in Moses, we split them into
syllables using the heuristic rules described in (Jiang
et al., 2007), such that one syllable only contains one
vowel letter or a combination of a consonant and a
vowel letter.
English syllable sequences are used in the
grapheme-based and hybrid models. In the
phoneme-based model, we transfer English names
into phonemes and Chinese characters into Pinyins
in virtue of the CMU pronunciation dictionary6 and
the LDC Chinese character-to-pinyin list7.
In the mass, the grapheme-based model is the
most robust model, since no additional resources are
needed. However, it suffers from the Chinese homo-
phonic character problem. For instance, pinyin ai
corresponds to numerous Chinese characters which
are applicable to personal names. The phoneme-
based model is the most suitable model that reflects
the essence of transliteration, while restricted by ad-
ditional grapheme to phoneme dictionaries. In or-
der to eliminate the confusion of Chinese homo-
phonic characters and alleviate the dependency on
additional resources, we implement a hybrid model
that accepts English syllables and Chinese pinyins
as formats of the training data. This model is called
hybrid, since English syllables are graphemes and
Chinese pinyins are phonemes.
</bodyText>
<footnote confidence="0.9995695">
5The tones of Chinese pinyins are ignored in our translitera-
tion models for simplicity.
6http://www.speech.cs.cmu.edu/cgi-bin/cmudict
7http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt
</footnote>
<page confidence="0.996845">
428
</page>
<figure confidence="0.98720525">
gotpheme-based ph$neox b-d
BIEU WE !E E&amp;quot;at#h BIEI WE !E E&amp;quot;at#h
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
02 0.2
0.0 0.0
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
®z_ph­e_length nae_phrase_length
h(brid-based %$mpar&amp;s$n $n Wat#h
BIEU WE !E E&amp;quot;at#h gwphem ph$neme h(brid
1.0 05
0.8 0.4
0.6 03
0.4
0.2 02
0.1
0.0 0.0
�����������������
</figure>
<figureCaption confidence="0.9995445">
Figure 2: The performances of the transliteration models
and their comparison on EMatch.
</figureCaption>
<figure confidence="0.592451333333333">
1 2 3 4 5 6 7 �
�����������������
� � � � � � � �
</figure>
<subsectionHeader confidence="0.95587">
5.2 Experimental model selection
</subsectionHeader>
<bodyText confidence="0.999710086956522">
Similar to (Jiang et al., 2007), the transliteration
models were trained and tested on the LDC Chinese-
English Named Entity Lists Version 1.08. The origi-
nal list contains 572,213 English people names with
Chinese transliterations. We extracted 74,725 en-
tries in which the English names also appeared in
the CMU pronunciation dictionary. We randomly
selected 3,736 entries as an open testing set and the
remaining entries as a training set9. The results were
evaluated using the character/pinyin-based 4-gram
BLEU score (Papineni et al., 2002), word error rate
(WER), position independent word error rate (PER),
and exact match (EMatch).
Figure 2 reports the performances of the three
models and the comparison based on EMatch. From
the results, we can easily draw the conclusion that
the hybrid model performs the best under the maxi-
mal phrase length (mpl, the maximal phrase length
allowed in Moses) from 1 to 8. The performances
of the models converge at or right after mpl =
4. The pinyin-based WER of the hybrid model is
39.13%, comparable to the pinyin error rate 39.6%,
reported in (Jiang et al., 2007)10. Thus, our further
</bodyText>
<footnote confidence="0.9803135">
8Linguistic Data Consortium catalog number:
LDC2005T34 (former catalog number: LDC2003E01)
9Jiang et al. (2007) selected 25,718 personal name pairs
from LDC2003E01 as the experiment data: 200 as development
set, 200 as test set, and the remaining entries as training set.
10It should be notified that we achieved this result by using
larger training set (70,989 vs. 25,718) and larger test set (3,736
vs. 200) comparing with (Jiang et al., 2007), and we did not use
</footnote>
<table confidence="0.990253">
% 0t 1t 2t 3t 4t 5t Strategy
5 .3879 .3937 .3971 .3958 .3972 .3971 top1 em
.3911 .3979 .3954 .3974 .3965 top1 am
.4062 .4182 .4208 .4218 .4201 top5 em
.3987 .4177 .4190 .4192 .4189 top5 am
10 .4092 .4282 .4258 .4202 .4203 .4205 top1 em
.4121 .4190 .4180 .4174 .4200 top1 am
.4305 .4386 .4399 .4438 .4403 top5 em
.4289 .4263 .4292 .4291 .4288 top5 am
20 .4561 .4538 .4562 .4550 .4543 .4551 top1 em
.4532 .4578 .4544 .4545 .4541 top1 am
.4624 .4762 .4754 .4748 .4746 top5 em
.4605 .4677 .4677 .4674 .4679 top5 am
40 .4779 .4791 .4793 .4799 .4794 .4808 top1 em
.4774 .4794 .4779 .4789 .4784 top1 am
.4808 .4811 .4791 .4795 .4790 top5 em
.4775 .4778 .4781 .4785 .4779 top5 am
60 .5032 .4939 .5004 .5012 .5012 .5016 top1 em
.4919 .4988 .4990 .4994 .4990 top1 am
.5013 .5063 .5059 .5066 .5065 top5 em
.4919 .4960 .4970 .4977 .4962 top5 am
80 .5038 .4984 .4984 .5004 .5006 .4995 top1 em
.4916 .4916 .4914 .4915 .4916 top1 am
.5039 .5037 .5053 .5054 .5042 top5 em
.4950 .5028 .5027 .5032 .5032 top5 am
100 .5045 .5077 .5053 .5067 .5063 .5066 top1 em
.5045 .5054 .5046 .5050 .5055 top1 am
.5108 .5102 .5111 .5108 .5115 top5 em
.5105 .5106 .5100 .5094 .5109 top5 am
</table>
<tableCaption confidence="0.991495">
Table 3: The BLEU score of self-trained h4 translitera-
tion models under four selection strategies. nt (n=1..5)
stands for the n-th iteration.
</tableCaption>
<bodyText confidence="0.99509">
self-training experiments are pursued on the hybrid
model taking mpl to be 4 (short for h4, hereafter).
</bodyText>
<subsectionHeader confidence="0.6065005">
5.3 Experiments on the self-trained hybrid
model
</subsectionHeader>
<bodyText confidence="0.998587333333333">
As former mentioned, we investigate the scalability
of the self-trained h4 model by respectively using 5,
10, 20, 40, 60, 80, and 100 percent of initial training
data, and the performances of using exact matching
(em) or approximate matching (am, line 8 in Algo-
rithm 1) on the top-1 and top-5 outputs (line 7 in Al-
gorithm 1) for selecting new training samples. We
used edit distance (ed) to measure the em and am
similarities:
</bodyText>
<equation confidence="0.972277">
ed(c, C&apos;) = 0 or &lt; syllable number(C&apos;)/2. (5)
</equation>
<bodyText confidence="0.974348909090909">
When applying Algorithm 1 for transliteration lexi-
con mining, we decode each word in el respectively.
The algorithm terminated in five iterations when we
set the terminal threshold 2 (Line 13 in Algorithm 1)
to be 100.
For simplicity, Table 3 only illustrates the BLEU
score of h4 models under four selection strategies.
From this table, we can draw the following conclu-
sions. First, with fewer initial training data, the im-
provement is better. The best relative improvements
additional Web resources as Jiang et al. (2007) did.
</bodyText>
<page confidence="0.998274">
429
</page>
<bodyText confidence="0.999904909090909">
are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%,
and 1.39%, respectively. Second, using top-5 and
em for new training data selection performs the best
among the four strategies. Compared under each it-
eration, using top-5 is better than using top-1; em
is better than am; and top-5 with am is a little bet-
ter than top-1 with em. We mined 39,424, 42,466,
46,116, 47,057, 49,551, 49,622, and 50,313 distinct
entries under the six types of initial data with top-5
plus em strategy. The 50,313 entries are taken as the
final transliteration lexicon for further comparison.
</bodyText>
<sectionHeader confidence="0.941785" genericHeader="method">
6 Self-Training for a Cascaded Translation
Model
</sectionHeader>
<bodyText confidence="0.99998995">
We classify the parenthetical translation candidates
by employing a translation model. In contrast to
(Lin et al., 2008), wherein the lengthes of prefixes
and suffixes of English words were assumed to be
three bytes, we segment words into morphemes (se-
quences of prefixes, stems, and suffixes) by Morfes-
sor 0.9.211, an unsupervised language-independent
morphological analyzer (Creutz and Lagus, 2007).
We use the morpheme-level translation similarity
explicitly in our cascaded translation model (Wu et
al., 2008), which makes use of morpheme, word,
and phrase level translation units. We train Moses
to gain a phrase-level translation table. To gain a
morpheme-level translation table, we run GIZA++
(Och and Ney, 2003) on both directions between En-
glish morphemes and Chinese characters, and take
the intersection of Iiterbi alignments. The English-
to-Chinese translation probabilities computed by
GIZA++ are attached to each morpheme-character
element in the intersection set.
</bodyText>
<subsectionHeader confidence="0.994959">
6.1 Experiment
</subsectionHeader>
<bodyText confidence="0.999838875">
The Wanfang Chinese-English technical term dictio-
nary12, which contains 525,259 entries in total, was
used for training and testing. 10,000 entries were
randomly selected as the test set and the remaining
as the training set. Again, we investigated the scala-
bility of the self-trained cascaded translation model
by respectively using 20, 40, 60, 80, and 100 per-
cent of initial training data. An aggressive similar-
</bodyText>
<footnote confidence="0.987919">
11http://www.cis.hut.fi/projects/morpho/
12http://www.wanfangdata.com.cn/Search/ResourceBrowse
.aspx
</footnote>
<table confidence="0.9993515">
% 0t 1t 2t 3t 4t 5t
20 .1406 .1196 .1243 .1239 .1176 .1179
40 .1091 .1224 .1386 .1345 .1479 .1466
60 .1630 .1624 .1429 .1714 .1309 .1398
80 .1944 .1783 .1886 .1870 .1884 .1873
100 .1810 .1814 .1539 .1981 .1542 .1944
</table>
<tableCaption confidence="0.9932015">
Table 4: The BLEU score of self-trained cascaded trans-
lation model under five initial training sets.
</tableCaption>
<bodyText confidence="0.991598235294117">
ity measurement was used for selecting new training
samples:
first char(c) = first char(C&apos;) n min{ed(c, C&apos;)}.
(6)
Here, we judge if the first characters of c and C&apos;
are similar or not. c was gained by deleting zero
or more characters from the left side of fi . When
more than one c satisfied this condition, the c that
had the smallest edit distance with C&apos; was selected.
When applying Algorithm 1 for translation lexicon
mining, we took el as one input for decoding instead
of decoding each word respectively. Only the top-1
output (C&apos;) was used for comparing. The algorithm
stopped in five iterations when we set the terminal
threshold 2 to be 2000.
For simplicity, Table 4 only illustrates the BLEU
score of the cascaded translation model under five
initial training sets. For the reason that there are fi-
nite phonemes in English and Chinese while the se-
mantic correspondences between the two languages
tend to be infinite, Table 4 is harder to be analyzed
than Table 3. When initially using 40%, 60%, and
100% training data for self-training, the results tend
to be better at some iterations. We gain 35.6%,
5.2%, and 9.4% relative improvements, respectively.
However, the results tend to be worse when 20% and
80% training data were used initially, with 11.6%
and 3.0% minimal relative loss. The best BLEU
scores tend to be better when more initial training
data are available. We mined 1,038,617, 1,025,606,
1,048,761, 1,056,311, and 1,060,936 distinct entries
under the five types of initial training data. The
1,060,936 entries are taken as the final translation
lexicon for further comparison.
</bodyText>
<sectionHeader confidence="0.988533" genericHeader="method">
7 Wikipedia Evaluation
</sectionHeader>
<bodyText confidence="0.9996735">
We have mined three kinds of lexicons till now,
an abbreviation lexicon containing 107,856 dis-
</bodyText>
<page confidence="0.992777">
430
</page>
<table confidence="0.97325275">
En. to Ch. Ch. to En.
Cov EMatch Cov EMatch
Our Lexicon 22.8% 5.2% 23.2% 5.5%
Unsupervised 23.5% 5.4% 24.0% 5.4%
</table>
<tableCaption confidence="0.864673333333333">
Table 5: The results of our lexicon and an unsupervised-
mined lexicon (Lin et al., 2008) evaluated under
Wikipedia title dictionary. Cov is short for coverage.
</tableCaption>
<bodyText confidence="0.973648205882353">
similar English acronyms with 2,020,012 Chinese
long-form candidates; a transliteration lexicon with
50,313 distinct entries; and a translation lexicon
with 1,060,936 distinct entries. The three lexicons
are combined together as our final lexicon.
Similar with (Lin et al., 2008), we compare our
final mined lexicon with a dictionary extracted from
Wikipedia, the biggest multilingual free-content en-
cyclopedia on the Web. We extracted the titles of
Chinese and English Wikipedia articles13 that are
linked to each other. Since most titles contain less
than five words, we take a linked title pair as a trans-
lation entry without considering the word alignment
relation between the words inside the titles. The re-
sult lexicon contains 105,320 translation pairs be-
tween 103,823 Chinese titles and 103,227 English
titles. Obviously, only a small percentage of titles
have more than one translation. Whenever there is
more than one translation, we take the candidate en-
try as correct if and only if it matches one of the
translations.
Moreover, we compare our semi-supervised ap-
proach with an unsupervised approach (Lin et al.,
2008). Lin et al. (2008) took co&apos;(fj, ei) score
14(Gale and Church, 1991) with threshold 0.001 as
the word alignment probability in a word alignment
algorithm, Competitive Link. Competitive Link tries
to align an unlinked ei with an unlinked fj by the
condition that co&apos;(fj, ei) is the biggest. Lin et al.
(2008) relaxed the unlinked constraints to allow con-
secutive sequence of words on one side to be linked
to the same word on the other side15. The left
13English and Chinese Wikipedia pages due to 2008.09.23
are used here.
</bodyText>
<equation confidence="0.8386505">
14(P2(fj� ei) � (a+b)(a+c)(b+d)(c+d), where a is the number
(ad�bc)�
</equation>
<bodyText confidence="0.986191714285714">
of fJ1 (eI1) containing both ei and fj; (a + b) is the number of
fJ1 (eI1) containing ei; (a + c) is the number of fJ1 (eI1) contain-
ing fj; and d is the number of fJ1 (eI1) containing neither ei nor
fj.
15Instead of requiring both ei and fj to have no previous link-
boundary inside fJ1 is determined when each ei in
eI1 is aligned. After applying the modified Compet-
itive Link on the partially parallel corpus which in-
cludes 12,444,264 entries (Section 4.2), we obtained
2,628,366 distinct pairs.
Table 5 shows the results of the two lexicons eval-
uated under Wikipedia title dictionary. The coverage
is measured by the percentage of titles which ap-
pears in the mined lexicon. We then check whether
the translation in the mined lexicon is an exact match
of one of the translations in the Wikipedia lexicon.
Through comparing the results, our mined lexicon is
comparable with the lexicon mined in an unsuper-
vised way. Since the selection is based on phone-
mic and semantic clues instead of frequency, a par-
enthetical translation candidate will not be selected
if the in-parenthetical English text is failed to be
transliterated or translated. This is one reason that
explains why we earned a little lower coverage. An-
other reason comes from the low coverage rate of
seed lexicons used for self-training, only 8.65% En-
glish words in the partially parallel corpus are cov-
ered by the Wanfang dictionary.
</bodyText>
<sectionHeader confidence="0.997389" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.997448409090909">
We have proposed a semi-supervised learning
framework for mining bilingual lexicons from par-
enthetical expressions in monolingual Web pages.
We classified the parenthesis expressions into three
categories: abbreviation, transliteration, and transla-
tion. A set of heuristic rules, a self-trained hybrid
transliteration model, and a self-trained cascaded
translation model were proposed for each category,
respectively.
We investigated the scalability of the self-trained
transliteration and translation models by training
them with different amount of data. The results shew
the stability (transliteration) and feasibility (transla-
tion) of our proposals. Through employing the par-
allel Wikipedia article titles as a gold standard lex-
icon, we gained the comparable results comparing
our semi-supervised framework with our implemen-
tation of Lin et al. (2008)’s unsupervised mining
approach.
ages, they only require that at least one of them be unlinked and
that (suppose ei is unlinked and fj is linked to ek) none of the
words between ei and ek be linked to any word other than fj.
</bodyText>
<page confidence="0.998715">
431
</page>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999890428571429">
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan). We thank
the anonymous reviewers for their constructive com-
ments.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999588021505377">
Cao, Guihong, Jianfeng Gao, and Jian-Yun Nie. 2007.
A system to Mine Large-Scale Bilingual Dictionar-
ies from Monolingual Web Pages. In MT Summit XI.
pages 57–64, Copenhagen, Denmark.
Cheng, Pu-Jen, Yi-Cheng Pan, Wen-Hsiang Lu, and Lee-
Feng Chien. 2004. Creating Multilingual Translation
Lexicons with Regional Variations Using Web Cor-
pora. In ACL 2004, pages 534–541, Barcelona,
Spain.
Creutz, Mathias and Krista Lagus. 2007. Unsupervised
Models for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):Article 3.
Dempster, A. P., N. M. Laird and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, 39:1–38.
Gale, W. and K. Church. 1991. Identifying word corre-
spondence in parallel text. In DARPA NLP Workshop.
Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang.
2006. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguistics, 31(4):531–574.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein 2008. Learning Bilingual Lexicons
from Monolingual Corpora. In ACL-08:HLT. pages
771–779, Columbus, Ohio.
Jiang, Long, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named Entity Translation with Web Min-
ing and Transliteration. In IJCAI 2007. pages 1629–
1634, Hyderabad, India.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007 Poster Session, pages 177–180.
Koehn, Philipp and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora. In
SIGLEX 2002, pages 9–16.
Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning Transliteration Lexicons from the Web. In
COLING-ACL 2006. pages 1129–1136.
Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, and
Marius Pas¸ca. 2008. Mining Parenthetical Transla-
tions from the Web by Word Alignment. In ACL-
08:HLT, pages 994–1002, Columbus, Ohio.
Matthews, David. 2007. Machine Transliteration of
Proper Names. A Thesis of Master. University of Ed-
inburgh.
McClosky, David, Eugene Charniak, and Mark Johnson
2008. When is Self-Training Effective for Parsing? In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 561–
568, manchester, UK.
Melamed, I. Dan. 2000. Models of Translational Equiv-
alence among Words. Computational Linguistics,
26(2):221–249.
Och, Franz Josef and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Oh, Jong-Hoon, Key-Sun Choi, and Hitoshi Isahara.
2006. A Comparison of Different Machine Translit-
eration Models. Journal of Artifical Intelligence Re-
search, 27:119–151.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Building
an Abbreviation Dictionary Using a Term Recognition
Approach. Bioinformatics, 22(22):3089–3095.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL). pages 311–318, Philadel-
phia.
Shao, Li and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING), pages 618–624,
Geneva, Switzerland.
Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, and
Jun’ichi Tsujii. 2008. Improving English-to-Chinese
Translation for Technical Terms Using Morphological
Information. In Proceedings of the 8th Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA), pages 202–211, Waikiki, Hawai’i.
Yarowsky, David. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, pages 189–196, Cam-
bridge, Massachusetts.
Zhu, Xiaojin. 2007. Semi-Supervised Learning Litera-
ture Survery. University of Wisconsin - Madison.
</reference>
<page confidence="0.998562">
432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423091">
<title confidence="0.9978215">Semi-Supervised Lexicon Mining from Parenthetical in Monolingual Web Pages</title>
<author confidence="0.996939">Naoaki Jun’ichi</author>
<affiliation confidence="0.994859">Science, Graduate School of Information Science and Technology, University of</affiliation>
<address confidence="0.93587">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656,</address>
<affiliation confidence="0.948421">of Computer Science, University of National Centre for Text Mining</affiliation>
<address confidence="0.747147">Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK okazaki,</address>
<abstract confidence="0.99927795">This paper presents a semi-supervised learning framework for mining Chinese-English lexicons from large amount of Chinese Web pages. The issue is motivated by the observation that many Chinese neologisms are accompanied by their English translations in the form of parenthesis. We classify parenthetical translations into bilingual abbreviations, transliterations, and translations. A frequency-based term recognition approach is applied for extracting bilingual abbreviations. A self-training algorithm is proposed for mining transliteration and translation lexicons. In which, we employ available lexicons in terms of morpheme levels, i.e., phoneme correspondences in transliteration and grapheme (e.g., suffix, stem, and prefix) correspondences in translation. The experimental results verified the effectiveness of our approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Guihong Cao</author>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
</authors>
<title>A system to Mine Large-Scale Bilingual Dictionaries from Monolingual Web Pages.</title>
<date>2007</date>
<booktitle>In MT Summit XI.</booktitle>
<pages>57--64</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2790" citStr="Cao et al., 2007" startWordPosition="393" endWordPosition="396">he purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can be expressed by the following patter</context>
<context position="6144" citStr="Cao et al., 2007" startWordPosition="888" endWordPosition="891">wing issues. First, we need to distinguish parenthetical translations from parenthetical expressions, since parenthesis has many functions (e.g., defining abbreviations, elaborations, ellipsis, citations, annotations, etc.) other than translation. Second, the left boundary (denoted as 11 in Table 1) of the preparenthesis text need to be determined to get rid of the unrelated words. Third, we need further distinguish different translation types, such as bilingual abbreviation, the mixture of translation and transliteration, as shown in Table 1. In order to deal with these problems, supervised (Cao et al., 2007) and unsupervised (Li et al., 2008) methods have been proposed. However, supervised approaches are restricted by the quality and quantity of manually constructed training data, and unsupervised approaches are totally frequency-based without using any semantic clues. In contrast, we propose a semi-supervised framework for mining parenthetical translations. We apply a monolingual abbreviation extraction approach to bilingual abbreviation extraction. We construct an English-syllable to Chinese-pinyin transliteration model which is selftrained using phonemic similarity measurements. We further emp</context>
<context position="7901" citStr="Cao et al. (2007)" startWordPosition="1150" endWordPosition="1153"> Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other large-scale corpora. Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information. Kuo et al. (2006) used active learning and unsupervised learning for mining transliteration lexicon from the Web pages, in which an EM process was used for estimating the phonetic similarities between English syllables and Chinese characters. Cao et al. (2007) split parenthetical translation mining task into two parts, transliteration detection and translation detection. They employed a transliteration lexicon for constructing a grapheme-based transliteration model and annotated boundaries manually to train a classifier. Lin et al. (2008) applied a frequency-based word alignment approach, Competitive Link (Melanmed, 2000), to determine the outer boundary (Section 7). On the other hand, there have been many semisupervised approaches in numerous applications 425 Chinese Web pages Parenthetical expression extraction{C(E)} S-MSRSeg Chinese word segment</context>
</contexts>
<marker>Cao, Gao, Nie, 2007</marker>
<rawString>Cao, Guihong, Jianfeng Gao, and Jian-Yun Nie. 2007. A system to Mine Large-Scale Bilingual Dictionaries from Monolingual Web Pages. In MT Summit XI. pages 57–64, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pu-Jen Cheng</author>
<author>Yi-Cheng Pan</author>
<author>Wen-Hsiang Lu</author>
<author>LeeFeng Chien</author>
</authors>
<title>Creating Multilingual Translation Lexicons with Regional Variations Using Web Corpora.</title>
<date>2004</date>
<booktitle>In ACL 2004,</booktitle>
<pages>534--541</pages>
<location>Barcelona,</location>
<contexts>
<context position="2753" citStr="Cheng et al., 2004" startWordPosition="385" endWordPosition="388"> for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can</context>
</contexts>
<marker>Cheng, Pan, Lu, Chien, 2004</marker>
<rawString>Cheng, Pu-Jen, Yi-Cheng Pan, Wen-Hsiang Lu, and LeeFeng Chien. 2004. Creating Multilingual Translation Lexicons with Regional Variations Using Web Corpora. In ACL 2004, pages 534–541, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised Models for Morpheme Segmentation and Morphology Learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>3</pages>
<contexts>
<context position="25902" citStr="Creutz and Lagus, 2007" startWordPosition="4107" endWordPosition="4110">3 distinct entries under the six types of initial data with top-5 plus em strategy. The 50,313 entries are taken as the final transliteration lexicon for further comparison. 6 Self-Training for a Cascaded Translation Model We classify the parenthetical translation candidates by employing a translation model. In contrast to (Lin et al., 2008), wherein the lengthes of prefixes and suffixes of English words were assumed to be three bytes, we segment words into morphemes (sequences of prefixes, stems, and suffixes) by Morfessor 0.9.211, an unsupervised language-independent morphological analyzer (Creutz and Lagus, 2007). We use the morpheme-level translation similarity explicitly in our cascaded translation model (Wu et al., 2008), which makes use of morpheme, word, and phrase level translation units. We train Moses to gain a phrase-level translation table. To gain a morpheme-level translation table, we run GIZA++ (Och and Ney, 2003) on both directions between English morphemes and Chinese characters, and take the intersection of Iiterbi alignments. The Englishto-Chinese translation probabilities computed by GIZA++ are attached to each morpheme-character element in the intersection set. 6.1 Experiment The Wa</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Creutz, Mathias and Krista Lagus. 2007. Unsupervised Models for Morpheme Segmentation and Morphology Learning. ACM Transactions on Speech and Language Processing, 4(1):Article 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="2291" citStr="Dempster et al., 1977" startWordPosition="314" endWordPosition="317">are many facts cumber the manual development of bilingual lexicons, such as the continuous emergence of neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical tr</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P., N. M. Laird and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
</authors>
<title>Identifying word correspondence in parallel text.</title>
<date>1991</date>
<booktitle>In DARPA NLP Workshop.</booktitle>
<contexts>
<context position="30520" citStr="Gale and Church, 1991" startWordPosition="4842" endWordPosition="4845">ke a linked title pair as a translation entry without considering the word alignment relation between the words inside the titles. The result lexicon contains 105,320 translation pairs between 103,823 Chinese titles and 103,227 English titles. Obviously, only a small percentage of titles have more than one translation. Whenever there is more than one translation, we take the candidate entry as correct if and only if it matches one of the translations. Moreover, we compare our semi-supervised approach with an unsupervised approach (Lin et al., 2008). Lin et al. (2008) took co&apos;(fj, ei) score 14(Gale and Church, 1991) with threshold 0.001 as the word alignment probability in a word alignment algorithm, Competitive Link. Competitive Link tries to align an unlinked ei with an unlinked fj by the condition that co&apos;(fj, ei) is the biggest. Lin et al. (2008) relaxed the unlinked constraints to allow consecutive sequence of words on one side to be linked to the same word on the other side15. The left 13English and Chinese Wikipedia pages due to 2008.09.23 are used here. 14(P2(fj� ei) � (a+b)(a+c)(b+d)(c+d), where a is the number (ad�bc)� of fJ1 (eI1) containing both ei and fj; (a + b) is the number of fJ1 (eI1) c</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>Gale, W. and K. Church. 1991. Identifying word correspondence in parallel text. In DARPA NLP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="9256" citStr="Gao et al., 2006" startWordPosition="1340" endWordPosition="1343">icon mining Section 6 Translation lexicon mining Figure 1: The system framework of mining lexicons from Chinese Web pages. (Zhu, 2007), such as self-training in word sense disambiguation (Yarowsky, 2005) and parsing (McClosky et al., 2008). In this paper, we apply selftraining to a new topic, lexicon mining. 3 System Framework and Self-Training Algorithm Figure 1 illustrates our system framework for mining lexicons from Chinese Web pages. First, parenthetical expressions matching Pattern 1 are extracted. Then, pre-parenthetical Chinese sequences are segmented into word sequences by S-MSRSeg2 (Gao et al., 2006). The initial parenthetical translation corpus is constructed by applying the heuristic rules defined in (Lin et al., 2008)3. Based on this corpus, we mine three lexicons step by step, a bilingual abbreviation lexicon, a transliteration lexicon, and a translation lexicon. The abbreviation candidates are extracted firstly by using a heuristic rule (Section 4.1). Then, the transliteration candidates are selected by employing a transliteration model (Section 5.1). Specially, fJ1 (eI1) is taken as a transliteration candidate only if a word ei in eI1 can be transliterated. In addition, a transliter</context>
</contexts>
<marker>Gao, Li, Wu, Huang, 2006</marker>
<rawString>Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang. 2006. Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach. Computational Linguistics, 31(4):531–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning Bilingual Lexicons from Monolingual Corpora. In</title>
<date>2008</date>
<booktitle>ACL-08:HLT.</booktitle>
<pages>771--779</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2390" citStr="Haghighi et al. (2008)" startWordPosition="330" endWordPosition="333">e of neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written i</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein 2008. Learning Bilingual Lexicons from Monolingual Corpora. In ACL-08:HLT. pages 771–779, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Lee-Feng Chien</author>
<author>Cheng Niu</author>
</authors>
<title>Named Entity Translation with Web Mining and Transliteration. In IJCAI</title>
<date>2007</date>
<pages>1629--1634</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="18564" citStr="Jiang et al., 2007" startWordPosition="2898" endWordPosition="2901"> models are phrase-based, i.e., adjacent phonemes or graphemes are allowable to form phrase-level transliteration units. Building the correspondences on phrase level can effectively tackle the missing or redundant phoneme/grapheme problem during transliteration. For example, when Aamodt is transliterated into a m¯o t`e5, a and d are missing. The problem can be easily solved when taking Aa and dt as single units for transliterating. Making use of Moses (Koehn et al., 2007), a phrase-based SMT system, Matthews (2007) has shown that the performance was comparable to recent state-of-the-art work (Jiang et al., 2007) in English-to-Chinese personal name transliteration. Matthews (2007) took transliteration as translation at the surface level. Inspired by his idea, we also implemented our transliteration models employing Moses. The main difference is that, while Matthews (2007) tokenized the English names into individual letters before training in Moses, we split them into syllables using the heuristic rules described in (Jiang et al., 2007), such that one syllable only contains one vowel letter or a combination of a consonant and a vowel letter. English syllable sequences are used in the grapheme-based and</context>
<context position="20867" citStr="Jiang et al., 2007" startWordPosition="3257" endWordPosition="3260">://www.speech.cs.cmu.edu/cgi-bin/cmudict 7http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt 428 gotpheme-based ph$neox b-d BIEU WE !E E&amp;quot;at#h BIEI WE !E E&amp;quot;at#h 1.0 1.0 0.8 0.8 0.6 0.6 0.4 0.4 02 0.2 0.0 0.0 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 ®z_phe_length nae_phrase_length h(brid-based %$mpar&amp;s$n $n Wat#h BIEU WE !E E&amp;quot;at#h gwphem ph$neme h(brid 1.0 05 0.8 0.4 0.6 03 0.4 0.2 02 0.1 0.0 0.0 ����������������� Figure 2: The performances of the transliteration models and their comparison on EMatch. 1 2 3 4 5 6 7 � ����������������� � � � � � � � � 5.2 Experimental model selection Similar to (Jiang et al., 2007), the transliteration models were trained and tested on the LDC ChineseEnglish Named Entity Lists Version 1.08. The original list contains 572,213 English people names with Chinese transliterations. We extracted 74,725 entries in which the English names also appeared in the CMU pronunciation dictionary. We randomly selected 3,736 entries as an open testing set and the remaining entries as a training set9. The results were evaluated using the character/pinyin-based 4-gram BLEU score (Papineni et al., 2002), word error rate (WER), position independent word error rate (PER), and exact match (EMat</context>
<context position="22410" citStr="Jiang et al., 2007" startWordPosition="3506" endWordPosition="3509"> or right after mpl = 4. The pinyin-based WER of the hybrid model is 39.13%, comparable to the pinyin error rate 39.6%, reported in (Jiang et al., 2007)10. Thus, our further 8Linguistic Data Consortium catalog number: LDC2005T34 (former catalog number: LDC2003E01) 9Jiang et al. (2007) selected 25,718 personal name pairs from LDC2003E01 as the experiment data: 200 as development set, 200 as test set, and the remaining entries as training set. 10It should be notified that we achieved this result by using larger training set (70,989 vs. 25,718) and larger test set (3,736 vs. 200) comparing with (Jiang et al., 2007), and we did not use % 0t 1t 2t 3t 4t 5t Strategy 5 .3879 .3937 .3971 .3958 .3972 .3971 top1 em .3911 .3979 .3954 .3974 .3965 top1 am .4062 .4182 .4208 .4218 .4201 top5 em .3987 .4177 .4190 .4192 .4189 top5 am 10 .4092 .4282 .4258 .4202 .4203 .4205 top1 em .4121 .4190 .4180 .4174 .4200 top1 am .4305 .4386 .4399 .4438 .4403 top5 em .4289 .4263 .4292 .4291 .4288 top5 am 20 .4561 .4538 .4562 .4550 .4543 .4551 top1 em .4532 .4578 .4544 .4545 .4541 top1 am .4624 .4762 .4754 .4748 .4746 top5 em .4605 .4677 .4677 .4674 .4679 top5 am 40 .4779 .4791 .4793 .4799 .4794 .4808 top1 em .4774 .4794 .4779 .47</context>
<context position="24878" citStr="Jiang et al. (2007)" startWordPosition="3945" endWordPosition="3948">tance (ed) to measure the em and am similarities: ed(c, C&apos;) = 0 or &lt; syllable number(C&apos;)/2. (5) When applying Algorithm 1 for transliteration lexicon mining, we decode each word in el respectively. The algorithm terminated in five iterations when we set the terminal threshold 2 (Line 13 in Algorithm 1) to be 100. For simplicity, Table 3 only illustrates the BLEU score of h4 models under four selection strategies. From this table, we can draw the following conclusions. First, with fewer initial training data, the improvement is better. The best relative improvements additional Web resources as Jiang et al. (2007) did. 429 are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%, and 1.39%, respectively. Second, using top-5 and em for new training data selection performs the best among the four strategies. Compared under each iteration, using top-5 is better than using top-1; em is better than am; and top-5 with am is a little better than top-1 with em. We mined 39,424, 42,466, 46,116, 47,057, 49,551, 49,622, and 50,313 distinct entries under the six types of initial data with top-5 plus em strategy. The 50,313 entries are taken as the final transliteration lexicon for further comparison. 6 Self-Training for a Cas</context>
</contexts>
<marker>Jiang, Zhou, Chien, Niu, 2007</marker>
<rawString>Jiang, Long, Ming Zhou, Lee-Feng Chien, and Cheng Niu. 2007. Named Entity Translation with Web Mining and Transliteration. In IJCAI 2007. pages 1629– 1634, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL 2007 Poster Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="18421" citStr="Koehn et al., 2007" startWordPosition="2876" endWordPosition="2879">se pinyins). Similar models have been compared in (Oh et al., 2006) for English-to-Korean and Englishto-Japanese transliteration. All the three models are phrase-based, i.e., adjacent phonemes or graphemes are allowable to form phrase-level transliteration units. Building the correspondences on phrase level can effectively tackle the missing or redundant phoneme/grapheme problem during transliteration. For example, when Aamodt is transliterated into a m¯o t`e5, a and d are missing. The problem can be easily solved when taking Aa and dt as single units for transliterating. Making use of Moses (Koehn et al., 2007), a phrase-based SMT system, Matthews (2007) has shown that the performance was comparable to recent state-of-the-art work (Jiang et al., 2007) in English-to-Chinese personal name transliteration. Matthews (2007) took transliteration as translation at the surface level. Inspired by his idea, we also implemented our transliteration models employing Moses. The main difference is that, while Matthews (2007) tokenized the English names into individual letters before training in Moses, we split them into syllables using the heuristic rules described in (Jiang et al., 2007), such that one syllable o</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 2007 Poster Session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In SIGLEX</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2366" citStr="Koehn and Knight, 2002" startWordPosition="326" endWordPosition="329">s the continuous emergence of neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and te</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Koehn, Philipp and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In SIGLEX 2002, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Shea Kuo</author>
<author>Haizhou Li</author>
<author>Ying-Kuei Yang</author>
</authors>
<title>Learning Transliteration Lexicons from the Web. In COLING-ACL</title>
<date>2006</date>
<pages>1129--1136</pages>
<contexts>
<context position="7658" citStr="Kuo et al. (2006)" startWordPosition="1113" endWordPosition="1116">ual abbreviation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other large-scale corpora. Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information. Kuo et al. (2006) used active learning and unsupervised learning for mining transliteration lexicon from the Web pages, in which an EM process was used for estimating the phonetic similarities between English syllables and Chinese characters. Cao et al. (2007) split parenthetical translation mining task into two parts, transliteration detection and translation detection. They employed a transliteration lexicon for constructing a grapheme-based transliteration model and annotated boundaries manually to train a classifier. Lin et al. (2008) applied a frequency-based word alignment approach, Competitive Link (Mel</context>
</contexts>
<marker>Kuo, Li, Yang, 2006</marker>
<rawString>Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang. 2006. Learning Transliteration Lexicons from the Web. In COLING-ACL 2006. pages 1129–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Benjamin Van Durme</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Mining Parenthetical Translations from the Web by Word Alignment.</title>
<date>2008</date>
<booktitle>In ACL08:HLT,</booktitle>
<pages>994--1002</pages>
<location>Columbus, Ohio.</location>
<marker>Lin, Zhao, Van Durme, Pas¸ca, 2008</marker>
<rawString>Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, and Marius Pas¸ca. 2008. Mining Parenthetical Translations from the Web by Word Alignment. In ACL08:HLT, pages 994–1002, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Matthews</author>
</authors>
<title>Machine Transliteration of Proper Names. A Thesis of</title>
<date>2007</date>
<institution>Master. University of Edinburgh.</institution>
<contexts>
<context position="18465" citStr="Matthews (2007)" startWordPosition="2884" endWordPosition="2885">n (Oh et al., 2006) for English-to-Korean and Englishto-Japanese transliteration. All the three models are phrase-based, i.e., adjacent phonemes or graphemes are allowable to form phrase-level transliteration units. Building the correspondences on phrase level can effectively tackle the missing or redundant phoneme/grapheme problem during transliteration. For example, when Aamodt is transliterated into a m¯o t`e5, a and d are missing. The problem can be easily solved when taking Aa and dt as single units for transliterating. Making use of Moses (Koehn et al., 2007), a phrase-based SMT system, Matthews (2007) has shown that the performance was comparable to recent state-of-the-art work (Jiang et al., 2007) in English-to-Chinese personal name transliteration. Matthews (2007) took transliteration as translation at the surface level. Inspired by his idea, we also implemented our transliteration models employing Moses. The main difference is that, while Matthews (2007) tokenized the English names into individual letters before training in Moses, we split them into syllables using the heuristic rules described in (Jiang et al., 2007), such that one syllable only contains one vowel letter or a combinati</context>
</contexts>
<marker>Matthews, 2007</marker>
<rawString>Matthews, David. 2007. Machine Transliteration of Proper Names. A Thesis of Master. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>When is Self-Training Effective for Parsing?</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>561--568</pages>
<location>manchester, UK.</location>
<contexts>
<context position="8878" citStr="McClosky et al., 2008" startWordPosition="1281" endWordPosition="1285">elanmed, 2000), to determine the outer boundary (Section 7). On the other hand, there have been many semisupervised approaches in numerous applications 425 Chinese Web pages Parenthetical expression extraction{C(E)} S-MSRSeg Chinese word segmentation{c...(e...)} (Lin et al., 2008) Heuristic filtering{c...(e...)} Section 4 Bilingual abbreviation mining Section 5 Transliteration lexicon mining Section 6 Translation lexicon mining Figure 1: The system framework of mining lexicons from Chinese Web pages. (Zhu, 2007), such as self-training in word sense disambiguation (Yarowsky, 2005) and parsing (McClosky et al., 2008). In this paper, we apply selftraining to a new topic, lexicon mining. 3 System Framework and Self-Training Algorithm Figure 1 illustrates our system framework for mining lexicons from Chinese Web pages. First, parenthetical expressions matching Pattern 1 are extracted. Then, pre-parenthetical Chinese sequences are segmented into word sequences by S-MSRSeg2 (Gao et al., 2006). The initial parenthetical translation corpus is constructed by applying the heuristic rules defined in (Lin et al., 2008)3. Based on this corpus, we mine three lexicons step by step, a bilingual abbreviation lexicon, a t</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2008</marker>
<rawString>McClosky, David, Eugene Charniak, and Mark Johnson 2008. When is Self-Training Effective for Parsing? In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561– 568, manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of Translational Equivalence among Words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<marker>Melamed, 2000</marker>
<rawString>Melamed, I. Dan. 2000. Models of Translational Equivalence among Words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="26222" citStr="Och and Ney, 2003" startWordPosition="4156" endWordPosition="4159">(Lin et al., 2008), wherein the lengthes of prefixes and suffixes of English words were assumed to be three bytes, we segment words into morphemes (sequences of prefixes, stems, and suffixes) by Morfessor 0.9.211, an unsupervised language-independent morphological analyzer (Creutz and Lagus, 2007). We use the morpheme-level translation similarity explicitly in our cascaded translation model (Wu et al., 2008), which makes use of morpheme, word, and phrase level translation units. We train Moses to gain a phrase-level translation table. To gain a morpheme-level translation table, we run GIZA++ (Och and Ney, 2003) on both directions between English morphemes and Chinese characters, and take the intersection of Iiterbi alignments. The Englishto-Chinese translation probabilities computed by GIZA++ are attached to each morpheme-character element in the intersection set. 6.1 Experiment The Wanfang Chinese-English technical term dictionary12, which contains 525,259 entries in total, was used for training and testing. 10,000 entries were randomly selected as the test set and the remaining as the training set. Again, we investigated the scalability of the self-trained cascaded translation model by respectivel</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A Comparison of Different Machine Transliteration Models.</title>
<date>2006</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>27--119</pages>
<contexts>
<context position="17869" citStr="Oh et al., 2006" startWordPosition="2796" endWordPosition="2799">tion models. Then, we select and train the best model following Algorithm 1 for lexicon mining. We investigate two things, the scalability of the self-trained model given different amount of initial training data, and the performance of several strategies for selecting new training samples. 5.1 Model description We construct and compare three forward transliteration models, a phoneme-based model (English phonemes to Chinese pinyins), a grapheme-based model (English syllables to Chinese characters) and a hybrid model (English syllables to Chinese pinyins). Similar models have been compared in (Oh et al., 2006) for English-to-Korean and Englishto-Japanese transliteration. All the three models are phrase-based, i.e., adjacent phonemes or graphemes are allowable to form phrase-level transliteration units. Building the correspondences on phrase level can effectively tackle the missing or redundant phoneme/grapheme problem during transliteration. For example, when Aamodt is transliterated into a m¯o t`e5, a and d are missing. The problem can be easily solved when taking Aa and dt as single units for transliterating. Making use of Moses (Koehn et al., 2007), a phrase-based SMT system, Matthews (2007) has</context>
</contexts>
<marker>Oh, Choi, Isahara, 2006</marker>
<rawString>Oh, Jong-Hoon, Key-Sun Choi, and Hitoshi Isahara. 2006. A Comparison of Different Machine Transliteration Models. Journal of Artifical Intelligence Research, 27:119–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Building an Abbreviation Dictionary Using a Term Recognition Approach.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<volume>22</volume>
<issue>22</issue>
<marker>Okazaki, Ananiadou, 2006</marker>
<rawString>Okazaki, Naoaki and Sophia Ananiadou. 2006. Building an Abbreviation Dictionary Using a Term Recognition Approach. Bioinformatics, 22(22):3089–3095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<pages>311--318</pages>
<location>Philadelphia.</location>
<contexts>
<context position="21377" citStr="Papineni et al., 2002" startWordPosition="3335" endWordPosition="3338">2 3 4 5 6 7 � ����������������� � � � � � � � � 5.2 Experimental model selection Similar to (Jiang et al., 2007), the transliteration models were trained and tested on the LDC ChineseEnglish Named Entity Lists Version 1.08. The original list contains 572,213 English people names with Chinese transliterations. We extracted 74,725 entries in which the English names also appeared in the CMU pronunciation dictionary. We randomly selected 3,736 entries as an open testing set and the remaining entries as a training set9. The results were evaluated using the character/pinyin-based 4-gram BLEU score (Papineni et al., 2002), word error rate (WER), position independent word error rate (PER), and exact match (EMatch). Figure 2 reports the performances of the three models and the comparison based on EMatch. From the results, we can easily draw the conclusion that the hybrid model performs the best under the maximal phrase length (mpl, the maximal phrase length allowed in Moses) from 1 to 8. The performances of the models converge at or right after mpl = 4. The pinyin-based WER of the hybrid model is 39.13%, comparable to the pinyin error rate 39.6%, reported in (Jiang et al., 2007)10. Thus, our further 8Linguistic </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). pages 311–318, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Shao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Mining New Word Translations from Comparable Corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>618--624</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2772" citStr="Shao and Ng, 2004" startWordPosition="389" endWordPosition="392">on mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can be expressed by th</context>
<context position="7451" citStr="Shao and Ng (2004)" startWordPosition="1080" endWordPosition="1083">eme-level translation similarity. This paper is organized as follows. We briefly review the related work in the next section. Our system framework and self-training algorithm is described in Section 3. Bilingual abbreviation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other large-scale corpora. Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information. Kuo et al. (2006) used active learning and unsupervised learning for mining transliteration lexicon from the Web pages, in which an EM process was used for estimating the phonetic similarities between English syllables and Chinese characters. Cao et al. (2007) split parenthetical translation mining task into two parts, transliteration detection and translation detection. They employed a transliteration lexi</context>
</contexts>
<marker>Shao, Ng, 2004</marker>
<rawString>Shao, Li and Hwee Tou Ng. 2004. Mining New Word Translations from Comparable Corpora. In Proceedings of the 20th International Conference on Computational Linguistics (COLING), pages 618–624, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Naoaki Okazaki</author>
<author>Takashi Tsunakawa</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Improving English-to-Chinese Translation for Technical Terms Using Morphological Information.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>202--211</pages>
<location>Waikiki, Hawai’i.</location>
<contexts>
<context position="6796" citStr="Wu et al., 2008" startWordPosition="979" endWordPosition="982">methods have been proposed. However, supervised approaches are restricted by the quality and quantity of manually constructed training data, and unsupervised approaches are totally frequency-based without using any semantic clues. In contrast, we propose a semi-supervised framework for mining parenthetical translations. We apply a monolingual abbreviation extraction approach to bilingual abbreviation extraction. We construct an English-syllable to Chinese-pinyin transliteration model which is selftrained using phonemic similarity measurements. We further employ our cascaded translation model (Wu et al., 2008) which is self-trained based on morpheme-level translation similarity. This paper is organized as follows. We briefly review the related work in the next section. Our system framework and self-training algorithm is described in Section 3. Bilingual abbreviation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web</context>
<context position="26015" citStr="Wu et al., 2008" startWordPosition="4123" endWordPosition="4126">final transliteration lexicon for further comparison. 6 Self-Training for a Cascaded Translation Model We classify the parenthetical translation candidates by employing a translation model. In contrast to (Lin et al., 2008), wherein the lengthes of prefixes and suffixes of English words were assumed to be three bytes, we segment words into morphemes (sequences of prefixes, stems, and suffixes) by Morfessor 0.9.211, an unsupervised language-independent morphological analyzer (Creutz and Lagus, 2007). We use the morpheme-level translation similarity explicitly in our cascaded translation model (Wu et al., 2008), which makes use of morpheme, word, and phrase level translation units. We train Moses to gain a phrase-level translation table. To gain a morpheme-level translation table, we run GIZA++ (Och and Ney, 2003) on both directions between English morphemes and Chinese characters, and take the intersection of Iiterbi alignments. The Englishto-Chinese translation probabilities computed by GIZA++ are attached to each morpheme-character element in the intersection set. 6.1 Experiment The Wanfang Chinese-English technical term dictionary12, which contains 525,259 entries in total, was used for training</context>
</contexts>
<marker>Wu, Okazaki, Tsunakawa, Tsujii, 2008</marker>
<rawString>Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, and Jun’ichi Tsujii. 2008. Improving English-to-Chinese Translation for Technical Terms Using Morphological Information. In Proceedings of the 8th Conference of the Association for Machine Translation in the Americas (AMTA), pages 202–211, Waikiki, Hawai’i.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts.</location>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189–196, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-Supervised Learning Literature Survery.</title>
<date>2007</date>
<institution>University of Wisconsin - Madison.</institution>
<contexts>
<context position="8773" citStr="Zhu, 2007" startWordPosition="1268" endWordPosition="1269">ier. Lin et al. (2008) applied a frequency-based word alignment approach, Competitive Link (Melanmed, 2000), to determine the outer boundary (Section 7). On the other hand, there have been many semisupervised approaches in numerous applications 425 Chinese Web pages Parenthetical expression extraction{C(E)} S-MSRSeg Chinese word segmentation{c...(e...)} (Lin et al., 2008) Heuristic filtering{c...(e...)} Section 4 Bilingual abbreviation mining Section 5 Transliteration lexicon mining Section 6 Translation lexicon mining Figure 1: The system framework of mining lexicons from Chinese Web pages. (Zhu, 2007), such as self-training in word sense disambiguation (Yarowsky, 2005) and parsing (McClosky et al., 2008). In this paper, we apply selftraining to a new topic, lexicon mining. 3 System Framework and Self-Training Algorithm Figure 1 illustrates our system framework for mining lexicons from Chinese Web pages. First, parenthetical expressions matching Pattern 1 are extracted. Then, pre-parenthetical Chinese sequences are segmented into word sequences by S-MSRSeg2 (Gao et al., 2006). The initial parenthetical translation corpus is constructed by applying the heuristic rules defined in (Lin et al.,</context>
</contexts>
<marker>Zhu, 2007</marker>
<rawString>Zhu, Xiaojin. 2007. Semi-Supervised Learning Literature Survery. University of Wisconsin - Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>