<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003188">
<title confidence="0.958076">
TATO: Leveraging on Multiple Strategies for Semantic Textual Similarity
</title>
<author confidence="0.973188">
Tu Thanh Vu†, Quan Hung Tran††, Son Bao Pham††University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam
</author>
<affiliation confidence="0.987785">
††Japan Advanced Institute of Science and Technology, Japan
</affiliation>
<email confidence="0.948392">
† {tuvt,sonpb}@vnu.edu.vn
†† quanth@jaist.ac.jp
</email>
<sectionHeader confidence="0.998328" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836944444444">
In this paper, we describe the TATO sys-
tem which participated in the SemEval-2015
Task 2a: “Semantic Textual Similarity (STS)
for English”. Our system is trained on pub-
lished datasets from the previous competi-
tions. Based on some machine learning tech-
niques, it combines multiple similarity mea-
sures of varying complexity ranging from sim-
ple lexical and syntactic similarity measures
to complex semantic similarity ones to com-
pute semantic textual similarity. Our final
model consists of a simple linear combination
of about 30 main features out of a numerous
number of features experimented. The results
are promising, with Pearson’s coefficients on
each individual dataset ranging from 0.6796 to
0.8167 and an overall weighted mean score of
0.7422, well above the task baseline system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999716183673469">
Measuring semantic textual similarity (STS) can be
defined as the task of computing the degree of se-
mantic equivalence between pairs of texts. It has
drawn an increasing amount of attention from the
NLP community, especially at level of short text
fragments, as partly reflected in the SemEval tasks
in recent years. In the SemEval-2015 Task 2, the de-
gree of semantic equivalence for each sentence pair
is represented by a similarity score between 0 (no
relation) and 5 (semantic equivalence). STS has a
wide range of applications which includes applica-
tions for machine translation evaluation, information
extraction, question answering, and summarization.
STS is related to, but different from textual en-
tailment (TE) (Dagan et al., 2006) and paraphrase
recognition (PARA) (Dolan et al., 2004) as it aims
to render a graded notion of semantic equivalence
between two textual snippets, rather than a binary
yes/no decision. STS requires a bidirectional sim-
ilarity relation between sentences, while TE anno-
tates them with an unidirectional entailment relation.
The literature of STS is rife with attempts to
compute similarity between texts using a multitude
of measures at different levels of depth: lexical
(Malakasiotis and Androutsopoulos, 2007), syntac-
tic (Malakasiotis, 2009; Zanzotto et al., 2009), and
semantic (Rinaldi et al., 2003; Bos and Markert,
2005). (Gomaa and Fahmy, 2013) discusses exist-
ing works on STS and partitions them into three cat-
egories based on the similarity measures used: (i)
string-based approaches (B¨ar et al., 2012; Malakasi-
otis and Androutsopoulos, 2007) which operate on
string sequences and character composition to com-
pute similarities and can be categorized into two
groups: character-based and term-based approaches;
(ii) corpus-based approaches (Li et al., 2006) which
gain statistics information about words from large
corpora and reflect their semantics in distributional
high semantic space to determine the similarity, such
as Latent Semantic Analysis (LSA) (Landauer et
al., 1998; Foltz et al., 1998) and Explicit Seman-
tic Analysis (ESA) (Gabrilovich and Markovitch,
2007); (iii) knowledge-based approaches (Mihalcea
et al., 2006) which determine the degree of similar-
ity between texts using information derived from se-
mantic networks, such as WordNet (Miller, 1995).
Though each of these existing measures has its
own advantages, they are typically used in separa-
tion. In our work, we integrate multiple similarity
</bodyText>
<page confidence="0.963535">
190
</page>
<bodyText confidence="0.902089166666666">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 190–195,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
measures of varying complexity ranging from sim-
ple lexical and syntactic similarity measures to com-
plex semantic similarity ones and rely on supervised
machine learning to take advantage of the different
contributions of different features.
We organize the remainder of the paper as fol-
lows: Section 2 describes the features in detail. Sec-
tion 3 presents the machine learning setup and our
submitted system. Sections 4 discusses the results.
The conclusions follow in the final section.
</bodyText>
<sectionHeader confidence="0.897838" genericHeader="method">
2 Text Similarity Measures
</sectionHeader>
<bodyText confidence="0.999487">
In this section, we describe the various features we
experimented and selected for our final model.
</bodyText>
<subsectionHeader confidence="0.987556">
2.1 Lexical Similarity Measures
2.1.1 Word/Phrase Alignment Measures
</subsectionHeader>
<bodyText confidence="0.999992909090909">
When two sentences are related semantically,
they tend to be similar in appearance. Hence,
we develop an automatic word/phrase alignment
module based on the METEOR metric (Denkowski
and Lavie, 2010) to align corresponding words and
phrases between each pair of sentences. Alignments
here are based on exact, stem, synonym (via Word-
Net), and paraphrase (via a lookup table) matches
between words and phrases. Given two sentences of
text, s1 and s2 (stop-words are removed from each
sentence), we define the following metrics:
</bodyText>
<equation confidence="0.939738857142857">
��
numOfMatches(sl,s2) — Min{len(s2),/en(s2)}
Y(81,82) = I
and
D(s1, s2) =2×numOf(s)minJ n�n((2)}2)
1
,
</equation>
<bodyText confidence="0.999494">
where numOfMatches(s1, s2) and len(s) are the
number of aligned word/phrase pairs between s1 and
s2, and the number of words in s, respectively.
</bodyText>
<subsectionHeader confidence="0.926337">
2.1.2 Machine Translation Measures
</subsectionHeader>
<bodyText confidence="0.999750821428571">
We treat the task as a monolingual machine trans-
lation (MT) task (the source and target languages are
the same, and the input and output should be simi-
lar in meaning), and take advantage of a variety of
MT measures. At the lexical level, we experiment
different n-gram and edit-distance-based metrics.
BLEU (Papineni et al., 2002), NIST (Doddington,
2002), and METEOR (Denkowski and Lavie, 2010)
are n-gram-based metrics commonly used for MT
evaluation. BLEU scores the target output by count-
ing n-gram matches with the reference, relying on
exact matching and has no concept of synonymy or
paraphrasing. NIST is similar to BLEU, however, it
uses the arithmetic mean of n-gram overlaps, rather
than the geometric mean. Unlike BLEU which fo-
cuses on precision, METEOR uses a combination of
both precision and recall. Moreover, it incorporates
stemming, synonymy and paraphrase. MAXSIM
(Chan and Ng, 2008) models the MT problem as
a maximum bipartite matching one and maps each
word in one sentence to at most one word in the other
sentence. We also experiment with TESLA (Liu et
al., 2010) - a variant of MAXSIM.
Besides those, we also use edit-distance-based
metrics. TER (Snover et al., 2006) and TERp
(Snover et al., 2009) measure the number of edit
operations (e.g. insertions, deletions, and substitu-
tions) necessary to transform one text into the other.
</bodyText>
<subsectionHeader confidence="0.976279">
2.2 Syntactic Similarity Measures
2.2.1 Content Word Match and Mismatch
</subsectionHeader>
<bodyText confidence="0.999994333333333">
Given a sentence pair, we extract corresponding
content words (nouns, verbs, adjectives, and ad-
verbs) between the sentences. This syntactic infor-
mation is obtained from the Stanford parser (Klein
and Manning, 2003). We have both the proportions
of aligned words and the proportions of unaligned
words in the two sentences (by normalizing with the
harmonic mean of their number of content words)
for each lexical category of content word.
</bodyText>
<subsubsectionHeader confidence="0.926592">
2.2.2 Subject-Verb-Object Comparison
</subsubsectionHeader>
<bodyText confidence="0.999986333333333">
We also employ dependency parsing in measur-
ing semantic similarity. Specifically, some attributes
like subjects, verbs, objects are identified for each
pair of sentences. These attributes are used for our
matching procedure which is based on the following
comparisons between each pair of sentences:
</bodyText>
<listItem confidence="0.999976166666667">
• Subject-Subject Comparison
• Verb-Verb Comparison
• Object-Object Comparison
• Subject-Verb Comparison
• Verb-Object Comparison
• Cross Subject-Object Comparison
</listItem>
<bodyText confidence="0.952275">
For each of these comparisons, we assign a matching
score of 1.0 (match) or 0.0 (mismatch).
</bodyText>
<page confidence="0.992021">
191
</page>
<subsectionHeader confidence="0.944768333333333">
2.3 Semantic Similarity Measures
2.3.1 Named Entity, Number, Time Expression
Match and Mismatch
</subsectionHeader>
<bodyText confidence="0.9998079">
Careful observation of the development dataset
revealed that mismatch of named entities, numbers
or time expressions might cause semantic dissimi-
larity, for example, when s1 consists of a named en-
tity that does not appear in s2. Based on this, we
detect both match and mismatch of named entities,
numbers and time expressions between each pair of
sentences (similar to that of content words). We use
the Stanford Named Entity Recognizer (Finkel et al.,
2005) to detect named entities in sentences.
</bodyText>
<subsubsectionHeader confidence="0.592954">
2.3.2 LDA-based measures
</subsubsectionHeader>
<bodyText confidence="0.999915111111111">
We build two Latent Dirichilet Allocation (LDA)
models (Blei et al., 2003) from Wikipedia and
the training dataset separately, using the Gensim
(ˇReh˚uˇrek and Sojka, 2010) and Mallet (McCallum,
2002) software with 100 requested latent topics.
Each sentence is represented by a vector using topics
estimated by LDA. The similarity between two sen-
tences is calculated as the cosine similarity between
their corresponding vectors.
</bodyText>
<subsubsectionHeader confidence="0.507131">
2.3.3 Word-representation-based measures
</subsubsectionHeader>
<bodyText confidence="0.9904498">
Word representation computes vector representa-
tions of each word based on its context from very
large datasets, usually capturing both syntactic and
semantic information of words. Given two sentences
s1 and s2 (stop-words are removed), each word of
the sentences is represented as a single vector. We
develop two different strategies as follows:
Strategy 1 For each word wi in s1, we iden-
tify a word wj most similar to wi in s2 by using
cosine similarity measure. We define a measure
</bodyText>
<equation confidence="0.8892715">
01 21Y (s1, s2) as follows:
cos(wi, wj)
,
len(s1)
</equation>
<bodyText confidence="0.999894181818182">
where cos(wi, wj) is the cosine similarity between
the word vectors of wi and wj. We also apply this
strategy for each category of content words (noun,
verb, adjective, and adverb) separately.
Strategy 2 We sum up all of the vectors of words
that occur in each sentence and define a sentence
similarity measure Y21Y (s1, s2) as follows:
For word representation, we use both the Word2vec
model (Mikolov et al., 2013) trained on Google
News and the GloVe model (Pennington et al., 2014)
trained on Common Crawl data.
</bodyText>
<subsubsectionHeader confidence="0.457146">
2.3.4 WordNet-based measures
</subsubsectionHeader>
<bodyText confidence="0.999982176470589">
WordNet (Miller, 1995) is a commonly used lex-
ical database of English where words of the same
meaning are grouped into synonym sets (synsets).
By using information derived from WordNet, we
construct some similarity measures as follows:
Strategy 1 This is similar to Strategy 1 for word-
representation-based measures, however, instead of
using cosine similarity, we use the Wordnet path
similarity (the shortest path that connects the senses
in the is-a (hypernym/hypnoym) taxonomy).
Strategy 2 We determine some semantic relation-
ships, e.g, synonym, antonym, and hypernym be-
tween sentences. The proportions of synonym word
pairs, antonym word pairs, hypernym word pairs in
two sentences (by normalizing with the harmonic
mean of their number of content words) are taken
as proxies of their semantic similarity.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="method">
3 System Description
</sectionHeader>
<subsectionHeader confidence="0.99955">
3.1 Machine Learning Setup
</subsectionHeader>
<bodyText confidence="0.999916777777778">
The machine learning setup is described as follows:
Pre-processing The pre-processing phase in-
cludes tokenization, POS tagging, lemmatization,
NER, syntactic parsing with the Stanford CoreNLP
Toolkit (Manning et al., 2014). For some measures,
we filter out punctuations and stop-words by using a
pre-compiled stop-words list.
Feature Generation We run each of the similar-
ity measures separately and use the resulting scores
as features for a machine learning classifier. A fea-
ture is selected for our final model if it proves useful
in improving the performance of the system.
Feature Combination The pre-computed simi-
larity score vectors serve as features for this step.
Our system utilizes a classifier combination ap-
proach, using a simple linear regression model to
combine all the similarity measures. We use the
trial dataset that comprises the 2012, 2013 and 2014
</bodyText>
<equation confidence="0.997817571428572">
�Y21Y (s1, s2) = cos( �wi, wj),
wiEs1 wjEs2
01 21Y (s1, s2) =
E
wiEs1
max
wjEs2
</equation>
<page confidence="0.992324">
192
</page>
<bodyText confidence="0.999955">
datasets to develop and train our model. In the devel-
opment cycle, we used a training dataset consisting
of 6842 sentence pairs and a test dataset consisting
of 3750 sentence pairs, with gold standard scores.
We use the WEKA machine learning toolkit (Hall et
al., 2009) to perform our experiments.
Post-processing If the pre-processed sentences
match, we set their similarity score to 5 regardless
of the output of our classifier. If the classifier out-
puts an invalid similarity score s which is not in the
score range [0-5], we set the similarity score to f(s)
</bodyText>
<equation confidence="0.943143">
� 0 + α if s &lt; 0
defined as follows: f(s) =
5 − α if s &gt; 5
</equation>
<bodyText confidence="0.882521">
In our experiments, the best value for α is 0.5.
</bodyText>
<subsectionHeader confidence="0.998822">
3.2 Submitted System
</subsectionHeader>
<bodyText confidence="0.999948777777778">
TATO-1stWTW Because of our limited time, we
submitted only one run to the SemEval-2015 Task
2a. After the development cycle, we identified about
30 main features out of a numerous number of fea-
tures experimented. These features achieved the best
performance on the training dataset. For our final
system, we trained the classifier on a joint dataset
of all known training datasets, instead of training a
separate classifier for each individual dataset.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.994351">
4.1 Results on the 2014 Test Data
</subsectionHeader>
<bodyText confidence="0.99999775">
We evaluated our model on the 2014 test data
comprising pairs of news headlines (headlines),
pairs of glosses (OnWN), image descriptions (im-
ages), DEFT-related discussion forums (deft-forum)
and news (deft-news), and tweet comments and
newswire headline mappings (tweet-news). We used
the 2012, 2013 datasets consisting of 6842 sentence
pairs to train our model. The test dataset contains
3750 sentence pairs excluded from training. Our
model was compared against the best performing
system on the SemEval-2014 English STS sub-task
(DLS@CU-run2) using the official scorer. The re-
sults are summarized in Table 1. With regard to
Deft-forum and Tweets, our system outperformed
the DLS@CU’s system, we also achieved a higher
score in the weighted mean across all datasets.
</bodyText>
<subsectionHeader confidence="0.935746">
4.2 Results on the 2015 Test Data
</subsectionHeader>
<bodyText confidence="0.9961305">
The official score is based on the average of Pearson
correlation. Besides Pearson correlations computed
</bodyText>
<table confidence="0.92137">
Run DF DN H I OWN TN Mean
TATO1 .550 .748 .755 .807 .817 .777 .764
DLS@CU2 .483 .766 .765 .821 .859 .764 .761
</table>
<tableCaption confidence="0.984548333333333">
Table 1: Results on the 2014 test datasets: deft-forum
(DF), deft-news (DN), headlines (H), images (I), OnWN
(OWN), tweet-news (TN).
</tableCaption>
<bodyText confidence="0.998864307692308">
for individual datasets, including answers-forums,
answers-students, belief, headlines, and images,
Mean scores are provided to show the weighted
means across all datasets (the weight is based on the
number of sentence pairs in each dataset).
Table 2 reports our official results achieved on
the test data (TATO-1stWTW), besides the highest-
performance and lowest-performance systems (ac-
cording to Mean), and also the task baseline system.
Our system was ranked among the most robust sys-
tems out of more than 70 participating systems and
achieved good performance on answers-forums and
belief datasets.
</bodyText>
<table confidence="0.999209125">
# Run AF AS B H I Mean
1 DLS@CU1 .739 .773 .749 .825 .864 .802
... ... ... ... ... ... ... ...
25 TATO1 .680 .685 .721 .767 .817 .742
... ... ... ... ... ... ... ...
59 baseline1 .445 .665 .652 .531 .604 .587
... ... ... ... ... ... ... ...
73 DalGTM1 .290 -.053 .063 .060 .066 .062
</table>
<tableCaption confidence="0.923112666666667">
Table 2: Official results on the test datasets: answers-
forums (AF), answers-students (AS), belief (B), head-
lines (H), and images (I).
</tableCaption>
<sectionHeader confidence="0.990104" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999974714285714">
This paper describes the TATO team’s submission to
the SemEval-2015 Task 2a: “Semantic Textual Sim-
ilarity for English”. Our system uses a simple linear
regression model to combine multiple text similarity
measures at different levels of depth: lexical, syn-
tactic, and semantic. While we did not achieve the
highest ranks on any of the particular datasets, our
system was ranked among the most robust systems
out of more than 70 participating systems.
For the future work, we will explore other eval-
uation measures for STS and try to train a sepa-
rate classifier for each type of the existing datasets.
We also suggest that we should work on some other
types of data, such as legal or medical data.
</bodyText>
<page confidence="0.999206">
193
</page>
<sectionHeader confidence="0.985051" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999663641509434">
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the First Joint Confer-
ence on Lexical and Computational Semantics, pages
435–440.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Johan Bos and Katja Markert. 2005. Recognising Tex-
tual Entailment with Logical Inference. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 628–635.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
Maximum Similarity Metric for Machine Translation
Evaluation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 55–62.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. In Machine Learning Challenges, Lecture
Notes in Computer Science, pages 177–190.
Michael Denkowski and Alon Lavie. 2010. Extending
the METEOR Machine Translation Evaluation Metric
to the Phrase Level. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 250–253.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138–145.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised Construction of Large Paraphrase Cor-
pora: Exploiting Massively Parallel News Sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 350–356.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
363–370.
P. Foltz, W. Kintsch, and T. Landauer. 1998. The Mea-
surement of Textual Coherence with Latent Seman-
tic Analysis. In Journal of the Discourse Processes,
25(2&amp;3):285–307.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artifical Intel-
ligence, pages 1606–1611.
Wael H. Gomaa and Aly A. Fahmy. 2013. Article: A
Survey of Text Similarity Approaches. International
Journal of Computer Applications, 68(13):13–18.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10–18.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423–430.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An
Introduction to Latent Semantic Analysis. In Journal
of the Discourse Processes, 25:259–284.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 18(8):1138–1150.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation Evaluation of Sentences with
Linear-programming-based Analysis. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 354–359.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning Textual Entailment Using SVMs and
String Similarity measures. In Proceedings of of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42–47.
Prodromos Malakasiotis. 2009. Paraphrase Recognition
Using Machine Learning to Combine Similarity mea-
sures. In Proceedings of the 47th Annual Meeting of
ACL and the 4th Int. Joint Conference on Natural Lan-
guage Processing of AFNLP, pages 27–35.
Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language process-
ing toolkit. In Proceedings of 52nd Annual Meeting of
the Association for Computational Linguistics: System
Demonstrations, pages 55–60.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775–780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient Estimation of Word
Representations in Vector Space. arXiv preprint
arXiv:1310.4546.
</reference>
<page confidence="0.986734">
194
</page>
<reference confidence="0.999923714285714">
George A. Miller. 1995. Wordnet: A Lexical Database
for English. Communications of the Association for
Computing Machinery, 38(11):39–41.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word Rep-
resentation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing,
pages 1532–1543.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50.
Fabio Rinaldi, James Dowdall, Kaarel Kaljurand,
Michael Hess, and Diego Moll´a. 2003. Exploiting
Paraphrases in a Question Answering System. In Pro-
ceedings of the 2nd International Workshop on Para-
phrasing, pages 25–32.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase, Se-
mantic, and Alignment Enhancements to Translation
Edit Rate. Machine Translation, 23(2-3):117–127.
Fabio massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A Machine Learning
Approach to Textual Entailment Recognition. Natural
Language Engineering, 15(4):551–582.
</reference>
<page confidence="0.998942">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948962">
<title confidence="0.995019">TATO: Leveraging on Multiple Strategies for Semantic Textual Similarity</title>
<author confidence="0.979171">Thanh Quan Hung Son Bao of Engineering</author>
<author confidence="0.979171">Vietnam National University Technology</author>
<author confidence="0.979171">Hanoi</author>
<affiliation confidence="0.992829">Advanced Institute of Science and Technology,</affiliation>
<abstract confidence="0.998245052631579">In this paper, we describe the TATO system which participated in the SemEval-2015 Task 2a: “Semantic Textual Similarity (STS) for English”. Our system is trained on published datasets from the previous competitions. Based on some machine learning techniques, it combines multiple similarity measures of varying complexity ranging from simple lexical and syntactic similarity measures to complex semantic similarity ones to compute semantic textual similarity. Our final model consists of a simple linear combination of about 30 main features out of a numerous number of features experimented. The results are promising, with Pearson’s coefficients on each individual dataset ranging from 0.6796 to 0.8167 and an overall weighted mean score of 0.7422, well above the task baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8424" citStr="Blei et al., 2003" startWordPosition="1281" endWordPosition="1284">n Match and Mismatch Careful observation of the development dataset revealed that mismatch of named entities, numbers or time expressions might cause semantic dissimilarity, for example, when s1 consists of a named entity that does not appear in s2. Based on this, we detect both match and mismatch of named entities, numbers and time expressions between each pair of sentences (similar to that of content words). We use the Stanford Named Entity Recognizer (Finkel et al., 2005) to detect named entities in sentences. 2.3.2 LDA-based measures We build two Latent Dirichilet Allocation (LDA) models (Blei et al., 2003) from Wikipedia and the training dataset separately, using the Gensim (ˇReh˚uˇrek and Sojka, 2010) and Mallet (McCallum, 2002) software with 100 requested latent topics. Each sentence is represented by a vector using topics estimated by LDA. The similarity between two sentences is calculated as the cosine similarity between their corresponding vectors. 2.3.3 Word-representation-based measures Word representation computes vector representations of each word based on its context from very large datasets, usually capturing both syntactic and semantic information of words. Given two sentences s1 a</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising Textual Entailment with Logical Inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>628--635</pages>
<contexts>
<context position="2488" citStr="Bos and Markert, 2005" startWordPosition="372" endWordPosition="375">nd paraphrase recognition (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). (Gomaa and Fahmy, 2013) discusses existing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity,</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising Textual Entailment with Logical Inference. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 628–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>MAXSIM: A Maximum Similarity Metric for Machine Translation Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="6168" citStr="Chan and Ng, 2008" startWordPosition="932" endWordPosition="935">ce-based metrics. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2010) are n-gram-based metrics commonly used for MT evaluation. BLEU scores the target output by counting n-gram matches with the reference, relying on exact matching and has no concept of synonymy or paraphrasing. NIST is similar to BLEU, however, it uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. Unlike BLEU which focuses on precision, METEOR uses a combination of both precision and recall. Moreover, it incorporates stemming, synonymy and paraphrase. MAXSIM (Chan and Ng, 2008) models the MT problem as a maximum bipartite matching one and maps each word in one sentence to at most one word in the other sentence. We also experiment with TESLA (Liu et al., 2010) - a variant of MAXSIM. Besides those, we also use edit-distance-based metrics. TER (Snover et al., 2006) and TERp (Snover et al., 2009) measure the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text into the other. 2.2 Syntactic Similarity Measures 2.2.1 Content Word Match and Mismatch Given a sentence pair, we extract corresponding content words (nouns, ve</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A Maximum Similarity Metric for Machine Translation Evaluation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges, Lecture Notes in Computer Science,</booktitle>
<pages>177--190</pages>
<contexts>
<context position="1864" citStr="Dagan et al., 2006" startWordPosition="278" endWordPosition="281">tween pairs of texts. It has drawn an increasing amount of attention from the NLP community, especially at level of short text fragments, as partly reflected in the SemEval tasks in recent years. In the SemEval-2015 Task 2, the degree of semantic equivalence for each sentence pair is represented by a similarity score between 0 (no relation) and 5 (semantic equivalence). STS has a wide range of applications which includes applications for machine translation evaluation, information extraction, question answering, and summarization. STS is related to, but different from textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges, Lecture Notes in Computer Science, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>250--253</pages>
<contexts>
<context position="4655" citStr="Denkowski and Lavie, 2010" startWordPosition="693" endWordPosition="696">e remainder of the paper as follows: Section 2 describes the features in detail. Section 3 presents the machine learning setup and our submitted system. Sections 4 discusses the results. The conclusions follow in the final section. 2 Text Similarity Measures In this section, we describe the various features we experimented and selected for our final model. 2.1 Lexical Similarity Measures 2.1.1 Word/Phrase Alignment Measures When two sentences are related semantically, they tend to be similar in appearance. Hence, we develop an automatic word/phrase alignment module based on the METEOR metric (Denkowski and Lavie, 2010) to align corresponding words and phrases between each pair of sentences. Alignments here are based on exact, stem, synonym (via WordNet), and paraphrase (via a lookup table) matches between words and phrases. Given two sentences of text, s1 and s2 (stop-words are removed from each sentence), we define the following metrics: �� numOfMatches(sl,s2) — Min{len(s2),/en(s2)} Y(81,82) = I and D(s1, s2) =2×numOf(s)minJ n�n((2)}2) 1 , where numOfMatches(s1, s2) and len(s) are the number of aligned word/phrase pairs between s1 and s2, and the number of words in s, respectively. 2.1.2 Machine Translatio</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 250–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="5621" citStr="Doddington, 2002" startWordPosition="848" endWordPosition="849"> — Min{len(s2),/en(s2)} Y(81,82) = I and D(s1, s2) =2×numOf(s)minJ n�n((2)}2) 1 , where numOfMatches(s1, s2) and len(s) are the number of aligned word/phrase pairs between s1 and s2, and the number of words in s, respectively. 2.1.2 Machine Translation Measures We treat the task as a monolingual machine translation (MT) task (the source and target languages are the same, and the input and output should be similar in meaning), and take advantage of a variety of MT measures. At the lexical level, we experiment different n-gram and edit-distance-based metrics. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2010) are n-gram-based metrics commonly used for MT evaluation. BLEU scores the target output by counting n-gram matches with the reference, relying on exact matching and has no concept of synonymy or paraphrasing. NIST is similar to BLEU, however, it uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. Unlike BLEU which focuses on precision, METEOR uses a combination of both precision and recall. Moreover, it incorporates stemming, synonymy and paraphrase. MAXSIM (Chan and Ng, 2008) models the MT problem as a maximum bipartite matchin</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics. In Proceedings of the Second International Conference on Human Language Technology Research, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<contexts>
<context position="1919" citStr="Dolan et al., 2004" startWordPosition="286" endWordPosition="289"> of attention from the NLP community, especially at level of short text fragments, as partly reflected in the SemEval tasks in recent years. In the SemEval-2015 Task 2, the degree of semantic equivalence for each sentence pair is represented by a similarity score between 0 (no relation) and 5 (semantic equivalence). STS has a wide range of applications which includes applications for machine translation evaluation, information extraction, question answering, and summarization. STS is related to, but different from textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). (Gomaa and Fahmy, 2013) discu</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of the 20th International Conference on Computational Linguistics, pages 350–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="8285" citStr="Finkel et al., 2005" startWordPosition="1260" endWordPosition="1263"> we assign a matching score of 1.0 (match) or 0.0 (mismatch). 191 2.3 Semantic Similarity Measures 2.3.1 Named Entity, Number, Time Expression Match and Mismatch Careful observation of the development dataset revealed that mismatch of named entities, numbers or time expressions might cause semantic dissimilarity, for example, when s1 consists of a named entity that does not appear in s2. Based on this, we detect both match and mismatch of named entities, numbers and time expressions between each pair of sentences (similar to that of content words). We use the Stanford Named Entity Recognizer (Finkel et al., 2005) to detect named entities in sentences. 2.3.2 LDA-based measures We build two Latent Dirichilet Allocation (LDA) models (Blei et al., 2003) from Wikipedia and the training dataset separately, using the Gensim (ˇReh˚uˇrek and Sojka, 2010) and Mallet (McCallum, 2002) software with 100 requested latent topics. Each sentence is represented by a vector using topics estimated by LDA. The similarity between two sentences is calculated as the cosine similarity between their corresponding vectors. 2.3.3 Word-representation-based measures Word representation computes vector representations of each word </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Foltz</author>
<author>W Kintsch</author>
<author>T Landauer</author>
</authors>
<title>The Measurement of Textual Coherence with Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>In Journal of the Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="3171" citStr="Foltz et al., 1998" startWordPosition="473" endWordPosition="476">rtitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic Analysis (LSA) (Landauer et al., 1998; Foltz et al., 1998) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007); (iii) knowledge-based approaches (Mihalcea et al., 2006) which determine the degree of similarity between texts using information derived from semantic networks, such as WordNet (Miller, 1995). Though each of these existing measures has its own advantages, they are typically used in separation. In our work, we integrate multiple similarity 190 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 190–195, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguisti</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>P. Foltz, W. Kintsch, and T. Landauer. 1998. The Measurement of Textual Coherence with Latent Semantic Analysis. In Journal of the Discourse Processes, 25(2&amp;3):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical Intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="3243" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="483" endWordPosition="486">y measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic Analysis (LSA) (Landauer et al., 1998; Foltz et al., 1998) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007); (iii) knowledge-based approaches (Mihalcea et al., 2006) which determine the degree of similarity between texts using information derived from semantic networks, such as WordNet (Miller, 1995). Though each of these existing measures has its own advantages, they are typically used in separation. In our work, we integrate multiple similarity 190 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 190–195, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics measures of varying complexity ranging from simple lexical and syntac</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wael H Gomaa</author>
<author>Aly A Fahmy</author>
</authors>
<title>Article: A Survey of Text Similarity Approaches.</title>
<date>2013</date>
<journal>International Journal of Computer Applications,</journal>
<volume>68</volume>
<issue>13</issue>
<contexts>
<context position="2513" citStr="Gomaa and Fahmy, 2013" startWordPosition="376" endWordPosition="379"> (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). (Gomaa and Fahmy, 2013) discusses existing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic </context>
</contexts>
<marker>Gomaa, Fahmy, 2013</marker>
<rawString>Wael H. Gomaa and Aly A. Fahmy. 2013. Article: A Survey of Text Similarity Approaches. International Journal of Computer Applications, 68(13):13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11979" citStr="Hall et al., 2009" startWordPosition="1850" endWordPosition="1853">computed similarity score vectors serve as features for this step. Our system utilizes a classifier combination approach, using a simple linear regression model to combine all the similarity measures. We use the trial dataset that comprises the 2012, 2013 and 2014 �Y21Y (s1, s2) = cos( �wi, wj), wiEs1 wjEs2 01 21Y (s1, s2) = E wiEs1 max wjEs2 192 datasets to develop and train our model. In the development cycle, we used a training dataset consisting of 6842 sentence pairs and a test dataset consisting of 3750 sentence pairs, with gold standard scores. We use the WEKA machine learning toolkit (Hall et al., 2009) to perform our experiments. Post-processing If the pre-processed sentences match, we set their similarity score to 5 regardless of the output of our classifier. If the classifier outputs an invalid similarity score s which is not in the score range [0, 1, 2, 3, 4, 5], we set the similarity score to f(s) � 0 + α if s &lt; 0 defined as follows: f(s) = 5 − α if s &gt; 5 In our experiments, the best value for α is 0.5. 3.2 Submitted System TATO-1stWTW Because of our limited time, we submitted only one run to the SemEval-2015 Task 2a. After the development cycle, we identified about 30 main features out</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="6910" citStr="Klein and Manning, 2003" startWordPosition="1052" endWordPosition="1055">he other sentence. We also experiment with TESLA (Liu et al., 2010) - a variant of MAXSIM. Besides those, we also use edit-distance-based metrics. TER (Snover et al., 2006) and TERp (Snover et al., 2009) measure the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text into the other. 2.2 Syntactic Similarity Measures 2.2.1 Content Word Match and Mismatch Given a sentence pair, we extract corresponding content words (nouns, verbs, adjectives, and adverbs) between the sentences. This syntactic information is obtained from the Stanford parser (Klein and Manning, 2003). We have both the proportions of aligned words and the proportions of unaligned words in the two sentences (by normalizing with the harmonic mean of their number of content words) for each lexical category of content word. 2.2.2 Subject-Verb-Object Comparison We also employ dependency parsing in measuring semantic similarity. Specifically, some attributes like subjects, verbs, objects are identified for each pair of sentences. These attributes are used for our matching procedure which is based on the following comparisons between each pair of sentences: • Subject-Subject Comparison • Verb-Ver</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>In Journal of the Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="3150" citStr="Landauer et al., 1998" startWordPosition="469" endWordPosition="472">ing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic Analysis (LSA) (Landauer et al., 1998; Foltz et al., 1998) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007); (iii) knowledge-based approaches (Mihalcea et al., 2006) which determine the degree of similarity between texts using information derived from semantic networks, such as WordNet (Miller, 1995). Though each of these existing measures has its own advantages, they are typically used in separation. In our work, we integrate multiple similarity 190 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 190–195, Denver, Colorado, June 4-5, 2015. c�2015 Association for Co</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An Introduction to Latent Semantic Analysis. In Journal of the Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence Similarity Based on Semantic Nets and Corpus Statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138–1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Tesla: Translation Evaluation of Sentences with Linear-programming-based Analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>354--359</pages>
<contexts>
<context position="6353" citStr="Liu et al., 2010" startWordPosition="967" endWordPosition="970">target output by counting n-gram matches with the reference, relying on exact matching and has no concept of synonymy or paraphrasing. NIST is similar to BLEU, however, it uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. Unlike BLEU which focuses on precision, METEOR uses a combination of both precision and recall. Moreover, it incorporates stemming, synonymy and paraphrase. MAXSIM (Chan and Ng, 2008) models the MT problem as a maximum bipartite matching one and maps each word in one sentence to at most one word in the other sentence. We also experiment with TESLA (Liu et al., 2010) - a variant of MAXSIM. Besides those, we also use edit-distance-based metrics. TER (Snover et al., 2006) and TERp (Snover et al., 2009) measure the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text into the other. 2.2 Syntactic Similarity Measures 2.2.1 Content Word Match and Mismatch Given a sentence pair, we extract corresponding content words (nouns, verbs, adjectives, and adverbs) between the sentences. This syntactic information is obtained from the Stanford parser (Klein and Manning, 2003). We have both the proportions of aligned w</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. Tesla: Translation Evaluation of Sentences with Linear-programming-based Analysis. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 354–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Learning Textual Entailment Using SVMs and String Similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<contexts>
<context position="2373" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="354" endWordPosition="357">raction, question answering, and summarization. STS is related to, but different from textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). (Gomaa and Fahmy, 2013) discusses existing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about word</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>Prodromos Malakasiotis and Ion Androutsopoulos. 2007. Learning Textual Entailment Using SVMs and String Similarity measures. In Proceedings of of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
</authors>
<title>Paraphrase Recognition Using Machine Learning to Combine Similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP,</booktitle>
<pages>27--35</pages>
<contexts>
<context position="2404" citStr="Malakasiotis, 2009" startWordPosition="360" endWordPosition="361">is related to, but different from textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). (Gomaa and Fahmy, 2013) discusses existing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflec</context>
</contexts>
<marker>Malakasiotis, 2009</marker>
<rawString>Prodromos Malakasiotis. 2009. Paraphrase Recognition Using Machine Learning to Combine Similarity measures. In Proceedings of the 47th Annual Meeting of ACL and the 4th Int. Joint Conference on Natural Language Processing of AFNLP, pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven Bethard</author>
<author>David McClosky</author>
</authors>
<title>The stanford corenlp natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="10978" citStr="Manning et al., 2014" startWordPosition="1681" endWordPosition="1684">ym/hypnoym) taxonomy). Strategy 2 We determine some semantic relationships, e.g, synonym, antonym, and hypernym between sentences. The proportions of synonym word pairs, antonym word pairs, hypernym word pairs in two sentences (by normalizing with the harmonic mean of their number of content words) are taken as proxies of their semantic similarity. 3 System Description 3.1 Machine Learning Setup The machine learning setup is described as follows: Pre-processing The pre-processing phase includes tokenization, POS tagging, lemmatization, NER, syntactic parsing with the Stanford CoreNLP Toolkit (Manning et al., 2014). For some measures, we filter out punctuations and stop-words by using a pre-compiled stop-words list. Feature Generation We run each of the similarity measures separately and use the resulting scores as features for a machine learning classifier. A feature is selected for our final model if it proves useful in improving the performance of the system. Feature Combination The pre-computed similarity score vectors serve as features for this step. Our system utilizes a classifier combination approach, using a simple linear regression model to combine all the similarity measures. We use the trial</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="8550" citStr="McCallum, 2002" startWordPosition="1301" endWordPosition="1302">ressions might cause semantic dissimilarity, for example, when s1 consists of a named entity that does not appear in s2. Based on this, we detect both match and mismatch of named entities, numbers and time expressions between each pair of sentences (similar to that of content words). We use the Stanford Named Entity Recognizer (Finkel et al., 2005) to detect named entities in sentences. 2.3.2 LDA-based measures We build two Latent Dirichilet Allocation (LDA) models (Blei et al., 2003) from Wikipedia and the training dataset separately, using the Gensim (ˇReh˚uˇrek and Sojka, 2010) and Mallet (McCallum, 2002) software with 100 requested latent topics. Each sentence is represented by a vector using topics estimated by LDA. The similarity between two sentences is calculated as the cosine similarity between their corresponding vectors. 2.3.3 Word-representation-based measures Word representation computes vector representations of each word based on its context from very large datasets, usually capturing both syntactic and semantic information of words. Given two sentences s1 and s2 (stop-words are removed), each word of the sentences is represented as a single vector. We develop two different strateg</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="3301" citStr="Mihalcea et al., 2006" startWordPosition="490" endWordPosition="493">kasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic Analysis (LSA) (Landauer et al., 1998; Foltz et al., 1998) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007); (iii) knowledge-based approaches (Mihalcea et al., 2006) which determine the degree of similarity between texts using information derived from semantic networks, such as WordNet (Miller, 1995). Though each of these existing measures has its own advantages, they are typically used in separation. In our work, we integrate multiple similarity 190 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 190–195, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics measures of varying complexity ranging from simple lexical and syntactic similarity measures to complex semantic similarity one</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the 21st National Conference on Artificial Intelligence, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1310.4546.</title>
<date>2013</date>
<contexts>
<context position="9769" citStr="Mikolov et al., 2013" startWordPosition="1500" endWordPosition="1503">egies as follows: Strategy 1 For each word wi in s1, we identify a word wj most similar to wi in s2 by using cosine similarity measure. We define a measure 01 21Y (s1, s2) as follows: cos(wi, wj) , len(s1) where cos(wi, wj) is the cosine similarity between the word vectors of wi and wj. We also apply this strategy for each category of content words (noun, verb, adjective, and adverb) separately. Strategy 2 We sum up all of the vectors of words that occur in each sentence and define a sentence similarity measure Y21Y (s1, s2) as follows: For word representation, we use both the Word2vec model (Mikolov et al., 2013) trained on Google News and the GloVe model (Pennington et al., 2014) trained on Common Crawl data. 2.3.4 WordNet-based measures WordNet (Miller, 1995) is a commonly used lexical database of English where words of the same meaning are grouped into synonym sets (synsets). By using information derived from WordNet, we construct some similarity measures as follows: Strategy 1 This is similar to Strategy 1 for wordrepresentation-based measures, however, instead of using cosine similarity, we use the Wordnet path similarity (the shortest path that connects the senses in the is-a (hypernym/hypnoym) </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1310.4546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3437" citStr="Miller, 1995" startWordPosition="513" endWordPosition="514">to two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in distributional high semantic space to determine the similarity, such as Latent Semantic Analysis (LSA) (Landauer et al., 1998; Foltz et al., 1998) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007); (iii) knowledge-based approaches (Mihalcea et al., 2006) which determine the degree of similarity between texts using information derived from semantic networks, such as WordNet (Miller, 1995). Though each of these existing measures has its own advantages, they are typically used in separation. In our work, we integrate multiple similarity 190 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 190–195, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics measures of varying complexity ranging from simple lexical and syntactic similarity measures to complex semantic similarity ones and rely on supervised machine learning to take advantage of the different contributions of different features. We organize the remain</context>
<context position="9920" citStr="Miller, 1995" startWordPosition="1525" endWordPosition="1526">21Y (s1, s2) as follows: cos(wi, wj) , len(s1) where cos(wi, wj) is the cosine similarity between the word vectors of wi and wj. We also apply this strategy for each category of content words (noun, verb, adjective, and adverb) separately. Strategy 2 We sum up all of the vectors of words that occur in each sentence and define a sentence similarity measure Y21Y (s1, s2) as follows: For word representation, we use both the Word2vec model (Mikolov et al., 2013) trained on Google News and the GloVe model (Pennington et al., 2014) trained on Common Crawl data. 2.3.4 WordNet-based measures WordNet (Miller, 1995) is a commonly used lexical database of English where words of the same meaning are grouped into synonym sets (synsets). By using information derived from WordNet, we construct some similarity measures as follows: Strategy 1 This is similar to Strategy 1 for wordrepresentation-based measures, however, instead of using cosine similarity, we use the Wordnet path similarity (the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy). Strategy 2 We determine some semantic relationships, e.g, synonym, antonym, and hypernym between sentences. The proportions of synonym word </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A Lexical Database for English. Communications of the Association for Computing Machinery, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="5596" citStr="Papineni et al., 2002" startWordPosition="843" endWordPosition="846">etrics: �� numOfMatches(sl,s2) — Min{len(s2),/en(s2)} Y(81,82) = I and D(s1, s2) =2×numOf(s)minJ n�n((2)}2) 1 , where numOfMatches(s1, s2) and len(s) are the number of aligned word/phrase pairs between s1 and s2, and the number of words in s, respectively. 2.1.2 Machine Translation Measures We treat the task as a monolingual machine translation (MT) task (the source and target languages are the same, and the input and output should be similar in meaning), and take advantage of a variety of MT measures. At the lexical level, we experiment different n-gram and edit-distance-based metrics. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2010) are n-gram-based metrics commonly used for MT evaluation. BLEU scores the target output by counting n-gram matches with the reference, relying on exact matching and has no concept of synonymy or paraphrasing. NIST is similar to BLEU, however, it uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. Unlike BLEU which focuses on precision, METEOR uses a combination of both precision and recall. Moreover, it incorporates stemming, synonymy and paraphrase. MAXSIM (Chan and Ng, 2008) models the MT problem as a </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global Vectors for Word Representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1532--1543</pages>
<contexts>
<context position="9838" citStr="Pennington et al., 2014" startWordPosition="1512" endWordPosition="1515"> word wj most similar to wi in s2 by using cosine similarity measure. We define a measure 01 21Y (s1, s2) as follows: cos(wi, wj) , len(s1) where cos(wi, wj) is the cosine similarity between the word vectors of wi and wj. We also apply this strategy for each category of content words (noun, verb, adjective, and adverb) separately. Strategy 2 We sum up all of the vectors of words that occur in each sentence and define a sentence similarity measure Y21Y (s1, s2) as follows: For word representation, we use both the Word2vec model (Mikolov et al., 2013) trained on Google News and the GloVe model (Pennington et al., 2014) trained on Common Crawl data. 2.3.4 WordNet-based measures WordNet (Miller, 1995) is a commonly used lexical database of English where words of the same meaning are grouped into synonym sets (synsets). By using information derived from WordNet, we construct some similarity measures as follows: Strategy 1 This is similar to Strategy 1 for wordrepresentation-based measures, however, instead of using cosine similarity, we use the Wordnet path similarity (the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy). Strategy 2 We determine some semantic relationships, e.g, </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Rinaldi</author>
<author>James Dowdall</author>
<author>Kaarel Kaljurand</author>
<author>Michael Hess</author>
<author>Diego Moll´a</author>
</authors>
<title>Exploiting Paraphrases in a Question Answering System.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<pages>25--32</pages>
<marker>Rinaldi, Dowdall, Kaljurand, Hess, Moll´a, 2003</marker>
<rawString>Fabio Rinaldi, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Moll´a. 2003. Exploiting Paraphrases in a Question Answering System. In Proceedings of the 2nd International Workshop on Paraphrasing, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="6458" citStr="Snover et al., 2006" startWordPosition="984" endWordPosition="987">ept of synonymy or paraphrasing. NIST is similar to BLEU, however, it uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. Unlike BLEU which focuses on precision, METEOR uses a combination of both precision and recall. Moreover, it incorporates stemming, synonymy and paraphrase. MAXSIM (Chan and Ng, 2008) models the MT problem as a maximum bipartite matching one and maps each word in one sentence to at most one word in the other sentence. We also experiment with TESLA (Liu et al., 2010) - a variant of MAXSIM. Besides those, we also use edit-distance-based metrics. TER (Snover et al., 2006) and TERp (Snover et al., 2009) measure the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text into the other. 2.2 Syntactic Similarity Measures 2.2.1 Content Word Match and Mismatch Given a sentence pair, we extract corresponding content words (nouns, verbs, adjectives, and adverbs) between the sentences. This syntactic information is obtained from the Stanford parser (Klein and Manning, 2003). We have both the proportions of aligned words and the proportions of unaligned words in the two sentences (by normalizing with the harmonic mean o</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="6489" citStr="Snover et al., 2009" startWordPosition="990" endWordPosition="993">. NIST is similar to BLEU, however, it uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. Unlike BLEU which focuses on precision, METEOR uses a combination of both precision and recall. Moreover, it incorporates stemming, synonymy and paraphrase. MAXSIM (Chan and Ng, 2008) models the MT problem as a maximum bipartite matching one and maps each word in one sentence to at most one word in the other sentence. We also experiment with TESLA (Liu et al., 2010) - a variant of MAXSIM. Besides those, we also use edit-distance-based metrics. TER (Snover et al., 2006) and TERp (Snover et al., 2009) measure the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text into the other. 2.2 Syntactic Similarity Measures 2.2.1 Content Word Match and Mismatch Given a sentence pair, we extract corresponding content words (nouns, verbs, adjectives, and adverbs) between the sentences. This syntactic information is obtained from the Stanford parser (Klein and Manning, 2003). We have both the proportions of aligned words and the proportions of unaligned words in the two sentences (by normalizing with the harmonic mean of their number of content words</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate. Machine Translation, 23(2-3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A Machine Learning Approach to Textual Entailment Recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="2428" citStr="Zanzotto et al., 2009" startWordPosition="362" endWordPosition="365">ifferent from textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (PARA) (Dolan et al., 2004) as it aims to render a graded notion of semantic equivalence between two textual snippets, rather than a binary yes/no decision. STS requires a bidirectional similarity relation between sentences, while TE annotates them with an unidirectional entailment relation. The literature of STS is rife with attempts to compute similarity between texts using a multitude of measures at different levels of depth: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005). (Gomaa and Fahmy, 2013) discusses existing works on STS and partitions them into three categories based on the similarity measures used: (i) string-based approaches (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which operate on string sequences and character composition to compute similarities and can be categorized into two groups: character-based and term-based approaches; (ii) corpus-based approaches (Li et al., 2006) which gain statistics information about words from large corpora and reflect their semantics in dis</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A Machine Learning Approach to Textual Entailment Recognition. Natural Language Engineering, 15(4):551–582.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>