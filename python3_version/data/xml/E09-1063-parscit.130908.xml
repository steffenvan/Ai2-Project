<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000473">
<title confidence="0.9982265">
Bilingually Motivated Domain-Adapted Word Segmentation
for Statistical Machine Translation
</title>
<author confidence="0.999689">
Yanjun Ma Andy Way
</author>
<affiliation confidence="0.991814333333333">
National Centre for Language Technology
School of Computing
Dublin City University
</affiliation>
<address confidence="0.747799">
Dublin 9, Ireland
</address>
<email confidence="0.997581">
{yma, away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.993835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854045454546">
We introduce a word segmentation ap-
proach to languages where word bound-
aries are not orthographically marked,
with application to Phrase-Based Statis-
tical Machine Translation (PB-SMT). In-
stead of using manually segmented mono-
lingual domain-specific corpora to train
segmenters, we make use of bilingual cor-
pora and statistical word alignment tech-
niques. First of all, our approach is
adapted for the specific translation task at
hand by taking the corresponding source
(target) language into account. Secondly,
this approach does not rely on manu-
ally segmented training data so that it
can be automatically adapted for differ-
ent domains. We evaluate the perfor-
mance of our segmentation approach on
PB-SMT tasks from two domains and
demonstrate that our approach scores con-
sistently among the best results across dif-
ferent data conditions.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999395607142857">
State-of-the-art Statistical Machine Translation
(SMT) requires a certain amount of bilingual cor-
pora as training data in order to achieve compet-
itive results. The only assumption of most cur-
rent statistical models (Brown et al., 1993; Vogel
et al., 1996; Deng and Byrne, 2005) is that the
aligned sentences in such corpora should be seg-
mented into sequences of tokens that are meant to
be words. Therefore, for languages where word
boundaries are not orthographically marked, tools
which segment a sentence into words are required.
However, this segmentation is normally performed
as a preprocessing step using various word seg-
menters. Moreover, most of these segmenters are
usually trained on a manually segmented domain-
specific corpus, which is not adapted for the spe-
cific translation task at hand given that the manual
segmentation is performed in a monolingual con-
text. Consequently, such segmenters cannot pro-
duce consistently good results when used across
different domains.
A substantial amount of research has been car-
ried out to address the problems of word segmen-
tation. However, most research focuses on com-
bining various segmenters either in SMT training
or decoding (Dyer et al., 2008; Zhang et al., 2008).
One important yet often neglected fact is that the
optimal segmentation of the source (target) lan-
guage is dependent on the target (source) language
itself, its domain and its genre. Segmentation con-
sidered to be “good” from a monolingual point
of view may be unadapted for training alignment
models or PB-SMT decoding (Ma et al., 2007).
The resulting segmentation will consequently in-
fluence the performance of an SMT system.
In this paper, we propose a bilingually moti-
vated automatically domain-adapted approach for
SMT. We utilise a small bilingual corpus with
the relevant language segmented into basic writ-
ing units (e.g. characters for Chinese or kana for
Japanese). Our approach consists of using the
output from an existing statistical word aligner
to obtain a set of candidate “words”. We evalu-
ate the reliability of these candidates using sim-
ple metrics based on co-occurrence frequencies,
similar to those used in associative approaches to
word alignment (Melamed, 2000). We then mod-
ify the segmentation of the respective sentences
in the parallel corpus according to these candi-
date words; these modified sentences are then
given back to the word aligner, which produces
new alignments. We evaluate the validity of our
approach by measuring the influence of the seg-
mentation process on Chinese-to-English Machine
Translation (MT) tasks in two different domains.
The remainder of this paper is organised as fol-
</bodyText>
<note confidence="0.92333">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998148">
549
</page>
<bodyText confidence="0.9990052">
lows. In section 2, we study the influence of
word segmentation on PB-SMT across different
domains. Section 3 describes the working mecha-
nism of our bilingually motivated word segmenta-
tion approach. In section 4, we illustrate the adap-
tation of our decoder to this segmentation scheme.
The experiments conducted in two different do-
mains are reported in Section 5 and 6. We discuss
related work in section 7. Section 8 concludes and
gives avenues for future work.
</bodyText>
<sectionHeader confidence="0.7400125" genericHeader="introduction">
2 The Influence of Word Segmentation
on SMT: A Pilot Investigation
</sectionHeader>
<bodyText confidence="0.999782382352941">
The monolingual word segmentation step in tra-
ditional SMT systems has a substantial impact on
the performance of such systems. A considerable
amount of recent research has focused on the in-
fluence of word segmentation on SMT (Ma et al.,
2007; Chang et al., 2008; Zhang et al., 2008);
however, most explorations focused on the impact
of various segmentation guidelines and the mech-
anisms of the segmenters themselves. A current
research interest concerns consistency of perfor-
mance across different domains. From our ex-
periments, we show that monolingual segmenters
cannot produce consistently good results when ap-
plied to a new domain.
Our pilot investigation into the influence of
word segmentation on SMT involves three off-
the-shelf Chinese word segmenters including
ICTCLAS (ICT) Olympic version1, LDC seg-
menter2 and Stanford segmenter version 2006-05-
113. Both ICTCLAS and Stanford segmenters
utilise machine learning techniques, with Hidden
Markov Models for ICT (Zhang et al., 2003) and
conditional random fields for the Stanford seg-
menter (Tseng et al., 2005). Both segmenta-
tion models were trained on news domain data
with named entity recognition functionality. The
LDC segmenter is dictionary-based with word fre-
quency information to help disambiguation, both
of which are collected from data in the news do-
main. We used Chinese character-based and man-
ual segmentations as contrastive segmentations.
The experiments were carried out on a range of
data sizes from news and dialogue domains using
a state-of-the-art Phrase-Based SMT (PB-SMT)
</bodyText>
<footnote confidence="0.9993975">
1http://ictclas.org/index.html
2http://www.ldc.upenn.edu/Projects/
Chinese
3http://nlp.stanford.edu/software/
</footnote>
<page confidence="0.466271">
segmenter.shtml
</page>
<bodyText confidence="0.979560473684211">
system—Moses (Koehn et al., 2007). The perfor-
mance of PB-SMT system is measured with BLEU
score (Papineni et al., 2002).
We firstly measure the influence of word seg-
mentation on in-domain data with respect to the
three above mentioned segmenters, namely UN
data from the NIST 2006 evaluation campaign. As
can be seen from Table 1, using monolingual seg-
menters achieves consistently better SMT perfor-
mance than character-based segmentation (CS) on
different data sizes, which means character-based
segmentation is not good enough for this domain
where the vocabulary tends to be large. We can
also observe that the ICT and Stanford segmenter
consistently outperform the LDC segmenter. Even
using 3M sentence pairs for training, the differ-
ences between them are still statistically signifi-
cant (p &lt; 0.05) using approximate randomisation
(Noreen, 1989) for significance testing.
</bodyText>
<table confidence="0.991362">
40K 160K 640K 3M
CS 8.33 12.47 14.40 17.80
ICT 10.17 14.85 17.20 20.50
LDC 9.37 13.88 15.86 19.59
Stanford 10.45 15.26 16.94 20.64
</table>
<tableCaption confidence="0.999864">
Table 1: Word segmentation on NIST data sets
</tableCaption>
<bodyText confidence="0.999940047619048">
However, when tested on out-of-domain data,
i.e. IWSLT data in the dialogue domain, the re-
sults seem to be more difficult to predict. We
trained the system on different sizes of data and
evaluated the system on two test sets: IWSLT
2006 and 2007. From Table 2, we can see that on
the IWSLT 2006 test sets, LDC achieves consis-
tently good results and the Stanford segmenter is
the worst.4 Furthermore, the character-based seg-
mentation also achieves competitive results. On
IWSLT 2007, all monolingual segmenters outper-
form character-based segmentation and the LDC
segmenter is only slightly better than the other seg-
menters.
From the experiments reported above, we
can reach the following conclusions. First of
all, character-based segmentation cannot achieve
state-of-the-art results in most experimental SMT
settings. This also motivates the necessity to
work on better segmentation strategies. Second,
monolingual segmenters cannot achieve consis-
</bodyText>
<footnote confidence="0.66740775">
4Interestingly, the developers themselves also note the
sensitivity of the Stanford segmenter and incorporate exter-
nal lexical information to address such problems (Chang et
al., 2008).
</footnote>
<page confidence="0.990821">
550
</page>
<table confidence="0.999141727272727">
40K 160K
IWSLT06 CS 19.31 23.06
Manual 19.94 -
ICT 20.34 23.36
LDC 20.37 24.34
Stanford 18.25 21.40
IWSLT07 CS 29.59 30.25
Manual 33.85 -
ICT 31.18 33.38
LDC 31.74 33.44
Stanford 30.97 33.41
</table>
<tableCaption confidence="0.999268">
Table 2: Word segmentation on IWSLT data sets
</tableCaption>
<bodyText confidence="0.99962">
tently good results when used in another domain.
In the following sections, we propose a bilingually
motivated segmentation approach which can be
automatically derived from a small representative
data set and the experiments show that we can con-
sistently obtain state-of-the-art results in different
domains.
</bodyText>
<sectionHeader confidence="0.9923995" genericHeader="method">
3 Bilingually Motivated Word
Segmentation
</sectionHeader>
<subsectionHeader confidence="0.998458">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.999957071428571">
While in this paper, we focus on Chinese–English,
the method proposed is applicable to other lan-
guage pairs. The notation, however, assumes
Chinese–English MT. Given a Chinese sentence
cJ1 consisting of J characters fc1, ... , cJ} and
an English sentence eI1 consisting of I words
fe1, ... , eI}, AC→E will denote a Chinese-to-
English word alignment between cJ1 and eI1. Since
we are primarily interested in 1-to-n alignments,
AC→E can be represented as a set of pairs ai =
(Ci, ei) denoting a link between one single En-
glish word ei and a few Chinese characters Ci.The
set Ci is empty if the word ei is not aligned to any
character in cJ1 .
</bodyText>
<subsectionHeader confidence="0.99974">
3.2 Candidate Extraction
</subsectionHeader>
<bodyText confidence="0.9976706875">
In the following, we assume the availability of an
automatic word aligner that can output alignments
AC→E for any sentence pair (cJ1, eI1) in a paral-
lel corpus. We also assume that AC→E contain
1-to-n alignments. Our method for Chinese word
segmentation is as follows: whenever a single En-
glish word is aligned with several consecutive Chi-
nese characters, they are considered candidates for
grouping. Formally, given an alignment AC→E
between cJ1 and eI1, if ai = (Ci, ei) E AC→E,
with Ci = fci1, ... , ci.} and bk E Q1, m − 1�,
ik+1 − ik = 1, then the alignment ai between ei
and the sequence of words Ci is considered a can-
didate word. Some examples of such 1-to-n align-
ments between Chinese and English we can derive
automatically are displayed in Figure 1.5
</bodyText>
<figureCaption confidence="0.981793">
Figure 1: Example of 1-to-n word alignments be-
tween English words and Chinese characters
</figureCaption>
<subsectionHeader confidence="0.99857">
3.3 Candidate Reliability Estimation
</subsectionHeader>
<bodyText confidence="0.999601235294118">
Of course, the process described above is error-
prone, especially on a small amount of training
data. If we want to change the input segmentation
to give to the word aligner, we need to make sure
that we are not making harmful modifications. We
thus additionally evaluate the reliability of the can-
didates we extract and filter them before inclusion
in our bilingual dictionary. To perform this filter-
ing, we use two simple statistical measures. In the
following, ai = (Ci, ei) denotes a candidate.
The first measure we consider is co-occurrence
frequency (COOC(Ci, ei)), i.e. the number of
times Ci and ei co-occur in the bilingual corpus.
This very simple measure is frequently used in as-
sociative approaches (Melamed, 2000). The sec-
ond measure is the alignment confidence (Ma et
al., 2007), defined as
</bodyText>
<equation confidence="0.969064333333333">
C(ai)
AC(ai) =
COOC(Ci, ei),
</equation>
<bodyText confidence="0.9998876">
where C(ai) denotes the number of alignments
proposed by the word aligner that are identical to
ai. In other words, AC(ai) measures how often
the aligner aligns Ci and ei when they co-occur.
We also impose that |Ci  |~ k, where k is a fixed
integer that may depend on the language pair (be-
tween 3 and 5 in practice). The rationale behind
this is that it is very rare to get reliable alignments
between one word and k consecutive words when
k is high.
</bodyText>
<footnote confidence="0.6924432">
5While in this paper we are primarily concerned with lan-
guages where the word boundaries are not orthographically
marked, this approach, however, can also be applied to lan-
guages marked with word boundaries to construct bilingually
motivated “words”.
</footnote>
<page confidence="0.993942">
551
</page>
<bodyText confidence="0.999949538461539">
The candidates are included in our bilingual dic-
tionary if and only if their measures are above
some fixed thresholds tCOOC and tAC, which al-
low for the control of the size of the dictionary and
the quality of its contents. Some other measures
(including the Dice coefficient) could be consid-
ered; however, it has to be noted that we are more
interested here in the filtering than in the discov-
ery of alignments per se, since our method builds
upon an existing aligner. Moreover, we will see
that even these simple measures can lead to an im-
provement in the alignment process in an MT con-
text.
</bodyText>
<subsectionHeader confidence="0.975259">
3.4 Bootstrapped word segmentation
</subsectionHeader>
<bodyText confidence="0.999980086956522">
Once the candidates are extracted, we perform
word segmentation using the bilingual dictionar-
ies constructed using the method described above;
this provides us with an updated training corpus,
in which some character sequences have been re-
placed by a single token. This update is totally
naive: if an entry ai = hCi, eii is present in the
dictionary and matches one sentence pair (cJ1 , eI1)
(i.e. Ci and ei are respectively contained in cJ1 and
eI1), then we replace the sequence of characters Ci
with a single token which becomes a new lexical
unit.6 Note that this replacement occurs even if
no alignment was found between Ci and ei for the
pair (cJ1 , eI1). This is motivated by the fact that the
filtering described above is quite conservative; we
trust the entry ai to be correct.
This process can be applied several times: once
we have grouped some characters together, they
become the new basic unit to consider, and we can
re-run the same method to get additional group-
ings. However, we have not seen in practice much
benefit from running it more than twice (few new
candidates are extracted after two iterations).
</bodyText>
<sectionHeader confidence="0.987486" genericHeader="method">
4 Word Lattice Decoding
</sectionHeader>
<subsectionHeader confidence="0.990667">
4.1 Word Lattices
</subsectionHeader>
<bodyText confidence="0.999595857142857">
In the decoding stage, the various segmentation
alternatives can be encoded into a compact rep-
resentation of word lattices. A word lattice G =
hV, Ei is a directed acyclic graph that formally is
a weighted finite state automaton. In the case of
word segmentation, each edge is a candidate word
associated with its weights. A straightforward es-
</bodyText>
<footnote confidence="0.854187333333333">
6In case of overlap between several groups of words to
replace, we select the one with the highest confidence (ac-
cording to tAC).
</footnote>
<bodyText confidence="0.9510515">
timation of the weights is to distribute the proba-
bility mass for each node uniformly to each out-
going edge. The single node having no outgoing
edges is designated the “end node”. An example
of word lattices for a Chinese sentence is shown in
Figure 2.
</bodyText>
<subsectionHeader confidence="0.993426">
4.2 Word Lattice Generation
</subsectionHeader>
<bodyText confidence="0.99970275">
Previous research on generating word lattices re-
lies on multiple monolingual segmenters (Xu et
al., 2005; Dyer et al., 2008). One advantage of
our approach is that the bilingually motivated seg-
mentation process facilitates word lattice genera-
tion without relying on other segmenters. As de-
scribed in section 3.4, the update of the training
corpus based on the constructed bilingual dictio-
nary requires that the sentence pair meets the bilin-
gual constraints. Such a segmentation process in
the training stage facilitates the utilisation of word
lattice decoding.
</bodyText>
<subsectionHeader confidence="0.994401">
4.3 Phrase-Based Word Lattice Decoding
</subsectionHeader>
<bodyText confidence="0.831283625">
Given a Chinese input sentence cJ1 consisting of J
characters, the traditional approach is to determine
the best word segmentation and perform decoding
afterwards. In such a case, we first seek a single
best segmentation:
ˆfK 1 = arg max {Pr(fK1 |cJ1 )}
fK ,K
Then in the decoding stage, we seek:
</bodyText>
<equation confidence="0.984038">
ˆeI1 = arg max
e1,I
z {Pr(eI1 |ˆfK1 )}
</equation>
<bodyText confidence="0.999952">
In such a scenario, some segmentations which are
potentially optimal for the translation may be lost.
This motivates the need for word lattice decoding.
The search process can be rewritten as:
</bodyText>
<equation confidence="0.9937118">
ˆeI1 = arg max
z
e1,I
Pr(eI 1)Pr(fK 1 |eI 1,cJ1)}
Pr(eI 1)Pr(fK 1 |eI1)Pr(fK1 |cJ1)}
</equation>
<bodyText confidence="0.998238857142857">
Given the fact that the number of segmentations
fK 1 grows exponentially with respect to the num-
ber of characters K, it is impractical to firstly enu-
merate all possible fK1 and then to decode. How-
ever, it is possible to enumerate all the alternative
segmentations for a substring of cJ1 , making the
utilisation of word lattices tractable in PB-SMT.
</bodyText>
<table confidence="0.997818375">
{max Pr(eI1, fK1 |cJ1)}
fK ,K
= arg max {max
z fK ,K
e1,I
= arg max {max
z fK ,K
e1,I
</table>
<page confidence="0.983887">
552
</page>
<figureCaption confidence="0.99928">
Figure 2: Example of a word lattice
</figureCaption>
<sectionHeader confidence="0.992139" genericHeader="method">
5 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.843214">
5.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999901954545454">
The intrinsic quality of word segmentation is nor-
mally evaluated against a manually segmented
gold-standard corpus using F-score. While this
approach can give a direct evaluation of the qual-
ity of the word segmentation, it is faced with sev-
eral limitations. First of all, it is really difficult to
build a reliable and objective gold-standard given
the fact that there is only 70% agreement between
native speakers on this task (Sproat et al., 1996).
Second, an increase in F-score does not necessar-
ily imply an improvement in translation quality. It
has been shown that F-score has a very weak cor-
relation with SMT translation quality in terms of
BLEU score (Zhang et al., 2008). Consequently,
we chose to extrinsically evaluate the performance
of our approach via the Chinese–English transla-
tion task, i.e. we measure the influence of the
segmentation process on the final translation out-
put. The quality of the translation output is mainly
evaluated using BLEU, with NIST (Doddington,
2002) and METEOR (Banerjee and Lavie, 2005)
as complementary metrics.
</bodyText>
<subsectionHeader confidence="0.9904">
5.2 Data
</subsectionHeader>
<bodyText confidence="0.999654318181818">
The data we used in our experiments are from
two different domains, namely news and travel di-
alogues. For the news domain, we trained our
system using a portion of UN data for NIST
2006 evaluation campaign. The system was de-
veloped on LDC Multiple-Translation Chinese
(MTC) Corpus and tested on MTC part 2, which
was also used as a test set for NIST 2002 evalua-
tion campaign.
For the dialogue data, we used the Chinese–
English datasets provided within the IWSLT 2007
evaluation campaign. Specifically, we used the
standard training data, to which we added devset1
and devset2. Devset4 was used to tune the param-
eters and the performance of the system was tested
on both IWSLT 2006 and 2007 test sets. We used
both test sets because they are quite different in
terms of sentence length and vocabulary size. To
test the scalability of our approach, we used HIT
corpus provided within IWSLT 2008 evaluation
campaign. The various statistics for the corpora
are shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.994727">
5.3 Baseline System
</subsectionHeader>
<bodyText confidence="0.999978416666667">
We conducted experiments using different seg-
menters with a standard log-linear PB-SMT
model: GIZA++ implementation of IBM word
alignment model 4 (Och and Ney, 2003), the
refinement and phrase-extraction heuristics de-
scribed in (Koehn et al., 2003), minimum-error-
rate training (Och, 2003), a 5-gram language
model with Kneser-Ney smoothing trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al., 2007; Dyer
et al., 2008) to translate both single best segmen-
tation and word lattices.
</bodyText>
<sectionHeader confidence="0.813402" genericHeader="method">
6 Experiments
6.1 Results
</sectionHeader>
<bodyText confidence="0.999932">
The initial word alignments are obtained using
the baseline configuration described above by seg-
menting the Chinese sentences into characters.
From these we build a bilingual 1-to-n dictionary,
and the training corpus is updated by grouping the
characters in the dictionaries into a single word,
using the method presented in section 3.4. As pre-
viously mentioned, this process can be repeated
several times. We then extract aligned phrases us-
ing the same procedure as for the baseline sys-
tem; the only difference is the basic unit we are
considering. Once the phrases are extracted, we
perform the estimation of weights for the fea-
tures of the log-linear model. We then use a
simple dictionary-based maximum matching algo-
rithm to obtain a single-best segmentation for the
Chinese sentences in the development set so that
</bodyText>
<page confidence="0.997641">
553
</page>
<table confidence="0.99944275">
Train Dev. Eval.
Zh En Zh En Zh En
Dialogue Sentences 40,958 489 (7 ref.) 489 (6 ref.)/489 (7 ref.)
Running words 488,303 385,065 8,141 46,904 8,793/4,377 51,500/23,181
Vocabulary size 2,742 9,718 835 1,786 936/772 2,016/1,339
News Sentences 40,000 993 (9 ref.) 878 (4 ref.)
Running words 1,412,395 956,023 41,466 267,222 38,700 105,530
Vocabulary size 6057 20,068 1,983 10,665 1,907 7,388
</table>
<tableCaption confidence="0.999907">
Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En)
</tableCaption>
<bodyText confidence="0.909473888888889">
minimum-error-rate training can be performed.7
Finally, in the decoding stage, we use the same
segmentation algorithm to obtain the single-best
segmentation on the test set, and word lattices can
also be generated using the bilingual dictionary.
The various parameters of the method (k, tCOOC,
tAC, cf. section 3) were optimised on the develop-
ment set. One iteration of character grouping on
the NIST task was found to be enough; the optimal
set of values was found to be k = 3, tAC = 0.0
and tCOOC = 0, meaning that all the entries in the
bilingually dictionary are kept. On IWSLT data,
we found that two iterations of character grouping
were needed: the optimal set of values was found
to be k = 3, tAC = 0.3, tCOOC = 8 for the first
iteration, and tAC = 0.2, tCOOC = 15 for the
second.
As can be seen from Table 4, our bilingually
motivated segmenter (BS) achieved statistically
significantly better results than character-based
segmentation when enhanced with word lattice de-
coding.8 Compared to the best in-domain seg-
menter, namely the Stanford segmenter on this
particular task, our approach is inferior accord-
ing to BLEU and NIST. We firstly attribute this
to the small amount of training data, from which
a high quality bilingual dictionary cannot be ob-
tained due to data sparseness problems. We also
attribute this to the vast amount of named entity
terms in the test sets, which is extremely difficult
for our approach.9 We expect to see better re-
sults when a larger amount of data is used and the
segmenter is enhanced with a named entity recog-
niser. On IWSLT data (cf. Tables 5 and 6), our
7In order to save computational time, we used the same
set of parameters obtained above to decode both the single-
best segmentation and the word lattice.
8Note the BLEU scores are particularly low due to the
number of references used (4 references), in addition to the
small amount of training data available.
9As we previously point out, both ICT and Stanford seg-
menters are equipped with named entity recognition func-
tionality. This may risk causing data sparseness problems on
small training data. However, this is beneficial in the transla-
tion process compared to character-based segmentation.
</bodyText>
<figureCaption confidence="0.7600052">
approach yielded a consistently good performance
on both translation tasks compared to the best in-
domain segmenter—the LDC segmenter. More-
over, the good performance is confirmed by all
three evaluation measures.
</figureCaption>
<table confidence="0.9982936">
BLEU NIST METEOR
CS 8.43 4.6272 0.3778
Stanford 10.45 5.0675 0.3699
BS-SingleBest 7.98 4.4374 0.3510
BS-WordLattice 9.04 4.6667 0.3834
</table>
<tableCaption confidence="0.995002">
Table 4: BS on NIST task
</tableCaption>
<table confidence="0.9998496">
BLEU NIST METEOR
CS 0.1931 6.1816 0.4998
LDC 0.2037 6.2089 0.4984
BS-SingleBest 0.1865 5.7816 0.4602
BS-WordLattice 0.2041 6.2874 0.5124
</table>
<tableCaption confidence="0.992359">
Table 5: BS on IWSLT 2006 task
</tableCaption>
<table confidence="0.999817">
BLEU NIST METEOR
CS 0.2959 6.1216 0.5216
LDC 0.3174 6.2464 0.5403
BS-SingleBest 0.3023 6.0476 0.5125
BS-WordLattice 0.3171 6.3518 0.5603
</table>
<tableCaption confidence="0.998666">
Table 6: BS on IWSLT 2007 task
</tableCaption>
<subsectionHeader confidence="0.997937">
6.2 Parameter Search Graph
</subsectionHeader>
<bodyText confidence="0.9989867">
The reliability estimation process is computation-
ally intensive. However, this can be easily paral-
lelised. From our experiments, we observed that
the translation results are very sensitive to the pa-
rameters and this search process is essential to
achieve good results. Figure 3 is the search graph
on the IWSLT data set in the first iteration step.
From this graph, we can see that filtering of the
bilingual dictionary is essential in order to achieve
better performance.
</bodyText>
<page confidence="0.995876">
554
</page>
<table confidence="0.998325833333333">
Voc. Char.voc Run. Words
CS 2,742 2,742 488,303
ICT 11,441 1,629 358,504
LDC 9,293 1,963 364,253
Stanford 18,676 981 348,251
BS 3,828 2,740 402,845
</table>
<tableCaption confidence="0.958854">
Table 8: Vocabulary size of IWSLT task (40K)
</tableCaption>
<figureCaption confidence="0.9237635">
Figure 3: The search graph on development set of
IWSLT task
</figureCaption>
<subsectionHeader confidence="0.998898">
6.3 Vocabulary Size
</subsectionHeader>
<bodyText confidence="0.99997680952381">
Our bilingually motivated segmentation approach
has to overcome another challenge in order to
produce competitive results, i.e. data sparseness.
Given that our segmentation is based on bilingual
dictionaries, the segmentation process can signif-
icantly increase the size of the vocabulary, which
could potentially lead to a data sparseness prob-
lem when the size of the training data is small. Ta-
bles 7 and 8 list the statistics of the Chinese side
of the training data, including the total vocabulary
(Voc), number of character vocabulary (Char.voc)
in Voc, and the running words (Run.words) when
different word segmentations were used. From Ta-
ble 7, we can see that our approach suffered from
data sparseness on the NIST task, i.e. a large
vocabulary was generated, of which a consider-
able amount of characters still remain as separate
words. On the IWSLT task, since the dictionary
generation process is more conservative, we main-
tained a reasonable vocabulary size, which con-
tributed to the final good performance.
</bodyText>
<table confidence="0.999849833333333">
Voc. Char.voc Run. Words
CS 6,057 6,057 1,412,395
ICT 16,775 1,703 870,181
LDC 16,100 2,106 881,861
Stanford 22,433 1,701 880,301
BS 18,111 2,803 927,182
</table>
<tableCaption confidence="0.999542">
Table 7: Vocabulary size of NIST task (40K)
</tableCaption>
<subsectionHeader confidence="0.986658">
6.4 Scalability
</subsectionHeader>
<bodyText confidence="0.999779416666667">
The experimental results reported above are based
on a small training corpus containing roughly
40,000 sentence pairs. We are particularly inter-
ested in the performance of our segmentation ap-
proach when it is scaled up to larger amounts of
data. Given that the optimisation of the bilingual
dictionary is computationally intensive, it is im-
practical to directly extract candidate words and
estimate their reliability. As an alternative, we can
use the obtained bilingual dictionary optimised on
the small corpus to perform segmentation on the
larger corpus. We expect competitive results when
the small corpus is a representative sample of the
larger corpus and large enough to produce reliable
bilingual dictionaries without suffering severely
from data sparseness.
As we can see from Table 9, our segmenta-
tion approach achieved consistent results on both
IWSLT 2006 and 2007 test sets. On the NIST task
(cf. Table 10), our approach outperforms the basic
character-based segmentation; however, it is still
inferior compared to the other in-domain mono-
lingual segmenters due to the low quality of the
bilingual dictionary induced (cf. section 6.1).
</bodyText>
<table confidence="0.997853857142857">
IWSLT06 IWSLT07
CS 23.06 30.25
ICT 23.36 33.38
LDC 24.34 33.44
Stanford 21.40 33.41
BS-SingleBest 22.45 30.76
BS-WordLattice 24.18 32.99
</table>
<tableCaption confidence="0.989969">
Table 9: Scale-up to 160K on IWSLT data sets
</tableCaption>
<table confidence="0.999448142857143">
160K 640K
CS 12.47 14.40
ICT 14.85 17.20
LDC 13.88 15.86
Stanford 15.26 16.94
BS-SingleBest 12.58 14.11
BS-WordLattice 13.74 15.33
</table>
<tableCaption confidence="0.998869">
Table 10: Scalability of BS on NIST task
</tableCaption>
<page confidence="0.995394">
555
</page>
<subsectionHeader confidence="0.986779">
6.5 Using different word aligners
</subsectionHeader>
<bodyText confidence="0.999294125">
The above experiments rely on GIZA++ to per-
form word alignment. We next show that our ap-
proach is not dependent on the word aligner given
that we have a conservative reliability estimation
procedure. Table 11 shows the results obtained on
the IWSLT data set using the MTTK alignment
tool (Deng and Byrne, 2005; Deng and Byrne,
2006).
</bodyText>
<table confidence="0.998483714285714">
IWSLT06 IWSLT07
CS 21.04 31.41
ICT 20.48 31.11
LDC 20.79 30.51
Stanford 17.84 29.35
BS-SingleBest 19.22 29.75
BS-WordLattice 21.76 31.75
</table>
<tableCaption confidence="0.998845">
Table 11: BS on IWSLT data sets using MTTK
</tableCaption>
<sectionHeader confidence="0.999508" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.99994240625">
(Xu et al., 2004) were the first to question the use
of word segmentation in SMT and showed that the
segmentation proposed by word alignments can be
used in SMT to achieve competitive results com-
pared to using monolingual segmenters. Our ap-
proach differs from theirs in two aspects. Firstly,
(Xu et al., 2004) use word aligners to reconstruct
a (monolingual) Chinese dictionary and reuse this
dictionary to segment Chinese sentences as other
monolingual segmenters. Our approach features
the use of a bilingual dictionary and conducts a
different segmentation. In addition, we add a pro-
cess which optimises the bilingual dictionary ac-
cording to translation quality. (Ma et al., 2007)
proposed an approach to improve word alignment
by optimising the segmentation of both source and
target languages. However, the reported experi-
ments still rely on some monolingual segmenters
and the issue of scalability is not addressed. Our
research focuses on avoiding the use of monolin-
gual segmenters in order to improve the robustness
of segmenters across different domains.
(Xu et al., 2005) were the first to propose the
use of word lattice decoding in PB-SMT, in order
to address the problems of segmentation. (Dyer
et al., 2008) extended this approach to hierarchi-
cal SMT systems and other language pairs. How-
ever, both of these methods require some mono-
lingual segmentation in order to generate word lat-
tices. Our approach facilitates word lattice gener-
ation given that our segmentation is driven by the
bilingual dictionary.
</bodyText>
<sectionHeader confidence="0.987634" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992258064516">
In this paper, we introduced a bilingually moti-
vated word segmentation approach for SMT. The
assumption behind this motivation is that the lan-
guage to be segmented can be tokenised into ba-
sic writing units. Firstly, we extract 1-to-n word
alignments using statistical word aligners to con-
struct a bilingual dictionary in which each entry
indicates a correspondence between one English
word and n Chinese characters. This dictionary is
then filtered using a few simple association mea-
sures and the final bilingual dictionary is deployed
for word segmentation. To overcome the segmen-
tation problem in the decoding stage, we deployed
word lattice decoding.
We evaluated our approach on translation tasks
from two different domains and demonstrate that
our approach is (i) not as sensitive as monolingual
segmenters, and (ii) that the SMT system using
our word segmentation can achieve state-of-the-art
performance. Moreover, our approach can easily
be scaled up to larger data sets and achieves com-
petitive results if the small data used is a represen-
tative sample.
As for future work, firstly we plan to integrate
some named entity recognisers into our approach.
We also plan to try our approach in more do-
mains and on other language pairs (e.g. Japanese–
English). Finally, we intend to explore the corre-
lation between vocabulary size and the amount of
training data needed in order to achieve good re-
sults using our approach.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99973175">
This work is supported by Science Foundation Ire-
land (O5/IN/1732) and the Irish Centre for High-
End Computing.10 We would like to thank the re-
viewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.9837" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.789365142857143">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, MI.
</bodyText>
<footnote confidence="0.833637">
10http://www.ichec.ie/
</footnote>
<page confidence="0.987511">
556
</page>
<reference confidence="0.986650142857143">
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224–232, Columbus, OH.
Yonggang Deng and William Byrne. 2005. HMM
word and phrase alignment for statistical machine
translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
169–176, Vancouver, BC, Canada.
Yonggang Deng and William Byrne. 2006. MTTK:
An alignment toolkit for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, pages 265–268,
New York City, NY.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138–145, San Francisco,
CA.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1012–1020, Colum-
bus, OH.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 48–54, Edmonton, AL,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 304–
311, Prague, Czech Republic.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221–249.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Associationfor Com-
putational Linguistics, pages 311–318, Philadelphia,
PA.
Richard W Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377–404.
Andrea Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901–904, Denver, CO.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 168–
171, Jeju Island, Republic of Korea.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics, pages
836–841, Copenhagen, Denmark.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In ACL SIGHAN Workshop
2004, pages 122–128, Barcelona, Spain.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings
of the International Workshop on Spoken Language
Translation, pages 141–147, Pittsburgh, PA.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical ana-
lyzer ICTCLAS. In Proceedings ofSecond SIGHAN
Workshop on Chinese Language Processing, pages
184–187, Sappora, Japan.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216–223, Columbus, OH.
</reference>
<page confidence="0.997389">
557
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951864">
<title confidence="0.9998125">Bilingually Motivated Domain-Adapted Word Segmentation for Statistical Machine Translation</title>
<author confidence="0.999652">Yanjun Ma Andy Way</author>
<affiliation confidence="0.996542666666667">National Centre for Language Technology School of Computing Dublin City University</affiliation>
<address confidence="0.994551">Dublin 9, Ireland</address>
<abstract confidence="0.99853352173913">We introduce a word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PB-SMT). Inof using manually segmented monocorpora to train segmenters, we make use of bilingual corpora and statistical word alignment techniques. First of all, our approach is adapted for the specific translation task at hand by taking the corresponding source (target) language into account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1337" citStr="Brown et al., 1993" startWordPosition="194" endWordPosition="197">target) language into account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results. The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainspecific corpus, which is not adapted for the specific translation task at hand given that the manual segmentati</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="4718" citStr="Chang et al., 2008" startWordPosition="738" endWordPosition="741">rd segmentation approach. In section 4, we illustrate the adaptation of our decoder to this segmentation scheme. The experiments conducted in two different domains are reported in Section 5 and 6. We discuss related work in section 7. Section 8 concludes and gives avenues for future work. 2 The Influence of Word Segmentation on SMT: A Pilot Investigation The monolingual word segmentation step in traditional SMT systems has a substantial impact on the performance of such systems. A considerable amount of recent research has focused on the influence of word segmentation on SMT (Ma et al., 2007; Chang et al., 2008; Zhang et al., 2008); however, most explorations focused on the impact of various segmentation guidelines and the mechanisms of the segmenters themselves. A current research interest concerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1, LDC segmenter2 and Stanford segmenter version 2006-05- 113. </context>
<context position="8317" citStr="Chang et al., 2008" startWordPosition="1273" endWordPosition="1276">tperform character-based segmentation and the LDC segmenter is only slightly better than the other segmenters. From the experiments reported above, we can reach the following conclusions. First of all, character-based segmentation cannot achieve state-of-the-art results in most experimental SMT settings. This also motivates the necessity to work on better segmentation strategies. Second, monolingual segmenters cannot achieve consis4Interestingly, the developers themselves also note the sensitivity of the Stanford segmenter and incorporate external lexical information to address such problems (Chang et al., 2008). 550 40K 160K IWSLT06 CS 19.31 23.06 Manual 19.94 - ICT 20.34 23.36 LDC 20.37 24.34 Stanford 18.25 21.40 IWSLT07 CS 29.59 30.25 Manual 33.85 - ICT 31.18 33.38 LDC 31.74 33.44 Stanford 30.97 33.41 Table 2: Word segmentation on IWSLT data sets tently good results when used in another domain. In the following sections, we propose a bilingually motivated segmentation approach which can be automatically derived from a small representative data set and the experiments show that we can consistently obtain state-of-the-art results in different domains. 3 Bilingually Motivated Word Segmentation 3.1 No</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>169--176</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="1380" citStr="Deng and Byrne, 2005" startWordPosition="202" endWordPosition="205"> this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results. The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainspecific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context. C</context>
<context position="26975" citStr="Deng and Byrne, 2005" startWordPosition="4388" endWordPosition="4391">41 BS-SingleBest 22.45 30.76 BS-WordLattice 24.18 32.99 Table 9: Scale-up to 160K on IWSLT data sets 160K 640K CS 12.47 14.40 ICT 14.85 17.20 LDC 13.88 15.86 Stanford 15.26 16.94 BS-SingleBest 12.58 14.11 BS-WordLattice 13.74 15.33 Table 10: Scalability of BS on NIST task 555 6.5 Using different word aligners The above experiments rely on GIZA++ to perform word alignment. We next show that our approach is not dependent on the word aligner given that we have a conservative reliability estimation procedure. Table 11 shows the results obtained on the IWSLT data set using the MTTK alignment tool (Deng and Byrne, 2005; Deng and Byrne, 2006). IWSLT06 IWSLT07 CS 21.04 31.41 ICT 20.48 31.11 LDC 20.79 30.51 Stanford 17.84 29.35 BS-SingleBest 19.22 29.75 BS-WordLattice 21.76 31.75 Table 11: BS on IWSLT data sets using MTTK 7 Related Work (Xu et al., 2004) were the first to question the use of word segmentation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters. Our approach differs from theirs in two aspects. Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary an</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Yonggang Deng and William Byrne. 2005. HMM word and phrase alignment for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 169–176, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>MTTK: An alignment toolkit for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>265--268</pages>
<location>New York City, NY.</location>
<contexts>
<context position="26998" citStr="Deng and Byrne, 2006" startWordPosition="4392" endWordPosition="4395"> 30.76 BS-WordLattice 24.18 32.99 Table 9: Scale-up to 160K on IWSLT data sets 160K 640K CS 12.47 14.40 ICT 14.85 17.20 LDC 13.88 15.86 Stanford 15.26 16.94 BS-SingleBest 12.58 14.11 BS-WordLattice 13.74 15.33 Table 10: Scalability of BS on NIST task 555 6.5 Using different word aligners The above experiments rely on GIZA++ to perform word alignment. We next show that our approach is not dependent on the word aligner given that we have a conservative reliability estimation procedure. Table 11 shows the results obtained on the IWSLT data set using the MTTK alignment tool (Deng and Byrne, 2005; Deng and Byrne, 2006). IWSLT06 IWSLT07 CS 21.04 31.41 ICT 20.48 31.11 LDC 20.79 30.51 Stanford 17.84 29.35 BS-SingleBest 19.22 29.75 BS-WordLattice 21.76 31.75 Table 11: BS on IWSLT data sets using MTTK 7 Related Work (Xu et al., 2004) were the first to question the use of word segmentation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters. Our approach differs from theirs in two aspects. Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary and reuse this dictionary</context>
</contexts>
<marker>Deng, Byrne, 2006</marker>
<rawString>Yonggang Deng and William Byrne. 2006. MTTK: An alignment toolkit for statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, pages 265–268, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="17287" citStr="Doddington, 2002" startWordPosition="2811" endWordPosition="2812"> 70% agreement between native speakers on this task (Sproat et al., 1996). Second, an increase in F-score does not necessarily imply an improvement in translation quality. It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008). Consequently, we chose to extrinsically evaluate the performance of our approach via the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using BLEU, with NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) as complementary metrics. 5.2 Data The data we used in our experiments are from two different domains, namely news and travel dialogues. For the news domain, we trained our system using a portion of UN data for NIST 2006 evaluation campaign. The system was developed on LDC Multiple-Translation Chinese (MTC) Corpus and tested on MTC part 2, which was also used as a test set for NIST 2002 evaluation campaign. For the dialogue data, we used the Chinese– English datasets provided within the IWSLT 2007 evaluation campaign. Specifically, we used the standard tr</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1012--1020</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2302" citStr="Dyer et al., 2008" startWordPosition="348" endWordPosition="351">as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainspecific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context. Consequently, such segmenters cannot produce consistently good results when used across different domains. A substantial amount of research has been carried out to address the problems of word segmentation. However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008). One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre. Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007). The resulting segmentation will consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant</context>
<context position="14689" citStr="Dyer et al., 2008" startWordPosition="2372" endWordPosition="2375">on, each edge is a candidate word associated with its weights. A straightforward es6In case of overlap between several groups of words to replace, we select the one with the highest confidence (according to tAC). timation of the weights is to distribute the probability mass for each node uniformly to each outgoing edge. The single node having no outgoing edges is designated the “end node”. An example of word lattices for a Chinese sentence is shown in Figure 2. 4.2 Word Lattice Generation Previous research on generating word lattices relies on multiple monolingual segmenters (Xu et al., 2005; Dyer et al., 2008). One advantage of our approach is that the bilingually motivated segmentation process facilitates word lattice generation without relying on other segmenters. As described in section 3.4, the update of the training corpus based on the constructed bilingual dictionary requires that the sentence pair meets the bilingual constraints. Such a segmentation process in the training stage facilitates the utilisation of word lattice decoding. 4.3 Phrase-Based Word Lattice Decoding Given a Chinese input sentence cJ1 consisting of J characters, the traditional approach is to determine the best word segme</context>
<context position="18822" citStr="Dyer et al., 2008" startWordPosition="3064" endWordPosition="3067">ach, we used HIT corpus provided within IWSLT 2008 evaluation campaign. The various statistics for the corpora are shown in Table 3. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section 3.4. As previously mentioned, this process can be repeated several times. We then extract aligned phrases using the same procedure as for the baseline system; the only differenc</context>
<context position="28416" citStr="Dyer et al., 2008" startWordPosition="4621" endWordPosition="4624">ses the bilingual dictionary according to translation quality. (Ma et al., 2007) proposed an approach to improve word alignment by optimising the segmentation of both source and target languages. However, the reported experiments still rely on some monolingual segmenters and the issue of scalability is not addressed. Our research focuses on avoiding the use of monolingual segmenters in order to improve the robustness of segmenters across different domains. (Xu et al., 2005) were the first to propose the use of word lattice decoding in PB-SMT, in order to address the problems of segmentation. (Dyer et al., 2008) extended this approach to hierarchical SMT systems and other language pairs. However, both of these methods require some monolingual segmentation in order to generate word lattices. Our approach facilitates word lattice generation given that our segmentation is driven by the bilingual dictionary. 8 Conclusions and Future Work In this paper, we introduced a bilingually motivated word segmentation approach for SMT. The assumption behind this motivation is that the language to be segmented can be tokenised into basic writing units. Firstly, we extract 1-to-n word alignments using statistical wor</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1012–1020, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>48--54</pages>
<location>Edmonton, AL,</location>
<contexts>
<context position="18604" citStr="Koehn et al., 2003" startWordPosition="3029" endWordPosition="3032">e performance of the system was tested on both IWSLT 2006 and 2007 test sets. We used both test sets because they are quite different in terms of sentence length and vocabulary size. To test the scalability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign. The various statistics for the corpora are shown in Table 3. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single wor</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 48–54, Edmonton, AL, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6167" citStr="Koehn et al., 2007" startWordPosition="941" endWordPosition="944">ls were trained on news domain data with named entity recognition functionality. The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain. We used Chinese character-based and manual segmentations as contrastive segmentations. The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/Projects/ Chinese 3http://nlp.stanford.edu/software/ segmenter.shtml system—Moses (Koehn et al., 2007). The performance of PB-SMT system is measured with BLEU score (Papineni et al., 2002). We firstly measure the influence of word segmentation on in-domain data with respect to the three above mentioned segmenters, namely UN data from the NIST 2006 evaluation campaign. As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large. We can also observe that the ICT and Stanford se</context>
<context position="18802" citStr="Koehn et al., 2007" startWordPosition="3060" endWordPosition="3063">ability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign. The various statistics for the corpora are shown in Table 3. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section 3.4. As previously mentioned, this process can be repeated several times. We then extract aligned phrases using the same procedure as for the baseline system</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Nicolas Stroppa</author>
<author>Andy Way</author>
</authors>
<title>Bootstrapping word alignment via word packing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>304--311</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2661" citStr="Ma et al., 2007" startWordPosition="408" endWordPosition="411">ults when used across different domains. A substantial amount of research has been carried out to address the problems of word segmentation. However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008). One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre. Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007). The resulting segmentation will consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant language segmented into basic writing units (e.g. characters for Chinese or kana for Japanese). Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidate “words”. We evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associativ</context>
<context position="4698" citStr="Ma et al., 2007" startWordPosition="734" endWordPosition="737">ally motivated word segmentation approach. In section 4, we illustrate the adaptation of our decoder to this segmentation scheme. The experiments conducted in two different domains are reported in Section 5 and 6. We discuss related work in section 7. Section 8 concludes and gives avenues for future work. 2 The Influence of Word Segmentation on SMT: A Pilot Investigation The monolingual word segmentation step in traditional SMT systems has a substantial impact on the performance of such systems. A considerable amount of recent research has focused on the influence of word segmentation on SMT (Ma et al., 2007; Chang et al., 2008; Zhang et al., 2008); however, most explorations focused on the impact of various segmentation guidelines and the mechanisms of the segmenters themselves. A current research interest concerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1, LDC segmenter2 and Stanford segmenter ve</context>
<context position="11269" citStr="Ma et al., 2007" startWordPosition="1776" endWordPosition="1779">e need to make sure that we are not making harmful modifications. We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary. To perform this filtering, we use two simple statistical measures. In the following, ai = (Ci, ei) denotes a candidate. The first measure we consider is co-occurrence frequency (COOC(Ci, ei)), i.e. the number of times Ci and ei co-occur in the bilingual corpus. This very simple measure is frequently used in associative approaches (Melamed, 2000). The second measure is the alignment confidence (Ma et al., 2007), defined as C(ai) AC(ai) = COOC(Ci, ei), where C(ai) denotes the number of alignments proposed by the word aligner that are identical to ai. In other words, AC(ai) measures how often the aligner aligns Ci and ei when they co-occur. We also impose that |Ci |~ k, where k is a fixed integer that may depend on the language pair (between 3 and 5 in practice). The rationale behind this is that it is very rare to get reliable alignments between one word and k consecutive words when k is high. 5While in this paper we are primarily concerned with languages where the word boundaries are not orthographi</context>
<context position="27878" citStr="Ma et al., 2007" startWordPosition="4534" endWordPosition="4537">tation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters. Our approach differs from theirs in two aspects. Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary and reuse this dictionary to segment Chinese sentences as other monolingual segmenters. Our approach features the use of a bilingual dictionary and conducts a different segmentation. In addition, we add a process which optimises the bilingual dictionary according to translation quality. (Ma et al., 2007) proposed an approach to improve word alignment by optimising the segmentation of both source and target languages. However, the reported experiments still rely on some monolingual segmenters and the issue of scalability is not addressed. Our research focuses on avoiding the use of monolingual segmenters in order to improve the robustness of segmenters across different domains. (Xu et al., 2005) were the first to propose the use of word lattice decoding in PB-SMT, in order to address the problems of segmentation. (Dyer et al., 2008) extended this approach to hierarchical SMT systems and other </context>
</contexts>
<marker>Ma, Stroppa, Way, 2007</marker>
<rawString>Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304– 311, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="3307" citStr="Melamed, 2000" startWordPosition="509" endWordPosition="510">l consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant language segmented into basic writing units (e.g. characters for Chinese or kana for Japanese). Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidate “words”. We evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment (Melamed, 2000). We then modify the segmentation of the respective sentences in the parallel corpus according to these candidate words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the segmentation process on Chinese-to-English Machine Translation (MT) tasks in two different domains. The remainder of this paper is organised as folProceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computatio</context>
<context position="11203" citStr="Melamed, 2000" startWordPosition="1766" endWordPosition="1767"> to change the input segmentation to give to the word aligner, we need to make sure that we are not making harmful modifications. We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary. To perform this filtering, we use two simple statistical measures. In the following, ai = (Ci, ei) denotes a candidate. The first measure we consider is co-occurrence frequency (COOC(Ci, ei)), i.e. the number of times Ci and ei co-occur in the bilingual corpus. This very simple measure is frequently used in associative approaches (Melamed, 2000). The second measure is the alignment confidence (Ma et al., 2007), defined as C(ai) AC(ai) = COOC(Ci, ei), where C(ai) denotes the number of alignments proposed by the word aligner that are identical to ai. In other words, AC(ai) measures how often the aligner aligns Ci and ei when they co-occur. We also impose that |Ci |~ k, where k is a fixed integer that may depend on the language pair (between 3 and 5 in practice). The rationale behind this is that it is very rare to get reliable alignments between one word and k consecutive words when k is high. 5While in this paper we are primarily conc</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience,</title>
<date>1989</date>
<location>New York, NY.</location>
<contexts>
<context position="6983" citStr="Noreen, 1989" startWordPosition="1070" endWordPosition="1071">egmenters, namely UN data from the NIST 2006 evaluation campaign. As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large. We can also observe that the ICT and Stanford segmenter consistently outperform the LDC segmenter. Even using 3M sentence pairs for training, the differences between them are still statistically significant (p &lt; 0.05) using approximate randomisation (Noreen, 1989) for significance testing. 40K 160K 640K 3M CS 8.33 12.47 14.40 17.80 ICT 10.17 14.85 17.20 20.50 LDC 9.37 13.88 15.86 19.59 Stanford 10.45 15.26 16.94 20.64 Table 1: Word segmentation on NIST data sets However, when tested on out-of-domain data, i.e. IWSLT data in the dialogue domain, the results seem to be more difficult to predict. We trained the system on different sizes of data and evaluated the system on two test sets: IWSLT 2006 and 2007. From Table 2, we can see that on the IWSLT 2006 test sets, LDC achieves consistently good results and the Stanford segmenter is the worst.4 Furthermor</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="18521" citStr="Och and Ney, 2003" startWordPosition="3017" endWordPosition="3020">which we added devset1 and devset2. Devset4 was used to tune the parameters and the performance of the system was tested on both IWSLT 2006 and 2007 test sets. We used both test sets because they are quite different in terms of sentence length and vocabulary size. To test the scalability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign. The various statistics for the corpora are shown in Table 3. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="18644" citStr="Och, 2003" startWordPosition="3036" endWordPosition="3037">SLT 2006 and 2007 test sets. We used both test sets because they are quite different in terms of sentence length and vocabulary size. To test the scalability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign. The various statistics for the corpora are shown in Table 3. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6253" citStr="Papineni et al., 2002" startWordPosition="956" endWordPosition="959">e LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain. We used Chinese character-based and manual segmentations as contrastive segmentations. The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/Projects/ Chinese 3http://nlp.stanford.edu/software/ segmenter.shtml system—Moses (Koehn et al., 2007). The performance of PB-SMT system is measured with BLEU score (Papineni et al., 2002). We firstly measure the influence of word segmentation on in-domain data with respect to the three above mentioned segmenters, namely UN data from the NIST 2006 evaluation campaign. As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large. We can also observe that the ICT and Stanford segmenter consistently outperform the LDC segmenter. Even using 3M sentence pairs for tr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard W Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="16743" citStr="Sproat et al., 1996" startWordPosition="2722" endWordPosition="2725">B-SMT. {max Pr(eI1, fK1 |cJ1)} fK ,K = arg max {max z fK ,K e1,I = arg max {max z fK ,K e1,I 552 Figure 2: Example of a word lattice 5 Experimental Setting 5.1 Evaluation The intrinsic quality of word segmentation is normally evaluated against a manually segmented gold-standard corpus using F-score. While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations. First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996). Second, an increase in F-score does not necessarily imply an improvement in translation quality. It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008). Consequently, we chose to extrinsically evaluate the performance of our approach via the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using BLEU, with NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) as complementary </context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard W Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="18730" citStr="Stolcke, 2002" startWordPosition="3048" endWordPosition="3049">nt in terms of sentence length and vocabulary size. To test the scalability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign. The various statistics for the corpora are shown in Table 3. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section 3.4. As previously mentioned, this process can be repeated several times. We then ext</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andrea Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<location>Jeju Island, Republic of</location>
<contexts>
<context position="5524" citStr="Tseng et al., 2005" startWordPosition="858" endWordPosition="861">cerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1, LDC segmenter2 and Stanford segmenter version 2006-05- 113. Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al., 2003) and conditional random fields for the Stanford segmenter (Tseng et al., 2005). Both segmentation models were trained on news domain data with named entity recognition functionality. The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain. We used Chinese character-based and manual segmentations as contrastive segmentations. The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/Projects/ Chinese 3http://nlp.stanford.edu/software/ segmen</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of Fourth SIGHAN Workshop on Chinese Language Processing, pages 168– 171, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1357" citStr="Vogel et al., 1996" startWordPosition="198" endWordPosition="201">o account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results. The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainspecific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Do we need Chinese word segmentation for statistical machine translation?</title>
<date>2004</date>
<booktitle>In ACL SIGHAN Workshop</booktitle>
<pages>122--128</pages>
<location>Barcelona,</location>
<contexts>
<context position="27212" citStr="Xu et al., 2004" startWordPosition="4428" endWordPosition="4431"> Scalability of BS on NIST task 555 6.5 Using different word aligners The above experiments rely on GIZA++ to perform word alignment. We next show that our approach is not dependent on the word aligner given that we have a conservative reliability estimation procedure. Table 11 shows the results obtained on the IWSLT data set using the MTTK alignment tool (Deng and Byrne, 2005; Deng and Byrne, 2006). IWSLT06 IWSLT07 CS 21.04 31.41 ICT 20.48 31.11 LDC 20.79 30.51 Stanford 17.84 29.35 BS-SingleBest 19.22 29.75 BS-WordLattice 21.76 31.75 Table 11: BS on IWSLT data sets using MTTK 7 Related Work (Xu et al., 2004) were the first to question the use of word segmentation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters. Our approach differs from theirs in two aspects. Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary and reuse this dictionary to segment Chinese sentences as other monolingual segmenters. Our approach features the use of a bilingual dictionary and conducts a different segmentation. In addition, we add a process which optimises the biling</context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we need Chinese word segmentation for statistical machine translation? In ACL SIGHAN Workshop 2004, pages 122–128, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Integrated Chinese word segmentation in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>141--147</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="14669" citStr="Xu et al., 2005" startWordPosition="2368" endWordPosition="2371">f word segmentation, each edge is a candidate word associated with its weights. A straightforward es6In case of overlap between several groups of words to replace, we select the one with the highest confidence (according to tAC). timation of the weights is to distribute the probability mass for each node uniformly to each outgoing edge. The single node having no outgoing edges is designated the “end node”. An example of word lattices for a Chinese sentence is shown in Figure 2. 4.2 Word Lattice Generation Previous research on generating word lattices relies on multiple monolingual segmenters (Xu et al., 2005; Dyer et al., 2008). One advantage of our approach is that the bilingually motivated segmentation process facilitates word lattice generation without relying on other segmenters. As described in section 3.4, the update of the training corpus based on the constructed bilingual dictionary requires that the sentence pair meets the bilingual constraints. Such a segmentation process in the training stage facilitates the utilisation of word lattice decoding. 4.3 Phrase-Based Word Lattice Decoding Given a Chinese input sentence cJ1 consisting of J characters, the traditional approach is to determine</context>
<context position="28276" citStr="Xu et al., 2005" startWordPosition="4596" endWordPosition="4599"> Our approach features the use of a bilingual dictionary and conducts a different segmentation. In addition, we add a process which optimises the bilingual dictionary according to translation quality. (Ma et al., 2007) proposed an approach to improve word alignment by optimising the segmentation of both source and target languages. However, the reported experiments still rely on some monolingual segmenters and the issue of scalability is not addressed. Our research focuses on avoiding the use of monolingual segmenters in order to improve the robustness of segmenters across different domains. (Xu et al., 2005) were the first to propose the use of word lattice decoding in PB-SMT, in order to address the problems of segmentation. (Dyer et al., 2008) extended this approach to hierarchical SMT systems and other language pairs. However, both of these methods require some monolingual segmentation in order to generate word lattices. Our approach facilitates word lattice generation given that our segmentation is driven by the bilingual dictionary. 8 Conclusions and Future Work In this paper, we introduced a bilingually motivated word segmentation approach for SMT. The assumption behind this motivation is t</context>
</contexts>
<marker>Xu, Matusov, Zens, Ney, 2005</marker>
<rawString>Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated Chinese word segmentation in statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation, pages 141–147, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Hongkui Yu</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese lexical analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings ofSecond SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>184--187</pages>
<location>Sappora, Japan.</location>
<contexts>
<context position="5446" citStr="Zhang et al., 2003" startWordPosition="845" endWordPosition="848">d the mechanisms of the segmenters themselves. A current research interest concerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1, LDC segmenter2 and Stanford segmenter version 2006-05- 113. Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al., 2003) and conditional random fields for the Stanford segmenter (Tseng et al., 2005). Both segmentation models were trained on news domain data with named entity recognition functionality. The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain. We used Chinese character-based and manual segmentations as contrastive segmentations. The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) 1http://ictclas.org/index.html 2http:/</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu. 2003. HHMM-based Chinese lexical analyzer ICTCLAS. In Proceedings ofSecond SIGHAN Workshop on Chinese Language Processing, pages 184–187, Sappora, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>216--223</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2323" citStr="Zhang et al., 2008" startWordPosition="352" endWordPosition="355">step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainspecific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context. Consequently, such segmenters cannot produce consistently good results when used across different domains. A substantial amount of research has been carried out to address the problems of word segmentation. However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008). One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre. Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007). The resulting segmentation will consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant language segmented i</context>
<context position="4739" citStr="Zhang et al., 2008" startWordPosition="742" endWordPosition="745">oach. In section 4, we illustrate the adaptation of our decoder to this segmentation scheme. The experiments conducted in two different domains are reported in Section 5 and 6. We discuss related work in section 7. Section 8 concludes and gives avenues for future work. 2 The Influence of Word Segmentation on SMT: A Pilot Investigation The monolingual word segmentation step in traditional SMT systems has a substantial impact on the performance of such systems. A considerable amount of recent research has focused on the influence of word segmentation on SMT (Ma et al., 2007; Chang et al., 2008; Zhang et al., 2008); however, most explorations focused on the impact of various segmentation guidelines and the mechanisms of the segmenters themselves. A current research interest concerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1, LDC segmenter2 and Stanford segmenter version 2006-05- 113. Both ICTCLAS and Stan</context>
<context position="16973" citStr="Zhang et al., 2008" startWordPosition="2762" endWordPosition="2765">ated against a manually segmented gold-standard corpus using F-score. While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations. First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996). Second, an increase in F-score does not necessarily imply an improvement in translation quality. It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008). Consequently, we chose to extrinsically evaluate the performance of our approach via the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using BLEU, with NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) as complementary metrics. 5.2 Data The data we used in our experiments are from two different domains, namely news and travel dialogues. For the news domain, we trained our system using a portion of UN data for NIST 2006 evaluation campaign. The s</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 216–223, Columbus, OH.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>