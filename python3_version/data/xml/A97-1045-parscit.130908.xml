<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001307">
<title confidence="0.814021">
Construction and Visualization of Key Term Hierarchies
</title>
<note confidence="0.9164385">
Joe Zhou and Troy Tanner
LEXIS-NEXIS, a Division of Reed Elsevier
9555 Springboro Pike
Miamisburg, OH 45342
</note>
<email confidence="0.493722">
{joez, tlt}@lexis-nexis.com
</email>
<sectionHeader confidence="0.973506" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999634909090909">
This paper presents a prototype system for key
term manipulation and visualization in a real-world
commercial environment. The system consists of
two components. A preprocessor generates a set
of key terms from a text dataset which represents
a specific topic. The generated key terms are orga-
nized in a hierarchical structure and fed into a
graphic user interface (GUI). The friendly and inter-
active GUI toolkit allows the user to visualize the
key terms in context and explore the content of the
original dataset.
</bodyText>
<sectionHeader confidence="0.995827" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999993266666667">
As the amount of on-line text grows at an exponen-
tial rate, developing useful text analysis techniques
and tools to access information content from vari-
ous electronic sources is becoming increasingly
important. In this paper we present an applied
research prototype system that intends to accom-
plish two major tasks. First, a set of key terms,
ranging from single word terms to four word terms,
are automatically generated and organized in a
hierarchical structure out of a text dataset which
represents a specific topic. Second, a graphic user
interface (GUI) is established that provides the
domain expert or the user with an interactive envi-
ronment to visualize the key term hierarchy in the
context of the original dataset.
</bodyText>
<sectionHeader confidence="0.998658" genericHeader="method">
2. SYSTEM DESCRIPTION
</sectionHeader>
<bodyText confidence="0.997144846153846">
The ultimate goal of this prototype system is to
offer an automated toolkit which allows the domain
expert or the user to visualize and examine key
terms in a large information collection. Such a tool-
kit has proven to be useful in a number of real
applications. For example, it has helped us reduce
the time and manual effort needed to develop and
maintain our on-line document indexing and classi-
fication schemes.
The system consists of two components: a prepro-
cessing component for the automatic construction
of key terms and the front-end component for user-
guided graphic interface.
</bodyText>
<subsectionHeader confidence="0.949852">
2.1 Automatic Generation of Key Terms
</subsectionHeader>
<bodyText confidence="0.999973388888889">
Automatically identifying meaningful terms from
naturally running texts has been an important task
for information technologists. It is widely believed
that a set of good terms can be used to express the
content of the document. By capturing a set of
good terms, for example, relevant documents can
be searched and retrieved from a large document
collection. Though what constitutes a good term
still remains to be answered, we know that a good
term can be a word stem, a single word, a multiple
word term (a phrase), or simply a syntactic unit.
Various existing and workable term extraction tools
are either statistically driven, or linguistically
oriented, or some hybrid of the two. They all target
frequently co-occurring words in running text. The
earlier work of Choueka (1988) proposed a pure
frequency approach in which only quantitative
selection criteria were established and applied.
Church and Hanks (1990) introduced a statistical
measurement called mutual information for
extracting strongly associated or collocated words.
Tools like Xtract (Smadja 1993) were based on the
work of Church and others, but made a step
forward by incorporating various statistical
measurements like z-score and variance of
distribution, as well as shallow linguistic techniques
like part-of-speech tagging and lemmatization of
input data and partial parsing of raw output.
Exemplary linguistic approaches can be found in
the work by Strzalkowsky (1993) where a fast and
accurate syntactic parser is the prerequisite for the
selection of significant phrasal terms.
Different applications aim at different types of key
terms. For the purpose of generating key terms for
our prototype system, we have adopted a learn
data from data&amp;quot; approach. The novelty of this
</bodyText>
<page confidence="0.997505">
307
</page>
<bodyText confidence="0.999967064516129">
approach lies in the automatic comparison of two
sample datasets, a topic focused dataset based on
a predefined topic and a larger and more general
base dataset. The focused dataset is created by
the domain expert either through a submission of
an on-line search or through a compilation of
documents from a specific source. The
construction of the corresponding base dataset is
performed by pulling documents out of a number of
sources, such as news wires, newspapers,
magazines and legal databases. The intention is to
make the resulted corpora cover a much greater
variety of topics or domain subjects than the
focused dataset.
To identify interesting word patterns in both sam-
ples a set of statistical measures are applied. The
identification of single word terms is based on the
variation of a t-test. Two-word terms are captured
through the computation of mutual information
(Church et al. 1991), and an extension of mutual
information assists in extracting three-word and
four-word terms. Once the significant terms of
these four types are identified, a comparison algo-
rithm is applied to differentiate terms across the
two samples. If significant changes in the values of
certain statistical variables are detected, associ-
ated terms are selected from the focused sample
and included in the final generated lists. (For a
complete description of the algorithm and prelimi-
nary experiments, please refer to Zhou and Dap-
kus 1995.)
</bodyText>
<subsectionHeader confidence="0.999294">
2.2 Graphic User Interface (GUI)
</subsectionHeader>
<bodyText confidence="0.999993583333333">
We view our prototype system as a means to
achieve information visualization. Analogous to sci-
entific visualization that allows scientists to make
sense out of intellectually large data collections,
information visualization aims at organizing large
information spaces so that information technolo-
gists can visualize what is out there and how vari-
ous parts are related to each other (Robertson et
al. 1991). The guiding principle for building the GUI
component of our prototype system is to automate
the manual process of capturing information con-
tent out of large document collections.
</bodyText>
<subsubsectionHeader confidence="0.746499">
2.2.1 General Presentation
</subsubsectionHeader>
<bodyText confidence="0.999992">
The design of the GUI component relies on a num-
ber of well understood elements which include a
suggestive graphic design and a direct manipula-
tion metaphor to achieve an easy-to-learn user
interface. The layout of the graphic design is
intended to facilitate the quick comprehension of
the displayed information. The GUI component is
divided into two main areas, one for interacting with
key terms structures and one for browsing targeted
document collections.
The following descriptions should be viewed
together with the appropriate figures of the GUI
component. Figure 1, attached at the end of the
paper, represents the overall GUI picture. Figures 2
and 3 capture the area where the interaction with
the key term structures occurs. Figures 4 and 5
present the area for document browsing and key
terms selection. The topic illustrated in the figures
is the legal topic &amp;quot;Medical Malpractice&amp;quot;.
</bodyText>
<subsubsectionHeader confidence="0.472012">
2.2.2 Term Access Mechanism
</subsubsectionHeader>
<bodyText confidence="0.999851428571429">
The left area of the GUI component (see figures 2
and 3) is devoted to selecting, retrieving and oper-
ating on the key terms generated by the prepro-
cessing component of the prototype system. As
can be seen, the key terms, ranging from single
word terms to four word terms, are organized in a
tree structure. The tree is a two dimensional visual-
ization of the term hierarchy. Single word terms are
represented as root nodes and multiple word terms
can be positioned uniformly below the parent node
in the term hierarchy. The goal of the visualization
is to present the key term lists in such a way that a
high percentage of the hierarchy is visible with min-
imal scrolling.
</bodyText>
<figureCaption confidence="0.672552">
Figure 2
</figureCaption>
<page confidence="0.994008">
308
</page>
<bodyText confidence="0.989724893617021">
The user interaction is structured around term
retrieval and navigation as the top level user inter-
actions. The retrieval of the key terms is treated as
an iterative process in which the user may select
single world terms from the term hierarchy and
navigate to multiple word terms accordingly.
The user begins term navigation by selecting from
a list of available topics. In this case, the legal topic
&amp;quot;Medical Malpractice&amp;quot; (i.e., medmal3) is selected
(see figure 2). Often data structures are organized
linearly by some metric. Frequency of key term
usage is the metric used to organize and partition
the term hierarchy in an ascending numerical
order. The partitioning is necessary as it is difficult
to accommodate the large ratio of the term hierar-
chy on the screen. Currently, each partition con-
tains 100 root nodes (or folders), representing
single word terms. Once a partition has been
selected, the corresponding document collection is
loaded into the document browser. The browser
provides the user with the ability to quickly navigate
through the document collection to locate relevant
key terms.
Figure 3
The primary interaction with the key term hierarchy
is accomplished by direct manipulation of the tree
visualization. The user can select individual nodes
in the tree structure by pointing and clicking the
corresponding folders. When selecting nodes with
children, the tree will expand, resulting in the dis-
play of multiple word terms of the root key term. For
example, when &amp;quot;malpractice&amp;quot; is selected as the
root key term, a list of multiple word terms will be
displayed including multiple key terms such as
&amp;quot;medical malpractice&amp;quot;, &amp;quot;malpractice cases&amp;quot;, &amp;quot;medi-
cal malpractice action&amp;quot;, &amp;quot;medical malpractice
claims&amp;quot;, &amp;quot;limitations for medical malpractice&amp;quot;, etc.
(see figure 3)
Functionality to shrink and collapse subtrees is
also in place. When a term is selected from the
tree, a corresponding term lookup is conducted on
the document collection to locate the selected term
within the currently displayed document. Docu-
ments representing the four highest frequencies for
the selected term will be displayed first. Upon loca-
tion the selected term is always highlighted within
the document browser.
</bodyText>
<subsectionHeader confidence="0.667514">
2.2.3 Document Browsing Mechanism
</subsectionHeader>
<bodyText confidence="0.9849474">
The right area of the GUI component (see figures 4
and 5) is occupied by the document browser. The
design of the document browser is intended to pro-
vide an easy-to-learn interface for the management
and manipulation of the document collection.
There are three subwindows: the document identi-
fier window, the document window and the naviga-
tion window. The document identifier window
identifies the document that is currently displayed
in the document window. It shows the document id
and the total frequency of the selected key term in
the document collection. The document window
provides a view of the content of the targeted docu-
ment (see figure 4).
•This court reviews de now this Matra:4 court&apos;s determination IhatERISA •
preempts a state law claire Airports Co: u. Custom Benefit Servs. of Austin,
Int, 28 F.34 1082;1064 (1011).Cit,1944). ERISA preempts state ham that &amp;quot;
the is bend on state law Mel •Ipacillcare&apos;s
&apos;relate to* employee benefit plans. 24U.S.C,1144(e):.Them lens dispute here
that
plan is en employee benefit
plan. The issue is whether the I=.111= &apos;relates to&apos; the
Facticam plan.
&apos;A taw &apos;relates to&apos; en employee benefit plan, in the normal sense of the
phrase, fill has a Connection with or reference to such a plan.&apos; Shows. Dada
Air Lines, Inc., 483 U.S. 05, 96-97, 77 L. Ed. 24 440,183 S. CL 2040 (1983).
&apos;Mare is no idmple test for determining when a taw &apos;relates to&apos; a plan.&apos;&amp;quot;
Airports Co., 28 F.34 at 1064 (quoting National Elevator Indus., Inc. v. Coitus°
.851 F.24 1555, 1558 (10th Cir), cert. denied, 121 L Ed. 24 331, 113 S. Cl
408 (1992)). This court has IdeMitled the Meowing tour totogortes of lee&apos;s
which relate to an employee benefit plan:
&apos;First, ism that regulate the type of benefits or terms of ERISA piens. Spume
law; that mute yiporting, disclosure, funding, or vesting
requimmerds for ERISA plans Third, taws that provide rules for Ise
calculation of the amount or Gentrits to be paid under ERISA plans Fourth, I
</bodyText>
<page confidence="0.6917815">
Doe vito
2
</page>
<figureCaption confidence="0.984386">
Figure 4
</figureCaption>
<figure confidence="0.987000846153846">
medical malpractice
legal malpractice
,
malpractice action
Medical malpractice ertton
legal malpractice action
malpractice Osier •
frielpraCtiatiates
alleged malpractice
malpractice claims
Malpractice actions
malpractice caelit
TtsN
</figure>
<page confidence="0.997726">
309
</page>
<bodyText confidence="0.999842923076923">
The user can move through the document by mak-
ing use of the scroll bar, document buttons in the
navigation window, or by dragging the mouse up
and down while depressing the middle mouse but-
ton. The user can copy relevant key terms to a
holding area by selecting &amp;quot;Edit&amp;quot; from the menubar.
The user is presented with a popup dialog for
importing the selected key terms (see figure 5).
The navigation window enables the user to navi-
gate through the documents to view the selected
key terms in context. In addition, the user is pro-
vided with information regarding term frequencies
and term relevance ranking scores.
</bodyText>
<figure confidence="0.931596785714286">
t,file Edit
mal1.ctcQ
medic at malpractice
medical malpractice claim
medical malpractice soil
action tor medical malpractice
salon for medical malpractice
nefigont
wrongful death
Insurance company ,
emotional tHstress
dedberide indifference
health maintenance organization
legal malpractice action
</figure>
<figureCaption confidence="0.958175">
Figure 5
</figureCaption>
<subsectionHeader confidence="0.70333">
2.2.4 Implementation
</subsectionHeader>
<bodyText confidence="0.9998666875">
The GUI component described above is imple-
mented using the C++ programing language and
the OSF Motif graphical user interface toolkit. The
user interface consists of a small set of classes
that play various roles in the overall architecture.
The two major objects of the user interface interac-
tion model are the ListTree and the Document
Store objects.
ListTree is the primary class for implementing the
tree visualization. Operations for growing, shrinking
and manipulating the tree visualization have been
implemented.
Document Store provides the interface to docu-
ment collections. In particular, a document store
provides operations to create, modify and navigate
document collections.
</bodyText>
<sectionHeader confidence="0.997" genericHeader="method">
3. RESULTS OF USABILITY TESTING
</sectionHeader>
<bodyText confidence="0.999994655172414">
The prototype system, despite its prototype mode,
has proven to be useful and applicable in the com-
mercial business environment. Since the system is
in place, we have conducted a series of usability
testing within our company. The preliminary results
indicate that the system can provide internal spe-
cialized library developers, as well as subject
indexing domain experts with an ideal automated
toolkit to select and examine significant terms from
a sample dataset.
A number of general topics have been tested for
developing specialized libraries for our on-line
search system. These include four legal topics
&amp;quot;State Tax&amp;quot;, &amp;quot;Medical Malpractice&amp;quot;, &amp;quot;Uniform Com-
mercial Code&amp;quot;, and &amp;quot;Energy&amp;quot;, and three news top-
ics &amp;quot;Campaign&amp;quot;, &amp;quot;Legislature&amp;quot;, and &amp;quot;Executives&amp;quot;.
Specific subject indexing topics that have been
tested are &amp;quot;Advertising Expenditure&amp;quot;, &amp;quot;Intranet&amp;quot;,
&amp;quot;Job interview&amp;quot; and &amp;quot;Mutual fund&amp;quot;. Two sets of
questionnaires were filled out by the domain
experts who participated in the usability testing.
The overall ranking for the prototype system falls
between &amp;quot;somewhat useful&amp;quot; to &amp;quot;very useful&amp;quot;,
depending on the topics. They pointed out that the
system is particularly helpful when dealing with a
completely new or unfamiliar topic. It helps spot
significant terms which would normally be missed
and objectively examine the significance level of
certain fuzzy and ambiguous terms.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997330846153846">
K. Church and P. Hanks. Word association norms,
mutual information and lexicography.
Computational Linguistics, 16(1), March
1990.
K. Church, et al. Using statistics in lexical analysis.
In U. Zernik, editor, Lexical Acquisition:
Exploring On-line Resources to Build a
Lexicon, Lawrence Erlbaum Association,
1991.
Y. Choueka. Looking for needles in a haystack. In
Proceedings, R1AO, Conference on User-
Oriented Context Based Text and Image
Handling. Cambridge, MA. 1988.
</reference>
<page confidence="0.991487">
310
</page>
<reference confidence="0.991769411764706">
G. Robertson. Cone trees: Animated 3rd
visualizations of hierarchical information.
In proceedings SIGCHI &apos;91: Human
Factors in Computing Systems, pages
189-194. ACM, 1991.
F. Smadja. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1),
March 1993.
T. Strzalkowski. Document Indexing and Retrieval
Using Natural Language Processing. In
Proceedings, R1AO, New York, NY. 1994.
J. Zhou and P. Dapkus. Automatic Suggestion of
Significant Terms for a Predefined Topic.
In Proceedings of the 3rd Workshop on
Very Large Corpora, Association for
Computational Linguistics, MIT, Boston,
1995.
</reference>
<figure confidence="0.970693285714286">
File
e
an
I Frequericy
List
3opumentT0-1000173
CCurreriOes4 17
</figure>
<bodyText confidence="0.96528447826087">
Equal protection tights of medical malpractice plaintiffs. The Garcias
challenge Section 41-5-13 as a violation of the equal protection guarantee c
the New Me4co Constitution. N.M. Const an. 18. S.ecificall , the
Garcias claim that by requiring plaintiffs to file Priir1 rriM )r:+clic P claims
within three years of the act of malpractice regardless of the time at witilch the
plaintiff discovers his or her injury, Section 41-5-13 Infringes Anthony&apos;s
important interest in access to the cOurts. Further, the Genies claim that the
statute of repose conferred upon qualified health care providers does not beal
substantial relationship to the legislature&apos;s professed goal of alleviating the
insurance ctisls. See Roberts, 114 N.M. at 252, 257,837 P.2d at 448,451
(holding that the limitations period contained within iamreaxenes Act i
a &apos;benefit&apos; of the Act available ant to ualified health care providers and
holding that a cause Of action for lipri,c si nsIrtsctc against a nonqualified
health care provider Is governed by the discovery rule). We conclude that
Section 41-5-13 does not implicate the equal protection rights of medical
malpractice plaintiffs.
- Discriminatory classMcations. The basic guarantee of the Equal Prntectiol
Clause of the New Mexico Constitution is that the legislature may not enact a
statutewhich treats similarly situated persons differently. See Gruschus v.
Bureau of Revenue, 74 N.M. 775, 778,399 P.24I I 07 C1965) (stating that
satisfy mandates of equal protection, legislative classifications must be &amp;quot;s0
flamed as to embrace equally all who may be in like circumstances and
situations&apos;). in order to raise a claim that a statute has violated this basic
</bodyText>
<figure confidence="0.9873165625">
Documentr.. .:1 rig
2 3
malpractice
elAIC al Apra e
iegat malpractice
malpractice action
medical malpractice acti
legal malpractice action
malpractice claim
malpractice cases
alleged malpractice
malpractice claims
malpractice actions
malpractice case
301
714
</figure>
<figureCaption confidence="0.980752">
Figure 1
</figureCaption>
<page confidence="0.996624">
311
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917104">
<title confidence="0.999949">Construction and Visualization of Key Term Hierarchies</title>
<author confidence="0.999769">Joe Zhou</author>
<author confidence="0.999769">Troy Tanner</author>
<affiliation confidence="0.943355">LEXIS-NEXIS, a Division of Reed Elsevier</affiliation>
<address confidence="0.999291">9555 Springboro Pike Miamisburg, OH 45342</address>
<email confidence="0.998977">joez@lexis-nexis.com</email>
<email confidence="0.998977">tlt@lexis-nexis.com</email>
<abstract confidence="0.997771083333333">This paper presents a prototype system for key term manipulation and visualization in a real-world commercial environment. The system consists of two components. A preprocessor generates a set of key terms from a text dataset which represents a specific topic. The generated key terms are organized in a hierarchical structure and fed into a graphic user interface (GUI). The friendly and interactive GUI toolkit allows the user to visualize the key terms in context and explore the content of the original dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="3024" citStr="Church and Hanks (1990)" startWordPosition="481" endWordPosition="484">can be searched and retrieved from a large document collection. Though what constitutes a good term still remains to be answered, we know that a good term can be a word stem, a single word, a multiple word term (a phrase), or simply a syntactic unit. Various existing and workable term extraction tools are either statistically driven, or linguistically oriented, or some hybrid of the two. They all target frequently co-occurring words in running text. The earlier work of Choueka (1988) proposed a pure frequency approach in which only quantitative selection criteria were established and applied. Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words. Tools like Xtract (Smadja 1993) were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output. Exemplary linguistic approaches can be found in the work by Strzalkowsky (1993) where a fast and accurate syntactic parser is the prerequisite for the</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. Church and P. Hanks. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1), March 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>Using statistics in lexical analysis.</title>
<date>1991</date>
<booktitle>Lexical Acquisition: Exploring On-line Resources to Build a Lexicon, Lawrence Erlbaum Association,</booktitle>
<editor>In U. Zernik, editor,</editor>
<marker>Church, 1991</marker>
<rawString>K. Church, et al. Using statistics in lexical analysis. In U. Zernik, editor, Lexical Acquisition: Exploring On-line Resources to Build a Lexicon, Lawrence Erlbaum Association, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
</authors>
<title>Looking for needles in a haystack.</title>
<date>1988</date>
<booktitle>In Proceedings, R1AO, Conference on UserOriented Context Based Text and Image Handling.</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="2889" citStr="Choueka (1988)" startWordPosition="464" endWordPosition="465">d terms can be used to express the content of the document. By capturing a set of good terms, for example, relevant documents can be searched and retrieved from a large document collection. Though what constitutes a good term still remains to be answered, we know that a good term can be a word stem, a single word, a multiple word term (a phrase), or simply a syntactic unit. Various existing and workable term extraction tools are either statistically driven, or linguistically oriented, or some hybrid of the two. They all target frequently co-occurring words in running text. The earlier work of Choueka (1988) proposed a pure frequency approach in which only quantitative selection criteria were established and applied. Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words. Tools like Xtract (Smadja 1993) were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output. Exemplary lingu</context>
</contexts>
<marker>Choueka, 1988</marker>
<rawString>Y. Choueka. Looking for needles in a haystack. In Proceedings, R1AO, Conference on UserOriented Context Based Text and Image Handling. Cambridge, MA. 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Robertson</author>
</authors>
<title>Cone trees: Animated 3rd visualizations of hierarchical information.</title>
<date>1991</date>
<booktitle>In proceedings SIGCHI &apos;91: Human Factors in Computing Systems,</booktitle>
<pages>189--194</pages>
<publisher>ACM,</publisher>
<marker>Robertson, 1991</marker>
<rawString>G. Robertson. Cone trees: Animated 3rd visualizations of hierarchical information. In proceedings SIGCHI &apos;91: Human Factors in Computing Systems, pages 189-194. ACM, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Retrieving collocations from text: Xtract.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="3175" citStr="Smadja 1993" startWordPosition="502" endWordPosition="503">a word stem, a single word, a multiple word term (a phrase), or simply a syntactic unit. Various existing and workable term extraction tools are either statistically driven, or linguistically oriented, or some hybrid of the two. They all target frequently co-occurring words in running text. The earlier work of Choueka (1988) proposed a pure frequency approach in which only quantitative selection criteria were established and applied. Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words. Tools like Xtract (Smadja 1993) were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output. Exemplary linguistic approaches can be found in the work by Strzalkowsky (1993) where a fast and accurate syntactic parser is the prerequisite for the selection of significant phrasal terms. Different applications aim at different types of key terms. For the purpose of generating key terms for our pr</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>F. Smadja. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1), March 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
</authors>
<title>Document Indexing and Retrieval Using Natural Language Processing.</title>
<date>1994</date>
<booktitle>In Proceedings, R1AO,</booktitle>
<location>New York, NY.</location>
<marker>Strzalkowski, 1994</marker>
<rawString>T. Strzalkowski. Document Indexing and Retrieval Using Natural Language Processing. In Proceedings, R1AO, New York, NY. 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhou</author>
<author>P Dapkus</author>
</authors>
<title>Automatic Suggestion of Significant Terms for a Predefined Topic.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora, Association for Computational Linguistics,</booktitle>
<location>MIT, Boston,</location>
<contexts>
<context position="5296" citStr="Zhou and Dapkus 1995" startWordPosition="837" endWordPosition="841">word terms are captured through the computation of mutual information (Church et al. 1991), and an extension of mutual information assists in extracting three-word and four-word terms. Once the significant terms of these four types are identified, a comparison algorithm is applied to differentiate terms across the two samples. If significant changes in the values of certain statistical variables are detected, associated terms are selected from the focused sample and included in the final generated lists. (For a complete description of the algorithm and preliminary experiments, please refer to Zhou and Dapkus 1995.) 2.2 Graphic User Interface (GUI) We view our prototype system as a means to achieve information visualization. Analogous to scientific visualization that allows scientists to make sense out of intellectually large data collections, information visualization aims at organizing large information spaces so that information technologists can visualize what is out there and how various parts are related to each other (Robertson et al. 1991). The guiding principle for building the GUI component of our prototype system is to automate the manual process of capturing information content out of large</context>
</contexts>
<marker>Zhou, Dapkus, 1995</marker>
<rawString>J. Zhou and P. Dapkus. Automatic Suggestion of Significant Terms for a Predefined Topic. In Proceedings of the 3rd Workshop on Very Large Corpora, Association for Computational Linguistics, MIT, Boston, 1995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>