<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002719">
<title confidence="0.9991155">
Dependency Parser Adaptation with Subtrees
from Auto-Parsed Target Domain Data
</title>
<author confidence="0.997535">
Xuezhe Ma
</author>
<affiliation confidence="0.9980445">
Department of Linguistics
University of Washington
</affiliation>
<address confidence="0.785234">
Seattle, WA 98195, USA
</address>
<email confidence="0.999351">
xzma@uw.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858714285714">
In this paper, we propose a simple and
effective approach to domain adaptation
for dependency parsing. This is a fea-
ture augmentation approach in which the
new features are constructed based on sub-
tree information extracted from the auto-
parsed target domain data. To demon-
strate the effectiveness of the proposed ap-
proach, we evaluate it on three pairs of
source-target data, compared with sever-
al common baseline systems and previous
approaches. Our approach achieves signif-
icant improvement on all the three pairs of
data sets.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999771608695652">
In recent years, several dependency parsing algo-
rithms (Nivre and Scholz, 2004; McDonald et al.,
2005a; McDonald et al., 2005b; McDonald and
Pereira, 2006; Carreras, 2007; Koo and Collins,
2010; Ma and Zhao, 2012) have been proposed
and achieved high parsing accuracies on several
treebanks of different languages. However, the
performance of such parsers declines when train-
ing and test data come from different domain-
s. Furthermore, the manually annotated treebanks
that these parsers rely on are highly expensive to
create. Therefore, developing dependency pars-
ing algorithms that can be easily ported from one
domain to another—say, from a resource-rich do-
main to a resource-poor domain—is of great im-
portance.
Several approaches have been proposed for the
task of parser adaptation. McClosky et at. (2006)
successfully applied self-training to domain adap-
tation for constituency parsing using the rerank-
ing parser of Charniak and Johnson (2005). Re-
ichart and Rappoport (2007) explored self-training
when the amount of the annotated data is small
</bodyText>
<author confidence="0.849608">
Fei Xia
</author>
<affiliation confidence="0.994243">
Department of Linguistics
University of Washington
</affiliation>
<address confidence="0.598636">
Seattle, WA 98195, USA
</address>
<email confidence="0.992663">
fxia@uw.edu
</email>
<bodyText confidence="0.999963">
and achieved significant improvement. Zhang and
Wang (2009) enhanced the performance of depen-
dency parser adaptation by utilizing a large-scale
hand-crafted HPSG grammar. Plank and van No-
ord (2011) proposed a data selection method based
on effective measures of domain similarity for de-
pendency parsing.
There are roughly two varieties of domain adap-
tation problem—fully supervised case in which
there are a small amount of labeled data in the
target domain, and semi-supervised case in which
there are no labeled data in the target domain. In
this paper, we present a parsing adaptation ap-
proach focused on the fully supervised case. It is a
feature augmentation approach in which the new
features are constructed based on subtree infor-
mation extracted from the auto-parsed target do-
main data. For evaluation, we run experiments
on three pairs of source-target domains—WSJ-
Brown, Brown-WSJ, and WSJ-Genia. Our ap-
proach achieves significant improvement on al-
l these data sets.
</bodyText>
<sectionHeader confidence="0.670346" genericHeader="method">
2 Our Approach for Parsing Adaptation
</sectionHeader>
<bodyText confidence="0.999952555555556">
Our approach is inspired by Chen et al. (2009)’s
work on semi-supervised parsing with addition-
al subtree-based features extracted from unlabeled
data and by the feature augmentation method pro-
posed by Daume III (2007). In this section, we
first summarize Chen et al.’s work and explain
how we extend that for domain adaptation. We
will then highlight the similarity and difference
between our work and Daume’s method.
</bodyText>
<subsectionHeader confidence="0.9634685">
2.1 Semi-supervised parsing with
subtree-based features
</subsectionHeader>
<bodyText confidence="0.9972396">
One of the most well-known semi-supervised
parsing methods is self-training, where a parser
trained from the labeled data set is used to parse
unlabeled data, and some of those auto-parsed data
are added to the labeled data set to retrain the pars-
</bodyText>
<page confidence="0.980449">
585
</page>
<bodyText confidence="0.926287578947368">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 585–590,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
ing models. Chen et al. (2009)’s approach differs
from self-training in that partial information (i.e.,
subtrees), instead of the entire trees, from the auto-
parsed data is used to re-train the parsing models.
A subtree is a small part of a dependency
tree. For example, a first-order subtree is a single
edge consisting of a head and a dependent, and a
second-order sibling subtree is one that consists of
a head and two dependents. In Chen et al. (2009),
they first extract all the subtrees in auto-parsed da-
ta and store them in a list Lst. Then they count
the frequency of these subtrees and divide them
into three groups according to their levels of fre-
quency. Finally, they construct new features for
the subtrees based on which groups they belongs
to and retrain a new parser with feature-augmented
training data.1
</bodyText>
<subsectionHeader confidence="0.988307">
2.2 Parser adaptation with subtree-based
Features
</subsectionHeader>
<bodyText confidence="0.999241058823529">
Chen et al. (2009)’s work is for semi-supervised
learning, where the labeled training data and the
test data come from the same domain; the subtree-
based features collected from auto-parsed data are
added to all the labeled training data to retrain the
parsing model. In the supervised setting for do-
main adaptation, there is a large amount of labeled
data in the source domain and a small amount of
labeled data in the target domain. One intuitive
way of applying Chen’s method to this setting is to
simply take the union of the labeled training data
from both domains and add subtree-based features
to all the data in the union when re-training the
parsing model. However, it turns out that adding
subtree-based features to only the labeled training
data in the target domain works better. The steps
of our approach are as follows:
</bodyText>
<listItem confidence="0.980952181818182">
1. Train a baseline parser with the small amount
of labeled data in the target domain and use
the parser to parse the large amount of unla-
beled sentences in the target domain.
2. Extract subtrees from the auto-parsed data
and add subtree-based features to the labeled
training data in the target domain.
3. Retrain the parser with the union of the la-
beled training data in the two domains, where
the instances from the target domain are aug-
mented with the subtree-based features.
</listItem>
<footnote confidence="0.629101">
1If a subtree does not appear in Lit, it falls to the fourth
group for “unseen subtrees”.
</footnote>
<bodyText confidence="0.994691555555555">
To state our feature augmentation approach
more formally, we use X to denote the input s-
pace, and Ds and Dt to denote the labeled da-
ta in the source and target domains, respective-
ly. Let X′ be the augmented input space, and Φs
and Φt be the mappings from X to X′ for the in-
stances in the source and target domains respec-
tively. The mappings are defined by Eq 1, where
0 =&lt; 0, 0, ... , 0 &gt;E X is the zero vector.
</bodyText>
<equation confidence="0.999885">
Φs(xorg) = &lt; xorg, 0 &gt;
Φt(xorg) = &lt; xorg, xnew &gt; (1)
</equation>
<bodyText confidence="0.9997725625">
Here, xorg is the original feature vector in X,
and xnew is the vector of the subtree-based fea-
tures extracted from auto-parsed data of the target
domain. The subtree extraction method used in
our approach is the same as in (Chen et al., 2009)
except that we use different thresholds when di-
viding subtrees into three frequency groups: the
threshold for the high-frequency level is TOP 1%
of the subtrees, the one for the middle-frequency
level is TOP 10%, and the rest of subtrees belong
to the low-frequency level. These thresholds are
chosen empirically on some development data set.
The idea of distinguishing the source and tar-
get data is similar to the method in (Daume III,
2007), which did feature augmentation by defin-
ing the following mappings:2
</bodyText>
<equation confidence="0.999787">
Φs(xorg) = &lt; xorg, 0 &gt;
Φt(xorg) = &lt; xorg, xorg &gt; (2)
</equation>
<bodyText confidence="0.999607416666667">
Daume III showed that differentiating features
from the source and target domains improved per-
formance for multiple NLP tasks. The difference
between that study and our approach is that our
new features are based on subtree information in-
stead of copies of original features. Since the new
features are based on the subtree information ex-
tracted from the auto-parsed target data, they rep-
resent certain properties of the target domain and
that explains why adding them to the target data
works better than adding them to both the source
and target data.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998791">
For evaluation, we tested our approach on three
pairs of source-target data and compared it with
</bodyText>
<footnote confidence="0.991891666666667">
2The mapping in Eq 2 looks different from the one pro-
posed in (Daume III, 2007), but it can be proved that the two
are equivalent.
</footnote>
<page confidence="0.997461">
586
</page>
<bodyText confidence="0.999406285714286">
several common baseline systems and previous
approaches. In this section, we first describe the
data sets and parsing models used in each of the
three experiments in section 3.1. Then we pro-
vide a brief introduction to the systems we have
reimplemented for comparison in section 3.2. The
experimental results are reported in section 3.3.
</bodyText>
<subsectionHeader confidence="0.999257">
3.1 Data and Tools
</subsectionHeader>
<bodyText confidence="0.999983435897436">
In the first two experiments, we used the Wal-
l Street Journal (WSJ) and Brown (B) portion-
s of the English Penn TreeBank (Marcus et al.,
1993). In the first experiment denoted by “WSJ-
to-B”, WSJ corpus is used as the source domain
and Brown corpus as the target domain. In the
second experiment, we use the reverse order of
the two corpora and denote it by “B-to-WSJ”. The
phrase structures in the treebank are converted into
dependencies using Penn2Malt tool3 with the stan-
dard head rules (Yamada and Matsumoto, 2003).
For the WSJ corpus, we used the standard data
split: sections 2-21 for training and section 23 for
test. In the experiment of B-to-WSJ, we random-
ly selected about 2000 sentences from the training
portion of WSJ as the labeled data in the target do-
main. The rest of training data in WSJ is regarded
as the unlabeled data of the target domain.
For Brown corpus, we followed Reichart and
Rappoport (2007) for data split. The training and
test sections consist of sentences from all of the
genres that form the corpus. The training portion
consists of 90% (9 of each 10 consecutive sen-
tences) of the data, and the test portion is the re-
maining 10%. For the experiment of WSJ-to-B,
we randomly selected about 2000 sentences from
training portion of Brown and use them as labeled
data and the rest as unlabeled data in the target do-
main.
In the third experiment denoted by ’“WSJ-to-
G”, we used WSJ corpus as the source domain and
Genia corpus (G)4 as the target domain. Following
Plank and van Noord (2011), we used the train-
ing data in CoNLL 2008 shared task (Surdeanu
et al., 2008) which are also from WSJ sections
2-21 but converted into dependency structure by
the LTH converter (Johansson and Nugues, 2007).
The Genia corpus is converted to CoNLL format
with LTH converter, too. We randomly selected
</bodyText>
<footnote confidence="0.99732325">
3http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html
4Genia distribution in Penn Treebank format is avail-
able at http://bllip.cs.brown.edu/download/genia1.0-division-
rel1.tar.gz
</footnote>
<table confidence="0.9992018">
Source training Target test
training unlabeled
WSJ-to-B 39,832 2,182 19,632 2,429
B-to-WSJ 21,814 2,097 37,735 2,416
WSJ-to-G 39,279 1,024 13,302 1,360
</table>
<tableCaption confidence="0.8640805">
Table 1: The number of sentences for each data set
used in our experiments
</tableCaption>
<bodyText confidence="0.99926196">
about 1000 sentences from the training portion of
Genia data and use them as the labeled data of the
target domain, and the rest of training data of Ge-
nia as the unlabeled data of the target domain. Ta-
ble 1 shows the number of sentences of each data
set used in the experiments.
The dependency parsing models we used in this
study are the graph-based first-order and second-
order sibling parsing models (McDonald et al.,
2005a; McDonald and Pereira, 2006). To be more
specific, we use the implementation of MaxPars-
er5 with 10-best MIRA (Crammer et al., 2006; M-
cDonald, 2006) learning algorithm and each pars-
er is trained for 10 iterations. The feature sets of
first-order and second-order sibling parsing mod-
els used in our experiments are the same as the
ones in (Ma and Zhao, 2012). The input to Max-
Parser are sentences with Part-of-Speech tags; we
use gold-standard POS tags in the experiments.
Parsing accuracy is measured with unlabeled at-
tachment score (UAS) and the percentage of com-
plete matches (CM) for the first and second experi-
ments. For the third experiment, we also report la-
beled attachment score (LAS) in order to compare
with the results in (Plank and van Noord, 2011).
</bodyText>
<subsectionHeader confidence="0.999379">
3.2 Comparison Systems
</subsectionHeader>
<bodyText confidence="0.999006769230769">
For comparison, we re-implemented the follow-
ing well-known baselines and previous approach-
es, and tested them on the three data sets:
SrcOnly: Train a parser with the labeled data
from the source domain only.
TgtOnly: Train a parser with the labeled data
from the target domain only.
Src&amp;Tgt: Train a parser with the labeled data
from the source and target domains.
Self-Training: Following Reichart and Rap-
poport (2007), we train a parser with the
union of the source and target labeled data,
parse the unlabeled data in the target domain,
</bodyText>
<footnote confidence="0.981938">
5http://sourceforge.net/projects/maxparser/
</footnote>
<page confidence="0.997278">
587
</page>
<bodyText confidence="0.998471952380953">
add the entire auto-parsed trees to the man-
ually labeled data in a single step without
checking their parsing quality, and retrain the
parser.
Co-Training: In the co-training system, we first
train two parsers with the labeled data from
the source and target domains, respectively.
Then we use the parsers to parse unlabeled
data in the target domain and select sentences
for which the two parsers produce identical
trees. Finally, we add the analyses for those
sentences to the union of the source and tar-
get labeled data to retrain a new parser. This
approach is similar to the one used in (Sagae
and Tsujii, 2007), which achieved the highest
scores in the domain adaptation track of the
CoNLL 2007 shared task (Nivre et al., 2007).
Feature-Augmentation: This is the approach
proposed in (Daume III, 2007).
Chen et al. (2009): The algorithm has been ex-
plained in Section 2.1. We use the union of
the labeled data from the source and target
domains as the labeled training data. The
unlabeled data needed to construct subtree-
based features come from the target domain.
Plank and van Noord (2011): This system per-
forms data selection on a data pool consisting
of large amount of labeled data to get a train-
ing set that is similar to the test domain. The
results of the system come from their paper,
not from the reimplementation of their sys-
tem.
Per-corpus: The parser is trained with the large
training set from the target domain. For ex-
ample, for the experiment of WSJ-to-B, all
the labeled training data from the Brown cor-
pus is used for training, including the subset
of data which are treated as unlabeled in our
approach and other comparison systems. The
results serve as an upper bound of domain
adaptation when there is a large amount of
labeled data in the target domain.
</bodyText>
<subsectionHeader confidence="0.765674">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.98602875">
Table 2 illustrates the results of our approach with
the first-order parsing model in the first and sec-
ond experiments, together with the results of the
comparison systems described in section 3.2. The
</bodyText>
<table confidence="0.999457363636364">
WSJ-to-B B-to-WSJ
UAS CM UAS CM
SrcOnlys 88.8 43.8 86.3 26.5
TgtOnlyt 86.6 38.8 88.2 29.3
Src&amp;Tgts,t 89.1 44.3 89.4 31.2
Self-Trainings,t 89.2 45.1 89.8 32.1
Co-Trainings,t 89.2 45.1 89.8 32.7
Feature-Augs,t 89.1 45.1 89.8 32.8
Chen (2009)s,t 89.3 45.0 89.7 31.8
this papers,t 89.5 45.5 90.2 33.4
Per-corpusT 89.9 47.0 92.7 42.1
</table>
<tableCaption confidence="0.65741125">
Table 2: Results with the first-order parsing model
in the first and second experiments. The super-
script indicates the source of labeled data used in
training.
</tableCaption>
<table confidence="0.999522181818182">
WSJ-to-B B-to-WSJ
UAS CM UAS CM
SrcOnlys 89.8 47.3 88.0 30.4
TgtOnlyt 87.7 42.2 89.7 34.2
Src&amp;Tgts,t 90.2 48.2 90.9 36.6
Self-Trainings,t 90.3 48.8 91.0 37.1
Co-Trainings,t 90.3 48.5 90.9 38.0
Feature-Augs,t 90.0 48.4 91.0 37.4
Chen (2009)s,t 90.3 49.1 91.0 37.6
this papers,t 90.6 49.6 91.5 38.8
Per-corpusT 91.1 51.1 93.6 47.9
</table>
<tableCaption confidence="0.8029665">
Table 3: Results with the second-order sibling
parsing model in the first and second experiments.
</tableCaption>
<bodyText confidence="0.996901666666667">
results with the second-order sibling parsing mod-
el is shown in Table 3. The superscript s, t and T
indicates from which domain the labeled data are
used in training: tag s refers to the labeled data in
the source domain, tag t refers to the small amount
of labeled data in the target domain, and tag T in-
dicates that all the labeled training data from the
target domain, including the ones that are treated
as unlabeled in our approach, are used for training.
Table 4 shows the results in the third experimen-
t with the first-order parsing model. We also in-
clude the result from (Plank and van Noord, 2011),
which use the same parsing model as ours. Note
that this result is not comparable with other num-
bers in the table as it uses a larger set of labeled
data, as indicated by the † superscript.
All three tables show that our system out-
performs the comparison systems in all three
</bodyText>
<page confidence="0.993873">
588
</page>
<table confidence="0.999291583333333">
WSJ-to-G
UAS LAS
SrcOnlys 83.8 82.0
TgtOnlyt 87.0 85.7
Src&amp;Tgts,t 87.2 85.9
Self-Trainings,t 87.3 86.0
Co-Trainings,t 87.3 86.0
Feature-Augs,t 87.9 86.5
Chen (2009)s,t 87.5 86.2
this papers,t 88.4 87.1
Plank (2011)† - 86.8
Per-corpusT 90.5 89.7
</table>
<tableCaption confidence="0.997568">
Table 4: Results with first-order parsing model in
</tableCaption>
<bodyText confidence="0.995801823529412">
the third experiment. “Plank (2011)” refers to the
approach in Plank and van Noord (2011).
experiments.6 The improvement of our ap-
proach over the feature augmentation approach
in Daume III (2007) indicates that adding subtree-
based features provides better results than making
several copies of the original features. Our system
outperforms the system in (Chen et al., 2009), im-
plying that adding subtree-based features to only
the target labeled data is better than adding them
to the labeled data in both the source and target
domains.
Considering the three steps of our approach in
Section 2.2, the training data used to train the pars-
er in Step 1 can be from the target domain only or
from the source and target domains. Similarly, in
Step 3 the subtree-based features can be added to
the labeled data from the target domain only or
from the source and target domains. Therefore,
there are four combinations. Our approach is the
one that uses the labeled data from the target do-
main only in both steps, and Chen’s system uses
labeled data from the source and target domains in
both steps. Table 5 compares the performance of
the final parser in the WSJ-to-Genia experimen-
t when the parser is created with one of the four
combinations. The column label and the row label
indicate the choice in Step 1 and 3, respectively.
The table shows the choice in Step 1 does not have
a significant impact on the performance of the fi-
nal models; in contrast, the choice in Step 3 does
matter— adding subtree-based features to the la-
beled data in the target domain only is much better
than adding features to the data in both domains.
</bodyText>
<footnote confidence="0.977657">
6The results of Per-corpus are better than ours but it uses
a much larger labeled training set in the target domain.
</footnote>
<table confidence="0.988516666666667">
TgtOnly Src&amp;Tgt
TgtOnly 88.4/87.1 88.4/87.1
Src&amp;Tgt 87.6/86.3 87.5/86.2
</table>
<tableCaption confidence="0.991283">
Table 5: The performance (UAS/LAS) of the fi-
</tableCaption>
<bodyText confidence="0.968507">
nal parser in the WSJ-to-Genia experiment when
different training data are used to create the final
parser. The column label and row label indicate
the choice of the labeled data used in Step 1 and 3
of the process described in Section 2.2.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999585">
In this paper, we propose a feature augmentation
approach for dependency parser adaptation which
constructs new features based on subtree informa-
tion extracted from auto-parsed data from the tar-
get domain. We distinguish the source and target
domains by adding the new features only to the
data from the target domain. The experimental re-
sults on three source-target domain pairs show that
our approach outperforms all the comparison sys-
tems.
For the future work, we will explore the po-
tential benefits of adding other types of features
extracted from unlabeled data in the target do-
main. We will also experiment with various ways
of combining our current approach with other do-
main adaptation methods (such as self-training
and co-training) to further improve system perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996564157894737">
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CONLL, pages 957–961.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative r-
eranking. In Proceedings of the 43rd Meeting of
the Association for Computional Linguistics (ACL
2005), pages 132–139.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimo-
to, and Kentaro Torisawa. 2009. Improving de-
pendency parsing with subtrees from auto-parsed
data. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 570–579, Singapore, August.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Jornal of Machine
Learning Research, 7:551–585.
</reference>
<page confidence="0.994199">
589
</page>
<reference confidence="0.999519925531915">
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics (A-
CL 2007), pages 256–263, Prague, Czech Republic,
June.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings ofNODALIDA, Tartu, Estonia.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of 48th
Meeting of the Association for Computional Linguis-
tics (ACL 2010), pages 1–11, Uppsala, Sweden, July.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters, pages 785–796, Mumbai, India, December.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for pars-
er adaptation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL 2006), pages
337–344, Sydney, Australia, July.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of European Association for
Computational Linguistics (EACL-2006), pages 81–
88, Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics (ACL-2005), pages 91–98, Ann Arbor,
Michigan, USA, June 25-30.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language (HLT/EMNLP 05), pages 523–530, Van-
couver, Canada, October.
Ryan McDonald. 2006. Discriminative learning span-
ning tree algorithm for dependency parsing. Ph.D.
thesis, University of Pennsylvania.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Com-
putational Linguistics (COLING’04), pages 64–70,
Geneva, Switzerland, August 23-27.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech, June.
Barbara Plank and Gertjan van Noord. 2011. Effec-
tive measures of domain similarity for parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 1566–
1576, Portland, Oregon, USA, June.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics (ACL-2007), pages 616–623,
Prague, Czech Republic, June.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependen-
cy parsing and domain adaptation with LR models
and parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044–1050, Prague, Czech Republic, June.
Mihai Surdeanu, Richard Johansson, Adam Meyers, L-
luis Marquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
Conference on Computational Natural Language
Learning (CoNLL-2008), pages 159–177, Manch-
ester, UK, Augest.
Hiroyasu Yamada and Yuji Matsumoto. 2003. S-
tatistical dependency analysis with support vector
machines. In Proceedings of the 8th Internation-
al Workshop on Parsing Technologies (IWPT-2003),
pages 195–206, Nancy, France, April.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
(ACL-IJCNLP 2009), pages 378–386, Suntec, Sin-
gapore, August.
</reference>
<page confidence="0.997335">
590
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593573">
<title confidence="0.934941">Dependency Parser Adaptation with from Auto-Parsed Target Domain Data</title>
<author confidence="0.618981">Xuezhe</author>
<affiliation confidence="0.9987625">Department of University of</affiliation>
<address confidence="0.998524">Seattle, WA 98195,</address>
<email confidence="0.999712">xzma@uw.edu</email>
<abstract confidence="0.9996612">In this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLPCONLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="899" citStr="Carreras, 2007" startWordPosition="136" endWordPosition="137">. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of p</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLPCONLL, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the Association for Computional Linguistics (ACL</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1678" citStr="Charniak and Johnson (2005)" startWordPosition="253" endWordPosition="256">, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) successfully applied self-training to domain adaptation for constituency parsing using the reranking parser of Charniak and Johnson (2005). Reichart and Rappoport (2007) explored self-training when the amount of the annotated data is small Fei Xia Department of Linguistics University of Washington Seattle, WA 98195, USA fxia@uw.edu and achieved significant improvement. Zhang and Wang (2009) enhanced the performance of dependency parser adaptation by utilizing a large-scale hand-crafted HPSG grammar. Plank and van Noord (2011) proposed a data selection method based on effective measures of domain similarity for dependency parsing. There are roughly two varieties of domain adaptation problem—fully supervised case in which there ar</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine-grained n-best parsing and discriminative reranking. In Proceedings of the 43rd Meeting of the Association for Computional Linguistics (ACL 2005), pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>570--579</pages>
<location>Singapore,</location>
<contexts>
<context position="2934" citStr="Chen et al. (2009)" startWordPosition="451" endWordPosition="454">e target domain, and semi-supervised case in which there are no labeled data in the target domain. In this paper, we present a parsing adaptation approach focused on the fully supervised case. It is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the auto-parsed target domain data. For evaluation, we run experiments on three pairs of source-target domains—WSJBrown, Brown-WSJ, and WSJ-Genia. Our approach achieves significant improvement on all these data sets. 2 Our Approach for Parsing Adaptation Our approach is inspired by Chen et al. (2009)’s work on semi-supervised parsing with additional subtree-based features extracted from unlabeled data and by the feature augmentation method proposed by Daume III (2007). In this section, we first summarize Chen et al.’s work and explain how we extend that for domain adaptation. We will then highlight the similarity and difference between our work and Daume’s method. 2.1 Semi-supervised parsing with subtree-based features One of the most well-known semi-supervised parsing methods is self-training, where a parser trained from the labeled data set is used to parse unlabeled data, and some of t</context>
<context position="4254" citStr="Chen et al. (2009)" startWordPosition="661" endWordPosition="664"> Annual Meeting of the Association for Computational Linguistics, pages 585–590, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ing models. Chen et al. (2009)’s approach differs from self-training in that partial information (i.e., subtrees), instead of the entire trees, from the autoparsed data is used to re-train the parsing models. A subtree is a small part of a dependency tree. For example, a first-order subtree is a single edge consisting of a head and a dependent, and a second-order sibling subtree is one that consists of a head and two dependents. In Chen et al. (2009), they first extract all the subtrees in auto-parsed data and store them in a list Lst. Then they count the frequency of these subtrees and divide them into three groups according to their levels of frequency. Finally, they construct new features for the subtrees based on which groups they belongs to and retrain a new parser with feature-augmented training data.1 2.2 Parser adaptation with subtree-based Features Chen et al. (2009)’s work is for semi-supervised learning, where the labeled training data and the test data come from the same domain; the subtreebased features collected from auto-pa</context>
<context position="6781" citStr="Chen et al., 2009" startWordPosition="1116" endWordPosition="1119">ce, and Ds and Dt to denote the labeled data in the source and target domains, respectively. Let X′ be the augmented input space, and Φs and Φt be the mappings from X to X′ for the instances in the source and target domains respectively. The mappings are defined by Eq 1, where 0 =&lt; 0, 0, ... , 0 &gt;E X is the zero vector. Φs(xorg) = &lt; xorg, 0 &gt; Φt(xorg) = &lt; xorg, xnew &gt; (1) Here, xorg is the original feature vector in X, and xnew is the vector of the subtree-based features extracted from auto-parsed data of the target domain. The subtree extraction method used in our approach is the same as in (Chen et al., 2009) except that we use different thresholds when dividing subtrees into three frequency groups: the threshold for the high-frequency level is TOP 1% of the subtrees, the one for the middle-frequency level is TOP 10%, and the rest of subtrees belong to the low-frequency level. These thresholds are chosen empirically on some development data set. The idea of distinguishing the source and target data is similar to the method in (Daume III, 2007), which did feature augmentation by defining the following mappings:2 Φs(xorg) = &lt; xorg, 0 &gt; Φt(xorg) = &lt; xorg, xorg &gt; (2) Daume III showed that differentiat</context>
<context position="13350" citStr="Chen et al. (2009)" startWordPosition="2220" endWordPosition="2223"> with the labeled data from the source and target domains, respectively. Then we use the parsers to parse unlabeled data in the target domain and select sentences for which the two parsers produce identical trees. Finally, we add the analyses for those sentences to the union of the source and target labeled data to retrain a new parser. This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al., 2007). Feature-Augmentation: This is the approach proposed in (Daume III, 2007). Chen et al. (2009): The algorithm has been explained in Section 2.1. We use the union of the labeled data from the source and target domains as the labeled training data. The unlabeled data needed to construct subtreebased features come from the target domain. Plank and van Noord (2011): This system performs data selection on a data pool consisting of large amount of labeled data to get a training set that is similar to the test domain. The results of the system come from their paper, not from the reimplementation of their system. Per-corpus: The parser is trained with the large training set from the target dom</context>
<context position="16991" citStr="Chen et al., 2009" startWordPosition="2839" endWordPosition="2842">5.7 Src&amp;Tgts,t 87.2 85.9 Self-Trainings,t 87.3 86.0 Co-Trainings,t 87.3 86.0 Feature-Augs,t 87.9 86.5 Chen (2009)s,t 87.5 86.2 this papers,t 88.4 87.1 Plank (2011)† - 86.8 Per-corpusT 90.5 89.7 Table 4: Results with first-order parsing model in the third experiment. “Plank (2011)” refers to the approach in Plank and van Noord (2011). experiments.6 The improvement of our approach over the feature augmentation approach in Daume III (2007) indicates that adding subtreebased features provides better results than making several copies of the original features. Our system outperforms the system in (Chen et al., 2009), implying that adding subtree-based features to only the target labeled data is better than adding them to the labeled data in both the source and target domains. Considering the three steps of our approach in Section 2.2, the training data used to train the parser in Step 1 can be from the target domain only or from the source and target domains. Similarly, in Step 3 the subtree-based features can be added to the labeled data from the target domain only or from the source and target domains. Therefore, there are four combinations. Our approach is the one that uses the labeled data from the t</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 570–579, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Jornal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="11280" citStr="Crammer et al., 2006" startWordPosition="1877" endWordPosition="1880">e number of sentences for each data set used in our experiments about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Genia as the unlabeled data of the target domain. Table 1 shows the number of sentences of each data set used in the experiments. The dependency parsing models we used in this study are the graph-based first-order and secondorder sibling parsing models (McDonald et al., 2005a; McDonald and Pereira, 2006). To be more specific, we use the implementation of MaxParser5 with 10-best MIRA (Crammer et al., 2006; McDonald, 2006) learning algorithm and each parser is trained for 10 iterations. The feature sets of first-order and second-order sibling parsing models used in our experiments are the same as the ones in (Ma and Zhao, 2012). The input to MaxParser are sentences with Part-of-Speech tags; we use gold-standard POS tags in the experiments. Parsing accuracy is measured with unlabeled attachment score (UAS) and the percentage of complete matches (CM) for the first and second experiments. For the third experiment, we also report labeled attachment score (LAS) in order to compare with the results i</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Jornal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>256--263</pages>
<location>Prague, Czech Republic,</location>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007), pages 256–263, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proceedings ofNODALIDA,</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="10226" citStr="Johansson and Nugues, 2007" startWordPosition="1715" endWordPosition="1718"> sentences) of the data, and the test portion is the remaining 10%. For the experiment of WSJ-to-B, we randomly selected about 2000 sentences from training portion of Brown and use them as labeled data and the rest as unlabeled data in the target domain. In the third experiment denoted by ’“WSJ-toG”, we used WSJ corpus as the source domain and Genia corpus (G)4 as the target domain. Following Plank and van Noord (2011), we used the training data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007). The Genia corpus is converted to CoNLL format with LTH converter, too. We randomly selected 3http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 4Genia distribution in Penn Treebank format is available at http://bllip.cs.brown.edu/download/genia1.0-divisionrel1.tar.gz Source training Target test training unlabeled WSJ-to-B 39,832 2,182 19,632 2,429 B-to-WSJ 21,814 2,097 37,735 2,416 WSJ-to-G 39,279 1,024 13,302 1,360 Table 1: The number of sentences for each data set used in our experiments about 1000 sentences from the training portion of Genia data and use them as the labeled data of the t</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proceedings ofNODALIDA, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of 48th Meeting of the Association for Computional Linguistics (ACL 2010),</booktitle>
<pages>1--11</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="922" citStr="Koo and Collins, 2010" startWordPosition="138" endWordPosition="141">ure augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClo</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of 48th Meeting of the Association for Computional Linguistics (ACL 2010), pages 1–11, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuezhe Ma</author>
<author>Hai Zhao</author>
</authors>
<title>Fourth-order dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>785--796</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="942" citStr="Ma and Zhao, 2012" startWordPosition="142" endWordPosition="145">ch in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) su</context>
<context position="11506" citStr="Ma and Zhao, 2012" startWordPosition="1917" endWordPosition="1920">labeled data of the target domain. Table 1 shows the number of sentences of each data set used in the experiments. The dependency parsing models we used in this study are the graph-based first-order and secondorder sibling parsing models (McDonald et al., 2005a; McDonald and Pereira, 2006). To be more specific, we use the implementation of MaxParser5 with 10-best MIRA (Crammer et al., 2006; McDonald, 2006) learning algorithm and each parser is trained for 10 iterations. The feature sets of first-order and second-order sibling parsing models used in our experiments are the same as the ones in (Ma and Zhao, 2012). The input to MaxParser are sentences with Part-of-Speech tags; we use gold-standard POS tags in the experiments. Parsing accuracy is measured with unlabeled attachment score (UAS) and the percentage of complete matches (CM) for the first and second experiments. For the third experiment, we also report labeled attachment score (LAS) in order to compare with the results in (Plank and van Noord, 2011). 3.2 Comparison Systems For comparison, we re-implemented the following well-known baselines and previous approaches, and tested them on the three data sets: SrcOnly: Train a parser with the label</context>
</contexts>
<marker>Ma, Zhao, 2012</marker>
<rawString>Xuezhe Ma and Hai Zhao. 2012. Fourth-order dependency parsing. In Proceedings of COLING 2012: Posters, pages 785–796, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="8645" citStr="Marcus et al., 1993" startWordPosition="1436" endWordPosition="1439">in Eq 2 looks different from the one proposed in (Daume III, 2007), but it can be proved that the two are equivalent. 586 several common baseline systems and previous approaches. In this section, we first describe the data sets and parsing models used in each of the three experiments in section 3.1. Then we provide a brief introduction to the systems we have reimplemented for comparison in section 3.2. The experimental results are reported in section 3.3. 3.1 Data and Tools In the first two experiments, we used the Wall Street Journal (WSJ) and Brown (B) portions of the English Penn TreeBank (Marcus et al., 1993). In the first experiment denoted by “WSJto-B”, WSJ corpus is used as the source domain and Brown corpus as the target domain. In the second experiment, we use the reverse order of the two corpora and denote it by “B-to-WSJ”. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool3 with the standard head rules (Yamada and Matsumoto, 2003). For the WSJ corpus, we used the standard data split: sections 2-21 for training and section 23 for test. In the experiment of B-to-WSJ, we randomly selected about 2000 sentences from the training portion of WSJ as the label</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>337--344</pages>
<location>Sydney, Australia,</location>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 337–344, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of European Association for Computational Linguistics (EACL-2006),</booktitle>
<pages>81--88</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="883" citStr="McDonald and Pereira, 2006" startWordPosition="132" endWordPosition="135">ation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed f</context>
<context position="11178" citStr="McDonald and Pereira, 2006" startWordPosition="1859" endWordPosition="1862">B 39,832 2,182 19,632 2,429 B-to-WSJ 21,814 2,097 37,735 2,416 WSJ-to-G 39,279 1,024 13,302 1,360 Table 1: The number of sentences for each data set used in our experiments about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Genia as the unlabeled data of the target domain. Table 1 shows the number of sentences of each data set used in the experiments. The dependency parsing models we used in this study are the graph-based first-order and secondorder sibling parsing models (McDonald et al., 2005a; McDonald and Pereira, 2006). To be more specific, we use the implementation of MaxParser5 with 10-best MIRA (Crammer et al., 2006; McDonald, 2006) learning algorithm and each parser is trained for 10 iterations. The feature sets of first-order and second-order sibling parsing models used in our experiments are the same as the ones in (Ma and Zhao, 2012). The input to MaxParser are sentences with Part-of-Speech tags; we use gold-standard POS tags in the experiments. Parsing accuracy is measured with unlabeled attachment score (UAS) and the percentage of complete matches (CM) for the first and second experiments. For the </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of European Association for Computational Linguistics (EACL-2006), pages 81– 88, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="830" citStr="McDonald et al., 2005" startWordPosition="124" endWordPosition="127"> a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of grea</context>
<context position="11148" citStr="McDonald et al., 2005" startWordPosition="1855" endWordPosition="1858">aining unlabeled WSJ-to-B 39,832 2,182 19,632 2,429 B-to-WSJ 21,814 2,097 37,735 2,416 WSJ-to-G 39,279 1,024 13,302 1,360 Table 1: The number of sentences for each data set used in our experiments about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Genia as the unlabeled data of the target domain. Table 1 shows the number of sentences of each data set used in the experiments. The dependency parsing models we used in this study are the graph-based first-order and secondorder sibling parsing models (McDonald et al., 2005a; McDonald and Pereira, 2006). To be more specific, we use the implementation of MaxParser5 with 10-best MIRA (Crammer et al., 2006; McDonald, 2006) learning algorithm and each parser is trained for 10 iterations. The feature sets of first-order and second-order sibling parsing models used in our experiments are the same as the ones in (Ma and Zhao, 2012). The input to MaxParser are sentences with Part-of-Speech tags; we use gold-standard POS tags in the experiments. Parsing accuracy is measured with unlabeled attachment score (UAS) and the percentage of complete matches (CM) for the first an</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005), pages 91–98, Ann Arbor, Michigan, USA, June 25-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLT/EMNLP 05),</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="830" citStr="McDonald et al., 2005" startWordPosition="124" endWordPosition="127"> a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of grea</context>
<context position="11148" citStr="McDonald et al., 2005" startWordPosition="1855" endWordPosition="1858">aining unlabeled WSJ-to-B 39,832 2,182 19,632 2,429 B-to-WSJ 21,814 2,097 37,735 2,416 WSJ-to-G 39,279 1,024 13,302 1,360 Table 1: The number of sentences for each data set used in our experiments about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Genia as the unlabeled data of the target domain. Table 1 shows the number of sentences of each data set used in the experiments. The dependency parsing models we used in this study are the graph-based first-order and secondorder sibling parsing models (McDonald et al., 2005a; McDonald and Pereira, 2006). To be more specific, we use the implementation of MaxParser5 with 10-best MIRA (Crammer et al., 2006; McDonald, 2006) learning algorithm and each parser is trained for 10 iterations. The feature sets of first-order and second-order sibling parsing models used in our experiments are the same as the ones in (Ma and Zhao, 2012). The input to MaxParser are sentences with Part-of-Speech tags; we use gold-standard POS tags in the experiments. Parsing accuracy is measured with unlabeled attachment score (UAS) and the percentage of complete matches (CM) for the first an</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLT/EMNLP 05), pages 523–530, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative learning spanning tree algorithm for dependency parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="11297" citStr="McDonald, 2006" startWordPosition="1881" endWordPosition="1883">for each data set used in our experiments about 1000 sentences from the training portion of Genia data and use them as the labeled data of the target domain, and the rest of training data of Genia as the unlabeled data of the target domain. Table 1 shows the number of sentences of each data set used in the experiments. The dependency parsing models we used in this study are the graph-based first-order and secondorder sibling parsing models (McDonald et al., 2005a; McDonald and Pereira, 2006). To be more specific, we use the implementation of MaxParser5 with 10-best MIRA (Crammer et al., 2006; McDonald, 2006) learning algorithm and each parser is trained for 10 iterations. The feature sets of first-order and second-order sibling parsing models used in our experiments are the same as the ones in (Ma and Zhao, 2012). The input to MaxParser are sentences with Part-of-Speech tags; we use gold-standard POS tags in the experiments. Parsing accuracy is measured with unlabeled attachment score (UAS) and the percentage of complete matches (CM) for the first and second experiments. For the third experiment, we also report labeled attachment score (LAS) in order to compare with the results in (Plank and van </context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative learning spanning tree algorithm for dependency parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING’04),</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="807" citStr="Nivre and Scholz, 2004" startWordPosition="120" endWordPosition="123">n this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 1 Introduction In recent years, several dependency parsing algorithms (Nivre and Scholz, 2004; McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012) have been proposed and achieved high parsing accuracies on several treebanks of different languages. However, the performance of such parsers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of the 20th international conference on Computational Linguistics (COLING’04), pages 64–70, Geneva, Switzerland, August 23-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech,</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>Effective measures of domain similarity for parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>1566--1576</pages>
<location>Portland, Oregon, USA,</location>
<marker>Plank, van Noord, 2011</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2011. Effective measures of domain similarity for parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 1566– 1576, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-2007),</booktitle>
<pages>616--623</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1709" citStr="Reichart and Rappoport (2007)" startWordPosition="257" endWordPosition="261">sers declines when training and test data come from different domains. Furthermore, the manually annotated treebanks that these parsers rely on are highly expensive to create. Therefore, developing dependency parsing algorithms that can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) successfully applied self-training to domain adaptation for constituency parsing using the reranking parser of Charniak and Johnson (2005). Reichart and Rappoport (2007) explored self-training when the amount of the annotated data is small Fei Xia Department of Linguistics University of Washington Seattle, WA 98195, USA fxia@uw.edu and achieved significant improvement. Zhang and Wang (2009) enhanced the performance of dependency parser adaptation by utilizing a large-scale hand-crafted HPSG grammar. Plank and van Noord (2011) proposed a data selection method based on effective measures of domain similarity for dependency parsing. There are roughly two varieties of domain adaptation problem—fully supervised case in which there are a small amount of labeled dat</context>
<context position="9423" citStr="Reichart and Rappoport (2007)" startWordPosition="1572" endWordPosition="1575">nt, we use the reverse order of the two corpora and denote it by “B-to-WSJ”. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool3 with the standard head rules (Yamada and Matsumoto, 2003). For the WSJ corpus, we used the standard data split: sections 2-21 for training and section 23 for test. In the experiment of B-to-WSJ, we randomly selected about 2000 sentences from the training portion of WSJ as the labeled data in the target domain. The rest of training data in WSJ is regarded as the unlabeled data of the target domain. For Brown corpus, we followed Reichart and Rappoport (2007) for data split. The training and test sections consist of sentences from all of the genres that form the corpus. The training portion consists of 90% (9 of each 10 consecutive sentences) of the data, and the test portion is the remaining 10%. For the experiment of WSJ-to-B, we randomly selected about 2000 sentences from training portion of Brown and use them as labeled data and the rest as unlabeled data in the target domain. In the third experiment denoted by ’“WSJ-toG”, we used WSJ corpus as the source domain and Genia corpus (G)4 as the target domain. Following Plank and van Noord (2011), </context>
<context position="12354" citStr="Reichart and Rappoport (2007)" startWordPosition="2055" endWordPosition="2059">s (CM) for the first and second experiments. For the third experiment, we also report labeled attachment score (LAS) in order to compare with the results in (Plank and van Noord, 2011). 3.2 Comparison Systems For comparison, we re-implemented the following well-known baselines and previous approaches, and tested them on the three data sets: SrcOnly: Train a parser with the labeled data from the source domain only. TgtOnly: Train a parser with the labeled data from the target domain only. Src&amp;Tgt: Train a parser with the labeled data from the source and target domains. Self-Training: Following Reichart and Rappoport (2007), we train a parser with the union of the source and target labeled data, parse the unlabeled data in the target domain, 5http://sourceforge.net/projects/maxparser/ 587 add the entire auto-parsed trees to the manually labeled data in a single step without checking their parsing quality, and retrain the parser. Co-Training: In the co-training system, we first train two parsers with the labeled data from the source and target domains, respectively. Then we use the parsers to parse unlabeled data in the target domain and select sentences for which the two parsers produce identical trees. Finally,</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-2007), pages 616–623, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>1044--1050</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13139" citStr="Sagae and Tsujii, 2007" startWordPosition="2186" endWordPosition="2189">r/ 587 add the entire auto-parsed trees to the manually labeled data in a single step without checking their parsing quality, and retrain the parser. Co-Training: In the co-training system, we first train two parsers with the labeled data from the source and target domains, respectively. Then we use the parsers to parse unlabeled data in the target domain and select sentences for which the two parsers produce identical trees. Finally, we add the analyses for those sentences to the union of the source and target labeled data to retrain a new parser. This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al., 2007). Feature-Augmentation: This is the approach proposed in (Daume III, 2007). Chen et al. (2009): The algorithm has been explained in Section 2.1. We use the union of the labeled data from the source and target domains as the labeled training data. The unlabeled data needed to construct subtreebased features come from the target domain. Plank and van Noord (2011): This system performs data selection on a data pool consisting of large amount of labeled data to get a training set th</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044–1050, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis Marquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The conll2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<pages>159--177</pages>
<location>Manchester, UK, Augest.</location>
<contexts>
<context position="10098" citStr="Surdeanu et al., 2008" startWordPosition="1695" endWordPosition="1698">st of sentences from all of the genres that form the corpus. The training portion consists of 90% (9 of each 10 consecutive sentences) of the data, and the test portion is the remaining 10%. For the experiment of WSJ-to-B, we randomly selected about 2000 sentences from training portion of Brown and use them as labeled data and the rest as unlabeled data in the target domain. In the third experiment denoted by ’“WSJ-toG”, we used WSJ corpus as the source domain and Genia corpus (G)4 as the target domain. Following Plank and van Noord (2011), we used the training data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007). The Genia corpus is converted to CoNLL format with LTH converter, too. We randomly selected 3http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 4Genia distribution in Penn Treebank format is available at http://bllip.cs.brown.edu/download/genia1.0-divisionrel1.tar.gz Source training Target test training unlabeled WSJ-to-B 39,832 2,182 19,632 2,429 B-to-WSJ 21,814 2,097 37,735 2,416 WSJ-to-G 39,279 1,024 13,302 1,360 Table 1: The number of sentences for each data se</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Marquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis Marquez, and Joakim Nivre. 2008. The conll2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008), pages 159–177, Manchester, UK, Augest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT-2003),</booktitle>
<pages>195--206</pages>
<location>Nancy, France,</location>
<contexts>
<context position="9020" citStr="Yamada and Matsumoto, 2003" startWordPosition="1500" endWordPosition="1503">mented for comparison in section 3.2. The experimental results are reported in section 3.3. 3.1 Data and Tools In the first two experiments, we used the Wall Street Journal (WSJ) and Brown (B) portions of the English Penn TreeBank (Marcus et al., 1993). In the first experiment denoted by “WSJto-B”, WSJ corpus is used as the source domain and Brown corpus as the target domain. In the second experiment, we use the reverse order of the two corpora and denote it by “B-to-WSJ”. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool3 with the standard head rules (Yamada and Matsumoto, 2003). For the WSJ corpus, we used the standard data split: sections 2-21 for training and section 23 for test. In the experiment of B-to-WSJ, we randomly selected about 2000 sentences from the training portion of WSJ as the labeled data in the target domain. The rest of training data in WSJ is regarded as the unlabeled data of the target domain. For Brown corpus, we followed Reichart and Rappoport (2007) for data split. The training and test sections consist of sentences from all of the genres that form the corpus. The training portion consists of 90% (9 of each 10 consecutive sentences) of the da</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT-2003), pages 195–206, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Rui Wang</author>
</authors>
<title>Cross-domain dependency parsing using a deep linguistic grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP</booktitle>
<pages>378--386</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1933" citStr="Zhang and Wang (2009)" startWordPosition="290" endWordPosition="293"> can be easily ported from one domain to another—say, from a resource-rich domain to a resource-poor domain—is of great importance. Several approaches have been proposed for the task of parser adaptation. McClosky et at. (2006) successfully applied self-training to domain adaptation for constituency parsing using the reranking parser of Charniak and Johnson (2005). Reichart and Rappoport (2007) explored self-training when the amount of the annotated data is small Fei Xia Department of Linguistics University of Washington Seattle, WA 98195, USA fxia@uw.edu and achieved significant improvement. Zhang and Wang (2009) enhanced the performance of dependency parser adaptation by utilizing a large-scale hand-crafted HPSG grammar. Plank and van Noord (2011) proposed a data selection method based on effective measures of domain similarity for dependency parsing. There are roughly two varieties of domain adaptation problem—fully supervised case in which there are a small amount of labeled data in the target domain, and semi-supervised case in which there are no labeled data in the target domain. In this paper, we present a parsing adaptation approach focused on the fully supervised case. It is a feature augmenta</context>
</contexts>
<marker>Zhang, Wang, 2009</marker>
<rawString>Yi Zhang and Rui Wang. 2009. Cross-domain dependency parsing using a deep linguistic grammar. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2009), pages 378–386, Suntec, Singapore, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>