<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.085168">
<title confidence="0.996897">
A Broad-Coverage Word Sense Tagger
</title>
<author confidence="0.991177">
Dekang Lin
</author>
<affiliation confidence="0.962379666666667">
Department of Computer Science
University of Manitoba
Winnipeg, Manitoba, Canada R3T 2N2
</affiliation>
<email confidence="0.932918">
lindek@cs.umanitoba.ca
</email>
<bodyText confidence="0.995936714285714">
Previous corpus-based Word Sense Disambigua-
tion (WSD) algorithms (Hearst, 1991; Bruce and
Wiebe, 1994; Leacock et al., 1996; Ng and Lee,
1996; Yarowsky, 1992; Yarowsky, 1995) determine
the meanings of polysemous words by exploiting
their local contexts. A basic intuition that un-
derlies those algorithms is the following:
</bodyText>
<listItem confidence="0.602123333333333">
(1) Two occurrences of the same word have
identical meanings if they have similar local
contexts.
</listItem>
<bodyText confidence="0.99982695">
In other words, previous corpus-based WSD algo-
rithms learn to disambiguate a polysemous word
from previous usages of the same word. This has
several undesirable consequences. Firstly, a word
must occur thousands of times before a good clas-
sifier can be trained. There are thousands of poly-
semous words, e.g., 11,562 polysemous nouns in
WordNet (Miller, 1990). For every polysemous
word to occur thousands of times each, the corpus
must contain billions of words. Secondly, learning
to disambiguate a word from the previous usages of
the same word means that whatever was learned
for one word is not used on other words, which
obviously missed generality in natural languages.
Thirdly, these algorithms cannot deal with words
for which classifiers have not been trained, which
explains why most previous WSD algorithms only
deal with a dozen of polysemous words.
We demonstrate a new WSD algorithm that re-
lies on a different intuition:
</bodyText>
<listItem confidence="0.844588333333333">
(2) Two different words are likely to have similar
meanings if they occur in identical local
contexts.
</listItem>
<bodyText confidence="0.985961428571429">
The local context of a word is defined in our algo-
rithm as a syntactic dependency relationship that
the word participates in. To disambiguate a pol-
ysemous word, we search a local context database
to retrieve the list of words (called selectors) that
appeared in the same local context as the polyse-
mous word in the training corpus. The meaning of
the polysemous word is determined by maximizing
its similarity to the selectors.
For example, consider the sentence:
(3) The new facility will employ 500 of the
existing 600 employees
The word &amp;quot;facility&amp;quot; has 5 possible meanings in
WordNet 1.5:
</bodyText>
<listItem confidence="0.9991948">
1. installation
2. proficiency/technique
3. adeptness
4. readiness
5. toilet/bathroom
</listItem>
<bodyText confidence="0.993811833333333">
Since the word &amp;quot;facility&amp;quot; is the subject of &amp;quot;em-
ploy&amp;quot; and is modified by &amp;quot;new&amp;quot; in (3), we retrieve
other words that appeared in the same contexts
and obtain the following two groups of selectors
(the log A column shows the likelihood ratios (Dun-
ning, 1993) of these words in the local contexts):
</bodyText>
<listItem confidence="0.992427">
• Subjects of &amp;quot;employ&amp;quot; with top-20 highest likeli-
hood ratios.
</listItem>
<table confidence="0.949593166666667">
word freq log A word freq log A
ORG• 64 50.4 machine 3 6.56
plant 14 31.0 corporation 3 6.47
company rri 28.6 manufacturer 3 6.21
operation 8 23.0 insurance company 2 6.06
industry 9 14.6 aerospace 2 5.81
firm 8 13.5 memory device 1 5.79
pirate 2 12.1 department 3 5.55
unit 9 9.32 foreign office 1 5.41
shift 3 8.48 enterprise 2 5.39
postal service 2 7.73 pilot 2 5.37
*ORG includes all proper names recognized as organizations
</table>
<page confidence="0.982133">
18
</page>
<listItem confidence="0.9093735">
• Modifiees of &amp;quot;new&amp;quot; with top-20 highest likeli-
hood ratio
</listItem>
<table confidence="0.9985">
word freq log A word freq log A
post 432 952.9 bonds 223 245.4
issue 805 902.8 capital 178 241.8
product 675 888.6 order 228 236.5
rule 459 875.8 version 158 223.7
law 356 541.5 position 236 207.3
technology 237 382.7 high 152 201.2
generation 150 323.2 contract 279 198.1
model 207 319.3 bill 208 194.9
job 260 269.2 venture 123 193.7
system 318 251.8 program 283 183.8
</table>
<bodyText confidence="0.9937253">
Since the similarity between Sense 1 of &amp;quot;facility&amp;quot;
and the selectors is greater than that of other
senses, the word &amp;quot;facility&amp;quot; in (3) is tagged &amp;quot;Sense
1,,.
The key innovation of our algorithm is that a
polysemous word is disambiguated with past us-
ages of other words. Whether or not it appears in
the training corpus is irrelevant.
Compared with previous corpus-based algo-
rithms, our approach offers several advantages:
</bodyText>
<listItem confidence="0.97080625">
• The same knowledge sources are used for all
words, as opposed to using a separate classifier
for each individual word. For example, the same
set of selectors can also be used to disambiguate
&amp;quot;school&amp;quot; in &amp;quot;the new school employed 100 peo-
ple&amp;quot;.
• It requires a much smaller training corpus that
needs not be sense-tagged.
• It is able to deal with words that are infrequent
or do not even appear in the training corpus.
• The same mechanism can also be used to infer
the semantic categories of unknown words.
</listItem>
<bodyText confidence="0.999929916666667">
In the demonstrated system, the local context
database is constructed with 8,665,362 dependency
relationships that are extracted from a 25-million-
word Wall Street Journal corpus. The corpus
is parsed with a broad-coverage parser, PRINCI-
PAR, in 126 hours on a SPARC-Ultra 1/140 with
96MB of memory. The nouns in the input text are
tagged with their senses in WordNet 1.5. Proper
nouns that do not contain simple markers (e.g.,
Mr., Inc.) to indicate their categories are treated
as 3-way ambiguous and are tagged as &amp;quot;group&amp;quot;,
&amp;quot;person&amp;quot;, or &amp;quot;location&amp;quot; by the system.
</bodyText>
<sectionHeader confidence="0.980148" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.990762">
Rebecca Bruce and Janyce Wiebe. 1994. Word-
sense disambiguation using decomposable mod-
els. In Proceedings of the 32nd Annual Meeting
of the Associations for Computational Linguis-
tics, pages 139-145, Las Cruces, New Mexico.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61-74, March.
Marti Hearst. 1991. noun homograph disambigua-
tion using local context in large text corpora. In
Conference on Research and Development in In-
formation Retrieval ACM/SIGIR, pages 36-47,
Pittsburgh, PA.
Claudia Leacock, Goeffrey Towwell, and Ellen M.
Voorhees. 1996. Towards building contextual
representations of word senses using statistical
models. In Corpus Processing for Lexical Acqui-
sition, chapter 6, pages 97-113. The MIT Press.
George A. Miller. 1990. WordNet: An on-line
lexical database. International Journal of Lexi-
cography, 3(4):235-312.
Hwee Tow Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate
word sense: An examplar-based approach. In
Proceedings of 34th Annual Meeting of the As-
sociation for Computational Linguistics, pages
40-47, Santa Cruz, California.
David Yarowsky. 1992. Word-sense disambigua-
tion using statistical models of Roget&apos;s cate-
gories trained on large corpora. In Proceedings
of COLING-92, Nantes, France.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of 33rd Annual Meeting of the As-
sociation for Computational Linguistics, pages
189-196, Cambridge, Massachusetts, June.
</reference>
<page confidence="0.999332">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.024213">
<title confidence="0.999588">A Broad-Coverage Word Sense Tagger</title>
<author confidence="0.999123">Dekang Lin</author>
<affiliation confidence="0.999879">Department of Computer Science University of Manitoba</affiliation>
<address confidence="0.998314">Winnipeg, Manitoba, Canada R3T 2N2</address>
<email confidence="0.963728">lindek@cs.umanitoba.ca</email>
<abstract confidence="0.981111075630252">Previous corpus-based Word Sense Disambiguation (WSD) algorithms (Hearst, 1991; Bruce and Wiebe, 1994; Leacock et al., 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1995) determine the meanings of polysemous words by exploiting contexts. A intuition that underlies those algorithms is the following: Two occurrences of the have if they have contexts. In other words, previous corpus-based WSD algorithms learn to disambiguate a polysemous word from previous usages of the same word. This has several undesirable consequences. Firstly, a word must occur thousands of times before a good classifier can be trained. There are thousands of polysemous words, e.g., 11,562 polysemous nouns in WordNet (Miller, 1990). For every polysemous word to occur thousands of times each, the corpus must contain billions of words. Secondly, learning to disambiguate a word from the previous usages of means that whatever was learned for one word is not used on other words, which missed generality in natural Thirdly, these algorithms cannot deal with words for which classifiers have not been trained, which explains why most previous WSD algorithms only deal with a dozen of polysemous words. We demonstrate a new WSD algorithm that relies on a different intuition: Two are likely to have if they occur in contexts. The local context of a word is defined in our algorithm as a syntactic dependency relationship that the word participates in. To disambiguate a polysemous word, we search a local context database retrieve the list of words (called appeared in the same local context as the polysemous word in the training corpus. The meaning of the polysemous word is determined by maximizing its similarity to the selectors. For example, consider the sentence: (3) The new facility will employ 500 of the existing 600 employees The word &amp;quot;facility&amp;quot; has 5 possible meanings in WordNet 1.5: 1. installation 2. proficiency/technique 3. adeptness 4. readiness 5. toilet/bathroom Since the word &amp;quot;facility&amp;quot; is the subject of &amp;quot;employ&amp;quot; and is modified by &amp;quot;new&amp;quot; in (3), we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors (the log A column shows the likelihood ratios (Dunning, 1993) of these words in the local contexts): • Subjects of &amp;quot;employ&amp;quot; with top-20 highest likelihood ratios. word freq log A word freq log A ORG• 64 50.4 machine 3 6.56 plant 14 31.0 corporation 3 6.47 company rri 28.6 manufacturer 3 6.21 operation 8 23.0 insurance company 2 6.06 industry 9 14.6 aerospace 2 5.81 firm 8 13.5 memory device 1 5.79 pirate 2 12.1 department 3 5.55 unit 9 9.32 foreign office 1 5.41 shift 3 8.48 enterprise 2 5.39 postal service 2 7.73 pilot 2 5.37 *ORG includes all proper names recognized as organizations 18 • Modifiees of &amp;quot;new&amp;quot; with top-20 highest likelihood ratio word freq log A word freq log A post 432 952.9 bonds 223 245.4 issue 805 902.8 capital 178 241.8 product 675 888.6 order 228 236.5 rule 459 875.8 version 158 223.7 law 356 541.5 position 236 207.3 technology 237 382.7 high 152 201.2 generation 150 323.2 contract 279 198.1 model 207 319.3 bill 208 194.9 job 260 269.2 venture 123 193.7 system 318 251.8 program 283 183.8 Since the similarity between Sense 1 of &amp;quot;facility&amp;quot; and the selectors is greater than that of other senses, the word &amp;quot;facility&amp;quot; in (3) is tagged &amp;quot;Sense The key innovation of our algorithm is that a polysemous word is disambiguated with past usages of other words. Whether or not it appears in the training corpus is irrelevant. Compared with previous corpus-based algorithms, our approach offers several advantages: • The same knowledge sources are used for all words, as opposed to using a separate classifier for each individual word. For example, the same set of selectors can also be used to disambiguate &amp;quot;school&amp;quot; in &amp;quot;the new school employed 100 people&amp;quot;. • It requires a much smaller training corpus that needs not be sense-tagged. • It is able to deal with words that are infrequent or do not even appear in the training corpus. • The same mechanism can also be used to infer the semantic categories of unknown words. In the demonstrated system, the local context database is constructed with 8,665,362 dependency relationships that are extracted from a 25-millionword Wall Street Journal corpus. The corpus is parsed with a broad-coverage parser, PRINCI- PAR, in 126 hours on a SPARC-Ultra 1/140 with 96MB of memory. The nouns in the input text are tagged with their senses in WordNet 1.5. Proper nouns that do not contain simple markers (e.g., Mr., Inc.) to indicate their categories are treated as 3-way ambiguous and are tagged as &amp;quot;group&amp;quot;, &amp;quot;person&amp;quot;, or &amp;quot;location&amp;quot; by the system.</abstract>
<note confidence="0.882527117647059">References Rebecca Bruce and Janyce Wiebe. 1994. Wordsense disambiguation using decomposable mod- In of the 32nd Annual Meeting of the Associations for Computational Linguis- 139-145, Las Cruces, New Mexico. Ted Dunning. 1993. Accurate methods for the of surprise and coincidence. Computa- Linguistics, March. Marti Hearst. 1991. noun homograph disambiguation using local context in large text corpora. In Conference on Research and Development in In- Retrieval ACM/SIGIR, 36-47, Pittsburgh, PA. Claudia Leacock, Goeffrey Towwell, and Ellen M. Voorhees. 1996. Towards building contextual representations of word senses using statistical Corpus Processing for Lexical Acqui- 6, pages 97-113. The MIT Press. George A. Miller. 1990. WordNet: An on-line database. Journal of Lexi- Hwee Tow Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach. In Proceedings of 34th Annual Meeting of the Asfor Computational Linguistics, 40-47, Santa Cruz, California. David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget&apos;s catetrained on large corpora. In COLING-92, France. David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of 33rd Annual Meeting of the As-</note>
<affiliation confidence="0.602597">for Computational Linguistics,</affiliation>
<address confidence="0.756461">189-196, Cambridge, Massachusetts, June.</address>
<intro confidence="0.532497">19</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Wordsense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Associations for Computational Linguistics,</booktitle>
<pages>139--145</pages>
<location>Las Cruces, New Mexico.</location>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>Rebecca Bruce and Janyce Wiebe. 1994. Wordsense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Associations for Computational Linguistics, pages 139-145, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="2548" citStr="Dunning, 1993" startWordPosition="405" endWordPosition="407">ining corpus. The meaning of the polysemous word is determined by maximizing its similarity to the selectors. For example, consider the sentence: (3) The new facility will employ 500 of the existing 600 employees The word &amp;quot;facility&amp;quot; has 5 possible meanings in WordNet 1.5: 1. installation 2. proficiency/technique 3. adeptness 4. readiness 5. toilet/bathroom Since the word &amp;quot;facility&amp;quot; is the subject of &amp;quot;employ&amp;quot; and is modified by &amp;quot;new&amp;quot; in (3), we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors (the log A column shows the likelihood ratios (Dunning, 1993) of these words in the local contexts): • Subjects of &amp;quot;employ&amp;quot; with top-20 highest likelihood ratios. word freq log A word freq log A ORG• 64 50.4 machine 3 6.56 plant 14 31.0 corporation 3 6.47 company rri 28.6 manufacturer 3 6.21 operation 8 23.0 insurance company 2 6.06 industry 9 14.6 aerospace 2 5.81 firm 8 13.5 memory device 1 5.79 pirate 2 12.1 department 3 5.55 unit 9 9.32 foreign office 1 5.41 shift 3 8.48 enterprise 2 5.39 postal service 2 7.73 pilot 2 5.37 *ORG includes all proper names recognized as organizations 18 • Modifiees of &amp;quot;new&amp;quot; with top-20 highest likelihood ratio word fre</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>noun homograph disambiguation using local context in large text corpora.</title>
<date>1991</date>
<booktitle>In Conference on Research and Development in Information Retrieval ACM/SIGIR,</booktitle>
<pages>36--47</pages>
<location>Pittsburgh, PA.</location>
<marker>Hearst, 1991</marker>
<rawString>Marti Hearst. 1991. noun homograph disambiguation using local context in large text corpora. In Conference on Research and Development in Information Retrieval ACM/SIGIR, pages 36-47, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Goeffrey Towwell</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Towards building contextual representations of word senses using statistical models.</title>
<date>1996</date>
<booktitle>In Corpus Processing for Lexical Acquisition, chapter 6,</booktitle>
<pages>97--113</pages>
<publisher>The MIT Press.</publisher>
<marker>Leacock, Towwell, Voorhees, 1996</marker>
<rawString>Claudia Leacock, Goeffrey Towwell, and Ellen M. Voorhees. 1996. Towards building contextual representations of word senses using statistical models. In Corpus Processing for Lexical Acquisition, chapter 6, pages 97-113. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="936" citStr="Miller, 1990" startWordPosition="139" endWordPosition="140">ky, 1995) determine the meanings of polysemous words by exploiting their local contexts. A basic intuition that underlies those algorithms is the following: (1) Two occurrences of the same word have identical meanings if they have similar local contexts. In other words, previous corpus-based WSD algorithms learn to disambiguate a polysemous word from previous usages of the same word. This has several undesirable consequences. Firstly, a word must occur thousands of times before a good classifier can be trained. There are thousands of polysemous words, e.g., 11,562 polysemous nouns in WordNet (Miller, 1990). For every polysemous word to occur thousands of times each, the corpus must contain billions of words. Secondly, learning to disambiguate a word from the previous usages of the same word means that whatever was learned for one word is not used on other words, which obviously missed generality in natural languages. Thirdly, these algorithms cannot deal with words for which classifiers have not been trained, which explains why most previous WSD algorithms only deal with a dozen of polysemous words. We demonstrate a new WSD algorithm that relies on a different intuition: (2) Two different words</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tow Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<location>Santa Cruz, California.</location>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tow Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach. In Proceedings of 34th Annual Meeting of the Association for Computational Linguistics, pages 40-47, Santa Cruz, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<location>Nantes, France.</location>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of COLING-92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, Massachusetts,</location>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, Cambridge, Massachusetts, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>