<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024459">
<title confidence="0.988738">
Syntactic Reordering Integrated with Phrase-based SMT
</title>
<author confidence="0.984505">
Jakob Elming
</author>
<affiliation confidence="0.9679005">
Computational Linguistics
Copenhagen Business School
</affiliation>
<email confidence="0.996175">
jel.isv@cbs.dk
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993839375">
We present a novel approach to word re-
ordering which successfully integrates syn-
tactic structural knowledge with phrase-based
SMT. This is done by constructing a lattice
of alternatives based on automatically learned
probabilistic syntactic rules. In decoding, the
alternatives are scored based on the output
word order, not the order of the input. Un-
like previous approaches, this makes it possi-
ble to successfully integrate syntactic reorder-
ing with phrase-based SMT. On an English-
Danish task, we achieve an absolute improve-
ment in translation quality of 1.1 % BLEU.
Manual evaluation supports the claim that the
present approach is significantly superior to
previous approaches.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999110625">
The emergence of phrase-based statistical machine
translation (PSMT) (Koehn et al., 2003) has been
one of the major developments in statistical ap-
proaches to translation. Allowing translation of
word sequences (phrases) instead of single words
provides SMT with a robustness in word selection
and local word reordering.
PSMT has two means of reordering the words. Ei-
ther a phrase pair has been learned where the target
word order differs from the source (phrase internal
reordering), or distance penalized orderings of target
phrases are attempted in decoding (phrase external
reordering). The first solution is strong, the second
is weak.
The second solution is necessary for reorderings
within a previously unseen sequence or over dis-
</bodyText>
<page confidence="0.989762">
46
</page>
<bodyText confidence="0.999972294117647">
tances greater than the maximal phrase length. In
this case, the system in essence relies on the tar-
get side language model to get the correct word or-
der. The choice is made without knowing what the
source is. Basically, it is a bias against phrase exter-
nal reordering.
It seems clear that reordering often depends on
higher level linguistic information, which is absent
from PSMT. In recent work, there has been some
progress towards integrating syntactic information
with the statistical approach to reordering. In works
such as (Xia and McCord, 2004; Collins et al., 2005;
Wang et al., 2007; Habash, 2007), reordering de-
cisions are done “deterministically”, thus placing
these decisions outside the actual PSMT system by
learning to translate from a reordered source lan-
guage. (Crego and Mari˜no, 2007; Zhang et al., 2007;
Li et al., 2007) are more in the spirit of PSMT, in
that multiple reorderings are presented to the PSMT
system as (possibly weighted) options.
Still, there remains a basic conflict between the
syntactic reordering rules and the PSMT system:
one that is most likely due to the discrepancy be-
tween the translation units (phrases) and units of the
linguistic rules, as (Zhang et al., 2007) point out.
In this paper, we proceed in the spirit of the non-
deterministic approaches by providing the decoder
with multiple source reorderings. But instead of
scoring the input word order, we score the order of
the output. By doing this, we avoid the integration
problems of previous approaches.
It should be noted that even though the experi-
ments are conducted within a source reordering ap-
proach, this scoring is also compatible with other ap-
</bodyText>
<note confidence="0.9760665">
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 46–54,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997455">
proach. We will, however, not look further into this
possiblity in the present paper.
In addition, we automatically learn reordering
rules based on several levels of linguistic informa-
tion from word form to subordination and syntac-
tic structure to produce reordering rules that are not
restricted to operations on syntactic tree structure
nodes.
In the next section, we discuss and contrast re-
lated work. Section 3 describes aspects of English
and Danish structure that are relevant to reordering.
Section 4 describes the automatic induction of re-
ordering rules and its integration in PSMT. In sec-
tion 5, we describe the SMT system used in the
experiments. Section 6 evaluates and discusses the
present approach.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978655737705">
While several recent authors have achieved positive
results, it has been difficult to integrate syntactic in-
formation while retaining the strengths of the statis-
tical approach.
Several approaches do deterministic reordering.
These do not integrate the reordering in the PSMT
system; instead they place it outside the system by
first reordering the source language, and then having
a PSMT system translate from reordered source lan-
guage to target language. (Collins et al., 2005; Wang
et al., 2007) do this using manually created rules,
and (Xia and McCord, 2004) and (Habash, 2007)
use automatically extracted rules. All use rules ex-
tracted from syntactic parses.
As mentioned by (Al-Onaizan and Papineni,
2006), it can be problematic that these determinis-
tic choices are beyond the scope of optimization and
cannot be undone by the decoder. That is, there is no
way to make up for bad information in later transla-
tion steps.
Another approach is non-deterministic. This pro-
vides the decoder with both the original and the re-
ordered source sentence. (Crego and Mari˜no, 2007)
operate within Ngram-based SMT. They make use
of syntactic structure to reorder the input into a word
lattice. Since the paths are not weighted, the lattice
merely narrows down the size of the search space.
The decoder is not given reason to trust one path (re-
ordering) over another.
(Zhang et al., 2007) assign weights to the paths
of their input word lattice. Instead of hierarchical
linguistic structure, they use reordering rules based
on POS and syntactic chunks, and train the system
with both original and reordered source word order
on a restricted data set (&lt;500K words). Their sys-
tem does not out-perform a standard PSMT system.
As they themselves point out, a reason for this might
be that their reordering approach is not fully inte-
grated with PSMT. This is one of the main problems
addressed in the present work.
(Li et al., 2007) use weighted n-best lists as input
for the decoder. They use rules based on a syntac-
tic parse, allowing children of a tree node to swap
place. This is excessively restrictive. For example,
a common reordering in English-Danish translation
has the subject change place with the finite verb.
Since the verb is often embedded in a VP contain-
ing additional words that should not be moved, such
rules cannot be captured by local reordering on tree
nodes.
In many cases, the exact same word order that
is obtained through a source sentence reordering, is
also accessible through a phrase internal reordering.
A negative consequence of source order (SO) scor-
ing as done by (Zhang et al., 2007) and (Li et al.,
2007) is that they bias against the valuable phrase
internal reorderings by only promoting the source
sentence reordering. As described in section 4.3, we
solve this problem by reordering the input string, but
scoring the output string, thus allowing the strengths
of PSMT to co-exist with rule-based reordering.
</bodyText>
<sectionHeader confidence="0.96581" genericHeader="method">
3 Language comparison
</sectionHeader>
<bodyText confidence="0.999969538461539">
The two languages examined in this investigation,
English and Danish, are very similar from a struc-
tural point of view. A word alignment will most of-
ten display an almost one-to-one correlation. In the
hand-aligned data, only 39% of the sentences con-
tain reorderings (following the notion of reordering
as defined in 4.1). On average, a sentence contains
0.66 reorderings.
One of the main differences between English and
Danish word order is that Danish is a verb-second
language: the finite verb of a declarative main clause
must always be the second constituent. Since this
is not the case for English, a reordering rule should
</bodyText>
<page confidence="0.954666">
47
</page>
<equation confidence="0.9993787">
� � � � � � �
■
� � � �
�
�
■
� � � � �
�
� � � � � � �
� � � � � � �
� � � � � � �
� � � � � � �
s1 s2 s3 s4 s5 sg s7
t7
tg
t5
t4
t3
t2
t1
</equation>
<bodyText confidence="0.999923714285714">
move the subject of an English sentence to the right
of the finite verb, if the first position is filled by
something other than the subject. This is exempli-
fied by (1) (examples are annotated with English
gloss and translation), where ’they’ should move to
the right of ’come’ to get the Danish word order as
seen in the gloss.
</bodyText>
<tableCaption confidence="0.965431">
Table 1: Reordering example
</tableCaption>
<equation confidence="0.6399975">
(1) nu kommer de
[ now come they ]
</equation>
<bodyText confidence="0.984554904761905">
’here they come’
Another difference is that Danish sentence adver-
bials in a subordinate clause move to the left of the
finite verb. This is illustrated in example (2). This
example also shows the difficulty for a PSMT sys-
tem. Since the trigram ’han kan ikke’ is frequent in
Danish main clauses, and ’han ikke kan’ is frequent
in subordinate clauses, we need information on sub-
ordination to get the correct word order. This infor-
mation can be obtained from the conjunction ’that’.
A trigram PSMT system would not be able to handle
the reordering in (2), since ’that’ is beyond the scope
of ’not’.
[ han siger at han ikke kan se
he says that he not can see ]
’he says that he can not see’
In the main clause, on the other hand, Danish prefers
the sentence adverbial to appear to the right of the
finite verb. Therefore, if the English adverbial ap-
pears to the left of the finite verb in a main clause, it
should move right as exemplified by example (3).
</bodyText>
<listItem confidence="0.997135">
(3) hun
[ she
</listItem>
<bodyText confidence="0.953291285714286">
’she never saw the ship’
Other differences are of a more conventionalized na-
ture. E.g. address numbers are written after the
street in Danish (example (4)).
nygade
nygade
’he lives at 14 nygade’
</bodyText>
<sectionHeader confidence="0.994912" genericHeader="method">
4 Reordering rules
</sectionHeader>
<subsectionHeader confidence="0.999896">
4.1 Definition of reordering
</subsectionHeader>
<bodyText confidence="0.999831666666667">
In this experiment, reordering is defined as two
word sequences exchanging positions. These two
sequences are restricted by the following conditions:
</bodyText>
<listItem confidence="0.987033857142857">
• Parallel consecutive: They have to make up
consecutive sequences of words, and each has
to align to a consecutive sequence of words.
• Maximal: They have to be the longest possible
consecutive sequences changing place.
• Adjacent: They have to appear next to each
other on both source and target side.
</listItem>
<bodyText confidence="0.999675727272727">
The sequences are not restricted in length, mak-
ing both short and long distance reordering possible.
Furthermore, they need not be phrases in the sense
that they appear as an entry in the phrase table.
Table 1 illustrates reordering in a word alignment
matrix. The table contains reorderings between the
light grey sequences (s32 and s64)1 and the dark grey
sequences (s5 5 and s66). On the other hand, the se-
quences s3 3 and s5 4 are e.g. not considered reordered,
since neither are maximal, and s54 is not consecutive
on the target side.
</bodyText>
<subsectionHeader confidence="0.986786">
4.2 Rule induction
</subsectionHeader>
<bodyText confidence="0.98795025">
In section 3, we pointed out that subordination is
very important for word order differences between
English and Danish. In addition, the sentence posi-
tion of constituents plays a role. All this informa-
tion is present in a syntactic sentence parse. A sub-
ordinate clause is defined as inside an SBAR con-
1Notation: s&apos;Y means the consecutive source sequence cov-
ering words x to y.
</bodyText>
<figure confidence="0.852228666666667">
s˚a aldrig skibet
saw never the ship ]
(4) han bor
[ he lives
14
14 ]
</figure>
<page confidence="0.99422">
48
</page>
<table confidence="0.8480332">
Level LC LS RS RC
WORD &lt;s&gt; today ,  ||today,  ||, he was driving home  ||home.  ||home. &lt; /s&gt;
POS &lt;S&gt; NN ,  ||NN ,  ||, PRP AUX VBG NN  ||NN .  ||NN . &lt; /S&gt;
PS &lt;S&gt; NP ,  ||NP ,  ||, NP AUX VBG ADVP  ||ADVP .  ||ADVP . &lt; /S&gt;
SUBORD main main main main
</table>
<tableCaption confidence="0.999018">
Table 2: Example of experience for learning. Possible contexts separated by ||.
</tableCaption>
<bodyText confidence="0.9999025">
stituent; otherwise it is a main clause. The con-
stituent position can be extracted from the sentence
start tag and the following syntactic phrases. POS
and word form are also included to allow for more
specific/lexicalized rules.
Besides including this information for the candi-
date reordering sequences (left sequence (LS) and
right sequence (RS)), we also include it for the set of
possible left (LC) and right (RC) contexts of these.
The span of the contexts varies from a single word to
all the way to the sentence border. Table 2 contains
an example of the information available to the learn-
ing algorithm. In the example, LS and RS should
change place, since the first position is occupied by
something other than the subject in a main clause.
In order to minimize the training data, word
and POS sequences are limited to 4 words, and
phrase structure (PS) sequences are limited to 3 con-
stituents. In addition, an entry is only used if at least
one of these three levels is not too long for both LS
and RS, and too long contexts are not included in
the set. This does not constrain the possible length
of a reordering, since a PS sequence of length 1 can
cover an entire sentence.
In order to extract rules from the annotated data,
we use a rule-based classifier, Ripper (Cohen, 1996).
The motivation for using Ripper is that it allows fea-
tures to be sets of strings, which fits well with our
representation of the context, and it produces easily
readable rules that allow better understanding of the
decisions being made. In section 6.2, extracted rules
are exemplified and analyzed.
The probabilities of the rules are estimated using
Maximum Likelihood Estimation based on the in-
formation supplied by Ripper on the performance of
the individual rules on the training data. These log-
arithmic probabilities are easily integratable in the
log-linear PSMT model as an additional parameter
by simple addition.
The rules are extracted from the hand-aligned,
Copenhagen Danish-English Dependency Treebank
(Buch-Kromann et al., 2007). 5478 sentences from
the news paper domain containing 111,805 English
words and 100,185 Danish words. The English side
is parsed using a state-of-the-art statistical English
parser (Charniak, 2000).
</bodyText>
<subsectionHeader confidence="0.995398">
4.3 Integrating rule-based reordering in PSMT
</subsectionHeader>
<bodyText confidence="0.9974035">
The integration of the rule-based reordering in our
PSMT system is carried out in two separate stages:
</bodyText>
<listItem confidence="0.99702475">
1. Reorder the source sentence to assimilate the
word order of the target language.
2. Score the target word order according to the rel-
evant rules.
</listItem>
<bodyText confidence="0.99990319047619">
Stage 1) is done in a non-deterministic fashion by
generating a word lattice as input in the spirit of e.g.
(Zens et al., 2002; Crego and Mari˜no, 2007; Zhang
et al., 2007). This way, the system has both the orig-
inal word order, and the reorderings predicted by the
rule set. The different paths of the word lattice are
merely given as equal suggestions to the decoder.
They are in no way individually weighted.
Separating stage 2) from stage 1) is motivated by
the fact that reordering can have two distinct ori-
gins. They can occur because of stage 1), i.e. the
lattice reordering of the original English word or-
der (phrase external reordering), and they can oc-
cur inside a single phrase (phrase internal reorder-
ing). We are, however, interested in doing phrase-
independent, word reordering. We want to promote
rule-predicted reorderings, regardless of whether
they owe their existence to a syntactic rule or a
phrase table entry.
This is accomplished by letting the actual scoring
of the reordering focus on the target string. The de-
</bodyText>
<page confidence="0.994484">
49
</page>
<table confidence="0.9956898">
Source sentence: today1 ,2 he3 was4 late5
Rule: 3 4 —* 4 3
Hypothesis Target string SPTO
H1 idag han var 1 3 4
H2 idag var han 1 4 3
</table>
<tableCaption confidence="0.9919685">
Table 3: Example of SPTO scoring during decoding at
source word 4.
</tableCaption>
<bodyText confidence="0.999413605263158">
coder is informed of where a rule has predicted a re-
ordering, how much it costs to do the reordering, and
how much it costs to avoid it. This is then checked
for each hypothezised target string by keeping track
of what source position target order (SPTO) it cor-
responds to.
The SPTO is a representation of which source
position the word in each target position originates
from. Putting it differently, the hypotheses con-
tain two parallel strings; a target word string and its
SPTO string. In order to access this information,
each phrase table entry is annotated with its internal
word alignment, which is available as an interme-
diate product from phrase table creation. If a phrase
pair has multiple word alignments, the most frequent
is chosen.
Table 3 exemplifies the SPTO scoring. The source
sentence is ’today he was late’, and a rule has pre-
dicted that word 3 and 4 should change place. When
the decoder has covered the first four input words,
two of the hypothesis target strings might be H1
and H2. At this point, it becomes apparent that H2
contains the desired SPTO (namely ’4 3’), and it
get assigned the reordering cost. H1 does not con-
tain the rule-suggested SPTO (in stead, the words
are in the order ’3 4’), and it gets the violation
cost. Both these scorings are performed in a phrase-
independent manner. The decoder assigns the re-
ordering cost to H2 without knowing whether the
reordering is internal (due to a phrase table entry)
or external (due to a syntactic rule).
Phrase internal reorderings at other points of the
sentence, i.e. points that are not covered by a rule,
are not judged by the reordering model. Our rule
extraction does not learn every possible reordering
between the two languages, but only the most gen-
eral ones. If no rule has an opinion at a certain point
in a sentence, the decoder is free to chose the phrase
</bodyText>
<figureCaption confidence="0.998115">
Figure 1: Example word lattice.
</figureCaption>
<bodyText confidence="0.986514857142857">
translation it prefers without reordering cost.
Separating the scoring from the source language
reordering also has the advantage that the SPTO
scoring in essence is compatible with other ap-
proaches such as a traditional PSMT system. We
will, however, not examine this possibility further in
the present paper.
</bodyText>
<sectionHeader confidence="0.996006" genericHeader="method">
5 The PSMT system
</sectionHeader>
<bodyText confidence="0.999933870967742">
The baseline is the PSMT system used for the 2006
NAACL SMT workshop (Koehn and Monz, 2006)
with phrase length 3 and a trigram language model
(Stolcke, 2002). The system was trained on the En-
glish and Danish part of the Europarl corpus version
3 (Koehn, 2005). Fourth quarter of 2000 was re-
moved in order to use the common test set of 11369
sentences (330,082 English words and 309,942 Dan-
ish words with one reference) for testing. In addi-
tion, fourth quarter of 2001 was removed for devel-
opment purposes. Of these, 10194 were used for
various analysis purposes, thereby keeping the test
data perfectly unseen. 500 sentences were taken
from the development set for tuning the decoder pa-
rameters. This was done using the Downhill Sim-
plex algorithm. In total, 1,137,088 sentences con-
taining 31,376,034 English words and 29,571,518
Danish words were left for training the phrase table
and language model.
The decoder used for the baseline system is
Pharaoh (Koehn, 2004) with its distance-penalizing
reordering model. For the experiments, we use
our own decoder which — except for the reorder-
ing model — uses the same knowledge sources
as Pharaoh, i.e. bidirectional phrase translation
model and lexical weighting model, phrase and word
penalty, and target language model. Its behavior is
comparable to Pharaoh when doing monotone de-
coding.
The search algorithm of our decoder is similar to
the RG graph decoder of (Zens et al., 2002). It ex-
</bodyText>
<page confidence="0.968988">
50
</page>
<table confidence="0.9995528">
System Dev Test Swap Subset
Baseline 0.262 0.252 0.234
no scoring 0.267 0.256 0.241
SO scoring 0.268 0.258 0.244
SPTO scoring 0.268 0.258 0.245
</table>
<tableCaption confidence="0.999624">
Table 4: BLEU scores for different scoring methods.
</tableCaption>
<bodyText confidence="0.999788307692308">
pects a word lattice as input. Figure 1 shows the
word lattice for the example in table 3.
Since the input format defines all possible word
orders, a simple monotone search is sufficient. Us-
ing a language model of order n, for each hy-
pothezised target string ending in the same n-1-
gram, we only have to extend the highest scoring
hypothesis. None of the others can possibly outper-
form this one later on. This is because the maximal
context evaluating a phrase extending this hypothe-
sis, is the history (n-1-gram) of the first word of that
phrase. The decoder is not able to look any further
back at the preceeding string.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.985694">
6.1 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999799782608696">
The SPTO reordering approach is evaluated on the
11369 sentences of the common test set. Results are
listed in table 4 along with results on the develop-
ment set. We also report on the swap subset. These
are the 3853 sentences where the approach actually
motivated reorderings in the test set, internal or ex-
ternal. The remaining 7516 sentences were not in-
fluenced by the SPTO reordering approach.
We report on 1) the baseline PSMT system, 2) a
system provided with a rule reordered word lattice
but no scoring, 3) the same system but with an SO
scoring in the spirit of (Zhang et al., 2007; Li et al.,
2007), and finally 4) the same system but with the
SPTO scoring.
The SPTO approach gets an increase over the
baseline PSMT system of 0.6 % BLEU. The swap
subset, however, shows that the extracted rules are
somewhat restricted, only resulting in swap in s of
the sentences. The relevant set, i.e. the set where the
present approach actually differs from the baseline,
is therefore the swap subset. This way, we concen-
trate on the actual focus of the paper, namely the
syntactically motivated SPTO reordering. Here we
</bodyText>
<table confidence="0.9981676">
System BLEU Avr. Human rating
Baseline 0.234 3.00 (2.56)
no scoring 0.240 3.00 (2.74)
SO scoring 0.239 3.00 (2.62)
SPTO scoring 0.244 2.00 (2.08)
</table>
<tableCaption confidence="0.937408">
Table 5: Evaluation on the set where SO and SPTO pro-
duce different translations. Average human ratings are
medians with means in parenthesis, lower scores are bet-
ter, 1 is the best score.
</tableCaption>
<bodyText confidence="0.991163333333333">
achieve an increase in performance of 1.1 % BLEU.
Comparing to the other scoring approaches does
not show much improvement. A possible explana-
tion is that the rules do not apply very often, in com-
bination with the fact that the SO and SPTO scoring
mechanisms most often behave alike. The difference
in SO and SPTO scoring only leads to a difference in
translation in 10% of the sentences where reordering
is done. This set is interesting, since it provides a fo-
cus on the difference between the SO and the SPTO
approaches. In table 5, we evaluate on this set.
The BLEU scores on the entire set indicate that
SPTO is a superior scoring method. To back this ob-
servation, the 100 first sentences are manually eval-
uated by two native speakers of Danish. (Callison-
Burch et al., 2007) show that ranking sentences
gives higher inter-annotator agreement than scor-
ing adequacy and fluency. We therefore employ
this evaluation method, asking the evaluators to rank
sentences from the four systems given the input sen-
tence. Ties are allowed. The annotators had reason-
able inter-annotator agreement (n = 0.523, P(A) =
0.69, P(E) = 0.35). Table 5 shows the aver-
age ratings of the systems. This clearly shows the
SPTO scoring to be significantly superior to the
other methods (p &lt; 0.05).
Most of the cases (55) where SPTO outperforms
SO are cases where SPTO knows that a phrase pair
contains the desired reordering, but SO does not.
Therefore, SO has to use an external reordering
which brings poorer translation than the internal re-
ordering, because the words are translated individ-
ually rather than by a single phrase (37 cases), or it
has to reject the desired reordering (18 cases), which
also hurts translation, since it does not get the correct
word order.
</bodyText>
<page confidence="0.995971">
51
</page>
<table confidence="0.99985075">
Decoder choice SO SPTO
Phrase internal reordering 401 1538
Phrase external reordering 3846 2849
Reject reordering 1468 1328
</table>
<tableCaption confidence="0.969577333333333">
Table 6: The choices made based on the SO and SPTO
scoring for the 5715 reorderings proposed by the rules
for the test data.
</tableCaption>
<bodyText confidence="0.996711833333333">
Table 6 shows the effect of SO and SPTO scoring
in decoding. Most noticeable is that the SO scoring
is strongly biased against phrase internal reorder-
ings; SPTO uses nearly four times as many phrase
internal reorderings as SO. In addition, SPTO is a
little less likely to reject a rule proposed reordering.
</bodyText>
<subsectionHeader confidence="0.999965">
6.2 Rule analysis
</subsectionHeader>
<bodyText confidence="0.999824032258065">
The rule induction resulted in a rule set containing
27 rules. Of these, 22 concerned different ways of
identifying contexts where a reordering should oc-
cur due to the verb second nature of Danish. 4 rules
had to do with adverbials in main and in subordinate
clauses, and the remaining rule expressed that cur-
rency is written after the amount in Danish, while it
is the other way around in English. Since the train-
ing data however only includes Danish Crowns, the
rule was lexicalized to ’DKK’.
Table 7 shows a few of the most frequently used
rules. The first three rules deal with the verb second
phenomenon. The only difference among these is
the left context. Either it is a prepositional phrase, a
subordinate clause or an adverbial. These are three
ways that the algorithm has learned to identify the
verb second phenomenon conditions. Rule 3 is inter-
esting in that it is lexicalized. In the learning data,
the Danish correspondent to ’however’ is most of-
ten not topicalized, and the subject is therefore not
forced from the initial position. As a consequence,
the rule states that it should only apply, if ’however’
is not included in the left context of the reordering.
Rule 4 handles the placement of adverbials in a
subordinate clause. Since the right context is subor-
dinate and a verb phrase, the current sequences must
also be subordinate. In contrast, the fifth rule deals
with adverbials in a main clause, since the left con-
text noun phrase is in a main clause.
A problem with the hand-aligned data used for
rule-induction is that it is out of domain compared
</bodyText>
<table confidence="0.9825382">
No LC LS RS RC
1 PS: &lt;S&gt; PP , PS: NP POS: FV
2 PS: SBAR , PS: NP POS: FV
3 PS: ADVP , PS: NP POS: FV
! WORD:
however,
4 PS: FV POS: RB PS: VP
SUB: sub
5 PS: &lt;S&gt; NP PS: ADVP POS: FV
SUB: main
</table>
<tableCaption confidence="0.999593">
Table 7: Example rules and their application statistics.
</tableCaption>
<bodyText confidence="0.987276088235294">
to the Europarl data used to train the SMT system.
The hand-aligned data is news paper texts, and Eu-
roparl is transcribed spoken language from the Euro-
pean Parliament. Due to its spoken nature, Europarl
contains frequent sentence-initial forms of address.
That is, left adjacent elements that are not integrated
parts of the sentence as illustrated by example (5).
This is not straightforward, because on the surface
these look a lot like topicalized constructions, as in
example (6). In topicalized constructions, it is an
integrated part of the sentence that is moved to the
front in order to affect the flow of discourse infor-
mation. This difference is crucial for the reordering
rules, since ’i’ and ’have’ should reorder in (6), but
not in (5), in order to get Danish word order.
(5) mr president, i have three points .
(6) as president, i have three points .
When translating the development set, it became
clear that many constructions like (5) were reordered
by a rule. Since these constructions were not present
in the hand-aligned data, the learning algorithm did
not have the data to learn this difference.
We therefore included a manual, lexicalized rule
stating that if the left context contained one of a set
of titles (mr, mrs, ms, madam, gentlemen), the re-
ordering should not take place. Since the learning
includes word form information, this is a rule that
the learning algorithm is able to learn. To a great
extent, the rule eliminates the problem.
The above examples also illustrate that local re-
ordering (in this case as local as two neighboring
words) can be a problem for PSMT, since even
though the reordering is local, the information about
whether to reorder or not is not necessarily local.
</bodyText>
<page confidence="0.992986">
52
</page>
<table confidence="0.996954866666667">
1 S based on this viewpoint, every small port and every ferry port which handles
a great deal of tourist traffic should feature on the european list .
B baseret p˚a dette synspunkt , ethvert lille havn og alle færgehavnen som
h˚andterer en stor turist trafik skal st˚a p˚a den europæiske liste .
P baseret p˚a dette synspunkt , skal alle de sm˚a havne , og alle færgehavnen
som behandler mange af turister trafik stod p˚a den europæiske liste .
2 S the rapporteur generally welcomes the proposals in the commission white paper on this
subject but is apprehensive of the possible implications of the reform, which aims
principally to decentralise the implementation of competition rules .
B ordf¢reren generelt bifalder forslagene i kommissionens hvidbog om dette emne , men er
bekymret for de mulige konsekvenser af den reform, som sigter hovedsagelig at
decentralisere gennemf¢relsen af konkurrencereglerne .
P ordf¢reren bifalder generelt forslagene i kommissionens hvidbog om dette emne , men er
bekymret for de mulige konsekvenser af den reform, som især sigter mod at
decentralisere gennemf¢relsen af konkurrencereglerne .
</table>
<tableCaption confidence="0.941610666666667">
Table 8: Examples of reorderings. S is source, B is baseline, and P is the SPTO approach. The elements that have
been reordered in the P sentence are marked alike in all sentences. The text in bold has changed place with the text in
italics.
</tableCaption>
<subsectionHeader confidence="0.999867">
6.3 Reordering analysis
</subsectionHeader>
<bodyText confidence="0.999971923076923">
In this section, we will show and discuss a few ex-
amples of the reorderings made by the SPTO ap-
proach. Table 8 contain two translations taken from
the test set.
In translation 1), the subject (bold) is correctly
moved to the right of the finite verb (italics), which
the baseline system fails to do. Moving the finite
verb away from the infinite verb ’feature’, however,
leads to incorrect agreement between these. While
the baseline correctly retains the infinite form (’st˚a’),
the language model forces another finite form (the
past tense ’stod’) in the SPTO reordering approach.
Translation 2) illustrates the handling of adver-
bials. The first reordering is in a main clause, there-
fore, the adverbial is moved to the right of the finite
verb. The second reordering occurs in a subordinate
clause, and the adverbial is moved to the left of the
finite verb. Neither of these are handled successfully
by the baseline system.
In this case, the reordering leads to better word
selection. The English ’aims to’ corresponds to the
Danish ’sigter mod’, which the SPTO approach gets
correct. However, the baseline system translates ’to’
to its much more common translation ’at’, because
’to’ is separated from ’aims’ by the adverbial ’prin-
cipally’.
</bodyText>
<sectionHeader confidence="0.989577" genericHeader="conclusions">
7 Conclusion and Future Plans
</sectionHeader>
<bodyText confidence="0.99998716">
We have described a novel approach to word re-
ordering in SMT, which successfully integrates
syntactically motivated reordering in phrase-based
SMT. This is achieved by reordering the input string,
but scoring on the output string. As opposed to pre-
vious approaches, this neither biases against phrase
internal nor external reorderings. We achieve an ab-
solute improvement in translation quality of 1.1 %
BLEU. A result that is supported by manual evalua-
tion, which shows that the SPTO approach is signif-
icantly superior to previous approaches.
In the future, we plan to apply this approach to
English-Arabic translation. We expect greater gains,
due to the higher need for reordering between these
less-related languages. We also want to examine the
relation between word alignment method and the ex-
tracted rules and the relationship between reordering
and word selection. Finally, a limitation of the cur-
rent experiments is that they only allow rule-based
external reorderings. Since the SPTO scoring is not
tied to a source reordering approach, we want to ex-
amine the effect of simply adding it as an additional
parameter to the baseline PSMT system. This way,
all external reorderings are made possible, but only
the rule-supported ones get promoted.
</bodyText>
<page confidence="0.998241">
53
</page>
<sectionHeader confidence="0.99588" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999570472727273">
Y. Al-Onaizan and K. Papineni. 2006. Distortion models
for statistical machine translation. In Proceedings of
44th ACL.
M. Buch-Kromann, J. Wedekind, and J. Elming. 2007.
The Copenhagen Danish-English Dependency Tree-
bank v. 2.0. http://www.isv.cbs.dk/∼mbk/cdt2.0.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proceedings ofACL-2007 Workshop on
Statistical Machine Translation.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st NAACL.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In Proceedings of the 14th AAAI.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of the 43rd ACL.
J. M. Crego and J. B. Mari˜no. 2007. Syntax-enhanced n-
gram-based smt. In Proceedings of the 11th MT Sum-
mit.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In Proceedings of the 11th MT
Summit.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. In Proceedings on the WSMT.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proceedings of AMTA.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of MT Sum-
mit.
C. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.
2007. A probabilistic approach to syntax-based re-
ordering for statistical machine translation. In Pro-
ceedings of the 45th ACL.
A. Stolcke. 2002. Srilm – an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proceedings of EMNLP-CoNLL.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
Proceedings of Coling.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In M. Jarke, J. Koehler,
and G. Lakemeyer, editors, KI - 2002: Advances in
Artificial Intelligence. 25. Annual German Conference
on AI. Springer Verlag.
Y. Zhang, R. Zens, and H. Ney. 2007. Improved chunk-
level reordering for statistical machine translation. In
Proceedings of the IWSLT.
</reference>
<page confidence="0.999017">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888316">
<title confidence="0.999629">Syntactic Reordering Integrated with Phrase-based SMT</title>
<author confidence="0.997056">Jakob</author>
<affiliation confidence="0.969578">Computational Copenhagen Business</affiliation>
<email confidence="0.966622">jel.isv@cbs.dk</email>
<abstract confidence="0.998153470588235">We present a novel approach to word reordering which successfully integrates syntactic structural knowledge with phrase-based SMT. This is done by constructing a lattice of alternatives based on automatically learned probabilistic syntactic rules. In decoding, the alternatives are scored based on the output word order, not the order of the input. Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT. On an English- Danish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU. Manual evaluation supports the claim that the present approach is significantly superior to previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of 44th ACL.</booktitle>
<contexts>
<context position="4884" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="762" endWordPosition="765">t to integrate syntactic information while retaining the strengths of the statistical approach. Several approaches do deterministic reordering. These do not integrate the reordering in the PSMT system; instead they place it outside the system by first reordering the source language, and then having a PSMT system translate from reordered source language to target language. (Collins et al., 2005; Wang et al., 2007) do this using manually created rules, and (Xia and McCord, 2004) and (Habash, 2007) use automatically extracted rules. All use rules extracted from syntactic parses. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. That is, there is no way to make up for bad information in later translation steps. Another approach is non-deterministic. This provides the decoder with both the original and the reordered source sentence. (Crego and Mari˜no, 2007) operate within Ngram-based SMT. They make use of syntactic structure to reorder the input into a word lattice. Since the paths are not weighted, the lattice merely narrows down the size of the search space. The decoder is not given reas</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Y. Al-Onaizan and K. Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of 44th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Buch-Kromann</author>
<author>J Wedekind</author>
<author>J Elming</author>
</authors>
<title>The Copenhagen Danish-English Dependency Treebank v.</title>
<date>2007</date>
<note>2.0. http://www.isv.cbs.dk/∼mbk/cdt2.0.</note>
<contexts>
<context position="13319" citStr="Buch-Kromann et al., 2007" startWordPosition="2268" endWordPosition="2271">tation of the context, and it produces easily readable rules that allow better understanding of the decisions being made. In section 6.2, extracted rules are exemplified and analyzed. The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition. The rules are extracted from the hand-aligned, Copenhagen Danish-English Dependency Treebank (Buch-Kromann et al., 2007). 5478 sentences from the news paper domain containing 111,805 English words and 100,185 Danish words. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). 4.3 Integrating rule-based reordering in PSMT The integration of the rule-based reordering in our PSMT system is carried out in two separate stages: 1. Reorder the source sentence to assimilate the word order of the target language. 2. Score the target word order according to the relevant rules. Stage 1) is done in a non-deterministic fashion by generating a word lattice as input in the spirit of </context>
</contexts>
<marker>Buch-Kromann, Wedekind, Elming, 2007</marker>
<rawString>M. Buch-Kromann, J. Wedekind, and J. Elming. 2007. The Copenhagen Danish-English Dependency Treebank v. 2.0. http://www.isv.cbs.dk/∼mbk/cdt2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL-2007 Workshop on Statistical Machine Translation.</booktitle>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and J. Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings ofACL-2007 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st NAACL.</booktitle>
<contexts>
<context position="13517" citStr="Charniak, 2000" startWordPosition="2298" endWordPosition="2299">rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition. The rules are extracted from the hand-aligned, Copenhagen Danish-English Dependency Treebank (Buch-Kromann et al., 2007). 5478 sentences from the news paper domain containing 111,805 English words and 100,185 Danish words. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). 4.3 Integrating rule-based reordering in PSMT The integration of the rule-based reordering in our PSMT system is carried out in two separate stages: 1. Reorder the source sentence to assimilate the word order of the target language. 2. Score the target word order according to the relevant rules. Stage 1) is done in a non-deterministic fashion by generating a word lattice as input in the spirit of e.g. (Zens et al., 2002; Crego and Mari˜no, 2007; Zhang et al., 2007). This way, the system has both the original word order, and the reorderings predicted by the rule set. The different paths of th</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Learning trees and rules with setvalued features.</title>
<date>1996</date>
<booktitle>In Proceedings of the 14th AAAI.</booktitle>
<contexts>
<context position="12576" citStr="Cohen, 1996" startWordPosition="2157" endWordPosition="2158">ition is occupied by something other than the subject in a main clause. In order to minimize the training data, word and POS sequences are limited to 4 words, and phrase structure (PS) sequences are limited to 3 constituents. In addition, an entry is only used if at least one of these three levels is not too long for both LS and RS, and too long contexts are not included in the set. This does not constrain the possible length of a reordering, since a PS sequence of length 1 can cover an entire sentence. In order to extract rules from the annotated data, we use a rule-based classifier, Ripper (Cohen, 1996). The motivation for using Ripper is that it allows features to be sets of strings, which fits well with our representation of the context, and it produces easily readable rules that allow better understanding of the decisions being made. In section 6.2, extracted rules are exemplified and analyzed. The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional paramet</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>W. Cohen. 1996. Learning trees and rules with setvalued features. In Proceedings of the 14th AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="2154" citStr="Collins et al., 2005" startWordPosition="327" endWordPosition="330">in a previously unseen sequence or over dis46 tances greater than the maximal phrase length. In this case, the system in essence relies on the target side language model to get the correct word order. The choice is made without knowing what the source is. Basically, it is a bias against phrase external reordering. It seems clear that reordering often depends on higher level linguistic information, which is absent from PSMT. In recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering. In works such as (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007), reordering decisions are done “deterministically”, thus placing these decisions outside the actual PSMT system by learning to translate from a reordered source language. (Crego and Mari˜no, 2007; Zhang et al., 2007; Li et al., 2007) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options. Still, there remains a basic conflict between the syntactic reordering rules and the PSMT system: one that is most likely due to the discrepancy between the translation units (phrases) and units of the lin</context>
<context position="4650" citStr="Collins et al., 2005" startWordPosition="725" endWordPosition="728">PSMT. In section 5, we describe the SMT system used in the experiments. Section 6 evaluates and discusses the present approach. 2 Related Work While several recent authors have achieved positive results, it has been difficult to integrate syntactic information while retaining the strengths of the statistical approach. Several approaches do deterministic reordering. These do not integrate the reordering in the PSMT system; instead they place it outside the system by first reordering the source language, and then having a PSMT system translate from reordered source language to target language. (Collins et al., 2005; Wang et al., 2007) do this using manually created rules, and (Xia and McCord, 2004) and (Habash, 2007) use automatically extracted rules. All use rules extracted from syntactic parses. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. That is, there is no way to make up for bad information in later translation steps. Another approach is non-deterministic. This provides the decoder with both the original and the reordered source sentence. (Crego and Mari˜no, 2007) op</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mari˜no</author>
</authors>
<title>Syntax-enhanced ngram-based smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th MT Summit.</booktitle>
<marker>Crego, Mari˜no, 2007</marker>
<rawString>J. M. Crego and J. B. Mari˜no. 2007. Syntax-enhanced ngram-based smt. In Proceedings of the 11th MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>Syntactic preprocessing for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th MT Summit.</booktitle>
<contexts>
<context position="2188" citStr="Habash, 2007" startWordPosition="335" endWordPosition="336">is46 tances greater than the maximal phrase length. In this case, the system in essence relies on the target side language model to get the correct word order. The choice is made without knowing what the source is. Basically, it is a bias against phrase external reordering. It seems clear that reordering often depends on higher level linguistic information, which is absent from PSMT. In recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering. In works such as (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007), reordering decisions are done “deterministically”, thus placing these decisions outside the actual PSMT system by learning to translate from a reordered source language. (Crego and Mari˜no, 2007; Zhang et al., 2007; Li et al., 2007) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options. Still, there remains a basic conflict between the syntactic reordering rules and the PSMT system: one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (Zhang et al., 2</context>
<context position="4754" citStr="Habash, 2007" startWordPosition="745" endWordPosition="746">resent approach. 2 Related Work While several recent authors have achieved positive results, it has been difficult to integrate syntactic information while retaining the strengths of the statistical approach. Several approaches do deterministic reordering. These do not integrate the reordering in the PSMT system; instead they place it outside the system by first reordering the source language, and then having a PSMT system translate from reordered source language to target language. (Collins et al., 2005; Wang et al., 2007) do this using manually created rules, and (Xia and McCord, 2004) and (Habash, 2007) use automatically extracted rules. All use rules extracted from syntactic parses. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. That is, there is no way to make up for bad information in later translation steps. Another approach is non-deterministic. This provides the decoder with both the original and the reordered source sentence. (Crego and Mari˜no, 2007) operate within Ngram-based SMT. They make use of syntactic structure to reorder the input into a word latt</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>N. Habash. 2007. Syntactic preprocessing for statistical machine translation. In Proceedings of the 11th MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings on the WSMT.</booktitle>
<contexts>
<context position="17356" citStr="Koehn and Monz, 2006" startWordPosition="2968" endWordPosition="2971">eordering between the two languages, but only the most general ones. If no rule has an opinion at a certain point in a sentence, the decoder is free to chose the phrase Figure 1: Example word lattice. translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the SPTO scoring in essence is compatible with other approaches such as a traditional PSMT system. We will, however, not examine this possibility further in the present paper. 5 The PSMT system The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The system was trained on the English and Danish part of the Europarl corpus version 3 (Koehn, 2005). Fourth quarter of 2000 was removed in order to use the common test set of 11369 sentences (330,082 English words and 309,942 Danish words with one reference) for testing. In addition, fourth quarter of 2001 was removed for development purposes. Of these, 10194 were used for various analysis purposes, thereby keeping the test data perfectly unseen. 500 sentences were taken from the development set for tuning the decoder paramet</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings on the WSMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="930" citStr="Koehn et al., 2003" startWordPosition="129" endWordPosition="132">ce of alternatives based on automatically learned probabilistic syntactic rules. In decoding, the alternatives are scored based on the output word order, not the order of the input. Unlike previous approaches, this makes it possible to successfully integrate syntactic reordering with phrase-based SMT. On an EnglishDanish task, we achieve an absolute improvement in translation quality of 1.1 % BLEU. Manual evaluation supports the claim that the present approach is significantly superior to previous approaches. 1 Introduction The emergence of phrase-based statistical machine translation (PSMT) (Koehn et al., 2003) has been one of the major developments in statistical approaches to translation. Allowing translation of word sequences (phrases) instead of single words provides SMT with a robustness in word selection and local word reordering. PSMT has two means of reordering the words. Either a phrase pair has been learned where the target word order differs from the source (phrase internal reordering), or distance penalized orderings of target phrases are attempted in decoding (phrase external reordering). The first solution is strong, the second is weak. The second solution is necessary for reorderings </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="18232" citStr="Koehn, 2004" startWordPosition="3117" endWordPosition="3118">,082 English words and 309,942 Danish words with one reference) for testing. In addition, fourth quarter of 2001 was removed for development purposes. Of these, 10194 were used for various analysis purposes, thereby keeping the test data perfectly unseen. 500 sentences were taken from the development set for tuning the decoder parameters. This was done using the Downhill Simplex algorithm. In total, 1,137,088 sentences containing 31,376,034 English words and 29,571,518 Danish words were left for training the phrase table and language model. The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. For the experiments, we use our own decoder which — except for the reordering model — uses the same knowledge sources as Pharaoh, i.e. bidirectional phrase translation model and lexical weighting model, phrase and word penalty, and target language model. Its behavior is comparable to Pharaoh when doing monotone decoding. The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002). It ex50 System Dev Test Swap Subset Baseline 0.262 0.252 0.234 no scoring 0.267 0.256 0.241 SO scoring 0.268 0.258 0.244 SPTO scoring </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="17524" citStr="Koehn, 2005" startWordPosition="3000" endWordPosition="3001">: Example word lattice. translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the SPTO scoring in essence is compatible with other approaches such as a traditional PSMT system. We will, however, not examine this possibility further in the present paper. 5 The PSMT system The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The system was trained on the English and Danish part of the Europarl corpus version 3 (Koehn, 2005). Fourth quarter of 2000 was removed in order to use the common test set of 11369 sentences (330,082 English words and 309,942 Danish words with one reference) for testing. In addition, fourth quarter of 2001 was removed for development purposes. Of these, 10194 were used for various analysis purposes, thereby keeping the test data perfectly unseen. 500 sentences were taken from the development set for tuning the decoder parameters. This was done using the Downhill Simplex algorithm. In total, 1,137,088 sentences containing 31,376,034 English words and 29,571,518 Danish words were left for tra</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>M Li</author>
<author>D Zhang</author>
<author>M Li</author>
<author>M Zhou</author>
<author>Y Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL.</booktitle>
<contexts>
<context position="2422" citStr="Li et al., 2007" startWordPosition="370" endWordPosition="373"> a bias against phrase external reordering. It seems clear that reordering often depends on higher level linguistic information, which is absent from PSMT. In recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering. In works such as (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007), reordering decisions are done “deterministically”, thus placing these decisions outside the actual PSMT system by learning to translate from a reordered source language. (Crego and Mari˜no, 2007; Zhang et al., 2007; Li et al., 2007) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options. Still, there remains a basic conflict between the syntactic reordering rules and the PSMT system: one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (Zhang et al., 2007) point out. In this paper, we proceed in the spirit of the nondeterministic approaches by providing the decoder with multiple source reorderings. But instead of scoring the input word order, we score the order of the output. By do</context>
<context position="6091" citStr="Li et al., 2007" startWordPosition="969" endWordPosition="972">iven reason to trust one path (reordering) over another. (Zhang et al., 2007) assign weights to the paths of their input word lattice. Instead of hierarchical linguistic structure, they use reordering rules based on POS and syntactic chunks, and train the system with both original and reordered source word order on a restricted data set (&lt;500K words). Their system does not out-perform a standard PSMT system. As they themselves point out, a reason for this might be that their reordering approach is not fully integrated with PSMT. This is one of the main problems addressed in the present work. (Li et al., 2007) use weighted n-best lists as input for the decoder. They use rules based on a syntactic parse, allowing children of a tree node to swap place. This is excessively restrictive. For example, a common reordering in English-Danish translation has the subject change place with the finite verb. Since the verb is often embedded in a VP containing additional words that should not be moved, such rules cannot be captured by local reordering on tree nodes. In many cases, the exact same word order that is obtained through a source sentence reordering, is also accessible through a phrase internal reorderi</context>
<context position="20173" citStr="Li et al., 2007" startWordPosition="3453" endWordPosition="3456">scussion The SPTO reordering approach is evaluated on the 11369 sentences of the common test set. Results are listed in table 4 along with results on the development set. We also report on the swap subset. These are the 3853 sentences where the approach actually motivated reorderings in the test set, internal or external. The remaining 7516 sentences were not influenced by the SPTO reordering approach. We report on 1) the baseline PSMT system, 2) a system provided with a rule reordered word lattice but no scoring, 3) the same system but with an SO scoring in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally 4) the same system but with the SPTO scoring. The SPTO approach gets an increase over the baseline PSMT system of 0.6 % BLEU. The swap subset, however, shows that the extracted rules are somewhat restricted, only resulting in swap in s of the sentences. The relevant set, i.e. the set where the present approach actually differs from the baseline, is therefore the swap subset. This way, we concentrate on the actual focus of the paper, namely the syntactically motivated SPTO reordering. Here we System BLEU Avr. Human rating Baseline 0.234 3.00 (2.56) no scoring 0.240 3.00 (2.74) SO </context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>C. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of the 45th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="17422" citStr="Stolcke, 2002" startWordPosition="2981" endWordPosition="2982">o rule has an opinion at a certain point in a sentence, the decoder is free to chose the phrase Figure 1: Example word lattice. translation it prefers without reordering cost. Separating the scoring from the source language reordering also has the advantage that the SPTO scoring in essence is compatible with other approaches such as a traditional PSMT system. We will, however, not examine this possibility further in the present paper. 5 The PSMT system The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The system was trained on the English and Danish part of the Europarl corpus version 3 (Koehn, 2005). Fourth quarter of 2000 was removed in order to use the common test set of 11369 sentences (330,082 English words and 309,942 Danish words with one reference) for testing. In addition, fourth quarter of 2001 was removed for development purposes. Of these, 10194 were used for various analysis purposes, thereby keeping the test data perfectly unseen. 500 sentences were taken from the development set for tuning the decoder parameters. This was done using the Downhill Simplex algorithm. In total,</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2173" citStr="Wang et al., 2007" startWordPosition="331" endWordPosition="334"> sequence or over dis46 tances greater than the maximal phrase length. In this case, the system in essence relies on the target side language model to get the correct word order. The choice is made without knowing what the source is. Basically, it is a bias against phrase external reordering. It seems clear that reordering often depends on higher level linguistic information, which is absent from PSMT. In recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering. In works such as (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007), reordering decisions are done “deterministically”, thus placing these decisions outside the actual PSMT system by learning to translate from a reordered source language. (Crego and Mari˜no, 2007; Zhang et al., 2007; Li et al., 2007) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options. Still, there remains a basic conflict between the syntactic reordering rules and the PSMT system: one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (</context>
<context position="4670" citStr="Wang et al., 2007" startWordPosition="729" endWordPosition="732"> describe the SMT system used in the experiments. Section 6 evaluates and discusses the present approach. 2 Related Work While several recent authors have achieved positive results, it has been difficult to integrate syntactic information while retaining the strengths of the statistical approach. Several approaches do deterministic reordering. These do not integrate the reordering in the PSMT system; instead they place it outside the system by first reordering the source language, and then having a PSMT system translate from reordered source language to target language. (Collins et al., 2005; Wang et al., 2007) do this using manually created rules, and (Xia and McCord, 2004) and (Habash, 2007) use automatically extracted rules. All use rules extracted from syntactic parses. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. That is, there is no way to make up for bad information in later translation steps. Another approach is non-deterministic. This provides the decoder with both the original and the reordered source sentence. (Crego and Mari˜no, 2007) operate within Ngram-b</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and P. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling.</booktitle>
<contexts>
<context position="2132" citStr="Xia and McCord, 2004" startWordPosition="323" endWordPosition="326">y for reorderings within a previously unseen sequence or over dis46 tances greater than the maximal phrase length. In this case, the system in essence relies on the target side language model to get the correct word order. The choice is made without knowing what the source is. Basically, it is a bias against phrase external reordering. It seems clear that reordering often depends on higher level linguistic information, which is absent from PSMT. In recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering. In works such as (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007), reordering decisions are done “deterministically”, thus placing these decisions outside the actual PSMT system by learning to translate from a reordered source language. (Crego and Mari˜no, 2007; Zhang et al., 2007; Li et al., 2007) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options. Still, there remains a basic conflict between the syntactic reordering rules and the PSMT system: one that is most likely due to the discrepancy between the translation units (phrases</context>
<context position="4735" citStr="Xia and McCord, 2004" startWordPosition="740" endWordPosition="743">aluates and discusses the present approach. 2 Related Work While several recent authors have achieved positive results, it has been difficult to integrate syntactic information while retaining the strengths of the statistical approach. Several approaches do deterministic reordering. These do not integrate the reordering in the PSMT system; instead they place it outside the system by first reordering the source language, and then having a PSMT system translate from reordered source language to target language. (Collins et al., 2005; Wang et al., 2007) do this using manually created rules, and (Xia and McCord, 2004) and (Habash, 2007) use automatically extracted rules. All use rules extracted from syntactic parses. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. That is, there is no way to make up for bad information in later translation steps. Another approach is non-deterministic. This provides the decoder with both the original and the reordered source sentence. (Crego and Mari˜no, 2007) operate within Ngram-based SMT. They make use of syntactic structure to reorder the inp</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>F. Xia and M. McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>2002: Advances in Artificial Intelligence. 25. Annual German Conference on AI.</booktitle>
<editor>In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="13942" citStr="Zens et al., 2002" startWordPosition="2369" endWordPosition="2372"> sentences from the news paper domain containing 111,805 English words and 100,185 Danish words. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). 4.3 Integrating rule-based reordering in PSMT The integration of the rule-based reordering in our PSMT system is carried out in two separate stages: 1. Reorder the source sentence to assimilate the word order of the target language. 2. Score the target word order according to the relevant rules. Stage 1) is done in a non-deterministic fashion by generating a word lattice as input in the spirit of e.g. (Zens et al., 2002; Crego and Mari˜no, 2007; Zhang et al., 2007). This way, the system has both the original word order, and the reorderings predicted by the rule set. The different paths of the word lattice are merely given as equal suggestions to the decoder. They are in no way individually weighted. Separating stage 2) from stage 1) is motivated by the fact that reordering can have two distinct origins. They can occur because of stage 1), i.e. the lattice reordering of the original English word order (phrase external reordering), and they can occur inside a single phrase (phrase internal reordering). We are,</context>
<context position="18696" citStr="Zens et al., 2002" startWordPosition="3190" endWordPosition="3193"> and 29,571,518 Danish words were left for training the phrase table and language model. The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. For the experiments, we use our own decoder which — except for the reordering model — uses the same knowledge sources as Pharaoh, i.e. bidirectional phrase translation model and lexical weighting model, phrase and word penalty, and target language model. Its behavior is comparable to Pharaoh when doing monotone decoding. The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002). It ex50 System Dev Test Swap Subset Baseline 0.262 0.252 0.234 no scoring 0.267 0.256 0.241 SO scoring 0.268 0.258 0.244 SPTO scoring 0.268 0.258 0.245 Table 4: BLEU scores for different scoring methods. pects a word lattice as input. Figure 1 shows the word lattice for the example in table 3. Since the input format defines all possible word orders, a simple monotone search is sufficient. Using a language model of order n, for each hypothezised target string ending in the same n-1- gram, we only have to extend the highest scoring hypothesis. None of the others can possibly outperform this on</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical machine translation. In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI - 2002: Advances in Artificial Intelligence. 25. Annual German Conference on AI. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improved chunklevel reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the IWSLT.</booktitle>
<contexts>
<context position="2404" citStr="Zhang et al., 2007" startWordPosition="366" endWordPosition="369">is. Basically, it is a bias against phrase external reordering. It seems clear that reordering often depends on higher level linguistic information, which is absent from PSMT. In recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering. In works such as (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007), reordering decisions are done “deterministically”, thus placing these decisions outside the actual PSMT system by learning to translate from a reordered source language. (Crego and Mari˜no, 2007; Zhang et al., 2007; Li et al., 2007) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options. Still, there remains a basic conflict between the syntactic reordering rules and the PSMT system: one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (Zhang et al., 2007) point out. In this paper, we proceed in the spirit of the nondeterministic approaches by providing the decoder with multiple source reorderings. But instead of scoring the input word order, we score the order of</context>
<context position="5552" citStr="Zhang et al., 2007" startWordPosition="877" endWordPosition="880">choices are beyond the scope of optimization and cannot be undone by the decoder. That is, there is no way to make up for bad information in later translation steps. Another approach is non-deterministic. This provides the decoder with both the original and the reordered source sentence. (Crego and Mari˜no, 2007) operate within Ngram-based SMT. They make use of syntactic structure to reorder the input into a word lattice. Since the paths are not weighted, the lattice merely narrows down the size of the search space. The decoder is not given reason to trust one path (reordering) over another. (Zhang et al., 2007) assign weights to the paths of their input word lattice. Instead of hierarchical linguistic structure, they use reordering rules based on POS and syntactic chunks, and train the system with both original and reordered source word order on a restricted data set (&lt;500K words). Their system does not out-perform a standard PSMT system. As they themselves point out, a reason for this might be that their reordering approach is not fully integrated with PSMT. This is one of the main problems addressed in the present work. (Li et al., 2007) use weighted n-best lists as input for the decoder. They use</context>
<context position="6778" citStr="Zhang et al., 2007" startWordPosition="1086" endWordPosition="1089">based on a syntactic parse, allowing children of a tree node to swap place. This is excessively restrictive. For example, a common reordering in English-Danish translation has the subject change place with the finite verb. Since the verb is often embedded in a VP containing additional words that should not be moved, such rules cannot be captured by local reordering on tree nodes. In many cases, the exact same word order that is obtained through a source sentence reordering, is also accessible through a phrase internal reordering. A negative consequence of source order (SO) scoring as done by (Zhang et al., 2007) and (Li et al., 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. As described in section 4.3, we solve this problem by reordering the input string, but scoring the output string, thus allowing the strengths of PSMT to co-exist with rule-based reordering. 3 Language comparison The two languages examined in this investigation, English and Danish, are very similar from a structural point of view. A word alignment will most often display an almost one-to-one correlation. In the hand-aligned data, only 39% of the sentences c</context>
<context position="13988" citStr="Zhang et al., 2007" startWordPosition="2377" endWordPosition="2380">ning 111,805 English words and 100,185 Danish words. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). 4.3 Integrating rule-based reordering in PSMT The integration of the rule-based reordering in our PSMT system is carried out in two separate stages: 1. Reorder the source sentence to assimilate the word order of the target language. 2. Score the target word order according to the relevant rules. Stage 1) is done in a non-deterministic fashion by generating a word lattice as input in the spirit of e.g. (Zens et al., 2002; Crego and Mari˜no, 2007; Zhang et al., 2007). This way, the system has both the original word order, and the reorderings predicted by the rule set. The different paths of the word lattice are merely given as equal suggestions to the decoder. They are in no way individually weighted. Separating stage 2) from stage 1) is motivated by the fact that reordering can have two distinct origins. They can occur because of stage 1), i.e. the lattice reordering of the original English word order (phrase external reordering), and they can occur inside a single phrase (phrase internal reordering). We are, however, interested in doing phraseindependen</context>
<context position="20155" citStr="Zhang et al., 2007" startWordPosition="3449" endWordPosition="3452">n 6.1 Results and discussion The SPTO reordering approach is evaluated on the 11369 sentences of the common test set. Results are listed in table 4 along with results on the development set. We also report on the swap subset. These are the 3853 sentences where the approach actually motivated reorderings in the test set, internal or external. The remaining 7516 sentences were not influenced by the SPTO reordering approach. We report on 1) the baseline PSMT system, 2) a system provided with a rule reordered word lattice but no scoring, 3) the same system but with an SO scoring in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally 4) the same system but with the SPTO scoring. The SPTO approach gets an increase over the baseline PSMT system of 0.6 % BLEU. The swap subset, however, shows that the extracted rules are somewhat restricted, only resulting in swap in s of the sentences. The relevant set, i.e. the set where the present approach actually differs from the baseline, is therefore the swap subset. This way, we concentrate on the actual focus of the paper, namely the syntactically motivated SPTO reordering. Here we System BLEU Avr. Human rating Baseline 0.234 3.00 (2.56) no scoring 0.2</context>
</contexts>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Y. Zhang, R. Zens, and H. Ney. 2007. Improved chunklevel reordering for statistical machine translation. In Proceedings of the IWSLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>