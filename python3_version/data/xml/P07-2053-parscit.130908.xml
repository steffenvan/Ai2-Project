<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002244">
<title confidence="0.96174">
HunPos – an open source trigram tagger
</title>
<author confidence="0.989609">
P´eter Hal´acsy
</author>
<affiliation confidence="0.937338">
Budapest U. of Technology
MOKK Media Research
</affiliation>
<address confidence="0.911704">
H-1111 Budapest, Stoczek u 2
</address>
<email confidence="0.997762">
peter@halacsy.com
</email>
<author confidence="0.736214">
Andr´as Kornai
</author>
<affiliation confidence="0.631186">
MetaCarta Inc.
</affiliation>
<address confidence="0.58585">
350 Massachusetts Ave.
Cambridge MA 02139
</address>
<email confidence="0.995957">
andras@kornai.com
</email>
<author confidence="0.951154">
Csaba Oravecz
</author>
<affiliation confidence="0.964612">
Hungarian Academy of Sciences
Institute of Linguistics
</affiliation>
<address confidence="0.884194">
H-1068 Budapest, Benczur u. 33.
</address>
<email confidence="0.99934">
oravecz@nytud.hu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906947368421">
In the world of non-proprietary NLP soft-
ware the standard, and perhaps the best,
HMM-based POS tagger is TnT (Brants,
2000). We argue here that some of the crit-
icism aimed at HMM performance on lan-
guages with rich morphology should more
properly be directed at TnT’s peculiar li-
cense, free but not open source, since it is
those details of the implementation which
are hidden from the user that hold the key
for improved POS tagging across a wider
variety of languages. We present HunPos1,
a free and open source (LGPL-licensed) al-
ternative, which can be tuned by the user to
fully utilize the potential of HMM architec-
tures, offering performance comparable to
more complex models, but preserving the
ease and speed of the training and tagging
process.
</bodyText>
<sectionHeader confidence="0.997845" genericHeader="introduction">
0 Introduction
</sectionHeader>
<bodyText confidence="0.9998579">
Even without a formal survey it is clear that
TnT (Brants, 2000) is used widely in research
labs throughout the world: Google Scholar shows
over 400 citations. For research purposes TnT is
freely available, but only in executable form (closed
source). Its greatest advantage is its speed, impor-
tant both for a fast tuning cycle and when dealing
with large corpora, especially when the POS tag-
ger is but one component in a larger information re-
trieval, information extraction, or question answer-
</bodyText>
<footnote confidence="0.941212">
1http://mokk.bme.hu/resources/hunpos/
</footnote>
<bodyText confidence="0.995069838709678">
ing system. Though taggers based on dependency
networks (Toutanova et al., 2003), SVM (Gim´enez
and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996),
CRF (Smith et al., 2005), and other methods may
reach slightly better results, their train/test cycle is
orders of magnitude longer.
A ubiquitous problem in HMM tagging originates
from the standard way of calculating lexical prob-
abilities by means of a lexicon generated during
training. In highly inflecting languages considerably
more unseen words will be present in the test data
than in more isolating languages, which largely ac-
counts for the drop in the performance of n-gram
taggers when moving away from English. To mit-
igate the effect one needs a morphological dictio-
nary (Hajiˇc et al., 2001) or a morphological analyzer
(Hakkani-T¨ur et al., 2000), but if the implementation
source is closed there is no handy way to incorporate
morphological knowledge in the tagger.
The paper is structured as follows. In Section 1
we present our own system, HunPos, while in Sec-
tion 2 we describe some of the implementation de-
tails of TnT that we believe influence the perfor-
mance of a HMM based tagging system. We eval-
uate the system and compare it to TnT on a variety
of tasks in Section 3. We don’t necessarily consider
HunPos to be significantly better than TnT, but we
argue that we could reach better results, and so could
others coming after us, because the system is open
to explore all kinds of fine-tuning strategies. Some
concluding remarks close the paper in Section 4.
</bodyText>
<page confidence="0.982984">
209
</page>
<bodyText confidence="0.4724845">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 209–212,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.813035" genericHeader="method">
1 Main features of HunPos
</sectionHeader>
<bodyText confidence="0.999964951219512">
HunPos has been implemented in OCaml, a high-
level language which supports a succinct, well-
maintainable coding style. OCaml has a high-
performance native-code compiler (Doligez et al.,
2004) that can produce a C library with the speed
of a C/C++ implementation.
On the whole HunPos is a straightforward trigram
system estimating the probabilities
for a given sequence of words w1 ... wT (the addi-
tional tags t−1, t0, and tT+1 are for sentence bound-
ary markers). Notice that unlike traditional HMM
models, we estimate emission/lexicon probabilities
based on the current tag and the previous tag as well.
As we shall see in the next Section, using tag bi-
grams to condition the emissions can lead to as much
as 10% reduction in the error rate. (In fact, HunPos
can handle a context window of any size, but on the
limited training sets available to us increasing this
parameter beyond 2 gives no further improvement.)
As for contextualized lexical probabilities, our ex-
tension is very similar to Banko and Moore (2004)
who use P(wi|ti−1,ti,ti+1) lexical probabilities
and found, on the Penn Treebank, that “incorporat-
ing more context into an HMM when estimating lex-
ical probabilities improved accuracy from 95.87% to
96.59%”. One difficulty with their approach, noted
by Banko and Moore (2004), is the treatment of un-
seen words: their method requires a full dictionary
that lists what tags are possible for each word. To
be sure, for isolating languages such information is
generally available from machine readable dictio-
naries which are often large enough to make the out
of vocabulary problem negligible. But in our situ-
ation this amounts to idealized morphological ana-
lyzers (MA) that have their stem list extended so as
to have no OOV on the test set.
The strong side of TnT is its suffix guessing algo-
rithm that is triggered by unseen words. From the
training set TnT builds a trie from the endings of
words appearing less than n times in the corpus, and
memorizes the tag distribution for each suffix.2 A
</bodyText>
<footnote confidence="0.549292">
2The parameter n cannot be externally set — it is docu-
mented as 10 but we believe it to be higher.
</footnote>
<bodyText confidence="0.9999242">
clear advantage of this approach is the probabilis-
tic weighting of each label, however, under default
settings the algorithm proposes a lot more possible
tags than a morphological analyzer would. To facil-
itate the use of MA, HunPos has hooks to work with
a morphological analyzer (lexicon), which might
still leave some OOV items. As we shall see in
Section 3, the key issue is that for unseen words
the HMM search space may be narrowed down to
the alternatives proposed by this module, which not
only speeds up search but also very significantly
improves precision. That is, for unseen words the
MA will generate the possible labels, to which the
weights are assigned by the suffix guessing algo-
rithm.
</bodyText>
<sectionHeader confidence="0.995641" genericHeader="method">
2 Inside TnT
</sectionHeader>
<bodyText confidence="0.999866181818182">
Here we describe, following the lead of (Jurish,
2003), some non-trivial features of TnT sometimes
only hinted at in the user guide, but clearly evident
from its behavior on real and experimentally ad-
justed corpora. For the most part, these features are
clever hacks, and it is unfortunate that neither Brants
(2000) nor the standard HMM textbooks mention
them, especially as they often yield more signifi-
cant error reduction than the move from HMM to
other architectures. Naturally, these features are also
available in HunPos.
</bodyText>
<subsectionHeader confidence="0.88152">
2.1 Cardinals
</subsectionHeader>
<bodyText confidence="0.9834108">
For the following regular expressions TnT learns the
tag distribution of the training corpus separately to
give more reliable estimates for open class items like
numbers unseen during training:
ˆ[0-9]+$
ˆ[0-9]+\.$
ˆ[0-9.,:-]+[0-9]+$
ˆ[0-9]+[a-zA-Z]{1,3}$
(The regexps are only inferred – we haven’t at-
tempted to trace the execution.) After this, at test
time, if the word is not found in the lexicon (nu-
merals are added to the lexicon like all other items)
TnT checks whether the unseen word matches some
of the regexps, and uses the distribution learned for
this regexp to guess the tag.
</bodyText>
<equation confidence="0.542581">
argmax P(tT+1|tT) T P(ti|ti−1, ti−2)P(wi|ti−1, ti)
t1...tT i=1
</equation>
<page confidence="0.983304">
210
</page>
<subsectionHeader confidence="0.944608">
2.2 Upper- and lowercase
</subsectionHeader>
<bodyText confidence="0.9999682">
The case of individual words may carry relevant in-
formation for tagging, so it is well worth preserving
the uppercase feature for items seen as such in train-
ing. For unseen words TnT builds two suffix tries:
if the word begins with uppercase one trie is used,
for lowercase words the other trie is applied. The
undocumented trick is to try to lookup the word in
sentence initial position from the training lexicon in
its lowercase variant, which contributes noticeably
to the better performance of the system.
</bodyText>
<sectionHeader confidence="0.99872" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99791475">
English For the English evaluation we used the
WSJ data from Penn Treebank II. We extracted sen-
tences from the parse trees. We split data into train-
ing and test set in the standard way (Table 1).
</bodyText>
<table confidence="0.995513">
Set Sect’ns Sent. Tokens Unseen
Train 0-18 38,219 912,344 0
Test 22-24 5,462 129,654 2.81%
</table>
<tableCaption confidence="0.999591">
Table 1: Data set splits used for English
</tableCaption>
<bodyText confidence="0.90833953125">
As Table 2 shows HunPos achieves performance
comparable to TnT for English. The increase in the
emission order clearly improves this performance.
seen unseen overall
TnT 96.77% 85.91% 96.46%
HunPos 1 96.76% 86.90% 96.49%
HunPos 2 96.88% 86.13% 96.58%
Table 2: WSJ tagging accuracy, HunPos with first
and second order emission/lexicon probabilities
If we follow Banko and Moore (2004) and con-
struct a full (no OOV) morphological lexicon from
the tagged version of the test corpus, we obtain
96.95% precision where theirs was 96.59%. For
words seen, precision improves by an entirely neg-
ligible 0.01%, but for unseen words it improves by
10%, from 86.13% to 98.82%. This surprising result
arises from the fact that there are a plenty of unam-
biguous tokens (especially the proper names that are
usually unseen) in the test corpus.
What this shows is not just that morphology mat-
ters (this is actually not that visible for English), but
that the difference between systems can only be ap-
preciated once the small (and scantily documented)
tricks are factored out. The reason why Banko and
Moore (2004) get less than HunPos is not because
their system is inherently worse, but rather because
it lacks the engineering hacks built into TnT and
HunPos.
Hungarian We evaluated the different models
by tenfold cross-validation on the Szeged Corpus
(Csendes et al., 2004), with the relevant data in pre-
sented Table 3.
</bodyText>
<table confidence="0.997352333333333">
Set Sent. Tokens Unseens OOV
Train 63,075 1,044,914 0 N.A
Test 7,008 116,101 9.59% 5.64%
</table>
<tableCaption confidence="0.999874">
Table 3: Data set splits used for Hungarian.
</tableCaption>
<bodyText confidence="0.995105">
Note that the proportion of unseen words, nearly
10%, is more than three times higher than in En-
glish. Most of these words were covered by the mor-
phological analyzer (Tr´on et al., 2006) but still 28%
of unseen words were only guessed. However, this
is just 2.86% of the whole corpus, in the magnitude
similar to English.
seen unseen overall
98.34% 88.96% 97.27%
98.58% 87.97% 97.40%
98.32% 96.01% 98.03%
98.56% 95.96% 98.24%
Table 4: Tagging accuracy for Hungarian of HunPos
with and without morphological lexicon and with
first and second order emission/lexicon probabili-
ties.
On the same corpus TnT had 97.42% and Hal´acsy
et al. (2006) reached 98.17% with a MaxEnt tag-
ger that used the TnT output as a feature. HunPos
gets as good performance in one minute as this Max-
Ent model which took three hours to go through the
train/test cycle.
</bodyText>
<sectionHeader confidence="0.938283" genericHeader="conclusions">
4 Concluding remarks
</sectionHeader>
<bodyText confidence="0.99956725">
Though there can be little doubt that the ruling sys-
tem of bakeoffs actively encourages a degree of one-
upmanship, our paper and our software are not of-
fered in a competitive spirit. As we said at the out-
</bodyText>
<figure confidence="0.5382208">
morph lex order
no 1
yes 2
1
2
</figure>
<page confidence="0.998127">
211
</page>
<bodyText confidence="0.999979791666667">
set, we don’t necessarily believe HunPos to be in any
way better than TnT, and certainly the main ideas
have been pioneered by DeRose (1988), Church
(1988), and others long before this generation of
HMM work. But to improve the results beyond what
a basic HMM can achieve one needs to tune the sys-
tem, and progress can only be made if the experi-
ments are end to end replicable.
There is no doubt many other systems could be
tweaked further and improve on our results – what
matters is that anybody could now also tweak Hun-
Pos without any restriction to improve the state of
the art. Such tweaking can bring surprising results,
e.g. the conclusion, strongly supported by the results
presented here, that HMM tagging is actually quite
competitive with, and orders of magnitude faster
than, the current generation of learning algorithms
including SVM and MaxEnt. No matter how good
TnT was to begin with, the closed source has hin-
dered its progress to the point that inherently clum-
sier, but better tweakable algorithms could overtake
HMMs, a situation that HunPos has now hopefully
changed at least for languages with more complex
morphologies.
</bodyText>
<sectionHeader confidence="0.966731" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9633545">
We thank Thorsten Brants for TnT, and Gy¨orgy
Gyepesi for constant help and encouragement.
</bodyText>
<sectionHeader confidence="0.998511" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999539015151515">
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING ’04: Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 556, Morristown, NJ, USA.
Association for Computational Linguistics.
Thorsten Brants. 2000. TnT – a statistical part-of-speech
tagger. In Proceedings of the Sixth Applied Natural
Language Processing Conference (ANLP-2000), Seat-
tle, WA.
Kenneth Ward Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In Pro-
ceedings of the second conference on Applied natural
language processing, pages 136–143, Morristown, NJ,
USA. Association for Computational Linguistics.
D´ora Csendes, J´an´os Csirik, and Tibor Gyim´othy. 2004.
The Szeged Corpus: A POS tagged and syntacti-
cally annotated Hungarian natural language corpus.
In Karel Pala Petr Sojka, Ivan Kopecek, editor, Text,
Speech and Dialogue: 7th International Conference,
TSD, pages 41–47.
Steven J. DeRose. 1988. Grammatical category disam-
biguation by statistical optimization. Computational
Linguistics, 14:31–39.
Damien Doligez, Jacques Garrigue, Didier R´emy, and
J´erˆome Vouillon, 2004. The Objective Caml system.
Institut National de Recherche en Informatique et en
Automatique.
Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accu-
rate part-of-speech tagging: The svm approach revis-
ited. In Proceedings of RANLP, pages 153–163.
Jan Hajiˇc, Pavel Krbec, Karel Oliva, Pavel Kvˇetoˇn, and
Vladimir Petkeviˇc. 2001. Serial combination of rules
and statistics: A case study in Czech tagging. In
Proceedings of the 39th Association of Computational
Linguistics Conference, pages 260–267, Toulouse,
France.
Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proceedings of the 18th
conference on Computational linguistics, pages 285–
291, Saarbr¨ucken, Germany.
P´eter Hal´acsy, Andr´as Kornai, Csaba Oravecz, Viktor
Tr´on, and D´aniel Varga. 2006. Using a morphological
analyzer in high precision POS tagging of Hungarian.
In Proceedings ofLREC 2006, pages 2245–2248.
Bryan Jurish. 2003. A hybrid approach to part-of-speech
tagging. Technical report, Berlin-Brandenburgische
Akademie der Wissenschaften.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Karel Pala Petr Sojka,
Ivan Kopecek, editor, Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 133–142, University of Pennsylvania.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, Vancouver.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings ofHLT-NAACL, pages 252–259.
Viktor Tr´on, P´eter Hal´acsy, P´eter Rebrus, Andr´as Rung,
P´eter Vajda, and Eszter Simon. 2006. Morphdb.hu:
Hungarian lexical database and morphological gram-
mar. In Proceedings ofLREC 2006, pages 1670–1673.
</reference>
<page confidence="0.998494">
212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.484808">
<title confidence="0.998609">HunPos – an open source trigram tagger</title>
<author confidence="0.999516">P´eter Hal´acsy</author>
<affiliation confidence="0.9265165">Budapest U. of Technology MOKK Media Research</affiliation>
<address confidence="0.876375">H-1111 Budapest, Stoczek u 2</address>
<email confidence="0.999402">peter@halacsy.com</email>
<author confidence="0.999867">Andr´as Kornai</author>
<affiliation confidence="0.999847">MetaCarta Inc.</affiliation>
<address confidence="0.9997345">350 Massachusetts Ave. Cambridge MA 02139</address>
<email confidence="0.999387">andras@kornai.com</email>
<author confidence="0.74813">Csaba Oravecz</author>
<affiliation confidence="0.9856905">Hungarian Academy of Sciences Institute of Linguistics</affiliation>
<address confidence="0.919795">H-1068 Budapest, Benczur u. 33.</address>
<email confidence="0.988926">oravecz@nytud.hu</email>
<abstract confidence="0.99755495">In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000). We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT’s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider of languages. We present a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part of speech tagging in context.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4368" citStr="Banko and Moore (2004)" startWordPosition="709" endWordPosition="712">additional tags t−1, t0, and tT+1 are for sentence boundary markers). Notice that unlike traditional HMM models, we estimate emission/lexicon probabilities based on the current tag and the previous tag as well. As we shall see in the next Section, using tag bigrams to condition the emissions can lead to as much as 10% reduction in the error rate. (In fact, HunPos can handle a context window of any size, but on the limited training sets available to us increasing this parameter beyond 2 gives no further improvement.) As for contextualized lexical probabilities, our extension is very similar to Banko and Moore (2004) who use P(wi|ti−1,ti,ti+1) lexical probabilities and found, on the Penn Treebank, that “incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%”. One difficulty with their approach, noted by Banko and Moore (2004), is the treatment of unseen words: their method requires a full dictionary that lists what tags are possible for each word. To be sure, for isolating languages such information is generally available from machine readable dictionaries which are often large enough to make the out of vocabulary problem negligible. But in our</context>
<context position="8736" citStr="Banko and Moore (2004)" startWordPosition="1424" endWordPosition="1427">d sentences from the parse trees. We split data into training and test set in the standard way (Table 1). Set Sect’ns Sent. Tokens Unseen Train 0-18 38,219 912,344 0 Test 22-24 5,462 129,654 2.81% Table 1: Data set splits used for English As Table 2 shows HunPos achieves performance comparable to TnT for English. The increase in the emission order clearly improves this performance. seen unseen overall TnT 96.77% 85.91% 96.46% HunPos 1 96.76% 86.90% 96.49% HunPos 2 96.88% 86.13% 96.58% Table 2: WSJ tagging accuracy, HunPos with first and second order emission/lexicon probabilities If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%. For words seen, precision improves by an entirely negligible 0.01%, but for unseen words it improves by 10%, from 86.13% to 98.82%. This surprising result arises from the fact that there are a plenty of unambiguous tokens (especially the proper names that are usually unseen) in the test corpus. What this shows is not just that morphology matters (this is actually not that visible for English), but that the difference between systems can only be ap</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>Michele Banko and Robert C. Moore. 2004. Part of speech tagging in context. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 556, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-2000),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="1197" citStr="Brants, 2000" startWordPosition="190" endWordPosition="191">ology should more properly be directed at TnT’s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages. We present HunPos1, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process. 0 Introduction Even without a formal survey it is clear that TnT (Brants, 2000) is used widely in research labs throughout the world: Google Scholar shows over 400 citations. For research purposes TnT is freely available, but only in executable form (closed source). Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer1http://mokk.bme.hu/resources/hunpos/ ing system. Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gim´enez and M`arquez, 2003), MaxEnt (Ratnapa</context>
<context position="6493" citStr="Brants (2000)" startWordPosition="1075" endWordPosition="1076">e may be narrowed down to the alternatives proposed by this module, which not only speeds up search but also very significantly improves precision. That is, for unseen words the MA will generate the possible labels, to which the weights are assigned by the suffix guessing algorithm. 2 Inside TnT Here we describe, following the lead of (Jurish, 2003), some non-trivial features of TnT sometimes only hinted at in the user guide, but clearly evident from its behavior on real and experimentally adjusted corpora. For the most part, these features are clever hacks, and it is unfortunate that neither Brants (2000) nor the standard HMM textbooks mention them, especially as they often yield more significant error reduction than the move from HMM to other architectures. Naturally, these features are also available in HunPos. 2.1 Cardinals For the following regular expressions TnT learns the tag distribution of the training corpus separately to give more reliable estimates for open class items like numbers unseen during training: ˆ[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]+$ ˆ[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]+\.$ ˆ[0-9.,:-]+[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]+$ ˆ[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]+[a-zA-Z]{1,3}$ (The regexps are o</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-2000), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the second conference on Applied natural language processing,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11148" citStr="Church (1988)" startWordPosition="1846" endWordPosition="1847">ached 98.17% with a MaxEnt tagger that used the TnT output as a feature. HunPos gets as good performance in one minute as this MaxEnt model which took three hours to go through the train/test cycle. 4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit. As we said at the outmorph lex order no 1 yes 2 1 2 211 set, we don’t necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work. But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable. There is no doubt many other systems could be tweaked further and improve on our results – what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art. Such tweaking can bring surprising results, e.g. the conclusion, strongly supported by the results presented here, that HMM tagging is actually quite competitive with, and </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Ward Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the second conference on Applied natural language processing, pages 136–143, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D´ora Csendes</author>
<author>J´an´os Csirik</author>
<author>Tibor Gyim´othy</author>
</authors>
<title>The Szeged Corpus: A POS tagged and syntactically annotated Hungarian natural language corpus.</title>
<date>2004</date>
<booktitle>Text, Speech and Dialogue: 7th International Conference, TSD,</booktitle>
<pages>41--47</pages>
<editor>In Karel Pala Petr Sojka, Ivan Kopecek, editor,</editor>
<marker>Csendes, Csirik, Gyim´othy, 2004</marker>
<rawString>D´ora Csendes, J´an´os Csirik, and Tibor Gyim´othy. 2004. The Szeged Corpus: A POS tagged and syntactically annotated Hungarian natural language corpus. In Karel Pala Petr Sojka, Ivan Kopecek, editor, Text, Speech and Dialogue: 7th International Conference, TSD, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<pages>14--31</pages>
<contexts>
<context position="11133" citStr="DeRose (1988)" startWordPosition="1844" endWordPosition="1845">t al. (2006) reached 98.17% with a MaxEnt tagger that used the TnT output as a feature. HunPos gets as good performance in one minute as this MaxEnt model which took three hours to go through the train/test cycle. 4 Concluding remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit. As we said at the outmorph lex order no 1 yes 2 1 2 211 set, we don’t necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work. But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable. There is no doubt many other systems could be tweaked further and improve on our results – what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art. Such tweaking can bring surprising results, e.g. the conclusion, strongly supported by the results presented here, that HMM tagging is actually quite competi</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>Steven J. DeRose. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14:31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damien Doligez</author>
<author>Jacques Garrigue</author>
<author>Didier R´emy</author>
<author>J´erˆome Vouillon</author>
</authors>
<title>The Objective Caml system. Institut National de Recherche en Informatique et en Automatique.</title>
<date>2004</date>
<marker>Doligez, Garrigue, R´emy, Vouillon, 2004</marker>
<rawString>Damien Doligez, Jacques Garrigue, Didier R´emy, and J´erˆome Vouillon, 2004. The Objective Caml system. Institut National de Recherche en Informatique et en Automatique.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Fast and accurate part-of-speech tagging: The svm approach revisited.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>153--163</pages>
<marker>Gim´enez, M`arquez, 2003</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accurate part-of-speech tagging: The svm approach revisited. In Proceedings of RANLP, pages 153–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Pavel Krbec</author>
<author>Karel Oliva</author>
<author>Pavel Kvˇetoˇn</author>
<author>Vladimir Petkeviˇc</author>
</authors>
<title>Serial combination of rules and statistics: A case study in Czech tagging.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Association of Computational Linguistics Conference,</booktitle>
<pages>260--267</pages>
<location>Toulouse, France.</location>
<marker>Hajiˇc, Krbec, Oliva, Kvˇetoˇn, Petkeviˇc, 2001</marker>
<rawString>Jan Hajiˇc, Pavel Krbec, Karel Oliva, Pavel Kvˇetoˇn, and Vladimir Petkeviˇc. 2001. Serial combination of rules and statistics: A case study in Czech tagging. In Proceedings of the 39th Association of Computational Linguistics Conference, pages 260–267, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>285--291</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2000</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur. 2000. Statistical morphological disambiguation for agglutinative languages. In Proceedings of the 18th conference on Computational linguistics, pages 285– 291, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P´eter Hal´acsy</author>
<author>Andr´as Kornai</author>
<author>Csaba Oravecz</author>
<author>Viktor Tr´on</author>
<author>D´aniel Varga</author>
</authors>
<title>Using a morphological analyzer in high precision POS tagging of Hungarian.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC</booktitle>
<pages>2245--2248</pages>
<marker>Hal´acsy, Kornai, Oravecz, Tr´on, Varga, 2006</marker>
<rawString>P´eter Hal´acsy, Andr´as Kornai, Csaba Oravecz, Viktor Tr´on, and D´aniel Varga. 2006. Using a morphological analyzer in high precision POS tagging of Hungarian. In Proceedings ofLREC 2006, pages 2245–2248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Jurish</author>
</authors>
<title>A hybrid approach to part-of-speech tagging.</title>
<date>2003</date>
<booktitle>Berlin-Brandenburgische Akademie der Wissenschaften.</booktitle>
<tech>Technical report,</tech>
<contexts>
<context position="6231" citStr="Jurish, 2003" startWordPosition="1032" endWordPosition="1033">s than a morphological analyzer would. To facilitate the use of MA, HunPos has hooks to work with a morphological analyzer (lexicon), which might still leave some OOV items. As we shall see in Section 3, the key issue is that for unseen words the HMM search space may be narrowed down to the alternatives proposed by this module, which not only speeds up search but also very significantly improves precision. That is, for unseen words the MA will generate the possible labels, to which the weights are assigned by the suffix guessing algorithm. 2 Inside TnT Here we describe, following the lead of (Jurish, 2003), some non-trivial features of TnT sometimes only hinted at in the user guide, but clearly evident from its behavior on real and experimentally adjusted corpora. For the most part, these features are clever hacks, and it is unfortunate that neither Brants (2000) nor the standard HMM textbooks mention them, especially as they often yield more significant error reduction than the move from HMM to other architectures. Naturally, these features are also available in HunPos. 2.1 Cardinals For the following regular expressions TnT learns the tag distribution of the training corpus separately to give</context>
</contexts>
<marker>Jurish, 2003</marker>
<rawString>Bryan Jurish. 2003. A hybrid approach to part-of-speech tagging. Technical report, Berlin-Brandenburgische Akademie der Wissenschaften.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<editor>In Karel Pala Petr Sojka, Ivan Kopecek, editor,</editor>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1808" citStr="Ratnaparkhi, 1996" startWordPosition="281" endWordPosition="282">, 2000) is used widely in research labs throughout the world: Google Scholar shows over 400 citations. For research purposes TnT is freely available, but only in executable form (closed source). Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer1http://mokk.bme.hu/resources/hunpos/ ing system. Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gim´enez and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al., 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer. A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training. In highly inflecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English. To mitigate the effect one needs a morphological dictionary (Hajiˇc et al., 2</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Karel Pala Petr Sojka, Ivan Kopecek, editor, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="1834" citStr="Smith et al., 2005" startWordPosition="284" endWordPosition="287"> research labs throughout the world: Google Scholar shows over 400 citations. For research purposes TnT is freely available, but only in executable form (closed source). Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer1http://mokk.bme.hu/resources/hunpos/ ing system. Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gim´enez and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al., 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer. A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training. In highly inflecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English. To mitigate the effect one needs a morphological dictionary (Hajiˇc et al., 2001) or a morphological an</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="1745" citStr="Toutanova et al., 2003" startWordPosition="271" endWordPosition="274">troduction Even without a formal survey it is clear that TnT (Brants, 2000) is used widely in research labs throughout the world: Google Scholar shows over 400 citations. For research purposes TnT is freely available, but only in executable form (closed source). Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer1http://mokk.bme.hu/resources/hunpos/ ing system. Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gim´enez and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al., 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer. A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training. In highly inflecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English. To mitigate th</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings ofHLT-NAACL, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Tr´on</author>
<author>P´eter Hal´acsy</author>
<author>P´eter Rebrus</author>
<author>Andr´as Rung</author>
<author>P´eter Vajda</author>
<author>Eszter Simon</author>
</authors>
<title>Morphdb.hu: Hungarian lexical database and morphological grammar.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC</booktitle>
<pages>1670--1673</pages>
<marker>Tr´on, Hal´acsy, Rebrus, Rung, Vajda, Simon, 2006</marker>
<rawString>Viktor Tr´on, P´eter Hal´acsy, P´eter Rebrus, Andr´as Rung, P´eter Vajda, and Eszter Simon. 2006. Morphdb.hu: Hungarian lexical database and morphological grammar. In Proceedings ofLREC 2006, pages 1670–1673.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>