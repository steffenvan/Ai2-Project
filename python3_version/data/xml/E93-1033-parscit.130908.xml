<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990263">
Abductive Explanation of Dialogue Misunderstandings
</title>
<author confidence="0.998847">
Susan McRoy and Graeme Hirst
</author>
<affiliation confidence="0.9989605">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.540625">
Toronto, Canada M5S 1A4
</address>
<sectionHeader confidence="0.990502" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908307692308">
To respond to an utterance, a listener must
interpret what others have said and why
they have said it. Misunderstandings oc-
cur when agents differ in their beliefs about
what has been said or why. Our work com-
bines intentional and social accounts of dis-
course, unifying theories of speech act pro-
duction, interpretation, and the repair of
misunderstandings. A unified theory has
been developed by characterizing the gen-
eration of utterances as default reasoning
and using abduction to characterize inter-
pretation and repair.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939070175439">
When agents participate in a dialogue, they bring
to it different beliefs and goals. These differences
can lead them to make different assumptions about
one another&apos;s actions, construct different interpre-
tations of discourse objects, or produce utterances
that are either too specific or too vague for others
to interpret as intended. As a result, agents may
fail to understand some part of the dialogue or
unknowingly diverge in their understanding of it—
making a breakdown in communication likely. One
strategy an agent might use to address the prob-
lem of breakdowns is to try to circumvent them,
for example, by trying to identify and correct appar-
ent confusions about objects or concepts mentioned
in the discourse [Goodman, 1985; McCoy, 1985;
Calistri-Yeh, 1991; Eller and Carberry, 1992]. The
work reported here takes a different, but complemen-
tary, approach: it models how an agent can use what
she or he knows about the discourse to recognize
whether either participant has misunderstood some
previous utterance to repair the misunderstanding.
This strategy handles cases that the preventive ap-
proaches cannot anticipate. It is also more general,
because our system can generate repairs on the basis
of the relatively few types of manifestations of mis-
understanding, rather than the much broader (and
hence more difficult to anticipate) range of sources.
In this paper, we shall describe an abductive ac-
count of interpreting speech acts and recognizing
misunderstandings (we discuss the generation of re-
pairs of misunderstandings in McRoy and Hirst,
1992). This account is part of a unified theory
of speech act production, interpretation, and re-
pair [McRoy, 1993]. According to the theory, speak-
ers use their beliefs about the discourse context and
which speech acts are expected to follow from a
given speech act in order to select one that accom-
plishes their goals and then to produce an utter-
ance that performs the chosen speech act. Interpre-
tation and repair attempt to retrace this selection
process abductively—when a hearer attempts to in-
terpret an observed utterance, he tries to identify the
goals, expectations, or misunderstandings that might
have led the to produce it. Previous plan-based ap-
proaches [Allen, 1979; Allen, 1983; Litman, 1985;
Carberry, 1985] have had difficulty constraining this
inference—from only a germ of content, potentially a
tremendous number of goals could be inferred. A key
assumption of our approach, which follows from in-
sights provided by Conversation Analysis [Garfinkel,
1967; Schegloff and Sacks, 1973], is that participants
can rely primarily on expectations derived from so-
cial conventions about language use. These expec-
tations enable participants to determine whether
the conversation is proceeding smoothly: if noth-
ing unusual is detected, then understanding is pre-
sumed to occur. Conversely, when a hearer finds
</bodyText>
<page confidence="0.992146">
277
</page>
<bodyText confidence="0.999952688888889">
that a speaker&apos;s utterance is inconsistent with his
expectations, he may change his interpretation of
an earlier turn and generate a repair [Fox, 1987;
Suchman, 1987]. Our approach differs from stan-
dard CA accounts in that it treats Gricean inten-
tions [Grice, 1957] as part of these conventions and
uses them to constrain an agent&apos;s expectations; the
work thus represents a synthesis of intentional and
structural accounts.
Recognizing misunderstanding is like abduction
because hearers must explain why, given their knowl-
edge of how differences in understanding are mani-
fested, a speaker might have said what she did. At-
tributions of misunderstanding are assumptions that
might be abduced in constructing such an explana-
tion. Recognizing misunderstanding also resembles a
diagnosis in which utterances play the role of &amp;quot;symp-
toms&amp;quot; and misunderstandings are &amp;quot;faults&amp;quot;. Previ-
ous work on diagnosis has shown abduction to be
a useful characterization [Ahuja and Reggia, 1986;
Poole, 1986].
An alternative approach to diagnosing discourse
misunderstandings is to reason deductively from a
speaker&apos;s utterances to his or her goals on the basis
of (default) prior beliefs and then rely on belief revi-
sion to retract inconsistent interpretations [Cawsey,
1991]; however, this approach has a number of disad-
vantages. First, any set of rules of this form will be
unable to specify all the conditions (such as insincer-
ity) that might also influence the agent&apos;s interpreta-
tion; a reasoner will need also to assume that there
are no &amp;quot;abnormalities&amp;quot; relevant to the participants
or the speech event [Poole, 1989]. This approach
also ignores the many other possible interpretations
that participants might achieve through negotiation,
independent of their actual beliefs. For example, an
agent&apos;s response to a yes-no question might treat it
as a question, a request, a warning, a test, an insult,
a challenge, or just a vacuous statement intended to
keep the conversation going. If conversational par-
ticipants can negotiate such ambiguities, then utter-
ances are at most a reason for attributing a certain
goal to an agent. That is, they are a symptom, not a
cause. Any deductive account would thus be counter-
intuitive, and very likely false as well.
</bodyText>
<sectionHeader confidence="0.720767" genericHeader="introduction">
2 The abductive framework
</sectionHeader>
<bodyText confidence="0.999867916666667">
We have chosen to develop the proposed account
of dialogue using the Prioritized Theorist frame-
work [Poole et al., 1987; Brewka, 1989; van Arragon,
1990]. Theorist typifies what is known as a &amp;quot;proof-
based approach&amp;quot; to abduction because it relies on a
theorem prover to collect the assumptions that would
be needed to prove a given set of observations and to
verify their consistency. This framework was selected
because of its first-order syntax and its support for
both default and abductive reasoning. Within The-
orist, we represent linguistic knowledge and the dis-
course context, and also model how speakers reason
about their actions and misunderstandings.
We have used Poole&apos;s implementation of Theo-
rist, extended to incorporate preferences among de-
faults as suggested by Van Arragon [1990]. Poole&apos;s
Theorist implements a full first-order clausal theo-
rem prover in Prolog. It extends Prolog with a true
negation symbol and the contrapositive forms of each
clause. Thus, a Theorist clause ct D is interpreted
as 1/3 4- a,-&apos;a -,#). A Prioritized Theorist rea-
soner can also assume any default d that the pro-
grammer has designated as a potential hypothesis,
unless it can prove -id from some fact or overriding
hypothesis.
The reasoning algorithm uses model elimina-
tion [Loveland, 1978; Stickel, 1989; Umrigar and
Pitchumani, 19851 as its proof strategy. Like Pro-
log, it is a resolution-based procedure that chains
backward from goals to subgoals, using rules of the
form goal 4-- subgoali A • • • A subgoal,,, to reduce the
goals to their subgoals. However, unlike Prolog, it
records each subgoal that occurs in the proof tree
leading to the current one and checks this list before
searching the knowledge base for a relevant clause;
this permits it to reason by cases.
</bodyText>
<sectionHeader confidence="0.968336" genericHeader="method">
3 The formal language
</sectionHeader>
<bodyText confidence="0.98805278125">
The model is based on a sorted first-order lan-
guage, L, comprising a denumerable set of predi-
cates, variables, constants, and functions, along with
the boolean connectives V, A, -1, D, and E., and the
predicate =. The terms of ,C come in six sorts:
agents, turns, sequences of turns, actions, descrip-
tions, and suppositions 1. C includes an infinite num-
ber of variables and function symbols of every sort
and arity. We also define a number of special ones:
do, mistake, intend, knowif, knowref, knows-
BetterRef, not, and and. Each of of these func-
tions takes an agent as its first argument and an ac-
tion, supposition, or description for each of its other
arguments; each of them returns a supposition. The
function symbols that return speech acts each take
two agents as their first two argument and an action,
supposition, or description for each of their other ar-
guments.
For the abductive model, we define a correspond-
ing language LTh in the Prioritized Theorist frame-
work. £m includes all the sorts, terms, functions,
and predicates of L; however, Gm lacks explicit
quantification, distinguishes facts from defaults, and
associates with each default a priority value. Vari-
able names are understood to be universally quan-
tified in facts and defaults (but existentially quan-
tified in an explanation). Facts are given by &amp;quot;FACT
w.&amp;quot;, where w is a wff. A default can be given ei-
ther by &amp;quot;DEFAULT (p, d).&amp;quot; or &amp;quot;DEFAULT (p,d) :
&apos;Suppositions represent the propositions that speak-
ers express in a conversation, independent of the truth
values that those propositions might have.
</bodyText>
<page confidence="0.995365">
278
</page>
<bodyText confidence="0.91714375">
where p is a priority value, d is an atomic symbol
with only free variables as arguments, and w is a
wff. For example, we can express the default that
birds normally fly, as:
DEFAULT (2, birdsFly(b)): bird(b) j fly(b).
If ..7&amp;quot; is the set of facts and AP is the set of defaults
with priority p, then an expression DEFAULT(p, d) : w
asserts that d E AP and (d 3 w) E F.
</bodyText>
<sectionHeader confidence="0.903782" genericHeader="method">
4 The architecture of the model
</sectionHeader>
<bodyText confidence="0.999889882352941">
In the architecture that we have formulated, pro-
ducing an utterance is a default, deductive process
of choosing both a speech act that meets an agent&apos;s
communicative and interactional goals and a utter-
ance that will be interpretable as this act in the cur-
rent context. Utterance interpretation is the com-
plementary (abductive) process of attributing to the
speaker communicative and interactional goals by at-
tributing to him or her a discourse-level form that
provides a reasonable explanation for an observed ut-
terance in the current context. Social norms delimit
the range of responses that a participant may pro-
duce without becoming accountable for additional
explanation.2 The attitudes that speakers express
provide additional constraints, because speakers are
expected not to contradict themselves. We therefore
attribute to each agent:
</bodyText>
<listItem confidence="0.9700632">
• A theory T describing his or her linguistic
knowledge, including principles of interaction
and facts relating linguistic acts.
• A set B of prior assumptions about the beliefs
and goals expressed by the speakers (including
assumptions about misunderstanding).
• A set M of potential assumptions about misun-
derstandings and meta-planning3 decisions that
agents can make to select among coherent alter-
natives.
</listItem>
<bodyText confidence="0.835381">
To interpret an utterance u, by speaker s, the hearer
h will attempt to solve:
</bodyText>
<sectionHeader confidence="0.759167" genericHeader="method">
TUBUMF utter(s,h,u,ts)
</sectionHeader>
<bodyText confidence="0.883544666666667">
for some set M c M, where is refers to the current
context.
In addition, acts of interpretation and generation
update the set of beliefs and goals assumed to be
expressed during the discourse. Our current formal-
ization focuses on the problems of identifying how
an utterance relates to a context and whether it has
been understood. The update of expressed beliefs
2These norms include guidelines such as &amp;quot;If someone
asks you a question, you should answer it&amp;quot; or &amp;quot;If someone
offers their opinion and you disagree, you should let them
know&amp;quot;.
&apos;Our notion of &amp;quot;meta-planning&amp;quot; is similar to Lit-
man&apos;s [1985] use of meta-plans, but we prefer to treat
meta-planning as a pattern of inference that is part of
the task specification rather than as an action.
is handled in the implementation, but outside the
formal language.4
</bodyText>
<subsectionHeader confidence="0.999349">
4.1 Speech acts
</subsectionHeader>
<bodyText confidence="0.999922333333333">
For simplicity, we represent utterances as surface-
level speech acts in the manner first used by Perrault
and Allen [1980]. For example, if speaker m asks
speaker r the question &amp;quot;Do you know who&apos;s going
to that meeting?&amp;quot; we would represent this as: s-
request(m, r, informif(r, m, knowref(r, w))).
Following Cohen and Levesque [1985], we limit the
surface language to the acts s-request, s-inform, s-
informref, and s-informif. Discourse-level acts in-
clude inform, informif, informref, askref, askif,
request, pretelP, testref, testif and warn, and
are represented using a similar notation.
</bodyText>
<subsectionHeader confidence="0.989616">
4.2 Expressed attitudes
</subsectionHeader>
<bodyText confidence="0.969013382352941">
We distinguish the beliefs that speakers act as if they
have during a course of a conversation from those
they might actually have. Most models of discourse
incorporate notions of belief and mutual belief to de-
scribe what happens when a speaker talks about a
proposition, without distinguishing the expressing of
belief from believing (see Cohen et al. 1990). How-
ever, real belief involves notions of evidence, trust-
worthiness, and expertise, not accounted for in these
models; it is not automatic. Moreover, the beliefs
that speakers as if they have need not match their
real ones. For example, a speaker might simplify
or ignore certain facts that could interfere with the
accomplishment of a primary goal [Gutwin and Mc-
Calla, 1992]. Speakers need to keep track of what
others say, in addition to whether they believe them,
because even insincere attitudes can affect the inter-
pretation and production of utterances. Although
speakers normally choose to be consistent in the at-
titudes they express, they can recant if it appears
that doing so will lead (or has led) to conversational
breakdown.
Following Thomason [1990], we call the contents of
the attitudes that speakers express during a dialogue
suppositions and the attitude itself simply active.6
Thus, when a speaker performs a particular speech
act, she activates the linguistic intentions associated
with the act, along with a belief that the act has
been done. These attitudes do not depend on the
4A related concern is how an agent&apos;s beliefs might
change after an utterance has been understood as an act
of a particular type. Although we have nothing new to
add here, Perrault [1990] shows how Default Logic might
be used to address this problem.
</bodyText>
<footnote confidence="0.436904166666667">
5A pmtellingis a preannouncement that says, in effect,
&amp;quot;I&apos;m going to tell you something that will surprise you.
You might think you know, but you don&apos;t.&amp;quot;
6Supposition differs from belief in that speakers need
not distinguish their own suppositions from those of an-
other [Stalnaker, 1972; Thomason, 1990].
</footnote>
<page confidence="0.994392">
279
</page>
<bodyText confidence="0.982771333333333">
speakers&apos; real beliefs.7
The following expressions are used to denote sup-
positions:
</bodyText>
<listItem confidence="0.98874865">
• do(s, a) expresses that agent s has performed
the action a;
• mistake(s, al, a2) expresses that agent s has
mistaken an act al for act az;
• intend(s, p) expresses that agent s intends to
achieve a situation described by supposition p;
• knowif(s, p) expresses that the agent s knows
whether the proposition named by supposition
p is true;
• knowref(s, d) expresses that the agent s knows
the referent of description d;
• knowsBetterRef(si , s2 , d) expresses that
agent s1 has &amp;quot;expert&amp;quot; knowledge about the ref-
erent of description d, so that if s2 has a different
belief about the referent, then s2 is likely to be
wrong;8 and
• and(pi, p2) expresses the conjunction of suppo-
sitions p1 and p2;
• not(p) expresses the negation of supposition p.9
4.3 Linguistic knowledge relations
</listItem>
<bodyText confidence="0.999901">
We represent agents&apos; linguistic knowledge with three
relations: decomp, a binary relation on utterance
forms and speech acts; &apos;intention, a binary rela-
tion on speech acts and suppositions; &apos;expectation, a
three-place relation on speech acts, suppositions, and
speech acts. The decomp relation specifies the speech
acts that each utterance form might accomplish. The
&apos;intention relation specifies the beliefs and intentions
that each speech act conventionally expresses. The
&apos;expectation relation specifies, for each speech act,
which speech acts an agent believing the given con-
dition can expect to follow.
</bodyText>
<subsectionHeader confidence="0.999632">
4.4 Beliefs and goals
</subsectionHeader>
<bodyText confidence="0.91102325">
We assume that an agent&apos;s beliefs and goals are given
explicitly by statements of the form believe(S, 1) and
hasGoal(S, P, TS), respectively, where S is an agent,
P is a supposition and TS is a turn sequence.
</bodyText>
<subsectionHeader confidence="0.996529">
4.5 Activation
</subsectionHeader>
<bodyText confidence="0.705810166666667">
To represent the dialogue as a whole, including re-
pairs, we introduce the notion of a turn sequence and
7It is essential that these suppositions name proposi-
tions independent of their truth values, so that we may
represent agents talking about knowing and intending
without fully analyzing these concepts.
</bodyText>
<footnote confidence="0.968249166666667">
8This specialization is needed to capture the prag-
matic force of pretelling.
8The function not is distinct from boolean connective
-1. It is used to capture the supposition expressed by an
agent who says something negative, e.g., &amp;quot;I do not want
to go.&amp;quot;
</footnote>
<listItem confidence="0.828020214285714">
the activation of a supposition with respect to a se-
quence. A turn sequence represents the interpreta-
tions of the discourse that a speaker has considered.
Turn sequences are characterized by the following
three relations:
• turn0f(ts,t) holds if and only if t is a turn in
the sequence is;
• succ(ti, ti, ts) holds if and only if turn0f(ts, ti),
turn0f(ts,tj), ti follows ti in ts, and there is no
4 such that turn0f(ts,tk), succ(tk,ti,ts), and
succ(ti,tk,ts);
• focus(ts,t) holds if t is a distinguished turn upon
which the sequence is focused; normally this is
the last turn of is.
</listItem>
<bodyText confidence="0.9443366">
We also define a successor relation on turn sequences.
A turn sequence TS2 is a successor to turn sequence
TS1 if T52 is identical to TS1 except that TS2 has
an additional turn t that is not a turn of TS1 and
that is the successor to the focused turn of TS1.
The set of prior assumptions about the beliefs and
goals expressed by the participants in a dialogue is
represented as the activation of suppositions. For ex-
ample, an agent nan performing an informref(nan,
bob, theTime) expresses the supposition do(nan,
informref(nan, bob, theTime)) and the Gricean
intention,
and(knowref(nan, theTime),
intend(nan, knowref(bob, theTime)))
given by the &apos;intention relation. We assume
that an agent will maintain a record of both par-
ticipants&apos; suppositions, indexed by the turns in
which they were expressed. It is represented as
a set of statements of the form expressed(P,T) or
expressedNot(P,T) where P is a simple supposition
and T is a turn.
Beliefs and intentions that participants express
during a turn of a sequence isi become and remain
active in all sequences that are successors to tsi, un-
less they are explicitly refuted.
</bodyText>
<listItem confidence="0.807486692307692">
DEFINITION 1: If, according to the interpretation of
the conversation represented by turn sequence
TS with focused turn T, the supposition P was
expressed during turn T, we say that P becomes
active with respect to that interpretation and
the predicate active(P,TS) is derivable:
FACT expressed(p,t) A focus(ts, t)
• active(p, ts).
FACT expressedNot(p,t) A focus (is, t)
• active(not(p),ts).
FACT -,(active(p,ts) A active(not(p),ts)).
If formula P is active within a sequence TS, it
will remain active until not(P) is expressed:
</listItem>
<page confidence="0.956104">
280
</page>
<figure confidence="0.793241214285714">
FACT expressed(p,t) A focus(ts,t)
• —,aetivationPersists(not(p),t).
FACT expressedNot(p,t) A focus(ts,t)
• --tactivationPersists(p,t).
DEFAULT (1, activationF&apos;ersists(p,t)) :
active(p,tsi)
A successorTS(isn. ,tsi)
A focus(isnow,t)
active(p, isnew ).
4.6 Expectation
The following definition captures the notion of &amp;quot;ex-
pectation&amp;quot;.
DEFINITION 2: A discourse-level action R is ex-
pected by speaker S in turn sequence TS when:
</figure>
<listItem confidence="0.997555285714286">
• An action of type A has occurred;
• There is a planning rule corresponding to
an adjacency pair A—R with condition C;
• S believes that C;
• The linguistic intentions expressed by R are
consistent with TS; and
• R has not occurred yet in TS.
</listItem>
<figure confidence="0.8472298">
DEFAULT (2, ex pected Reply(p , p, do(s , a2),ts)):
active(pd,,,ts)
A lexpectation(pdo,p,do(si, 02))
A believe(si IP)
A lintentionsOk(si, a2,t8)
</figure>
<listItem confidence="0.567632333333333">
• expected(s1,02,ts).
FACT active(pdo,ts)
• --,expectedReplY(Pdo,P,Preply,t8).
</listItem>
<bodyText confidence="0.931837214285714">
The predicate expect edReply is a default. Although
activation might depend on default persistence, acti-
vation always takes precedence over expectation be-
cause it has a higher priority (on the assumption that
memory for suppositions is stronger than expecta-
tion).
The predicate lintentionsOk(S, A, TS) is true if
speaker S expresses the linguistic intentions of the
act A in turn sequence TS, and these intentions are
consistent with TS.
We also introduce a subjunctive form of expecta-
tion, which depends only on a speaker&apos;s real beliefs:
FACT lexpectation(do(si, al), p, do(s2, a2))
A believe(si,P)
</bodyText>
<listItem confidence="0.887292">
• wouldEx(si,ai, a2).
</listItem>
<subsectionHeader confidence="0.992858">
4.7 Recognizing misunderstandings
</subsectionHeader>
<bodyText confidence="0.999575764705882">
When a dialogue proceeds normally, a speaker&apos;s ut-
terance can be explained by abducing that a dis-
course action has been planned using one of a known
range of discourse strategies: plan adoption, accep-
tance, challenge, repair, or closing. (Figure 1 in-
cludes some examples in Theorist.) In cases of appar-
ent misunderstanding, the same explanation process
suggests a misunderstanding, rather than a planned
act, as the reason for the utterance. To handle these
cases, the model needs a theory of the symptoms of
a failure to understand [Poole, 1989]. For example,
a speaker S2 might explain an otherwise unexpected
response by a speaker Si by hypothesizing that 52
has mistaken some speech act by Si for another with
a similar decomposition or 52 might hypothesize that
Si has misunderstood (see Figure 2). We shall now
consider some applications.
</bodyText>
<sectionHeader confidence="0.965756" genericHeader="method">
5 Some applications
</sectionHeader>
<table confidence="0.567493583333333">
This first example (from [Schegloff, 199* illustrates
both normal interpretation and the recognition of an
agent&apos;s own misunderstanding:
Ti Mother: Do you know who&apos;s going to that
meeting?
T2 Russ: Who?
T3 Mother: I don&apos;t know.
T4 Russ: Oh. Probably Mrs. McOwen and
probably Mrs. Cadry and some of
the teachers.
The surface-level representation of this conversation
is given as the following:
</table>
<tableCaption confidence="0.916840857142857">
Ti m: s-request(m, r,
informif(r, m,knowref(r, w)))
T2 r: s-request(r, m, informref(m, r, w))
T3 in: s-inform(m, r, not(knowref(m, w)))
T4 r: s-informref(r, in, w)
5.1 Russ&apos;s interpretation of Ti in the
meeting example
</tableCaption>
<bodyText confidence="0.99841505">
i,From Russ&apos;s perspective, Ti can be explained as a
pretelling, an attempt by Mother to get him to ask
her who is going. Russ&apos;s rules about the relationship
between surface forms and speech acts (decomp) in-
clude that:
FACT decomp(s-request (si , 82,
informif(s2 , si, knowref(s2 , p))),
pretell(si, 82
FACT decomp(s-request(si, 82,
informif(s2, Si, knowref(s2,p))),
askref(si , 82, .73)).
FACT decomp(s-request (si , S2,
informif(s2, Si, knowref(s2 P))),
askif(si , 82, knowref(82 P)))-
Russ has linguistic expectation rules for the ad-
jacency pairs pretell—askref, askref—informref, and
askif—informif (as well as for pairs of other types).
Russ also has believes that he knows who&apos;s going to
the meeting, that he knows he knows this, and that
Mother&apos;s knowledge about the meeting is likely to be
</bodyText>
<page confidence="0.99049">
281
</page>
<figure confidence="0.895892">
Utterance Explanation
FACT decomp(u, ai )
A try(si , sz, ts)
D utter(si, sz,u,ts).
Plan Adoption
</figure>
<equation confidence="0.4860426">
DEFAULT (3, adopt(si, sz, 01,02, is)):
hasGoal(si, do(sz, az), is)
A wouldEx(si,do(si, ai),do(s2, az))
A lintentionsOk(si, a, is)
shouldTry(si, s2,ai, is).
</equation>
<bodyText confidence="0.5264615">
&amp;quot;If agent Si intends that agent S2 perform the action A2
and A2 is the expected reply to the action Al, and it
would be coherent for Si to perform Al, then Si should
do so.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.758997">
Planned Actions
</subsectionHeader>
<construct confidence="0.869598285714286">
DEFAULT (2, intendact(si, sz, al , is)) :
shouldTry(sz , sz, ai , is)
D try(si,s2,ai,ts).
Acceptance
DEFAULT (2, accept(si, a, is)):
expected(si , a, is)
shouldTry(si, sz , a, is).
</construct>
<figureCaption confidence="0.624953666666667">
&amp;quot;If agent Si believes that act A is the expected next
action, then Si should perform A.&amp;quot;
Figure 1: Theorist rules for producing and interpreting utterances
</figureCaption>
<figure confidence="0.888771777777778">
Failure to understand
DEFAULT (3, sel f Mis(si, sz,p, a2,ts)):
active(do(si aM)) is)
A ambiguous(am , al)
A lintention(a2,pii)
A lintention(am,pti2)
A inconsistentLI(pz1,p1i2)
A p = mistake(s2, 01, am))
try(si , s2, a2 , is).
</figure>
<figureCaption confidence="0.7566822">
&amp;quot;Speaker S might be attempting action A in discourse TS
if: S was thought to have performed action Am; but, the
linguistic intentions of Am are inconsistent with those of
A; acts AI and Am have a similar surface form (and hence
could be mistaken); and, H may have made this mistake.&amp;quot;
</figureCaption>
<figure confidence="0.926507625">
Failure to be understood
DEFAULT (3, other M is(si , sz, p, az, ts)) :
active(do(s2, a/), is)
A ambiguous(a , am)
A wouldEx(sa, do(82, am), do(si , a2))
A p = mistake(si, al, am))
try(si,s2,a2,ts).
&amp;quot;Speaker S might be attempting action A in discourse
</figure>
<figureCaption confidence="0.829594555555556">
TS if: speaker H was thought to have performed ac-
tion Ai; but, acts A1 and Am have a similar surface
form; if H had performed Am, A would be expected;
S may express the linguistic intentions of A; and, S
may have made the mistake.&amp;quot;
Figure 2: Rules for diagnosing misunderstanding
better than his own. We assume that he can make
default assumptions about what Mother believes and
wants:
</figureCaption>
<construct confidence="0.6351734">
FACT believe(r, knowref(r, w)).
FACT believe (r, knowif(r,knowref(r,w))).
FACT belleve(r, knowsBetterRef(m,r,w)).
DEFAULT (1, credulousB(p)) : believe(m,p).
DEFAULT (1, credulousH (p,ts)) : hasGoal(m,p,ts).
</construct>
<bodyText confidence="0.843091333333333">
Russ&apos;s interpretation of Ti as a pretelling is pos-
sible using the meta-plan for plan adoption and the
rule for planned action.
</bodyText>
<listItem confidence="0.8356675">
1. The proposition
hasGoal(m, do(r, askref(r, m, w)), ts(0))
may be explained by abducing
credulousH(do(r,askref(r, m, w)),ts(0)).
2. An askref by Russ would be the expected reply
to a pretell by Mother:
</listItem>
<bodyText confidence="0.812650666666667">
wouldEx(m,do(m,pretell(m, r, w)),
do(r,askref(r, m, w)))
It would be expected by Mother because:
</bodyText>
<listItem confidence="0.89814375">
• The &apos;expectation relation suggests that she
might try to pretell in order to get him to
produce an askref:
/expectation(do(m,pretell(m,r,w)),
</listItem>
<equation confidence="0.5218515">
knowsBetterRef(m,r,w),
do(r,askref(r,m,w)))
</equation>
<listItem confidence="0.449857">
• Russ may abduce
credulousB(knowsBetterRef(m, r, w))
to explain
believe (m,knowsBet terRef(m, r, w)).
3. The discourse context is empty at this point,
so the linguistic intentions of pretelling satisfy
lintentionsOk.
</listItem>
<page confidence="0.991979">
282
</page>
<note confidence="0.342407">
4. Lastly, Russ may assumel°
adopt(m, r, pretell(m, r, w),
askref(r, m, w), ts(0))
</note>
<bodyText confidence="0.982246">
Thus, the conditions of the plan-adoption
meta-rule are satisfied, and Russ can explain
shouldTry(m, r, pretell(m, r, w), ts(0)). This
enables him to explain
try(m, r, pretell(m, r, w), ts(0))
as a planned action. Once Russ explains the
pretelling, his decomp relation and utterance expla-
nation rule allow him to explain the utterance.
</bodyText>
<subsectionHeader confidence="0.934919">
5.2 Russ&apos;s detection of his own
misunderstanding in the meeting
example
</subsectionHeader>
<bodyText confidence="0.8496318">
,From Russ&apos;s perspective, the inform-not-knowref
that Mother performs in T3 signals a misunderstand-
ing. Assuming Ti is a pretelling, just prior to T3,
Russ&apos;s model of the discourse corresponds to the fol-
lowing:
</bodyText>
<construct confidence="0.990706272727273">
expressed(do(m, pretell(m, r, w)), 1)
expressed(knowref(m, w), 1)
expressed(knowsBetterRef(m, r, w), 1)
expressed(int end (m, .
do(m, informref(m, r, w))), 1)
expressed(intend(m, knowref(r, w)), 1)
expressed(do(r, askref(r, m, w)), 2)
expressedNot(knowref(r, w), 2)
expressed(intend(r, knowref(r, w)), 2)
expressed(int end (r,
do(m, informref(m, r, w))), 2)
</construct>
<bodyText confidence="0.989365">
T3 does not demonstrate acceptance because in-
form(m, r, not(knowref(m, w))) is not coherent
with this interpretation of the discourse. This act is
incoherent because not(knowref(m, w)) is among
the linguistic intentions of this inform, while accord-
ing to the model active(knowref(m, w),ts(2)).
Thus, it is not the case that:
</bodyText>
<equation confidence="0.376522333333333">
lintentionsOk(m,
inform(m, r, not(knowref(m, w))),
ts(2))
</equation>
<bodyText confidence="0.999660833333333">
As a result, Russ cannot attribute to Mother any
expected act, and must attribute a misunderstanding
to himself or to her.
Russ may attribute T3 to a self-misunderstanding
using the rule for detecting failure to understand.
We sketch the proof below.
</bodyText>
<listItem confidence="0.711149">
1. According to the context,
</listItem>
<bodyText confidence="0.807557">
expressed(do(m,pretell(m,r,w)),0).
And, Russ may assume that the activation of
&amp;quot;The only constraint on adopting a plan, is that the
result not yet be achieved:
</bodyText>
<equation confidence="0.522562">
FACT active(do(s, a2),ts)
D -,adopt(si, 82, ai,a2,ts).
this supposition persists:
</equation>
<figure confidence="0.318522142857143">
activationPersists(do(m,pretell(m,r,w)),0)
activationPersist8(do(m,pretell(m,r,w)),1)
Thus,
active(do(m, pretell(m, r, w)), ts(2)).
2. The acts pretell and asIcref have a surface form
that is similar,
s-request(m,r,informif(r,m,knowref(r,w)))
So,
a mbiguous(pretell(m,r,w), askref(m,r,w)).
3. The linguistic intentions of the pretelling are:
and(knowref(m, w),
and(knowsBetterRef(m, r, w),
and(
intend(m,
</figure>
<construct confidence="0.7752785">
do(m, informref(m, r,
intend(m, knowref(r, w)))))
</construct>
<bodyText confidence="0.687462833333333">
The linguistic intentions of inform-not-knowref
are
and(not(knowref(m, w)),
intend(m,
knowif(r,not(knowref(m, w))))).
But these intentions are inconsistent.
</bodyText>
<listItem confidence="0.658652">
4. Russ may assume
</listItem>
<bodyText confidence="0.65718225">
selfMis(m,r,
mistake(r,askref(m, r, w),
pretell(m, r, w)),
inform(m, r, not(knowref(m, w))),
ts(2)).
Once Russ explains the inform-not-knowref, his
decomp relation and utterance explanation rule al-
low him to explain the utterance.
</bodyText>
<subsectionHeader confidence="0.929838666666667">
5.3 A case of other-misunderstanding:
Speaker A finds that speaker B has
misunderstood
</subsectionHeader>
<bodyText confidence="0.95372925">
We now consider a new example (from McLaugh-
lin [1984]), in which a participant A recognizes that
a another participant, B, has mistaken a request in
Ti for a test:
</bodyText>
<note confidence="0.95733725">
Ti A: When is the dinner for Alfred?
T2 B: Is it at seven-thirty?
T3 A: No, I&apos;m asking you.
T4 B: Oh. I don&apos;t know.
</note>
<bodyText confidence="0.7847265">
The surface-level representation of this conversation
is given as the following:
</bodyText>
<note confidence="0.4865538">
Ti a: s-request(a, b, informref(b, a, d))
T2 b: s-request(b, a, informif(a, b, p))
T3 a: s-inform(a, b,
intend(a, do(a, askref(a, b, d))))
T4 b: s-inform(b, a, not(knowref(b, d)))
</note>
<page confidence="0.998766">
283
</page>
<bodyText confidence="0.9994645">
A has linguistic expectation rules for the adjacency
pairs pretell-askref, askref-informref, askif-informif,
and testref-askif. A also believes that she does not
know the time of the dinner, that B does know the
time of the dinner.11 We assume that A can make de-
fault assumptions about what B believes and wants:
</bodyText>
<equation confidence="0.557238">
FACT believe (a, not(knowref(a,d))).
FACT believe(a, knowref(b,d)).
FACT hasGoal(a,do(b,informref(b,a,d)),ts(0)).
DEFAULT (1, credulous B(p)) : believe(b, p).
DEFAULT (1, credulousH(p,ts)): hasGoal(b,p,ts).
</equation>
<bodyText confidence="0.99868415">
,From A&apos;s perspective, after generating Ti, her
model of the discourse is the following:
expressed(do(a, askref(a, b, d)), 1)
expressedNot(knowref(a, d), 1)
expressed(intend(a, knowref(a, d)), 1)
expressed(intend(a,
do(b, informref(b, a, d))), 1)
According to the decomp relation, T2 might be in-
terpretable as askif(b, a, p). However, T2 does not
demonstrate acceptance, because there is no askref-
askif adjacency-pair from which to derive an expec-
tation. T2 is not a plan adoption because A does not
believe that B believes that A knows whether the din-
ner is at seven-thirty. However, there is evidence for
misunderstanding, because both information-seeking
questions and tests can be formulated as surface re-
quests. Also, T2 is interpretable as a guess and re-
quest for confirmation (represented as askif), which
would be expected after a test. We sketch the proof
below.
</bodyText>
<listItem confidence="0.600839">
1. According to the context:
</listItem>
<bodyText confidence="0.8973185">
expressed(do(a, askref(a, b, d)), 0).
A may assume that the activation of this sup-
position persists:
activationPersists(do(a, askref(a, b, d)), 0).
Thus, active(do(a,askref(a,b,d)),ts(1)).
2. The acts askref and testref have a surface form
that is similar, namely
s-request (a,b,informref(b,a,knowref(b,d))).
So,
ambiguous(askref(a,b,d), testref(a,b,d)).
</bodyText>
<sectionHeader confidence="0.348275" genericHeader="method">
3. An askif by B would be the expected reply to a
testref by A:
</sectionHeader>
<bodyText confidence="0.919101214285714">
wouldEx(b,do(a,testref(a, b, d)),
do(b,askif(b, a, p)))
From A&apos;s perspective, it would be expected by
B because:
• The lexpectation relation suggests that A
might try to produce a testref in order to
get him to produce an askif:
&amp;quot;A must believe that B knows when the dinner is for
her to have adopted a plan in Ti to produce an aslcref
get B to perform the desired informref.
lexpectation(do(a,testref(a,b,d)),
and (knowref(b,d) ,
and(knowif(b,p),
and(pred(p,X),
pred(d,X))),
do(b,askif(b,a,p)))
The condition of this rule requires that B
believe he knows the referent of descrip-
tion d and that p asserts that the de-
scribed property holds of the referent that
he knows. For example, if we represent &amp;quot;B
knows when the dinner is&amp;quot; as the descrip-
tion
knowref(b, the(X, time(dinner, X))),
then the condition requires that
knowif(b, time(dinner, q)) for some q.
This is a gross simplification, but the best
that the notation allows.
</bodyText>
<listItem confidence="0.827992">
• A may assume that B believes the condition
of this lexpectation by default.
</listItem>
<sectionHeader confidence="0.999335" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999966">
The primary contribution of this work is that it
treats misunderstanding and repair as intrinsic to
conversants&apos; core language abilities, accounting for
them with the same processing mechanisms that un-
derlie normal speech. In particular, it formulates
both interpretation and the detection of misunder-
standings as explanation problems and models them
as abduction.
We have implemented our model in Prolog and
the Theorist framework for abduction with Priori-
tized defaults. Program executions on a Sun-4 for
four-turn dialogues take 2 cpu seconds per turn on
average.
Directions for future work include extending the
model to handle more than one communicative act
per turn, misunderstood reference [Beeman and
Hirst, 1992], and integrating the account with sen-
tence processing and domain planning.
</bodyText>
<sectionHeader confidence="0.996492" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999837076923077">
This work was supported by the University of
Toronto and the Natural Sciences and Engineering
Research Council of Canada. We thank Ray Reiter
for his suggestions regarding abduction; James Allen
for his advice; Paul van Arragon and Randy Goebel
for their help on using Theorist; Hector Levesque,
Mike Gruninger, Sheila McIlraith, Javier Pinto, and
Steven Shapiro for their comments on many of the
formal aspects of this work; Phil Edmonds, Stephen
Green, Diane Horton, Linda Peto, and the other
members of the natural language group for their com-
ments; and Suzanne Stevenson for her comments on
earlier drafts of this paper.
</bodyText>
<page confidence="0.996581">
284
</page>
<sectionHeader confidence="0.995509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991411100917431">
[Ahuja and Reggia, 1986] Sanjiev B. Ahuja and
James A. Reggia. The parsimonious covering
model for inexact abductive reasoning in diagnos-
tic systems. In Recent Developments in the The-
ory and Applications of Fuzzy Sets. Proceedings
of NAFIPS &apos;86 - 1986 Conference of the North
American Fuzzy Information Processing Society,
pages 1-20,1986.
[Allen, 1979] James F. Allen. A Plan-Based Ap-
proach to Speech Act Recognition. PhD thesis,
Department of Computer Science, University of
Toronto, Toronto, Canada, 1979. Published as
University of Toronto, Department of Computer
Science Technical Report No. 131.
[Allen, 1983] James F. Allen. Recognizing inten-
tions from natural language utterances. In Michael
Brady, Robert C. Berwick, and James F. Allen, ed-
itors, Computational Models of Discourse, pages
107-166. The MIT Press, 1983.
[Brewka, 1989] Gerhard Brewka. Preferred subthe-
ories: An extended logical framework for default
reasoning. In Proceedings of the 11th International
Joint Conference on Artificial Intelligence, pages
1043-1048, Detroit, MI, 1989.
[Calistri-Yeh, 1991] Randall J. Calistri-Yeh. Utiliz-
ing user models to handle ambiguity and miscon-
ceptions in robust plan recognition. User Mod-
elling and User Adapted Interaction, 1(4):289-322,
1991.
[Carberry, 1985] Sandra Carberry. Pragmatics Mod-
eling in Information Systems Interfaces. PhD the-
sis, University of Delaware, Newark, Delaware,
1985.
[Cawsey, 1991] Alison J. Cawsey. A belief revision
model of repair sequences in dialogue. In Ernesto
Costa, editor, New Directions in Intelligent Tutor-
ing Systems. Springer Verlag, 1991.
[Cohen and Levesque, 1985] Philip R. Cohen and
Hector J. Levesque. Speech acts and rationality. In
23th Annual Meeting of the Association for Com-
putational Linguistics, Proceedings of the Confer-
ence, pages 49-60,1985.
[Cohen et al., 1990] Philip R. Cohen, Jerry Morgan,
and Martha Pollack, editors. Intentions in Com-
munication. The MIT Press, 1990.
[Eller and Carberry, 1992] Rhonda Eller and Sandra
Carberry. A meta-rule approach to flexible plan
recognition in dialogue. User Modelling and User
Adapted Interaction, 2(1-2):27-53,1992.
[Fox, 1987] Barbara Fox. Interactional reconstruc-
tion in real-time language processing. Cognitive
Science, 11:365-387,1987.
[Garfinkel, 1967] Harold Garfinkel. Studies in Eth-
nomethodology. Prentice Hall, Englewood Cliffs,
NJ, 1967. (Reprinted: Cambridge, England:
Polity Press, in association with Basil Blackwell,
1984.).
[Goodman, 19851 Bradley Goodman. Repairing ref-
erence identification failures by relaxation. In The
23rd Annual Meeting of the Association for Com-
putational Linguistics: Proceedings of the Confer-
ence, pages 204-217, Chicago, 1985.
[Grice, 1957] H. P. Grice. Meaning. The Philosoph-
ical Review, 66:377-388,1957.
[Gutwin and McCalla, 1992] Carl Gutwin and Gor-
don McCalla. Would I lie to you? Modelling con-
text and pedagogic misrepresentation in tutorial
dialogue. In 30th Annual Meeting of the Associa-
tion for Computational Linguistics, Proceedings of
the Conference, pages 152-158, Newark, DE, 1992.
[Heeman and Hirst, 1992] Peter Heeman
and Graeme Hirst. Collaborating on referring ex-
pressions. Technical Report 435, Department of
Computer Science, University of Rochester, 1992.
[Litman, 1985] Diane J. Litman. Plan Recogni-
tion and Discourse Analysis: An Integrated Ap-
proach for Understanding Dialogues. PhD the-
sis, Department of Computer Science, University
of Rochester, Rochester, NY, 1985. Published as
University of Rochester Computer Science Techni-
cal Report 170.
[Loveland, 1978] D. W. Loveland. Automated The-
orem Proving: A Logical Basis. North-Holland,
Amsterdam, The Netherlands, 1978.
[McCoy, 1985] Kathleen F. McCoy. The role of per-
spective in responding to property misconceptions.
In Proceedings of the Ninth International Joint
Conference on Artificial Intelligence, volume 2,
pages 791-793,1985.
[McLaughlin, 1984] Margaret L. McLaughlin. Con-
versation: How Talk is Organized. Sage Publica-
tions, Beverly Hills, 1984.
[McRoy and Hirst, 1992] Susan W. McRoy and
Graeme Hirst. The repair of speech act misunder-
standings by abductive inference. 1992. Submitted
for publication.
[McRoy, 1993] Susan W. McRoy. Abductive Inter-
pretation and Reinterpretation of Natural Lan-
guage Utterances. PhD thesis, Department
of Computer Science, University of Toronto,
Toronto, Canada, 1993. In preparation.
[Perrault and Allen, 1980] C. Raymond Per-
rault and James F. Allen. A plan-based analysis
of indirect speech acts. Computational Linguistics,
6:167-183,1980.
[Perrault, 1990] C. Raymond Perrault. An appli-
cation of default logic to speech act theory. In
Philip It. Cohen, Jerry Morgan, and Martha Pol-
lack, editors, Intentions in Communication, pages
</reference>
<page confidence="0.975159">
285
</page>
<reference confidence="0.995785830188679">
161-186. The MIT Press, 1990. An earlier version
of this paper was published as Technical Report
CSLI-87-90 by the Center for the Study of Lan-
guage and Information.
[Poole et at., 1987] David Poole, Randy Goebel, and
Romas Aleliunas. Theorist: A logical reasoning
system for defaults and diagnosis. In Nick Cer-
cone and Gordon McCalla, editors, The Knowl-
edge Frontier: Essays in the Representation of
Knowledge, pages 331-352. Springer-Verlag, New
York, 1987. Also published as Research Report
CS-86-06, Faculty of Mathematics, University of
Waterloo, February, 1986.
[Poole, 1986] David Poole. Default reasoning and
diagnosis as theory formation. Technical Report
CS-86-08, Department of Computer Science, Uni-
versity of Waterloo, Waterloo, Ontario, 1986.
[Poole, 1989] David Poole. Normality and faults in
logic-based diagnosis. In Proceedings of the 11th
International Joint Conference on Artificial Intel-
ligence, pages 1304-1310,1989.
[Schegloff and Sacks, 1973] Emanuel A. Schegloff
and Harvey Sacks. Opening up closings. Semi-
otica, 7:289-327,1973.
[Schegloff, 1992] Emanuel A. Schegloff. Repair af-
ter next turn: The last structurally provided de-
fense of intersubjectivity in conversation. Ameri-
can Journal of Sociology, 97(5):1295-1345,1992.
[Stalnaker, 1972] Robert C. Stalnaker. Pragmatics.
In Semantics of Natural Language, pages 380-397.
D. Reidel Publishing Company, Dordrecht, 1972.
[Stickel, 1989] M. E. Stickel. A Prolog technology
theorem prover. Journal of Automated Reasoning,
4:353-360,1989.
[Suchman, 19871 Lucy A. Suchman. Plans and Sit-
uated Actions. Cambridge University Press, Cam-
bridge, UK, 1987.
[Thomason, 1990] Richmond H. Thomason. Propa-
gating epistemic coordination through mutual de-
faults I. In Rohit Parikh, editor, Proceedings,
Third Conference on Theoretical Aspects of Rea-
soning about Knowledge (TARK 1990), pages 29-
39, Pacific Grove, CA, 1990.
[Umrigar and Pitchumani, 1985] Zerksis D. Umri-
gar and Vijay Pitchumani. An experiment in pro-
gramming with full first-order logic. In Symposium
of Logic Programming, Boston, MA, 1985. IEEE
Computer Society Press.
[van Arragon, 1990] Paul van Arragon. Nested De-
fault Reasoning for User Modeling. PhD thesis,
Department of Computer Science, University of
Waterloo, Waterloo, Ontario, 1990. Published by
the department as Research Report CS-90-25.
</reference>
<page confidence="0.998429">
286
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814201">
<title confidence="0.999842">Abductive Explanation of Dialogue Misunderstandings</title>
<author confidence="0.999734">Susan McRoy</author>
<author confidence="0.999734">Graeme Hirst</author>
<affiliation confidence="0.999983">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.999483">Toronto, Canada M5S 1A4</address>
<abstract confidence="0.986736928571428">To respond to an utterance, a listener must interpret what others have said and why they have said it. Misunderstandings occur when agents differ in their beliefs about what has been said or why. Our work combines intentional and social accounts of discourse, unifying theories of speech act production, interpretation, and the repair of misunderstandings. A unified theory has been developed by characterizing the generation of utterances as default reasoning and using abduction to characterize interpretation and repair.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Sanjiev B Ahuja</author>
<author>James A Reggia</author>
</authors>
<title>The parsimonious covering model for inexact abductive reasoning in diagnostic systems.</title>
<booktitle>In Recent Developments in the Theory and Applications of Fuzzy Sets. Proceedings of NAFIPS &apos;86 - 1986 Conference of the North American Fuzzy Information Processing Society,</booktitle>
<pages>1--20</pages>
<marker>[Ahuja and Reggia, 1986]</marker>
<rawString>Sanjiev B. Ahuja and James A. Reggia. The parsimonious covering model for inexact abductive reasoning in diagnostic systems. In Recent Developments in the Theory and Applications of Fuzzy Sets. Proceedings of NAFIPS &apos;86 - 1986 Conference of the North American Fuzzy Information Processing Society, pages 1-20,1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>A Plan-Based Approach to Speech Act Recognition.</title>
<date>1979</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, University of Toronto,</institution>
<location>Toronto, Canada,</location>
<note>Published as</note>
<marker>[Allen, 1979]</marker>
<rawString>James F. Allen. A Plan-Based Approach to Speech Act Recognition. PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada, 1979. Published as University of Toronto, Department of Computer Science Technical Report No. 131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Recognizing intentions from natural language utterances.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>107--166</pages>
<editor>In Michael Brady, Robert C. Berwick, and James F. Allen, editors,</editor>
<publisher>The MIT Press,</publisher>
<marker>[Allen, 1983]</marker>
<rawString>James F. Allen. Recognizing intentions from natural language utterances. In Michael Brady, Robert C. Berwick, and James F. Allen, editors, Computational Models of Discourse, pages 107-166. The MIT Press, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Brewka</author>
</authors>
<title>Preferred subtheories: An extended logical framework for default reasoning.</title>
<date>1989</date>
<booktitle>In Proceedings of the 11th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1043--1048</pages>
<location>Detroit, MI,</location>
<marker>[Brewka, 1989]</marker>
<rawString>Gerhard Brewka. Preferred subtheories: An extended logical framework for default reasoning. In Proceedings of the 11th International Joint Conference on Artificial Intelligence, pages 1043-1048, Detroit, MI, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randall J Calistri-Yeh</author>
</authors>
<title>Utilizing user models to handle ambiguity and misconceptions in robust plan recognition. User Modelling and User Adapted Interaction,</title>
<date>1991</date>
<pages>1--4</pages>
<marker>[Calistri-Yeh, 1991]</marker>
<rawString>Randall J. Calistri-Yeh. Utilizing user models to handle ambiguity and misconceptions in robust plan recognition. User Modelling and User Adapted Interaction, 1(4):289-322, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Pragmatics Modeling in Information Systems Interfaces.</title>
<date>1985</date>
<tech>PhD thesis,</tech>
<institution>University of Delaware,</institution>
<location>Newark, Delaware,</location>
<marker>[Carberry, 1985]</marker>
<rawString>Sandra Carberry. Pragmatics Modeling in Information Systems Interfaces. PhD thesis, University of Delaware, Newark, Delaware, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison J Cawsey</author>
</authors>
<title>A belief revision model of repair sequences in dialogue.</title>
<date>1991</date>
<booktitle>New Directions in Intelligent Tutoring Systems.</booktitle>
<editor>In Ernesto Costa, editor,</editor>
<publisher>Springer Verlag,</publisher>
<marker>[Cawsey, 1991]</marker>
<rawString>Alison J. Cawsey. A belief revision model of repair sequences in dialogue. In Ernesto Costa, editor, New Directions in Intelligent Tutoring Systems. Springer Verlag, 1991.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<title>Speech acts and rationality.</title>
<booktitle>In 23th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>49--60</pages>
<marker>[Cohen and Levesque, 1985]</marker>
<rawString>Philip R. Cohen and Hector J. Levesque. Speech acts and rationality. In 23th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 49-60,1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
</authors>
<date>1990</date>
<booktitle>Intentions in Communication. The</booktitle>
<editor>Jerry Morgan, and Martha Pollack, editors.</editor>
<publisher>MIT Press,</publisher>
<marker>[Cohen et al., 1990]</marker>
<rawString>Philip R. Cohen, Jerry Morgan, and Martha Pollack, editors. Intentions in Communication. The MIT Press, 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rhonda Eller</author>
<author>Sandra Carberry</author>
</authors>
<title>A meta-rule approach to flexible plan recognition in dialogue. User Modelling and User Adapted Interaction,</title>
<pages>2--1</pages>
<marker>[Eller and Carberry, 1992]</marker>
<rawString>Rhonda Eller and Sandra Carberry. A meta-rule approach to flexible plan recognition in dialogue. User Modelling and User Adapted Interaction, 2(1-2):27-53,1992.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Barbara Fox</author>
</authors>
<title>Interactional reconstruction in real-time language processing.</title>
<journal>Cognitive Science,</journal>
<pages>11--365</pages>
<marker>[Fox, 1987]</marker>
<rawString>Barbara Fox. Interactional reconstruction in real-time language processing. Cognitive Science, 11:365-387,1987.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Harold Garfinkel</author>
</authors>
<title>Studies in Ethnomethodology. Prentice Hall, Englewood Cliffs, NJ,</title>
<date>1967</date>
<booktitle>In The 23rd Annual Meeting of the Association for Computational Linguistics: Proceedings of the Conference,</booktitle>
<pages>204--217</pages>
<location>Chicago,</location>
<marker>[Garfinkel, 1967]</marker>
<rawString>Harold Garfinkel. Studies in Ethnomethodology. Prentice Hall, Englewood Cliffs, NJ, 1967. (Reprinted: Cambridge, England: Polity Press, in association with Basil Blackwell, 1984.). [Goodman, 19851 Bradley Goodman. Repairing reference identification failures by relaxation. In The 23rd Annual Meeting of the Association for Computational Linguistics: Proceedings of the Conference, pages 204-217, Chicago, 1985.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H P Grice</author>
</authors>
<journal>Meaning. The Philosophical Review,</journal>
<pages>66--377</pages>
<marker>[Grice, 1957]</marker>
<rawString>H. P. Grice. Meaning. The Philosophical Review, 66:377-388,1957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Gutwin</author>
<author>Gordon McCalla</author>
</authors>
<title>Would I lie to you? Modelling context and pedagogic misrepresentation in tutorial dialogue.</title>
<date>1992</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>152--158</pages>
<location>Newark, DE,</location>
<marker>[Gutwin and McCalla, 1992]</marker>
<rawString>Carl Gutwin and Gordon McCalla. Would I lie to you? Modelling context and pedagogic misrepresentation in tutorial dialogue. In 30th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 152-158, Newark, DE, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating on referring expressions.</title>
<date>1992</date>
<tech>Technical Report 435,</tech>
<institution>Department of Computer Science, University of Rochester,</institution>
<marker>[Heeman and Hirst, 1992]</marker>
<rawString>Peter Heeman and Graeme Hirst. Collaborating on referring expressions. Technical Report 435, Department of Computer Science, University of Rochester, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
</authors>
<title>Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues.</title>
<date>1985</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, University of Rochester,</institution>
<location>Rochester, NY,</location>
<note>Published as</note>
<marker>[Litman, 1985]</marker>
<rawString>Diane J. Litman. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues. PhD thesis, Department of Computer Science, University of Rochester, Rochester, NY, 1985. Published as University of Rochester Computer Science Technical Report 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Loveland</author>
</authors>
<title>Automated Theorem Proving: A Logical Basis.</title>
<date>1978</date>
<publisher>North-Holland,</publisher>
<location>Amsterdam, The Netherlands,</location>
<marker>[Loveland, 1978]</marker>
<rawString>D. W. Loveland. Automated Theorem Proving: A Logical Basis. North-Holland, Amsterdam, The Netherlands, 1978.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kathleen F McCoy</author>
</authors>
<title>The role of perspective in responding to property misconceptions.</title>
<booktitle>In Proceedings of the Ninth International Joint Conference on Artificial Intelligence,</booktitle>
<volume>2</volume>
<pages>791--793</pages>
<marker>[McCoy, 1985]</marker>
<rawString>Kathleen F. McCoy. The role of perspective in responding to property misconceptions. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, volume 2, pages 791-793,1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret L McLaughlin</author>
</authors>
<title>Conversation: How Talk is Organized.</title>
<date>1984</date>
<publisher>Sage Publications,</publisher>
<location>Beverly Hills,</location>
<marker>[McLaughlin, 1984]</marker>
<rawString>Margaret L. McLaughlin. Conversation: How Talk is Organized. Sage Publications, Beverly Hills, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
<author>Graeme Hirst</author>
</authors>
<title>The repair of speech act misunderstandings by abductive inference.</title>
<date>1992</date>
<note>Submitted for publication.</note>
<marker>[McRoy and Hirst, 1992]</marker>
<rawString>Susan W. McRoy and Graeme Hirst. The repair of speech act misunderstandings by abductive inference. 1992. Submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Abductive Interpretation and Reinterpretation of Natural Language Utterances.</title>
<date>1993</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, University of Toronto,</institution>
<location>Toronto, Canada,</location>
<note>In preparation.</note>
<marker>[McRoy, 1993]</marker>
<rawString>Susan W. McRoy. Abductive Interpretation and Reinterpretation of Natural Language Utterances. PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada, 1993. In preparation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Raymond Perrault</author>
<author>James F Allen</author>
</authors>
<title>A plan-based analysis of indirect speech acts.</title>
<journal>Computational Linguistics,</journal>
<pages>6--167</pages>
<marker>[Perrault and Allen, 1980]</marker>
<rawString>C. Raymond Perrault and James F. Allen. A plan-based analysis of indirect speech acts. Computational Linguistics, 6:167-183,1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
</authors>
<title>An application of default logic to speech act theory.</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>161--186</pages>
<editor>In Philip It. Cohen, Jerry Morgan, and Martha Pollack, editors,</editor>
<publisher>The MIT Press,</publisher>
<marker>[Perrault, 1990]</marker>
<rawString>C. Raymond Perrault. An application of default logic to speech act theory. In Philip It. Cohen, Jerry Morgan, and Martha Pollack, editors, Intentions in Communication, pages 161-186. The MIT Press, 1990. An earlier version of this paper was published as Technical Report CSLI-87-90 by the Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Poole</author>
<author>Randy Goebel</author>
<author>Romas Aleliunas</author>
</authors>
<title>Theorist: A logical reasoning system for defaults and diagnosis.</title>
<date>1987</date>
<booktitle>The Knowledge Frontier: Essays in the Representation of Knowledge,</booktitle>
<pages>331--352</pages>
<editor>In Nick Cercone and Gordon McCalla, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<institution>Faculty of Mathematics, University of Waterloo,</institution>
<location>New York,</location>
<note>Also published as Research Report CS-86-06,</note>
<marker>[Poole et at., 1987]</marker>
<rawString>David Poole, Randy Goebel, and Romas Aleliunas. Theorist: A logical reasoning system for defaults and diagnosis. In Nick Cercone and Gordon McCalla, editors, The Knowledge Frontier: Essays in the Representation of Knowledge, pages 331-352. Springer-Verlag, New York, 1987. Also published as Research Report CS-86-06, Faculty of Mathematics, University of Waterloo, February, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Poole</author>
</authors>
<title>Default reasoning and diagnosis as theory formation.</title>
<date>1986</date>
<tech>Technical Report CS-86-08,</tech>
<institution>Department of Computer Science, University of Waterloo,</institution>
<location>Waterloo, Ontario,</location>
<marker>[Poole, 1986]</marker>
<rawString>David Poole. Default reasoning and diagnosis as theory formation. Technical Report CS-86-08, Department of Computer Science, University of Waterloo, Waterloo, Ontario, 1986.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Poole</author>
</authors>
<title>Normality and faults in logic-based diagnosis.</title>
<booktitle>In Proceedings of the 11th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1304--1310</pages>
<marker>[Poole, 1989]</marker>
<rawString>David Poole. Normality and faults in logic-based diagnosis. In Proceedings of the 11th International Joint Conference on Artificial Intelligence, pages 1304-1310,1989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Emanuel</author>
</authors>
<title>Schegloff and Harvey Sacks. Opening up closings.</title>
<journal>Semiotica,</journal>
<pages>7--289</pages>
<marker>[Schegloff and Sacks, 1973]</marker>
<rawString>Emanuel A. Schegloff and Harvey Sacks. Opening up closings. Semiotica, 7:289-327,1973.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Repair after next turn: The last structurally provided defense of intersubjectivity in conversation.</title>
<journal>American Journal of Sociology,</journal>
<pages>97--5</pages>
<marker>[Schegloff, 1992]</marker>
<rawString>Emanuel A. Schegloff. Repair after next turn: The last structurally provided defense of intersubjectivity in conversation. American Journal of Sociology, 97(5):1295-1345,1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Stalnaker</author>
</authors>
<title>Pragmatics. In Semantics of Natural Language,</title>
<date>1972</date>
<pages>380--397</pages>
<publisher>D. Reidel Publishing Company,</publisher>
<location>Dordrecht,</location>
<marker>[Stalnaker, 1972]</marker>
<rawString>Robert C. Stalnaker. Pragmatics. In Semantics of Natural Language, pages 380-397. D. Reidel Publishing Company, Dordrecht, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Stickel</author>
</authors>
<title>A Prolog technology theorem prover.</title>
<date>1987</date>
<journal>Journal of Automated Reasoning, 4:353-360,1989. [Suchman,</journal>
<volume>19871</volume>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<marker>[Stickel, 1989]</marker>
<rawString>M. E. Stickel. A Prolog technology theorem prover. Journal of Automated Reasoning, 4:353-360,1989. [Suchman, 19871 Lucy A. Suchman. Plans and Situated Actions. Cambridge University Press, Cambridge, UK, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
</authors>
<title>Propagating epistemic coordination through mutual defaults I.</title>
<date>1990</date>
<booktitle>Proceedings, Third Conference on Theoretical Aspects of Reasoning about Knowledge (TARK</booktitle>
<pages>29--39</pages>
<editor>In Rohit Parikh, editor,</editor>
<location>Pacific Grove, CA,</location>
<marker>[Thomason, 1990]</marker>
<rawString>Richmond H. Thomason. Propagating epistemic coordination through mutual defaults I. In Rohit Parikh, editor, Proceedings, Third Conference on Theoretical Aspects of Reasoning about Knowledge (TARK 1990), pages 29-39, Pacific Grove, CA, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zerksis D Umrigar</author>
<author>Vijay Pitchumani</author>
</authors>
<title>An experiment in programming with full first-order logic.</title>
<date>1985</date>
<booktitle>In Symposium of Logic Programming,</booktitle>
<publisher>IEEE Computer Society Press.</publisher>
<location>Boston, MA,</location>
<marker>[Umrigar and Pitchumani, 1985]</marker>
<rawString>Zerksis D. Umrigar and Vijay Pitchumani. An experiment in programming with full first-order logic. In Symposium of Logic Programming, Boston, MA, 1985. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul van Arragon</author>
</authors>
<title>Nested Default Reasoning for User Modeling.</title>
<date>1990</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, University of Waterloo,</institution>
<location>Waterloo, Ontario,</location>
<note>Published by the department as Research Report CS-90-25.</note>
<marker>[van Arragon, 1990]</marker>
<rawString>Paul van Arragon. Nested Default Reasoning for User Modeling. PhD thesis, Department of Computer Science, University of Waterloo, Waterloo, Ontario, 1990. Published by the department as Research Report CS-90-25.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>