<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.156633">
<title confidence="0.90914">
Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment
</title>
<email confidence="0.922236">
huangsj@nlp.nju.edu.cn
</email>
<author confidence="0.971976">
Shujian Huang
</author>
<affiliation confidence="0.969673">
State Key Laboratory for
Novel Software Technology
Nanjing University
</affiliation>
<author confidence="0.993175">
Stephan Vogel
</author>
<affiliation confidence="0.9903595">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<email confidence="0.99225">
vogel@cs.cmu.edu
</email>
<author confidence="0.95254">
Jiajun Chen
</author>
<affiliation confidence="0.979359333333333">
State Key Laboratory for
Novel Software Technology
Nanjing University
</affiliation>
<email confidence="0.990006">
chenjj@nlp.nju.edu.cn
</email>
<sectionHeader confidence="0.998544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999052823529412">
Word alignment has an exponentially large
search space, which often makes exact infer-
ence infeasible. Recent studies have shown
that inversion transduction grammars are rea-
sonable constraints for word alignment, and
that the constrained space could be efficiently
searched using synchronous parsing algo-
rithms. However, spurious ambiguity may oc-
cur in synchronous parsing and cause prob-
lems in both search efficiency and accuracy. In
this paper, we conduct a detailed study of the
causes of spurious ambiguity and how it ef-
fects parsing and discriminative learning. We
also propose a variant of the grammar which
eliminates those ambiguities. Our grammar
shows advantages over previous grammars in
both synthetic and real-world experiments.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793928571429">
In statistical machine translation, word alignment at-
tempts to find word correspondences in parallel sen-
tence pairs. The search space of word alignment
will grow exponentially with the length of source
and target sentences, which makes the inference for
complex models infeasible (Brown et al., 1993). Re-
cently, inversion transduction grammars (Wu, 1997),
namely ITG, have been used to constrain the search
space for word alignment (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Haghighi et al., 2009; Liu et
al., 2010). ITG is a family of grammars in which the
right hand side of the rule is either two nonterminals
or a terminal sequence. The most general case of the
ITG family is the bracketing transduction grammar
</bodyText>
<figure confidence="0.539193">
A — [AA]  |(AA)  |e/f  |e/f  |e/e
</figure>
<figureCaption confidence="0.9955695">
Figure 1: BTG rules. [AA] denotes a monotone concate-
nation and (AA) denotes an inverted concatenation.
</figureCaption>
<bodyText confidence="0.999230413793104">
(BTG, Figure 1), which has only one nonterminal
symbol.
Synchronous parsing of ITG may generate a large
number of different derivations for the same under-
lying word alignment. This is often referred to as
the spurious ambiguity problem. Calculating and
saving those derivations will slow down the parsing
speed significantly. Furthermore, spurious deriva-
tions may fill up the n-best list and supersede po-
tentially good results, making it harder to find the
best alignment. Besides, over-counting those spu-
rious derivations will also affect the likelihood es-
timation. In order to reduce spurious derivations,
Wu (1997), Haghighi et al. (2009), Liu et al. (2010)
propose different variations of the grammar. These
grammars have different behaviors in parsing effi-
ciency and accuracy, but so far no detailed compari-
son between them has been done.
In this paper, we formally analyze alignments un-
der ITG constraints and the different causes of spu-
rious ambiguity for those alignments. We do an em-
pirical study of the influence of spurious ambiguity
on parsing and discriminative learning by compar-
ing different grammars in both synthetic and real-
data experiments. To our knowledge, this is the first
in-depth analysis on this specific issue. A new vari-
ant of the grammar is proposed, which efficiently re-
moves all spurious ambiguities. Our grammar shows
advantages over previous ones in both experiments.
</bodyText>
<page confidence="0.989889">
379
</page>
<note confidence="0.77833">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 379–383,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.999553">
A
A
A A
e1 e2 e3
f1 f2 f3
A
A A
A A
e1 e2 e3
f1 f2 f3
A
A
A A
e1 e2 e3
f1 f2 f3
A
A A
A A
e1 e2 e3
f1 f2 f3
A
A
A → [AB]  |[BB]  |[CB]  |[AC]  |[BC]  |[CC]
B → hAAi  |hBAi  |hCAi  |hACi  |hBCi  |hCCi
C → e/f  |E/f  |e/E
</figure>
<figureCaption confidence="0.99678">
Figure 3: A Left heavy Grammar (LG).
Figure 2: Possible monotone/inverted t-splits (dashed
lines) under BTG, causing branching ambiguities.
</figureCaption>
<sectionHeader confidence="0.981182" genericHeader="method">
2 ITG Alignment Family
</sectionHeader>
<bodyText confidence="0.999516785714286">
By lexical rules like A → e/f, each ITG derivation
actually represents a unique alignment between the
two sequences. Thus the family of ITG derivations
represents a family of word alignment.
Definition 1. The ITG alignment family is a set of
word alignments that has at least one BTG deriva-
tion.
ITG alignment family is only a subset of word
alignments because there are cases, known as inside-
outside alignments (Wu, 1997), that could not be
represented by any ITG derivation. On the other
hand, an ITG alignment may have multiple deriva-
tions.
Definition 2. For a given grammar G, spurious am-
biguity in word alignment is the case where two or
more derivations d1, d2, ... dk of G have the same
underlying word alignment A. A grammar G is non-
spurious if for any given word alignment, there exist
at most one derivation under G.
In any given derivation, an ITG rule applies by ei-
ther generating a bilingual word pair (lexical rules)
or splitting the current alignment into two parts,
which will recursively generate two sub-derivations
(transition rules).
Definition 3. Applying a monotone (or inverted)
concatenation transition rule forms a monotone t-
split (or inverted t-split) of the original alignment
(Figure 2).
</bodyText>
<sectionHeader confidence="0.87708" genericHeader="method">
3 Causes of Spurious Ambiguity
</sectionHeader>
<subsectionHeader confidence="0.99961">
3.1 Branching Ambiguity
</subsectionHeader>
<bodyText confidence="0.999150806451613">
As shown in Figure 2, left-branching and right-
branching will produce different derivations under
BTG, but yield the same word alignment. Branching
ambiguity was identified and solved in Wu (1997),
using the grammar in Figure 3, denoted as LG. LG
uses two separate non-terminals for monotone and
inverted concatenation, respectively. It only allows
left branching of such non-terminals, by excluding
rules like A → [BA].
Theorem 1. For each ITG alignment A, in which
all the words are aligned, LG will produce a unique
derivation.
Proof: Induction on n, the length of A. Case n=1
is trivial. Induction hypothesis: the theorem holds
for any A with length less than n.
For A of length n, let s be the right most t-split
which splits A into 51 and 52. s exists because A is
an ITG alignment. Assume that there exists another
t-split s&apos;, splitting A into 511 and (51252). Because
A is fixed and fully aligned, it is easy to see that if
s is a monotone t-split, s&apos; could only be monotone,
and 512 and 52 in the right sub-derivation of t-split s&apos;
could only be combined by monotone concatenation
as well. So s&apos; will have a right branching of mono-
tone concatenation, which contradicts with the def-
inition of LG because right branching of monotone
concatenations is prohibited. A similar contradic-
tion occurs if s is an inverted t-split. Thus s should
be the unique t-split for A. By I.H., 51 and 52 have a
unique derivation, because their lengths are less than
n. Thus the derivation for A will be unique.
</bodyText>
<subsectionHeader confidence="0.99792">
3.2 Null-word Attachment Ambiguity
</subsectionHeader>
<bodyText confidence="0.9982336">
Definition 4. For any given sentence pair (e, f) and
its alignment A, let (e&apos;, f&apos;) be the sentence pairs
with all null-aligned words removed from (e, f).
The alignment skeleton AS is the alignment between
(e&apos;, f&apos;) that preserves all links in A.
From Theorem 1 we know that every ITG align-
ment has a unique LG derivation for its alignment
skeleton (Figure 4 (c)).
However, because of the lexical or syntactic dif-
ferences between languages, some words may have
</bodyText>
<page confidence="0.960095">
380
</page>
<figure confidence="0.991989578947368">
A
B
A
C B
A
C C
C
C
A
C
C
C
A
C C
B
e1/E e2 e3 e4 e1/E e2 e3 e4
f1 f2 f3 f1 f2 f3
(a) (b)
C� C
</figure>
<page confidence="0.989307">
381
</page>
<table confidence="0.989815666666667">
% 0 5 10 15 20 25
LG 1 42.2 1920.8 9914.1+ 10000+ 10000+
HaG 1 3.5 10.9 34.1 89.2 219.9
</table>
<tableCaption confidence="0.944036333333333">
Table 1: Average #derivations per alignment for LG and
HaG v.s. Percentage of unaligned words. (+ marked
parses have reached the beam size limit of 10000.)
</tableCaption>
<figure confidence="0.9991770625">
0.2
0
0.1
0.1
0.1
0.1
0.1
600
500
400
300
200
100
0
0 5 10
Percenaget of null-al
</figure>
<figureCaption confidence="0.99873">
Figure 6: Total parsing time (in seconds) v.s. Percentage
of un-aligned words.
</figureCaption>
<bodyText confidence="0.998918916666667">
the 10-best alignments for sentence pairs that have
10% of words unaligned, the top 109 HaG deriva-
tions should be generated, while the top 10 LiuG or
LGFN derivations are already enough.
Figure 6 shows the total parsing time using each
grammar. LG and HaG showed better performances
when most of the words were aligned because their
grammars are simpler and less constrained. How-
ever, when the number of null-aligned words in-
creased, the parsing times for LG and HaG became
much longer, caused by the calculation of the large
number of spurious derivations. Parsings using LG
for 10 and 15 percent of null-aligned words took
around 15 and 80 minutes, respectively, which can-
not be plotted in the same scale with other gram-
mars. The parsing times of LGFN and LiuG also
slowly increased, but parsing LGFN consistently
took less time than LiuG.
It should be noticed that the above results came
from parsing according to some given alignment.
When searching without knowing the correct align-
ment, it is possible for every word to stay unaligned,
which makes spurious ambiguity a much more seri-
ous issue.
</bodyText>
<subsectionHeader confidence="0.9888">
4.2 Discriminative Learning Experiments
</subsectionHeader>
<bodyText confidence="0.9994685">
To further study how spurious ambiguity affects the
discriminative learning, we implemented a frame-
work following Haghighi et al. (2009). We used
a log-linear model, with features like IBM model1
</bodyText>
<figureCaption confidence="0.998672">
Figure 7: Test set AER after each iteration.
</figureCaption>
<bodyText confidence="0.999933054054054">
probabilities (collected from FBIS data), relative
distances, matchings of high frequency words,
matchings of pos-tags, etc. Online training was
performed using the margin infused relaxed algo-
rithm (Crammer et al., 2006), MIRA. For each
sentence pair (e, f ), we optimized with alignment
results generated from the nbest parsinggresults.
Alignment error rate (Och and Ney, 2003), AER,
was used as the loss function. We ran MIRA train-
ing for 20 iterations and evaluated the alignments of
the best-scored derivations on the test set using the
average weights.
We used the manually aligned Chinese-English
corpus in NIST MT02 evaluation. The first 200 sen-
tence pairs were used for training, and the last 150
for testing. There are, on average, 10.3% words stay
null-aligned in each sentence, but if restricted to sure
links the average ratio increases to 22.6%.
We compared training using LGFN with 1-best,
20-best and HaG with 20-best (Figure 7). Train-
ing with HaG only obtained similar results with 1-
best trained LGFN, which demonstrated that spu-
rious ambiguity highly affected the nbest list here,
resulting in a less accurate training. Actually, the
20-best parsing using HaG only generated 4.53 dif-
ferent alignments on average. 20-best training us-
ing LGFN converged quickly after the first few it-
erations and obtained an AER score (17.23) better
than other systems, which is also lower than the re-
fined IBM Model 4 result (19.07).
We also trained a similar discriminative model but
extended the lexical rule of LGFN to accept at max-
imum 3 consecutive words. The model was used
to align FBIS data for machine translation exper-
iments. Without initializing by phrases extracted
from existing alignments (Cherry and Lin, 2007) or
using complicated block features (Haghighi et al.,
</bodyText>
<figure confidence="0.967485">
HaG
LFG
I
I f
</figure>
<page confidence="0.992356">
382
</page>
<bodyText confidence="0.999906">
2009), we further reduced AER on the test set to
12.25. An average improvement of 0.52 BLEU (Pa-
pineni et al., 2002) score and 2.05 TER (Snover
et al., 2006) score over 5 test sets for a typical
phrase-based translation system, Moses (Koehn et
al., 2003), validated the effectiveness of our experi-
ments.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999895263157895">
Great efforts have been made in reducing spurious
ambiguities in parsing combinatory categorial gram-
mar (Karttunen, 1986; Eisner, 1996). However, to
our knowledge, we give the first detailed analysis on
spurious ambiguity of word alignment. Empirical
comparisons between different grammars also vali-
dates our analysis.
This paper makes its own contribution in demon-
strating that spurious ambiguity has a negative im-
pact on discriminative learning. We will continue
working on this line of research and improve our
discriminative learning model in the future, for ex-
ample, by adding more phrase level features.
It is worth noting that the definition of spuri-
ous ambiguity actually varies for different tasks. In
some cases, e.g. bilingual chunking, keeping differ-
ent null-aligned word attachments could be useful.
It will also be interesting to explore spurious ambi-
guity and its effects in those different tasks.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974857142857">
The authors would like to thank Alon Lavie, Qin
Gao and the anonymous reviewers for their valu-
able comments. This work is supported by the Na-
tional Natural Science Foundation of China (No.
61003112), the National Fundamental Research
Program of China (2010CB327903) and by NSF un-
der the CluE program, award IIS 084450.
</bodyText>
<sectionHeader confidence="0.999306" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99953429032258">
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL-HLT 2007/AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, SSST ’07, pages 17–24, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551–
585, December.
Jason Eisner. 1996. Efficient normal-form parsing for
combinatory categorial grammar. In Proceedings of
the 34th annual meeting on Association for Compu-
tational Linguistics, ACL ’96, pages 79–86, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Aria Haghighi, John Blitzer, and Dan Klein. 2009. Bet-
ter word alignments with supervised itg models. In
Association for Computational Linguistics, Singapore.
Lauri Karttunen. 1986. Radical lexicalism. Technical
Report CSLI-86-68, Stanford University.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Shujie Liu, Chi-Ho Li, and Ming Zhou. 2010. Dis-
criminative pruning for discriminative itg alignment.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ’10,
pages 316–324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ’02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311–318, Morristown, NJ,
USA. Association for Computational Linguistics.
Matthew Snover, Bonnie J. Dorr, and Richard Schwartz.
2006. A study of translation edit rate with targeted
human annotation. In Proceedings of AMTA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23:377–403, September.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexi-
calized inversion transduction grammar for alignment.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ’05,
pages 475–482, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics, pages 256–263, Morristown, NJ,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.999367">
383
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308445">
<title confidence="0.9344465">Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment huangsj@nlp.nju.edu.cn</title>
<author confidence="0.875647">Shujian</author>
<affiliation confidence="0.978703333333333">State Key Laboratory Novel Software Nanjing University</affiliation>
<author confidence="0.903995">Stephan</author>
<affiliation confidence="0.8325065">Language Technologies Carnegie Mellon</affiliation>
<email confidence="0.983589">vogel@cs.cmu.edu</email>
<affiliation confidence="0.879266333333333">Jiajun State Key Laboratory Novel Software</affiliation>
<address confidence="0.885326">Nanjing</address>
<email confidence="0.972208">chenjj@nlp.nju.edu.cn</email>
<abstract confidence="0.999637166666667">Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematic of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1435" citStr="Brown et al., 1993" startWordPosition="198" endWordPosition="201">s paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar A — [AA] |(AA) |e/f |e/f |e/e Figure 1: BTG rules. [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. (BTG, Figure 1), which has only one nonterminal</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation, SSST ’07,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1612" citStr="Cherry and Lin, 2007" startWordPosition="226" endWordPosition="229">eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar A — [AA] |(AA) |e/f |e/f |e/e Figure 1: BTG rules. [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambigu</context>
<context position="10842" citStr="Cherry and Lin, 2007" startWordPosition="1816" endWordPosition="1819">ting in a less accurate training. Actually, the 20-best parsing using HaG only generated 4.53 different alignments on average. 20-best training using LGFN converged quickly after the first few iterations and obtained an AER score (17.23) better than other systems, which is also lower than the refined IBM Model 4 result (19.07). We also trained a similar discriminative model but extended the lexical rule of LGFN to accept at maximum 3 consecutive words. The model was used to align FBIS data for machine translation experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., HaG LFG I I f 382 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation, SSST ’07, pages 17–24, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>7</volume>
<pages>585</pages>
<contexts>
<context position="9336" citStr="Crammer et al., 2006" startWordPosition="1568" endWordPosition="1571">t alignment, it is possible for every word to stay unaligned, which makes spurious ambiguity a much more serious issue. 4.2 Discriminative Learning Experiments To further study how spurious ambiguity affects the discriminative learning, we implemented a framework following Haghighi et al. (2009). We used a log-linear model, with features like IBM model1 Figure 7: Test set AER after each iteration. probabilities (collected from FBIS data), relative distances, matchings of high frequency words, matchings of pos-tags, etc. Online training was performed using the margin infused relaxed algorithm (Crammer et al., 2006), MIRA. For each sentence pair (e, f ), we optimized with alignment results generated from the nbest parsinggresults. Alignment error rate (Och and Ney, 2003), AER, was used as the loss function. We ran MIRA training for 20 iterations and evaluated the alignments of the best-scored derivations on the test set using the average weights. We used the manually aligned Chinese-English corpus in NIST MT02 evaluation. The first 200 sentence pairs were used for training, and the last 150 for testing. There are, on average, 10.3% words stay null-aligned in each sentence, but if restricted to sure links</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. J. Mach. Learn. Res., 7:551– 585, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient normal-form parsing for combinatory categorial grammar.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11366" citStr="Eisner, 1996" startWordPosition="1906" endWordPosition="1907">Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., HaG LFG I I f 382 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative learning. We will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features. It is worth noting that the definition of spurious ambiguity actually varies for different tasks. In some cases, e.g. bilingu</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Efficient normal-form parsing for combinatory categorial grammar. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96, pages 79–86, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1635" citStr="Haghighi et al., 2009" startWordPosition="230" endWordPosition="233">uities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar A — [AA] |(AA) |e/f |e/f |e/e Figure 1: BTG rules. [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculatin</context>
<context position="9011" citStr="Haghighi et al. (2009)" startWordPosition="1519" endWordPosition="1522">pectively, which cannot be plotted in the same scale with other grammars. The parsing times of LGFN and LiuG also slowly increased, but parsing LGFN consistently took less time than LiuG. It should be noticed that the above results came from parsing according to some given alignment. When searching without knowing the correct alignment, it is possible for every word to stay unaligned, which makes spurious ambiguity a much more serious issue. 4.2 Discriminative Learning Experiments To further study how spurious ambiguity affects the discriminative learning, we implemented a framework following Haghighi et al. (2009). We used a log-linear model, with features like IBM model1 Figure 7: Test set AER after each iteration. probabilities (collected from FBIS data), relative distances, matchings of high frequency words, matchings of pos-tags, etc. Online training was performed using the margin infused relaxed algorithm (Crammer et al., 2006), MIRA. For each sentence pair (e, f ), we optimized with alignment results generated from the nbest parsinggresults. Alignment error rate (Och and Ney, 2003), AER, was used as the loss function. We ran MIRA training for 20 iterations and evaluated the alignments of the best</context>
</contexts>
<marker>Haghighi, Blitzer, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, and Dan Klein. 2009. Better word alignments with supervised itg models. In Association for Computational Linguistics, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Radical lexicalism.</title>
<date>1986</date>
<tech>Technical Report CSLI-86-68,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="11351" citStr="Karttunen, 1986" startWordPosition="1904" endWordPosition="1905">ion experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., HaG LFG I I f 382 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative learning. We will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features. It is worth noting that the definition of spurious ambiguity actually varies for different tasks. In some case</context>
</contexts>
<marker>Karttunen, 1986</marker>
<rawString>Lauri Karttunen. 1986. Radical lexicalism. Technical Report CSLI-86-68, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="11168" citStr="Koehn et al., 2003" startWordPosition="1876" endWordPosition="1879">. We also trained a similar discriminative model but extended the lexical rule of LGFN to accept at maximum 3 consecutive words. The model was used to align FBIS data for machine translation experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., HaG LFG I I f 382 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative learning. We will continue working on this line of research and improve our discriminative learning</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Discriminative pruning for discriminative itg alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>316--324</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1654" citStr="Liu et al., 2010" startWordPosition="234" endWordPosition="237">ws advantages over previous grammars in both synthetic and real-world experiments. 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar A — [AA] |(AA) |e/f |e/f |e/e Figure 1: BTG rules. [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those </context>
</contexts>
<marker>Liu, Li, Zhou, 2010</marker>
<rawString>Shujie Liu, Chi-Ho Li, and Ming Zhou. 2010. Discriminative pruning for discriminative itg alignment. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 316–324, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9494" citStr="Och and Ney, 2003" startWordPosition="1593" endWordPosition="1596">o further study how spurious ambiguity affects the discriminative learning, we implemented a framework following Haghighi et al. (2009). We used a log-linear model, with features like IBM model1 Figure 7: Test set AER after each iteration. probabilities (collected from FBIS data), relative distances, matchings of high frequency words, matchings of pos-tags, etc. Online training was performed using the margin infused relaxed algorithm (Crammer et al., 2006), MIRA. For each sentence pair (e, f ), we optimized with alignment results generated from the nbest parsinggresults. Alignment error rate (Och and Ney, 2003), AER, was used as the loss function. We ran MIRA training for 20 iterations and evaluated the alignments of the best-scored derivations on the test set using the average weights. We used the manually aligned Chinese-English corpus in NIST MT02 evaluation. The first 200 sentence pairs were used for training, and the last 150 for testing. There are, on average, 10.3% words stay null-aligned in each sentence, but if restricted to sure links the average ratio increases to 22.6%. We compared training using LGFN with 1-best, 20-best and HaG with 20-best (Figure 7). Training with HaG only obtained s</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11030" citStr="Papineni et al., 2002" startWordPosition="1851" endWordPosition="1855">t few iterations and obtained an AER score (17.23) better than other systems, which is also lower than the refined IBM Model 4 result (19.07). We also trained a similar discriminative model but extended the lexical rule of LGFN to accept at maximum 3 consecutive words. The model was used to align FBIS data for machine translation experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., HaG LFG I I f 382 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity h</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="11071" citStr="Snover et al., 2006" startWordPosition="1860" endWordPosition="1863">(17.23) better than other systems, which is also lower than the refined IBM Model 4 result (19.07). We also trained a similar discriminative model but extended the lexical rule of LGFN to accept at maximum 3 consecutive words. The model was used to align FBIS data for machine translation experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., HaG LFG I I f 382 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative le</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, and Richard Schwartz. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<pages>23--377</pages>
<contexts>
<context position="1489" citStr="Wu, 1997" startWordPosition="207" endWordPosition="208">mbiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar A — [AA] |(AA) |e/f |e/f |e/e Figure 1: BTG rules. [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a lar</context>
<context position="4387" citStr="Wu, 1997" startWordPosition="702" endWordPosition="703">|hCCi C → e/f |E/f |e/E Figure 3: A Left heavy Grammar (LG). Figure 2: Possible monotone/inverted t-splits (dashed lines) under BTG, causing branching ambiguities. 2 ITG Alignment Family By lexical rules like A → e/f, each ITG derivation actually represents a unique alignment between the two sequences. Thus the family of ITG derivations represents a family of word alignment. Definition 1. The ITG alignment family is a set of word alignments that has at least one BTG derivation. ITG alignment family is only a subset of word alignments because there are cases, known as insideoutside alignments (Wu, 1997), that could not be represented by any ITG derivation. On the other hand, an ITG alignment may have multiple derivations. Definition 2. For a given grammar G, spurious ambiguity in word alignment is the case where two or more derivations d1, d2, ... dk of G have the same underlying word alignment A. A grammar G is nonspurious if for any given word alignment, there exist at most one derivation under G. In any given derivation, an ITG rule applies by either generating a bilingual word pair (lexical rules) or splitting the current alignment into two parts, which will recursively generate two sub-</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23:377–403, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>475--482</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1590" citStr="Zhang and Gildea, 2005" startWordPosition="222" endWordPosition="225">nt of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar A — [AA] |(AA) |e/f |e/f |e/e Figure 1: BTG rules. [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to </context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 475–482, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 256–263, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>