<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.959115">
It’s Not You, it’s Me: Detecting Flirting and its Misperception in
Speed-Dates
</title>
<author confidence="0.992755">
Rajesh Ranganath Dan Jurafsky Dan McFarland
</author>
<affiliation confidence="0.998894">
Computer Science Department Linguistics Department School of Education
Stanford University Stanford University Stanford University
</affiliation>
<email confidence="0.998853">
rajeshr@cs.stanford.edu jurafsky@stanford.edu dmcfarla@stanford.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851703703704">
Automatically detecting human social in-
tentions from spoken conversation is an
important task for dialogue understand-
ing. Since the social intentions of the
speaker may differ from what is perceived
by the hearer, systems that analyze human
conversations need to be able to extract
both the perceived and the intended social
meaning. We investigate this difference
between intention and perception by using
a spoken corpus of speed-dates in which
both the speaker and the listener rated the
speaker on flirtatiousness. Our flirtation-
detection system uses prosodic, dialogue,
and lexical features to detect a speaker’s
intent to flirt with up to 71.5% accuracy,
significantly outperforming the baseline,
but also outperforming the human inter-
locuters. Our system addresses lexical fea-
ture sparsity given the small amount of
training data by using an autoencoder net-
work to map sparse lexical feature vectors
into 30 compressed features. Our analy-
sis shows that humans are very poor per-
ceivers of intended flirtatiousness, instead
often projecting their own intended behav-
ior onto their interlocutors.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990768627451">
Detecting human social meaning is a difficult task
for automatic conversational understanding sys-
tems. One cause of this difficulty is the pervasive
difference between intended social signals and the
uptake by the perceiver. The cues that a speaker
may use to attempt to signal a particular social
meaning may not be the cues that the hearer fo-
cuses on, leading to misperception.
In order to understand the impact of this dif-
ference between perception and intention, in this
paper we describe machine learning models that
can detect both the social meaning intended by the
speaker and the social meaning perceived by the
hearer. Automated systems that detect and model
these differences can lead both to richer socially
aware systems for conversational understanding
and more sophisticated analyses of conversational
interactions like meetings and interviews.
This task thus extends the wide literature on
social meaning and its detection, including the
detection of emotions such as annoyance, anger,
sadness, or boredom (Ang et al., 2002; Lee and
Narayanan, 2002; Liscombe et al., 2003), speaker
characteristics such as charisma (Rosenberg and
Hirschberg, 2005), personality features like ex-
troversion or agreeability (Mairesse et al., 2007;
Mairesse and Walker, 2008), speaker depression
or stress (Rude et al., 2004; Pennebaker and Lay,
2002; Cohn et al., 2004), and dating willingness
or liking (Madan et al., 2005; Pentland, 2005).
We chose to work on the domain of flirtation
in speed-dating. Our earlier work on this cor-
pus showed that it is possible to detect whether
speakers are perceived as flirtatious, awkward, or
friendly with reasonable accuracy (Jurafsky et al.,
2009). In this paper we extend that work to de-
tect whether speakers themselves intended to flirt,
explore the differences in these variables, and ex-
plore the ability and inability of humans to cor-
rectly perceive the flirtation cues.
While many of the features that we use to build
these detectors are drawn from the previous liter-
ature, we also explore new features. Conventional
methods for lexical feature extraction, for exam-
ple, generally consist of hand coded classes of
words related to concepts like sex or eating (Pen-
nebaker et al., 2007). The classes tend to per-
form well in their specific domains, but may not
be robust across domains, suggesting the need for
unsupervised domain-specific lexical feature ex-
traction. The naive answer to extracting domain-
</bodyText>
<page confidence="0.98285">
334
</page>
<note confidence="0.996616">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334–342,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998945571428571">
specific lexical features would just be to throw
counts for every word into a huge feature vector,
but the curse of dimensionality rules this method
out in small training set situations. We propose
a new solution to this problem, using an unsuper-
vised deep autoencoder to automatically compress
and extract complex high level lexical features.
</bodyText>
<sectionHeader confidence="0.998623" genericHeader="introduction">
2 Dataset
</sectionHeader>
<bodyText confidence="0.999965527777778">
Our experiments make use of the SpeedDate Cor-
pus collected by the third author, and described
in Jurafsky et al. (2009). The corpus is based
on three speed-dating sessions run at an Ameri-
can university in 2005, inspired by prior speed-
dating research (Madan et al., 2005). The grad-
uate student participants volunteered to be in the
study and were promised emails of persons with
whom they reported mutual liking. All partici-
pants wore audio recorders on a shoulder sash,
thus resulting in two audio recordings of the ap-
proximately 1100 4-minute dates. Each date was
conducted in an open setting where there was sub-
stantial background noise. This noisy audio was
thus hand-transcribed and turn start and end were
hand-aligned with the audio. In addition to the au-
dio, the corpus includes various attitude and de-
mographic questions answered by the participants.
Each speaker was also asked to report how of-
ten their date’s speech reflected different conver-
sational styles (awkward, flirtatious, funny, as-
sertive) on a scale of 1-10 (1=never, 10=con-
stantly): “How often did the other person behave
in the following ways on this ‘date’?”. In addition
they were also asked to rate their own intentions:
”How often did you behave in the following ways
on this ‘date’?” on a scale of 1-10.
In this study, we focus on the flirtation ratings,
examining how often each participant said they
were flirting, as well as how often each participant
was judged by the interlocutor as flirting.
Of the original 1100 dates only 991 total dates
are in the SpeedDate corpus due to various losses
during recording or processing. The current study
focuses on 946 of these, for which we have com-
plete audio, transcript, and survey information.
</bodyText>
<sectionHeader confidence="0.99876" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.999954380952381">
To understand how the perception of flirting dif-
fers from the intention of flirting, we trained bi-
nary classifiers to predict both perception and in-
tention. In each date, the speaker and the inter-
locutor both labeled the speaker’s behavioral traits
on a Likert scale from 1-10. To generate binary
responses we took the top ten percent of Likert
ratings in each task and labeled those as positive
examples. We similarly took the bottom ten per-
cent of Likert ratings and labeled those as negative
examples. We ran our binary classification exper-
iments to predict this output variable. Our experi-
ments were split by gender. For the female exper-
iment the speaker was female and the interlocu-
tor was male, while for the male experiment the
speaker was male and the interlocutor was female.
For each speaker side of each 4-minute conver-
sation, we extracted features from wavefiles and
transcripts, as described in the next section. We
then trained four separate binary classifiers (for
each gender for both perception and intention).
</bodyText>
<sectionHeader confidence="0.998267" genericHeader="method">
4 Feature Descriptions
</sectionHeader>
<bodyText confidence="0.999982818181818">
We used the features reported by Jurafsky et
al. (2009), which are briefly summarized here.
The features for a conversation side thus indicate
whether a speaker who talks a lot, laughs, is more
disfluent, has higher F0, etc., is more or less likely
to consider themselves flirtatious, or be considered
flirtatious by the interlocutor. We also computed
the same features for the alter interlocutor. Al-
ter features thus indicate the conversational behav-
ior of the speaker talking with an interlocutor they
considered to be flirtatious or not.
</bodyText>
<subsectionHeader confidence="0.996468">
4.1 Prosodic Features
</subsectionHeader>
<bodyText confidence="0.999878583333333">
F0 and RMS amplitude features were extracted us-
ing Praat scripts (Boersma and Weenink, 2005).
Since the start and end of each turn were time-
marked by hand, each feature was easily extracted
over a turn, and then averages and standard devia-
tions were taken over the turns in an entire conver-
sation side. Thus the feature F0 MIN for a conver-
sation side was computed by taking the F0 min of
each turn in that side (not counting zero values of
F0), and then averaging these values over all turns
in the side. F0 MIN SD is the standard deviation
across turns of this same measure.
</bodyText>
<subsectionHeader confidence="0.981077">
4.2 Dialogue and Disfluency Features
</subsectionHeader>
<bodyText confidence="0.9999444">
A number of discourse features were extracted,
following Jurafsky et al. (2009) and the dialogue
literature. The dialog acts shown in Table 2
were detected by hand-built regular expressions,
based on analyses of the dialogue acts in the
</bodyText>
<page confidence="0.998117">
335
</page>
<table confidence="0.753970555555556">
F0 MIN minimum (non-zero) F0 per turn, averaged
overturns
F0 MIN SD standard deviation from F0 min
F0 MAX maximum F0 per turn, averaged over turns
F0 MAX SD standard deviation from F0 max
F0 MEAN mean F0 per turn, averaged over turns
F0 MEAN SD standard deviation (across turns) from F0
mean
F0 SD standard deviation (within a turn) from F0
mean, averaged over turns
F0 SD SD standard deviation from the f0 sd
PITCH RANGE f0 max - f0 min per turn, averaged over
turns
PITCH RANGE SD standard deviation from mean pitch range
RMS MIN minimum amplitude per turn, averaged
overturns
RMS MIN SD standard deviation from RMS min
RMS MAX maximum amplitude per turn, averaged
overturns
RMS MAX SD standard deviation from RMS max
RMS MEAN mean amplitude per turn, averaged over
turns
RMS MEAN SD standard deviation from RMS mean
TURN DUR duration of turn in seconds, averaged over
turns
TIME total time for a speaker for a conversation
side, in seconds
</table>
<tableCaption confidence="0.593655333333333">
RATE OF number of words in turn divided by dura-
SPEECH tion of turn in seconds, averaged over turns
Table 1: Prosodic features from Jurafsky et al.
</tableCaption>
<bodyText confidence="0.953411083333333">
(2009) for each conversation side, extracted using
Praat from the hand-segmented turns of each side.
hand-labeled Switchboard corpus of dialog acts.
Collaborative completions, turns where a speaker
completes the utterance begun by the alter, were
detected by finding sentences for which the first
word of the speaker was extremely predictable
from the last two words of the previous speaker,
based on a trigram grammar trained on the Tree-
bank 3 Switchboard transcripts. Laughter, disflu-
encies, and overlap were all marked in the tran-
scripts by the transcribers.
</bodyText>
<subsectionHeader confidence="0.99968">
4.3 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9997725">
We drew our lexical features from the LIWC lex-
icons of Pennebaker et al. (2007), the standard
for social psychological analysis of lexical fea-
tures. We chose ten LIWC categories that have
proven useful in detecting personality-related fea-
tures (Mairesse et al., 2007): Anger, Assent, In-
gest, Insight, Negemotion, Sexual, Swear, I, We,
and You. We also added two new lexical features:
“past tense auxiliary”, a heuristic for automati-
cally detecting narrative or story-telling behavior,
and Metadate, for discussion about the speed-date
itself. The features are summarized in Table 3.
</bodyText>
<subsectionHeader confidence="0.996877">
4.4 Inducing New Lexical Features
</subsectionHeader>
<bodyText confidence="0.99998046">
In Jurafsky et al. (2009) we found the LIWC lex-
ical features less useful in detecting social mean-
ing than the dialogue and prosodic features, per-
haps because lexical cues to flirtation lie in differ-
ent classes of words than previously investigated.
We therefore investigated the induction of lexical
features from the speed-date corpus, using a prob-
abilisitic graphical model.
We began with a pilot investigation to see
whether lexical cues were likely to be useful; with
a small corpus, it is possible that lexical fea-
tures are simply too sparse to play a role given
the limited data. The pilot was based on us-
ing Naive Bayes with word existence features (bi-
nomial Naive Bayes). Naive Bayes assumes all
features are conditionally independent given the
class, and is known to perform well with small
amounts of data (Rish, 2001). Our Naive Bayes
pilot system performed above chance, suggesting
that lexical cues are indeed informative.
A simple approach to including lexical fea-
tures in our more general classification system
would be to include the word counts in a high di-
mensional feature vector with our other features.
This method, unfortunately, would suffer from
the well-known high dimensionality/small train-
ing set problem. We propose a method for build-
ing a much smaller number of features that would
nonetheless capture lexical information. Our ap-
proach is based on using autoencoders to con-
struct high level lower dimension features from the
words in a nonlinear manner.
A deep autoencoder is a hierarchichal graphical
model with multiple layers. Each layer consists of
a number of units. The input layer has the same
number of units as the output layer, where the out-
put layer is the model’s reconstruction of the input
layer. The number of units in the intermediate lay-
ers tends to get progressively smaller to produce a
compact representation.
We defined our autoencoder with visible units
modeling the probabilities of the 1000 most com-
mon words in the conversation for the speaker
and the probabilities of the 1000 most common
words for the interlocutor (after first removing
a stop list of the most common words). We
train a deep autoencoder with stochastic nonlin-
ear feature detectors and linear feature detectors
in the final layer. As shown in Figure 1, we used
a 2000-1000-500-250-30 autoencoder. Autoen-
</bodyText>
<page confidence="0.995475">
336
</page>
<bodyText confidence="0.800392909090909">
BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.)
APPRECIATIONS number of appreciations in side (Wow, That’s true, Oh, great)
QUESTIONS number of questions in side
NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)
COMPLETION (an approximation to) utterances that were ‘collaborative completions’
LAUGH number of instances of laughter in side
TURNS total number of turns in side
DISPREFERRED (approximation to) dispreferred responses, beginning with discourse marker well
UH/UM total number of filled pauses (uh or um) in conversation side
RESTART total number of disfluent restarts in conversation side
OVERLAP number of turns in side which the two speakers overlapped
</bodyText>
<tableCaption confidence="0.986118">
Table 2: Dialog act and disfluency features from Jurafsky et al. (2009).
</tableCaption>
<table confidence="0.5710075">
TOTAL WORDS total number of words
PAST TENSE uses of past tense auxiliaries was, were, had
</table>
<construct confidence="0.985847363636364">
METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research
YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know)
WE lets, let’s, our, ours, ourselves, us, we, we’d, we’ll, we’re, we’ve
I I’d, I’ll, I’m, I’ve, me, mine, my, myself (not counting I mean)
ASSENT yeah, okay, cool, yes, awesome, absolutely, agree
SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*
INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder
ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit
NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,
SEXUAL love*, passion*, virgin, sex, screw
INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish
</construct>
<bodyText confidence="0.994197954545454">
Table 3: Lexical features from Jurafsky et al. (2009). Each feature value is a total count of the words in
that class for each conversation side; asterisks indicate including suffixed forms (e.g., love, loves, loving).
All except the first three are from LIWC (Pennebaker et al., 2007) (modified slightly, e.g., by removing
you know and I mean). The last five classes include more words in addition to those shown.
coders tend to perform poorly if they are initialized
incorrectly, so we use the Restricted Boltzmann
Machine (RBM) pretraining procedure described
in Hinton and Salakhutdinov (2006) to initialize
the encoder. Each individual RBM is trained using
contrastive divergence as an update rule which has
been shown to produce reasonable results quickly
(Hinton et al., 2006). Finally, we use backpropa-
gation to fine tune the weights of our encoder by
minimizing the cross entropy error. To extract fea-
tures from each conversation, we sample the code
layer (30 unit layer in our encoder) with the visi-
ble units corresponding to the most common word
probabilities from that document, creating 30 new
features that we can use for classification. The
conditional distributions of the first layer features
can be given by the softmax of the activations for
each gender:
</bodyText>
<equation confidence="0.987412142857143">
exp(biasi + E hj * wij)
j
11 exp(biask + E
k∈K j
E
1 + exp(bias(j) +
i
</equation>
<bodyText confidence="0.935232222222222">
where K is the set of all the units representing the
same speaker as i 1, vi is the ith visible unit, hj is
the jth hidden unit, wij is the weight between visi-
ble unit i and hidden unit j, and biasm is the offset
of unit m. Intuitively, this means that the proba-
bility that a hidden unit is activated by the visible
layer is sigmoid of the weighted sum of all the vis-
ible units plus the unit’s bias term. Similarly, the
visible units are activated through a weighted sum
of the hidden units, but they undergo an additional
normalization (softmax) over all the other visible
units from the speaker to effectively model the
multinomial distribution from each speaker. Since
in a RBM hidden units are conditionally indepen-
dent given the visible units, and visible units are
1The visible unit i models word probabilities of either the
speaker or the interlocutor, so the softmax is done over the
distribution of words for the speaker that unit i is modeling.
</bodyText>
<equation confidence="0.984928857142857">
p(vi|h) =
(1)
vj * wkj)
1
p(hj|v) =
(2)
vi * wij)
</equation>
<page confidence="0.983879">
337
</page>
<bodyText confidence="0.9999087">
conditionally independent given hidden layer, the
above equations completely specify the first layer
of the model.
To account for the fact that each visible unit in
the first layer contained 1000 observations from
the underlying distribution we upweighted our fea-
tures by that factor. During pretraining the “train-
ing data” for the higher layers is the activation
probabilities of the hidden units of layer directly
below when driven by that layer’s input data. The
intermediate layers in the model are symmetric
where the activation probabilities for both the vis-
ible and hidden units are of the same form as
p(hj|v) in layer 1. To produce real valued features
in the code layer we used linear hidden units. In
addition to the likelihood portion of the objective
we penalized large weights by using l2 regulariza-
tion and penalize all weights by applying a small
constant weight cost that gets applied at every up-
date. After training to find a good initial point
for the autoencoder we unroll the weights and use
backpropogation to fine tune our autoencoder.
While interpreting high level nonlinear features
can be challenging, we did a pilot analysis of one
of the 30 features fixing a large (positive or neg-
ative) weight on the feature unit (code layer) and
sampling the output units.
The top weighted words for a positive weight
are: O did, O live, S did, S friends, S went,
O live, S lot, S wait, O two, and O wasn’t (S for
speaker and O for interlocutor). The top weighted
words for a negative weight are: S long, O school,
S school, S phd, O years, S years, O stanford,
S lot, O research, O interesting and O education.
At least for this one feature, a large positive value
seemed to indicate the prevalence of questions
(wait, did) or storytelling (
em live, wasn’t). A large negative weight indicates
the conversation focused on the mundane details
of grad student life.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="method">
5 Classification
</sectionHeader>
<bodyText confidence="0.984281302325582">
Before performing the classification task, we pre-
processed the data in two ways. First, we stan-
dardized all the variables to have zero mean and
unit variance. We did this to avoid imposing a
prior on any of the features based on their numer-
ical values. Consider a feature A with mean 100
and a feature B with mean .1 where A and B are
correlated with the output. Since the SVM prob-
lem minimizes the norm of the weight vector, there
Figure 1: Pretraining is a fully unsupervised pro-
cedure that trains an RBM at each layer. Once the
pretraining of one layer is complete, the top layer
units are used as input to the next layer. We then
fine-tune our weights using backprop. The 30 fea-
tures are extracted from the code layer.
is a bias to put weight on feature A because intu-
itively the weight on feature B would need to be
1000 times larger to carry the same effect. This
argument holds similarly for the reduction to unit
variance. Second, we removed features correlated
greater than .7. One goal of removing correlated
features was to remove as much colinearity as pos-
sible from the regression so that the regression
weights could be ranked for their importance in the
classification. In addition, we hoped to improve
classification because a large number of features
require more training examples (Ng, 2004). For
example for perception of female flirt we removed
the number of turns by the alter (O turns) and the
number of sentence from the ego (S sentences) be-
cause they were highly correlated with S turns.
To ensure comparisons (see Section 7) between
the interlocutors’ ratings and our classifier (and
because of our small dataset) we use k-fold cross
validation to learn our model and evaluate our
model. We train our binary model with the top
ten percent of ratings labeled as positive class ex-
amples and bottom ten percent of ratings as the
negative class examples. We used five-fold cross
validation in which the data is split into five equal
folds of size 40. We used four of the folds for
training and one for test. K-fold cross validation
does this in a round robin manner so every exam-
</bodyText>
<figure confidence="0.999566025316456">
Encoder
Pretraining Unrolling Fine-tuning
Dialogue:
F: ...
M: ...
F: ...
2000 RBM
2000
1000
1000
250
250
500
500
30
W1
W5
W4
W3
W2
RBM
RBM
RBM
Reconstruct
Dialogue:
F: ...
M: ...
Dialogue:
F: ...
M: ...
F: ...
WT1
2000 2000
2000
1000
1000
500
250
250
500
WT5
30 Code layer
W4
W1
W5
W3
W2
WT2
WT3
WT4
Decoder
WT S
2+ε9
Reconstruct
Dialogue:
F: ...
M: ...
Diaogue:
F: ...
M: ...
F: ...
2000
1000
1000
500
250
250
500
30
W1+ε1
WT5+ε6
W5+ε5
T
W 1+ε10
W4+ε4
W3+ε3
W2+ε2
WT4+ε7
WT3+ε8
</figure>
<page confidence="0.994573">
338
</page>
<bodyText confidence="0.999971857142857">
ple ends up in the test set. This yields a datasplit
of 160 training examples and 40 test examples. To
ensure that we were not learning something spe-
cific to our data split, we randomized our data or-
dering.
For classification we used a support vector ma-
chine (SVM). SVMs generally do not produce ex-
plicit feature weights for analysis because they are
a kernelized classifier. We solved the linear C-
SVM problem. Normally the problem is solved
in the dual form, but to facilitate feature analysis
we expand back to the primal form to retrieve w,
the weight vector. Our goal in the C-SVM is to
solve, in primal form,
</bodyText>
<equation confidence="0.996951">
m
minγ,w,b 1  ||w  ||2 + CE
i=1
s.t. y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ... , m
ξi ≥ 0, i = 1, ... , m (3)
</equation>
<bodyText confidence="0.999976">
where m is the number of training examples, x(i)
is the ith training examples, and y(i) is the ith class
(1 for the positive class, -1 for the negative class).
The ξi are the slack variables that allow this algo-
rithm to work for non linearly separable datasets.
A test example is classified by looking at the
sign of y(x) = wTx(test) + b. To explore mod-
els that captured interactions, but do not allow for
direct feature analysis we solved the C-SVM prob-
lem using a radial basis function (RBF) as a kernel
(Scholkopf et al., 1997). Our RBF kernel is based
on a Gaussian with unit variance.
</bodyText>
<equation confidence="0.943227">
K(x(i),x(9)) = exp(−||x(i) − x(9)||2
2σ ) (4)
</equation>
<bodyText confidence="0.993040315789474">
In this case predictions can be made by looking
at y(x(test)) = Em i=1 α(i)y(i)rbf(x(i), t(test)) + b,
where each α(i), for i = 1, ... , m is a member
of the set of dual variables that comes from trans-
forming the primal form into the dual form. The
SVM kernel trick allows us to explore higher di-
mensions while limiting the curse of dimensional-
ity that plagues small datasets like ours.
We evaluated both our linear C-SVM and our
radial basis function C-SVM using parameters
learned on the training sets by computing the ac-
curacy on the test set. Accuracy is the number of
correct examples / total number of test examples.
We found that the RBM classifier that handled in-
teraction terms outperformed linear methods like
logistic regression.
For feature weight extraction we aggregated the
feature weights calculated from each of the test
folds by taking the mean between them.2
</bodyText>
<sectionHeader confidence="0.999817" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999744">
We report in Table 4 the results for detecting flirt
intention (whether a speaker said they were flirt-
ing) as well as flirt perception (whether the listener
said the speaker was flirting).
</bodyText>
<table confidence="0.9911598">
Flirt Intention Flirt Perception
by M by F of M of F
RBM SVM 61.5% 70.0% 77.0% 59.5%
+autoencoder 69.0% 71.5% 79.5% 68.0%
features
</table>
<tableCaption confidence="0.997813">
Table 4: Accuracy of binary classification of each
</tableCaption>
<bodyText confidence="0.974748789473684">
conversation side, where chance is 50%. The first
row uses all the Jurafsky et al. (2009) features for
both the speaker and interlocutor. The second row
adds the new autoencoder features.
In our earlier study of flirt perception, we
achieved 71% accuracy for men and 60% for
women (Jurafsky et al., 2009). Our current num-
bers for flirt perception are much better for both
men (79.5%), and women (68.0%). The improve-
ment is due both to the new autoencoder features
and the RBF kernel that considers feature inter-
actions (feature interactions were not included in
the logistic regression classifiers of Jurafsky et al.
(2009)).
Our number for flirt intention are 69.0% for men
and 71.5% for women. Note that our accuracies
are better for detecting women’s intentions as well
as women’s perceptions (of men) than men’s in-
tentions and perceptions.
</bodyText>
<sectionHeader confidence="0.94863" genericHeader="method">
7 Feature Analysis
</sectionHeader>
<bodyText confidence="0.9089103">
We first considered the features that helped clas-
sification of flirt intention. Table 5 shows feature
weights for the features (features were normed so
weights are comparable), and is summarized in the
following paragraphs:
• Men who say they are flirting ask more ques-
tions, and use more you and we. They laugh more,
and use more sexual, anger, and negative emo-
tional words. Prosodically they speak faster, with
higher pitch, but quieter (lower intensity min).
</bodyText>
<footnote confidence="0.977733333333333">
2We could not use the zero median criteria used in Juraf-
sky et al. (2009) because C-SVMs under the l-2 metric pro-
vide no sparse weight guarantees.
</footnote>
<page confidence="0.7177455">
ξi
339
</page>
<table confidence="0.99991090625">
FEMALE FLIRT MALE FLIRT
O backchannel -0.0369 S you 0.0279
S appreciation -0.0327 S negemotion 0.0249
O appreciation -0.0281 S we 0.0236
O question 0.0265 S anger 0.0190
O avimin -0.0249 S sexual 0.0184
S turns -0.0247 O negemotion 0.0180
S backchannel -0.0245 O avpmax 0.0174
O you 0.0239 O swear 0.0172
S avtndur 0.0229 O laugh 0.0164
S avpmin -0.0227 O wordcount 0.0151
O rate 0.0212 S laugh 0.0144
S laugh 0.0204 S rate 0.0143
S wordcount 0.0192 S well 0.0131
S well 0.0192 S question 0.0131
O negemotion 0.019 O sexual 0.0128
S repair q 0.0188 S completion 0.0128
O sexual 0.0176 S avpmax 0.011
O overlap -0.0176 O completion 0.010
O sdpmean 0.0171 O sdimin 0.010
O avimax -0.0151 O metatalk -0.012
S avpmean -0.015 S sdpsd -0.015
S question -0.0146 S avimin -0.015
O sdimin 0.0136 S backchannel -0.022
S avpmax 0.0131
S we -0.013
S I 0.0117
S assent 0.0114
S metatalk -0.0107
S sexual 0.0105
S avimin -0.0104
O uh -0.0102
</table>
<tableCaption confidence="0.981329">
Table 5: Feature weights (mean weights of the ran-
</tableCaption>
<bodyText confidence="0.946737230769231">
domized runs) for the predictors with |weight |&gt;
0.01 for the male and female classifiers. An S pre-
fix indicates features of the speaker (the candidate
flirter) while an O prefix indicates features of the
other. Weights for autoencoder features were also
significant but are omitted for compactness.
Features of the alter (the woman) that helped
our system detect men who say they are flirting
include the woman’s laughing, sexual words or
swear words, talking more, and having a higher
f0 (max).
• Women who say they are flirting have a much
expanded pitch range (lower pitch min, higher
pitch max), laugh more, use more I and well, use
repair questions but not other kinds of questions,
use more sexual terms, use far less appreciations
and backchannels, and use fewer, longer turns,
with more words in general. Features of the alter
(the man) that helped our system detect women
who say they are flirting include the male use of
you, questions, and faster and quieter speech.
We also summarize here the features for the per-
ception classification task; predicting which peo-
ple will be labeled by their dates as flirting. Here
the task is the same as for Jurafsky et al. (2009)
and the values are similar.
</bodyText>
<listItem confidence="0.754962">
• Men who are labeled by their female date as
</listItem>
<bodyText confidence="0.892487259259259">
flirting present many of the same linguistic behav-
iors as when they express their intention to flirt.
Some of the largest differences are that men are
perceived to flirt when they use less appreciations
and overlap less, while these features were not sig-
nificant for men who said they were flirting. We
also found that fast speech and more questions are
more important features for flirtation perception
than intention.
• Women who are labeled by their male date
as flirting also present much of the same linguis-
tic behavior as women who intend to flirt. Laugh-
ter, repair questions, and taking fewer, longer turns
were not predictors of women labeled as flirting,
although these were strong predictors of women
intending to flirt.
Both genders convey intended flirtation by
laughing more, speaking faster, and using higher
pitch. However, we do find gender differences;
men ask more questions when they say they are
flirting, women ask fewer, although they do use
more repair questions, which men do not. Women
use more “I” and less “we”; men use more “we”
and “you”. Men labeled as flirting are softer, but
women labeled as flirting are not. Women flirting
use much fewer appreciations; appreciations were
not a significant factor in men flirting.
</bodyText>
<sectionHeader confidence="0.906198" genericHeader="method">
8 Human Performance on this task
</sectionHeader>
<bodyText confidence="0.997164">
To evaluate the performance of our classifiers we
compare against human labeled data.
We used the same test set as for our machine
classifier; recall that this was created by taking the
top ten percent of Likert ratings of the speaker’s
intention ratings by gender and called those posi-
tive for flirtation intention. We constructed nega-
tive examples by taking the bottom ten percent of
intention Likert ratings. We called the interlocu-
tor correct on the positive examples if the inter-
locutor’s rating was greater than 5. Symmetrically
for the negative examples, we said the interlocutor
was correct if their rating was less than or equal
to 5. Note that this metric is biased somewhat to-
ward the humans and against our systems, because
we do not penalize for intermediate values, while
the system is trained to make binary predictions
only on extremes. The results of the human per-
ceivers on classifying flirtation intent are shown in
Table 6.
</bodyText>
<page confidence="0.991689">
340
</page>
<table confidence="0.897782666666667">
Male speaker Female speaker
(Female perceiver) (Male perceiver)
62.2% 56.2%
</table>
<tableCaption confidence="0.7947825">
Table 6: Accuracy of human listeners at labeling
speakers as flirting or not.
</tableCaption>
<bodyText confidence="0.973583941176471">
We were quite surprised by the poor quality of
the human results. Our system outperforms both
men’s performance in detecting women flirters
(system 71.5% versus human 56.2%) and also
women’s performance in detecting male flirters
(system 69.0% versus human 62.2%).
Why are humans worse than machines at detect-
ing flirtation? We found a key insight by examin-
ing how the participants in a date label themselves
and each other. Table 7 shows the 1-10 Likert val-
ues for the two participants in one of the dates,
between Male 101 and Female 127. The two par-
ticipants clearly had very different perspectives on
the date. More important, however, we see that
each participant labels their own flirting (almost)
identically with their partner’s flirting.
I am flirting Other is flirting
</bodyText>
<table confidence="0.670192">
Male 101 says: 8 7
Female 127 says: 1 1
</table>
<tableCaption confidence="0.9237175">
Table 7: Likert scores for the date between Female
127 and Male 101.
</tableCaption>
<bodyText confidence="0.932021772727273">
We therefore asked whether speakers in general
tend to assign similar values to their own flirting
and their partner’s flirting. The Pearson correla-
tion coefficient between these two variables (my
perception of my own flirting, and my perception
of other’s flirting) is .73. By contrast, the poor per-
formance of subjects at detecting flirting in their
partners is coherent with the lower (.15) correla-
tion coefficient between those two variables (my
perception of the other’s flirting, and the other’s
perception of their own flirting). This discrepancy
is summarized in boldface in Table 8.
Since the speed-date data was also labeled for
three other variables, we then asked the same
question about these variables. As Table 8 shows,
for all four styles, speakers’ perception of others
is strongly correlated with the speakers’ percep-
tion of themselves, far more so than with what the
others actually think they are doing.3
3This was true no matter how the correlations were run,
whether with raw Likert values, with ego-centered (trans-
formed) values and with self ego-centered but other raw.
</bodyText>
<table confidence="0.9976615">
Variable Self-perceive-Other Self-perceive-Other &amp;
&amp; Self-perceive-Self Other-perceive-Other
Flirting .73 .15
Friendly .77 .05
Awkward .58 .07
Assertive .58 .09
</table>
<tableCaption confidence="0.9317965">
Table 8: Correlations between speaker intentions
and perception for all four styles.
</tableCaption>
<bodyText confidence="0.9998415">
Note that although perception of the other does
not correlate highly with the other’s intent for any
of the styles, the correlations are somewhat bet-
ter (.15) for flirting, perhaps because in the speed-
date setting speakers are focusing more on detect-
ing this behavior (Higgins and Bargh, 1987). It is
also possible that for styles with positive valence
(friendliness and flirting) speakers see more simi-
larity between the self and the other than for nega-
tive styles (awkward and assertive) (Krah´e, 1983).
Why should this strong bias exist to link self-
flirting with perception of the other? One pos-
sibility is that speakers are just not very good at
capturing the intentions of others in four minutes.
Speakers instead base their judgments on their
own behavior or intentions, perhaps because of a
bias to maintain consistency in attitudes and rela-
tions (Festinger, 1957; Taylor, 1970) or to assume
there is reciprocation in interpersonal perceptions
(Kenny, 1998).
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.991536842105263">
We have presented a new system that is able to
predict flirtation intention better than humans can,
despite humans having access to vastly richer in-
formation (visual features, gesture, etc.). This sys-
tem facilitates the analysis of human perception
and human interaction and provides a framework
for understanding why humans perform so poorly
on intention prediction.
At the heart of our system is a core set of
prosodic, dialogue, and lexical features that al-
low for accurate prediction of both flirtation inten-
tion and flirtation perception. Since previous word
lists don’t capture sufficient lexical information,
we used an autoencoder to automatically capture
new lexical cues. The autoencoder shows potential
for being a promising feature extraction method
for social tasks where cues are domain specific.
Acknowledgments: Thanks to the anonymous review-
ers and to a Google Research Award for partial funding.
</bodyText>
<page confidence="0.995355">
341
</page>
<bodyText confidence="0.9328864">
Franc¸ois Mairesse, Marilyn Walker, Matthias Mehl,
and Roger Moore. 2007. Using linguistic cues for
the automatic recognition of personality in conver-
sation and text. Journal ofArtificial Intelligence Re-
search (JAIR), 30:457–500.
</bodyText>
<sectionHeader confidence="0.935182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998710901234568">
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-Based Automatic De-
tection of Annoyance and Frustration in Human-
Computer Dialog. In INTERSPEECH-02.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer (version 4.3.14). [Computer
program]. Retrieved May 26, 2005, from http://
www.praat.org/.
M. A. Cohn, M. R. Mehl, and J. W. Pennebaker.
2004. Linguistic markers of psychological change
surrounding September 11, 2001. Psychological
Science, 15:687–693.
Leon Festinger. 1957. A Theory of Cognitive Disso-
nance. Row, Peterson, Evanston, IL.
E. Tory Higgins and John A. Bargh. 1987. Social cog-
nition and social perception. Annual Review of Psy-
chology, 38:369–425.
G. E. Hinton and R. R Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504–507.
G. E. Hinton, S. Osindero, and Y. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527–1554.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In NAACL
HLT 2009, Boulder, CO.
David Kenny. 1998. Interpersonal Perception: A So-
cial Relations Analysis. Guilford Press, New York,
NY.
B. Krah´e. 1983. Self-serving biases in perceived simi-
larity and causal attributions of other people’s per-
formance. Social Psychology Quarterly, 46:318–
329.
C. M. Lee and Shrikanth S. Narayanan. 2002. Com-
bining acoustic and language information for emo-
tion recognition. In ICSLP-02, pages 873–876,
Denver, CO.
Jackson Liscombe, Jennifer Venditti, and Julia
Hirschberg. 2003. Classifying Subject Ratings
of Emotional Speech Using Acoustic Features. In
INTERSPEECH-03.
Anmol Madan, Ron Caneel, and Alex Pentland. 2005.
Voices of attraction. Presented at Augmented Cog-
nition, HCI 2005, Las Vegas.
Franc¸ois Mairesse and Marilyn Walker. 2008. Train-
able generation of big-five personality styles through
data-driven parameter estimation. In ACL-08,
Columbus.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2
regularization, and rotational invariance. In ICML
2004.
J. W. Pennebaker and T. C. Lay. 2002. Language use
and personality during crises: Analyses of Mayor
Rudolph Giuliani’s press conferences. Journal of
Research in Personality, 36:271–282.
J. W. Pennebaker, R.E. Booth, and M.E. Francis. 2007.
Linguistic inquiry and word count: LIWC2007 op-
erator’s manual. Technical report, University of
Texas.
Alex Pentland. 2005. Socially aware computation and
communication. Computer, pages 63–70.
Irina Rish. 2001. An empirical study of the naive
bayes classifier. In IJCAI 2001 Workshop on Em-
pirical Methods in Artificial Intelligence.
Andrew Rosenberg and Julia Hirschberg. 2005.
Acoustic/prosodic and lexical correlates of charis-
matic speech. In EUROSPEECH-05, pages 513–
516, Lisbon, Portugal.
S. S. Rude, E. M. Gortner, and J. W. Pennebaker.
2004. Language use of depressed and depression-
vulnerable college students. Cognition and Emo-
tion, 18:1121–1133.
B. Scholkopf, K.K. Sung, CJC Burges, F. Girosi,
P. Niyogi, T. Poggio, and V. Vapnik. 1997. Com-
paring support vector machines with Gaussian ker-
nels to radialbasis function classifiers. IEEE Trans-
actions on Signal Processing, 45(11):2758–2765.
Howard Taylor. 1970. Chapter 2. In Balance in
Small Groups. Von Nostrand Reinhold Company,
New York, NY.
</reference>
<page confidence="0.998262">
342
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960084">
<title confidence="0.996769">It’s Not You, it’s Me: Detecting Flirting and its Misperception in Speed-Dates</title>
<author confidence="0.999857">Rajesh Ranganath Dan Jurafsky Dan McFarland</author>
<affiliation confidence="0.999771">Computer Science Department Linguistics Department School of Education Stanford University Stanford University Stanford University</affiliation>
<email confidence="0.99929">rajeshr@cs.stanford.edujurafsky@stanford.edudmcfarla@stanford.edu</email>
<abstract confidence="0.998813678571429">Automatically detecting human social intentions from spoken conversation is an important task for dialogue understanding. Since the social intentions of the speaker may differ from what is perceived by the hearer, systems that analyze human conversations need to be able to extract both the perceived and the intended social meaning. We investigate this difference between intention and perception by using a spoken corpus of speed-dates in which both the speaker and the listener rated the speaker on flirtatiousness. Our flirtationdetection system uses prosodic, dialogue, and lexical features to detect a speaker’s intent to flirt with up to 71.5% accuracy, significantly outperforming the baseline, but also outperforming the human interlocuters. Our system addresses lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vectors into 30 compressed features. Our analysis shows that humans are very poor perceivers of intended flirtatiousness, instead often projecting their own intended behavior onto their interlocutors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-Based Automatic Detection of Annoyance and Frustration in HumanComputer Dialog.</title>
<date>2002</date>
<booktitle>In INTERSPEECH-02.</booktitle>
<contexts>
<context position="2485" citStr="Ang et al., 2002" startWordPosition="364" endWordPosition="367">ifference between perception and intention, in this paper we describe machine learning models that can detect both the social meaning intended by the speaker and the social meaning perceived by the hearer. Automated systems that detect and model these differences can lead both to richer socially aware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly wi</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stolcke. 2002. Prosody-Based Automatic Detection of Annoyance and Frustration in HumanComputer Dialog. In INTERSPEECH-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>David Weenink</author>
</authors>
<title>Praat: doing phonetics by computer (version 4.3.14). [Computer program]. Retrieved</title>
<date>2005</date>
<note>from http:// www.praat.org/.</note>
<contexts>
<context position="7839" citStr="Boersma and Weenink, 2005" startWordPosition="1239" endWordPosition="1242"> by Jurafsky et al. (2009), which are briefly summarized here. The features for a conversation side thus indicate whether a speaker who talks a lot, laughs, is more disfluent, has higher F0, etc., is more or less likely to consider themselves flirtatious, or be considered flirtatious by the interlocutor. We also computed the same features for the alter interlocutor. Alter features thus indicate the conversational behavior of the speaker talking with an interlocutor they considered to be flirtatious or not. 4.1 Prosodic Features F0 and RMS amplitude features were extracted using Praat scripts (Boersma and Weenink, 2005). Since the start and end of each turn were timemarked by hand, each feature was easily extracted over a turn, and then averages and standard deviations were taken over the turns in an entire conversation side. Thus the feature F0 MIN for a conversation side was computed by taking the F0 min of each turn in that side (not counting zero values of F0), and then averaging these values over all turns in the side. F0 MIN SD is the standard deviation across turns of this same measure. 4.2 Dialogue and Disfluency Features A number of discourse features were extracted, following Jurafsky et al. (2009)</context>
</contexts>
<marker>Boersma, Weenink, 2005</marker>
<rawString>Paul Boersma and David Weenink. 2005. Praat: doing phonetics by computer (version 4.3.14). [Computer program]. Retrieved May 26, 2005, from http:// www.praat.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Cohn</author>
<author>M R Mehl</author>
<author>J W Pennebaker</author>
</authors>
<date>2004</date>
<booktitle>Linguistic markers of psychological change surrounding September 11,</booktitle>
<pages>15--687</pages>
<publisher>Psychological Science,</publisher>
<contexts>
<context position="2811" citStr="Cohn et al., 2004" startWordPosition="412" endWordPosition="415">ional understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While many of the features that we use to build thes</context>
</contexts>
<marker>Cohn, Mehl, Pennebaker, 2004</marker>
<rawString>M. A. Cohn, M. R. Mehl, and J. W. Pennebaker. 2004. Linguistic markers of psychological change surrounding September 11, 2001. Psychological Science, 15:687–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Festinger</author>
</authors>
<title>A Theory of Cognitive Dissonance.</title>
<date>1957</date>
<location>Row, Peterson, Evanston, IL.</location>
<contexts>
<context position="33596" citStr="Festinger, 1957" startWordPosition="5623" endWordPosition="5624">his behavior (Higgins and Bargh, 1987). It is also possible that for styles with positive valence (friendliness and flirting) speakers see more similarity between the self and the other than for negative styles (awkward and assertive) (Krah´e, 1983). Why should this strong bias exist to link selfflirting with perception of the other? One possibility is that speakers are just not very good at capturing the intentions of others in four minutes. Speakers instead base their judgments on their own behavior or intentions, perhaps because of a bias to maintain consistency in attitudes and relations (Festinger, 1957; Taylor, 1970) or to assume there is reciprocation in interpersonal perceptions (Kenny, 1998). 9 Conclusion We have presented a new system that is able to predict flirtation intention better than humans can, despite humans having access to vastly richer information (visual features, gesture, etc.). This system facilitates the analysis of human perception and human interaction and provides a framework for understanding why humans perform so poorly on intention prediction. At the heart of our system is a core set of prosodic, dialogue, and lexical features that allow for accurate prediction of </context>
</contexts>
<marker>Festinger, 1957</marker>
<rawString>Leon Festinger. 1957. A Theory of Cognitive Dissonance. Row, Peterson, Evanston, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tory Higgins</author>
<author>John A Bargh</author>
</authors>
<title>Social cognition and social perception. Annual Review of Psychology,</title>
<date>1987</date>
<pages>38--369</pages>
<contexts>
<context position="33019" citStr="Higgins and Bargh, 1987" startWordPosition="5526" endWordPosition="5529">es, with ego-centered (transformed) values and with self ego-centered but other raw. Variable Self-perceive-Other Self-perceive-Other &amp; &amp; Self-perceive-Self Other-perceive-Other Flirting .73 .15 Friendly .77 .05 Awkward .58 .07 Assertive .58 .09 Table 8: Correlations between speaker intentions and perception for all four styles. Note that although perception of the other does not correlate highly with the other’s intent for any of the styles, the correlations are somewhat better (.15) for flirting, perhaps because in the speeddate setting speakers are focusing more on detecting this behavior (Higgins and Bargh, 1987). It is also possible that for styles with positive valence (friendliness and flirting) speakers see more similarity between the self and the other than for negative styles (awkward and assertive) (Krah´e, 1983). Why should this strong bias exist to link selfflirting with perception of the other? One possibility is that speakers are just not very good at capturing the intentions of others in four minutes. Speakers instead base their judgments on their own behavior or intentions, perhaps because of a bias to maintain consistency in attitudes and relations (Festinger, 1957; Taylor, 1970) or to a</context>
</contexts>
<marker>Higgins, Bargh, 1987</marker>
<rawString>E. Tory Higgins and John A. Bargh. 1987. Social cognition and social perception. Annual Review of Psychology, 38:369–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<contexts>
<context position="15555" citStr="Hinton and Salakhutdinov (2006)" startWordPosition="2485" endWordPosition="2488">ne, beer, restaurant, lunch, dish Table 3: Lexical features from Jurafsky et al. (2009). Each feature value is a total count of the words in that class for each conversation side; asterisks indicate including suffixed forms (e.g., love, loves, loving). All except the first three are from LIWC (Pennebaker et al., 2007) (modified slightly, e.g., by removing you know and I mean). The last five classes include more words in addition to those shown. coders tend to perform poorly if they are initialized incorrectly, so we use the Restricted Boltzmann Machine (RBM) pretraining procedure described in Hinton and Salakhutdinov (2006) to initialize the encoder. Each individual RBM is trained using contrastive divergence as an update rule which has been shown to produce reasonable results quickly (Hinton et al., 2006). Finally, we use backpropagation to fine tune the weights of our encoder by minimizing the cross entropy error. To extract features from each conversation, we sample the code layer (30 unit layer in our encoder) with the visible units corresponding to the most common word probabilities from that document, creating 30 new features that we can use for classification. The conditional distributions of the first la</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>G. E. Hinton and R. R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>S Osindero</author>
<author>Y Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<journal>Neural Computation,</journal>
<pages>18--1527</pages>
<contexts>
<context position="15741" citStr="Hinton et al., 2006" startWordPosition="2514" endWordPosition="2517">e including suffixed forms (e.g., love, loves, loving). All except the first three are from LIWC (Pennebaker et al., 2007) (modified slightly, e.g., by removing you know and I mean). The last five classes include more words in addition to those shown. coders tend to perform poorly if they are initialized incorrectly, so we use the Restricted Boltzmann Machine (RBM) pretraining procedure described in Hinton and Salakhutdinov (2006) to initialize the encoder. Each individual RBM is trained using contrastive divergence as an update rule which has been shown to produce reasonable results quickly (Hinton et al., 2006). Finally, we use backpropagation to fine tune the weights of our encoder by minimizing the cross entropy error. To extract features from each conversation, we sample the code layer (30 unit layer in our encoder) with the visible units corresponding to the most common word probabilities from that document, creating 30 new features that we can use for classification. The conditional distributions of the first layer features can be given by the softmax of the activations for each gender: exp(biasi + E hj * wij) j 11 exp(biask + E k∈K j E 1 + exp(bias(j) + i where K is the set of all the units re</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>G. E. Hinton, S. Osindero, and Y. Teh. 2006. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Rajesh Ranganath</author>
<author>Dan McFarland</author>
</authors>
<title>Extracting social meaning: Identifying interactional style in spoken conversation.</title>
<date>2009</date>
<booktitle>In NAACL HLT 2009,</booktitle>
<location>Boulder, CO.</location>
<contexts>
<context position="3131" citStr="Jurafsky et al., 2009" startWordPosition="464" endWordPosition="467">2; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While many of the features that we use to build these detectors are drawn from the previous literature, we also explore new features. Conventional methods for lexical feature extraction, for example, generally consist of hand coded classes of words related to concepts like sex or eating (Pennebaker et al., 2007). The classes tend to perform well in their specific domain</context>
<context position="4519" citStr="Jurafsky et al. (2009)" startWordPosition="690" endWordPosition="693">ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334–342, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP specific lexical features would just be to throw counts for every word into a huge feature vector, but the curse of dimensionality rules this method out in small training set situations. We propose a new solution to this problem, using an unsupervised deep autoencoder to automatically compress and extract complex high level lexical features. 2 Dataset Our experiments make use of the SpeedDate Corpus collected by the third author, and described in Jurafsky et al. (2009). The corpus is based on three speed-dating sessions run at an American university in 2005, inspired by prior speeddating research (Madan et al., 2005). The graduate student participants volunteered to be in the study and were promised emails of persons with whom they reported mutual liking. All participants wore audio recorders on a shoulder sash, thus resulting in two audio recordings of the approximately 1100 4-minute dates. Each date was conducted in an open setting where there was substantial background noise. This noisy audio was thus hand-transcribed and turn start and end were hand-ali</context>
<context position="7239" citStr="Jurafsky et al. (2009)" startWordPosition="1144" endWordPosition="1147">s negative examples. We ran our binary classification experiments to predict this output variable. Our experiments were split by gender. For the female experiment the speaker was female and the interlocutor was male, while for the male experiment the speaker was male and the interlocutor was female. For each speaker side of each 4-minute conversation, we extracted features from wavefiles and transcripts, as described in the next section. We then trained four separate binary classifiers (for each gender for both perception and intention). 4 Feature Descriptions We used the features reported by Jurafsky et al. (2009), which are briefly summarized here. The features for a conversation side thus indicate whether a speaker who talks a lot, laughs, is more disfluent, has higher F0, etc., is more or less likely to consider themselves flirtatious, or be considered flirtatious by the interlocutor. We also computed the same features for the alter interlocutor. Alter features thus indicate the conversational behavior of the speaker talking with an interlocutor they considered to be flirtatious or not. 4.1 Prosodic Features F0 and RMS amplitude features were extracted using Praat scripts (Boersma and Weenink, 2005)</context>
<context position="9697" citStr="Jurafsky et al. (2009)" startWordPosition="1570" endWordPosition="1573">RANGE SD standard deviation from mean pitch range RMS MIN minimum amplitude per turn, averaged overturns RMS MIN SD standard deviation from RMS min RMS MAX maximum amplitude per turn, averaged overturns RMS MAX SD standard deviation from RMS max RMS MEAN mean amplitude per turn, averaged over turns RMS MEAN SD standard deviation from RMS mean TURN DUR duration of turn in seconds, averaged over turns TIME total time for a speaker for a conversation side, in seconds RATE OF number of words in turn divided by duraSPEECH tion of turn in seconds, averaged over turns Table 1: Prosodic features from Jurafsky et al. (2009) for each conversation side, extracted using Praat from the hand-segmented turns of each side. hand-labeled Switchboard corpus of dialog acts. Collaborative completions, turns where a speaker completes the utterance begun by the alter, were detected by finding sentences for which the first word of the speaker was extremely predictable from the last two words of the previous speaker, based on a trigram grammar trained on the Treebank 3 Switchboard transcripts. Laughter, disfluencies, and overlap were all marked in the transcripts by the transcribers. 4.3 Lexical Features We drew our lexical fea</context>
<context position="14018" citStr="Jurafsky et al. (2009)" startWordPosition="2255" endWordPosition="2258">number of questions in side NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me) COMPLETION (an approximation to) utterances that were ‘collaborative completions’ LAUGH number of instances of laughter in side TURNS total number of turns in side DISPREFERRED (approximation to) dispreferred responses, beginning with discourse marker well UH/UM total number of filled pauses (uh or um) in conversation side RESTART total number of disfluent restarts in conversation side OVERLAP number of turns in side which the two speakers overlapped Table 2: Dialog act and disfluency features from Jurafsky et al. (2009). TOTAL WORDS total number of words PAST TENSE uses of past tense auxiliaries was, were, had METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research YOU you, you’d, you’ll, your, you’re, yours, you’ve (not counting you know) WE lets, let’s, our, ours, ourselves, us, we, we’d, we’ll, we’re, we’ve I I’d, I’ll, I’m, I’ve, me, mine, my, myself (not counting I mean) ASSENT yeah, okay, cool, yes, awesome, absolutely, agree SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck* INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder</context>
<context position="24494" citStr="Jurafsky et al. (2009)" startWordPosition="4099" endWordPosition="4102">ic regression. For feature weight extraction we aggregated the feature weights calculated from each of the test folds by taking the mean between them.2 6 Results We report in Table 4 the results for detecting flirt intention (whether a speaker said they were flirting) as well as flirt perception (whether the listener said the speaker was flirting). Flirt Intention Flirt Perception by M by F of M of F RBM SVM 61.5% 70.0% 77.0% 59.5% +autoencoder 69.0% 71.5% 79.5% 68.0% features Table 4: Accuracy of binary classification of each conversation side, where chance is 50%. The first row uses all the Jurafsky et al. (2009) features for both the speaker and interlocutor. The second row adds the new autoencoder features. In our earlier study of flirt perception, we achieved 71% accuracy for men and 60% for women (Jurafsky et al., 2009). Our current numbers for flirt perception are much better for both men (79.5%), and women (68.0%). The improvement is due both to the new autoencoder features and the RBF kernel that considers feature interactions (feature interactions were not included in the logistic regression classifiers of Jurafsky et al. (2009)). Our number for flirt intention are 69.0% for men and 71.5% for </context>
<context position="25803" citStr="Jurafsky et al. (2009)" startWordPosition="4316" endWordPosition="4320">women’s perceptions (of men) than men’s intentions and perceptions. 7 Feature Analysis We first considered the features that helped classification of flirt intention. Table 5 shows feature weights for the features (features were normed so weights are comparable), and is summarized in the following paragraphs: • Men who say they are flirting ask more questions, and use more you and we. They laugh more, and use more sexual, anger, and negative emotional words. Prosodically they speak faster, with higher pitch, but quieter (lower intensity min). 2We could not use the zero median criteria used in Jurafsky et al. (2009) because C-SVMs under the l-2 metric provide no sparse weight guarantees. ξi 339 FEMALE FLIRT MALE FLIRT O backchannel -0.0369 S you 0.0279 S appreciation -0.0327 S negemotion 0.0249 O appreciation -0.0281 S we 0.0236 O question 0.0265 S anger 0.0190 O avimin -0.0249 S sexual 0.0184 S turns -0.0247 O negemotion 0.0180 S backchannel -0.0245 O avpmax 0.0174 O you 0.0239 O swear 0.0172 S avtndur 0.0229 O laugh 0.0164 S avpmin -0.0227 O wordcount 0.0151 O rate 0.0212 S laugh 0.0144 S laugh 0.0204 S rate 0.0143 S wordcount 0.0192 S well 0.0131 S well 0.0192 S question 0.0131 O negemotion 0.019 O se</context>
<context position="28039" citStr="Jurafsky et al. (2009)" startWordPosition="4709" endWordPosition="4712">h range (lower pitch min, higher pitch max), laugh more, use more I and well, use repair questions but not other kinds of questions, use more sexual terms, use far less appreciations and backchannels, and use fewer, longer turns, with more words in general. Features of the alter (the man) that helped our system detect women who say they are flirting include the male use of you, questions, and faster and quieter speech. We also summarize here the features for the perception classification task; predicting which people will be labeled by their dates as flirting. Here the task is the same as for Jurafsky et al. (2009) and the values are similar. • Men who are labeled by their female date as flirting present many of the same linguistic behaviors as when they express their intention to flirt. Some of the largest differences are that men are perceived to flirt when they use less appreciations and overlap less, while these features were not significant for men who said they were flirting. We also found that fast speech and more questions are more important features for flirtation perception than intention. • Women who are labeled by their male date as flirting also present much of the same linguistic behavior </context>
</contexts>
<marker>Jurafsky, Ranganath, McFarland, 2009</marker>
<rawString>Dan Jurafsky, Rajesh Ranganath, and Dan McFarland. 2009. Extracting social meaning: Identifying interactional style in spoken conversation. In NAACL HLT 2009, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kenny</author>
</authors>
<title>Interpersonal Perception: A Social Relations Analysis.</title>
<date>1998</date>
<publisher>Guilford Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="33690" citStr="Kenny, 1998" startWordPosition="5636" endWordPosition="5637"> (friendliness and flirting) speakers see more similarity between the self and the other than for negative styles (awkward and assertive) (Krah´e, 1983). Why should this strong bias exist to link selfflirting with perception of the other? One possibility is that speakers are just not very good at capturing the intentions of others in four minutes. Speakers instead base their judgments on their own behavior or intentions, perhaps because of a bias to maintain consistency in attitudes and relations (Festinger, 1957; Taylor, 1970) or to assume there is reciprocation in interpersonal perceptions (Kenny, 1998). 9 Conclusion We have presented a new system that is able to predict flirtation intention better than humans can, despite humans having access to vastly richer information (visual features, gesture, etc.). This system facilitates the analysis of human perception and human interaction and provides a framework for understanding why humans perform so poorly on intention prediction. At the heart of our system is a core set of prosodic, dialogue, and lexical features that allow for accurate prediction of both flirtation intention and flirtation perception. Since previous word lists don’t capture s</context>
</contexts>
<marker>Kenny, 1998</marker>
<rawString>David Kenny. 1998. Interpersonal Perception: A Social Relations Analysis. Guilford Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krah´e</author>
</authors>
<title>Self-serving biases in perceived similarity and causal attributions of other people’s performance. Social Psychology Quarterly,</title>
<date>1983</date>
<pages>46--318</pages>
<marker>Krah´e, 1983</marker>
<rawString>B. Krah´e. 1983. Self-serving biases in perceived similarity and causal attributions of other people’s performance. Social Psychology Quarterly, 46:318– 329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Lee</author>
<author>Shrikanth S Narayanan</author>
</authors>
<title>Combining acoustic and language information for emotion recognition. In</title>
<date>2002</date>
<booktitle>ICSLP-02,</booktitle>
<pages>873--876</pages>
<location>Denver, CO.</location>
<contexts>
<context position="2510" citStr="Lee and Narayanan, 2002" startWordPosition="368" endWordPosition="371">perception and intention, in this paper we describe machine learning models that can detect both the social meaning intended by the speaker and the social meaning perceived by the hearer. Automated systems that detect and model these differences can lead both to richer socially aware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (J</context>
</contexts>
<marker>Lee, Narayanan, 2002</marker>
<rawString>C. M. Lee and Shrikanth S. Narayanan. 2002. Combining acoustic and language information for emotion recognition. In ICSLP-02, pages 873–876, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackson Liscombe</author>
<author>Jennifer Venditti</author>
<author>Julia Hirschberg</author>
</authors>
<title>Classifying Subject Ratings of Emotional Speech Using Acoustic Features.</title>
<date>2003</date>
<booktitle>In INTERSPEECH-03.</booktitle>
<contexts>
<context position="2534" citStr="Liscombe et al., 2003" startWordPosition="372" endWordPosition="375"> in this paper we describe machine learning models that can detect both the social meaning intended by the speaker and the social meaning perceived by the hearer. Automated systems that detect and model these differences can lead both to richer socially aware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). I</context>
</contexts>
<marker>Liscombe, Venditti, Hirschberg, 2003</marker>
<rawString>Jackson Liscombe, Jennifer Venditti, and Julia Hirschberg. 2003. Classifying Subject Ratings of Emotional Speech Using Acoustic Features. In INTERSPEECH-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anmol Madan</author>
<author>Ron Caneel</author>
<author>Alex Pentland</author>
</authors>
<title>Voices of attraction. Presented at Augmented Cognition,</title>
<date>2005</date>
<location>HCI</location>
<contexts>
<context position="2865" citStr="Madan et al., 2005" startWordPosition="421" endWordPosition="424"> conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While many of the features that we use to build these detectors are drawn from the previous literature, we</context>
<context position="4670" citStr="Madan et al., 2005" startWordPosition="716" endWordPosition="719">fic lexical features would just be to throw counts for every word into a huge feature vector, but the curse of dimensionality rules this method out in small training set situations. We propose a new solution to this problem, using an unsupervised deep autoencoder to automatically compress and extract complex high level lexical features. 2 Dataset Our experiments make use of the SpeedDate Corpus collected by the third author, and described in Jurafsky et al. (2009). The corpus is based on three speed-dating sessions run at an American university in 2005, inspired by prior speeddating research (Madan et al., 2005). The graduate student participants volunteered to be in the study and were promised emails of persons with whom they reported mutual liking. All participants wore audio recorders on a shoulder sash, thus resulting in two audio recordings of the approximately 1100 4-minute dates. Each date was conducted in an open setting where there was substantial background noise. This noisy audio was thus hand-transcribed and turn start and end were hand-aligned with the audio. In addition to the audio, the corpus includes various attitude and demographic questions answered by the participants. Each speake</context>
</contexts>
<marker>Madan, Caneel, Pentland, 2005</marker>
<rawString>Anmol Madan, Ron Caneel, and Alex Pentland. 2005. Voices of attraction. Presented at Augmented Cognition, HCI 2005, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Marilyn Walker</author>
</authors>
<title>Trainable generation of big-five personality styles through data-driven parameter estimation.</title>
<date>2008</date>
<booktitle>In ACL-08,</booktitle>
<location>Columbus.</location>
<contexts>
<context position="2716" citStr="Mairesse and Walker, 2008" startWordPosition="396" endWordPosition="399">ms that detect and model these differences can lead both to richer socially aware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans </context>
</contexts>
<marker>Mairesse, Walker, 2008</marker>
<rawString>Franc¸ois Mairesse and Marilyn Walker. 2008. Trainable generation of big-five personality styles through data-driven parameter estimation. In ACL-08, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Feature selection, L1 vs. L2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In ICML</booktitle>
<contexts>
<context position="20517" citStr="Ng, 2004" startWordPosition="3363" endWordPosition="3364">d from the code layer. is a bias to put weight on feature A because intuitively the weight on feature B would need to be 1000 times larger to carry the same effect. This argument holds similarly for the reduction to unit variance. Second, we removed features correlated greater than .7. One goal of removing correlated features was to remove as much colinearity as possible from the regression so that the regression weights could be ranked for their importance in the classification. In addition, we hoped to improve classification because a large number of features require more training examples (Ng, 2004). For example for perception of female flirt we removed the number of turns by the alter (O turns) and the number of sentence from the ego (S sentences) because they were highly correlated with S turns. To ensure comparisons (see Section 7) between the interlocutors’ ratings and our classifier (and because of our small dataset) we use k-fold cross validation to learn our model and evaluate our model. We train our binary model with the top ten percent of ratings labeled as positive class examples and bottom ten percent of ratings as the negative class examples. We used five-fold cross validatio</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>T C Lay</author>
</authors>
<title>Language use and personality during crises: Analyses of Mayor Rudolph Giuliani’s press conferences.</title>
<date>2002</date>
<journal>Journal of Research in Personality,</journal>
<pages>36--271</pages>
<contexts>
<context position="2791" citStr="Pennebaker and Lay, 2002" startWordPosition="408" endWordPosition="411">ware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While many of the features that </context>
</contexts>
<marker>Pennebaker, Lay, 2002</marker>
<rawString>J. W. Pennebaker and T. C. Lay. 2002. Language use and personality during crises: Analyses of Mayor Rudolph Giuliani’s press conferences. Journal of Research in Personality, 36:271–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>R E Booth</author>
<author>M E Francis</author>
</authors>
<title>Linguistic inquiry and word count: LIWC2007 operator’s manual.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>University of Texas.</institution>
<contexts>
<context position="3672" citStr="Pennebaker et al., 2007" startWordPosition="554" endWordPosition="558"> flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While many of the features that we use to build these detectors are drawn from the previous literature, we also explore new features. Conventional methods for lexical feature extraction, for example, generally consist of hand coded classes of words related to concepts like sex or eating (Pennebaker et al., 2007). The classes tend to perform well in their specific domains, but may not be robust across domains, suggesting the need for unsupervised domain-specific lexical feature extraction. The naive answer to extracting domain334 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334–342, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP specific lexical features would just be to throw counts for every word into a huge feature vector, but the curse of dimensionality rules this method out in small training set situations. We propose a new solution to this probl</context>
<context position="10353" citStr="Pennebaker et al. (2007)" startWordPosition="1674" endWordPosition="1677">racted using Praat from the hand-segmented turns of each side. hand-labeled Switchboard corpus of dialog acts. Collaborative completions, turns where a speaker completes the utterance begun by the alter, were detected by finding sentences for which the first word of the speaker was extremely predictable from the last two words of the previous speaker, based on a trigram grammar trained on the Treebank 3 Switchboard transcripts. Laughter, disfluencies, and overlap were all marked in the transcripts by the transcribers. 4.3 Lexical Features We drew our lexical features from the LIWC lexicons of Pennebaker et al. (2007), the standard for social psychological analysis of lexical features. We chose ten LIWC categories that have proven useful in detecting personality-related features (Mairesse et al., 2007): Anger, Assent, Ingest, Insight, Negemotion, Sexual, Swear, I, We, and You. We also added two new lexical features: “past tense auxiliary”, a heuristic for automatically detecting narrative or story-telling behavior, and Metadate, for discussion about the speed-date itself. The features are summarized in Table 3. 4.4 Inducing New Lexical Features In Jurafsky et al. (2009) we found the LIWC lexical features l</context>
<context position="15243" citStr="Pennebaker et al., 2007" startWordPosition="2437" endWordPosition="2440">ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry, SEXUAL love*, passion*, virgin, sex, screw INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish Table 3: Lexical features from Jurafsky et al. (2009). Each feature value is a total count of the words in that class for each conversation side; asterisks indicate including suffixed forms (e.g., love, loves, loving). All except the first three are from LIWC (Pennebaker et al., 2007) (modified slightly, e.g., by removing you know and I mean). The last five classes include more words in addition to those shown. coders tend to perform poorly if they are initialized incorrectly, so we use the Restricted Boltzmann Machine (RBM) pretraining procedure described in Hinton and Salakhutdinov (2006) to initialize the encoder. Each individual RBM is trained using contrastive divergence as an update rule which has been shown to produce reasonable results quickly (Hinton et al., 2006). Finally, we use backpropagation to fine tune the weights of our encoder by minimizing the cross entr</context>
</contexts>
<marker>Pennebaker, Booth, Francis, 2007</marker>
<rawString>J. W. Pennebaker, R.E. Booth, and M.E. Francis. 2007. Linguistic inquiry and word count: LIWC2007 operator’s manual. Technical report, University of Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Pentland</author>
</authors>
<title>Socially aware computation and communication.</title>
<date>2005</date>
<journal>Computer,</journal>
<pages>63--70</pages>
<contexts>
<context position="2882" citStr="Pentland, 2005" startWordPosition="425" endWordPosition="426">ractions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While many of the features that we use to build these detectors are drawn from the previous literature, we also explore new</context>
</contexts>
<marker>Pentland, 2005</marker>
<rawString>Alex Pentland. 2005. Socially aware computation and communication. Computer, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Rish</author>
</authors>
<title>An empirical study of the naive bayes classifier.</title>
<date>2001</date>
<booktitle>In IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence.</booktitle>
<contexts>
<context position="11719" citStr="Rish, 2001" startWordPosition="1896" endWordPosition="1897">s than previously investigated. We therefore investigated the induction of lexical features from the speed-date corpus, using a probabilisitic graphical model. We began with a pilot investigation to see whether lexical cues were likely to be useful; with a small corpus, it is possible that lexical features are simply too sparse to play a role given the limited data. The pilot was based on using Naive Bayes with word existence features (binomial Naive Bayes). Naive Bayes assumes all features are conditionally independent given the class, and is known to perform well with small amounts of data (Rish, 2001). Our Naive Bayes pilot system performed above chance, suggesting that lexical cues are indeed informative. A simple approach to including lexical features in our more general classification system would be to include the word counts in a high dimensional feature vector with our other features. This method, unfortunately, would suffer from the well-known high dimensionality/small training set problem. We propose a method for building a much smaller number of features that would nonetheless capture lexical information. Our approach is based on using autoencoders to construct high level lower di</context>
</contexts>
<marker>Rish, 2001</marker>
<rawString>Irina Rish. 2001. An empirical study of the naive bayes classifier. In IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Acoustic/prosodic and lexical correlates of charismatic speech.</title>
<date>2005</date>
<booktitle>In EUROSPEECH-05,</booktitle>
<pages>513--516</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2609" citStr="Rosenberg and Hirschberg, 2005" startWordPosition="381" endWordPosition="384"> both the social meaning intended by the speaker and the social meaning perceived by the hearer. Automated systems that detect and model these differences can lead both to richer socially aware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves inte</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2005</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2005. Acoustic/prosodic and lexical correlates of charismatic speech. In EUROSPEECH-05, pages 513– 516, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Rude</author>
<author>E M Gortner</author>
<author>J W Pennebaker</author>
</authors>
<title>Language use of depressed and depressionvulnerable college students. Cognition and Emotion,</title>
<date>2004</date>
<pages>18--1121</pages>
<contexts>
<context position="2765" citStr="Rude et al., 2004" startWordPosition="404" endWordPosition="407">o richer socially aware systems for conversational understanding and more sophisticated analyses of conversational interactions like meetings and interviews. This task thus extends the wide literature on social meaning and its detection, including the detection of emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), personality features like extroversion or agreeability (Mairesse et al., 2007; Mairesse and Walker, 2008), speaker depression or stress (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), and dating willingness or liking (Madan et al., 2005; Pentland, 2005). We chose to work on the domain of flirtation in speed-dating. Our earlier work on this corpus showed that it is possible to detect whether speakers are perceived as flirtatious, awkward, or friendly with reasonable accuracy (Jurafsky et al., 2009). In this paper we extend that work to detect whether speakers themselves intended to flirt, explore the differences in these variables, and explore the ability and inability of humans to correctly perceive the flirtation cues. While </context>
</contexts>
<marker>Rude, Gortner, Pennebaker, 2004</marker>
<rawString>S. S. Rude, E. M. Gortner, and J. W. Pennebaker. 2004. Language use of depressed and depressionvulnerable college students. Cognition and Emotion, 18:1121–1133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Scholkopf</author>
<author>K K Sung</author>
<author>CJC Burges</author>
<author>F Girosi</author>
<author>P Niyogi</author>
<author>T Poggio</author>
<author>V Vapnik</author>
</authors>
<title>Comparing support vector machines with Gaussian kernels to radialbasis function classifiers.</title>
<date>1997</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="23040" citStr="Scholkopf et al., 1997" startWordPosition="3845" endWordPosition="3848"> ||w ||2 + CE i=1 s.t. y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ... , m ξi ≥ 0, i = 1, ... , m (3) where m is the number of training examples, x(i) is the ith training examples, and y(i) is the ith class (1 for the positive class, -1 for the negative class). The ξi are the slack variables that allow this algorithm to work for non linearly separable datasets. A test example is classified by looking at the sign of y(x) = wTx(test) + b. To explore models that captured interactions, but do not allow for direct feature analysis we solved the C-SVM problem using a radial basis function (RBF) as a kernel (Scholkopf et al., 1997). Our RBF kernel is based on a Gaussian with unit variance. K(x(i),x(9)) = exp(−||x(i) − x(9)||2 2σ ) (4) In this case predictions can be made by looking at y(x(test)) = Em i=1 α(i)y(i)rbf(x(i), t(test)) + b, where each α(i), for i = 1, ... , m is a member of the set of dual variables that comes from transforming the primal form into the dual form. The SVM kernel trick allows us to explore higher dimensions while limiting the curse of dimensionality that plagues small datasets like ours. We evaluated both our linear C-SVM and our radial basis function C-SVM using parameters learned on the trai</context>
</contexts>
<marker>Scholkopf, Sung, Burges, Girosi, Niyogi, Poggio, Vapnik, 1997</marker>
<rawString>B. Scholkopf, K.K. Sung, CJC Burges, F. Girosi, P. Niyogi, T. Poggio, and V. Vapnik. 1997. Comparing support vector machines with Gaussian kernels to radialbasis function classifiers. IEEE Transactions on Signal Processing, 45(11):2758–2765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Taylor</author>
</authors>
<title>Chapter 2. In Balance in Small Groups.</title>
<date>1970</date>
<publisher>Von Nostrand Reinhold Company,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="33611" citStr="Taylor, 1970" startWordPosition="5625" endWordPosition="5626">gins and Bargh, 1987). It is also possible that for styles with positive valence (friendliness and flirting) speakers see more similarity between the self and the other than for negative styles (awkward and assertive) (Krah´e, 1983). Why should this strong bias exist to link selfflirting with perception of the other? One possibility is that speakers are just not very good at capturing the intentions of others in four minutes. Speakers instead base their judgments on their own behavior or intentions, perhaps because of a bias to maintain consistency in attitudes and relations (Festinger, 1957; Taylor, 1970) or to assume there is reciprocation in interpersonal perceptions (Kenny, 1998). 9 Conclusion We have presented a new system that is able to predict flirtation intention better than humans can, despite humans having access to vastly richer information (visual features, gesture, etc.). This system facilitates the analysis of human perception and human interaction and provides a framework for understanding why humans perform so poorly on intention prediction. At the heart of our system is a core set of prosodic, dialogue, and lexical features that allow for accurate prediction of both flirtation</context>
</contexts>
<marker>Taylor, 1970</marker>
<rawString>Howard Taylor. 1970. Chapter 2. In Balance in Small Groups. Von Nostrand Reinhold Company, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>