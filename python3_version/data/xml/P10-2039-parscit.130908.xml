<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.975598">
Efficient Optimization of an MDL-Inspired Objective Function for
Unsupervised Part-of-Speech Tagging
</title>
<author confidence="0.99966">
Ashish Vaswani1 Adam Pauls2 David Chiang1
</author>
<affiliation confidence="0.79523825">
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
</affiliation>
<email confidence="0.998558">
{avaswani,chiang}@isi.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999224238095238">
The Minimum Description Length (MDL)
principle is a method for model selection
that trades off between the explanation of
the data by the model and the complexity
of the model itself. Inspired by the MDL
principle, we develop an objective func-
tion for generative models that captures
the description of the data by the model
(log-likelihood) and the description of the
model (model size). We also develop a ef-
ficient general search algorithm based on
the MAP-EM framework to optimize this
function. Since recent work has shown that
minimizing the model size in a Hidden
Markov Model for part-of-speech (POS)
tagging leads to higher accuracies, we test
our approach by applying it to this prob-
lem. The search algorithm involves a sim-
ple change to EM and achieves high POS
tagging accuracies on both English and
Italian data sets.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999660375">
The Minimum Description Length (MDL) princi-
ple is a method for model selection that provides a
generic solution to the overfitting problem (Barron
et al., 1998). A formalization of Ockham’s Razor,
it says that the parameters are to be chosen that
minimize the description length of the data given
the model plus the description length of the model
itself.
It has been successfully shown that minimizing
the model size in a Hidden Markov Model (HMM)
for part-of-speech (POS) tagging leads to higher
accuracies than simply running the Expectation-
Maximization (EM) algorithm (Dempster et al.,
1977). Goldwater and Griffiths (2007) employ a
Bayesian approach to POS tagging and use sparse
Dirichlet priors to minimize model size. More re-
</bodyText>
<affiliation confidence="0.90377">
2Computer Science Division
University of California at Berkeley
</affiliation>
<address confidence="0.6745635">
Soda Hall
Berkeley, CA 94720
</address>
<email confidence="0.986318">
adpauls@eecs.berkeley.edu
</email>
<bodyText confidence="0.999923666666667">
cently, Ravi and Knight (2009) alternately mini-
mize the model using an integer linear program
and maximize likelihood using EM to achieve the
highest accuracies on the task so far. However, in
the latter approach, because there is no single ob-
jective function to optimize, it is not entirely clear
how to generalize this technique to other prob-
lems. In this paper, inspired by the MDL princi-
ple, we develop an objective function for genera-
tive models that captures both the description of
the data by the model (log-likelihood) and the de-
scription of the model (model size). By using a
simple prior that encourages sparsity, we cast our
problem as a search for the maximum a poste-
riori (MAP) hypothesis and present a variant of
EM to approximately search for the minimum-
description-length model. Applying our approach
to the POS tagging problem, we obtain higher ac-
curacies than both EM and Bayesian inference as
reported by Goldwater and Griffiths (2007). On a
Italian POS tagging task, we obtain even larger
improvements. We find that our objective function
correlates well with accuracy, suggesting that this
technique might be useful for other problems.
</bodyText>
<sectionHeader confidence="0.985107" genericHeader="method">
2 MAP EM with Sparse Priors
</sectionHeader>
<subsectionHeader confidence="0.955131">
2.1 Objective function
</subsectionHeader>
<bodyText confidence="0.99995375">
In the unsupervised POS tagging task, we are
given a word sequence w = w1, ... , wN and want
to find the best tagging t = t1, ... , tN, where
ti E T, the tag vocabulary. We adopt the problem
formulation of Merialdo (1994), in which we are
given a dictionary of possible tags for each word
type.
We define a bigram HMM
</bodyText>
<equation confidence="0.992487333333333">
N
P(w, t  |0) = P(w, t  |0) · P(ti  |ti−1) (1)
i=1
</equation>
<bodyText confidence="0.83636">
In maximum likelihood estimation, the goal is to
</bodyText>
<page confidence="0.982774">
209
</page>
<note confidence="0.523194">
Proceedings of the ACL 2010 Conference Short Papers, pages 209–214,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.990501625">
find parameter estimates
The EM algorithm can be used to find a solution.
However, we would like to maximize likelihood
and minimize the size of the model simultane-
ously. We define the size of a model as the number
of non-zero probabilities in its parameter vector.
Let θ1, ... , θn be the components of θ. We would
like to find
</bodyText>
<equation confidence="0.9921925">
θˆ = arg max log P(w  |θ) (2)
θ
X= arg max log P(w, t  |θ) (3)
θ t
θˆ = arg min (− log P(w  |θ) + αkθk0) (4) Figure 1: Ideal model-size term and its approxima-
θ tions.
</equation>
<bodyText confidence="0.999982769230769">
where kθk0, called the L0 norm of θ, simply counts
the number of non-zero parameters in θ. The
hyperparameter α controls the tradeoff between
likelihood maximization and model minimization.
Note the similarity of this objective function with
MDL’s, where α would be the space (measured
in nats) needed to describe one parameter of the
model.
Unfortunately, minimization of the L0 norm
is known to be NP-hard (Hyder and Mahata,
2009). It is not smooth, making it unamenable
to gradient-based optimization algorithms. There-
fore, we use a smoothed approximation,
</bodyText>
<equation confidence="0.9994715">
kθk0 ≈ X
i (1 − e−θi )
</equation>
<bodyText confidence="0.929754">
β (5)
where 0 &lt; β ≤ 1 (Mohimani et al., 2007). For
smaller values of β, this closely approximates the
desired function (Figure 1). Inverting signs and ig-
noring constant terms, our objective function is
now:
</bodyText>
<equation confidence="0.987773">
θˆ = arg max
θ X⎛⎜⎜⎜⎜⎜⎝log P(w  |θ) + α
i
</equation>
<bodyText confidence="0.9987615">
We can think of the approximate model size as
a kind of prior:
</bodyText>
<equation confidence="0.968431">
−θi
P(θ) = exp α Pi e β (7)
Z
log P(θ) = α · X e β − logZ (8)
−θi
i
Rwhere Z = dθ exp α Pi e −θi
</equation>
<bodyText confidence="0.913203">
β is a normalization
constant. Then our goal is to find the maximum
a posterior parameter estimate, which we find us-
ing MAP-EM (Bishop, 2006):
</bodyText>
<equation confidence="0.97806725">
θˆ = arg max log P(w, θ) (9)
θ
= arg max (log P(w  |θ) + log P(θ)) (10)
θ
</equation>
<bodyText confidence="0.984350384615385">
Substituting (8) into (10) and ignoring the constant
term log Z, we get our objective function (6) again.
We can exercise finer control over the sparsity
of the tag-bigram and channel probability distri-
butions by using a different α for each:
In our experiments, we set αc = 0 since previ-
ous work has shown that minimizing the number
of tag n-gram parameters is more important (Ravi
and Knight, 2009; Goldwater and Griffiths, 2007).
A common method for preferring smaller mod-
els is minimizing the L1 norm, Pi |θi|. However,
for a model which is a product of multinomial dis-
tributions, the L1 norm is a constant.
</bodyText>
<equation confidence="0.994530714285714">
X |θi |= X θi
i i
X=
t ⎛⎜⎜⎜⎜⎜X X ⎞⎟⎟⎟⎟⎟
⎝ P(w  |t) + ⎠
w t0 P(t0  |t)
= 2|T|
</equation>
<bodyText confidence="0.997751">
Therefore, we cannot use the L1 norm as part of
the size term as the result will be the same as the
EM algorithm.
</bodyText>
<equation confidence="0.878699571428571">
(6)
−θi ⎞⎟⎟⎟⎟⎟
eβ ⎠
arg max log P(w  |θ) +
θ
Xαc
w,t
X
−P(w|t)
eβ + αt
t,t0
!
−P(t0|t)
eβ (11)
</equation>
<page confidence="0.993231">
210
</page>
<subsectionHeader confidence="0.994557">
2.2 Parameter optimization
</subsectionHeader>
<bodyText confidence="0.9999088">
To optimize (11), we use MAP EM, which is an it-
erative search procedure. The E step is the same as
in standard EM, which is to calculate P(t  |w, θt),
where the θt are the parameters in the current iter-
ation t. The M step in iteration (t + 1) looks like
</bodyText>
<equation confidence="0.619305">
EP(t|w,θt) [log P(w, t  |θ)] +
</equation>
<bodyText confidence="0.99941225">
Let C(t, w; t, w) count the number of times the
word w is tagged as t in t, and C(t, t&apos;; t) the number
of times the tag bigram (t, t&apos;) appears in t. We can
rewrite the M step as
</bodyText>
<equation confidence="0.9864066">
Z Z
θt+1 = arg max E[C(t, w)] log P(w  |t)+
θ
(E[C(t, t&apos;)] log P(t&apos;  |t) + αte−P(t&apos;|t) �������� (13)
β
</equation>
<bodyText confidence="0.94458425">
subject to the constraints Ew P(w  |t) = 1 and
Et&apos; P(t&apos;  |t) = 1. Note that we can optimize each
term of both summations over t separately. For
each t, the term
</bodyText>
<equation confidence="0.690941">
Z E[C(t, w)] log P(w  |t) (14)
w
</equation>
<bodyText confidence="0.829633">
is easily optimized as in EM: just let P(w  |t) oc
E[C(t, w)]. But the term
</bodyText>
<equation confidence="0.997773">
−P(t&apos;|t)
(E[C(t,t&apos;)]log P(t&apos;  |t) + αte�
β (15)
</equation>
<bodyText confidence="0.980486833333333">
is trickier. This is a non-convex optimization prob-
lem for which we invoke a publicly available
constrained optimization tool, ALGENCAN (An-
dreani et al., 2007). To carry out its optimization,
ALGENCAN requires computation of the follow-
ing in every iteration:
</bodyText>
<listItem confidence="0.987824166666667">
• Objective function, defined in equation (15).
This is calculated in polynomial time using
dynamic programming.
• Constraints: gt = Et&apos; P(t&apos;  |t) − 1 = 0 for
each tag t E T. Also, we constrain P(t&apos;  |t) to
the interval [c,1].1
</listItem>
<bodyText confidence="0.570737666666667">
1We must have e &gt; 0 because of the log P(t&apos;  |t) term
in equation (15). It seems reasonable to set e &lt;&lt; 1N; in our
experiments, we set e = 10−7.
</bodyText>
<listItem confidence="0.93544">
• Gradient of objective function:
</listItem>
<equation confidence="0.915266">
−P(t&apos;|t)
eβ (16)
</equation>
<listItem confidence="0.900668">
• Gradient of equality constraints:
</listItem>
<equation confidence="0.49676">
1 ift=t&apos;
(17)
0 otherwise
</equation>
<listItem confidence="0.995546666666667">
• Hessian of objective function, which is not
required but greatly speeds up the optimiza-
tion:
</listItem>
<equation confidence="0.879357666666667">
−P(t&apos;|t)
e β
(18)
</equation>
<bodyText confidence="0.998069166666667">
The other second-order partial derivatives are
all zero, as are those of the equality con-
straints.
We perform this optimization for each instance
of (15). These optimizations could easily be per-
formed in parallel for greater scalability.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998832">
We carried out POS tagging experiments on En-
glish and Italian.
</bodyText>
<subsectionHeader confidence="0.981612">
3.1 English POS tagging
</subsectionHeader>
<bodyText confidence="0.999997">
To set the hyperparameters αt and β, we prepared
three held-out sets H1, H2, and H3 from the Penn
Treebank. Each Hi comprised about 24, 000 words
annotated with POS tags. We ran MAP-EM for
100 iterations, with uniform probability initializa-
tion, for a suite of hyperparameters and averaged
their tagging accuracies over the three held-out
sets. The results are presented in Table 2. We then
picked the hyperparameter setting with the highest
average accuracy. These were αt = 80,β = 0.05.
We then ran MAP-EM again on the test data with
these hyperparameters and achieved a tagging ac-
curacy of 87.4% (see Table 1). This is higher than
the 85.2% that Goldwater and Griffiths (2007) ob-
tain using Bayesian methods for inferring both
POS tags and hyperparameters. It is much higher
than the 82.4% that standard EM achieves on the
test set when run for 100 iterations.
Using αt = 80,β = 0.05, we ran multiple ran-
dom restarts on the test set (see Figure 2). We find
that the objective function correlates well with ac-
curacy, and picking the point with the highest ob-
jective function value achieves 87.1% accuracy.
</bodyText>
<equation confidence="0.999069041666666">
θt+1 = arg max
θ
αtZ e � (12)
−P(t&apos;|t)
β
t w
Z
t
Z
t
Z
t&apos;
∂F E[C(t, t&apos;)] − αt
∂P(t&apos;  |t) = P(t&apos;  |t) β
∂gt
∂P(t&apos;&apos;  |t&apos;)
��� �
���
=
∂2F
∂P(t&apos;  |t)∂P(t&apos;  |t)
= −E[C(t, t&apos;)] + αt
P(t&apos;  |t)2
β2
</equation>
<page confidence="0.995731">
211
</page>
<table confidence="0.999885705882353">
αt 0.75 0.5 0.25 0.075 R 0.025 0.0075 0.005 0.0025
0.05
10 82.81 82.78 83.10 83.50 83.76 83.70 84.07 83.95 83.75
20 82.78 82.82 83.26 83.60 83.89 84.88 83.74 84.12 83.46
30 82.78 83.06 83.26 83.29 84.50 84.82 84.54 83.93 83.47
40 82.81 83.13 83.50 83.98 84.23 85.31 85.05 83.84 83.46
50 82.84 83.24 83.15 84.08 82.53 84.90 84.73 83.69 82.70
60 83.05 83.14 83.26 83.30 82.08 85.23 85.06 83.26 82.96
70 83.09 83.10 82.97 82.37 83.30 86.32 83.98 83.55 82.97
80 83.13 83.15 82.71 83.00 86.47 86.24 83.94 83.26 82.93
90 83.20 83.18 82.53 84.20 86.32 84.87 83.49 83.62 82.03
100 83.19 83.51 82.84 84.60 86.13 85.94 83.26 83.67 82.06
110 83.18 83.53 83.29 84.40 86.19 85.18 80.76 83.32 82.05
120 83.08 83.65 83.71 84.11 86.03 85.39 80.66 82.98 82.20
130 83.10 83.19 83.52 84.02 85.79 85.65 80.08 82.04 81.76
140 83.11 83.17 83.34 85.26 85.86 85.84 79.09 82.51 81.64
150 83.14 83.20 83.40 85.33 85.54 85.18 78.90 81.99 81.88
</table>
<tableCaption confidence="0.954892">
Table 2: Average accuracies over three held-out sets for English.
</tableCaption>
<table confidence="0.986029666666667">
system accuracy (%)
Standard EM 82.4
+ random restarts 84.5
(Goldwater and Griffiths, 2007) 85.2
our approach 87.4
+ random restarts 87.1
</table>
<tableCaption confidence="0.989014">
Table 1: MAP-EM with a L0 norm achieves higher
tagging accuracy on English than (2007) and much
higher than standard EM.
</tableCaption>
<table confidence="0.9465115">
system zero parameters bigram types
maximum possible 1389 –
EM, 100 iterations 444 924
MAP-EM, 100 iterations 695 648
</table>
<tableCaption confidence="0.9323385">
Table 3: MAP-EM with a smoothed L0 norm
yields much smaller models than standard EM.
</tableCaption>
<bodyText confidence="0.999786384615385">
We also carried out the same experiment with stan-
dard EM (Figure 3), where picking the point with
the highest corpus probability achieves 84.5% ac-
curacy.
We also measured the minimization effect of the
sparse prior against that of standard EM. Since our
method lower-bounds all the parameters by c, we
consider a parameter Oi as a zero if Oi &lt;_ c. We
also measured the number of unique tag bigram
types in the Viterbi tagging of the word sequence.
Table 3 shows that our method produces much
smaller models than EM, and produces Viterbi
taggings with many fewer tag-bigram types.
</bodyText>
<subsectionHeader confidence="0.991147">
3.2 Italian POS tagging
</subsectionHeader>
<bodyText confidence="0.999272">
We also carried out POS tagging experiments on
an Italian corpus from the Italian Turin Univer-
</bodyText>
<figure confidence="0.748357333333333">
αt=80,b=0.05,Test Set 24115 Words
-53200 -53000 -52800 -52600 -52400 -52200 -52000 -51800 -51600 -5
objective function value
</figure>
<figureCaption confidence="0.938308">
Figure 2: Tagging accuracy vs. objective func-
</figureCaption>
<bodyText confidence="0.936700615384615">
tion for 1152 random restarts of MAP-EM with
smoothed L0 norm.
sity Treebank (Bos et al., 2009). This test set com-
prises 21, 878 words annotated with POS tags and
a dictionary for each word type. Since this is all
the available data, we could not tune the hyperpa-
rameters on a held-out data set. Using the hyper-
parameters tuned on English (αt = 80,/3 = 0.05),
we obtained 89.7% tagging accuracy (see Table 4),
which was a large improvement over 81.2% that
standard EM achieved. When we tuned the hyper-
parameters on the test set, the best setting (αt =
120,8 = 0.05 gave an accuracy of 90.28%.
</bodyText>
<sectionHeader confidence="0.997718" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9994085">
A variety of other techniques in the literature have
been applied to this unsupervised POS tagging
task. Smith and Eisner (2005) use conditional ran-
dom fields with contrastive estimation to achieve
</bodyText>
<figure confidence="0.998991416666667">
0.89
0.88
0.87
0.86
0.85
0.84
0.83
0.82
0.81
0.8
0.79
0.78
</figure>
<page confidence="0.992115">
212
</page>
<table confidence="0.999870588235294">
αt 0.75 0.5 0.25 0.075 R 0.025 0.0075 0.005 0.0025
0.05
10 81.62 81.67 81.63 82.47 82.70 84.64 84.82 84.96 84.90
20 81.67 81.63 81.76 82.75 84.28 84.79 85.85 88.49 85.30
30 81.66 81.63 82.29 83.43 85.08 88.10 86.16 88.70 88.34
40 81.64 81.79 82.30 85.00 86.10 88.86 89.28 88.76 88.80
50 81.71 81.71 78.86 85.93 86.16 88.98 88.98 89.11 88.01
60 81.65 82.22 78.95 86.11 87.16 89.35 88.97 88.59 88.00
70 81.69 82.25 79.55 86.32 89.79 89.37 88.91 85.63 87.89
80 81.74 82.23 80.78 86.34 89.70 89.58 88.87 88.32 88.56
90 81.70 81.85 81.00 86.35 90.08 89.40 89.09 88.09 88.50
100 81.70 82.27 82.24 86.53 90.07 88.93 89.09 88.30 88.72
110 82.19 82.49 82.22 86.77 90.12 89.22 88.87 88.48 87.91
120 82.23 78.60 82.76 86.77 90.28 89.05 88.75 88.83 88.53
130 82.20 78.60 83.33 87.48 90.12 89.15 89.30 87.81 88.66
140 82.24 78.64 83.34 87.48 90.12 89.01 88.87 88.99 88.85
150 82.28 78.69 83.32 87.75 90.25 87.81 88.50 89.07 88.41
</table>
<tableCaption confidence="0.999916">
Table 4: Accuracies on test set for Italian.
</tableCaption>
<figure confidence="0.992391090909091">
EM, Test Set 24115 Words
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
-147500 -147400 -147300 -147200 -147100 -147000 -146900 -146800 -146700 -146600 -146500
objective function value
</figure>
<figureCaption confidence="0.9887245">
Figure 3: Tagging accuracy vs. likelihood for 1152
random restarts of standard EM.
</figureCaption>
<bodyText confidence="0.999722333333333">
racy supporting the MDL principle. Our approach
performs quite well on POS tagging for both En-
glish and Italian. We believe that, like EM, our
method can benefit from more unlabeled data, and
there is reason to hope that the success of these
experiments will carry over to other tasks as well.
</bodyText>
<sectionHeader confidence="0.996922" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999989692307693">
We would like to thank Sujith Ravi, Kevin Knight
and Steve DeNeefe for their valuable input, and
Jason Baldridge for directing us to the Italian
POS data. This research was supported in part by
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
88.6% accuracy. Goldberg et al. (2008) provide
a linguistically-informed starting point for EM to
achieve 91.4% accuracy. More recently, Chiang et
al. (2010) use GIbbs sampling for Bayesian in-
ference along with automatic run selection and
achieve 90.7%.
In this paper, our goal has been to investi-
gate whether EM can be extended in a generic
way to use an MDL-like objective function that
simultaneously maximizes likelihood and mini-
mizes model size. We have presented an efficient
search procedure that optimizes this function for
generative models and demonstrated that maxi-
mizing this function leads to improvement in tag-
ging accuracy over standard EM. We infer the hy-
perparameters of our model using held out data
and achieve better accuracies than (Goldwater and
Griffiths, 2007). We have also shown that the ob-
jective function correlates well with tagging accu-
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908777777778">
R. Andreani, E. G. Birgin, J. M. Martnez, and M. L.
Schuverdt. 2007. On Augmented Lagrangian meth-
ods with general lower-level constraints. SIAM
Journal on Optimization, 18:1286–1309.
A. Barron, J. Rissanen, and B. Yu. 1998. The min-
imum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743–2760.
C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorical grammar tree-
bank for italian. In Eighth International Workshop
on Treebanks and Linguistic Theories (TLT8).
D. Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi.
2010. Bayesian inference for Finite-State transduc-
ers. In Proceedings of the North American Associa-
tion of Computational Linguistics.
</reference>
<page confidence="0.98893">
213
</page>
<reference confidence="0.999424551724138">
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1–
38.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Proceedings of the 2009 IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155–171.
H. Mohimani, M. Babaie-Zadeh, and C. Jutten. 2007.
Fast sparse representation based on smoothed L0
norm. In Proceedings of the 7th International Con-
ference on Independent Component Analysis and
Signal Separation (ICA2007).
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
N. Smith. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the ACL.
</reference>
<page confidence="0.998947">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954021">
<title confidence="0.9989725">Optimization of an MDL-Inspired Objective Function for Unsupervised Part-of-Speech Tagging</title>
<author confidence="0.999143">Adam David</author>
<affiliation confidence="0.9986875">Sciences Institute University of Southern California</affiliation>
<address confidence="0.996377">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<abstract confidence="0.998334727272727">The Minimum Description Length (MDL) principle is a method for model selection trades the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Andreani</author>
<author>E G Birgin</author>
<author>J M Martnez</author>
<author>M L Schuverdt</author>
</authors>
<title>On Augmented Lagrangian methods with general lower-level constraints.</title>
<date>2007</date>
<journal>SIAM Journal on Optimization,</journal>
<pages>18--1286</pages>
<contexts>
<context position="7435" citStr="Andreani et al., 2007" startWordPosition="1341" endWordPosition="1345">, t&apos;) appears in t. We can rewrite the M step as Z Z θt+1 = arg max E[C(t, w)] log P(w |t)+ θ (E[C(t, t&apos;)] log P(t&apos; |t) + αte−P(t&apos;|t) �������� (13) β subject to the constraints Ew P(w |t) = 1 and Et&apos; P(t&apos; |t) = 1. Note that we can optimize each term of both summations over t separately. For each t, the term Z E[C(t, w)] log P(w |t) (14) w is easily optimized as in EM: just let P(w |t) oc E[C(t, w)]. But the term −P(t&apos;|t) (E[C(t,t&apos;)]log P(t&apos; |t) + αte� β (15) is trickier. This is a non-convex optimization problem for which we invoke a publicly available constrained optimization tool, ALGENCAN (Andreani et al., 2007). To carry out its optimization, ALGENCAN requires computation of the following in every iteration: • Objective function, defined in equation (15). This is calculated in polynomial time using dynamic programming. • Constraints: gt = Et&apos; P(t&apos; |t) − 1 = 0 for each tag t E T. Also, we constrain P(t&apos; |t) to the interval [c,1].1 1We must have e &gt; 0 because of the log P(t&apos; |t) term in equation (15). It seems reasonable to set e &lt;&lt; 1N; in our experiments, we set e = 10−7. • Gradient of objective function: −P(t&apos;|t) eβ (16) • Gradient of equality constraints: 1 ift=t&apos; (17) 0 otherwise • Hessian of obje</context>
</contexts>
<marker>Andreani, Birgin, Martnez, Schuverdt, 2007</marker>
<rawString>R. Andreani, E. G. Birgin, J. M. Martnez, and M. L. Schuverdt. 2007. On Augmented Lagrangian methods with general lower-level constraints. SIAM Journal on Optimization, 18:1286–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Barron</author>
<author>J Rissanen</author>
<author>B Yu</author>
</authors>
<title>The minimum description length principle in coding and modeling.</title>
<date>1998</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>44</volume>
<issue>6</issue>
<contexts>
<context position="1304" citStr="Barron et al., 1998" startWordPosition="199" endWordPosition="202">ze). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets. 1 Introduction The Minimum Description Length (MDL) principle is a method for model selection that provides a generic solution to the overfitting problem (Barron et al., 1998). A formalization of Ockham’s Razor, it says that the parameters are to be chosen that minimize the description length of the data given the model plus the description length of the model itself. It has been successfully shown that minimizing the model size in a Hidden Markov Model (HMM) for part-of-speech (POS) tagging leads to higher accuracies than simply running the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. More re2Computer Science Division </context>
</contexts>
<marker>Barron, Rissanen, Yu, 1998</marker>
<rawString>A. Barron, J. Rissanen, and B. Yu. 1998. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="5400" citStr="Bishop, 2006" startWordPosition="929" endWordPosition="930">refore, we use a smoothed approximation, kθk0 ≈ X i (1 − e−θi ) β (5) where 0 &lt; β ≤ 1 (Mohimani et al., 2007). For smaller values of β, this closely approximates the desired function (Figure 1). Inverting signs and ignoring constant terms, our objective function is now: θˆ = arg max θ X⎛⎜⎜⎜⎜⎜⎝log P(w |θ) + α i We can think of the approximate model size as a kind of prior: −θi P(θ) = exp α Pi e β (7) Z log P(θ) = α · X e β − logZ (8) −θi i Rwhere Z = dθ exp α Pi e −θi β is a normalization constant. Then our goal is to find the maximum a posterior parameter estimate, which we find using MAP-EM (Bishop, 2006): θˆ = arg max log P(w, θ) (9) θ = arg max (log P(w |θ) + log P(θ)) (10) θ Substituting (8) into (10) and ignoring the constant term log Z, we get our objective function (6) again. We can exercise finer control over the sparsity of the tag-bigram and channel probability distributions by using a different α for each: In our experiments, we set αc = 0 since previous work has shown that minimizing the number of tag n-gram parameters is more important (Ravi and Knight, 2009; Goldwater and Griffiths, 2007). A common method for preferring smaller models is minimizing the L1 norm, Pi |θi|. However, f</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>C. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>C Bosco</author>
<author>A Mazzei</author>
</authors>
<title>Converting a dependency treebank to a categorical grammar treebank for italian.</title>
<date>2009</date>
<booktitle>In Eighth International Workshop on Treebanks and Linguistic Theories (TLT8).</booktitle>
<contexts>
<context position="12174" citStr="Bos et al., 2009" startWordPosition="2175" endWordPosition="2178">. We also measured the number of unique tag bigram types in the Viterbi tagging of the word sequence. Table 3 shows that our method produces much smaller models than EM, and produces Viterbi taggings with many fewer tag-bigram types. 3.2 Italian POS tagging We also carried out POS tagging experiments on an Italian corpus from the Italian Turin Univerαt=80,b=0.05,Test Set 24115 Words -53200 -53000 -52800 -52600 -52400 -52200 -52000 -51800 -51600 -5 objective function value Figure 2: Tagging accuracy vs. objective function for 1152 random restarts of MAP-EM with smoothed L0 norm. sity Treebank (Bos et al., 2009). This test set comprises 21, 878 words annotated with POS tags and a dictionary for each word type. Since this is all the available data, we could not tune the hyperparameters on a held-out data set. Using the hyperparameters tuned on English (αt = 80,/3 = 0.05), we obtained 89.7% tagging accuracy (see Table 4), which was a large improvement over 81.2% that standard EM achieved. When we tuned the hyperparameters on the test set, the best setting (αt = 120,8 = 0.05 gave an accuracy of 90.28%. 4 Conclusion A variety of other techniques in the literature have been applied to this unsupervised PO</context>
</contexts>
<marker>Bos, Bosco, Mazzei, 2009</marker>
<rawString>J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a dependency treebank to a categorical grammar treebank for italian. In Eighth International Workshop on Treebanks and Linguistic Theories (TLT8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>A Pauls</author>
<author>S Ravi</author>
</authors>
<title>Bayesian inference for Finite-State transducers.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics.</booktitle>
<contexts>
<context position="14936" citStr="Chiang et al. (2010)" startWordPosition="2646" endWordPosition="2649"> can benefit from more unlabeled data, and there is reason to hope that the success of these experiments will carry over to other tasks as well. Acknowledgements We would like to thank Sujith Ravi, Kevin Knight and Steve DeNeefe for their valuable input, and Jason Baldridge for directing us to the Italian POS data. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and DARPA contract HR0011-09-1-0028. 88.6% accuracy. Goldberg et al. (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. More recently, Chiang et al. (2010) use GIbbs sampling for Bayesian inference along with automatic run selection and achieve 90.7%. In this paper, our goal has been to investigate whether EM can be extended in a generic way to use an MDL-like objective function that simultaneously maximizes likelihood and minimizes model size. We have presented an efficient search procedure that optimizes this function for generative models and demonstrated that maximizing this function leads to improvement in tagging accuracy over standard EM. We infer the hyperparameters of our model using held out data and achieve better accuracies than (Gol</context>
</contexts>
<marker>Chiang, Graehl, Knight, Pauls, Ravi, 2010</marker>
<rawString>D. Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi. 2010. Bayesian inference for Finite-State transducers. In Proceedings of the North American Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<pages>38</pages>
<contexts>
<context position="1739" citStr="Dempster et al., 1977" startWordPosition="268" endWordPosition="271"> data sets. 1 Introduction The Minimum Description Length (MDL) principle is a method for model selection that provides a generic solution to the overfitting problem (Barron et al., 1998). A formalization of Ockham’s Razor, it says that the parameters are to be chosen that minimize the description length of the data given the model plus the description length of the model itself. It has been successfully shown that minimizing the model size in a Hidden Markov Model (HMM) for part-of-speech (POS) tagging leads to higher accuracies than simply running the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. More re2Computer Science Division University of California at Berkeley Soda Hall Berkeley, CA 94720 adpauls@eecs.berkeley.edu cently, Ravi and Knight (2009) alternately minimize the model using an integer linear program and maximize likelihood using EM to achieve the highest accuracies on the task so far. However, in the latter approach, because there is no single objective function to optimize, it is not entirely clear how to generalize this technique to other pro</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Computational Linguistics, 39(4):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="14817" citStr="Goldberg et al. (2008)" startWordPosition="2629" endWordPosition="2632">ciple. Our approach performs quite well on POS tagging for both English and Italian. We believe that, like EM, our method can benefit from more unlabeled data, and there is reason to hope that the success of these experiments will carry over to other tasks as well. Acknowledgements We would like to thank Sujith Ravi, Kevin Knight and Steve DeNeefe for their valuable input, and Jason Baldridge for directing us to the Italian POS data. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and DARPA contract HR0011-09-1-0028. 88.6% accuracy. Goldberg et al. (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. More recently, Chiang et al. (2010) use GIbbs sampling for Bayesian inference along with automatic run selection and achieve 90.7%. In this paper, our goal has been to investigate whether EM can be extended in a generic way to use an MDL-like objective function that simultaneously maximizes likelihood and minimizes model size. We have presented an efficient search procedure that optimizes this function for generative models and demonstrated that maximizing this function leads to improvement in tagging accuracy </context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1771" citStr="Goldwater and Griffiths (2007)" startWordPosition="272" endWordPosition="275">on The Minimum Description Length (MDL) principle is a method for model selection that provides a generic solution to the overfitting problem (Barron et al., 1998). A formalization of Ockham’s Razor, it says that the parameters are to be chosen that minimize the description length of the data given the model plus the description length of the model itself. It has been successfully shown that minimizing the model size in a Hidden Markov Model (HMM) for part-of-speech (POS) tagging leads to higher accuracies than simply running the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. More re2Computer Science Division University of California at Berkeley Soda Hall Berkeley, CA 94720 adpauls@eecs.berkeley.edu cently, Ravi and Knight (2009) alternately minimize the model using an integer linear program and maximize likelihood using EM to achieve the highest accuracies on the task so far. However, in the latter approach, because there is no single objective function to optimize, it is not entirely clear how to generalize this technique to other problems. In this paper, inspired b</context>
<context position="5906" citStr="Goldwater and Griffiths, 2007" startWordPosition="1021" endWordPosition="1024">ion constant. Then our goal is to find the maximum a posterior parameter estimate, which we find using MAP-EM (Bishop, 2006): θˆ = arg max log P(w, θ) (9) θ = arg max (log P(w |θ) + log P(θ)) (10) θ Substituting (8) into (10) and ignoring the constant term log Z, we get our objective function (6) again. We can exercise finer control over the sparsity of the tag-bigram and channel probability distributions by using a different α for each: In our experiments, we set αc = 0 since previous work has shown that minimizing the number of tag n-gram parameters is more important (Ravi and Knight, 2009; Goldwater and Griffiths, 2007). A common method for preferring smaller models is minimizing the L1 norm, Pi |θi|. However, for a model which is a product of multinomial distributions, the L1 norm is a constant. X |θi |= X θi i i X= t ⎛⎜⎜⎜⎜⎜X X ⎞⎟⎟⎟⎟⎟ ⎝ P(w |t) + ⎠ w t0 P(t0 |t) = 2|T| Therefore, we cannot use the L1 norm as part of the size term as the result will be the same as the EM algorithm. (6) −θi ⎞⎟⎟⎟⎟⎟ eβ ⎠ arg max log P(w |θ) + θ Xαc w,t X −P(w|t) eβ + αt t,t0 ! −P(t0|t) eβ (11) 210 2.2 Parameter optimization To optimize (11), we use MAP EM, which is an iterative search procedure. The E step is the same as in sta</context>
<context position="9149" citStr="Goldwater and Griffiths (2007)" startWordPosition="1639" endWordPosition="1642">three held-out sets H1, H2, and H3 from the Penn Treebank. Each Hi comprised about 24, 000 words annotated with POS tags. We ran MAP-EM for 100 iterations, with uniform probability initialization, for a suite of hyperparameters and averaged their tagging accuracies over the three held-out sets. The results are presented in Table 2. We then picked the hyperparameter setting with the highest average accuracy. These were αt = 80,β = 0.05. We then ran MAP-EM again on the test data with these hyperparameters and achieved a tagging accuracy of 87.4% (see Table 1). This is higher than the 85.2% that Goldwater and Griffiths (2007) obtain using Bayesian methods for inferring both POS tags and hyperparameters. It is much higher than the 82.4% that standard EM achieves on the test set when run for 100 iterations. Using αt = 80,β = 0.05, we ran multiple random restarts on the test set (see Figure 2). We find that the objective function correlates well with accuracy, and picking the point with the highest objective function value achieves 87.1% accuracy. θt+1 = arg max θ αtZ e � (12) −P(t&apos;|t) β t w Z t Z t Z t&apos; ∂F E[C(t, t&apos;)] − αt ∂P(t&apos; |t) = P(t&apos; |t) β ∂gt ∂P(t&apos;&apos; |t&apos;) ��� � ��� = ∂2F ∂P(t&apos; |t)∂P(t&apos; |t) = −E[C(t, t&apos;)] + αt </context>
<context position="10840" citStr="Goldwater and Griffiths, 2007" startWordPosition="1950" endWordPosition="1953">3.55 82.97 80 83.13 83.15 82.71 83.00 86.47 86.24 83.94 83.26 82.93 90 83.20 83.18 82.53 84.20 86.32 84.87 83.49 83.62 82.03 100 83.19 83.51 82.84 84.60 86.13 85.94 83.26 83.67 82.06 110 83.18 83.53 83.29 84.40 86.19 85.18 80.76 83.32 82.05 120 83.08 83.65 83.71 84.11 86.03 85.39 80.66 82.98 82.20 130 83.10 83.19 83.52 84.02 85.79 85.65 80.08 82.04 81.76 140 83.11 83.17 83.34 85.26 85.86 85.84 79.09 82.51 81.64 150 83.14 83.20 83.40 85.33 85.54 85.18 78.90 81.99 81.88 Table 2: Average accuracies over three held-out sets for English. system accuracy (%) Standard EM 82.4 + random restarts 84.5 (Goldwater and Griffiths, 2007) 85.2 our approach 87.4 + random restarts 87.1 Table 1: MAP-EM with a L0 norm achieves higher tagging accuracy on English than (2007) and much higher than standard EM. system zero parameters bigram types maximum possible 1389 – EM, 100 iterations 444 924 MAP-EM, 100 iterations 695 648 Table 3: MAP-EM with a smoothed L0 norm yields much smaller models than standard EM. We also carried out the same experiment with standard EM (Figure 3), where picking the point with the highest corpus probability achieves 84.5% accuracy. We also measured the minimization effect of the sparse prior against that o</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hyder</author>
<author>K Mahata</author>
</authors>
<title>An approximate L0 norm minimization algorithm for compressed sensing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="4700" citStr="Hyder and Mahata, 2009" startWordPosition="783" endWordPosition="786">f θ. We would like to find θˆ = arg max log P(w |θ) (2) θ X= arg max log P(w, t |θ) (3) θ t θˆ = arg min (− log P(w |θ) + αkθk0) (4) Figure 1: Ideal model-size term and its approximaθ tions. where kθk0, called the L0 norm of θ, simply counts the number of non-zero parameters in θ. The hyperparameter α controls the tradeoff between likelihood maximization and model minimization. Note the similarity of this objective function with MDL’s, where α would be the space (measured in nats) needed to describe one parameter of the model. Unfortunately, minimization of the L0 norm is known to be NP-hard (Hyder and Mahata, 2009). It is not smooth, making it unamenable to gradient-based optimization algorithms. Therefore, we use a smoothed approximation, kθk0 ≈ X i (1 − e−θi ) β (5) where 0 &lt; β ≤ 1 (Mohimani et al., 2007). For smaller values of β, this closely approximates the desired function (Figure 1). Inverting signs and ignoring constant terms, our objective function is now: θˆ = arg max θ X⎛⎜⎜⎜⎜⎜⎝log P(w |θ) + α i We can think of the approximate model size as a kind of prior: −θi P(θ) = exp α Pi e β (7) Z log P(θ) = α · X e β − logZ (8) −θi i Rwhere Z = dθ exp α Pi e −θi β is a normalization constant. Then our g</context>
</contexts>
<marker>Hyder, Mahata, 2009</marker>
<rawString>M. Hyder and K. Mahata. 2009. An approximate L0 norm minimization algorithm for compressed sensing. In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="3426" citStr="Merialdo (1994)" startWordPosition="553" endWordPosition="554">ch to the POS tagging problem, we obtain higher accuracies than both EM and Bayesian inference as reported by Goldwater and Griffiths (2007). On a Italian POS tagging task, we obtain even larger improvements. We find that our objective function correlates well with accuracy, suggesting that this technique might be useful for other problems. 2 MAP EM with Sparse Priors 2.1 Objective function In the unsupervised POS tagging task, we are given a word sequence w = w1, ... , wN and want to find the best tagging t = t1, ... , tN, where ti E T, the tag vocabulary. We adopt the problem formulation of Merialdo (1994), in which we are given a dictionary of possible tags for each word type. We define a bigram HMM N P(w, t |0) = P(w, t |0) · P(ti |ti−1) (1) i=1 In maximum likelihood estimation, the goal is to 209 Proceedings of the ACL 2010 Conference Short Papers, pages 209–214, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics find parameter estimates The EM algorithm can be used to find a solution. However, we would like to maximize likelihood and minimize the size of the model simultaneously. We define the size of a model as the number of non-zero probabilities in its par</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mohimani</author>
<author>M Babaie-Zadeh</author>
<author>C Jutten</author>
</authors>
<title>Fast sparse representation based on smoothed L0 norm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Conference on Independent Component Analysis and Signal Separation (ICA2007).</booktitle>
<contexts>
<context position="4896" citStr="Mohimani et al., 2007" startWordPosition="821" endWordPosition="824">kθk0, called the L0 norm of θ, simply counts the number of non-zero parameters in θ. The hyperparameter α controls the tradeoff between likelihood maximization and model minimization. Note the similarity of this objective function with MDL’s, where α would be the space (measured in nats) needed to describe one parameter of the model. Unfortunately, minimization of the L0 norm is known to be NP-hard (Hyder and Mahata, 2009). It is not smooth, making it unamenable to gradient-based optimization algorithms. Therefore, we use a smoothed approximation, kθk0 ≈ X i (1 − e−θi ) β (5) where 0 &lt; β ≤ 1 (Mohimani et al., 2007). For smaller values of β, this closely approximates the desired function (Figure 1). Inverting signs and ignoring constant terms, our objective function is now: θˆ = arg max θ X⎛⎜⎜⎜⎜⎜⎝log P(w |θ) + α i We can think of the approximate model size as a kind of prior: −θi P(θ) = exp α Pi e β (7) Z log P(θ) = α · X e β − logZ (8) −θi i Rwhere Z = dθ exp α Pi e −θi β is a normalization constant. Then our goal is to find the maximum a posterior parameter estimate, which we find using MAP-EM (Bishop, 2006): θˆ = arg max log P(w, θ) (9) θ = arg max (log P(w |θ) + log P(θ)) (10) θ Substituting (8) into</context>
</contexts>
<marker>Mohimani, Babaie-Zadeh, Jutten, 2007</marker>
<rawString>H. Mohimani, M. Babaie-Zadeh, and C. Jutten. 2007. Fast sparse representation based on smoothed L0 norm. In Proceedings of the 7th International Conference on Independent Component Analysis and Signal Separation (ICA2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="2026" citStr="Ravi and Knight (2009)" startWordPosition="309" endWordPosition="312">description length of the data given the model plus the description length of the model itself. It has been successfully shown that minimizing the model size in a Hidden Markov Model (HMM) for part-of-speech (POS) tagging leads to higher accuracies than simply running the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. More re2Computer Science Division University of California at Berkeley Soda Hall Berkeley, CA 94720 adpauls@eecs.berkeley.edu cently, Ravi and Knight (2009) alternately minimize the model using an integer linear program and maximize likelihood using EM to achieve the highest accuracies on the task so far. However, in the latter approach, because there is no single objective function to optimize, it is not entirely clear how to generalize this technique to other problems. In this paper, inspired by the MDL principle, we develop an objective function for generative models that captures both the description of the data by the model (log-likelihood) and the description of the model (model size). By using a simple prior that encourages sparsity, we ca</context>
<context position="5874" citStr="Ravi and Knight, 2009" startWordPosition="1017" endWordPosition="1020">e −θi β is a normalization constant. Then our goal is to find the maximum a posterior parameter estimate, which we find using MAP-EM (Bishop, 2006): θˆ = arg max log P(w, θ) (9) θ = arg max (log P(w |θ) + log P(θ)) (10) θ Substituting (8) into (10) and ignoring the constant term log Z, we get our objective function (6) again. We can exercise finer control over the sparsity of the tag-bigram and channel probability distributions by using a different α for each: In our experiments, we set αc = 0 since previous work has shown that minimizing the number of tag n-gram parameters is more important (Ravi and Knight, 2009; Goldwater and Griffiths, 2007). A common method for preferring smaller models is minimizing the L1 norm, Pi |θi|. However, for a model which is a product of multinomial distributions, the L1 norm is a constant. X |θi |= X θi i i X= t ⎛⎜⎜⎜⎜⎜X X ⎞⎟⎟⎟⎟⎟ ⎝ P(w |t) + ⎠ w t0 P(t0 |t) = 2|T| Therefore, we cannot use the L1 norm as part of the size term as the result will be the same as the EM algorithm. (6) −θi ⎞⎟⎟⎟⎟⎟ eβ ⎠ arg max log P(w |θ) + θ Xαc w,t X −P(w|t) eβ + αt t,t0 ! −P(t0|t) eβ (11) 210 2.2 Parameter optimization To optimize (11), we use MAP EM, which is an iterative search procedure. </context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="12813" citStr="Eisner (2005)" startWordPosition="2292" endWordPosition="2293">21, 878 words annotated with POS tags and a dictionary for each word type. Since this is all the available data, we could not tune the hyperparameters on a held-out data set. Using the hyperparameters tuned on English (αt = 80,/3 = 0.05), we obtained 89.7% tagging accuracy (see Table 4), which was a large improvement over 81.2% that standard EM achieved. When we tuned the hyperparameters on the test set, the best setting (αt = 120,8 = 0.05 gave an accuracy of 90.28%. 4 Conclusion A variety of other techniques in the literature have been applied to this unsupervised POS tagging task. Smith and Eisner (2005) use conditional random fields with contrastive estimation to achieve 0.89 0.88 0.87 0.86 0.85 0.84 0.83 0.82 0.81 0.8 0.79 0.78 212 αt 0.75 0.5 0.25 0.075 R 0.025 0.0075 0.005 0.0025 0.05 10 81.62 81.67 81.63 82.47 82.70 84.64 84.82 84.96 84.90 20 81.67 81.63 81.76 82.75 84.28 84.79 85.85 88.49 85.30 30 81.66 81.63 82.29 83.43 85.08 88.10 86.16 88.70 88.34 40 81.64 81.79 82.30 85.00 86.10 88.86 89.28 88.76 88.80 50 81.71 81.71 78.86 85.93 86.16 88.98 88.98 89.11 88.01 60 81.65 82.22 78.95 86.11 87.16 89.35 88.97 88.59 88.00 70 81.69 82.25 79.55 86.32 89.79 89.37 88.91 85.63 87.89 80 81.74 82.</context>
</contexts>
<marker>Eisner, 2005</marker>
<rawString>N. Smith. and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>