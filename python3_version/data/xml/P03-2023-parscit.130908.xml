<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050265">
<title confidence="0.997255">
A Speech Translation System with Mobile Wireless Clients
</title>
<author confidence="0.642228">
Kiyoshi Yamabana Ken Hanazawa Ryosuke Isotani
Seiya Osada Akitoshi Okumura Takao Watanabe
</author>
<affiliation confidence="0.297791">
Multimedia Research Laboratories
NEC Corporation
</affiliation>
<address confidence="0.948434">
4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan
</address>
<email confidence="0.999204">
{yamabana,Kanazawa,isotani,osada,okumura,watanabe}@ccm.cl.nec.co.jp
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999288571428571">
We developed a client-server speech
translation system with mobile wireless
clients. The system performs speech
translation between English and Japanese
of travel conversation and helps foreign
language communication in an area where
wireless LAN connection is available.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938553571429">
Automatic speech translation has been one of pro-
spective applications of the speech and language
technology (Lavie et al., 1997; Sugaya et al., 1999;
Watanabe et al. 2000). One common target appli-
cation has been communication assistance for trav-
elers in a foreign travel situation. In this situation,
the user will want to carry the translation device
and use it in an open space, rather than sit in a
computer room in front of a display.
An obvious approach is to build a stand-alone
speech translation device that is compact enough to
carry around (Watanabe et al., 2000; Isotani et al.,
2002). This approach is getting realistic with recent
progress of hardware devices such as fast embed-
ded CPUs and small batteries. However, because
of current limitation in the amount of available
computational resource, a stand-alone system has
to compromise the speech translation accuracy.
In a client-server speech translation system, it is
easier to provide better quality and more function-
ality to the users, using abundant computational
resource provided by the server machines. If the
situation allows use of powerful backend server
machines, that is preferable to stand-alone systems.
In the Verbmobil system (Wahlster, 2000), mo-
bile phones were used as the client device. The
voice input to a mobile phone is sent to the central
speech translation server as an ordinary speech
data over the telephone network. A disadvantage of
this approach is the narrow bandwidth of the tele-
phone network connection. Even though an acous-
tic model can be trained with telephone-quality
speech data, the lack of higher frequency informa-
tion in the human voice affects the speech recogni-
tion accuracy. Introduction of 3G mobile phones
will change this in the future, but it will take some
time before they come into wider use in the world.
Recently, wireless LAN is getting popular as a
means for Internet connection. Wireless LAN con-
nection, such as Wi-Fi (802.11b), provides high-
speed mobile Internet access in a limited area using
standard Internet Protocol. Its speed is generally
enough to transmit high-quality voice for speech
recognition. It is an attractive possibility for a cli-
ent-server speech translation system.
We developed a client-server speech translation
system using PDAs with wireless LAN connec-
tivity as clients. A client PDA sends a voice input
to the server over a wireless LAN connection, and
the server performs bidirectional speech translation
between Japanese and English.
In the next section we show an overview of the
system. Section 3 and section 4 describe the speech
recognition module and the machine translation
module, respectively. Section 5 is for discussion,
and the last section concludes the paper.
</bodyText>
<figureCaption confidence="0.986227">
Figure 1. Overview of the Server Modules
</figureCaption>
<sectionHeader confidence="0.918221" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999792347826087">
The current system performs Japanese/English bi-
directional speech translation of travel conversa-
tion. The server modules run on Windows servers,
and the client modules run on Windows CE PDAs.
Figure 1 shows an overview of the server mod-
ules. It consists of Japanese and English speech
recognition modules, Japanese-English bidirec-
tional machine translation modules, Japanese and
English speech synthesis modules and a system
control module. The dictionary contains about 50K
Japanese words and about 20K English words.
Figure 2 shows the relation of the client and the
server. The client PDA provides the user interface,
and the server performs speech translation process-
ing. The client is connected to the server by
TCP/IP over wireless LAN connection (802.11b).
The client PDA accepts the voice from the micro-
phone, sends it to the server, receives the translated
voice and outputs it from a speaker. Multiple cli-
ents can be connected to the server simultaneously.
Input speech is converted into digital data with
16bit monaural 22 KHz sampling. Transmission of
this raw data requires a speed more than 352Kbps
</bodyText>
<subsectionHeader confidence="0.661339">
Client PDA Translation Server
</subsectionHeader>
<figureCaption confidence="0.988035">
Figure 2. Relation between Client and Server
</figureCaption>
<bodyText confidence="0.999964692307692">
(bit per second). Although it is well below the gen-
erally available speed for wireless connection, we
applied G.722 speech compression algorithm to
reduce the rate to 88Kbps. The original G.722 re-
duces a 256Kbps stream to 64Kbps (namely 1/4),
and the same core algorithm is applied to a
352Kbps stream. With a lower transmission rate,
more clients can simultaneously perform speech
translation task. In addition, the task becomes do-
able even in a less stable (slower) connection. A
preliminary evaluation showed that this speech
compression did not affect the speech recognition
accuracy.
</bodyText>
<sectionHeader confidence="0.990671" genericHeader="method">
3 Speech Recognition Module
</sectionHeader>
<bodyText confidence="0.999990928571428">
The speech recognition module performs large vo-
cabulary continuous speech recognition of
conversational Japanese and English based on
HMMs and statistical language models. The
speech recognition dictionary contains about 50K
Japanese words and about 20K English words.
As shown in Figure 3, the speech recognition
module consists of acoustic models, language
models, word dictionaries, and a search engine.
The search engine is composed of two passes:
word graph generation and optimal word sequence
search. The search engine is language independent
while the acoustic models, the language models,
and the word dictionaries are language dependent.
</bodyText>
<subsectionHeader confidence="0.991773">
3.1 Acoustic Model
</subsectionHeader>
<bodyText confidence="0.999982090909091">
A speech signal is sampled at 22KHz, with MFCC
analysis frame rate of 11 ms. Spectral subtraction
and cepstrum mean normalization are applied to
remove stationary additive and multiplicative
noises. The feature set including MFCC and en-
ergy with their time derivatives is transformed by
linear discriminant analysis. The speech recognizer
supports triphone HMMs with tree based state
clustering on phonetic contexts. The state emission
probability is represented by gaussian mixtures
with diagonal covariance matrices.
</bodyText>
<subsectionHeader confidence="0.994716">
3.2 Language Model
</subsectionHeader>
<bodyText confidence="0.9999608">
To cover the wide variety of colloquial and do-
main-specific expressions in travel conversation,
large text corpora of travel conversation in Japa-
nese and English has been developed and used to
train the language models. The total size of the
</bodyText>
<figure confidence="0.986377466666667">
Japanese
Speaker
Japanese
speech
recognition
System Integration Module
Japanese
speech
Synthesis
Japanese
to English
Translation
English
to Japanese
Translation
English
speech
recognition
English
Speech
Synthesis
English
Speaker
Input Speech
Wireless
Network
esult
speech input
recognition result
&amp;quot;Where is the postoffice?&amp;quot;
</figure>
<figureCaption confidence="0.999713">
Figure 3. Speech Recognition Module
</figureCaption>
<bodyText confidence="0.995309818181818">
corpora is around 100K sentences for each lan-
guage. The corpora contain travel conversation in
various situations such as hotel, restaurant, shop-
ping, transportation, entertainment, as well as gen-
eral expressions observed in oral communication.
Word bigram and word trigram language mod-
els were trained using these corpora. Class n-grams
were used for smoothing word n-grams. The
classes were defined based on parts of speech and
partly on manually defined semantic classes spe-
cific to the travel conversation domain.
</bodyText>
<subsectionHeader confidence="0.999758">
3.3 Search Engine
</subsectionHeader>
<bodyText confidence="0.999956714285714">
The search engine performs two-stage processing.
On the first stage, input speech is decoded to gen-
erate a word candidate graph using the dictionary,
the acoustic model and the bigram language model.
On the second stage, the graph is searched to find
an optimal word sequence using the trigram lan-
guage model.
</bodyText>
<sectionHeader confidence="0.992938" genericHeader="method">
4 Machine Translation Module
</sectionHeader>
<bodyText confidence="0.999942024390244">
The translation module accepts a recognized ex-
pression from the speech recognition modules, and
performs bi-directional translation between Japa-
nese and English. The output is sent to the corre-
sponding speech synthesis module.
In travel conversation, colloquial expressions
are frequently used, which seldom appear in writ-
ten texts such as newspapers. The module has to
deal with highly word-specific phenomena in-
cluded in such colloquial and idiomatic expres-
sions, as well as general expressions obeying more
abstract, standard grammar. In other words, the
module is required to deal with both instance-
specific example-like knowledge and abstract rule-
like knowledge at the same time.
With this requirement in mind, we employed
the Lexicalized Tree AutoMata-based Grammar
(LTAM Grammar) as the grammar formalism
(Yamabana et al., 2000). This method is in line
with the strong-lexicalization approach to the
grammar (Schabes et al., 1988), where each gram-
mar rule (tree) is associated with at least one word,
making all the rules lexical. An advantage of the
LTAM Grammars to other strongly lexicalized
grammars is an existence of a simple bottom-up
chart-parsing algorithm, which is a natural exten-
sion of the context-free grammar case.
Figure 4 is an overview of the bi-directional
machine translation module. It uses a combined
lexicalized grammar dictionary, instead of having
the grammar and the dictionary separately. Large
text corpora of travel conversation are used to
build the bilingual dictionary and to improve the
translation quality. The Japanese to English trans-
lation dictionary contains about 150K words, and
the English to Japanese dictionary contains about
70K words. The translation grammar has been built
on generic language phenomena, and expanded for
phenomena as will appear in a dialogue, such as
ellipsis, idioms, fixed expressions, imperatives,
requests and polite expressions.
</bodyText>
<sectionHeader confidence="0.989866" genericHeader="evaluation">
5 Discussions
</sectionHeader>
<bodyText confidence="0.9993824">
We made several experiments of the system using
PC servers with Pentium 4 2GHz CPU and 512MB
RAM. The whole system showed a good perform-
ance with respect to the speed and the accuracy.
We performed a preliminary evaluation of the
</bodyText>
<figure confidence="0.991930836734694">
travel
travel
coersatio
conversation
text textcorus
corpus
seec
speech
cors
corpus
wor ord
ictionar
dictionary
lanuae
language
oel
model
acostic
acoustic
oel
model
worord ra
graph
eerti
generation
otialoptimal or
word
seece sequencesearc
search
word
graph
kare
taberu
Input
He eats dinner
Morphological Analysis
Shared Trees
Syntactic Analysis &amp; Transfer
Generation
Output
Kare-ga yuusyoku-wo taber-u
he-subj dinner-dob eat
He
Lexicalized
Grammar
Rules
Dictionary
eats dinner
yuusyoku
</figure>
<figureCaption confidence="0.999922">
Figure 4. Machine Translation Module
</figureCaption>
<bodyText confidence="0.999944733333334">
speech recognition module and the machine trans-
lation module. Speech recognition accuracy was
measured using 1,800 utterances by 10 Japanese
male speakers and 10 English speakers, respec-
tively. The word accuracy was 97.5% for the Japa-
nese speakers and 92.0% for the English speakers.
A preliminary evaluation of the machine trans-
lation quality was conducted using 500 randomly-
chosen sentences from travel conversation corpus.
A bilingual evaluator classified the translation re-
sults into three categories: Good, Understandable
and Bad. A Good sentence should have no syntac-
tic errors and its meaning has to be correctly un-
derstood. Understandable sentences may have
some errors, but the central meaning of the original
sentence must be conveyed without misunder-
standing. If the translation does not convey the
central meaning of the original text, or if it causes
a misunderstanding, it is classified as Bad. With
this subjective measure, the ratio of Good or Un-
derstandable sentences, i.e. sentences with which
the meaning of the original sentence was properly
conveyed, was 88% for the Japanese-to-English
translation, and 90% for the English-to-Japanese
translation. The ratio of Good sentences was 66%
for the J to E translation, and 74% for the E to J
translation.
Compared to a stand-alone system, the devel-
oped system has better speech translation accuracy,
and a quick response. In addition, future expansion
to other domains and languages, for a wider use of
speech translation, will be easier. An obvious dis-
advantage is the limitation of the area where a cli-
ent can be used. However, it will not be an
essential obstruction if the service area is naturally
limited, for example, to the inside of a shop or a
building. In this situation, the system will be able
to provide a location-sensitive service, because the
client location can be easily assumed with the
Wireless LAN connectivity.
One problem with using PDAs as clients is an
inferior quality of the built-in microphone. In the
developed system, we modified the PDA hardware
so that an external compact microphone is usable.
This problem may disappear in the near future,
since some PDAs are equipped with external mi-
crophone jacks recently. Another remaining issue
is an inferior reliability of the wireless connection,
which should be solved in the near future.
An ideal client will have both functions as a
stand-alone speech translation device and a client
device for a client-server speech translation system.
The stand-alone function will be used in a usual
travel situation. In a hot spot where wireless con-
nection is available, it will behave as a client to
provide better-quality location-sensitive translation.
A mobile phone-based speech translation will be
used to talk to a party in a distant place. Thus these
functions will have respective roles depending on
the situation speech translation is used.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969">
We developed a client-server speech translation
system where mobile clients are connected with
the server via Wireless LAN connection. Further
work includes a system design where a client op-
eration and a stand-alone operation are compatible
so that the user can use appropriate function de-
pending on the situation.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999813517241379">
R. Isotani, K. Yamabana, S. Ando, K. Hanazawa, S.
Ishikawa, T. Emori, H. Hattori, A. Okumura and T.
Watanabe,. 2002. &amp;quot;An automatic speech translation
system on PDAs for travel conversation&amp;quot;, Proc.
ICMI-02, pp. 211-216.
A. Lavie, A. Waibel, L. Levin, M. Finke, D. Gates, M.
Gavalda, T. Zeppenfeld, and P. Zahn. 1997. &amp;quot;JANUS
III: Speech-to-speech translation in multiple lan-
guages&amp;quot;, Proc. ICASSP-97, pp. 99-102.
Y. Schabes, A. Abeille and A. K. Joshi. 1988. &amp;quot;Parsing
Strategies with &apos;Lexicalized&apos; Grammars&amp;quot;, Proc.
COLING&apos;88, pp.578-583.
F. Sugaya, T. Takezawa, A. Yokoo, and S. Yamamoto.
1999. &amp;quot;End-to-end evaluation in ATR-MATRIX:
Speech translation system between English and Japa-
nese&amp;quot;, Proc. Eurospeech-99, pp. 2431-2434.
W. Wahlster. 2000. &amp;quot;Mobile Speech-to-Speech Transla-
tion of Spontaneous Dialogs: An Overview of the Fi-
nal Verbmobil System&amp;quot;, In &amp;quot;Verbmobil: Foundations
of Speech-to-Speech Translation&amp;quot;, ed. W. Wahlster,
pp. 3-21, Springer.
T. Watanabe, A. Okumura, S. Sakai, K. Yamabana, S.
Doi, and K. Hanazawa. 2000. &amp;quot;An automatic inter-
pretation system for travel conversation&amp;quot;, Proc.
ICSLP-2000, pp. IV 444-447.
K. Yamabana, S. Ando and K. Mimura. 2000. &amp;quot;Lexical-
ized Tree Automata-based Grammars for Translating
Conversational Texts&amp;quot;, Proc. COLING 2000, pp 926-
932.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563288">
<title confidence="0.999803">A Speech Translation System with Mobile Wireless Clients</title>
<author confidence="0.79373">Kiyoshi Yamabana Ken Hanazawa Ryosuke Isotani Seiya Osada Akitoshi Okumura Takao Watanabe</author>
<affiliation confidence="0.995844">Multimedia Research Laboratories NEC Corporation</affiliation>
<address confidence="0.995445">4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan</address>
<email confidence="0.984131">yamabana@ccm.cl.nec.co.jp</email>
<email confidence="0.984131">Kanazawa@ccm.cl.nec.co.jp</email>
<email confidence="0.984131">isotani@ccm.cl.nec.co.jp</email>
<email confidence="0.984131">osada@ccm.cl.nec.co.jp</email>
<email confidence="0.984131">okumura@ccm.cl.nec.co.jp</email>
<email confidence="0.984131">watanabe@ccm.cl.nec.co.jp</email>
<abstract confidence="0.99517325">We developed a client-server speech translation system with mobile wireless clients. The system performs speech translation between English and Japanese of travel conversation and helps foreign language communication in an area where wireless LAN connection is available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Isotani</author>
<author>K Yamabana</author>
<author>S Ando</author>
<author>K Hanazawa</author>
<author>S Ishikawa</author>
<author>T Emori</author>
<author>H Hattori</author>
<author>A Okumura</author>
<author>T Watanabe</author>
</authors>
<title>An automatic speech translation system on PDAs for travel conversation&amp;quot;,</title>
<date>2002</date>
<booktitle>Proc. ICMI-02,</booktitle>
<pages>211--216</pages>
<contexts>
<context position="1208" citStr="Isotani et al., 2002" startWordPosition="170" endWordPosition="173">available. 1 Introduction Automatic speech translation has been one of prospective applications of the speech and language technology (Lavie et al., 1997; Sugaya et al., 1999; Watanabe et al. 2000). One common target application has been communication assistance for travelers in a foreign travel situation. In this situation, the user will want to carry the translation device and use it in an open space, rather than sit in a computer room in front of a display. An obvious approach is to build a stand-alone speech translation device that is compact enough to carry around (Watanabe et al., 2000; Isotani et al., 2002). This approach is getting realistic with recent progress of hardware devices such as fast embedded CPUs and small batteries. However, because of current limitation in the amount of available computational resource, a stand-alone system has to compromise the speech translation accuracy. In a client-server speech translation system, it is easier to provide better quality and more functionality to the users, using abundant computational resource provided by the server machines. If the situation allows use of powerful backend server machines, that is preferable to stand-alone systems. In the Verb</context>
</contexts>
<marker>Isotani, Yamabana, Ando, Hanazawa, Ishikawa, Emori, Hattori, Okumura, Watanabe, 2002</marker>
<rawString>R. Isotani, K. Yamabana, S. Ando, K. Hanazawa, S. Ishikawa, T. Emori, H. Hattori, A. Okumura and T. Watanabe,. 2002. &amp;quot;An automatic speech translation system on PDAs for travel conversation&amp;quot;, Proc. ICMI-02, pp. 211-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Waibel</author>
<author>L Levin</author>
<author>M Finke</author>
<author>D Gates</author>
<author>M Gavalda</author>
<author>T Zeppenfeld</author>
<author>P Zahn</author>
</authors>
<title>JANUS III: Speech-to-speech translation in multiple languages&amp;quot;,</title>
<date>1997</date>
<booktitle>Proc. ICASSP-97,</booktitle>
<pages>99--102</pages>
<contexts>
<context position="740" citStr="Lavie et al., 1997" startWordPosition="88" endWordPosition="91">i Okumura Takao Watanabe Multimedia Research Laboratories NEC Corporation 4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan {yamabana,Kanazawa,isotani,osada,okumura,watanabe}@ccm.cl.nec.co.jp Abstract We developed a client-server speech translation system with mobile wireless clients. The system performs speech translation between English and Japanese of travel conversation and helps foreign language communication in an area where wireless LAN connection is available. 1 Introduction Automatic speech translation has been one of prospective applications of the speech and language technology (Lavie et al., 1997; Sugaya et al., 1999; Watanabe et al. 2000). One common target application has been communication assistance for travelers in a foreign travel situation. In this situation, the user will want to carry the translation device and use it in an open space, rather than sit in a computer room in front of a display. An obvious approach is to build a stand-alone speech translation device that is compact enough to carry around (Watanabe et al., 2000; Isotani et al., 2002). This approach is getting realistic with recent progress of hardware devices such as fast embedded CPUs and small batteries. Howeve</context>
</contexts>
<marker>Lavie, Waibel, Levin, Finke, Gates, Gavalda, Zeppenfeld, Zahn, 1997</marker>
<rawString>A. Lavie, A. Waibel, L. Levin, M. Finke, D. Gates, M. Gavalda, T. Zeppenfeld, and P. Zahn. 1997. &amp;quot;JANUS III: Speech-to-speech translation in multiple languages&amp;quot;, Proc. ICASSP-97, pp. 99-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>A Abeille</author>
<author>A K Joshi</author>
</authors>
<title>Parsing Strategies with &apos;Lexicalized&apos; Grammars&amp;quot;,</title>
<date>1988</date>
<booktitle>Proc. COLING&apos;88,</booktitle>
<pages>578--583</pages>
<contexts>
<context position="8823" citStr="Schabes et al., 1988" startWordPosition="1339" endWordPosition="1342">n written texts such as newspapers. The module has to deal with highly word-specific phenomena included in such colloquial and idiomatic expressions, as well as general expressions obeying more abstract, standard grammar. In other words, the module is required to deal with both instancespecific example-like knowledge and abstract rulelike knowledge at the same time. With this requirement in mind, we employed the Lexicalized Tree AutoMata-based Grammar (LTAM Grammar) as the grammar formalism (Yamabana et al., 2000). This method is in line with the strong-lexicalization approach to the grammar (Schabes et al., 1988), where each grammar rule (tree) is associated with at least one word, making all the rules lexical. An advantage of the LTAM Grammars to other strongly lexicalized grammars is an existence of a simple bottom-up chart-parsing algorithm, which is a natural extension of the context-free grammar case. Figure 4 is an overview of the bi-directional machine translation module. It uses a combined lexicalized grammar dictionary, instead of having the grammar and the dictionary separately. Large text corpora of travel conversation are used to build the bilingual dictionary and to improve the translatio</context>
</contexts>
<marker>Schabes, Abeille, Joshi, 1988</marker>
<rawString>Y. Schabes, A. Abeille and A. K. Joshi. 1988. &amp;quot;Parsing Strategies with &apos;Lexicalized&apos; Grammars&amp;quot;, Proc. COLING&apos;88, pp.578-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sugaya</author>
<author>T Takezawa</author>
<author>A Yokoo</author>
<author>S Yamamoto</author>
</authors>
<title>End-to-end evaluation in ATR-MATRIX: Speech translation system between English and Japanese&amp;quot;,</title>
<date>1999</date>
<booktitle>Proc. Eurospeech-99,</booktitle>
<pages>2431--2434</pages>
<contexts>
<context position="761" citStr="Sugaya et al., 1999" startWordPosition="92" endWordPosition="95">nabe Multimedia Research Laboratories NEC Corporation 4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan {yamabana,Kanazawa,isotani,osada,okumura,watanabe}@ccm.cl.nec.co.jp Abstract We developed a client-server speech translation system with mobile wireless clients. The system performs speech translation between English and Japanese of travel conversation and helps foreign language communication in an area where wireless LAN connection is available. 1 Introduction Automatic speech translation has been one of prospective applications of the speech and language technology (Lavie et al., 1997; Sugaya et al., 1999; Watanabe et al. 2000). One common target application has been communication assistance for travelers in a foreign travel situation. In this situation, the user will want to carry the translation device and use it in an open space, rather than sit in a computer room in front of a display. An obvious approach is to build a stand-alone speech translation device that is compact enough to carry around (Watanabe et al., 2000; Isotani et al., 2002). This approach is getting realistic with recent progress of hardware devices such as fast embedded CPUs and small batteries. However, because of current</context>
</contexts>
<marker>Sugaya, Takezawa, Yokoo, Yamamoto, 1999</marker>
<rawString>F. Sugaya, T. Takezawa, A. Yokoo, and S. Yamamoto. 1999. &amp;quot;End-to-end evaluation in ATR-MATRIX: Speech translation system between English and Japanese&amp;quot;, Proc. Eurospeech-99, pp. 2431-2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>Mobile Speech-to-Speech Translation of Spontaneous Dialogs: An Overview of the Final Verbmobil System&amp;quot;, In &amp;quot;Verbmobil: Foundations of Speech-to-Speech Translation&amp;quot;,</title>
<date>2000</date>
<pages>3--21</pages>
<editor>ed. W. Wahlster,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1837" citStr="Wahlster, 2000" startWordPosition="265" endWordPosition="266">h is getting realistic with recent progress of hardware devices such as fast embedded CPUs and small batteries. However, because of current limitation in the amount of available computational resource, a stand-alone system has to compromise the speech translation accuracy. In a client-server speech translation system, it is easier to provide better quality and more functionality to the users, using abundant computational resource provided by the server machines. If the situation allows use of powerful backend server machines, that is preferable to stand-alone systems. In the Verbmobil system (Wahlster, 2000), mobile phones were used as the client device. The voice input to a mobile phone is sent to the central speech translation server as an ordinary speech data over the telephone network. A disadvantage of this approach is the narrow bandwidth of the telephone network connection. Even though an acoustic model can be trained with telephone-quality speech data, the lack of higher frequency information in the human voice affects the speech recognition accuracy. Introduction of 3G mobile phones will change this in the future, but it will take some time before they come into wider use in the world. R</context>
</contexts>
<marker>Wahlster, 2000</marker>
<rawString>W. Wahlster. 2000. &amp;quot;Mobile Speech-to-Speech Translation of Spontaneous Dialogs: An Overview of the Final Verbmobil System&amp;quot;, In &amp;quot;Verbmobil: Foundations of Speech-to-Speech Translation&amp;quot;, ed. W. Wahlster, pp. 3-21, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>A Okumura</author>
<author>S Sakai</author>
<author>K Yamabana</author>
<author>S Doi</author>
<author>K Hanazawa</author>
</authors>
<title>An automatic interpretation system for travel conversation&amp;quot;,</title>
<date>2000</date>
<booktitle>Proc. ICSLP-2000,</booktitle>
<pages>444--447</pages>
<contexts>
<context position="784" citStr="Watanabe et al. 2000" startWordPosition="96" endWordPosition="99">rch Laboratories NEC Corporation 4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan {yamabana,Kanazawa,isotani,osada,okumura,watanabe}@ccm.cl.nec.co.jp Abstract We developed a client-server speech translation system with mobile wireless clients. The system performs speech translation between English and Japanese of travel conversation and helps foreign language communication in an area where wireless LAN connection is available. 1 Introduction Automatic speech translation has been one of prospective applications of the speech and language technology (Lavie et al., 1997; Sugaya et al., 1999; Watanabe et al. 2000). One common target application has been communication assistance for travelers in a foreign travel situation. In this situation, the user will want to carry the translation device and use it in an open space, rather than sit in a computer room in front of a display. An obvious approach is to build a stand-alone speech translation device that is compact enough to carry around (Watanabe et al., 2000; Isotani et al., 2002). This approach is getting realistic with recent progress of hardware devices such as fast embedded CPUs and small batteries. However, because of current limitation in the amou</context>
</contexts>
<marker>Watanabe, Okumura, Sakai, Yamabana, Doi, Hanazawa, 2000</marker>
<rawString>T. Watanabe, A. Okumura, S. Sakai, K. Yamabana, S. Doi, and K. Hanazawa. 2000. &amp;quot;An automatic interpretation system for travel conversation&amp;quot;, Proc. ICSLP-2000, pp. IV 444-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamabana</author>
<author>S Ando</author>
<author>K Mimura</author>
</authors>
<title>Lexicalized Tree Automata-based Grammars for Translating Conversational Texts&amp;quot;,</title>
<date>2000</date>
<booktitle>Proc. COLING</booktitle>
<pages>926--932</pages>
<contexts>
<context position="8721" citStr="Yamabana et al., 2000" startWordPosition="1323" endWordPosition="1326">hesis module. In travel conversation, colloquial expressions are frequently used, which seldom appear in written texts such as newspapers. The module has to deal with highly word-specific phenomena included in such colloquial and idiomatic expressions, as well as general expressions obeying more abstract, standard grammar. In other words, the module is required to deal with both instancespecific example-like knowledge and abstract rulelike knowledge at the same time. With this requirement in mind, we employed the Lexicalized Tree AutoMata-based Grammar (LTAM Grammar) as the grammar formalism (Yamabana et al., 2000). This method is in line with the strong-lexicalization approach to the grammar (Schabes et al., 1988), where each grammar rule (tree) is associated with at least one word, making all the rules lexical. An advantage of the LTAM Grammars to other strongly lexicalized grammars is an existence of a simple bottom-up chart-parsing algorithm, which is a natural extension of the context-free grammar case. Figure 4 is an overview of the bi-directional machine translation module. It uses a combined lexicalized grammar dictionary, instead of having the grammar and the dictionary separately. Large text c</context>
</contexts>
<marker>Yamabana, Ando, Mimura, 2000</marker>
<rawString>K. Yamabana, S. Ando and K. Mimura. 2000. &amp;quot;Lexicalized Tree Automata-based Grammars for Translating Conversational Texts&amp;quot;, Proc. COLING 2000, pp 926-932.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>