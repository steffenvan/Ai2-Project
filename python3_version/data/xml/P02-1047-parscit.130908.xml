<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003976">
<note confidence="0.951995">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 368-375.
</note>
<title confidence="0.998363">
An Unsupervised Approach to Recognizing Discourse Relations
</title>
<author confidence="0.994378">
Daniel Marcu and Abdessamad Echihabi
</author>
<affiliation confidence="0.8384676">
Information Sciences Institute and
Department of Computer Science
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
</affiliation>
<email confidence="0.998773">
marcu,echihabi @isi.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999809416666667">
We present an unsupervised approach to
recognizing discourse relations of CON-
TRAST, EXPLANATION-EVIDENCE, CON-
DITION and ELABORATION that hold be-
tween arbitrary spans of texts. We show
that discourse relation classifiers trained
on examples that are automatically ex-
tracted from massive amounts of text can
be used to distinguish between some of
these relations with accuracies as high as
93%, even when the relations are not ex-
plicitly marked by cue phrases.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998575">
In the field of discourse research, it is now widely
agreed that sentences/clauses are usually not un-
derstood in isolation, but in relation to other sen-
tences/clauses. Given the high level of interest in
explaining the nature of these relations and in pro-
viding definitions for them (Mann and Thompson,
1988; Hobbs, 1990; Martin, 1992; Lascarides and
Asher, 1993; Hovy and Maier, 1993; Knott and
Sanders, 1998), it is surprising that there are no ro-
bust programs capable of identifying discourse rela-
tions that hold between arbitrary spans of text. Con-
sider, for example, the sentence/clause pairs below.
</bodyText>
<listItem confidence="0.891980125">
a. Such standards would preclude arms sales to (1)
states like Libya, which is also currently sub-
ject to a U.N. embargo.
b. But states like Rwanda before its present crisis
would still be able to legally buy arms.
a. South Africa can afford to forgo sales of guns (2)
and grenades
b. because it actually makes most of its profits
</listItem>
<bodyText confidence="0.991330408450705">
from the sale of expensive, high-technology
systems like laser-designated missiles, air-
craft electronic warfare systems, tactical ra-
dios, anti-radiation bombs and battlefield mo-
bility systems.
In these examples, the discourse markers But and
because help us figure out that a CONTRAST re-
lation holds between the text spans in (1) and an
EXPLANATION-EVIDENCE relation holds between
the spans in (2). Unfortunately, cue phrases do not
signal all relations in a text. In the corpus of Rhetori-
cal Structure trees (www.isi.edu/✂ marcu/discourse/)
built by Carlson et al. (2001), for example, we have
observed that only 61 of 238 CONTRAST relations
and 79 out of 307 EXPLANATION-EVIDENCE rela-
tions that hold between two adjacent clauses were
marked by a cue phrase.
So what shall we do when no discourse
markers are used? If we had access to ro-
bust semantic interpreters, we could, for
example, infer from sentence 1.a that “can-
not buy arms legally(libya)”, infer from sen-
tence 1.b that “can buy arms legally(rwanda)”, use
our background knowledge in order to infer that
“similar(libya,rwanda)”, and apply Hobbs’s (1990)
definitions of discourse relations to arrive at the
conclusion that a CONTRAST relation holds between
the sentences in (1). Unfortunately, the state of the
art in NLP does not provide us access to semantic
interpreters and general purpose knowledge bases
that would support these kinds of inferences.
The discourse relation definitions proposed by
others (Mann and Thompson, 1988; Lascarides
and Asher, 1993; Knott and Sanders, 1998) are
not easier to apply either because they assume
the ability to automatically derive, in addition to
the semantics of the text spans, the intentions and
illocutions associated with them as well.
In spite of the difficulty of determining the dis-
course relations that hold between arbitrary text
spans, it is clear that such an ability is important
in many applications. First, a discourse relation
recognizer would enable the development of im-
proved discourse parsers and, consequently, of high
performance single document summarizers (Marcu,
2000). In multidocument summarization (DUC,
2002), it would enable the development of summa-
rization programs capable of identifying contradic-
tory statements both within and across documents
and of producing summaries that reflect not only
the similarities between various documents, but also
their differences. In question-answering, it would
enable the development of systems capable of an-
swering sophisticated, non-factoid queries, such as
“what were the causes ofX?” or “what contradicts
Y?”, which are beyond the state of the art of current
systems (TREC, 2001).
In this paper, we describe experiments aimed at
building robust discourse-relation classification sys-
tems. To build such systems, we train a family of
Naive Bayes classifiers on a large set of examples
that are generated automatically from two corpora:
a corpus of 41,147,805 English sentences that have
no annotations, and BLIPP, a corpus of 1,796,386
automatically parsed English sentences (Charniak,
2000), which is available from the Linguistic Data
Consortium (www.ldc.upenn.edu). We study empir-
ically the adequacy of various features for the task
of discourse relation classification and we show that
some discourse relations can be correctly recognized
with accuracies as high as 93%.
</bodyText>
<sectionHeader confidence="0.777374" genericHeader="method">
2 Discourse relation definitions and
generation of training data
</sectionHeader>
<subsectionHeader confidence="0.988181">
2.1 Background
</subsectionHeader>
<bodyText confidence="0.999247773333333">
In order to build a discourse relation classifier, one
first needs to decide what relation definitions one
is going to use. In Section 1, we simply relied on
the reader’s intuition when we claimed that a CON-
TRAST relation holds between the sentences in (1).
In reality though, associating a discourse relation
with a text span pair is a choice that is clearly in-
fluenced by the theoretical framework one is willing
to adopt.
If we adopt, for example, Knott and
Sanders’s (1998) account, we would say that
the relation between sentences 1.a and 1.b is
ADDITIVE, because no causal connection exists
between the two sentences, PRAGMATIC, because
the relation pertains to illocutionary force and
not to the propositional content of the sentences,
and NEGATIVE, because the relation involves a
CONTRAST between the two sentences. In the
same framework, the relation between clauses 2.a
and 2.b will be labeled as CAUSAL-SEMANTIC-
POSITIVE-NONBASIC. In Lascarides and Asher’s
theory (1993), we would label the relation between
2.a and 2.b as EXPLANATION because the event in
2.b explains why the event in 2.a happened (perhaps
by CAUSING it). In Hobbs’s theory (1990), we
would also label the relation between 2.a and 2.b
as EXPLANATION because the event asserted by
2.b CAUSED or could CAUSE the event asserted in
2.a. And in Mann and Thompson theory (1988), we
would label sentence pairs 1.a, 1.b as CONTRAST
because the situations presented in them are the
same in many respects (the purchase of arms),
because the situations are different in some respects
(Libya cannot buy arms legally while Rwanda can),
and because these situations are compared with
respect to these differences. By a similar line of
reasoning, we would label the relation between 2.a
and 2.b as EVIDENCE.
The discussion above illustrates two points. First,
it is clear that although current discourse theories are
built on fundamentally different principles, they all
share some common intuitions. Sure, some theo-
ries talk about “negative polarity” while others about
“contrast”. Some theories refer to “causes”, some to
“potential causes”, and some to “explanations”. But
ultimately, all these theories acknowledge that there
are such things as CONTRAST, CAUSE, and EXPLA-
NATION relations. Second, given the complexity of
the definitions these theories propose, it is clear why
it is difficult to build programs that recognize such
relations in unrestricted texts. Current NLP tech-
niques do not enable us to reliably infer from sen-
tence 1.a that “cannot buy arms legally(libya)” and
do not give us access to general purpose knowledge
bases that assert that “similar(libya,rwanda)”.
The approach we advocate in this paper is in some
respects less ambitious than current approaches to
discourse relations because it relies upon a much
smaller set of relations than those used by Mann and
Thompson (1988) or Martin (1992). In our work,
we decide to focus only on four types of relations,
which we call: CONTRAST, CAUSE-EXPLANATION-
EVIDENCE (CEV), CONDITION, and ELABORA-
TION. (We define these relations in Section 2.2.) In
other respects though, our approach is more ambi-
tious because it focuses on the problem of recog-
nizing such discourse relations in unrestricted texts.
In other words, given as input sentence pairs such
as those shown in (1)–(2), we develop techniques
and programs that label the relations that hold be-
tween these sentence pairs as CONTRAST, CAUSE-
EXPLANATION-EVIDENCE, CONDITION, ELABO-
RATION or NONE-OF-THE-ABOVE, even when the
discourse relations are not explicitly signalled by
discourse markers.
</bodyText>
<subsectionHeader confidence="0.992558">
2.2 Discourse relation definitions
</subsectionHeader>
<bodyText confidence="0.999970511111111">
The discourse relations we focus on are defined
at a much coarser level of granularity than in
most discourse theories. For example, we con-
sider that a CONTRAST relation holds between two
text spans if one of the following relations holds:
CONTRAST, ANTITHESIS, CONCESSION, or OTH-
ERWISE, as defined by Mann and Thompson (1988),
CONTRAST or VIOLATED EXPECTATION, as defined
by Hobbs (1990), or any of the relations character-
ized by this regular expression of cognitive prim-
itives, as defined by Knott and Sanders (1998):
(CAUSAL ADDITIVE) – (SEMANTIC PRAGMATIC)
– NEGATIVE. In other words, in our approach, we do
not distinguish between contrasts of semantic and
pragmatic nature, contrasts specific to violated ex-
pectations, etc. Table 1 shows the definitions of the
relations we considered.
The advantage of operating with coarsely defined
discourse relations is that it enables us to automat-
ically construct relatively low-noise datasets that
can be used for learning. For example, by extract-
ing sentence pairs that have the keyword “But” at
the beginning of the second sentence, as the sen-
tence pair shown in (1), we can automatically col-
lect many examples of CONTRAST relations. And by
extracting sentences that contain the keyword “be-
cause”, we can automatically collect many examples
of CAUSE-EXPLANATION-EVIDENCE relations. As
previous research in linguistics (Halliday and Hasan,
1976; Schiffrin, 1987) and computational linguis-
tics (Marcu, 2000) show, some occurrences of “but”
and “because” do not have a discourse function; and
others signal other relations than CONTRAST and
CAUSE-EXPLANATION. So we can expect the ex-
amples we extract to be noisy. However, empiri-
cal work of Marcu (2000) and Carlson et al. (2001)
suggests that the majority of occurrences of “but”,
for example, do signal CONTRAST relations. (In the
RST corpus built by Carlson et al. (2001), 89 out of
the 106 occurrences of “but” that occur at the begin-
ning of a sentence signal a CONTRAST relation that
holds between the sentence that contains the word
“but” and the sentence that precedes it.) Our hope
is that simple extraction methods are sufficient for
collecting low-noise training corpora.
</bodyText>
<subsectionHeader confidence="0.999246">
2.3 Generation of training data
</subsectionHeader>
<bodyText confidence="0.958418958333333">
In order to collect training cases, we mined in an
unsupervised manner two corpora. The first corpus,
which we call Raw, is a corpus of 1 billion words of
unannotated English (41,147,805 sentences) that we
created by catenating various corpora made avail-
able over the years by the Linguistic Data Consor-
tium. The second, called BLIPP, is a corpus of only
1,796,386 sentences that were parsed automatically
by Charniak (2000). We extracted from both cor-
pora all adjacent sentence pairs that contained the
cue phrase “But” at the beginning of the second sen-
tence and we automatically labeled the relation be-
tween the two sentence pairs as CONTRAST. We also
extracted all the sentences that contained the word
“but” in the middle of a sentence; we split each ex-
tracted sentence into two spans, one containing the
words from the beginning of the sentence to the oc-
currence of the keyword “but” and one containing
the words from the occurrence of “but” to the end
of the sentence; and we labeled the relation between
the two resulting text spans as CONTRAST as well.
Table 2 lists some of the cue phrases we
used in order to extract CONTRAST, CAUSE-
EXPLANATION-EVIDENCE, ELABORATION, and
</bodyText>
<table confidence="0.997560230769231">
CONTRAST CAUSE-EXPLANATION-EVIDENCE ELABORATION CONDITION
ANTITHESIS (M&amp;T) EVIDENCE (M&amp;T) ELABORATION (M&amp;T) CONDITION (M&amp;T)
CONCESSION (M&amp;T) VOLITIONAL-CAUSE (M&amp;T) EXPANSION (Ho)
OTHERWISE (M&amp;T) NONVOLITIONAL-CAUSE (M&amp;T) EXEMPLIFICATION (Ho)
CONTRAST (M&amp;T) VOLITIONAL-RESULT (M&amp;T) ELABORATION (A&amp;L)
VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&amp;T)
EXPLANATION (Ho)
( CAUSAL ADDITIVE) - RESULT (A&amp;L)
( SEMANTIC PRAGMATIC) - EXPLANATION (A&amp;L)
NEGATIVE (K&amp;S)
CAUSAL-
(SEMANTIC PRAGMATIC) -
POSITIVE (K&amp;S)
</table>
<tableCaption confidence="0.950761333333333">
Table 1: Relation definitions as union of definitions proposed by other researchers (M&amp;T – (Mann and
Thompson, 1988); Ho – (Hobbs, 1990); A&amp;L – (Lascarides and Asher, 1993); K&amp;S – (Knott and Sanders,
1998)).
</tableCaption>
<table confidence="0.999955818181818">
CONTRAST – 3,881,588 examples
[BOS EOS] [BOS But EOS]
[BOS ] [but EOS]
[BOS ] [although EOS]
[BOS Although ,] [ EOS]
CAUSE-EXPLANATION-EVIDENCE — 889,946 examples
[BOS ] [because EOS]
[BOS Because ,] [ EOS]
[BOS EOS] [BOS Thus, EOS]
CONDITION— 1,203,813 examples
[BOS If ,] [ EOS]
[BOS If ] [then EOS]
[BOS ] [if EOS]
ELABORATION — 1,836,227 examples
[BOS EOS] [BOS for example EOS]
[BOS ] [which ,]
NO-RELATION-SAME-TEXT — 1,000,000 examples
Randomly extract two sentences that are more
than 3 sentences apart in a given text.
NO-RELATION-DIFFERENT-TEXTS — 1,000,000 examples
Randomly extract two sentences from two
different documents.
</table>
<tableCaption confidence="0.814644333333333">
Table 2: Patterns used to automatically construct a
corpus of text span pairs labeled with discourse re-
lations.
</tableCaption>
<bodyText confidence="0.997800551724138">
CONDITION relations and the number of examples
extracted from the Raw corpus for each type of dis-
course relation. In the patterns in Table 2, the sym-
bols BOS and EOS denote BeginningOfSentence
and EndOfSentence boundaries, the “ ” stand for
occurrences of any words and punctuation marks,
the square brackets stand for text span boundaries,
and the other words and punctuation marks stand for
the cue phrases that we used in order to extract dis-
course relation examples. For example, the pattern
[BOS Although ,] [ EOS] is used in order to
extract examples of CONTRAST relations that hold
between a span of text delimited to the left by the
cue phrase “Although” occurring in the beginning of
a sentence and to the right by the first occurrence of
a comma, and a span of text that contains the rest of
the sentence to which “Although” belongs.
We also extracted automatically 1,000,000 exam-
ples of what we hypothesize to be non-relations, by
randomly selecting non-adjacent sentence pairs that
are at least 3 sentences apart in a given text. We label
such examples NO-RELATION-SAME-TEXT. And
we extracted automatically 1,000,000 examples of
what we hypothesize to be cross-document non-
relations, by randomly selecting two sentences from
distinct documents. As in the case of CONTRAST
and CONDITION, the NO-RELATION examples are
also noisy because long distance relations are com-
mon in well-written texts.
</bodyText>
<sectionHeader confidence="0.7987995" genericHeader="method">
3 Determining discourse relations using
Naive Bayes classifiers
</sectionHeader>
<bodyText confidence="0.994814226415094">
We hypothesize that we can determine that a CON-
TRAST relation holds between the sentences in (3)
even if we cannot semantically interpret the two sen-
tences, simply because our background knowledge
tells us that good and fails are good indicators of
contrastive statements.
John is good in math and sciences. (3)
Paul fails almost every class he takes.
Similarly, we hypothesize that we can determine that
a CONTRAST relation holds between the sentences
in (1), because our background knowledge tells us
that embargo and legally are likely to occur in con-
texts of opposite polarity. In general, we hypothe-
size that lexical item pairs can provide clues about
the discourse relations that hold between the text
spans in which the lexical items occur.
To test this hypothesis, we need to solve two
problems. First, we need a means to acquire vast
amounts of background knowledge from which we
can derive, for example, that the word pairs good
– fails and embargo – legally are good indicators
of CONTRAST relations. The extraction patterns de-
scribed in Table 2 enable us to solve this problem.1
Second, given vast amounts of training material, we
need a means to learn which pairs of lexical items
are likely to co-occur in conjunction with each dis-
course relation and a means to apply the learned pa-
rameters to any pair of text spans in order to deter-
mine the discourse relation that holds between them.
We solve the second problem in a Bayesian proba-
bilistic framework.
We assume that a discourse relation that holds
between two text spans, , is determined by
the word pairs in the cartesian product defined over
the words in the two text spans .
In general, a word pair
can “signal” any relation . We determine the
most likely discourse relation that holds between
two text spans and by taking the maximum
over , which according to
Bayes rule, amounts to taking the maximum over
. If we
assume that the word pairs in the cartesian prod-
uct are independent, is equivalent
to . The values
are computed using maximum
likelihood estimators, which are smoothed using the
Laplace method (Manning and Sch¨utze, 1999).
For each discourse relation pair , we train
a word-pair-based classifier using the automatically
derived training examples in the Raw corpus, from
which we first removed the cue-phrases used for ex-
tracting the examples. This ensures that our classi-
</bodyText>
<footnote confidence="0.757421">
1Note that relying on the list of antonyms provided by Word-
net (Fellbaum, 1998) is not enough because the semantic rela-
tions in Wordnet are not defined across word class boundaries.
For example, Wordnet does not list the “antonymy”-like relation
between embargo and legally.
</footnote>
<bodyText confidence="0.9872080625">
fiers do not learn, for example, that the word pair
if – then is a good indicator of a CONDITION re-
lation, which would simply amount to learning to
distinguish between the extraction patterns used to
construct the corpus. We test each classifier on a
test corpus of 5000 examples labeled with and
5000 examples labeled with , which ensures that
the baseline is the same for all combinations and
, namely 50%.
Table 3 shows the performance of all discourse
relation classifiers. As one can see, each classifier
outperforms the 50% baseline, with some classifiers
being as accurate as that that distinguishes between
CAUSE-EXPLANATION-EVIDENCE and ELABORA-
TION relations, which has an accuracy of 93%. We
have also built a six-way classifier to distinguish be-
tween all six relation types. This classifier has a
performance of 49.7%, with a baseline of 16.67%,
which is achieved by labeling all relations as CON-
TRASTS.
We also examined the learning curves of various
classifiers and noticed that, for some of them, the ad-
dition of training examples does not appear to have a
significant impact on their performance. For exam-
ple, the classifier that distinguishes between CON-
TRAST and CAUSE-EXPLANATION-EVIDENCE rela-
tions has an accuracy of 87.1% when trained on
2,000,000 examples and an accuracy of 87.3% when
trained on 4,771,534 examples. We hypothesized
that the flattening of the learning curve is explained
by the noise in our training data and the vast amount
of word pairs that are not likely to be good predictors
of discourse relations.
To test this hypothesis, we decided to carry out
a second experiment that used as predictors only
a subset of the word pairs in the cartesian product
defined over the words in two given text spans.
To achieve this, we used the patterns in Table 2 to
extract examples of discourse relations from the
BLIPP corpus. As expected, the BLIPP corpus
yielded much fewer learning cases: 185,846 CON-
TRAST; 44,776 CAUSE-EXPLANATION-EVIDENCE;
55,699 CONDITION; and 33,369 ELABORA-
TION relations. To these examples, we added
58,000 NO-RELATION-SAME-TEXT and 58,000
NO-RELATION-DIFFERENT-TEXTS relations.
To each text span in the BLIPP corpus corre-
sponds a parse tree (Charniak, 2000). We wrote
</bodyText>
<table confidence="0.999154333333333">
CONTRAST CEV COND ELAB NO-REL-SAME-TEXT NO-REL-DIFF-TEXTS
CONTRAST - 87 74 82 64 64
CEV 76 93 75 74
COND 89 69 71
ELAB 76 75
NO-REL-SAME-TEXT 64
</table>
<tableCaption confidence="0.994279">
Table 3: Performances of classifiers trained on the Raw corpus. The baseline in all cases is 50%.
</tableCaption>
<table confidence="0.999816666666667">
CONTRAST CEV COND ELAB NO-REL-SAME-TEXT NO-REL-DIFF-TEXTS
CONTRAST - 62 58 78 64 72
CEV 69 82 64 68
COND 78 63 65
ELAB 78 78
NO-REL-SAME-TEXT 66
</table>
<tableCaption confidence="0.999745">
Table 4: Performances of classifiers trained on the BLIPP corpus. The baseline in all cases is 50%.
</tableCaption>
<bodyText confidence="0.998481448275862">
a simple program that extracted the nouns, verbs,
and cue phrases in each sentence/clause. We
call these the most representative words of a sen-
tence/discourse unit. For example, the most repre-
sentative words of the sentence in example (4), are
those shown in italics.
Italy’s unadjusted industrial production fell in Jan- (4)
uary 3.4% from a year earlier but rose 0.4% from
December, the government said
We repeated the experiment we carried out in con-
junction with the Raw corpus on the data derived
from the BLIPP corpus as well. Table 4 summarizes
the results.
Overall, the performance of the systems trained
on the most representative word pairs in the BLIPP
corpus is clearly lower than the performance of the
systems trained on all the word pairs in the Raw
corpus. But a direct comparison between two clas-
sifiers trained on different corpora is not fair be-
cause with just 100,000 examples per relation, the
systems trained on the Raw corpus are much worse
than those trained on the BLIPP data. The learning
curves in Figure 1 are illuminating as they show that
if one uses as features only the most representative
word pairs, one needs only about 100,000 training
examples to achieve the same level of performance
one achieves using 1,000,000 training examples and
features defined over all word pairs. Also, since the
learning curve for the BLIPP corpus is steeper than
</bodyText>
<figureCaption confidence="0.947222333333333">
Figure 1: Learning curves for the ELABORATION
vs. CAUSE-EXPLANATION-EVIDENCE classifiers,
trained on the Raw and BLIPP corpora.
</figureCaption>
<bodyText confidence="0.9986395">
the learning curve for the Raw corpus, this suggests
that discourse relation classifiers trained on most
representative word pairs and millions of training
examples can achieve higher levels of performance
than classifiers trained on all word pairs (unanno-
tated data).
</bodyText>
<sectionHeader confidence="0.981851" genericHeader="method">
4 Relevance to RST
</sectionHeader>
<bodyText confidence="0.99906075">
The results in Section 3 indicate clearly that massive
amounts of automatically generated data can be used
to distinguish between discourse relations defined
as discussed in Section 2.2. What the experiments
</bodyText>
<table confidence="0.9988074">
CONTR CEV COND ELAB
# test cases 238 307 125 1761
CONTR — 63 56 80 65 64 88
CEV 87 71 76 85
COND 87 93
</table>
<tableCaption confidence="0.986304">
Table 5: Performances of Raw-trained classifiers on
</tableCaption>
<bodyText confidence="0.989342120967742">
manually labeled RST relations that hold between
elementary discourse units. Performance results are
shown in bold; baselines are shown in normal fonts.
in Section 3 do not show is whether the classifiers
built in this manner can be of any use in conjunction
with some established discourse theory. To test this,
we used the corpus of discourse trees built in the
style of RST by Carlson et al. (2001). We automati-
cally extracted from this manually annotated corpus
all CONTRAST, CAUSE-EXPLANATION-EVIDENCE,
CONDITION and ELABORATION relations that hold
between two adjacent elementary discourse units.
Since RST (Mann and Thompson, 1988) employs
a finer grained taxonomy of relations than we used,
we applied the definitions shown in Table 1. That is,
we considered that a CONTRAST relation held be-
tween two text spans if a human annotator labeled
the relation between those spans as ANTITHESIS,
CONCESSION, OTHERWISE or CONTRAST. We re-
trained then all classifiers on the Raw corpus, but
this time without removing from the corpus the cue
phrases that were used to generate the training ex-
amples. We did this because when trying to deter-
mine whether a CONTRAST relation holds between
two spans of texts separated by the cue phrase “but”,
for example, we want to take advantage of the cue
phrase occurrence as well. We employed our clas-
sifiers on the manually labeled examples extracted
from Carlson et al.’s corpus (2001). Table 5 displays
the performance of our two way classifiers for rela-
tions defined over elementary discourse units. The
table displays in the second row, for each discourse
relation, the number of examples extracted from the
RST corpus. For each binary classifier, the table lists
in bold the accuracy of our classifier and in non-bold
font the majority baseline associated with it.
The results in Table 5 show that the classifiers
learned from automatically generated training data
can be used to distinguish between certain types of
RST relations. For example, the results show that
the classifiers can be used to distinguish between
CONTRAST and CAUSE-EXPLANATION-EVIDENCE
relations, as defined in RST, but not so well between
ELABORATION and any other relation. This result
is consistent with the discourse model proposed by
Knott et al. (2001), who suggest that ELABORATION
relations are too ill-defined to be part of any dis-
course theory.
The analysis above is informative only from a
machine learning perspective. From a linguistic
perspective though, this analysis is not very use-
ful. If no cue phrases are used to signal the re-
lation between two elementary discourse units, an
automatic discourse labeler can at best guess that
an ELABORATION relation holds between the units,
because ELABORATION relations are the most fre-
quently used relations (Carlson et al., 2001). Fortu-
nately, with the classifiers described here, one can
label some of the unmarked discourse relations cor-
rectly.
For example, the RST-annotated corpus of Carl-
son et al. (2001) contains 238 CONTRAST rela-
tions that hold between two adjacent elementary dis-
course units. Of these, only 61 are marked by a cue
phrase, which means that a program trained only
on Carlson et al.’s corpus could identify at most
61/238 of the CONTRAST relations correctly. Be-
cause Carlson et al.’s corpus is small, all unmarked
relations will be likely labeled as ELABORATIONs.
However, when we run our CONTRAST vs. ELAB-
ORATION classifier on these examples, we can la-
bel correctly 60 of the 61 cue-phrase marked re-
lations and, in addition, we can also label 123 of
the 177 relations that are not marked explicitly with
cue phrases. This means that our classifier con-
tributes to an increase in accuracy from
to !!! Similarly, out
of the 307 CAUSE-EXPLANATION-EVIDENCE rela-
tions that hold between two discourse units in Carl-
son et al.’s corpus, only 79 are explicitly marked.
A program trained only on Carlson et al.’s cor-
pus, would, therefore, identify at most 79 of the
307 relations correctly. When we run our CAUSE-
EXPLANATION-EVIDENCE vs. ELABORATION clas-
sifier on these examples, we labeled correctly 73
of the 79 cue-phrase-marked relations and 102 of
the 228 unmarked relations. This corresponds to
an increase in accuracy from to
.
In a seminal paper, Banko and Brill (2001) have
recently shown that massive amounts of data can
be used to significantly increase the performance
of confusion set disambiguators. In our paper, we
show that massive amounts of data can have a ma-
jor impact on discourse processing research as well.
Our experiments show that discourse relation clas-
sifiers that use very simple features achieve unex-
pectedly high levels of performance when trained on
extremely large data sets. Developing lower-noise
methods for automatically collecting training data
and discovering features of higher predictive power
for discourse relation classification than the features
presented in this paper appear to be research avenues
that are worthwhile to pursue.
Over the last thirty years, the nature, number, and
taxonomy of discourse relations have been among
the most controversial issues in text/discourse lin-
guistics. This paper does not settle the controversy.
Rather, it raises some new, interesting questions be-
cause the lexical patterns learned by our algorithms
can be interpreted as empirical proof of existence
for discourse relations. If text production was not
governed by any rules above the sentence level, we
should have not been able to improve on any of
the baselines in our experiments. Our results sug-
gest that it may be possible to develop fully auto-
matic techniques for defining empirically justified
discourse relations.
Acknowledgments. This work was supported by
the National Science Foundation under grant num-
ber IIS-0097846 and by the Advanced Research and
Development Activity (ARDA)’s Advanced Ques-
tion Answering for Intelligence (AQUAINT) Pro-
gram under contract number MDA908-02-C-0007.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.996068314814815">
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL’01),
Toulouse, France, July 6–11.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceed-
ings of the 2nd SIGDIAL Workshop on Discourse and
Dialogue, Eurospeech 2001, Aalborg, Denmark.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics NAACL–2000, pages 132–
139, Seattle, Washington, April 29 – May 3.
DUC–2002. Proceedings of the Second Document Un-
derstanding Conference, Philadelphia, PA, July.
Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
tronic Lexical Database. The MIT Press.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman.
Jerry R. Hobbs. 1990. Literature and Cognition. CSLI
Lecture Notes Number 21.
Eduard H. Hovy and Elisabeth Maier. 1993. Parsimo-
nious or profligate: How many and which discourse
structure relations? Unpublished Manuscript.
Alistair Knott and Ted J.M. Sanders. 1998. The clas-
sification of coherence relations and their linguistic
markers: An exploration of two languages. Journal
of Pragmatics, 30:135–175.
Alistair Knott, Jon Oberlander, Mick O’Donnell, and
Chris Mellish. 2001. Beyond elaboration: The in-
teraction of relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and W. Spooren, editors,
Text representation: linguistic and psycholinguistic
aspects, pages 181–196. Benjamins.
Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations, and common sense
entailment. Linguistics and Philosophy, 16(5):437–
493.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Christopher Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing.
The MIT Press.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press.
James R. Martin. 1992. English Text. System and Struc-
ture. John Benjamin Publishing Company.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press.
TREC–2001. Proceedings of the Text Retrieval Confer-
ence, November. The Question-Answering Track.
</reference>
<sectionHeader confidence="0.799664" genericHeader="references">
5 Discussion
</sectionHeader>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876679">
<note confidence="0.996449">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 368-375.</note>
<title confidence="0.970242">An Unsupervised Approach to Recognizing Discourse Relations</title>
<author confidence="0.998147">Daniel Marcu</author>
<author confidence="0.998147">Abdessamad Echihabi</author>
<affiliation confidence="0.981073333333333">Information Sciences Institute and Department of Computer Science University of Southern California</affiliation>
<address confidence="0.997542">4676 Admiralty Way, Suite 1001 Marina del Rey, CA, 90292</address>
<email confidence="0.991561">marcu,echihabi@isi.edu</email>
<abstract confidence="0.998013166666667">We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="26962" citStr="Banko and Brill (2001)" startWordPosition="4339" endWordPosition="4342">fier contributes to an increase in accuracy from to !!! Similarly, out of the 307 CAUSE-EXPLANATION-EVIDENCE relations that hold between two discourse units in Carlson et al.’s corpus, only 79 are explicitly marked. A program trained only on Carlson et al.’s corpus, would, therefore, identify at most 79 of the 307 relations correctly. When we run our CAUSEEXPLANATION-EVIDENCE vs. ELABORATION classifier on these examples, we labeled correctly 73 of the 79 cue-phrase-marked relations and 102 of the 228 unmarked relations. This corresponds to an increase in accuracy from to . In a seminal paper, Banko and Brill (2001) have recently shown that massive amounts of data can be used to significantly increase the performance of confusion set disambiguators. In our paper, we show that massive amounts of data can have a major impact on discourse processing research as well. Our experiments show that discourse relation classifiers that use very simple features achieve unexpectedly high levels of performance when trained on extremely large data sets. Developing lower-noise methods for automatically collecting training data and discovering features of higher predictive power for discourse relation classification than</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL’01), Toulouse, France, July 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech</booktitle>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="2401" citStr="Carlson et al. (2001)" startWordPosition="371" endWordPosition="374">b. because it actually makes most of its profits from the sale of expensive, high-technology systems like laser-designated missiles, aircraft electronic warfare systems, tactical radios, anti-radiation bombs and battlefield mobility systems. In these examples, the discourse markers But and because help us figure out that a CONTRAST relation holds between the text spans in (1) and an EXPLANATION-EVIDENCE relation holds between the spans in (2). Unfortunately, cue phrases do not signal all relations in a text. In the corpus of Rhetorical Structure trees (www.isi.edu/✂ marcu/discourse/) built by Carlson et al. (2001), for example, we have observed that only 61 of 238 CONTRAST relations and 79 out of 307 EXPLANATION-EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase. So what shall we do when no discourse markers are used? If we had access to robust semantic interpreters, we could, for example, infer from sentence 1.a that “cannot buy arms legally(libya)”, infer from sentence 1.b that “can buy arms legally(rwanda)”, use our background knowledge in order to infer that “similar(libya,rwanda)”, and apply Hobbs’s (1990) definitions of discourse relations to arrive at the concl</context>
<context position="10604" citStr="Carlson et al. (2001)" startWordPosition="1655" endWordPosition="1658">hown in (1), we can automatically collect many examples of CONTRAST relations. And by extracting sentences that contain the keyword “because”, we can automatically collect many examples of CAUSE-EXPLANATION-EVIDENCE relations. As previous research in linguistics (Halliday and Hasan, 1976; Schiffrin, 1987) and computational linguistics (Marcu, 2000) show, some occurrences of “but” and “because” do not have a discourse function; and others signal other relations than CONTRAST and CAUSE-EXPLANATION. So we can expect the examples we extract to be noisy. However, empirical work of Marcu (2000) and Carlson et al. (2001) suggests that the majority of occurrences of “but”, for example, do signal CONTRAST relations. (In the RST corpus built by Carlson et al. (2001), 89 out of the 106 occurrences of “but” that occur at the beginning of a sentence signal a CONTRAST relation that holds between the sentence that contains the word “but” and the sentence that precedes it.) Our hope is that simple extraction methods are sufficient for collecting low-noise training corpora. 2.3 Generation of training data In order to collect training cases, we mined in an unsupervised manner two corpora. The first corpus, which we call</context>
<context position="23104" citStr="Carlson et al. (2001)" startWordPosition="3710" endWordPosition="3713">ations defined as discussed in Section 2.2. What the experiments CONTR CEV COND ELAB # test cases 238 307 125 1761 CONTR — 63 56 80 65 64 88 CEV 87 71 76 85 COND 87 93 Table 5: Performances of Raw-trained classifiers on manually labeled RST relations that hold between elementary discourse units. Performance results are shown in bold; baselines are shown in normal fonts. in Section 3 do not show is whether the classifiers built in this manner can be of any use in conjunction with some established discourse theory. To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al. (2001). We automatically extracted from this manually annotated corpus all CONTRAST, CAUSE-EXPLANATION-EVIDENCE, CONDITION and ELABORATION relations that hold between two adjacent elementary discourse units. Since RST (Mann and Thompson, 1988) employs a finer grained taxonomy of relations than we used, we applied the definitions shown in Table 1. That is, we considered that a CONTRAST relation held between two text spans if a human annotator labeled the relation between those spans as ANTITHESIS, CONCESSION, OTHERWISE or CONTRAST. We retrained then all classifiers on the Raw corpus, but this time wi</context>
<context position="25503" citStr="Carlson et al., 2001" startWordPosition="4093" endWordPosition="4096">relation. This result is consistent with the discourse model proposed by Knott et al. (2001), who suggest that ELABORATION relations are too ill-defined to be part of any discourse theory. The analysis above is informative only from a machine learning perspective. From a linguistic perspective though, this analysis is not very useful. If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al., 2001). Fortunately, with the classifiers described here, one can label some of the unmarked discourse relations correctly. For example, the RST-annotated corpus of Carlson et al. (2001) contains 238 CONTRAST relations that hold between two adjacent elementary discourse units. Of these, only 61 are marked by a cue phrase, which means that a program trained only on Carlson et al.’s corpus could identify at most 61/238 of the CONTRAST relations correctly. Because Carlson et al.’s corpus is small, all unmarked relations will be likely labeled as ELABORATIONs. However, when we run our CONTRAST vs. ELABO</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech 2001, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL–2000,</booktitle>
<volume>29</volume>
<pages>132--139</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="4897" citStr="Charniak, 2000" startWordPosition="757" endWordPosition="758">pment of systems capable of answering sophisticated, non-factoid queries, such as “what were the causes ofX?” or “what contradicts Y?”, which are beyond the state of the art of current systems (TREC, 2001). In this paper, we describe experiments aimed at building robust discourse-relation classification systems. To build such systems, we train a family of Naive Bayes classifiers on a large set of examples that are generated automatically from two corpora: a corpus of 41,147,805 English sentences that have no annotations, and BLIPP, a corpus of 1,796,386 automatically parsed English sentences (Charniak, 2000), which is available from the Linguistic Data Consortium (www.ldc.upenn.edu). We study empirically the adequacy of various features for the task of discourse relation classification and we show that some discourse relations can be correctly recognized with accuracies as high as 93%. 2 Discourse relation definitions and generation of training data 2.1 Background In order to build a discourse relation classifier, one first needs to decide what relation definitions one is going to use. In Section 1, we simply relied on the reader’s intuition when we claimed that a CONTRAST relation holds between </context>
<context position="11513" citStr="Charniak (2000)" startWordPosition="1807" endWordPosition="1808">ntains the word “but” and the sentence that precedes it.) Our hope is that simple extraction methods are sufficient for collecting low-noise training corpora. 2.3 Generation of training data In order to collect training cases, we mined in an unsupervised manner two corpora. The first corpus, which we call Raw, is a corpus of 1 billion words of unannotated English (41,147,805 sentences) that we created by catenating various corpora made available over the years by the Linguistic Data Consortium. The second, called BLIPP, is a corpus of only 1,796,386 sentences that were parsed automatically by Charniak (2000). We extracted from both corpora all adjacent sentence pairs that contained the cue phrase “But” at the beginning of the second sentence and we automatically labeled the relation between the two sentence pairs as CONTRAST. We also extracted all the sentences that contained the word “but” in the middle of a sentence; we split each extracted sentence into two spans, one containing the words from the beginning of the sentence to the occurrence of the keyword “but” and one containing the words from the occurrence of “but” to the end of the sentence; and we labeled the relation between the two resu</context>
<context position="20046" citStr="Charniak, 2000" startWordPosition="3194" endWordPosition="3195">ond experiment that used as predictors only a subset of the word pairs in the cartesian product defined over the words in two given text spans. To achieve this, we used the patterns in Table 2 to extract examples of discourse relations from the BLIPP corpus. As expected, the BLIPP corpus yielded much fewer learning cases: 185,846 CONTRAST; 44,776 CAUSE-EXPLANATION-EVIDENCE; 55,699 CONDITION; and 33,369 ELABORATION relations. To these examples, we added 58,000 NO-RELATION-SAME-TEXT and 58,000 NO-RELATION-DIFFERENT-TEXTS relations. To each text span in the BLIPP corpus corresponds a parse tree (Charniak, 2000). We wrote CONTRAST CEV COND ELAB NO-REL-SAME-TEXT NO-REL-DIFF-TEXTS CONTRAST - 87 74 82 64 64 CEV 76 93 75 74 COND 89 69 71 ELAB 76 75 NO-REL-SAME-TEXT 64 Table 3: Performances of classifiers trained on the Raw corpus. The baseline in all cases is 50%. CONTRAST CEV COND ELAB NO-REL-SAME-TEXT NO-REL-DIFF-TEXTS CONTRAST - 62 58 78 64 72 CEV 69 82 64 68 COND 78 63 65 ELAB 78 78 NO-REL-SAME-TEXT 66 Table 4: Performances of classifiers trained on the BLIPP corpus. The baseline in all cases is 50%. a simple program that extracted the nouns, verbs, and cue phrases in each sentence/clause. We call th</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL–2000, pages 132– 139, Seattle, Washington, April 29 – May 3.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>DUC–2002. Proceedings of the Second Document Understanding Conference,</booktitle>
<location>Philadelphia, PA,</location>
<marker></marker>
<rawString>DUC–2002. Proceedings of the Second Document Understanding Conference, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5738" citStr="(1998)" startWordPosition="894" endWordPosition="894">ectly recognized with accuracies as high as 93%. 2 Discourse relation definitions and generation of training data 2.1 Background In order to build a discourse relation classifier, one first needs to decide what relation definitions one is going to use. In Section 1, we simply relied on the reader’s intuition when we claimed that a CONTRAST relation holds between the sentences in (1). In reality though, associating a discourse relation with a text span pair is a choice that is clearly influenced by the theoretical framework one is willing to adopt. If we adopt, for example, Knott and Sanders’s (1998) account, we would say that the relation between sentences 1.a and 1.b is ADDITIVE, because no causal connection exists between the two sentences, PRAGMATIC, because the relation pertains to illocutionary force and not to the propositional content of the sentences, and NEGATIVE, because the relation involves a CONTRAST between the two sentences. In the same framework, the relation between clauses 2.a and 2.b will be labeled as CAUSAL-SEMANTICPOSITIVE-NONBASIC. In Lascarides and Asher’s theory (1993), we would label the relation between 2.a and 2.b as EXPLANATION because the event in 2.b explai</context>
<context position="9397" citStr="(1998)" startWordPosition="1471" endWordPosition="1471">s are not explicitly signalled by discourse markers. 2.2 Discourse relation definitions The discourse relations we focus on are defined at a much coarser level of granularity than in most discourse theories. For example, we consider that a CONTRAST relation holds between two text spans if one of the following relations holds: CONTRAST, ANTITHESIS, CONCESSION, or OTHERWISE, as defined by Mann and Thompson (1988), CONTRAST or VIOLATED EXPECTATION, as defined by Hobbs (1990), or any of the relations characterized by this regular expression of cognitive primitives, as defined by Knott and Sanders (1998): (CAUSAL ADDITIVE) – (SEMANTIC PRAGMATIC) – NEGATIVE. In other words, in our approach, we do not distinguish between contrasts of semantic and pragmatic nature, contrasts specific to violated expectations, etc. Table 1 shows the definitions of the relations we considered. The advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning. For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), w</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. Wordnet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="10271" citStr="Halliday and Hasan, 1976" startWordPosition="1601" endWordPosition="1604">elations we considered. The advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning. For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), we can automatically collect many examples of CONTRAST relations. And by extracting sentences that contain the keyword “because”, we can automatically collect many examples of CAUSE-EXPLANATION-EVIDENCE relations. As previous research in linguistics (Halliday and Hasan, 1976; Schiffrin, 1987) and computational linguistics (Marcu, 2000) show, some occurrences of “but” and “because” do not have a discourse function; and others signal other relations than CONTRAST and CAUSE-EXPLANATION. So we can expect the examples we extract to be noisy. However, empirical work of Marcu (2000) and Carlson et al. (2001) suggests that the majority of occurrences of “but”, for example, do signal CONTRAST relations. (In the RST corpus built by Carlson et al. (2001), 89 out of the 106 occurrences of “but” that occur at the beginning of a sentence signal a CONTRAST relation that holds b</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Literature and Cognition.</title>
<date>1990</date>
<journal>CSLI Lecture Notes Number</journal>
<volume>21</volume>
<contexts>
<context position="1214" citStr="Hobbs, 1990" startWordPosition="180" endWordPosition="181"> that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 1 Introduction In the field of discourse research, it is now widely agreed that sentences/clauses are usually not understood in isolation, but in relation to other sentences/clauses. Given the high level of interest in explaining the nature of these relations and in providing definitions for them (Mann and Thompson, 1988; Hobbs, 1990; Martin, 1992; Lascarides and Asher, 1993; Hovy and Maier, 1993; Knott and Sanders, 1998), it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text. Consider, for example, the sentence/clause pairs below. a. Such standards would preclude arms sales to (1) states like Libya, which is also currently subject to a U.N. embargo. b. But states like Rwanda before its present crisis would still be able to legally buy arms. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actually makes most </context>
<context position="9267" citStr="Hobbs (1990)" startWordPosition="1448" endWordPosition="1449">ese sentence pairs as CONTRAST, CAUSEEXPLANATION-EVIDENCE, CONDITION, ELABORATION or NONE-OF-THE-ABOVE, even when the discourse relations are not explicitly signalled by discourse markers. 2.2 Discourse relation definitions The discourse relations we focus on are defined at a much coarser level of granularity than in most discourse theories. For example, we consider that a CONTRAST relation holds between two text spans if one of the following relations holds: CONTRAST, ANTITHESIS, CONCESSION, or OTHERWISE, as defined by Mann and Thompson (1988), CONTRAST or VIOLATED EXPECTATION, as defined by Hobbs (1990), or any of the relations characterized by this regular expression of cognitive primitives, as defined by Knott and Sanders (1998): (CAUSAL ADDITIVE) – (SEMANTIC PRAGMATIC) – NEGATIVE. In other words, in our approach, we do not distinguish between contrasts of semantic and pragmatic nature, contrasts specific to violated expectations, etc. Table 1 shows the definitions of the relations we considered. The advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning. For example, by e</context>
<context position="12911" citStr="Hobbs, 1990" startWordPosition="2018" endWordPosition="2019">VIDENCE ELABORATION CONDITION ANTITHESIS (M&amp;T) EVIDENCE (M&amp;T) ELABORATION (M&amp;T) CONDITION (M&amp;T) CONCESSION (M&amp;T) VOLITIONAL-CAUSE (M&amp;T) EXPANSION (Ho) OTHERWISE (M&amp;T) NONVOLITIONAL-CAUSE (M&amp;T) EXEMPLIFICATION (Ho) CONTRAST (M&amp;T) VOLITIONAL-RESULT (M&amp;T) ELABORATION (A&amp;L) VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&amp;T) EXPLANATION (Ho) ( CAUSAL ADDITIVE) - RESULT (A&amp;L) ( SEMANTIC PRAGMATIC) - EXPLANATION (A&amp;L) NEGATIVE (K&amp;S) CAUSAL(SEMANTIC PRAGMATIC) - POSITIVE (K&amp;S) Table 1: Relation definitions as union of definitions proposed by other researchers (M&amp;T – (Mann and Thompson, 1988); Ho – (Hobbs, 1990); A&amp;L – (Lascarides and Asher, 1993); K&amp;S – (Knott and Sanders, 1998)). CONTRAST – 3,881,588 examples [BOS EOS] [BOS But EOS] [BOS ] [but EOS] [BOS ] [although EOS] [BOS Although ,] [ EOS] CAUSE-EXPLANATION-EVIDENCE — 889,946 examples [BOS ] [because EOS] [BOS Because ,] [ EOS] [BOS EOS] [BOS Thus, EOS] CONDITION— 1,203,813 examples [BOS If ,] [ EOS] [BOS If ] [then EOS] [BOS ] [if EOS] ELABORATION — 1,836,227 examples [BOS EOS] [BOS for example EOS] [BOS ] [which ,] NO-RELATION-SAME-TEXT — 1,000,000 examples Randomly extract two sentences that are more than 3 sentences apart in a given text. </context>
</contexts>
<marker>Hobbs, 1990</marker>
<rawString>Jerry R. Hobbs. 1990. Literature and Cognition. CSLI Lecture Notes Number 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Elisabeth Maier</author>
</authors>
<title>Parsimonious or profligate: How many and which discourse structure relations? Unpublished Manuscript.</title>
<date>1993</date>
<contexts>
<context position="1278" citStr="Hovy and Maier, 1993" startWordPosition="188" endWordPosition="191">s that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 1 Introduction In the field of discourse research, it is now widely agreed that sentences/clauses are usually not understood in isolation, but in relation to other sentences/clauses. Given the high level of interest in explaining the nature of these relations and in providing definitions for them (Mann and Thompson, 1988; Hobbs, 1990; Martin, 1992; Lascarides and Asher, 1993; Hovy and Maier, 1993; Knott and Sanders, 1998), it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text. Consider, for example, the sentence/clause pairs below. a. Such standards would preclude arms sales to (1) states like Libya, which is also currently subject to a U.N. embargo. b. But states like Rwanda before its present crisis would still be able to legally buy arms. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actually makes most of its profits from the sale of expensive, high-technology syste</context>
</contexts>
<marker>Hovy, Maier, 1993</marker>
<rawString>Eduard H. Hovy and Elisabeth Maier. 1993. Parsimonious or profligate: How many and which discourse structure relations? Unpublished Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Ted J M Sanders</author>
</authors>
<title>The classification of coherence relations and their linguistic markers: An exploration of two languages.</title>
<date>1998</date>
<journal>Journal of Pragmatics,</journal>
<pages>30--135</pages>
<contexts>
<context position="1304" citStr="Knott and Sanders, 1998" startWordPosition="192" endWordPosition="195">ly extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 1 Introduction In the field of discourse research, it is now widely agreed that sentences/clauses are usually not understood in isolation, but in relation to other sentences/clauses. Given the high level of interest in explaining the nature of these relations and in providing definitions for them (Mann and Thompson, 1988; Hobbs, 1990; Martin, 1992; Lascarides and Asher, 1993; Hovy and Maier, 1993; Knott and Sanders, 1998), it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text. Consider, for example, the sentence/clause pairs below. a. Such standards would preclude arms sales to (1) states like Libya, which is also currently subject to a U.N. embargo. b. But states like Rwanda before its present crisis would still be able to legally buy arms. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actually makes most of its profits from the sale of expensive, high-technology systems like laser-designated m</context>
<context position="3377" citStr="Knott and Sanders, 1998" startWordPosition="527" endWordPosition="530">t buy arms legally(libya)”, infer from sentence 1.b that “can buy arms legally(rwanda)”, use our background knowledge in order to infer that “similar(libya,rwanda)”, and apply Hobbs’s (1990) definitions of discourse relations to arrive at the conclusion that a CONTRAST relation holds between the sentences in (1). Unfortunately, the state of the art in NLP does not provide us access to semantic interpreters and general purpose knowledge bases that would support these kinds of inferences. The discourse relation definitions proposed by others (Mann and Thompson, 1988; Lascarides and Asher, 1993; Knott and Sanders, 1998) are not easier to apply either because they assume the ability to automatically derive, in addition to the semantics of the text spans, the intentions and illocutions associated with them as well. In spite of the difficulty of determining the discourse relations that hold between arbitrary text spans, it is clear that such an ability is important in many applications. First, a discourse relation recognizer would enable the development of improved discourse parsers and, consequently, of high performance single document summarizers (Marcu, 2000). In multidocument summarization (DUC, 2002), it w</context>
<context position="9397" citStr="Knott and Sanders (1998)" startWordPosition="1468" endWordPosition="1471">discourse relations are not explicitly signalled by discourse markers. 2.2 Discourse relation definitions The discourse relations we focus on are defined at a much coarser level of granularity than in most discourse theories. For example, we consider that a CONTRAST relation holds between two text spans if one of the following relations holds: CONTRAST, ANTITHESIS, CONCESSION, or OTHERWISE, as defined by Mann and Thompson (1988), CONTRAST or VIOLATED EXPECTATION, as defined by Hobbs (1990), or any of the relations characterized by this regular expression of cognitive primitives, as defined by Knott and Sanders (1998): (CAUSAL ADDITIVE) – (SEMANTIC PRAGMATIC) – NEGATIVE. In other words, in our approach, we do not distinguish between contrasts of semantic and pragmatic nature, contrasts specific to violated expectations, etc. Table 1 shows the definitions of the relations we considered. The advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning. For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), w</context>
<context position="12980" citStr="Knott and Sanders, 1998" startWordPosition="2028" endWordPosition="2031">M&amp;T) ELABORATION (M&amp;T) CONDITION (M&amp;T) CONCESSION (M&amp;T) VOLITIONAL-CAUSE (M&amp;T) EXPANSION (Ho) OTHERWISE (M&amp;T) NONVOLITIONAL-CAUSE (M&amp;T) EXEMPLIFICATION (Ho) CONTRAST (M&amp;T) VOLITIONAL-RESULT (M&amp;T) ELABORATION (A&amp;L) VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&amp;T) EXPLANATION (Ho) ( CAUSAL ADDITIVE) - RESULT (A&amp;L) ( SEMANTIC PRAGMATIC) - EXPLANATION (A&amp;L) NEGATIVE (K&amp;S) CAUSAL(SEMANTIC PRAGMATIC) - POSITIVE (K&amp;S) Table 1: Relation definitions as union of definitions proposed by other researchers (M&amp;T – (Mann and Thompson, 1988); Ho – (Hobbs, 1990); A&amp;L – (Lascarides and Asher, 1993); K&amp;S – (Knott and Sanders, 1998)). CONTRAST – 3,881,588 examples [BOS EOS] [BOS But EOS] [BOS ] [but EOS] [BOS ] [although EOS] [BOS Although ,] [ EOS] CAUSE-EXPLANATION-EVIDENCE — 889,946 examples [BOS ] [because EOS] [BOS Because ,] [ EOS] [BOS EOS] [BOS Thus, EOS] CONDITION— 1,203,813 examples [BOS If ,] [ EOS] [BOS If ] [then EOS] [BOS ] [if EOS] ELABORATION — 1,836,227 examples [BOS EOS] [BOS for example EOS] [BOS ] [which ,] NO-RELATION-SAME-TEXT — 1,000,000 examples Randomly extract two sentences that are more than 3 sentences apart in a given text. NO-RELATION-DIFFERENT-TEXTS — 1,000,000 examples Randomly extract two</context>
</contexts>
<marker>Knott, Sanders, 1998</marker>
<rawString>Alistair Knott and Ted J.M. Sanders. 1998. The classification of coherence relations and their linguistic markers: An exploration of two languages. Journal of Pragmatics, 30:135–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O’Donnell</author>
<author>Chris Mellish</author>
</authors>
<title>Beyond elaboration: The interaction of relations and focus in coherent text. In</title>
<date>2001</date>
<pages>181--196</pages>
<editor>T. Sanders, J. Schilperoord, and W. Spooren, editors,</editor>
<publisher>Benjamins.</publisher>
<marker>Knott, Oberlander, O’Donnell, Mellish, 2001</marker>
<rawString>Alistair Knott, Jon Oberlander, Mick O’Donnell, and Chris Mellish. 2001. Beyond elaboration: The interaction of relations and focus in coherent text. In T. Sanders, J. Schilperoord, and W. Spooren, editors, Text representation: linguistic and psycholinguistic aspects, pages 181–196. Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Temporal interpretation, discourse relations, and common sense entailment.</title>
<date>1993</date>
<journal>Linguistics and Philosophy,</journal>
<volume>16</volume>
<issue>5</issue>
<pages>493</pages>
<contexts>
<context position="1256" citStr="Lascarides and Asher, 1993" startWordPosition="184" endWordPosition="187">assifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 1 Introduction In the field of discourse research, it is now widely agreed that sentences/clauses are usually not understood in isolation, but in relation to other sentences/clauses. Given the high level of interest in explaining the nature of these relations and in providing definitions for them (Mann and Thompson, 1988; Hobbs, 1990; Martin, 1992; Lascarides and Asher, 1993; Hovy and Maier, 1993; Knott and Sanders, 1998), it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text. Consider, for example, the sentence/clause pairs below. a. Such standards would preclude arms sales to (1) states like Libya, which is also currently subject to a U.N. embargo. b. But states like Rwanda before its present crisis would still be able to legally buy arms. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actually makes most of its profits from the sale of expensive,</context>
<context position="3351" citStr="Lascarides and Asher, 1993" startWordPosition="523" endWordPosition="526">rom sentence 1.a that “cannot buy arms legally(libya)”, infer from sentence 1.b that “can buy arms legally(rwanda)”, use our background knowledge in order to infer that “similar(libya,rwanda)”, and apply Hobbs’s (1990) definitions of discourse relations to arrive at the conclusion that a CONTRAST relation holds between the sentences in (1). Unfortunately, the state of the art in NLP does not provide us access to semantic interpreters and general purpose knowledge bases that would support these kinds of inferences. The discourse relation definitions proposed by others (Mann and Thompson, 1988; Lascarides and Asher, 1993; Knott and Sanders, 1998) are not easier to apply either because they assume the ability to automatically derive, in addition to the semantics of the text spans, the intentions and illocutions associated with them as well. In spite of the difficulty of determining the discourse relations that hold between arbitrary text spans, it is clear that such an ability is important in many applications. First, a discourse relation recognizer would enable the development of improved discourse parsers and, consequently, of high performance single document summarizers (Marcu, 2000). In multidocument summa</context>
<context position="12947" citStr="Lascarides and Asher, 1993" startWordPosition="2022" endWordPosition="2025">ONDITION ANTITHESIS (M&amp;T) EVIDENCE (M&amp;T) ELABORATION (M&amp;T) CONDITION (M&amp;T) CONCESSION (M&amp;T) VOLITIONAL-CAUSE (M&amp;T) EXPANSION (Ho) OTHERWISE (M&amp;T) NONVOLITIONAL-CAUSE (M&amp;T) EXEMPLIFICATION (Ho) CONTRAST (M&amp;T) VOLITIONAL-RESULT (M&amp;T) ELABORATION (A&amp;L) VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&amp;T) EXPLANATION (Ho) ( CAUSAL ADDITIVE) - RESULT (A&amp;L) ( SEMANTIC PRAGMATIC) - EXPLANATION (A&amp;L) NEGATIVE (K&amp;S) CAUSAL(SEMANTIC PRAGMATIC) - POSITIVE (K&amp;S) Table 1: Relation definitions as union of definitions proposed by other researchers (M&amp;T – (Mann and Thompson, 1988); Ho – (Hobbs, 1990); A&amp;L – (Lascarides and Asher, 1993); K&amp;S – (Knott and Sanders, 1998)). CONTRAST – 3,881,588 examples [BOS EOS] [BOS But EOS] [BOS ] [but EOS] [BOS ] [although EOS] [BOS Although ,] [ EOS] CAUSE-EXPLANATION-EVIDENCE — 889,946 examples [BOS ] [because EOS] [BOS Because ,] [ EOS] [BOS EOS] [BOS Thus, EOS] CONDITION— 1,203,813 examples [BOS If ,] [ EOS] [BOS If ] [then EOS] [BOS ] [if EOS] ELABORATION — 1,836,227 examples [BOS EOS] [BOS for example EOS] [BOS ] [which ,] NO-RELATION-SAME-TEXT — 1,000,000 examples Randomly extract two sentences that are more than 3 sentences apart in a given text. NO-RELATION-DIFFERENT-TEXTS — 1,000,</context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Alex Lascarides and Nicholas Asher. 1993. Temporal interpretation, discourse relations, and common sense entailment. Linguistics and Philosophy, 16(5):437– 493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1201" citStr="Mann and Thompson, 1988" startWordPosition="176" endWordPosition="179">y spans of texts. We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 1 Introduction In the field of discourse research, it is now widely agreed that sentences/clauses are usually not understood in isolation, but in relation to other sentences/clauses. Given the high level of interest in explaining the nature of these relations and in providing definitions for them (Mann and Thompson, 1988; Hobbs, 1990; Martin, 1992; Lascarides and Asher, 1993; Hovy and Maier, 1993; Knott and Sanders, 1998), it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text. Consider, for example, the sentence/clause pairs below. a. Such standards would preclude arms sales to (1) states like Libya, which is also currently subject to a U.N. embargo. b. But states like Rwanda before its present crisis would still be able to legally buy arms. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actuall</context>
<context position="3323" citStr="Mann and Thompson, 1988" startWordPosition="519" endWordPosition="522">uld, for example, infer from sentence 1.a that “cannot buy arms legally(libya)”, infer from sentence 1.b that “can buy arms legally(rwanda)”, use our background knowledge in order to infer that “similar(libya,rwanda)”, and apply Hobbs’s (1990) definitions of discourse relations to arrive at the conclusion that a CONTRAST relation holds between the sentences in (1). Unfortunately, the state of the art in NLP does not provide us access to semantic interpreters and general purpose knowledge bases that would support these kinds of inferences. The discourse relation definitions proposed by others (Mann and Thompson, 1988; Lascarides and Asher, 1993; Knott and Sanders, 1998) are not easier to apply either because they assume the ability to automatically derive, in addition to the semantics of the text spans, the intentions and illocutions associated with them as well. In spite of the difficulty of determining the discourse relations that hold between arbitrary text spans, it is clear that such an ability is important in many applications. First, a discourse relation recognizer would enable the development of improved discourse parsers and, consequently, of high performance single document summarizers (Marcu, 2</context>
<context position="8129" citStr="Mann and Thompson (1988)" startWordPosition="1269" endWordPosition="1272">N relations. Second, given the complexity of the definitions these theories propose, it is clear why it is difficult to build programs that recognize such relations in unrestricted texts. Current NLP techniques do not enable us to reliably infer from sentence 1.a that “cannot buy arms legally(libya)” and do not give us access to general purpose knowledge bases that assert that “similar(libya,rwanda)”. The approach we advocate in this paper is in some respects less ambitious than current approaches to discourse relations because it relies upon a much smaller set of relations than those used by Mann and Thompson (1988) or Martin (1992). In our work, we decide to focus only on four types of relations, which we call: CONTRAST, CAUSE-EXPLANATIONEVIDENCE (CEV), CONDITION, and ELABORATION. (We define these relations in Section 2.2.) In other respects though, our approach is more ambitious because it focuses on the problem of recognizing such discourse relations in unrestricted texts. In other words, given as input sentence pairs such as those shown in (1)–(2), we develop techniques and programs that label the relations that hold between these sentence pairs as CONTRAST, CAUSEEXPLANATION-EVIDENCE, CONDITION, ELAB</context>
<context position="12891" citStr="Mann and Thompson, 1988" startWordPosition="2012" endWordPosition="2015">and CONTRAST CAUSE-EXPLANATION-EVIDENCE ELABORATION CONDITION ANTITHESIS (M&amp;T) EVIDENCE (M&amp;T) ELABORATION (M&amp;T) CONDITION (M&amp;T) CONCESSION (M&amp;T) VOLITIONAL-CAUSE (M&amp;T) EXPANSION (Ho) OTHERWISE (M&amp;T) NONVOLITIONAL-CAUSE (M&amp;T) EXEMPLIFICATION (Ho) CONTRAST (M&amp;T) VOLITIONAL-RESULT (M&amp;T) ELABORATION (A&amp;L) VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&amp;T) EXPLANATION (Ho) ( CAUSAL ADDITIVE) - RESULT (A&amp;L) ( SEMANTIC PRAGMATIC) - EXPLANATION (A&amp;L) NEGATIVE (K&amp;S) CAUSAL(SEMANTIC PRAGMATIC) - POSITIVE (K&amp;S) Table 1: Relation definitions as union of definitions proposed by other researchers (M&amp;T – (Mann and Thompson, 1988); Ho – (Hobbs, 1990); A&amp;L – (Lascarides and Asher, 1993); K&amp;S – (Knott and Sanders, 1998)). CONTRAST – 3,881,588 examples [BOS EOS] [BOS But EOS] [BOS ] [but EOS] [BOS ] [although EOS] [BOS Although ,] [ EOS] CAUSE-EXPLANATION-EVIDENCE — 889,946 examples [BOS ] [because EOS] [BOS Because ,] [ EOS] [BOS EOS] [BOS Thus, EOS] CONDITION— 1,203,813 examples [BOS If ,] [ EOS] [BOS If ] [then EOS] [BOS ] [if EOS] ELABORATION — 1,836,227 examples [BOS EOS] [BOS for example EOS] [BOS ] [which ,] NO-RELATION-SAME-TEXT — 1,000,000 examples Randomly extract two sentences that are more than 3 sentences apa</context>
<context position="23341" citStr="Mann and Thompson, 1988" startWordPosition="3740" endWordPosition="3743">ed RST relations that hold between elementary discourse units. Performance results are shown in bold; baselines are shown in normal fonts. in Section 3 do not show is whether the classifiers built in this manner can be of any use in conjunction with some established discourse theory. To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al. (2001). We automatically extracted from this manually annotated corpus all CONTRAST, CAUSE-EXPLANATION-EVIDENCE, CONDITION and ELABORATION relations that hold between two adjacent elementary discourse units. Since RST (Mann and Thompson, 1988) employs a finer grained taxonomy of relations than we used, we applied the definitions shown in Table 1. That is, we considered that a CONTRAST relation held between two text spans if a human annotator labeled the relation between those spans as ANTITHESIS, CONCESSION, OTHERWISE or CONTRAST. We retrained then all classifiers on the Raw corpus, but this time without removing from the corpus the cue phrases that were used to generate the training examples. We did this because when trying to determine whether a CONTRAST relation holds between two spans of texts separated by the cue phrase “but”,</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>The MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3927" citStr="Marcu, 2000" startWordPosition="614" endWordPosition="615">on, 1988; Lascarides and Asher, 1993; Knott and Sanders, 1998) are not easier to apply either because they assume the ability to automatically derive, in addition to the semantics of the text spans, the intentions and illocutions associated with them as well. In spite of the difficulty of determining the discourse relations that hold between arbitrary text spans, it is clear that such an ability is important in many applications. First, a discourse relation recognizer would enable the development of improved discourse parsers and, consequently, of high performance single document summarizers (Marcu, 2000). In multidocument summarization (DUC, 2002), it would enable the development of summarization programs capable of identifying contradictory statements both within and across documents and of producing summaries that reflect not only the similarities between various documents, but also their differences. In question-answering, it would enable the development of systems capable of answering sophisticated, non-factoid queries, such as “what were the causes ofX?” or “what contradicts Y?”, which are beyond the state of the art of current systems (TREC, 2001). In this paper, we describe experiments</context>
<context position="10333" citStr="Marcu, 2000" startWordPosition="1611" endWordPosition="1612">iscourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning. For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), we can automatically collect many examples of CONTRAST relations. And by extracting sentences that contain the keyword “because”, we can automatically collect many examples of CAUSE-EXPLANATION-EVIDENCE relations. As previous research in linguistics (Halliday and Hasan, 1976; Schiffrin, 1987) and computational linguistics (Marcu, 2000) show, some occurrences of “but” and “because” do not have a discourse function; and others signal other relations than CONTRAST and CAUSE-EXPLANATION. So we can expect the examples we extract to be noisy. However, empirical work of Marcu (2000) and Carlson et al. (2001) suggests that the majority of occurrences of “but”, for example, do signal CONTRAST relations. (In the RST corpus built by Carlson et al. (2001), 89 out of the 106 occurrences of “but” that occur at the beginning of a sentence signal a CONTRAST relation that holds between the sentence that contains the word “but” and the sente</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Martin</author>
</authors>
<title>English Text. System and Structure.</title>
<date>1992</date>
<publisher>John Benjamin Publishing Company.</publisher>
<contexts>
<context position="1228" citStr="Martin, 1992" startWordPosition="182" endWordPosition="183">se relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. 1 Introduction In the field of discourse research, it is now widely agreed that sentences/clauses are usually not understood in isolation, but in relation to other sentences/clauses. Given the high level of interest in explaining the nature of these relations and in providing definitions for them (Mann and Thompson, 1988; Hobbs, 1990; Martin, 1992; Lascarides and Asher, 1993; Hovy and Maier, 1993; Knott and Sanders, 1998), it is surprising that there are no robust programs capable of identifying discourse relations that hold between arbitrary spans of text. Consider, for example, the sentence/clause pairs below. a. Such standards would preclude arms sales to (1) states like Libya, which is also currently subject to a U.N. embargo. b. But states like Rwanda before its present crisis would still be able to legally buy arms. a. South Africa can afford to forgo sales of guns (2) and grenades b. because it actually makes most of its profits</context>
<context position="8146" citStr="Martin (1992)" startWordPosition="1274" endWordPosition="1275">he complexity of the definitions these theories propose, it is clear why it is difficult to build programs that recognize such relations in unrestricted texts. Current NLP techniques do not enable us to reliably infer from sentence 1.a that “cannot buy arms legally(libya)” and do not give us access to general purpose knowledge bases that assert that “similar(libya,rwanda)”. The approach we advocate in this paper is in some respects less ambitious than current approaches to discourse relations because it relies upon a much smaller set of relations than those used by Mann and Thompson (1988) or Martin (1992). In our work, we decide to focus only on four types of relations, which we call: CONTRAST, CAUSE-EXPLANATIONEVIDENCE (CEV), CONDITION, and ELABORATION. (We define these relations in Section 2.2.) In other respects though, our approach is more ambitious because it focuses on the problem of recognizing such discourse relations in unrestricted texts. In other words, given as input sentence pairs such as those shown in (1)–(2), we develop techniques and programs that label the relations that hold between these sentence pairs as CONTRAST, CAUSEEXPLANATION-EVIDENCE, CONDITION, ELABORATION or NONE-O</context>
</contexts>
<marker>Martin, 1992</marker>
<rawString>James R. Martin. 1992. English Text. System and Structure. John Benjamin Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Schiffrin</author>
</authors>
<title>Discourse Markers.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10289" citStr="Schiffrin, 1987" startWordPosition="1605" endWordPosition="1606">e advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning. For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), we can automatically collect many examples of CONTRAST relations. And by extracting sentences that contain the keyword “because”, we can automatically collect many examples of CAUSE-EXPLANATION-EVIDENCE relations. As previous research in linguistics (Halliday and Hasan, 1976; Schiffrin, 1987) and computational linguistics (Marcu, 2000) show, some occurrences of “but” and “because” do not have a discourse function; and others signal other relations than CONTRAST and CAUSE-EXPLANATION. So we can expect the examples we extract to be noisy. However, empirical work of Marcu (2000) and Carlson et al. (2001) suggests that the majority of occurrences of “but”, for example, do signal CONTRAST relations. (In the RST corpus built by Carlson et al. (2001), 89 out of the 106 occurrences of “but” that occur at the beginning of a sentence signal a CONTRAST relation that holds between the sentenc</context>
</contexts>
<marker>Schiffrin, 1987</marker>
<rawString>Deborah Schiffrin. 1987. Discourse Markers. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<title>The Question-Answering Track.</title>
<date></date>
<booktitle>TREC–2001. Proceedings of the Text Retrieval Conference,</booktitle>
<marker></marker>
<rawString>TREC–2001. Proceedings of the Text Retrieval Conference, November. The Question-Answering Track.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>