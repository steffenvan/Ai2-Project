<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<bodyText confidence="0.865915666666667">
properties and a sub-tree of the AAT thesaurus.
Related literature is examined in Section 5.
2 Semantic and lexical resources in
the cultural heritage domain
In this section we briefly describe the
resources that have been used in this work.
</bodyText>
<subsectionHeader confidence="0.874771">
2.1 The CIDOC CRM
</subsectionHeader>
<bodyText confidence="0.9991655">
The core ontology O is the CIDOC CRM
(Doerr, 2003), a formal core ontology whose
purpose is to facilitate the integration and
exchange of cultural heritage information
between heterogeneous sources. It is currently
being elaborated to become an ISO standard.
In the current version (4.0) the CIDOC
includes 84 taxonomically structured concepts
(called entities) and a flat set of 141 semantic
relations, called properties. Properties are
defined in terms of domain (the class for which
a property is formally defined) and range (the
class that comprises all potential values of a
property), e.g.:
</bodyText>
<note confidence="0.689533333333333">
P46 is composed of (forms part of)
Domain: E19 Physical Object
Range: E42 Object Identifier
</note>
<bodyText confidence="0.999727285714286">
The CIDOC is an “informal” resource. To
make it usable by a computer program, we
replaced specifications written in natural
language with formal ones. For each property
R, we created a tuple R(Cd,Cr) where Cd and Cr
are the domain and range entities specified in
the CIDOC reference manual.
</bodyText>
<subsectionHeader confidence="0.984821">
2.2 The AAT thesaurus
</subsectionHeader>
<bodyText confidence="0.978624083333333">
The domain glossary G is the Art and
Architecture Thesaurus (AAT) a controlled
vocabulary for use by indexers, catalogers, and
other professionals concerned with information
management in the fields of art and
architecture. In its current version3 it includes
more than 133,000 terms, descriptions,
bibliographic citations, and other information
relating to fine art, architecture, decorative
arts, archival materials, and material culture.
An example is the following:
maestˆ
</bodyText>
<note confidence="0.7699585">
Note: Refers to a work of a specific iconographic type,
depicting the Virgin Mary and Christ Child enthroned in
</note>
<footnote confidence="0.8988675">
3 http://www.getty.edu/research/conducting_research/
vocabularies/aat/
</footnote>
<bodyText confidence="0.9120884">
the center with saints and angels in adoration to each
side. The type developed in Italy in the 13th century and
was based on earlier Greek types. Works of this type are
typically two-dimensional, including painted panels
(often altarpieces), manuscript illuminations, and low-
relief carvings.
Hierarchical Position:
Objects Facet
.... Visual and Verbal Communication
........ Visual Works
............ &lt;visual works&gt;
................ &lt;visual works by subject type&gt;
.................... maestˆ
We manually mapped the top CIDOC
entities to AAT concepts, as shown in Table 1.
</bodyText>
<table confidence="0.983108076923077">
AAT topmost CIDOC entities
Top concept of AAT CRM Entity (E1), Persistent Item (E77)
Styles and Periods Period (E4)
Events Event (E5)
Activities Facet Activity (E7)
Processes/Techniques Beginning of Existence (E63)
Objects Facet Physical Stuff (E18),
Physical Object (E19)
Artifacts Physical Man-Made Stuff (E24)
Materials Facet Material (E57)
Agents Facet Actor (E39)
Time Time-Span (E52)
Place Place (E53)
</table>
<tableCaption confidence="0.999913">
Table 1: mapping between AAT and CIDOC.
</tableCaption>
<subsectionHeader confidence="0.994803">
2.3 Additional resources
</subsectionHeader>
<bodyText confidence="0.999922142857143">
A general purpose lexicalised ontology,
WordNet, is used to bridge the high level
concepts defined in the core ontology with the
words in a fragment of text. As better clarified
later, WordNet is used to verify that certain
words in a string of text f satisfy the range
constraints R(Cd,Cr) in the CIDOC. In order to
do so, we manually linked the WordNet
topmost concepts to the CIDOC entities. For
example, the concept E19 (Physical Object) is
mapped to the WordNet synset “object,
physical object”. Furthermore, we created a
gazetteer I of named entities extracting names
from the Dmoz4, a large human-edited
directory of the web, the Union List of Artist
Names (ULAN) and the Getty Thesaurus of
Geographic Names (GTG) provided by the
Getty institute, along with the AAT. Named
entities often occur in AAT definitions,
therefore, NE recognition is relevant for our
task.
</bodyText>
<footnote confidence="0.94091">
4 http://dmoz.org/about.html
</footnote>
<page confidence="0.994028">
2
</page>
<sectionHeader confidence="0.616238" genericHeader="abstract">
3 Enriching the CIDOC CRM with
the AAT thesaurus
</sectionHeader>
<bodyText confidence="0.999309272727273">
In this Section we describe in detail the method
for automatic semantic annotation and
ontology enrichment in the cultural heritage
domain.
We start with an example of the task to be
performed: given a gloss G of a term t in the
glossary G, the first objective is to annotate
certain gloss fragments with CIDOC relations.
For example, the following gloss fragment for
“vedute” is annotated with a CIDOC relation,
as follows:
</bodyText>
<footnote confidence="0.440109333333333">
[..]The first vedute probably were &lt;carried-out-
by&gt;painted by northern European artists&lt;/carried-
out-by&gt; [...]
</footnote>
<bodyText confidence="0.999340947368421">
Then, for each annotated fragment, we extract
a semantic relation instance R(Ct,Cw), where R
is a relation in O, Ct and Cw are respectively
the domain and range of R. The concept Ct
corresponds to its lexical realization t, while
Cw is the concept associated to the “head”
word w in the annotated segment of the gloss.
In the previous example, the relation instance
is: R carried_out_by(vedute,European_artist)
The annotation process allows to automatically
enrich O with an existing glossary in the same
domain of O, since each pair of term and gloss
(t,G) in the glossary G is transformed into a
formal definition, compliant with O.
Furthermore, the very same method used to
annotate definitions can be used to annotate
free text with the relations of the enriched
ontology O’.
We now describe the method in detail. Let G
</bodyText>
<listItem confidence="0.8971556">
be a glossary, t a term in G and G the
corresponding natural language definition
(gloss). The main steps of the algorithm are
the following:
1. Part-of-Speech analysis.
</listItem>
<bodyText confidence="0.996969">
Each input gloss is processed with a part-of-
speech tagger, TreeTagger5. As a result, for
each gloss G = w1 w2 ... wn, a string of part-of-
speech tags p1 p2 ... pn is produced, where pi
∀P is the part-of-speech tag chosen by
TreeTagger for word wi, and P = { N, A, V, J,
R, C, P, S, W } is a simplified set of syntactic
categories (respectively, nouns, articles, verbs,
adjectives, adverbs, conjunctions, prepositions,
</bodyText>
<footnote confidence="0.694265">
5 TreeTagger is available at: http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger.
</footnote>
<bodyText confidence="0.98575325">
symbols, wh-words). Terminological strings
(european artist) are detected using our Term
Extractor tool, already described in (Navigli
and Velardi, 2004).
</bodyText>
<listItem confidence="0.669591">
2. Named Entity recognition.
</listItem>
<bodyText confidence="0.999402285714286">
We augmented TreeTagger with the ability to
capture named entities of locations,
organizations, persons, numbers and time
expressions. In order to do so, we use regular
expressions (Friedl, 1997) in a rather standard
way, therefore we omit details. When a named
entity string wi wi+1 ... wi+j is recognized, it is
transformed into a single term and a specific
part of speech denoting the kind of entity is
assigned to it (L for cities (e.g. Venice),
countries and continents, T for time and
historical periods (e.g. Middle Ages), O for
organizations and persons (e.g. Leonardo Da
Vinci), B for numbers).
</bodyText>
<listItem confidence="0.9920105">
3. Annotation of sentence segments with
CIDOC properties.
</listItem>
<bodyText confidence="0.963353903225806">
Once the text has been parsed, we use
manually defined regular expressions to
capture relevant fragments. The regular
expressions are used to annotate gloss
segments with properties grounded on the
CIDOC-CRM relation model. Given a gloss G
and a property6 R, we define a relation checker
cR taking in input G and producing in output a
set FR of fragments of G annotated with the
property R: &lt;R&gt;f&lt;/R&gt;. The selection of a
fragment f to be included in the set FR is based
on three different kinds of constraints:
■ a part-of-speech constraint p(f, pos-
string) matches the part-of-speech (pos)
string associated with the fragment f
against a regular expression (pos-string),
specifying the required syntactic structure.
■ a lexical constraint l(f, k, lexical-
constraint) matches the lemma of the word
in k-th position of f against a regular
expression (lexical-constraint),
constraining the lexical conformation of
words occurring within the fragment f.
■ semantic constraints on domain and
range sD(f, semantic-domain) and s(f, k,
semantic-range) are valid, respectively, if
the term t and the word in the k-th position
of f match the semantic constraints on
domain and range imposed by the CIDOC,
i.e. if there exists at least one sense of t Ct
and one sense of w Cw such that:
</bodyText>
<footnote confidence="0.881362">
6 In what follows, we adopt the CIDOC terminology for
relations and concepts, i.e. properties and entities.
</footnote>
<page confidence="0.992771">
3
</page>
<note confidence="0.243234">
Rkind-of*(Cd, Ct) and Rkind-of*(Cr, Cw)7
</note>
<bodyText confidence="0.999835833333333">
More formally, the annotation process is
defined as follows:
A relation checker cR for a property R is a
logical expression composed with constraint
predicates and logical connectives, using the
following production rules:
</bodyText>
<equation confidence="0.9878126">
cR — sD(f, semantic-domain) ∀ cR’
cR’ — –cR‘ |(cR’ V cR’)  |(cR’ ∀ cR’)
cR’ — p(f, pos-string)  |l(f, k, lexical-constraint)
 |s(f, k, semantic-range)
!
</equation>
<bodyText confidence="0.9753444">
where f is a variable representing a sentence
!
fragment. Notice that a relation checker must
always specify a semantic constraint sD on the
domain of the relation R being checked on
fragment f. Optionally, it must also satisfy a
semantic constraint s on the k-th element of f,
the range of R.
For example, the following excerpt of the
checker for the is-composed-of relation (P46):
</bodyText>
<equation confidence="0.9992888">
(1) cis-composed-of(f) = sD(f, physical object#1)
∀ p(f, “(V)1(P)2R?A?[CRJVN]*(N)3”)
∀ l(f, 1,
“^(consisting|composed|comprised|constructed)$”)
∀ l(f, 2, “of”) ∀ s(f, 3, physical_object#1)
</equation>
<bodyText confidence="0.969203235294118">
reads as follows: “the fragment f is valid if it
consists of a verb in the set { consisting,
!composed, comprised, constructed }, followed
by a preposition “of”, a possibly empty number
of adverbs, adjectives, verbs and nouns, and
terminated by a noun interpretable as a
physical object in the WordNet concept
inventory”. The first predicate, sD, requires that
also the term t whose gloss contains f (i.e., its
domain) be interpretable as a physical object.
Notice that some letter in the regular
expression specified for the part-of-speech
constraint is enclosed in parentheses. This
allows it to identify the relative positions of
words to be matched against lexical and
semantic constraints, as shown graphically in
Figure 1.
</bodyText>
<footnote confidence="0.632783">
(V)1(P)2R?A?[CRJVN]*(N)3
Checker (1) recognizes, among others, the
following fragments (the words whose part-of-
</footnote>
<page confidence="0.984174">
7 R,,i„d_; denotes zero, one, or more applications of R,,i„d_��.
</page>
<bodyText confidence="0.9921175">
speech tags are enclosed in parentheses are
indicated in bold):
(consisting)1 (of)2 semi-precious (stones)3
(matching part-of-speech string: (V)1(P)2
J(N)3)
(composed)1 (of)2 (knots)3 (matching part-of-
speech string: (V) 1(P)2(N)3)
As a second example, an excerpt of the
checker for the consists-of (P45) relation is the
following:
</bodyText>
<equation confidence="0.994007142857143">
(2) cconsists-of(f) = sD(f, physical object#1)
∀p(f, “(V)1(P)2A?[JN,VC]*(N)3”)
∀ l(f, 1, “^(make|do|produce|decorated)$”)
∀ l(f, 2, “^(of|by|with)$”)
∀ –s(f, 3, color#1) ∀ –s(f, 3, activity#1)
∀ (s(f, 3, material#1) ∀ s(f, 3, solid#1)
∀ s(f, 3, liquid#1))
</equation>
<bodyText confidence="0.792951">
recognizing, among others, the following
phrases:
</bodyText>
<listItem confidence="0.986600833333333">
■ (made)1 (with)2 the red earth pigment
!(sinopia)3 (matching part-of-speech string:
(V)1(P)2AJNN(N)3)
■ (decorated)1 (with)2 red, black, and white
(paint)3 (matching part-of-speech string:
(V)1(P)2JJCJ(N)3)
</listItem>
<bodyText confidence="0.979325903225806">
Notice that in both checkers (1) and (2)
semantic constraints are specified in terms of
WordNet sense numbers (material#1, solid#1
and liquid#1), and can also be negative
(–color#1 and –activity#1). The motivation is
that CIDOC constraints are coarse-grained
due to the small number of available core
concepts: for example, the property P45
consists of simply requires that the range
belongs to the class Material (E57). Using
these coarse grained constraints would produce
false positives in the annotation task, as
discussed later. Using WordNet for semantic
constraints has two advantages: first, it is
possible to write more fine-grained (and hence
more reliable) constraints, second, regular
expressions can be re-used, at least in part, for
other domains and ontologies. In fact, several
CIDOC properties are rather general-purpose.
Notice that, as remarked in section 2.3,
replacing coarse CIDOC sense restrictions
with WordNet fine-grained restrictions is
possible since we mapped the 84 CIDOC
entities onto WordNet topmost concepts.
4. Formalisation of glosses.
The annotations generated in the previous step
are the basis for extracting property instances
to enrich the CIDOC CRM with a
conceptualization of the AAT terms. In
general, for each gloss G defining a concept Ct,
part-of-speech string
</bodyText>
<figureCaption confidence="0.9941795">
Figure 1. Correspondence between parenthesized
part-of-speech tags and words in a gloss fragment.
</figureCaption>
<figure confidence="0.6966435">
(composed)1 (of)2 two or more (negatives)3
gloss fragment
</figure>
<page confidence="0.976945">
4
</page>
<bodyText confidence="0.999981769230769">
and for each fragment f ∈ FR of G annotated
with the property R: &lt;R&gt;f&lt;/R&gt;, it is possible to
extract one or more property instances in the
form of a triple R(Ct, Cw), where Cw is the
concept associated with a term or multi-word
expression w occurring in f (i.e. its language
realization) and Ct is the concept associated to
the defined term t in AAT. For example, from
the definition of tatting (a kind of lace) the
algorithm automatically annotates the phrase
composed of knots, suggesting that this phrase
specifies the range of the is-composed-of
property for the term tatting:
</bodyText>
<subsectionHeader confidence="0.609465">
Ris-composed-of(Ctatting, Cknot)
</subsectionHeader>
<bodyText confidence="0.999414426470588">
In this property instance, Ctatting is the domain
of the property (a term in the AAT glossary)
and Cknot is the range (a specific term in the
definition G of tatting).
Selecting the concept associated to the domain
is rather straightforward: glossary terms are in
general not ambiguous, and, if they are, we
simply use a numbering policy to identify the
appropriate concept. In the example at hand,
Ctatting=tatting#1 (the first and only sense in
AAT). Therefore, if Ct matches the domain
restrictions in the regular expression for R,
then the domain of the relation is considered to
be Ct. Selecting the range of a relation is
instead more complicated. The first problem is
to select the correct words in a fragment f.
Only certain words of an annotated gloss
fragment can be exploited to extract the range
of a property instance. For example, in the
phrase “depiction of fruit, flowers, and other
objects” (from the definition of still life), only
fruit, flowers, objects represent the range of the
property instances of kind depicts (P62).
When writing relation checkers, as described
in the previous paragraph of this Section, we
can add markers of ontological relevance by
specifying a predicate r(f, k) for each relevant
position k in a fragment f. The purpose of these
markers is precisely to identify words in f
whose corresponding concepts are in the range
of a property. For instance, the checker (1) cis-
composed-of from the previous paragraph is
augmented with the conjunction: ∀ r(f, 3). We
added the predicate r(f, 3) because the third
parenthesis in the part-of-speech string refers
to an ontologically relevant element (i.e. the
!candidate range of the is-composed-of
property).
The second problem is that words that are
candidate ranges can be ambiguous, and they
often are, especially if they do not belong to
the domain glossary G. Considering the
previous example of the property depicts, the
word fruit is not a term of the AAT glossary,
and it has 3 senses in WordNet (the fruit of a
plant, the consequence of some action, an
amount of product). The property depicts, as
defined in the CIDOC, simply requires that the
range be of type Entity (E1). Therefore, all the
three senses of fruit in WordNet satisfy this
constraint. Whenever the range constraints in a
relation checker do not allow a full
disambiguation, we apply the SSI algorithm
(Navigli and Velardi, 2005), a semantic
disambiguation algorithm based on structural
pattern recognition, available on-line8. The
algorithm is applied to the words belonging to
the segment fragment f and is based on the
detection of relevant semantic interconnection
patterns between the appropriate senses. These
patterns are extracted from a lexical knowledge
base that merges WordNet with other
resources, like word collocations, on-line
dictionaries, etc.
For example, in the fragment “depictions of
fruit, flowers, and other objects” the following
properties are created for the concept still_
life#1:
</bodyText>
<equation confidence="0.9737">
Rdepicts(still_ life#1, fruit#1)
Rdepicts (still_ life#1, flower#2)
Rdepicts (still_ life#1, object#1)
</equation>
<bodyText confidence="0.997951272727273">
Some of the semantic patterns supporting this
sense selection are shown in Figure 2.
A further possibility is that the range of a
relation R is a concept instance. We create
concept instances if the word w extracted from
the fragment f is a named entity. For example,
the definition of Venetian lace is annotated as
“Refers to needle lace created &lt;current-or-
former-location&gt; in Venice&lt;/current-or-
former-location&gt; [É]”.
As a result, the following triple is produced:
</bodyText>
<equation confidence="0.9717215">
Rhas-current-or-former-location(Venetian_lace#1,
Venice:city#1)
</equation>
<bodyText confidence="0.9978798">
where Venetian_ lace#1 is the concept label
generated for the term Venetian lace in the
AAT and Venice is an instance of the concept
city#1 (city, metropolis, urban center) in
WordNet.
</bodyText>
<footnote confidence="0.9828">
8 SSI is an on-line knowledge-based WSD algorithm
accessible from http://lcl.di.uniroma1.it/ssi. The on-line
version also outputs the detected semantic connections
(as those in Figure 2).
</footnote>
<page confidence="0.983392">
5
</page>
<figureCaption confidence="0.980673">
Figure 2. Semantic Interconnections selected
by the SSI algorithm when given the word list:
“depiction, fruit, flower, object”.
</figureCaption>
<sectionHeader confidence="0.998089" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9990272">
Since the CIDOC-CRM model formalizes a
large number of fine-grained properties
(precisely, 141), we selected a subset of
properties for our experiments (reported in
Table 2). We wrote a relation checker for each
property in the Table. By applying the
checkers in cascade to a gloss G, a set of
annotations is produced. The following is an
example of an annotated gloss for the term
“vedute”:
</bodyText>
<figureCaption confidence="0.988291">
Refers to detailed, largely factual topographical views, especially
&lt;has-time-span&gt;18th-century&lt;/has-time-span&gt; Italian paintings,
drawings, or prints of cities. The first vedute probably were
&lt;carried-out-by&gt;painted by northern European artists&lt;/carried-
out-by&gt; who worked &lt;has former-or-current-location&gt;in
Italy&lt;/has former-or-current-location&gt;&lt;has-time-span&gt;in the
16th century&lt;/has-time-span&gt;. The term refers more generally to
any painting, drawing or print &lt;depicts&gt;representing a landscape
or town view&lt;/depicts&gt; that is largely topographical in conception.
</figureCaption>
<bodyText confidence="0.999814416666667">
Figure 3 shows a more comprehensive graph
representation of the outcome for the concepts
vedute#1 and maestri#1 (see the gloss in
Section 2.2).
To evaluate the methodology described in
Section 3 we considered 814 glosses from the
Visual Works sub-tree of the AAT thesaurus,
containing a total of 27,925 words. The authors
wrote the relation checkers by tuning them on
a subset of 122 glosses, and tested their
generality on the remaining 692. The test set
was manually tagged with the subset of the
CIDOC-CRM properties shown in Table 2 by
two annotators with adjudication (requiring a
careful comparison of the two sets of
annotations).
We performed two experiments: in the first, we
evaluated the gloss annotation task, in the
second the property instance extraction task,
i.e. the ability to identify the appropriate
domain and range of a property instance. In the
case of the gloss annotation task, for evaluating
each piece of information we adopted the
measures of “labeled” precision and recall.
These measures are commonly used to
evaluate parse trees obtained by a parser
(Charniak, 1997) and allow the rewarding of
good partial results. Given a property R,
labeled precision is the number of words
annotated correctly with R over the number of
words annotated automatically with R, while
labeled recall is the number of words
annotated correctly with R over the total
number of words manually annotated with R.
Table 3 shows the results obtained by applying
the checkers to tag the test set (containing a
total number of 1,328 distinct annotations and
5,965 annotated words). Note that here we are
evaluating the ability of the system to assign
the correct tag to every word in a gloss
fragment f, according to the appropriate
relation checker. We choose to evaluate the tag
assigned to single words rather than to a whole
phrase, because each misalignment would
count as a mistake even if the most part of a
phrase was tagged correctly by the automatic
annotator.
The second experiment consisted in the
evaluation of the property instances extracted.
Starting from 1,328 manually annotated
fragments of 692 glosses, the checkers
extracted an overall number of 1,101 property
instances. We randomly selected a subset of
160 glosses for evaluation, from which we
manually extracted 344 property instances.
Two aspects of the property instance extraction
task had to be assessed:
■ the extraction of the appropriate range
words in a gloss, for a given property
instance
■ the precision and recall in the extraction of
the appropriate concepts for both domain
and range of the property instance.
An overall number of 233 property instances
were automatically collected by the checkers,
out of which 203 were correct with respect to
the first assessment (87.12% precision
(203/233), 59.01% recall (203/344)).
In the second evaluation, for each property
instance R(Ct, Cw) we assessed the semantic
correctness of both the concepts Ct and Cw.
The appropriateness of the concept Ct chosen
</bodyText>
<figure confidence="0.999270846774193">
still life#1
r
e
l
a
plant#1
te
d
-
related-to
to
flower
head#1
kind-of
related-to
related-to
h
a
s
-p
a rt
flower#2
fruit#1
re
la
te
d
r
e
l
a
t
e
d
bunch#1
-
t
o
forest#2
-to
e
r
a
l
e
t
-
d
t
o
related-to
related-to
depiction#1
cyme#1
k
i
n
d
-
description#1
o
related-to
i
n
d
-
statement#1
o
f
kind-of
k
inflorescence
#2
f
thing#5-t
o
r
e
la
kind-of
land#3
r
e
l
at
e
dobject#
-
t
o
portrayal#1
te
d
r
e
l
a
t
e
appearance#1 o
d t
-
k
n
i
d
k
f
in
e
r
a
l
e
t
d
-
dt
o
-
o
f
living thing#1
organism#1 kind-of
</figure>
<page confidence="0.99091">
6
</page>
<bodyText confidence="0.969767666666667">
for the domain must be evaluated, since, even
if a term t satisfies the semantic constraints of
the domain for a property R, still it can be the
case that a fragment f in G does not refer to t,
like in the following example:
pastels (visual works) -- Works of art, typically on a paper or
vellum support, to which designs are applied using crayons
made of ground pigment held together with a binder, typically
oil or water and gum.
</bodyText>
<table confidence="0.992587454545455">
Code Name Domain Range Example
P26 moved to Move Place P26(installation of public sculpture, public place)
P27 moved from Move Place P27(removal of cornice pictures, wall)
P53 has former/current location Physical Stuff Place P53(fancy pictures, London)
P55 has current location Physical Object Place P55(macrame, Genoa)
P46 is composed of (is part of) Physical Stuff Physical Stuff P46(lace, knot)
P62 depicts Physical Man-Made Stuff Entity P62(still life, fruit)
P4 has time span Temporal Entity Time Span P4(pattern drawings, Renaissance)
P14 carried out by (performed) Activity Actor P14(blotted line drawings, Andy Warhol)
P92 brought into existence by Persistent Item Beginning of Existence P92(aquatints, aquatint process)
P45 consists of (incorporated in) Physical Stuff Material P45(sculpture, stone)
</table>
<tableCaption confidence="0.996888">
Table 2: A subset of the relations from the CIDOC-CRM model.
</tableCaption>
<figure confidence="0.999841263157895">
painted panel
altarpiece
illuminations
13th century
c
o
m -ofp
o
is-composed-of s
has-current
location
or-former-
is-composed
is-composed-of
carving
has span
e-
tim
i
s
-
maestà
e
d
h o
a f
worke
s
-
t
y
p
p
e
d
ts
ic
depicts
Virgin Mary
Italy
Christ child
carried-
by
epic
d
artist
ts
landscape
out
depicts
16th century has
time-span
h
s
a
tim
veduten
es
a
p
h
a
s
-
t
has-current
location
or-former-
Italy
18th century
y
topographical
p
views
e
town view
</figure>
<figureCaption confidence="0.9797675">
Figure 3. Extracted conceptualisation (in graphical form) of the terms maestri#1 and vedute#1 (sense
numbers are omitted for clarity).
</figureCaption>
<bodyText confidence="0.999166419354839">
In this example, ground pigment refers to
crayons (not to pastels).
The evaluation of the semantic correctness of the
domain and range of the property instances
extracted led to the final figures of 81.11%
(189/233) precision and 54.94% (189/344) recall,
due to 9 errors in the choice of Ct as a domain for
an instance R(Ct, Cw) and 5 errors in the semantic
disambiguation of range words w not appearing
in AAT, but encoded in WordNet (as described
in the last part of Section 3). A final experiment
was performed to evaluate the generality of the
approach presented in this paper.
As already remarked, the same procedure used
for annotating the glosses of a thesaurus can be
used to annotate web documents. Our objective
in this third experiment was to:
■ Evaluate the ability of the system to annotate
fragments of web documents with CIDOC
relations
■ Evaluate the domain dependency of the
relation checkers, by letting the system
annotate documents not in the cultural
heritage domain.
We then selected 5 documents at random from an
historical archive and an artist’s biographies
archive9 including about 6,000 words in total,
about 5,000 of which in the historical domain.
We then ran the automatic annotation procedure
on these documents and we evaluated the result,
using the same criteria as in Table 3.
</bodyText>
<table confidence="0.999585733333333">
Property Precision Recall
P26 – moved to 84.95% (79/93) 64.23% (79/123)
P27 – moved from 81.25% (39/48) 78.00% (39/50)
P53 - has former or 78.09% (916/1173) 67.80% (916/1351)
current location
P55 – has current 100.00% (8/8) 100.00% (8/8)
location
P46 –composed of 87.49% (944/1079) 70.76% (944/1334)
P62 – depicts 94.15% (370/393) 65.26% (370/567)
P4 – has time span 91.93% (547/595) 76.40% (547/716)
P14 - carried out by 91.71% (343/374) 71.91% (343/477)
P92 – brought into 89.54% (471/526) 62.72% (471/751)
existence
P45 – consists of 74.67% (398/533) 57.60% (398/691)
Avg. performance 85.34% (4115/4822) 67.81% (4115/6068)
</table>
<tableCaption confidence="0.95671475">
Table 3: Precision and Recall of the gloss
annotation task.
Table 4 presents the results of the experiment.
Only 5 out of 10 properties had at least one
</tableCaption>
<footnote confidence="0.9934035">
9 http://historicaltextarchive.com and
http://www.artnet.com/library
</footnote>
<page confidence="0.999282">
7
</page>
<bodyText confidence="0.999876375">
instance in the analysed documents. It is
remarkable that, especially for the less domain-
dependent properties, the precision and recall of
the algorithm is still high, thus showing the
generality of the method. Notice that the
historical documents influenced the result much
more than the artist biographies, because of their
dimension.
In Table 4 the recall of P14 (carried out by) is
omitted. This is motivated by the fact that this
property, in a generic domain, corresponds to the
agent relation (“an active animate entity that
voluntarily initiates an action”10), while in the
cultural heritage domain it has a more narrow
interpretation (an example of this relation in the
CIDOC handbook is: “the painting of the Sistine
Chapel (E7) was carried out by Michelangelo
Buonarroti (E21) in the role of master craftsman
(E55)”). However, the domain and range
restrictions for P14 correspond to an agent
relation, therefore, in a generic domain, one
should annotate as “carried out by” almost any
verb phrase with the subject (including pronouns
and anaphoric references) in the class Human.
</bodyText>
<table confidence="0.999432375">
Property Precision Recall
P53 – has former or 79.84% (198/248) 77.95% (198/254)
current location
P46 – composed of 83.58% (112/134) 96.55% (112/116)
P4 – has time span 78.32% (112/143) 50.68% (112/221)
P14 – carried out by 60.61% (40/66) - -
P45 – consists of 85.71% (6/7) 37.50% (6/16)
Avg. performance 78.26% (468/598) 77.10% (468/607)
</table>
<tableCaption confidence="0.95957">
Table 4: Precision and Recall of a web
document annotation task.
</tableCaption>
<sectionHeader confidence="0.999754" genericHeader="related work">
5 Related work
</sectionHeader>
<bodyText confidence="0.998833363636364">
This paper presented a method to automatically
annotate the glosses of a thesaurus, the AAT,
with the properties (conceptual relations) of a
core ontology, the CIDOC-CRM. Several
methods for ontology population and semantic
annotation described in literature (e.g. (Thelen
and Riloff, 2002; Califf and Mooney, 2004;
Cimiano et al. 2005; Valarakos et al. 2004)) use
regular expressions to identify named entities,
i.e. concept instances. Other methods extract
hypernym11 relations using syntactic and lexical
</bodyText>
<sectionHeader confidence="0.223298" genericHeader="conclusions">
10 http://www.jfsowa.com/ontology/thematic.htm
</sectionHeader>
<bodyText confidence="0.951723634920635">
11 In AAT the hypernym relation is already available,
since AAT is a thesaurus, not a glossary. However we
developed regular expressions also for hypernym extraction
from definitions. For sake of space this is not discussed in
this paper, however the remarkable result (wrt analogous
evaluations in literature) is that in 34% of the cases the
automatically extracted hypernym is the same as in AAT,
and in 26% of the cases, either the extracted hypernym is
more general than the one defined in AAT, or the contrary,
patterns (Snow et al. 2005; Morin and Jaquemin
2004) or supervised clustering techniques
(Kashyap et al. 2003).
In our work, we automatically learn formal
concepts, not simply instances or taxonomies
(e.g. the graphs of Figure 3) compliant with the
semantics of a well-established core ontology,
the CIDOC. The method is unsupervised, in the
sense that it does not need manual annotation of
a significant fragment of text. However, it relies
on a set of manually written regular expressions,
based on lexical, part-of-speech, and semantic
constraints. The structure of regular expressions
is rather more complex than in similar works
using regular expressions, especially for the use
of automatically verified semantic constraints.
This complexity is indeed necessary to identify
non-trivial relations in an unconstrained text and
without training. The issue is however how much
this method generalizes to other domains:
• A first problem is the availability of lexical
and semantic resources used by the
algorithm. The most critical requirement of
the method is the availability of sound
domain core ontologies, which hopefully
will be produced by other web communities
stimulated by the recent success of CIDOC
CRM. On the other side, in absence of an
agreed conceptual reference model, no large
scale annotation is possible at all. As for the
other resources used by our algorithm,
glossaries, thesaura and gazetteers are widely
available in “mature” domains. If not, we
developed a methodology, described in
(Navigli and Velardi, 2005b), to
automatically create a glossary in novel
domains (e.g. enterprise interoperability),
extracting definition sentences from domain-
relevant documents and authoritative web
sites.
• The second problem is about the generality
of regular expressions. Clearly, the relation
checkers that we defined are tuned on the
CIDOC properties. This however is
consistent with our target: in specific
domains users are interested to identify
specific relations, not general purpose.
Certain relevant application domains –like
cultural heritage, e-commerce, or tourism-
are those that dictate specifications for real-
world applications of NLP techniques.
However, several CIDOC properties are
rather general (especially locative and
wrt the AAT hierarchy.
</bodyText>
<page confidence="0.993141">
8
</page>
<bodyText confidence="0.999217238095238">
temporal relations) therefore some relation
checkers easily apply to other domains, as
demonstrated by the experiment on
automatic annotation of historical archives in
Table 4. Furthermore, the method used to
verify semantic constraints is fully general,
since it is based on WordNet and a general-
purpose, untrained semantic disambiguation
algorithm, SSI.
• Finally, the authors believe with some degree
of convincement that automatic pattern-
learning methods often require non-trivial
human effort just like manual methods
(because of the need of annotated data,
careful parameter setting, etc.), and
furthermore they are unable to combine in a
non-trivial way different types of features
(e.g. lexical, syntactic, semantic). To make
an example, a recent work on learning
hypernymy patterns (Morin and Jacquemin,
2004) provides the full list of learned
patterns. The complexity of these patterns is
certainly lower than the regular expression
structures used in this work, and many of
them are rather intuitive.
In the literature the tasks on which automatic
methods have been tested are rather constrained,
and do not convincingly demonstrate the
superiority of automatic with respect to manually
defined patterns. For example, in Senseval-3
(automated labeling of semantic roles12),
participating systems are requested to identify
semantic roles in a sentence fragment for which
the “frame semantics” is given, therefore the
possible semantic relations to be identified are
quite limited.
However, we believe that our method can be
automated to some degree (for example, machine
learning methods can be used to bootstrap the
syntactic patterns, and to learn semantic
constraints), a research line we are currently
exploring.
</bodyText>
<sectionHeader confidence="0.999537" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999504462686567">
M. E. Califf and R.J. Mooney, “Bottom-up relational
learning of pattern matching rules for information
extraction” Machine Learning research, 4 (2)177-
210, 2004.
E. Charniak, “Statistical Techniques for Natural
Language Parsing”, AI Magazine 18(4), 33-44,
1997.
P. Cimiano, G. Ladwig and S. Staab, “Gimme the
context: context-driven automatic semantic
12 http://www.clres.com/SensSemRoles.html
annotation with C-PANKOW” In: Proceedings of
the 14th International WWW Conference, WWW
2005, Chiba, Japan, May, 2005. ACM Press.
M. Doerr, “The CIDOC Conceptual Reference
Module: An Ontological Approach to Semantic
Interoperability of Metadata”. AI Magazine,
Volume 24, Number 3, Fall 2003.
M. S. Fox, M. Barbuceanu, M. Gruninger, and J. Lin,
&amp;quot;An Organisation Ontology for Enterprise
Modeling&amp;quot;, In Simulating Organizations:
Computational Models of Institutions and Groups,
M. Prietula, K. Carley &amp; L. Gasser (Eds), Menlo
Park CA: AAAI/MIT Press, pp. 131-152. 1997
J.E. F. Friedl “Mastering Regular Expressions”
O’Reilly eds., ISBN: 1-56592-257-3, First edition
January 1997.
V. Kashyap, C. Ramakrishnan, T. Rindflesch.
&amp;quot;Toward (Semi)-Automatic Generation of Bio-
medical Ontologies&amp;quot;, Proceedings of American
Medical Informatics Association, 2003
G. A. Miller, ̏WordNet: a lexical database for
English.&amp;quot; In: Communications of the ACM 38 (11),
November 1995, pp. 39 - 41.
E. Morin and C. Jacquemin “Automatic acquisition
and expansion of hypernym links” Computer and
the Humanities, 38: 363-396, 2004
R. Navigli, P. Velardi. Learning Domain Ontologies
from Document Warehouses and Dedicated
Websites, Computational Linguistics (30-2), MIT
Press, June, 2004.
R. Navigli and P. Velardi, “Structural Semantic
Interconnections: a knowledge-based approach to
word sense disambiguation”, Special Issue-
Syntactic and Structural Pattern Recognition, IEEE
TPAMI, Volume: 27, Issue: 7, 2005.
R. Navigli, P. Velardi. Automatic Acquisition of a
Thesaurus of Interoperability Terms, Proc. of 16th
IFAC World Congress, Praha, Czech Republic, July
4-8th, 2005b.
R. Snow, D. Jurafsky, A. Y. Ng, &amp;quot;Learning syntactic
patters for automatic hypernym discovery&amp;quot;, NIPS
17, 2005.
M. Thelen, E. Riloff, &amp;quot;A Bootstrapping Method for
Learning Semantic Lexicons using Extraction
Pattern Contexts&amp;quot;, Proceedings of the Conference
on Empirical Methods in Natural Language
Processing, 2002.
M. Uschold, M. King, S. Moralee and Y. Zorgios,
“The Enterprise Ontology”, The Knowledge
Engineering Review , Vol. 13, Special Issue on
Putting Ontologies to Use (eds. Uschold. M. and
Tate. A.), 1998
Valarakos, G. Paliouras, V. Karkaletsis, G. Vouros,
“Enhancing Ontological Knowledge through
Ontology Population and Enrichment” in
Proceedings of the 14th EKAW conf., LNAI, Vol.
3257, pp. 144-156, Springer Verlag, 2004.
</reference>
<page confidence="0.997111">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.576870333333333">properties and a sub-tree of the AAT thesaurus. Related literature is examined in Section 5. 2 Semantic and lexical resources in</note>
<abstract confidence="0.99427522">the cultural heritage domain In this section we briefly describe the resources that have been used in this work. 2.1 The CIDOC CRM core ontology the CRM (Doerr, 2003), a formal core ontology whose purpose is to facilitate the integration and exchange of cultural heritage information between heterogeneous sources. It is currently being elaborated to become an ISO standard. In the current version (4.0) the CIDOC includes 84 taxonomically structured concepts and a flat set of 141 semantic called Properties are in terms of class for which property is formally defined) and class that comprises all potential values of a property), e.g.: P46 is composed of (forms part of) Domain: E19 Physical Object Range: E42 Object Identifier The CIDOC is an “informal” resource. To make it usable by a computer program, we replaced specifications written in natural language with formal ones. For each property we created a tuple where are the domain and range entities specified in the CIDOC reference manual. 2.2 The AAT thesaurus domain glossary the Art and Architecture Thesaurus (AAT) a controlled vocabulary for use by indexers, catalogers, and other professionals concerned with information management in the fields of art and In its current it includes more than 133,000 terms, descriptions, bibliographic citations, and other information relating to fine art, architecture, decorative arts, archival materials, and material culture. An example is the following: maestˆ Refers to a work of a specific iconographic type, depicting the Virgin Mary and Christ Child enthroned in vocabularies/aat/ the center with saints and angels in adoration to each side. The type developed in Italy in the 13th century and was based on earlier Greek types. Works of this type are typically two-dimensional, including painted panels (often altarpieces), manuscript illuminations, and lowrelief carvings.</abstract>
<title confidence="0.631958333333333">Objects Facet .... Visual and Verbal Communication Works</title>
<abstract confidence="0.989520666666667">works&gt; works by subject type&gt; We manually mapped the top CIDOC</abstract>
<note confidence="0.917888538461539">entities to AAT concepts, as shown in Table 1. AAT topmost CIDOC entities Top concept of AAT CRM Entity (E1), Persistent Item (E77) Styles and Periods Period (E4) Events Event (E5) Activities Facet Activity (E7) Processes/Techniques Beginning of Existence (E63) Objects Facet Physical Stuff (E18), Physical Object (E19) Artifacts Physical Man-Made Stuff (E24) Materials Facet Material (E57) Agents Facet Actor (E39) Time Time-Span (E52) Place Place (E53)</note>
<abstract confidence="0.998056399239543">mapping between AAT and CIDOC. 2.3 Additional resources A general purpose lexicalised ontology, WordNet, is used to bridge the high level concepts defined in the core ontology with the words in a fragment of text. As better clarified later, WordNet is used to verify that certain in a string of text the range in the CIDOC. In order to do so, we manually linked the WordNet topmost concepts to the CIDOC entities. For example, the concept E19 (Physical Object) is to the WordNet synset Furthermore, we created a named entities extracting names the a large human-edited directory of the web, the Union List of Artist Names (ULAN) and the Getty Thesaurus of Geographic Names (GTG) provided by the Getty institute, along with the AAT. Named entities often occur in AAT definitions, therefore, NE recognition is relevant for our task. 2 3 Enriching the CIDOC CRM with the AAT thesaurus In this Section we describe in detail the method for automatic semantic annotation and ontology enrichment in the cultural heritage domain. We start with an example of the task to be given a gloss a term the the first objective is to certain gloss fragments with CIDOC relations. For example, the following gloss fragment for is annotated with a CIDOC relation, as follows: first vedute probably were &lt;carried-outby northern European Then, for each annotated fragment, we extract relation instance where R a relation in are respectively R. The concept to its lexical realization while is the concept associated to the “head” the annotated segment of the gloss. In the previous example, the relation instance R The annotation process allows to automatically an existing glossary in the same of since each pair of term and gloss in the glossary transformed into a compliant with Furthermore, the very same method used to annotate definitions can be used to annotate text the relations of the enriched now describe the method in detail. Let a glossary, term in corresponding natural language definition (gloss). The main steps of the algorithm are the following: analysis. Each input gloss is processed with a part-oftagger, As a result, for gloss a string of part-oftags is produced, where the part-of-speech tag chosen by for word and { S, W is a simplified set of syntactic categories (respectively, nouns, articles, verbs, adjectives, adverbs, conjunctions, prepositions, is available at: http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger. symbols, wh-words). Terminological strings (european artist) are detected using our Term Extractor tool, already described in (Navigli and Velardi, 2004). 2. Named Entity recognition. We augmented TreeTagger with the ability to capture named entities of locations, organizations, persons, numbers and time expressions. In order to do so, we use regular expressions (Friedl, 1997) in a rather standard way, therefore we omit details. When a named string ... is recognized, it is transformed into a single term and a specific part of speech denoting the kind of entity is to it cities (e.g. Venice), and continents, time and periods (e.g. Middle Ages), organizations and persons (e.g. Leonardo Da Vinci), B for numbers). 3. Annotation of sentence segments with CIDOC properties. Once the text has been parsed, we use manually defined regular expressions to capture relevant fragments. The regular expressions are used to annotate gloss segments with properties grounded on the relation model. Given a gloss a we define a checker in input producing in output a fragments of with the The selection of a be included in the set based on three different kinds of constraints: a constraint posmatches the part-of-speech associated with the fragment a regular expression specifying the required syntactic structure. a constraint lexicalmatches the lemma of the word of a regular constraining the lexical conformation of occurring within the fragment ■ semantic constraints on domain and and are valid, respectively, if term the word in the the semantic constraints on domain and range imposed by the CIDOC, if there exists at least one sense of one sense of such that: 6In what follows, we adopt the CIDOC terminology for relations and concepts, i.e. properties and entities. 3 and More formally, the annotation process is defined as follows: checker a property a logical expression composed with constraint predicates and logical connectives, using the following production rules: | | a variable representing a sentence ! fragment. Notice that a relation checker must specify a semantic constraint the the relation checked on Optionally, it must also satisfy a constraint the k-th element of range of For example, the following excerpt of the for the = 1, “^(consisting|composed|comprised|constructed)$”) 2, “of”) 3, as follows: “the fragment valid if it of a verb in the set { followed by a preposition “of”, a possibly empty number of adverbs, adjectives, verbs and nouns, and terminated by a noun interpretable as a object the WordNet concept The first predicate, requires that the term gloss contains its be interpretable as a Notice that some letter in the regular expression specified for the part-of-speech constraint is enclosed in parentheses. This allows it to identify the relative positions of words to be matched against lexical and semantic constraints, as shown graphically in Figure 1. Checker (1) recognizes, among others, the fragments (the words whose part-ofzero, one, or more applications of speech tags are enclosed in parentheses are indicated in bold): part-of-speech string: part-ofstring: As a second example, an excerpt of the for the relation is the following: = 1, “^(make|do|produce|decorated)$”) 2, “^(of|by|with)$”) 3, 3, 3, 3, 3, recognizing, among others, the following phrases: ■ red earth pigment part-of-speech string: ■ black, and white part-of-speech string: Notice that in both checkers (1) and (2) semantic constraints are specified in terms of sense numbers and can also be negative The motivation is constraints are coarse-grained due to the small number of available core for example, the property of requires that the range to the class Using these coarse grained constraints would produce positives the annotation task, as discussed later. Using WordNet for semantic constraints has two advantages: first, it is possible to write more fine-grained (and hence more reliable) constraints, second, regular expressions can be re-used, at least in part, for other domains and ontologies. In fact, several CIDOC properties are rather general-purpose. Notice that, as remarked in section 2.3, replacing coarse CIDOC sense restrictions with WordNet fine-grained restrictions is possible since we mapped the 84 CIDOC entities onto WordNet topmost concepts. of glosses. The annotations generated in the previous step the basis for extracting instances to enrich the CIDOC CRM with a conceptualization of the AAT terms. In for each gloss a concept part-of-speech string 1. between parenthesized part-of-speech tags and words in a gloss fragment. or more gloss fragment 4 for each fragment the property it is possible to extract one or more property instances in the of a triple where is the with a term or multi-word in its language and the to defined term AAT. For example, from definition of kind of lace) the algorithm automatically annotates the phrase of suggesting that this phrase the the for the term this property instance, is the of the property (a term in the AAT glossary) is the specific term in the Selecting the concept associated to the domain is rather straightforward: glossary terms are in general not ambiguous, and, if they are, we simply use a numbering policy to identify the appropriate concept. In the example at hand, first and only sense in Therefore, if the domain in the regular expression for then the domain of the relation is considered to Selecting the range of a relation is instead more complicated. The first problem is select the correct words in a fragment Only certain words of an annotated gloss fragment can be exploited to extract the range of a property instance. For example, in the phrase “depiction of fruit, flowers, and other (from the definition of only the range of the instances of kind When writing relation checkers, as described in the previous paragraph of this Section, we add of ontological relevance a predicate for each relevant a fragment The purpose of these is precisely to identify words in whose corresponding concepts are in the range a property. For instance, the checker (1) the previous paragraph is with the conjunction: We the predicate 3) because the third parenthesis in the part-of-speech string refers to an ontologically relevant element (i.e. the the property). The second problem is that words that are candidate ranges can be ambiguous, and they often are, especially if they do not belong to domain glossary Considering the example of the property the not a term of the AAT glossary, and it has 3 senses in WordNet (the fruit of a plant, the consequence of some action, an of product). The property as defined in the CIDOC, simply requires that the be of type Therefore, all the senses of WordNet satisfy this constraint. Whenever the range constraints in a relation checker do not allow a full disambiguation, we apply the SSI algorithm (Navigli and Velardi, 2005), a semantic disambiguation algorithm based on structural recognition, available The algorithm is applied to the words belonging to segment fragment is based on the detection of relevant semantic interconnection patterns between the appropriate senses. These patterns are extracted from a lexical knowledge base that merges WordNet with other resources, like word collocations, on-line dictionaries, etc. For example, in the fragment “depictions of fruit, flowers, and other objects” the following are created for the concept Some of the semantic patterns supporting this sense selection are shown in Figure 2. A further possibility is that the range of a a concept We create instances if the word from fragment a named entity. For example, definition of lace annotated as to needle lace created &lt;current-or- As a result, the following triple is produced: lace#1 the concept label for the term lace the and an instance of the concept metropolis, urban in is an on-line knowledge-based WSD algorithm accessible from http://lcl.di.uniroma1.it/ssi. The on-line version also outputs the detected semantic connections (as those in Figure 2). 5 2. Interconnections selected by the SSI algorithm when given the word list: “depiction, fruit, flower, object”. 4 Evaluation Since the CIDOC-CRM model formalizes a large number of fine-grained properties (precisely, 141), we selected a subset of properties for our experiments (reported in Table 2). We wrote a relation checker for each property in the Table. By applying the checkers in cascade to a gloss G, a set of annotations is produced. The following is an example of an annotated gloss for the term “vedute”: Refers to detailed, largely factual topographical views, especially paintings, drawings, or prints of cities. The first vedute probably were by northern European worked the The term refers more generally to painting, drawing or print a landscape town is largely topographical in conception. Figure 3 shows a more comprehensive graph representation of the outcome for the concepts and (see the gloss in Section 2.2). To evaluate the methodology described in Section 3 we considered 814 glosses from the Works of the AAT thesaurus, containing a total of 27,925 words. The authors wrote the relation checkers by tuning them on a subset of 122 glosses, and tested their generality on the remaining 692. The test set was manually tagged with the subset of the CIDOC-CRM properties shown in Table 2 by two annotators with adjudication (requiring a careful comparison of the two sets of annotations). We performed two experiments: in the first, we the annotation in the the instance extraction i.e. the ability to identify the appropriate domain and range of a property instance. In the of the annotation task,for evaluating each piece of information we adopted the of precision These measures are commonly used to evaluate parse trees obtained by a parser (Charniak, 1997) and allow the rewarding of partial results. Given a property the number of correctly with the number of annotated automatically with while the number of words correctly with the total of words manually annotated with Table 3 shows the results obtained by applying the checkers to tag the test set (containing a total number of 1,328 distinct annotations and 5,965 annotated words). Note that here we are evaluating the ability of the system to assign correct tag to word a gloss to the appropriate relation checker. We choose to evaluate the tag assigned to single words rather than to a whole phrase, because each misalignment would count as a mistake even if the most part of a phrase was tagged correctly by the automatic annotator. The second experiment consisted in the of the instancesextracted. Starting from 1,328 manually annotated fragments of 692 glosses, the checkers extracted an overall number of 1,101 property instances. We randomly selected a subset of 160 glosses for evaluation, from which we manually extracted 344 property instances. Two aspects of the property instance extraction task had to be assessed: the extraction of the appropriate a gloss, for a given property instance ■ the precision and recall in the extraction of appropriate both the property instance. An overall number of 233 property instances were automatically collected by the checkers, out of which 203 were correct with respect to the first assessment (87.12% precision (203/233), 59.01% recall (203/344)). In the second evaluation, for each property we assessed the semantic of both the concepts appropriateness of the concept still life#1 r e l a plant#1 d flower head#1 related-to h a s a rt flower#2 fruit#1 d r e l a t e d bunch#1 t o forest#2 e r a l e t d t o depiction#1 cyme#1 k n d description#1 o i n d statement#1 o f k inflorescence f o r e land#3 r e l e o portrayal#1 d r e l a t e d t k n i d k f e r a l e t d o o f living thing#1 organism#1 kind-of 6 for the domain must be evaluated, since, even a term the semantic constraints of the domain for a property R, still it can be the that a fragment not refer to like in the following example: (visual works) -of typically on a paper or vellum support, to which designs are applied using crayons of ground pigment together with a binder, typically oil or water and gum.</abstract>
<note confidence="0.923861090909091">Code Name Domain Range Example P26 moved to Move Place P26(installation of public sculpture, public place) P27 moved from Move Place P27(removal of cornice pictures, wall) P53 has former/current location Physical Stuff Place P53(fancy pictures, London) P55 has current location Physical Object Place P55(macrame, Genoa) P46 is composed of (is part of) Physical Stuff Physical Stuff P46(lace, knot) P62 depicts Physical Man-Made Stuff Entity P62(still life, fruit) P4 has time span Temporal Entity Time Span P4(pattern drawings, Renaissance) P14 carried out by (performed) Activity Actor P14(blotted line drawings, Andy Warhol) P92 brought into existence by Persistent Item Beginning of Existence P92(aquatints, aquatint process) P45 consists of (incorporated in) Physical Stuff Material P45(sculpture, stone)</note>
<abstract confidence="0.975260264367816">Table 2: A subset of the relations from the CIDOC-CRM model. painted panel altarpiece illuminations century c o m -ofp o is-composed-of s carving etim i s maestà e d h o a f s t y p p e d Virgin Mary Italy Christ child d artist landscape century h s a a p h a s t Italy century y topographical p views e town view Extracted conceptualisation (in graphical form) of the terms numbers are omitted for clarity). this example, pigment to to The evaluation of the semantic correctness of the domain and range of the property instances extracted led to the final figures of 81.11% (189/233) precision and 54.94% (189/344) recall, to 9 errors in the choice of a domain for instance and 5 errors in the semantic of range words appearing in AAT, but encoded in WordNet (as described in the last part of Section 3). A final experiment was performed to evaluate the generality of the approach presented in this paper. As already remarked, the same procedure used for annotating the glosses of a thesaurus can be used to annotate web documents. Our objective in this third experiment was to: ■ Evaluate the ability of the system to annotate fragments of web documents with CIDOC relations ■ Evaluate the domain dependency of the relation checkers, by letting the system annotate documents not in the cultural heritage domain. We then selected 5 documents at random from an historical archive and an artist’s biographies including about 6,000 words in total, about 5,000 of which in the historical domain. We then ran the automatic annotation procedure on these documents and we evaluated the result, using the same criteria as in Table 3.</abstract>
<note confidence="0.852330846153846">Property Precision Recall P26 – moved to 84.95% (79/93) 64.23% (79/123) P27 – moved from 81.25% (39/48) 78.00% (39/50) P53 has former or current location 78.09% (916/1173) 67.80% (916/1351) P55 – has current location 100.00% (8/8) 100.00% (8/8) P46 –composed of 87.49% (944/1079) 70.76% (944/1334) P62 – depicts 94.15% (370/393) 65.26% (370/567) P4 – has time span 91.93% (547/595) 76.40% (547/716) P14 carried out by 91.71% (343/374) 71.91% (343/477) P92 – brought into existence 89.54% (471/526) 62.72% (471/751) P45 – consists of 74.67% (398/533) 57.60% (398/691) Avg. performance 85.34% (4115/4822) 67.81% (4115/6068) Table 3: Precision and Recall of the gloss</note>
<abstract confidence="0.971129239766082">annotation task. Table 4 presents the results of the experiment. Only 5 out of 10 properties had at least one and http://www.artnet.com/library 7 instance in the analysed documents. It is remarkable that, especially for the less domaindependent properties, the precision and recall of the algorithm is still high, thus showing the generality of the method. Notice that the historical documents influenced the result much more than the artist biographies, because of their dimension. Table 4 the recall of P14 out is omitted. This is motivated by the fact that this property, in a generic domain, corresponds to the (“an active animate entity that initiates an while in the cultural heritage domain it has a more narrow interpretation (an example of this relation in the handbook is: painting of the Sistine (E7) was out by (E21) the role of craftsman However, the domain and range restrictions for P14 correspond to an agent relation, therefore, in a generic domain, one should annotate as “carried out by” almost any verb phrase with the subject (including pronouns and anaphoric references) in the class Human. Property Precision Recall P53 – has former or current location 79.84% (198/248) 77.95% (198/254) P46 – composed of 83.58% (112/134) 96.55% (112/116) P4 – has time span 78.32% (112/143) 50.68% (112/221) P14 – carried out by 60.61% (40/66) - - P45 – consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task. 5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al. 2005; Valarakos et al. 2004)) use regular expressions to identify named entities, concept methods extract relations using syntactic and lexical 10http://www.jfsowa.com/ontology/thematic.htm 11In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is the same as in AAT, and in 26% of the cases, either the extracted hypernym is more general than the one defined in AAT, or the contrary, patterns (Snow et al. 2005; Morin and Jaquemin 2004) or supervised clustering techniques (Kashyap et al. 2003). our work, we automatically learn not simply instances or taxonomies (e.g. the graphs of Figure 3) compliant with the semantics of a well-established core ontology, the CIDOC. The method is unsupervised, in the sense that it does not need manual annotation of a significant fragment of text. However, it relies on a set of manually written regular expressions, based on lexical, part-of-speech, and semantic constraints. The structure of regular expressions is rather more complex than in similar works using regular expressions, especially for the use of automatically verified semantic constraints. This complexity is indeed necessary to identify non-trivial relations in an unconstrained text and without training. The issue is however how much method other domains: • A first problem is the availability of lexical and semantic resources used by the algorithm. The most critical requirement of the method is the availability of sound which hopefully will be produced by other web communities stimulated by the recent success of CIDOC On the other side, absence of an agreed conceptual reference model, no large annotation is possible at all. for the other resources used by our algorithm, glossaries, thesaura and gazetteers are widely available in “mature” domains. If not, we developed a methodology, described in (Navigli and Velardi, 2005b), to automatically create a glossary in novel domains (e.g. enterprise interoperability), extracting definition sentences from domainrelevant documents and authoritative web sites. • The second problem is about the generality of regular expressions. Clearly, the relation checkers that we defined are tuned on the CIDOC properties. This however is consistent with our target: in specific domains users are interested to identify specific relations, not general purpose. Certain relevant application domains –like cultural heritage, e-commerce, or tourismare those that dictate specifications for realworld applications of NLP techniques. However, several CIDOC properties are rather general (especially locative and wrt the AAT hierarchy. 8 temporal relations) therefore some relation checkers easily apply to other domains, as demonstrated by the experiment on automatic annotation of historical archives in Table 4. Furthermore, the method used to verify semantic constraints is fully general, since it is based on WordNet and a generalpurpose, untrained semantic disambiguation algorithm, SSI. • Finally, the authors believe with some degree of convincement that automatic patternlearning methods often require non-trivial human effort just like manual methods (because of the need of annotated data, careful parameter setting, etc.), and furthermore they are unable to combine in a non-trivial way different types of features (e.g. lexical, syntactic, semantic). To make an example, a recent work on learning hypernymy patterns (Morin and Jacquemin, 2004) provides the full list of learned patterns. The complexity of these patterns is certainly lower than the regular expression structures used in this work, and many of them are rather intuitive. In the literature the tasks on which automatic methods have been tested are rather constrained, and do not convincingly demonstrate the superiority of automatic with respect to manually defined patterns. For example, in Senseval-3 labeling of semantic participating systems are requested to identify semantic roles in a sentence fragment for which the “frame semantics” is given, therefore the possible semantic relations to be identified are quite limited. However, we believe that our method can be automated to some degree (for example, machine learning methods can be used to bootstrap the syntactic patterns, and to learn semantic constraints), a research line we are currently exploring. References M. E. Califf and R.J. Mooney, “Bottom-up relational learning of pattern matching rules for information extraction” Machine Learning research, 4 (2)177- 210, 2004. E. Charniak, “Statistical Techniques for Natural Language Parsing”, AI Magazine 18(4), 33-44, 1997. P. Cimiano, G. Ladwig and S. Staab, “Gimme the context: context-driven automatic semantic 12http://www.clres.com/SensSemRoles.html annotation with C-PANKOW” In: Proceedings of the 14th International WWW Conference, WWW</abstract>
<address confidence="0.764048">2005, Chiba, Japan, May, 2005. ACM Press.</address>
<degree confidence="0.173457">M. Doerr, “The CIDOC Conceptual Reference Module: An Ontological Approach to Semantic</degree>
<affiliation confidence="0.870904">Interoperability of Metadata”. AI Magazine,</affiliation>
<address confidence="0.809188">Volume 24, Number 3, Fall 2003.</address>
<author confidence="0.995972">M S Fox</author>
<author confidence="0.995972">M Barbuceanu</author>
<author confidence="0.995972">M Gruninger</author>
<author confidence="0.995972">J Lin</author>
<title confidence="0.871576333333333">amp;quot;An Organisation Ontology for Enterprise Modeling&amp;quot;, In Simulating Organizations: Computational Models of Institutions and Groups,</title>
<email confidence="0.261772">M.Prietula,K.Carley&L.Gasser(Eds),Menlo</email>
<note confidence="0.935563766666667">Park CA: AAAI/MIT Press, pp. 131-152. 1997 J.E. F. Friedl “Mastering Regular Expressions” O’Reilly eds., ISBN: 1-56592-257-3, First edition January 1997. V. Kashyap, C. Ramakrishnan, T. Rindflesch. &amp;quot;Toward (Semi)-Automatic Generation of Biomedical Ontologies&amp;quot;, Proceedings of American Medical Informatics Association, 2003 G. A. Miller, ̏WordNet: a lexical database for English.&amp;quot; In: Communications of the ACM 38 (11), November 1995, pp. 39 - 41. E. Morin and C. Jacquemin “Automatic acquisition and expansion of hypernym links” Computer and the Humanities, 38: 363-396, 2004 R. Navigli, P. Velardi. Learning Domain Ontologies from Document Warehouses and Dedicated Linguistics MIT Press, June, 2004. R. Navigli and P. Velardi, “Structural Semantic Interconnections: a knowledge-based approach to word sense disambiguation”, Special Issue- Syntactic and Structural Pattern Recognition, IEEE TPAMI, Volume: 27, Issue: 7, 2005. R. Navigli, P. Velardi. Automatic Acquisition of a of Interoperability Terms, Proc. of World Praha, Czech Republic, July 4-8th, 2005b. R. Snow, D. Jurafsky, A. Y. Ng, &amp;quot;Learning syntactic patters for automatic hypernym discovery&amp;quot;, NIPS 17, 2005.</note>
<title confidence="0.71536525">M. Thelen, E. Riloff, &amp;quot;A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts&amp;quot;, Proceedings of the Conference on Empirical Methods in Natural Language</title>
<note confidence="0.626818363636364">Processing, 2002. M. Uschold, M. King, S. Moralee and Y. Zorgios, “The Enterprise Ontology”, The Knowledge Engineering Review , Vol. 13, Special Issue on Putting Ontologies to Use (eds. Uschold. M. and Tate. A.), 1998 Valarakos, G. Paliouras, V. Karkaletsis, G. Vouros, “Enhancing Ontological Knowledge through Ontology Population and Enrichment” in Proceedings of the 14th EKAW conf., LNAI, Vol. 3257, pp. 144-156, Springer Verlag, 2004.</note>
<intro confidence="0.564358">9</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M E Califf</author>
<author>R J Mooney</author>
</authors>
<title>Bottom-up relational learning of pattern matching rules for information extraction”</title>
<date>2004</date>
<journal>Machine Learning research,</journal>
<volume>4</volume>
<pages>2--177</pages>
<contexts>
<context position="27453" citStr="Califf and Mooney, 2004" startWordPosition="4355" endWordPosition="4358">ed of 83.58% (112/134) 96.55% (112/116) P4 – has time span 78.32% (112/143) 50.68% (112/221) P14 – carried out by 60.61% (40/66) - - P45 – consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task. 5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al. 2005; Valarakos et al. 2004)) use regular expressions to identify named entities, i.e. concept instances. Other methods extract hypernym11 relations using syntactic and lexical 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically ex</context>
</contexts>
<marker>Califf, Mooney, 2004</marker>
<rawString>M. E. Califf and R.J. Mooney, “Bottom-up relational learning of pattern matching rules for information extraction” Machine Learning research, 4 (2)177-210, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Techniques for Natural Language Parsing”,</title>
<date>1997</date>
<journal>AI Magazine</journal>
<volume>18</volume>
<issue>4</issue>
<pages>33--44</pages>
<contexts>
<context position="19172" citStr="Charniak, 1997" startWordPosition="2970" endWordPosition="2971">he subset of the CIDOC-CRM properties shown in Table 2 by two annotators with adjudication (requiring a careful comparison of the two sets of annotations). We performed two experiments: in the first, we evaluated the gloss annotation task, in the second the property instance extraction task, i.e. the ability to identify the appropriate domain and range of a property instance. In the case of the gloss annotation task, for evaluating each piece of information we adopted the measures of “labeled” precision and recall. These measures are commonly used to evaluate parse trees obtained by a parser (Charniak, 1997) and allow the rewarding of good partial results. Given a property R, labeled precision is the number of words annotated correctly with R over the number of words annotated automatically with R, while labeled recall is the number of words annotated correctly with R over the total number of words manually annotated with R. Table 3 shows the results obtained by applying the checkers to tag the test set (containing a total number of 1,328 distinct annotations and 5,965 annotated words). Note that here we are evaluating the ability of the system to assign the correct tag to every word in a gloss f</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak, “Statistical Techniques for Natural Language Parsing”, AI Magazine 18(4), 33-44, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>G Ladwig</author>
<author>S Staab</author>
</authors>
<title>Gimme the context: context-driven automatic semantic 12 http://www.clres.com/SensSemRoles.html annotation with C-PANKOW” In:</title>
<date>2005</date>
<booktitle>Proceedings of the 14th International WWW Conference, WWW 2005,</booktitle>
<publisher>ACM Press.</publisher>
<location>Chiba, Japan,</location>
<contexts>
<context position="27474" citStr="Cimiano et al. 2005" startWordPosition="4359" endWordPosition="4362">.55% (112/116) P4 – has time span 78.32% (112/143) 50.68% (112/221) P14 – carried out by 60.61% (40/66) - - P45 – consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task. 5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al. 2005; Valarakos et al. 2004)) use regular expressions to identify named entities, i.e. concept instances. Other methods extract hypernym11 relations using syntactic and lexical 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is t</context>
</contexts>
<marker>Cimiano, Ladwig, Staab, 2005</marker>
<rawString>P. Cimiano, G. Ladwig and S. Staab, “Gimme the context: context-driven automatic semantic 12 http://www.clres.com/SensSemRoles.html annotation with C-PANKOW” In: Proceedings of the 14th International WWW Conference, WWW 2005, Chiba, Japan, May, 2005. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Doerr</author>
</authors>
<title>The CIDOC Conceptual Reference Module: An Ontological Approach to Semantic Interoperability of Metadata”.</title>
<date>2003</date>
<journal>AI Magazine, Volume</journal>
<volume>24</volume>
<location>Fall</location>
<marker>Doerr, 2003</marker>
<rawString>M. Doerr, “The CIDOC Conceptual Reference Module: An Ontological Approach to Semantic Interoperability of Metadata”. AI Magazine, Volume 24, Number 3, Fall 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Fox</author>
<author>M Barbuceanu</author>
<author>M Gruninger</author>
<author>J Lin</author>
</authors>
<title>An Organisation Ontology for Enterprise Modeling&amp;quot;,</title>
<date>1997</date>
<booktitle>In Simulating Organizations: Computational Models of Institutions</booktitle>
<pages>131--152</pages>
<publisher>AAAI/MIT Press,</publisher>
<marker>Fox, Barbuceanu, Gruninger, Lin, 1997</marker>
<rawString>M. S. Fox, M. Barbuceanu, M. Gruninger, and J. Lin, &amp;quot;An Organisation Ontology for Enterprise Modeling&amp;quot;, In Simulating Organizations: Computational Models of Institutions and Groups, M. Prietula, K. Carley &amp; L. Gasser (Eds), Menlo Park CA: AAAI/MIT Press, pp. 131-152. 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E F</author>
</authors>
<title>Friedl “Mastering Regular Expressions”</title>
<date>1997</date>
<editor>O’Reilly eds., ISBN: 1-56592-257-3, First edition</editor>
<marker>F, 1997</marker>
<rawString>J.E. F. Friedl “Mastering Regular Expressions” O’Reilly eds., ISBN: 1-56592-257-3, First edition January 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kashyap</author>
<author>C Ramakrishnan</author>
<author>T Rindflesch</author>
</authors>
<title>Toward (Semi)-Automatic Generation of Biomedical Ontologies&amp;quot;,</title>
<date>2003</date>
<journal>Proceedings of American Medical Informatics Association,</journal>
<contexts>
<context position="28320" citStr="Kashyap et al. 2003" startWordPosition="4488" endWordPosition="4491">AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is the same as in AAT, and in 26% of the cases, either the extracted hypernym is more general than the one defined in AAT, or the contrary, patterns (Snow et al. 2005; Morin and Jaquemin 2004) or supervised clustering techniques (Kashyap et al. 2003). In our work, we automatically learn formal concepts, not simply instances or taxonomies (e.g. the graphs of Figure 3) compliant with the semantics of a well-established core ontology, the CIDOC. The method is unsupervised, in the sense that it does not need manual annotation of a significant fragment of text. However, it relies on a set of manually written regular expressions, based on lexical, part-of-speech, and semantic constraints. The structure of regular expressions is rather more complex than in similar works using regular expressions, especially for the use of automatically verified </context>
</contexts>
<marker>Kashyap, Ramakrishnan, Rindflesch, 2003</marker>
<rawString>V. Kashyap, C. Ramakrishnan, T. Rindflesch. &amp;quot;Toward (Semi)-Automatic Generation of Biomedical Ontologies&amp;quot;, Proceedings of American Medical Informatics Association, 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for English.&amp;quot; In:</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller, ̏WordNet: a lexical database for English.&amp;quot; In: Communications of the ACM 38 (11), November 1995, pp. 39 - 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Morin</author>
<author>C Jacquemin</author>
</authors>
<title>Automatic acquisition and expansion of hypernym links”</title>
<date>2004</date>
<journal>Computer and the Humanities,</journal>
<volume>38</volume>
<pages>363--396</pages>
<contexts>
<context position="31309" citStr="Morin and Jacquemin, 2004" startWordPosition="4931" endWordPosition="4934">e method used to verify semantic constraints is fully general, since it is based on WordNet and a generalpurpose, untrained semantic disambiguation algorithm, SSI. • Finally, the authors believe with some degree of convincement that automatic patternlearning methods often require non-trivial human effort just like manual methods (because of the need of annotated data, careful parameter setting, etc.), and furthermore they are unable to combine in a non-trivial way different types of features (e.g. lexical, syntactic, semantic). To make an example, a recent work on learning hypernymy patterns (Morin and Jacquemin, 2004) provides the full list of learned patterns. The complexity of these patterns is certainly lower than the regular expression structures used in this work, and many of them are rather intuitive. In the literature the tasks on which automatic methods have been tested are rather constrained, and do not convincingly demonstrate the superiority of automatic with respect to manually defined patterns. For example, in Senseval-3 (automated labeling of semantic roles12), participating systems are requested to identify semantic roles in a sentence fragment for which the “frame semantics” is given, there</context>
</contexts>
<marker>Morin, Jacquemin, 2004</marker>
<rawString>E. Morin and C. Jacquemin “Automatic acquisition and expansion of hypernym links” Computer and the Humanities, 38: 363-396, 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Learning Domain Ontologies from Document Warehouses and Dedicated Websites, Computational Linguistics</title>
<date></date>
<publisher>MIT Press,</publisher>
<marker>Navigli, Velardi, </marker>
<rawString>R. Navigli, P. Velardi. Learning Domain Ontologies from Document Warehouses and Dedicated Websites, Computational Linguistics (30-2), MIT Press, June, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Structural Semantic Interconnections: a knowledge-based approach to word sense disambiguation”, Special IssueSyntactic and Structural Pattern Recognition,</title>
<date>2005</date>
<journal>IEEE TPAMI, Volume:</journal>
<volume>27</volume>
<contexts>
<context position="15389" citStr="Navigli and Velardi, 2005" startWordPosition="2418" endWordPosition="2421">iguous, and they often are, especially if they do not belong to the domain glossary G. Considering the previous example of the property depicts, the word fruit is not a term of the AAT glossary, and it has 3 senses in WordNet (the fruit of a plant, the consequence of some action, an amount of product). The property depicts, as defined in the CIDOC, simply requires that the range be of type Entity (E1). Therefore, all the three senses of fruit in WordNet satisfy this constraint. Whenever the range constraints in a relation checker do not allow a full disambiguation, we apply the SSI algorithm (Navigli and Velardi, 2005), a semantic disambiguation algorithm based on structural pattern recognition, available on-line8. The algorithm is applied to the words belonging to the segment fragment f and is based on the detection of relevant semantic interconnection patterns between the appropriate senses. These patterns are extracted from a lexical knowledge base that merges WordNet with other resources, like word collocations, on-line dictionaries, etc. For example, in the fragment “depictions of fruit, flowers, and other objects” the following properties are created for the concept still_ life#1: Rdepicts(still_ life</context>
<context position="29748" citStr="Navigli and Velardi, 2005" startWordPosition="4708" endWordPosition="4711">omains: • A first problem is the availability of lexical and semantic resources used by the algorithm. The most critical requirement of the method is the availability of sound domain core ontologies, which hopefully will be produced by other web communities stimulated by the recent success of CIDOC CRM. On the other side, in absence of an agreed conceptual reference model, no large scale annotation is possible at all. As for the other resources used by our algorithm, glossaries, thesaura and gazetteers are widely available in “mature” domains. If not, we developed a methodology, described in (Navigli and Velardi, 2005b), to automatically create a glossary in novel domains (e.g. enterprise interoperability), extracting definition sentences from domainrelevant documents and authoritative web sites. • The second problem is about the generality of regular expressions. Clearly, the relation checkers that we defined are tuned on the CIDOC properties. This however is consistent with our target: in specific domains users are interested to identify specific relations, not general purpose. Certain relevant application domains –like cultural heritage, e-commerce, or tourismare those that dictate specifications for re</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>R. Navigli and P. Velardi, “Structural Semantic Interconnections: a knowledge-based approach to word sense disambiguation”, Special IssueSyntactic and Structural Pattern Recognition, IEEE TPAMI, Volume: 27, Issue: 7, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Automatic Acquisition of a Thesaurus of Interoperability Terms,</title>
<date>2005</date>
<booktitle>Proc. of 16th IFAC World Congress,</booktitle>
<location>Praha, Czech Republic,</location>
<contexts>
<context position="15389" citStr="Navigli and Velardi, 2005" startWordPosition="2418" endWordPosition="2421">iguous, and they often are, especially if they do not belong to the domain glossary G. Considering the previous example of the property depicts, the word fruit is not a term of the AAT glossary, and it has 3 senses in WordNet (the fruit of a plant, the consequence of some action, an amount of product). The property depicts, as defined in the CIDOC, simply requires that the range be of type Entity (E1). Therefore, all the three senses of fruit in WordNet satisfy this constraint. Whenever the range constraints in a relation checker do not allow a full disambiguation, we apply the SSI algorithm (Navigli and Velardi, 2005), a semantic disambiguation algorithm based on structural pattern recognition, available on-line8. The algorithm is applied to the words belonging to the segment fragment f and is based on the detection of relevant semantic interconnection patterns between the appropriate senses. These patterns are extracted from a lexical knowledge base that merges WordNet with other resources, like word collocations, on-line dictionaries, etc. For example, in the fragment “depictions of fruit, flowers, and other objects” the following properties are created for the concept still_ life#1: Rdepicts(still_ life</context>
<context position="29748" citStr="Navigli and Velardi, 2005" startWordPosition="4708" endWordPosition="4711">omains: • A first problem is the availability of lexical and semantic resources used by the algorithm. The most critical requirement of the method is the availability of sound domain core ontologies, which hopefully will be produced by other web communities stimulated by the recent success of CIDOC CRM. On the other side, in absence of an agreed conceptual reference model, no large scale annotation is possible at all. As for the other resources used by our algorithm, glossaries, thesaura and gazetteers are widely available in “mature” domains. If not, we developed a methodology, described in (Navigli and Velardi, 2005b), to automatically create a glossary in novel domains (e.g. enterprise interoperability), extracting definition sentences from domainrelevant documents and authoritative web sites. • The second problem is about the generality of regular expressions. Clearly, the relation checkers that we defined are tuned on the CIDOC properties. This however is consistent with our target: in specific domains users are interested to identify specific relations, not general purpose. Certain relevant application domains –like cultural heritage, e-commerce, or tourismare those that dictate specifications for re</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>R. Navigli, P. Velardi. Automatic Acquisition of a Thesaurus of Interoperability Terms, Proc. of 16th IFAC World Congress, Praha, Czech Republic, July 4-8th, 2005b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patters for automatic hypernym discovery&amp;quot;,</title>
<date>2005</date>
<journal>NIPS</journal>
<volume>17</volume>
<contexts>
<context position="28236" citStr="Snow et al. 2005" startWordPosition="4476" endWordPosition="4479">sing syntactic and lexical 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is the same as in AAT, and in 26% of the cases, either the extracted hypernym is more general than the one defined in AAT, or the contrary, patterns (Snow et al. 2005; Morin and Jaquemin 2004) or supervised clustering techniques (Kashyap et al. 2003). In our work, we automatically learn formal concepts, not simply instances or taxonomies (e.g. the graphs of Figure 3) compliant with the semantics of a well-established core ontology, the CIDOC. The method is unsupervised, in the sense that it does not need manual annotation of a significant fragment of text. However, it relies on a set of manually written regular expressions, based on lexical, part-of-speech, and semantic constraints. The structure of regular expressions is rather more complex than in simila</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky, A. Y. Ng, &amp;quot;Learning syntactic patters for automatic hypernym discovery&amp;quot;, NIPS 17, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts&amp;quot;,</title>
<date>2002</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="27428" citStr="Thelen and Riloff, 2002" startWordPosition="4351" endWordPosition="4354">ent location P46 – composed of 83.58% (112/134) 96.55% (112/116) P4 – has time span 78.32% (112/143) 50.68% (112/221) P14 – carried out by 60.61% (40/66) - - P45 – consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task. 5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al. 2005; Valarakos et al. 2004)) use regular expressions to identify named entities, i.e. concept instances. Other methods extract hypernym11 relations using syntactic and lexical 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the c</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen, E. Riloff, &amp;quot;A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts&amp;quot;, Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Uschold</author>
<author>M King</author>
<author>S Moralee</author>
<author>Y Zorgios</author>
</authors>
<title>The Enterprise Ontology”, The Knowledge Engineering Review ,</title>
<date>1998</date>
<booktitle>Special Issue on Putting Ontologies to Use</booktitle>
<volume>13</volume>
<editor>(eds. Uschold. M. and Tate. A.),</editor>
<marker>Uschold, King, Moralee, Zorgios, 1998</marker>
<rawString>M. Uschold, M. King, S. Moralee and Y. Zorgios, “The Enterprise Ontology”, The Knowledge Engineering Review , Vol. 13, Special Issue on Putting Ontologies to Use (eds. Uschold. M. and Tate. A.), 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Paliouras Valarakos</author>
<author>V Karkaletsis</author>
<author>G Vouros</author>
</authors>
<title>Enhancing Ontological Knowledge through Ontology Population and Enrichment”</title>
<date>2004</date>
<booktitle>in Proceedings of the 14th EKAW conf., LNAI,</booktitle>
<volume>3257</volume>
<pages>144--156</pages>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="27498" citStr="Valarakos et al. 2004" startWordPosition="4363" endWordPosition="4366">as time span 78.32% (112/143) 50.68% (112/221) P14 – carried out by 60.61% (40/66) - - P45 – consists of 85.71% (6/7) 37.50% (6/16) Avg. performance 78.26% (468/598) 77.10% (468/607) Table 4: Precision and Recall of a web document annotation task. 5 Related work This paper presented a method to automatically annotate the glosses of a thesaurus, the AAT, with the properties (conceptual relations) of a core ontology, the CIDOC-CRM. Several methods for ontology population and semantic annotation described in literature (e.g. (Thelen and Riloff, 2002; Califf and Mooney, 2004; Cimiano et al. 2005; Valarakos et al. 2004)) use regular expressions to identify named entities, i.e. concept instances. Other methods extract hypernym11 relations using syntactic and lexical 10 http://www.jfsowa.com/ontology/thematic.htm 11 In AAT the hypernym relation is already available, since AAT is a thesaurus, not a glossary. However we developed regular expressions also for hypernym extraction from definitions. For sake of space this is not discussed in this paper, however the remarkable result (wrt analogous evaluations in literature) is that in 34% of the cases the automatically extracted hypernym is the same as in AAT, and i</context>
</contexts>
<marker>Valarakos, Karkaletsis, Vouros, 2004</marker>
<rawString>Valarakos, G. Paliouras, V. Karkaletsis, G. Vouros, “Enhancing Ontological Knowledge through Ontology Population and Enrichment” in Proceedings of the 14th EKAW conf., LNAI, Vol. 3257, pp. 144-156, Springer Verlag, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>