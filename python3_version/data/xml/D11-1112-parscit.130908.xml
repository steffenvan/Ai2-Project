<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000157">
<title confidence="0.9793975">
Computation of Infix Probabilities
for Probabilistic Context-Free Grammars
</title>
<author confidence="0.983321">
Mark-Jan Nederhof
</author>
<affiliation confidence="0.999272">
School of Computer Science
University of St Andrews
</affiliation>
<address confidence="0.650201">
United Kingdom
</address>
<email confidence="0.993944">
markjan.nederhof@gmail.com
</email>
<author confidence="0.994946">
Giorgio Satta
</author>
<affiliation confidence="0.840003333333333">
Dept. of Information Engineering
University of Padua
Italy
</affiliation>
<email confidence="0.992168">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.998541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999332333333333">
The notion of infix probability has been intro-
duced in the literature as a generalization of
the notion of prefix (or initial substring) prob-
ability, motivated by applications in speech
recognition and word error correction. For the
case where a probabilistic context-free gram-
mar is used as language model, methods for
the computation of infix probabilities have
been presented in the literature, based on vari-
ous simplifying assumptions. Here we present
a solution that applies to the problem in its full
generality.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999694157894737">
Probabilistic context-free grammars (PCFGs for
short) are a statistical model widely used in natural
language processing. Several computational prob-
lems related to PCFGs have been investigated in
the literature, motivated by applications in model-
ing of natural language syntax. One such problem is
the computation of prefix probabilities for PCFGs,
where we are given as input a PCFG G and a string
w, and we are asked to compute the probability that
a sentence generated by G starts with w, that is, has
w as a prefix. This quantity is defined as the possi-
bly infinite sum of the probabilities of all strings of
the form wx, for any string x over the alphabet of G.
The problem of computation of prefix probabili-
ties for PCFGs was first formulated by Persoon and
Fu (1975). Efficient algorithms for its solution have
been proposed by Jelinek and Lafferty (1991) and
Stolcke (1995). Prefix probabilities can be used to
compute probability distributions for the next word
</bodyText>
<page confidence="0.806314">
1213
</page>
<bodyText confidence="0.999821911764706">
or part-of-speech, when a prefix of the input has al-
ready been processed, as discussed by Jelinek and
Lafferty (1991). Such distributions are useful for
speech recognition, where the result of the acous-
tic processor is represented as a lattice, and local
choices must be made for a next transition. In ad-
dition, distributions for the next word are also useful
for applications of word error correction, when one
is processing ‘noisy’ text and the parser recognizes
an error that must be recovered by operations of in-
sertion, replacement or deletion.
Motivated by the above applications, the problem
of the computation of infix probabilities for PCFGs
has been introduced in the literature as a generaliza-
tion of the prefix probability problem. We are now
given a PCFG G and a string w, and we are asked
to compute the probability that a sentence generated
by G has w as an infix. This probability is defined
as the possibly infinite sum of the probabilities of
all strings of the form xwy, for any pair of strings x
and y over the alphabet of G. Besides applications
in computation of the probability distribution for the
next word token and in word error correction, in-
fix probabilities can also be exploited in speech un-
derstanding systems to score partial hypotheses in
algorithms based on beam search, as discussed by
Corazza et al. (1991).
Corazza et al. (1991) have pointed out that the
computation of infix probabilities is more difficult
than the computation of prefix probabilities, due to
the added ambiguity that several occurrences of the
given infix can be found in a single string generated
by the PCFG. The authors developed solutions for
the case where some distribution can be defined on
</bodyText>
<note confidence="0.9485545">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1213–1221,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999100520833333">
the distance of the infix from the sentence bound- from rules in R to real numbers in the interval [0, 1].
aries, which is a simplifying assumption. The prob- The concept of left-most derivation in one step is
lem is also considered by Fred (2000), which pro- represented by the notation α π�g 0, which means
vides algorithms for the case where the language that the left-most occurrence of any nonterminal in
model is a probabilistic regular grammar. However, α E (E U N)* is rewritten by means of some rule
the algorithm in (Fred, 2000) does not apply to cases 7r E R. If the rewritten nonterminal is A, then 7r
with multiple occurrences of the given infix within must be of the form (A -+ -y) and 0 is the result
a string in the language, which is what was pointed of replacing the occurrence of A in α by -y. A left-
out to be the problematic case. most derivation with any number of steps, using a
In this paper we adopt a novel approach to the sequence d of rules, is denoted as α �g 0. We omit
problem of computation of infix probabilities, by re- d
moving the ambiguity that would be caused by mul- the subscript g when the PCFG is understood. We
tiple occurrences of the given infix. Although our also write α =*=&gt;. 0 when the involved sequence of
result is obtained by a combination of well-known rules is of no relevance. Henceforth, all derivations
techniques from the literature on PCFG parsing and we discuss are implicitly left-most.
pattern matching, as far as we know this is the first A complete derivation is either the empty se-
algorithm for the computation of infix probabilities quence of rules, or a sequence d = 7r1 · · · 7rm, m &gt;
that works for general PCFG models without any re- 1, of rules such that A A, w for some A E N and
strictive assumption. w E E*. In the latter case, we say the complete
The remainder of this paper is structured as fol- derivation starts with A, and in the former case, with
lows. In Section 2 we explain how the sum of the d an empty sequence of rules, we assume the com-
probabilities of all trees generated by a PCFG can plete derivation starts and ends with a single termi-
be computed as the least fixed-point solution of a nal, which is left unspecified. It is well-known that
non-linear system of equations. In Section 3 we re- there exists a bijective correspondence between left-
call the construction of a new PCFG out of a given most complete derivations starting with nonterminal
PCFG and a given finite automaton, such that the A and parse trees derived by the grammar with root
language generated by the new grammar is the in- A and a yield composed of terminal symbols only.
tersection of the languages generated by the given The depth of a complete derivation d is the length
PCFG and the automaton, and the probabilities of of the longest path from the root to a leaf in the parse
the generated strings are preserved. In Section 4 tree associated with d. The length of a path is de-
we show how one can efficiently construct an un- fined as the number of nodes it visits. Thus if d = 7r
ambiguous finite automaton that accepts all strings for some rule 7r = (A -+ w) with w E E*, then the
with a given infix. The material from these three depth of d is 2.
sections is combined into a new algorithm in Sec- The probability p(d) of a complete derivation d =
tion 5, which allows computation of the infix prob- 7r1 · · · 7rm, m &gt; 1, is:
ability for PCFGs. This is the main result of this p(d) = �m p(7ri).
paper. Several extensions of the basic technique are i=1
discussed in Section 6. Section 7 discusses imple- We also assume that p(d) = 1 when d is an empty
mentation and some experiments. sequence of rules. The probability p(w) of a string
2 Sum of probabilities of all derivations w is the sum of all complete derivations deriving that
Assume a probabilistic context-free grammar g, rep- string from the start symbol:
resented by a 5-tuple (E, N, 5, R, p), where E and p(w) = � p(d).
N are two finite disjoint sets of terminals and non- d: Sew
terminals, respectively, 5 E N is the start symbol, With this notation, consistency of a PCFG is de-
R is a finite set of rules, each of the form A -+ α,
where A E N and α E (E UN)*, and p is a function
1214
fined as the condition:
</bodyText>
<equation confidence="0.9830015">
E p(d) = 1.
d,w: Sd⇒w
</equation>
<bodyText confidence="0.999691">
In other words, a PCFG is consistent if the sum
of probabilities of all complete derivations starting
with S is 1. An equivalent definition of consistency
considers the sum of probabilities of all strings:
</bodyText>
<equation confidence="0.996909">
E
p(w) = 1.
w
</equation>
<bodyText confidence="0.99574025">
See (Booth and Thompson, 1973) for further discus-
sion.
In practice, PCFGs are often required to satisfy
the additional condition:
</bodyText>
<equation confidence="0.999329">
E p(π) = 1,
π=(A→α)
</equation>
<bodyText confidence="0.97673575">
for each A E N. This condition is called proper-
ness. PCFGs that naturally arise by parameter es-
timation from corpora are generally consistent; see
(S´anchez and Benedi, 1997; Chi and Geman, 1998).
However, in what follows, neither properness nor
consistency is guaranteed.
We define the partition function of !9 as the func-
tion Z that assigns to each A E N the value
</bodyText>
<equation confidence="0.9035185">
Z(A) = E p(A = w). (1)
d,w
</equation>
<bodyText confidence="0.999490277777778">
Note that Z(S) = 1 means that !9 is consistent.
More generally, in later sections we will need to
compute the partition function for non-consistent
PCFGs.
We can characterize the partition function of a
PCFG as a solution of a specific system of equa-
tions. Following the approach in (Harris, 1963; Chi,
1999), we introduce generating functions associated
with the nonterminals of the grammar. For A E N
and α E (N U Σ)∗, we write f(A, α) to denote the
number of occurrences of symbol A within string α.
Let N = {A1, A2,..., A|N|}. For each Ak E N, let
mk be the number of rules in R with left-hand side
Ak, and assume some fixed order for these rules. For
each i with 1 &lt; i &lt; mk, let Ak -+ αk,i be the i-th
rule with left-hand side Ak.
For each k with 1 &lt; k &lt; |N|, the generating
function associated with Ak is defined as
</bodyText>
<equation confidence="0.95552">
gAk(z1, z2, ... , z|N|) =
zf(Aj,αk,i)) (2)
j J
</equation>
<bodyText confidence="0.7081965">
Furthermore, for each i &gt; 1 we recursively define
functions g(Ak
</bodyText>
<equation confidence="0.998985272727273">
i) (z1, z2, ... , z|N|) by
1
gAk (z1, z2,... , z|N|) = gAk (z1, z2, ... , z|N|), (3)
and, for i &gt; 2, by
(gAk gAk(z1, z2,... , z|N|) =
(i) (4)
(i−1)
gA1 (z1, z2, ... , z|N|),
(i−1)
gA (z1, z2, ..., z|N|), ... ,
(i2 1)
</equation>
<bodyText confidence="0.992756230769231">
Using induction it is not difficult to show that, for
each k and i as above, g(i)
Ak(0, 0, ... , 0) is the sum of
the probabilities of all complete derivations from Ak
having depth not exceeding i. This implies that, for
i = 0, 1, 2, ..., the sequence of the g(i) (0, 0, ... , 0)
Ak
monotonically converges to Z(Ak).
For each k with 1 &lt; k &lt; |N |we can now write
= gAk(Z(A1), ... , Z(A|N|)).
The above shows that the values of the partition
function provide a solution to the system of the fol-
lowing equations, for 1 &lt; k &lt; |N|:
</bodyText>
<equation confidence="0.794843">
zk = gAk(z1, z2, ... , z|N|). (5)
</equation>
<bodyText confidence="0.99996725">
In the case of a general PCFG, the above equa-
tions are non-linear polynomials with positive (real)
coefficients. We can represent the resulting system
in vector form and write X = g(X). These systems
</bodyText>
<equation confidence="0.986980944444445">
|N|
(p(Ak -+ αk,i) � H
j=1
Emk
i=1
(i−1)
i→∞gAk ( gA1 (0, 0, ... , 0), ... ,
gA|N |(0, 0, ... , 0) )
(i−1)
(i−1)
= gAk ( limi→∞ gA1 (0, 0, ... , 0), ... ,
limi→∞ g(i−1)
A|N |(0, 0, ... , 0) )
Z(Ak) =
= lim g(i)
Ak(0, ... , 0)
i→∞
= lim
</equation>
<page confidence="0.850598">
1215
</page>
<bodyText confidence="0.999974561643836">
are called monotone systems of polynomial equa-
tions and have been investigated by Etessami and
Yannakakis (2009) and Kiefer et al. (2007). The
sought solution, that is, the partition function, is the
least fixed point solution of X = g(X).
For practical reasons, the set of nonterminals of
a grammar is usually divided into maximal subsets
of mutually recursive nonterminals, that is, for each
A and B in such a subset, we have A =*=&gt;. uBα
and B =*=&gt;. vAβ, for some u, v, α, β. This corre-
sponds to a strongly connected component if we
see the connection between the left-hand side of a
rule and a nonterminal member in its right-hand side
as an edge in a directed graph. For each strongly
connected component, there is a separate system of
equations of the form X = g(X). Such systems can
be solved one by one, in a bottom-up order. That
is, if one strongly connected component contains
nonterminal A, and another contains nonterminal B,
where A =*=&gt;. uBα for some u, α, then the system for
the latter component must be solved first.
The solution for a system of equations such as
those described above can be irrational and non-
expressible by radicals, even if we assume that all
the probabilities of the rules in the input PCFG are
rational numbers, as observed by Etessami and Yan-
nakakis (2009). Nonetheless, the partition function
can still be approximated to any degree of preci-
sion by iterative computation of the relation in (4),
as done for instance by Stolcke (1995) and by Ab-
ney et al. (1999). This corresponds to the so-called
fixed-point iteration method, which is well-known
in the numerical calculus literature and is frequently
applied to systems of non-linear equations because
it can be easily implemented.
When a number of standard conditions are met,
each iteration of (4) adds a fixed number of bits
to the precision of the solution; see Kelley (1995,
Chapter 4). Since each iteration can easily be im-
plemented to run in polynomial time, this means
that we can approximate the partition function of a
PCFG in polynomial time in the size of the PCFG
itself and in the number of bits of the desired preci-
sion.
In practical applications where large PCFGs are
empirically estimated from data sets, the standard
conditions mentioned above for the polynomial time
approximation of the partition function are usually
met. However, there are some degenerate cases for
which these standard conditions do not hold, result-
ing in exponential time behaviour of the fixed-point
iteration method. This has been firstly observed
in (Etessami and Yannakakis, 2005).
An alternative iterative algorithm for the approx-
imation of the partition function has been proposed
by Etessami and Yannakakis (2009), based on New-
ton’s method for the solution of non-linear systems
of equations. From a theoretical perspective, Kiefer
et al. (2007) have shown that, after a certain number
of initial iterations, Newton’s method adds a fixed
number of bits to the precision of the approximated
solution, even in the above mentioned cases in which
the fixed-point iteration method shows exponential
time behaviour. However, these authors also show
that, in some degenerate cases, the number of itera-
tions needed to compute the first bit of the solution
can be at least exponential in the size of the system.
Experiments with Newton’s method for the ap-
proximation of the partition functions of PCFGs
have been carried out in several application-oriented
settings, by Wojtczak and Etessami (2007) and by
Nederhof and Satta (2008), showing considerable
improvements over the fixed-point iteration method.
</bodyText>
<sectionHeader confidence="0.982171" genericHeader="method">
3 Intersection of PCFG and FA
</sectionHeader>
<bodyText confidence="0.99929665">
It was shown by Bar-Hillel et al. (1964) that context-
free languages are closed under intersection with
regular languages. Their proof relied on the con-
struction of a new CFG out of an input CFG and
an input finite automaton. Here we extend that con-
struction by letting the input grammar be a proba-
bilistic CFG. We refer the reader to (Nederhof and
Satta, 2003) for more details.
To avoid a number of technical complications, we
assume the finite automaton has no epsilon transi-
tions, and has only one final state. In the context
of our use of this construction in the following sec-
tions, these restrictions are without loss of general-
ity. Thus, a finite automaton (FA) M is represented
by a 5-tuple (E, Q, qo, q f, Δ), where E and Q are
two finite sets of terminals and states, respectively,
qo is the initial state, q f is the final state, and Δ is
a finite set of transitions, each of the form s t,
where s, t E Q and a E E.
A complete computation of M accepting string
</bodyText>
<page confidence="0.660471">
1216
</page>
<equation confidence="0.648812333333333">
w = a1 · · · an is a sequence c = T1 · · · Tn of tran-
sitions such that Ti = (si_1 7→ si) for each i (1 ≤
ai
</equation>
<bodyText confidence="0.958933583333333">
i ≤ n), for some s0, s1, ... , sn with s0 = q0 and
sn = qf. The language of all strings accepted by M
is denoted by L(M). A FA is unambiguous if at
most one complete computation exists for each ac-
cepted string. A FA is deterministic if there is at
most one transition s a
7→ t for each s and a.
For a FA M as above and a PCFG G = (E, N, S,
R, p) with the same set of terminals, we construct
a new PCFG G&apos; = (E, N&apos;, S&apos;, R&apos;, p&apos;), where N&apos; =
Q × (E ∪ N) × Q, S&apos; = (q0, S, qf), and R&apos; is the set
of rules that is obtained as follows.
</bodyText>
<listItem confidence="0.963678666666667">
• For each A → X1 · · · Xm in R and each se-
quence s0, ... , sm with si ∈ Q, 0 ≤ i ≤ m,
and m ≥ 0, let (s0, A, sm) → (s0, X1, s1) · · ·
(sm_1, Xm, sm) be in R&apos;; if m = 0, the new
rule is of the form (s0, A, s0) → E. Function p&apos;
assigns the same probability to the new rule as
p assigned to the original rule.
• For each s a
7→ t in Δ, let (s, a, t) → a be in R&apos;.
</listItem>
<bodyText confidence="0.99212115">
Function p&apos; assigns probability 1 to this rule.
Intuitively, a rule of G&apos; is either constructed out of
a rule of G or out of a transition of M. On the basis
of this correspondence between rules and transitions
of G&apos;, G and M, it is not difficult to see that each
derivation d&apos; in G&apos; deriving string w corresponds to a
unique derivation d in G deriving the same string and
a unique computation c in M accepting the same
string. Conversely, if there is a derivation d in G
deriving string w, and some computation c in M
accepting the same string, then the pair of d and c
corresponds to a unique derivation d&apos; in G&apos; deriving
the same string w. Furthermore, the probabilities of
d and d&apos; are equal, by definition of p&apos;.
Let us now assume that each string w is accepted
by at most one computation, i.e. M is unambigu-
ous. If a string w is accepted by M, then there are
as many derivations deriving w in G&apos; as there are in
G. If w is not accepted by M, then there are zero
derivations deriving w in G&apos;. Consequently:
</bodyText>
<equation confidence="0.991517166666667">
E p(d),
d,w:
S4GwnwEL(M)
or more succinctly:
E Ep&apos;(w) = p(w).
w wEL(M)
</equation>
<bodyText confidence="0.999961625">
Note that the above construction of G&apos; is exponen-
tial in the largest value of m in any rule from G. For
this reason, G is usually brought in binary form be-
fore the intersection, i.e. the input grammar is trans-
formed to let each right-hand side have at most two
members. Such a transformation can be realized in
linear time in the size of the grammar. We will return
to this issue in Section 7.
</bodyText>
<sectionHeader confidence="0.993379" genericHeader="method">
4 Obtaining unambiguous FAs
</sectionHeader>
<bodyText confidence="0.999456714285714">
In the previous section, we explained that unambigu-
ous finite automata have special properties with re-
spect to the grammar G&apos; that we may construct out
of a FA M and a PCFG G. In this section we dis-
cuss how unambiguity can be obtained for the spe-
cial case of finite automata accepting the language
of all strings with given infix w ∈ E*:
</bodyText>
<equation confidence="0.655867">
Linfix (w) = {xwy  |x, y ∈ E*}.
</equation>
<bodyText confidence="0.9984267">
Any deterministic automaton is also unambigu-
ous. Furthermore, there seem to be no practical al-
gorithms that turn FAs into equivalent unambiguous
FAs other than the algorithms that also determinize
them. Therefore, we will henceforth concentrate on
deterministic rather than unambiguous automata.
Given a string w = a1 · · · an, a finite automaton
accepting Linfix (w) can be straightforwardly con-
structed. This automaton has states s0, ... , sn, tran-
sitions s0 7→ s0 and sn
</bodyText>
<equation confidence="0.63123225">
a 7→ sn for each a ∈ E, and
a
transition si_1 7→ si for each i (1 ≤ i ≤ n). The
ai
</equation>
<bodyText confidence="0.99960175">
initial state is s0 and the final state is sn. Clearly,
there is nondeterminism in state s0.
One way to make this automaton deterministic is
to apply the general algorithm of determinization of
finite automata; see e.g. (Aho and Ullman, 1972).
This algorithm is exponential for general FAs. An
alternative approach is to construct a deterministic
finite automaton directly from w, in line with the
Knuth-Morris-Pratt algorithm (Knuth et al., 1977;
Gusfield, 1997). Both approaches result in the same
deterministic FA, which we denote by Iw. However,
the latter approach is easier to implement in such a
</bodyText>
<equation confidence="0.730653">
E p&apos;(d&apos;) =
</equation>
<page confidence="0.8968085">
d0,w:
,
S04,0w
1217
</page>
<bodyText confidence="0.979675">
way that the time complexity of constructing the au-
tomaton is linear in |w|.
The automaton Iw is described as follows. There
are n + 1 states t0, ... , tn, where as before n is
the length of w. The initial state is t0 and the final
state is tn. The intuition is that Iw reads a string
x = b1 · · · bm from left to right, and when it has
read the prefix b1 · · · bj (0 ≤ j ≤ m), it is in state
ti (0 ≤ i &lt; n) if and only if a1 · · · ai is the longest
prefix of w that is also a suffix of b1 · · · bj. If the
automaton is in state tn, then this means that w is an
infix of b1 ··· bj.
In more detail, for each i (1 ≤ i ≤ n) and each
a ∈ E, there is a transition ti_1 7→ tj, where j is
a
the length of the longest string that is both a prefix
of w and a suffix of a1 · · · ai_1a. If a = ai, then
clearly j = i, and otherwise j &lt; i. To ensure that we
remain in the final state once an occurrence of infix
w has been found, we also add transitions tn 7→ tn
</bodyText>
<figure confidence="0.919906">
a
</figure>
<figureCaption confidence="0.9575225">
for each a ∈ E. This construction is illustrated in
Figure 1.
</figureCaption>
<sectionHeader confidence="0.990502" genericHeader="method">
5 Infix probability
</sectionHeader>
<bodyText confidence="0.9946042">
With the material developed in the previous sections,
the problem of computing the infix probabilities can
be effectively solved. Our goal is to compute for
given infix w ∈ E* and PCFG G = (E, N, 5, R,
p):
</bodyText>
<equation confidence="0.986311">
EQinfix(w, G) = p(z).
zELinfix (w)
</equation>
<bodyText confidence="0.998369333333333">
In Section 4 we have shown the construction of finite
automaton Iw accepting Linfix (w), by which we ob-
tain:
</bodyText>
<sectionHeader confidence="0.998061" genericHeader="method">
6 Extensions
</sectionHeader>
<bodyText confidence="0.9999912">
The approach discussed above allows for a number
of generalizations. First, we can replace the infix w
by a sequence of infixes w1, ... , wm, which have to
occur in the given order, one strictly after the other,
with arbitrary infixes in between:
</bodyText>
<equation confidence="0.979414">
Qisland(w1, . . ., wm, G) =
E p(x0w1x1 ··· wmxm).
xp,...,xmEΣ∗
</equation>
<bodyText confidence="0.999843181818182">
This problem was discussed before by (Corazza et
al., 1991), who mentioned applications in speech
recognition. Further applications are found in com-
putational biology, but their discussion is beyond the
scope of this paper; see for instance (Apostolico et
al., 2005) and references therein. In order to solve
the problem, we only need a small addition to the
procedures we discussed before. First we construct
separate automata Iwj (1 ≤ j ≤ m) as explained in
Section 4. These automata are then composed into
a single automaton I(w1,...,wm). In this composition,
the outgoing transitions of the final state of Iwj, for
each j (1 ≤ j &lt; m), are removed and that final state
is merged with the initial state of the next automaton
Iwj+1. The initial state of the composed automaton
is the initial state of Iw1, and the final state is the
final state of Iwm. The time costs of constructing
I(w1,...,wm) are linear in the sum of the lengths of the
strings wj.
Another way to generalize the problem is to re-
place w by a finite set L = {w1, ... , wm}. The ob-
jective is to compute:
</bodyText>
<equation confidence="0.9953975">
Qinfix (L, G) = E p(xwy)
wEL,x,yEΣ∗
EQinfix(w, G) = p(z).
zEL(Zw)
</equation>
<bodyText confidence="0.98410375">
As Iw is deterministic and therefore unambiguous,
the results from Section 3 apply and if G&apos; = (E, N&apos;,
5&apos;, R&apos;, p&apos;) is the PCFG constructed out of G and Iw
then:
</bodyText>
<equation confidence="0.8311505">
EQinfix(w, G) = p&apos;(z).
z
</equation>
<bodyText confidence="0.999825357142857">
Finally, we can compute the above sum by applying
the iterative method discussed in Section 2.
Again, this can be solved by first constructing a de-
terministic FA, which is then intersected with G.
This FA can be obtained by determinizing a straight-
forward nondeterministic FA accepting L, or by di-
rectly constructing a deterministic FA along the lines
of the Aho-Corasick algorithm (Aho and Corasick,
1975). Construction of the automaton with the latter
approach takes linear time.
Further straightforward generalizations involve
formalisms such as probabilistic tree adjoining
grammars (Schabes, 1992; Resnik, 1992). The tech-
nique from Section 3 is also applicable in this case,
</bodyText>
<page confidence="0.948303">
1218
</page>
<figure confidence="0.99897525">
b,c c a b a, b, c
a b a c
t0 t1 t2 t3 t4
b, c a
</figure>
<figureCaption confidence="0.99998">
Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac.
</figureCaption>
<bodyText confidence="0.9943556">
as the construction from Bar-Hillel et al. (1964) car-
ries over from context-free grammars to tree ad-
joining grammars, and more generally to the linear
context-free rewriting systems of Vijay-Shanker et
al. (1987).
</bodyText>
<sectionHeader confidence="0.995676" genericHeader="method">
7 Implementation
</sectionHeader>
<bodyText confidence="0.999823452830189">
We have conducted experiments with the computa-
tion of infix probabilities. The objective was to iden-
tify parts of the computation that have a high time
or space demand, and that might be improved. The
experiments were run on a desktop with a 3.0 GHz
Pentium 4 processor. The implementation language
is C++.
The set-up of the experiments is similar to that in
(Nederhof and Satta, 2008). A probabilistic context-
free grammar was extracted from sections 2-21 of
the Penn Treebank version II. Subtrees that gener-
ated the empty string were systematically removed.
The result was a CFG with 10,035 rules, 28 nonter-
minals and 36 parts-of-speech. The rule probabili-
ties were determined by maximum likelihood esti-
mation. The grammar was subsequently binarized,
to avoid exponential behaviour, as explained in Sec-
tion 3.
We have considered 10 strings of length 7, ran-
domly generated, assuming each of the parts-of-
speech has the same probability. For all prefixes of
those strings from length 2 to length 7, we then com-
puted the infix probability. The duration of the full
computation, averaged over the 10 strings of length
7, is given in the first row of Table 1.
In order to solve the non-linear systems of equa-
tions, we used Broyden’s method. It can be seen
as an approximation of Newton’s method. It re-
quires more iterations, but seems to be faster over-
all, and more scalable to large problem sizes, due to
the avoidance of matrix inversion, which sometimes
makes Newton’s method prohibitively expensive. In
our experiments, Broyden’s method was generally
faster than Newton’s method and much faster than
the simple iteration method by the relation in (4).
For further details on Broyden’s method, we refer
the reader to (Kelley, 1995).
The main obstacle to computation for infixes sub-
stantially longer than 7 symbols is the memory con-
sumption rather than the running time. This is due
to the required square matrices, the dimension of
which is the number of nonterminals. The number
of nonterminals (of the intersection grammar) natu-
rally grows as the infix becomes longer.
As explained in Section 2, the problem is divided
into smaller problems by isolating disjoint sets of
mutually recursive nonterminals, or strongly con-
nected components. We found that for the applica-
tion to the automata discussed in Section 4, there
were exactly three strongly connected components
that contained more than one element, throughout
the experiments. For an infix of length n, these com-
ponents are:
</bodyText>
<listItem confidence="0.994651333333333">
• C1, which consists of nonterminals of the form
(tZ, A, tj), where i &lt; n and j &lt; n,
• C2, which consists of nonterminals of the form
(tZ, A, tj), where i = j = n, and
• C3, which consists of nonterminals of the form
(tZ, A, tj), where i &lt; j = n.
</listItem>
<bodyText confidence="0.9964272">
This can be easily explained by looking at the struc-
ture of our automata. See for example Figure 1, with
cycles running through states t0, ... , tn−1, and cy-
cles through state tn. Furthermore, the grammar ex-
tracted from the Penn Treebank is heavily recursive,
</bodyText>
<page confidence="0.966176">
1219
</page>
<table confidence="0.818031">
infix length 2 3 4 5 6 7
total running time 1.07 1.95 5.84 11.38 23.93 45.91
Broyden’s method for C1 0.46 0.90 3.42 6.63 12.91 24.38
Broyden’s method for C2 0.08 0.04 0.07 0.04 0.03 0.09
Broyden’s method for C3 0.20 0.36 0.81 1.74 5.30 9.02
</table>
<tableCaption confidence="0.988813">
Table 1: Running time for infixes from length 2 to length 7. The infixes are prefixes of 10 random strings of length 7,
and reported CPU times (in seconds) are averaged over the 10 strings.
</tableCaption>
<bodyText confidence="0.998551703703704">
so that almost every nonterminal can directly or in-
directly call any other.
The strongly connected component C2 is always
the same, consisting of 2402 nonterminals, for each
infix of any length. (Note that binarization of the
grammar introduced artificial nonterminals.) The
last three rows of Table 1 present the time costs of
Broyden’s method, for the three strongly connected
components.
The strongly connected component C3 happens to
correspond to a linear system of equations. This is
because a rule in the intersection grammar with a
left-hand side (ti, A, tj), where i &lt; j = n, must
have a right-hand side of the form (ti, A&apos;, tj), or of
the form (ti, A1, tk) (tk, A2, tj), with k ≤ n. If k &lt;
n, then only the second member can be in C3. If
k = n, only first member can be in C3. Hence,
such a rule corresponds to a linear equation within
the system of equations for the entire grammar.
A linear system of equations can be solved an-
alytically, for example by Gaussian elimination,
rather than approximated through Newton’s method
or Broyden’s method. This means that the running
times in the last row of Table 1 can be reduced by
treating C3 differently from the other strongly con-
nected components. However, the running time for
C1 dominates the total time consumption.
The above investigations were motivated by two
questions, namely whether any part of the computa-
tion can be precomputed, and second, whether infix
probabilities can be computed incrementally, for in-
fixes that are extended to the left or to the right. The
first question can be answered affirmatively for C2,
as it is always the same. However, as we can see in
Table 1, the computation of C2 amounts to a small
portion of the total time consumption.
The second question can be rephrased more pre-
cisely as follows. Suppose we have computed the
infix probability of a string w and have kept inter-
mediate results in memory. Can the computation of
the infix probability of a string of the form aw or wa,
a ∈ E, be computed by relying on the existing re-
sults, so that the computation is substantially faster
than if the computation were done from scratch?
Our investigations so far have not found a posi-
tive answer to this second question. In particular,
the systems of equations for C1 and C3 change fun-
damentally if the infix is extended by one more sym-
bol, which seems to at least make incremental com-
putation very difficult, if not impossible. Note that
the algorithms for the computation of prefix prob-
abilities by Jelinek and Lafferty (1991) and Stolcke
(1995) do allow incrementality, which contributes to
their practical usefulness for speech recognition.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999987">
We have shown that the problem of infix probabili-
ties for PCFGs can be solved by a construction that
intersects a context-free language with a regular lan-
guage. An important constraint is that the finite
automaton that is input to this construction be un-
ambiguous. We have shown that such an automa-
ton can be efficiently constructed. Once the input
probabilistic PCFG and the FA have been combined
into a new probabilistic CFG, the infix probability
can be straightforwardly solved by iterative algo-
rithms. Such algorithms include Newton’s method,
and Broyden’s method, which was used in our exper-
iments. Our discussion ended with an open question
about the possibility of incremental computation of
infix probabilities.
</bodyText>
<sectionHeader confidence="0.997377" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.809752">
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
</reference>
<page confidence="0.897139">
1220
</page>
<reference confidence="0.998949113207547">
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542–549,
Maryland, USA, June.
A.V. Aho and M.J. Corasick. 1975. Efficient string
matching: an aid to bibliographic search. Communi-
cations of the ACM, 18(6):333–340, June.
A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1
of The Theory of Parsing, Translation and Compiling.
Prentice-Hall, Englewood Cliffs, N.J.
A. Apostolico, M. Comin, and L. Parida. 2005. Con-
servative extraction of overrepresented extensible mo-
tifs. In Proceedings of Intelligent Systems for Molecu-
lar Biology (ISMB05).
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, chap-
ter 9, pages 116–150. Addison-Wesley, Reading, Mas-
sachusetts.
T.L. Booth and R.A. Thompson. 1973. Applying prob-
abilistic measures to abstract languages. IEEE Trans-
actions on Computers, C-22:442–450.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299–305.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160.
A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an island-
driven parser. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 13(9):936–950.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd International
Symposium on Theoretical Aspects of Computer Sci-
ence, volume 3404 of Lecture Notes in Computer Sci-
ence, pages 340–352, Stuttgart, Germany. Springer-
Verlag.
K. Etessami and M. Yannakakis. 2009. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. Journal of the ACM,
56(1):1–66.
A.L.N. Fred. 2000. Computation of substring proba-
bilities in stochastic grammars. In A. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Applica-
tions, volume 1891 of Lecture Notes in Artificial Intel-
ligence, pages 103–114. Springer-Verlag.
D. Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press, Cambridge.
T.E. Harris. 1963. The Theory of Branching Processes.
Springer-Verlag, Berlin, Germany.
F. Jelinek and J.D. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315–323.
C.T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the
convergence of Newton’s method for monotone sys-
tems of polynomial equations. In Proceedings of the
39th ACM Symposium on Theory of Computing, pages
217–266.
D.E. Knuth, J.H. Morris, Jr., and V.R. Pratt. 1977. Fast
pattern matching in strings. SIAM Journal on Comput-
ing, 6:323–350.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop on
Parsing Technologies, pages 137–148, LORIA, Nancy,
France, April.
M.-J. Nederhof and G. Satta. 2008. Computing parti-
tion functions of PCFGs. Research on Language and
Computation, 6(2):139–162.
E. Persoon and K.S. Fu. 1975. Sequential classification
of strings generated by SCFG’s. International Journal
of Computer and Information Sciences, 4(3):205–217.
P. Resnik. 1992. Probabilistic tree-adjoining grammar as
a framework for statistical natural language process-
ing. In Proc. of the fifteenth International Conference
on Computational Linguistics, Nantes, August, pages
418–424.
J.-A. S´anchez and J.-M. Benedi. 1997. Consistency
of stochastic context-free grammars from probabilis-
tic estimation based on growth transformations. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(9):1052–1055, September.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of the fifteenth International Con-
ference on Computational Linguistics, Nantes, Au-
gust, pages 426–432.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):167–201.
K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 104–111,
Stanford, California, USA, July.
D. Wojtczak and K. Etessami. 2007. PReMo: an an-
alyzer for Probabilistic Recursive Models. In Tools
and Algorithms for the Construction and Analysis of
Systems, 13th International Conference, volume 4424
of Lecture Notes in Computer Science, pages 66–71,
Braga, Portugal. Springer-Verlag.
</reference>
<page confidence="0.9913">
1221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820957">
<title confidence="0.996341">Computation of Infix for Probabilistic Context-Free Grammars</title>
<author confidence="0.978924">Mark-Jan</author>
<affiliation confidence="0.986238">School of Computer University of St United</affiliation>
<email confidence="0.995421">markjan.nederhof@gmail.com</email>
<author confidence="0.989385">Giorgio</author>
<affiliation confidence="0.998598">Dept. of Information University of</affiliation>
<email confidence="0.974446">satta@dei.unipd.it</email>
<abstract confidence="0.993808307692308">The notion of infix probability has been introduced in the literature as a generalization of the notion of prefix (or initial substring) probability, motivated by applications in speech recognition and word error correction. For the case where a probabilistic context-free grammar is used as language model, methods for the computation of infix probabilities have been presented in the literature, based on various simplifying assumptions. Here we present a solution that applies to the problem in its full generality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>D McAllester</author>
<author>F Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>542--549</pages>
<location>Maryland, USA,</location>
<contexts>
<context position="12378" citStr="Abney et al. (1999)" startWordPosition="2251" endWordPosition="2255">ains nonterminal A, and another contains nonterminal B, where A =*=&gt;. uBα for some u, α, then the system for the latter component must be solved first. The solution for a system of equations such as those described above can be irrational and nonexpressible by radicals, even if we assume that all the probabilities of the rules in the input PCFG are rational numbers, as observed by Etessami and Yannakakis (2009). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al. (1999). This corresponds to the so-called fixed-point iteration method, which is well-known in the numerical calculus literature and is frequently applied to systems of non-linear equations because it can be easily implemented. When a number of standard conditions are met, each iteration of (4) adds a fixed number of bits to the precision of the solution; see Kelley (1995, Chapter 4). Since each iteration can easily be implemented to run in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself and in the number of bits </context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>S. Abney, D. McAllester, and F. Pereira. 1999. Relating probabilistic grammars and automata. In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 542–549, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>M J Corasick</author>
</authors>
<title>Efficient string matching: an aid to bibliographic search.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="22973" citStr="Aho and Corasick, 1975" startWordPosition="4279" endWordPosition="4282"> EQinfix(w, G) = p(z). zEL(Zw) As Iw is deterministic and therefore unambiguous, the results from Section 3 apply and if G&apos; = (E, N&apos;, 5&apos;, R&apos;, p&apos;) is the PCFG constructed out of G and Iw then: EQinfix(w, G) = p&apos;(z). z Finally, we can compute the above sum by applying the iterative method discussed in Section 2. Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, 1218 b,c c a b a, b, c a b a c t0 t1 t2 t3 t4 b, c a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting</context>
</contexts>
<marker>Aho, Corasick, 1975</marker>
<rawString>A.V. Aho and M.J. Corasick. 1975. Efficient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333–340, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<journal>Parsing,</journal>
<booktitle>of The Theory of Parsing, Translation and Compiling. Prentice-Hall,</booktitle>
<volume>1</volume>
<location>Englewood Cliffs, N.J.</location>
<contexts>
<context position="19142" citStr="Aho and Ullman, 1972" startWordPosition="3538" endWordPosition="3541">t also determinize them. Therefore, we will henceforth concentrate on deterministic rather than unambiguous automata. Given a string w = a1 · · · an, a finite automaton accepting Linfix (w) can be straightforwardly constructed. This automaton has states s0, ... , sn, transitions s0 7→ s0 and sn a 7→ sn for each a ∈ E, and a transition si_1 7→ si for each i (1 ≤ i ≤ n). The ai initial state is s0 and the final state is sn. Clearly, there is nondeterminism in state s0. One way to make this automaton deterministic is to apply the general algorithm of determinization of finite automata; see e.g. (Aho and Ullman, 1972). This algorithm is exponential for general FAs. An alternative approach is to construct a deterministic finite automaton directly from w, in line with the Knuth-Morris-Pratt algorithm (Knuth et al., 1977; Gusfield, 1997). Both approaches result in the same deterministic FA, which we denote by Iw. However, the latter approach is easier to implement in such a E p&apos;(d&apos;) = d0,w: , S04,0w 1217 way that the time complexity of constructing the automaton is linear in |w|. The automaton Iw is described as follows. There are n + 1 states t0, ... , tn, where as before n is the length of w. The initial st</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A.V. Aho and J.D. Ullman. 1972. Parsing, volume 1 of The Theory of Parsing, Translation and Compiling. Prentice-Hall, Englewood Cliffs, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Apostolico</author>
<author>M Comin</author>
<author>L Parida</author>
</authors>
<title>Conservative extraction of overrepresented extensible motifs.</title>
<date>2005</date>
<booktitle>In Proceedings of Intelligent Systems for Molecular Biology (ISMB05).</booktitle>
<contexts>
<context position="21508" citStr="Apostolico et al., 2005" startWordPosition="4011" endWordPosition="4014">ng Linfix (w), by which we obtain: 6 Extensions The approach discussed above allows for a number of generalizations. First, we can replace the infix w by a sequence of infixes w1, ... , wm, which have to occur in the given order, one strictly after the other, with arbitrary infixes in between: Qisland(w1, . . ., wm, G) = E p(x0w1x1 ··· wmxm). xp,...,xmEΣ∗ This problem was discussed before by (Corazza et al., 1991), who mentioned applications in speech recognition. Further applications are found in computational biology, but their discussion is beyond the scope of this paper; see for instance (Apostolico et al., 2005) and references therein. In order to solve the problem, we only need a small addition to the procedures we discussed before. First we construct separate automata Iwj (1 ≤ j ≤ m) as explained in Section 4. These automata are then composed into a single automaton I(w1,...,wm). In this composition, the outgoing transitions of the final state of Iwj, for each j (1 ≤ j &lt; m), are removed and that final state is merged with the initial state of the next automaton Iwj+1. The initial state of the composed automaton is the initial state of Iw1, and the final state is the final state of Iwm. The time cos</context>
</contexts>
<marker>Apostolico, Comin, Parida, 2005</marker>
<rawString>A. Apostolico, M. Comin, and L. Parida. 2005. Conservative extraction of overrepresented extensible motifs. In Proceedings of Intelligent Systems for Molecular Biology (ISMB05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>Language and Information: Selected Essays on their Theory and Application, chapter 9,</booktitle>
<pages>116--150</pages>
<editor>In Y. Bar-Hillel, editor,</editor>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="14533" citStr="Bar-Hillel et al. (1964)" startWordPosition="2594" endWordPosition="2597">the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, the number of iterations needed to compute the first bit of the solution can be at least exponential in the size of the system. Experiments with Newton’s method for the approximation of the partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method. 3 Intersection of PCFG and FA It was shown by Bar-Hillel et al. (1964) that contextfree languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details. To avoid a number of technical complications, we assume the finite automaton has no epsilon transitions, and has only one final state. In the context of our use of this construction in the following sections, these restrictions are without loss of generality. T</context>
<context position="23449" citStr="Bar-Hillel et al. (1964)" startWordPosition="4361" endWordPosition="4364">ndeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, 1218 b,c c a b a, b, c a b a c t0 t1 t2 t3 t4 b, c a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or space demand, and that might be improved. The experiments were run on a desktop with a 3.0 GHz Pentium 4 processor. The implementation language is C++. The set-up of the experiments is similar to that in (Nederhof and Satta, 2008). A probabilistic contextfree gra</context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, chapter 9, pages 116–150. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Booth</author>
<author>R A Thompson</author>
</authors>
<title>Applying probabilistic measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<pages>22--442</pages>
<contexts>
<context position="8210" citStr="Booth and Thompson, 1973" startWordPosition="1445" endWordPosition="1448">ented by a 5-tuple (E, N, 5, R, p), where E and p(w) = � p(d). N are two finite disjoint sets of terminals and non- d: Sew terminals, respectively, 5 E N is the start symbol, With this notation, consistency of a PCFG is deR is a finite set of rules, each of the form A -+ α, where A E N and α E (E UN)*, and p is a function 1214 fined as the condition: E p(d) = 1. d,w: Sd⇒w In other words, a PCFG is consistent if the sum of probabilities of all complete derivations starting with S is 1. An equivalent definition of consistency considers the sum of probabilities of all strings: E p(w) = 1. w See (Booth and Thompson, 1973) for further discussion. In practice, PCFGs are often required to satisfy the additional condition: E p(π) = 1, π=(A→α) for each A E N. This condition is called properness. PCFGs that naturally arise by parameter estimation from corpora are generally consistent; see (S´anchez and Benedi, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed. We define the partition function of !9 as the function Z that assigns to each A E N the value Z(A) = E p(A = w). (1) d,w Note that Z(S) = 1 means that !9 is consistent. More generally, in later sections we w</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>T.L. Booth and R.A. Thompson. 1973. Applying probabilistic measures to abstract languages. IEEE Transactions on Computers, C-22:442–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
<author>S Geman</author>
</authors>
<title>Estimation of probabilistic context-free grammars.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="8525" citStr="Chi and Geman, 1998" startWordPosition="1498" endWordPosition="1501">ion 1214 fined as the condition: E p(d) = 1. d,w: Sd⇒w In other words, a PCFG is consistent if the sum of probabilities of all complete derivations starting with S is 1. An equivalent definition of consistency considers the sum of probabilities of all strings: E p(w) = 1. w See (Booth and Thompson, 1973) for further discussion. In practice, PCFGs are often required to satisfy the additional condition: E p(π) = 1, π=(A→α) for each A E N. This condition is called properness. PCFGs that naturally arise by parameter estimation from corpora are generally consistent; see (S´anchez and Benedi, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed. We define the partition function of !9 as the function Z that assigns to each A E N the value Z(A) = E p(A = w). (1) d,w Note that Z(S) = 1 means that !9 is consistent. More generally, in later sections we will need to compute the partition function for non-consistent PCFGs. We can characterize the partition function of a PCFG as a solution of a specific system of equations. Following the approach in (Harris, 1963; Chi, 1999), we introduce generating functions associated with the nonterminals of the grammar. For A E </context>
</contexts>
<marker>Chi, Geman, 1998</marker>
<rawString>Z. Chi and S. Geman. 1998. Estimation of probabilistic context-free grammars. Computational Linguistics, 24(2):299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="9032" citStr="Chi, 1999" startWordPosition="1592" endWordPosition="1593"> estimation from corpora are generally consistent; see (S´anchez and Benedi, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed. We define the partition function of !9 as the function Z that assigns to each A E N the value Z(A) = E p(A = w). (1) d,w Note that Z(S) = 1 means that !9 is consistent. More generally, in later sections we will need to compute the partition function for non-consistent PCFGs. We can characterize the partition function of a PCFG as a solution of a specific system of equations. Following the approach in (Harris, 1963; Chi, 1999), we introduce generating functions associated with the nonterminals of the grammar. For A E N and α E (N U Σ)∗, we write f(A, α) to denote the number of occurrences of symbol A within string α. Let N = {A1, A2,..., A|N|}. For each Ak E N, let mk be the number of rules in R with left-hand side Ak, and assume some fixed order for these rules. For each i with 1 &lt; i &lt; mk, let Ak -+ αk,i be the i-th rule with left-hand side Ak. For each k with 1 &lt; k &lt; |N|, the generating function associated with Ak is defined as gAk(z1, z2, ... , z|N|) = zf(Aj,αk,i)) (2) j J Furthermore, for each i &gt; 1 we recursiv</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corazza</author>
<author>R De Mori</author>
<author>R Gretter</author>
<author>G Satta</author>
</authors>
<title>Computation of probabilities for an islanddriven parser.</title>
<date>1991</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>13</volume>
<issue>9</issue>
<marker>Corazza, De Mori, Gretter, Satta, 1991</marker>
<rawString>A. Corazza, R. De Mori, R. Gretter, and G. Satta. 1991. Computation of probabilities for an islanddriven parser. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(9):936–950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Etessami</author>
<author>M Yannakakis</author>
</authors>
<title>Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations.</title>
<date>2005</date>
<booktitle>In 22nd International Symposium on Theoretical Aspects of Computer Science,</booktitle>
<volume>3404</volume>
<pages>340--352</pages>
<publisher>SpringerVerlag.</publisher>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="13443" citStr="Etessami and Yannakakis, 2005" startWordPosition="2422" endWordPosition="2425"> in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself and in the number of bits of the desired precision. In practical applications where large PCFGs are empirically estimated from data sets, the standard conditions mentioned above for the polynomial time approximation of the partition function are usually met. However, there are some degenerate cases for which these standard conditions do not hold, resulting in exponential time behaviour of the fixed-point iteration method. This has been firstly observed in (Etessami and Yannakakis, 2005). An alternative iterative algorithm for the approximation of the partition function has been proposed by Etessami and Yannakakis (2009), based on Newton’s method for the solution of non-linear systems of equations. From a theoretical perspective, Kiefer et al. (2007) have shown that, after a certain number of initial iterations, Newton’s method adds a fixed number of bits to the precision of the approximated solution, even in the above mentioned cases in which the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, th</context>
</contexts>
<marker>Etessami, Yannakakis, 2005</marker>
<rawString>K. Etessami and M. Yannakakis. 2005. Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations. In 22nd International Symposium on Theoretical Aspects of Computer Science, volume 3404 of Lecture Notes in Computer Science, pages 340–352, Stuttgart, Germany. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Etessami</author>
<author>M Yannakakis</author>
</authors>
<title>Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations.</title>
<date>2009</date>
<journal>Journal of the ACM,</journal>
<volume>56</volume>
<issue>1</issue>
<contexts>
<context position="10989" citStr="Etessami and Yannakakis (2009)" startWordPosition="2002" endWordPosition="2005"> the following equations, for 1 &lt; k &lt; |N|: zk = gAk(z1, z2, ... , z|N|). (5) In the case of a general PCFG, the above equations are non-linear polynomials with positive (real) coefficients. We can represent the resulting system in vector form and write X = g(X). These systems |N| (p(Ak -+ αk,i) � H j=1 Emk i=1 (i−1) i→∞gAk ( gA1 (0, 0, ... , 0), ... , gA|N |(0, 0, ... , 0) ) (i−1) (i−1) = gAk ( limi→∞ gA1 (0, 0, ... , 0), ... , limi→∞ g(i−1) A|N |(0, 0, ... , 0) ) Z(Ak) = = lim g(i) Ak(0, ... , 0) i→∞ = lim 1215 are called monotone systems of polynomial equations and have been investigated by Etessami and Yannakakis (2009) and Kiefer et al. (2007). The sought solution, that is, the partition function, is the least fixed point solution of X = g(X). For practical reasons, the set of nonterminals of a grammar is usually divided into maximal subsets of mutually recursive nonterminals, that is, for each A and B in such a subset, we have A =*=&gt;. uBα and B =*=&gt;. vAβ, for some u, v, α, β. This corresponds to a strongly connected component if we see the connection between the left-hand side of a rule and a nonterminal member in its right-hand side as an edge in a directed graph. For each strongly connected component, th</context>
<context position="13579" citStr="Etessami and Yannakakis (2009)" startWordPosition="2442" endWordPosition="2445">lf and in the number of bits of the desired precision. In practical applications where large PCFGs are empirically estimated from data sets, the standard conditions mentioned above for the polynomial time approximation of the partition function are usually met. However, there are some degenerate cases for which these standard conditions do not hold, resulting in exponential time behaviour of the fixed-point iteration method. This has been firstly observed in (Etessami and Yannakakis, 2005). An alternative iterative algorithm for the approximation of the partition function has been proposed by Etessami and Yannakakis (2009), based on Newton’s method for the solution of non-linear systems of equations. From a theoretical perspective, Kiefer et al. (2007) have shown that, after a certain number of initial iterations, Newton’s method adds a fixed number of bits to the precision of the approximated solution, even in the above mentioned cases in which the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, the number of iterations needed to compute the first bit of the solution can be at least exponential in the size of the system. Experiment</context>
</contexts>
<marker>Etessami, Yannakakis, 2009</marker>
<rawString>K. Etessami and M. Yannakakis. 2009. Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations. Journal of the ACM, 56(1):1–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L N Fred</author>
</authors>
<title>Computation of substring probabilities in stochastic grammars.</title>
<date>2000</date>
<booktitle>Grammatical Inference: Algorithms and Applications,</booktitle>
<volume>1891</volume>
<pages>103--114</pages>
<editor>In A. Oliveira, editor,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3947" citStr="Fred (2000)" startWordPosition="638" endWordPosition="639">ences of the given infix can be found in a single string generated by the PCFG. The authors developed solutions for the case where some distribution can be defined on Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1213–1221, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics the distance of the infix from the sentence bound- from rules in R to real numbers in the interval [0, 1]. aries, which is a simplifying assumption. The prob- The concept of left-most derivation in one step is lem is also considered by Fred (2000), which pro- represented by the notation α π�g 0, which means vides algorithms for the case where the language that the left-most occurrence of any nonterminal in model is a probabilistic regular grammar. However, α E (E U N)* is rewritten by means of some rule the algorithm in (Fred, 2000) does not apply to cases 7r E R. If the rewritten nonterminal is A, then 7r with multiple occurrences of the given infix within must be of the form (A -+ -y) and 0 is the result a string in the language, which is what was pointed of replacing the occurrence of A in α by -y. A leftout to be the problematic ca</context>
</contexts>
<marker>Fred, 2000</marker>
<rawString>A.L.N. Fred. 2000. Computation of substring probabilities in stochastic grammars. In A. Oliveira, editor, Grammatical Inference: Algorithms and Applications, volume 1891 of Lecture Notes in Artificial Intelligence, pages 103–114. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="19363" citStr="Gusfield, 1997" startWordPosition="3572" endWordPosition="3573"> This automaton has states s0, ... , sn, transitions s0 7→ s0 and sn a 7→ sn for each a ∈ E, and a transition si_1 7→ si for each i (1 ≤ i ≤ n). The ai initial state is s0 and the final state is sn. Clearly, there is nondeterminism in state s0. One way to make this automaton deterministic is to apply the general algorithm of determinization of finite automata; see e.g. (Aho and Ullman, 1972). This algorithm is exponential for general FAs. An alternative approach is to construct a deterministic finite automaton directly from w, in line with the Knuth-Morris-Pratt algorithm (Knuth et al., 1977; Gusfield, 1997). Both approaches result in the same deterministic FA, which we denote by Iw. However, the latter approach is easier to implement in such a E p&apos;(d&apos;) = d0,w: , S04,0w 1217 way that the time complexity of constructing the automaton is linear in |w|. The automaton Iw is described as follows. There are n + 1 states t0, ... , tn, where as before n is the length of w. The initial state is t0 and the final state is tn. The intuition is that Iw reads a string x = b1 · · · bm from left to right, and when it has read the prefix b1 · · · bj (0 ≤ j ≤ m), it is in state ti (0 ≤ i &lt; n) if and only if a1 · ·</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>D. Gusfield. 1997. Algorithms on Strings, Trees, and Sequences. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T E Harris</author>
</authors>
<title>The Theory of Branching Processes.</title>
<date>1963</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="9020" citStr="Harris, 1963" startWordPosition="1590" endWordPosition="1591">e by parameter estimation from corpora are generally consistent; see (S´anchez and Benedi, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed. We define the partition function of !9 as the function Z that assigns to each A E N the value Z(A) = E p(A = w). (1) d,w Note that Z(S) = 1 means that !9 is consistent. More generally, in later sections we will need to compute the partition function for non-consistent PCFGs. We can characterize the partition function of a PCFG as a solution of a specific system of equations. Following the approach in (Harris, 1963; Chi, 1999), we introduce generating functions associated with the nonterminals of the grammar. For A E N and α E (N U Σ)∗, we write f(A, α) to denote the number of occurrences of symbol A within string α. Let N = {A1, A2,..., A|N|}. For each Ak E N, let mk be the number of rules in R with left-hand side Ak, and assume some fixed order for these rules. For each i with 1 &lt; i &lt; mk, let Ak -+ αk,i be the i-th rule with left-hand side Ak. For each k with 1 &lt; k &lt; |N|, the generating function associated with Ak is defined as gAk(z1, z2, ... , z|N|) = zf(Aj,αk,i)) (2) j J Furthermore, for each i &gt; 1</context>
</contexts>
<marker>Harris, 1963</marker>
<rawString>T.E. Harris. 1963. The Theory of Branching Processes. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="1684" citStr="Jelinek and Lafferty (1991)" startWordPosition="260" endWordPosition="263">ns in modeling of natural language syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correction, when one is processing ‘noisy’ text and the parser recognizes an error that must be</context>
<context position="29574" citStr="Jelinek and Lafferty (1991)" startWordPosition="5430" endWordPosition="5433">mory. Can the computation of the infix probability of a string of the form aw or wa, a ∈ E, be computed by relying on the existing results, so that the computation is substantially faster than if the computation were done from scratch? Our investigations so far have not found a positive answer to this second question. In particular, the systems of equations for C1 and C3 change fundamentally if the infix is extended by one more symbol, which seems to at least make incremental computation very difficult, if not impossible. Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. 8 Conclusions We have shown that the problem of infix probabilities for PCFGs can be solved by a construction that intersects a context-free language with a regular language. An important constraint is that the finite automaton that is input to this construction be unambiguous. We have shown that such an automaton can be efficiently constructed. Once the input probabilistic PCFG and the FA have been combined into a new probabilistic CFG, the infix probability can be straightforw</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>F. Jelinek and J.D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Kelley</author>
</authors>
<title>Iterative Methods for Linear and Nonlinear Equations. Society for Industrial and Applied Mathematics,</title>
<date>1995</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="12746" citStr="Kelley (1995" startWordPosition="2312" endWordPosition="2313">erved by Etessami and Yannakakis (2009). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al. (1999). This corresponds to the so-called fixed-point iteration method, which is well-known in the numerical calculus literature and is frequently applied to systems of non-linear equations because it can be easily implemented. When a number of standard conditions are met, each iteration of (4) adds a fixed number of bits to the precision of the solution; see Kelley (1995, Chapter 4). Since each iteration can easily be implemented to run in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself and in the number of bits of the desired precision. In practical applications where large PCFGs are empirically estimated from data sets, the standard conditions mentioned above for the polynomial time approximation of the partition function are usually met. However, there are some degenerate cases for which these standard conditions do not hold, resulting in exponential time behaviour of th</context>
<context position="25360" citStr="Kelley, 1995" startWordPosition="4680" endWordPosition="4681"> is given in the first row of Table 1. In order to solve the non-linear systems of equations, we used Broyden’s method. It can be seen as an approximation of Newton’s method. It requires more iterations, but seems to be faster overall, and more scalable to large problem sizes, due to the avoidance of matrix inversion, which sometimes makes Newton’s method prohibitively expensive. In our experiments, Broyden’s method was generally faster than Newton’s method and much faster than the simple iteration method by the relation in (4). For further details on Broyden’s method, we refer the reader to (Kelley, 1995). The main obstacle to computation for infixes substantially longer than 7 symbols is the memory consumption rather than the running time. This is due to the required square matrices, the dimension of which is the number of nonterminals. The number of nonterminals (of the intersection grammar) naturally grows as the infix becomes longer. As explained in Section 2, the problem is divided into smaller problems by isolating disjoint sets of mutually recursive nonterminals, or strongly connected components. We found that for the application to the automata discussed in Section 4, there were exactl</context>
</contexts>
<marker>Kelley, 1995</marker>
<rawString>C.T. Kelley. 1995. Iterative Methods for Linear and Nonlinear Equations. Society for Industrial and Applied Mathematics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kiefer</author>
<author>M Luttenberger</author>
<author>J Esparza</author>
</authors>
<title>On the convergence of Newton’s method for monotone systems of polynomial equations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 39th ACM Symposium on Theory of Computing,</booktitle>
<pages>217--266</pages>
<contexts>
<context position="11014" citStr="Kiefer et al. (2007)" startWordPosition="2007" endWordPosition="2010"> &lt; |N|: zk = gAk(z1, z2, ... , z|N|). (5) In the case of a general PCFG, the above equations are non-linear polynomials with positive (real) coefficients. We can represent the resulting system in vector form and write X = g(X). These systems |N| (p(Ak -+ αk,i) � H j=1 Emk i=1 (i−1) i→∞gAk ( gA1 (0, 0, ... , 0), ... , gA|N |(0, 0, ... , 0) ) (i−1) (i−1) = gAk ( limi→∞ gA1 (0, 0, ... , 0), ... , limi→∞ g(i−1) A|N |(0, 0, ... , 0) ) Z(Ak) = = lim g(i) Ak(0, ... , 0) i→∞ = lim 1215 are called monotone systems of polynomial equations and have been investigated by Etessami and Yannakakis (2009) and Kiefer et al. (2007). The sought solution, that is, the partition function, is the least fixed point solution of X = g(X). For practical reasons, the set of nonterminals of a grammar is usually divided into maximal subsets of mutually recursive nonterminals, that is, for each A and B in such a subset, we have A =*=&gt;. uBα and B =*=&gt;. vAβ, for some u, v, α, β. This corresponds to a strongly connected component if we see the connection between the left-hand side of a rule and a nonterminal member in its right-hand side as an edge in a directed graph. For each strongly connected component, there is a separate system </context>
<context position="13711" citStr="Kiefer et al. (2007)" startWordPosition="2463" endWordPosition="2466">he standard conditions mentioned above for the polynomial time approximation of the partition function are usually met. However, there are some degenerate cases for which these standard conditions do not hold, resulting in exponential time behaviour of the fixed-point iteration method. This has been firstly observed in (Etessami and Yannakakis, 2005). An alternative iterative algorithm for the approximation of the partition function has been proposed by Etessami and Yannakakis (2009), based on Newton’s method for the solution of non-linear systems of equations. From a theoretical perspective, Kiefer et al. (2007) have shown that, after a certain number of initial iterations, Newton’s method adds a fixed number of bits to the precision of the approximated solution, even in the above mentioned cases in which the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, the number of iterations needed to compute the first bit of the solution can be at least exponential in the size of the system. Experiments with Newton’s method for the approximation of the partition functions of PCFGs have been carried out in several application-orient</context>
</contexts>
<marker>Kiefer, Luttenberger, Esparza, 2007</marker>
<rawString>S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the convergence of Newton’s method for monotone systems of polynomial equations. In Proceedings of the 39th ACM Symposium on Theory of Computing, pages 217–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
<author>J H Morris</author>
<author>V R Pratt</author>
</authors>
<title>Fast pattern matching in strings.</title>
<date>1977</date>
<journal>SIAM Journal on Computing,</journal>
<pages>6--323</pages>
<contexts>
<context position="19346" citStr="Knuth et al., 1977" startWordPosition="3568" endWordPosition="3571">rwardly constructed. This automaton has states s0, ... , sn, transitions s0 7→ s0 and sn a 7→ sn for each a ∈ E, and a transition si_1 7→ si for each i (1 ≤ i ≤ n). The ai initial state is s0 and the final state is sn. Clearly, there is nondeterminism in state s0. One way to make this automaton deterministic is to apply the general algorithm of determinization of finite automata; see e.g. (Aho and Ullman, 1972). This algorithm is exponential for general FAs. An alternative approach is to construct a deterministic finite automaton directly from w, in line with the Knuth-Morris-Pratt algorithm (Knuth et al., 1977; Gusfield, 1997). Both approaches result in the same deterministic FA, which we denote by Iw. However, the latter approach is easier to implement in such a E p&apos;(d&apos;) = d0,w: , S04,0w 1217 way that the time complexity of constructing the automaton is linear in |w|. The automaton Iw is described as follows. There are n + 1 states t0, ... , tn, where as before n is the length of w. The initial state is t0 and the final state is tn. The intuition is that Iw reads a string x = b1 · · · bm from left to right, and when it has read the prefix b1 · · · bj (0 ≤ j ≤ m), it is in state ti (0 ≤ i &lt; n) if a</context>
</contexts>
<marker>Knuth, Morris, Pratt, 1977</marker>
<rawString>D.E. Knuth, J.H. Morris, Jr., and V.R. Pratt. 1977. Fast pattern matching in strings. SIAM Journal on Computing, 6:323–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<location>LORIA, Nancy, France,</location>
<contexts>
<context position="14853" citStr="Nederhof and Satta, 2003" startWordPosition="2651" endWordPosition="2654">he partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method. 3 Intersection of PCFG and FA It was shown by Bar-Hillel et al. (1964) that contextfree languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details. To avoid a number of technical complications, we assume the finite automaton has no epsilon transitions, and has only one final state. In the context of our use of this construction in the following sections, these restrictions are without loss of generality. Thus, a finite automaton (FA) M is represented by a 5-tuple (E, Q, qo, q f, Δ), where E and Q are two finite sets of terminals and states, respectively, qo is the initial state, q f is the final state, and Δ is a finite set of transitions, each of the form s t, where s, t E Q and a E E. A complete computation of M accep</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137–148, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Computing partition functions of PCFGs.</title>
<date>2008</date>
<journal>Research on Language and Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="14388" citStr="Nederhof and Satta (2008)" startWordPosition="2572" endWordPosition="2575">iterations, Newton’s method adds a fixed number of bits to the precision of the approximated solution, even in the above mentioned cases in which the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, the number of iterations needed to compute the first bit of the solution can be at least exponential in the size of the system. Experiments with Newton’s method for the approximation of the partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method. 3 Intersection of PCFG and FA It was shown by Bar-Hillel et al. (1964) that contextfree languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details. To avoid a number of technical complications, we assume the finite automaton has no epsilon transitions, and has onl</context>
<context position="24016" citStr="Nederhof and Satta, 2008" startWordPosition="4455" endWordPosition="4458">ix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or space demand, and that might be improved. The experiments were run on a desktop with a 3.0 GHz Pentium 4 processor. The implementation language is C++. The set-up of the experiments is similar to that in (Nederhof and Satta, 2008). A probabilistic contextfree grammar was extracted from sections 2-21 of the Penn Treebank version II. Subtrees that generated the empty string were systematically removed. The result was a CFG with 10,035 rules, 28 nonterminals and 36 parts-of-speech. The rule probabilities were determined by maximum likelihood estimation. The grammar was subsequently binarized, to avoid exponential behaviour, as explained in Section 3. We have considered 10 strings of length 7, randomly generated, assuming each of the parts-ofspeech has the same probability. For all prefixes of those strings from length 2 t</context>
</contexts>
<marker>Nederhof, Satta, 2008</marker>
<rawString>M.-J. Nederhof and G. Satta. 2008. Computing partition functions of PCFGs. Research on Language and Computation, 6(2):139–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Persoon</author>
<author>K S Fu</author>
</authors>
<title>Sequential classification of strings generated by SCFG’s.</title>
<date>1975</date>
<journal>International Journal of Computer and Information Sciences,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="1595" citStr="Persoon and Fu (1975)" startWordPosition="247" endWordPosition="250"> related to PCFGs have been investigated in the literature, motivated by applications in modeling of natural language syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correc</context>
</contexts>
<marker>Persoon, Fu, 1975</marker>
<rawString>E. Persoon and K.S. Fu. 1975. Sequential classification of strings generated by SCFG’s. International Journal of Computer and Information Sciences, 4(3):205–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.</title>
<date>1992</date>
<booktitle>In Proc. of the fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>418--424</pages>
<location>Nantes,</location>
<contexts>
<context position="23183" citStr="Resnik, 1992" startWordPosition="4307" endWordPosition="4308">ally, we can compute the above sum by applying the iterative method discussed in Section 2. Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, 1218 b,c c a b a, b, c a b a c t0 t1 t2 t3 t4 b, c a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time </context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>P. Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proc. of the fifteenth International Conference on Computational Linguistics, Nantes, August, pages 418–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-A S´anchez</author>
<author>J-M Benedi</author>
</authors>
<title>Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>9</issue>
<marker>S´anchez, Benedi, 1997</marker>
<rawString>J.-A. S´anchez and J.-M. Benedi. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(9):1052–1055, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized tree-adjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. of the fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>426--432</pages>
<location>Nantes,</location>
<contexts>
<context position="23168" citStr="Schabes, 1992" startWordPosition="4305" endWordPosition="4306"> = p&apos;(z). z Finally, we can compute the above sum by applying the iterative method discussed in Section 2. Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, 1218 b,c c a b a, b, c a b a c t0 t1 t2 t3 t4 b, c a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that ha</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of the fifteenth International Conference on Computational Linguistics, Nantes, August, pages 426–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1703" citStr="Stolcke (1995)" startWordPosition="265" endWordPosition="266">ge syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correction, when one is processing ‘noisy’ text and the parser recognizes an error that must be recovered by opera</context>
<context position="12351" citStr="Stolcke (1995)" startWordPosition="2247" endWordPosition="2248">nnected component contains nonterminal A, and another contains nonterminal B, where A =*=&gt;. uBα for some u, α, then the system for the latter component must be solved first. The solution for a system of equations such as those described above can be irrational and nonexpressible by radicals, even if we assume that all the probabilities of the rules in the input PCFG are rational numbers, as observed by Etessami and Yannakakis (2009). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al. (1999). This corresponds to the so-called fixed-point iteration method, which is well-known in the numerical calculus literature and is frequently applied to systems of non-linear equations because it can be easily implemented. When a number of standard conditions are met, each iteration of (4) adds a fixed number of bits to the precision of the solution; see Kelley (1995, Chapter 4). Since each iteration can easily be implemented to run in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself</context>
<context position="29593" citStr="Stolcke (1995)" startWordPosition="5435" endWordPosition="5436"> infix probability of a string of the form aw or wa, a ∈ E, be computed by relying on the existing results, so that the computation is substantially faster than if the computation were done from scratch? Our investigations so far have not found a positive answer to this second question. In particular, the systems of equations for C1 and C3 change fundamentally if the infix is extended by one more symbol, which seems to at least make incremental computation very difficult, if not impossible. Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. 8 Conclusions We have shown that the problem of infix probabilities for PCFGs can be solved by a construction that intersects a context-free language with a regular language. An important constraint is that the finite automaton that is input to this construction be unambiguous. We have shown that such an automaton can be efficiently constructed. Once the input probabilistic PCFG and the FA have been combined into a new probabilistic CFG, the infix probability can be straightforwardly solved by ite</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):167–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In 25th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>104--111</pages>
<location>Stanford, California, USA,</location>
<contexts>
<context position="23612" citStr="Vijay-Shanker et al. (1987)" startWordPosition="4386" endWordPosition="4389">on of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, 1218 b,c c a b a, b, c a b a c t0 t1 t2 t3 t4 b, c a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or space demand, and that might be improved. The experiments were run on a desktop with a 3.0 GHz Pentium 4 processor. The implementation language is C++. The set-up of the experiments is similar to that in (Nederhof and Satta, 2008). A probabilistic contextfree grammar was extracted from sections 2-21 of the Penn Treebank version II. Subtrees that generated the empty string were systematically removed. The result was a CFG w</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, D.J. Weir, and A.K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 104–111, Stanford, California, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wojtczak</author>
<author>K Etessami</author>
</authors>
<title>PReMo: an analyzer for Probabilistic Recursive Models.</title>
<date>2007</date>
<booktitle>In Tools and Algorithms for the Construction and Analysis of Systems, 13th International Conference,</booktitle>
<volume>4424</volume>
<pages>66--71</pages>
<publisher>Springer-Verlag.</publisher>
<location>Braga,</location>
<contexts>
<context position="14355" citStr="Wojtczak and Etessami (2007)" startWordPosition="2566" endWordPosition="2569">, after a certain number of initial iterations, Newton’s method adds a fixed number of bits to the precision of the approximated solution, even in the above mentioned cases in which the fixed-point iteration method shows exponential time behaviour. However, these authors also show that, in some degenerate cases, the number of iterations needed to compute the first bit of the solution can be at least exponential in the size of the system. Experiments with Newton’s method for the approximation of the partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method. 3 Intersection of PCFG and FA It was shown by Bar-Hillel et al. (1964) that contextfree languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details. To avoid a number of technical complications, we assume the finite automaton has no</context>
</contexts>
<marker>Wojtczak, Etessami, 2007</marker>
<rawString>D. Wojtczak and K. Etessami. 2007. PReMo: an analyzer for Probabilistic Recursive Models. In Tools and Algorithms for the Construction and Analysis of Systems, 13th International Conference, volume 4424 of Lecture Notes in Computer Science, pages 66–71, Braga, Portugal. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>