<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000062">
<title confidence="0.868321">
Re-Ranking Models For Spoken Language Understanding
</title>
<author confidence="0.991496">
Marco Dinarelli
</author>
<affiliation confidence="0.9152595">
University of Trento
Italy
</affiliation>
<email confidence="0.957026">
dinarelli@disi.unitn.it
</email>
<author confidence="0.986555">
Alessandro Moschitti
</author>
<affiliation confidence="0.9139105">
University of Trento
Italy
</affiliation>
<email confidence="0.97282">
moschitti@disi.unitn.it
</email>
<author confidence="0.982725">
Giuseppe Riccardi
</author>
<affiliation confidence="0.9130595">
University of Trento
Italy
</affiliation>
<email confidence="0.975272">
riccardi@disi.unitn.it
</email>
<sectionHeader confidence="0.994251" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998638">
Spoken Language Understanding aims at
mapping a natural language spoken sen-
tence into a semantic representation. In
the last decade two main approaches have
been pursued: generative and discrimi-
native models. The former is more ro-
bust to overfitting whereas the latter is
more robust to many irrelevant features.
Additionally, the way in which these ap-
proaches encode prior knowledge is very
different and their relative performance
changes based on the task. In this pa-
per we describe a machine learning frame-
work where both models are used: a gen-
erative model produces a list of ranked hy-
potheses whereas a discriminative model
based on structure kernels and Support
Vector Machines, re-ranks such list. We
tested our approach on the MEDIA cor-
pus (human-machine dialogs) and on a
new corpus (human-machine and human-
human dialogs) produced in the Euro-
pean LUNA project. The results show a
large improvement on the state-of-the-art
in concept segmentation and labeling.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887924528302">
In Spoken Dialog Systems, the Language Under-
standing module performs the task of translating
a spoken sentence into its meaning representation
based on semantic constituents. These are the
units for meaning representation and are often re-
ferred to as concepts. Concepts are instantiated by
sequences of words, therefore a Spoken Language
Understanding (SLU) module finds the association
between words and concepts.
In the last decade two major approaches have
been proposed to find this correlation: (i) gener-
ative models, whose parameters refer to the joint
probability of concepts and constituents; and (ii)
discriminative models, which learn a classifica-
tion function to map words into concepts based
on geometric and statistical properties. An ex-
ample of generative model is the Hidden Vector
State model (HVS) (He and Young, 2005). This
approach extends the discrete Markov model en-
coding the context of each state as a vector. State
transitions are performed as stack shift operations
followed by a push of a preterminal semantic cat-
egory label. In this way the model can capture se-
mantic hierarchical structures without the use of
tree-structured data. Another simpler but effec-
tive generative model is the one based on Finite
State Transducers. It performs SLU as a transla-
tion process from words to concepts using Finite
State Transducers (FST). An example of discrim-
inative model used for SLU is the one based on
Support Vector Machines (SVMs) (Vapnik, 1995),
as shown in (Raymond and Riccardi, 2007). In
this approach, data are mapped into a vector space
and SLU is performed as a classification problem
using Maximal Margin Classifiers (Shawe-Taylor
and Cristianini, 2004).
Generative models have the advantage to be
more robust to overfitting on training data, while
discriminative models are more robust to irrele-
vant features. Both approaches, used separately,
have shown a good performance (Raymond and
Riccardi, 2007), but they have very different char-
acteristics and the way they encode prior knowl-
edge is very different, thus designing models able
to take into account characteristics of both ap-
proaches are particularly promising.
In this paper we propose a method for SLU
based on generative and discriminative models:
the former uses FSTs to generate a list of SLU hy-
potheses, which are re-ranked by SVMs. These
exploit all possible word/concept subsequences
(with gaps) of the spoken sentence as features (i.e.
all possible n-grams). Gaps allow for the encod-
</bodyText>
<note confidence="0.922994">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 202–210,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996425">
202
</page>
<bodyText confidence="0.999945818181818">
ing of long distance dependencies between words
in relatively small n-grams. Given the huge size
of this feature space, we adopted kernel methods
and in particular sequence kernels (Shawe-Taylor
and Cristianini, 2004) and tree kernels (Raymond
and Riccardi, 2007; Moschitti and Bejan, 2004;
Moschitti, 2006) to implicitly encode n-grams and
other structural information in SVMs.
We experimented with different approaches for
training the discriminative models and two dif-
ferent corpora: the well-known MEDIA corpus
(Bonneau-Maynard et al., 2005) and a new corpus
acquired in the European project LUNA1 (Ray-
mond et al., 2007). The results show a great
improvement with respect to both the FST-based
model and the SVM model alone, which are the
current state-of-the-art for concept classification
on such corpora. The rest of the paper is orga-
nized as follows: Sections 2 and 3 show the gener-
ative and discriminative models, respectively. The
experiments and results are reported in Section 4
whereas the conclusions are drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.949517" genericHeader="introduction">
2 Generative approach for concept
classification
</sectionHeader>
<bodyText confidence="0.999975708333333">
In the context of Spoken Language Understanding
(SLU), concept classification is the task of asso-
ciating the best sequence of concepts to a given
sentence, i.e. word sequence. A concept is a class
containing all the words carrying out the same se-
mantic meaning with respect to the application do-
main. In SLU, concepts are used as semantic units
and are represented with concept tags. The associ-
ation between words and concepts is learned from
an annotated corpus.
The Generative model used in our work for con-
cept classification is the same used in (Raymond
and Riccardi, 2007). Given a sequence of words
as input, a translation process based on FST is
performed to output a sequence of concept tags.
The translation process involves three steps: (1)
the mapping of words into classes (2) the mapping
of classes into concepts and (3) the selection of the
best concept sequence.
The first step is used to improve the generaliza-
tion power of the model. The word classes at this
level can be both domain-dependent, e.g. ”Hotel”
in MEDIA or ”Software” in the LUNA corpus, or
domain-independent, e.g. numbers, dates, months
</bodyText>
<footnote confidence="0.813407">
1Contract n. 33549
</footnote>
<bodyText confidence="0.999401181818182">
etc. The class of a word not belonging to any class
is the word itself.
In the second step, classes are mapped into con-
cepts. The mapping is not one-to-one: a class
may be associated with more than one concept, i.e.
more than one SLU hypothesis can be generated.
In the third step, the best or the m-best hy-
potheses are selected among those produced in the
previous step. They are chosen according to the
maximum probability evaluated by the Conceptual
Language Model, described in the next section.
</bodyText>
<sectionHeader confidence="0.490046" genericHeader="method">
2.1 Stochastic Conceptual Language Model
(SCLM)
</sectionHeader>
<bodyText confidence="0.997808166666667">
An SCLM is an n-gram language model built on
semantic tags. Using the same notation proposed
in (Moschitti et al., 2007) and (Raymond and Ric-
cardi, 2007), our SCLM trains joint probability
P(W, C) of word and concept sequences from an
annotated corpus:
</bodyText>
<equation confidence="0.995121666666667">
k
P(W, C) = P(wi, ci|hi),
i=1
</equation>
<bodyText confidence="0.984462470588236">
where W = w1..wk, C = c1..ck and
hi = wi−1ci−1..w1c1. Since we use a 3-gram
conceptual language model, the history hi is
{wi−1ci−1, wi−2ci−2}.
All the steps of the translation process described
here and above are implemented as Finite State
Transducers (FST) using the AT&amp;T FSM/GRM
tools and the SRILM (Stolcke, 2002) tools. In
particular the SCLM is trained using SRILM tools
and then converted to an FST. This allows the use
of a wide set of stochastic language models (both
back-off and interpolated models with several dis-
counting techniques like Good-Turing, Witten-
Bell, Natural, Kneser-Ney, Unchanged Kneser-
Ney etc). We represent the combination of all the
translation steps as a transducer ASLU (Raymond
and Riccardi, 2007) in terms of FST operations:
</bodyText>
<equation confidence="0.554948">
ASLU = AW o AW2C o ASLM,
</equation>
<bodyText confidence="0.9986492">
where AW is the transducer representation of the
input sentence, AW2C is the transducer mapping
words to classes and ASLM is the Semantic Lan-
guage Model (SLM) described above. The best
SLU hypothesis is given by
</bodyText>
<equation confidence="0.981046">
C = projectC(bestpath1(ASLU)),
</equation>
<bodyText confidence="0.985691">
where bestpathn (in this case n is 1 for the 1-best
hypothesis) performs a Viterbi search on the FST
</bodyText>
<page confidence="0.998132">
203
</page>
<bodyText confidence="0.9989624">
quences in common between two sentences, in the
space of n-grams (for any n).
and outputs the n-best hypotheses and projectC
performs a projection of the FST on the output la-
bels, in this case the concepts.
</bodyText>
<subsectionHeader confidence="0.999559">
2.2 Generation of m-best concept labeling
</subsectionHeader>
<bodyText confidence="0.9973008125">
Using the FSTs described above, we can generate
m best hypotheses ranked by the joint probability
of the SCLM.
After an analysis of the m-best hypotheses of
our SLU model, we noticed that many times the
hypothesis ranked first by the SCLM is not the
closest to the correct concept sequence, i.e. its er-
ror rate using the Levenshtein alignment with the
manual annotation of the corpus is not the low-
est among the m hypotheses. This means that
re-ranking the m-best hypotheses in a convenient
way could improve the SLU performance. The
best choice in this case is a discriminative model,
since it allows for the use of informative features,
which, in turn, can model easily feature dependen-
cies (also if they are infrequent in the training set).
</bodyText>
<sectionHeader confidence="0.997036" genericHeader="method">
3 Discriminative re-ranking
</sectionHeader>
<bodyText confidence="0.998246833333333">
Our discriminative re-ranking is based on SVMs
or a perceptron trained with pairs of conceptually
annotated sentences. The classifiers learn to select
which annotation has an error rate lower than the
others so that the m-best annotations can be sorted
based on their correctness.
</bodyText>
<subsectionHeader confidence="0.998833">
3.1 SVMs and Kernel Methods
</subsectionHeader>
<bodyText confidence="0.999962636363636">
Kernel Methods refer to a large class of learning
algorithms based on inner product vector spaces,
among which Support Vector Machines (SVMs)
are one of the most well known algorithms. SVMs
and perceptron learn a hyperplane H(x) = wx +
b = 0, where x� is the feature vector represen-
tation of a classifying object o, w� E Rn (a
vector space) and b E R are parameters (Vap-
nik, 1995). The classifying object o is mapped
into x� by a feature function 0. The kernel trick
allows us to rewrite the decision hyperplane as
</bodyText>
<equation confidence="0.7195865">
�
i=1..lyiαi0(oi)0(o) + b = 0, where yi is equal
</equation>
<bodyText confidence="0.999896125">
to 1 for positive and -1 for negative examples,
αi E R+, oiVi E {1..l} are the training instances
and the product K(oi, o) = (0(oi)0(o)) is the ker-
nel function associated with the mapping 0. Note
that we do not need to apply the mapping 0, we
can use K(oi, o) directly (Shawe-Taylor and Cris-
tianini, 2004). For example, next section shows a
kernel function that counts the number of word se-
</bodyText>
<subsectionHeader confidence="0.999906">
3.2 String Kernels
</subsectionHeader>
<bodyText confidence="0.999375655172414">
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. Gaps modify the
weight associated with the target substrings as
shown in the following.
Let E be a finite alphabet, E∗ = U∞n= 0 En is the
set of all strings. Given a string s E E∗, |s |denotes
the length of the strings and si its compounding
symbols, i.e s = s1..s|s|, whereas s[i : j] selects
the substring sisi+1..sj−1sj from the i-th to the
j-th character. u is a subsequence of s if there
is a sequence of indexes I = (i1, ..., i|u|), with
1 &lt; i1 &lt; ... &lt; i|u |&lt; |s|, such that u = si1..si|u|
or u = s[I] for short. d(I) is the distance between
the first and last character of the subsequence u in
s, i.e. d(I) = i|u |− i1 + 1. Finally, given s1, s2
E E∗, s1s2 indicates their concatenation.
The set of all substrings of a text corpus forms a
feature space denoted by F = {u1, u2, ..} C E∗.
To map a string s in R∞ space, we can use the
following functions: φu(s) = P~I:u=s[~I] λd(~I) for
some A &lt; 1. These functions count the num-
ber of occurrences of u in the string s and assign
them a weight Ad(I) proportional to their lengths.
Hence, the inner product of the feature vectors for
two strings s1 and s2 returns the sum of all com-
mon subsequences weighted according to their
frequency of occurrences and lengths, i.e.
</bodyText>
<equation confidence="0.9982026">
XSK(s1, s2) = Xφu(s1) ·φu(s2) = X λd(~I1)
u∈Σ* u∈Σ* ~I1:u=s1[~I1]
X λd( X~I2) = X X λd(~I1)+d(~I2) ,
u∈Σ*
~I2:u=s2[ ~I2] ~I1:u=s1[~I1] ~I2:u=s2[~I2]
</equation>
<bodyText confidence="0.999389666666667">
where d(.) counts the number of characters in the
substrings as well as the gaps that were skipped in
the original string. It is worth noting that:
</bodyText>
<listItem confidence="0.986351166666667">
(a) longer subsequences receive lower weights;
(b) some characters can be omitted, i.e. gaps;
and
(c) gaps determine a weight since the exponent
of A is the number of characters and gaps be-
tween the first and last character.
</listItem>
<page confidence="0.995323">
204
</page>
<bodyText confidence="0.999879428571429">
Characters in the sequences can be substituted
with any set of symbols. In our study we pre-
ferred to use words so that we can obtain word
sequences. For example, given the sentence: How
may I help you ? sample substrings, extracted by
the Sequence Kernel (SK), are: How help you ?,
How help ?, help you, may help you, etc.
</bodyText>
<subsectionHeader confidence="0.999202">
3.3 Tree kernels
</subsectionHeader>
<bodyText confidence="0.9997845">
Tree kernels represent trees in terms of their sub-
structures (fragments). The kernel function de-
tects if a tree subpart (common to both trees) be-
longs to the feature space that we intend to gen-
erate. For such purpose, the desired fragments
need to be described. We consider two important
characterizations: the syntactic tree (STF) and the
partial tree (PTF) fragments.
</bodyText>
<subsectionHeader confidence="0.736766">
3.3.1 Tree Fragment Types
</subsectionHeader>
<bodyText confidence="0.9999745">
An STF is a general subtree whose leaves can be
non-terminal symbols. For example, Figure 1(a)
shows 10 STFs (out of 17) of the subtree rooted in
VP (of the left tree). The STFs satisfy the con-
straint that grammatical rules cannot be broken.
For example, [VP [V NP]] is an STF, which
has two non-terminal symbols, V and NP, as leaves
whereas [VP [V]] is not an STF. If we relax
the constraint over the STFs, we obtain more gen-
eral substructures called partial trees fragments
(PTFs). These can be generated by the application
of partial production rules of the grammar, con-
sequently [VP [V]] and [VP [NP]] are valid
PTFs. Figure 1(b) shows that the number of PTFs
derived from the same tree as before is still higher
(i.e. 30 PTs).
</bodyText>
<subsectionHeader confidence="0.999674">
3.4 Counting Shared SubTrees
</subsectionHeader>
<bodyText confidence="0.999844555555556">
The main idea of tree kernels is to compute the
number of common substructures between two
trees T1 and T2 without explicitly considering the
whole fragment space. To evaluate the above ker-
nels between two T1 and T2, we need to define a
set F = {f1, f2,. .. , f|F|}, i.e. a tree fragment
space and an indicator function Ii(n), equal to 1
if the target fi is rooted at node n and equal to 0
otherwise. A tree-kernel function over T1 and T2
</bodyText>
<equation confidence="0.829323">
is TK(T1,T2) = P Pn2∈NT2 Δ(n1,n2),
n1∈NT1
</equation>
<bodyText confidence="0.9756385">
where NT1 and NT2 are the sets of the T1’s
and T2’s nodes, respectively and Δ(n1, n2) =
P|F|
i=1Ii(n1)Ii(n2). The latter is equal to the num-
ber of common fragments rooted in the n1 and
n2 nodes. In the following sections we report the
equation for the efficient evaluation of Δ for ST
and PT kernels.
</bodyText>
<subsectionHeader confidence="0.969339">
3.5 Syntactic Tree Kernels (STK)
</subsectionHeader>
<bodyText confidence="0.99846675">
The Δ function depends on the type of fragments
that we consider as basic features. For example,
to evaluate the fragments of type STF, it can be
defined as:
</bodyText>
<listItem confidence="0.9659655">
1. if the productions at n1 and n2 are different
then Δ(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf children
(i.e. they are pre-terminals symbols) then
Δ(n1, n2) = 1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
</listItem>
<equation confidence="0.996578">
nc(n1)
Δ(n1, n2) = Y (σ + Δ(cjn1, cj n2)) (1)
j=1
</equation>
<bodyText confidence="0.999230875">
where σ ∈ {0, 1}, nc(n1) is the number of chil-
dren of n1 and cjn is the j-th child of the node
n. Note that, since the productions are the same,
nc(n1) = nc(n2). Δ(n1, n2) evaluates the num-
ber of STFs common to n1 and n2 as proved in
(Collins and Duffy, 2002).
Moreover, a decay factor λ can be added by
modifying steps (2) and (3) as follows2:
</bodyText>
<equation confidence="0.999316333333333">
2. Δ(n1, n2) = λ,
3. Δ(n1, n2) = λQnc(n1)
j=1 (σ + Δ(cjn1, cjn2)).
</equation>
<bodyText confidence="0.9671784">
The computational complexity of Eq. 1 is
O(|NT1 |× |NT2|) but as shown in (Moschitti,
2006), the average running time tends to be lin-
ear, i.e. O(|NT1 |+ |NT2|), for natural language
syntactic trees.
</bodyText>
<subsectionHeader confidence="0.985334">
3.6 The Partial Tree Kernel (PTK)
</subsectionHeader>
<bodyText confidence="0.813838333333333">
PTFs have been defined in (Moschitti, 2006).
Their computation is carried out by the following
Δ function:
</bodyText>
<listItem confidence="0.867638">
1. if the node labels of n1 and n2 are different
then Δ(n1, n2) = 0;
2. else Δ(n1, n2) =
</listItem>
<equation confidence="0.9769135">
1+P Ql(~I1)
~I1,~I2,l(~I1)=l(~I2) j=1 Δ(cn1(
</equation>
<footnote confidence="0.673825">
2To have a similarity score between 0 and 1, we also apply
the normalization in the kernel space, i.e.:
</footnote>
<equation confidence="0.998023666666667">
K′(T1, T2) = TK(T1,T2)
√T K (T1,T1) × T K (T2 ,T2) .
~I1j), cn2( ~I2j))
</equation>
<page confidence="0.997484">
205
</page>
<figure confidence="0.999971970149254">
D N
a brought
cat
a
a
VP
VP
VP
VP
VP
NP
D N
a cat a cat
NP
...
NP
D N N D
brought
V
D N
NP
VP
V
D N
a cat
NP
VP
V
D N
NP
NP
VP
D N
a cat
NP
N D V N
D N
a
NP
...
Mary
D N
NP
cat
V
NP
VP
V
D N
NP
VP
D
NP
VP
NP
D N
D
NP
NP
N
NP
NP
D N
brought
a cat
a cat
(a) Syntactic Tree fragments (STF) (b) Partial Tree fragments (PTF)
</figure>
<figureCaption confidence="0.999998">
Figure 1: Examples of different classes of tree fragments.
</figureCaption>
<bodyText confidence="0.998871083333333">
where ~I1 = (h1,h2,h3,..) and ~I2 =
(k1, k2, k3, ..) are index sequences associated with
the ordered child sequences cn1 of n1 and cn2 of
n2, respectively, ~I1j an
d ~I2j point to the j-th child
in the corresponding sequence, and, again, l(·) re-
turns the sequence length, i.e. the number of chil-
dren.
Furthermore, we add two decay factors: µ for
the depth of the tree and λ for the length of the
child subsequences with respect to the original se-
quence, i.e. we account for gaps. It follows that
</bodyText>
<equation confidence="0.96996875">
Δ(n1, n2) =
�Δ(cn1(~I1j), cn2(~I2j)) ,
~I2)
(2)
</equation>
<bodyText confidence="0.984505714285714">
where d(~I1) = ~I1l(~I1) − ~I11 and d(~I2) = ~I2l(~I2) −
~I21. This way, we penalize both larger trees and
child subsequences with gaps. Eq. 2 is more gen-
eral than Eq. 1. Indeed, if we only consider the
contribution of the longest child sequence from
node pairs that have the same children, we imple-
ment the STK kernel.
</bodyText>
<subsectionHeader confidence="0.99355">
3.7 Re-ranking models using sequences
</subsectionHeader>
<bodyText confidence="0.9999916">
The FST generates the m most likely concept an-
notations. These are used to build annotation
pairs, (si, sj), which are positive instances if si
has a lower concept annotation error than sj, with
respect to the manual annotation in the corpus.
Thus, a trained binary classifier can decide if si
is more accurate than sj. Each candidate anno-
tation si is described by a word sequence where
each word is followed by its concept annotation.
For example, given the sentence:
</bodyText>
<figure confidence="0.4311437">
ho (I have) un (a) problema (problem) con
(with) la (the) scheda di rete (network card) ora
(now)
a pair of annotations (si, sj) could be
si: ho NULL un NULL problema PROBLEM-B con
NULL la NULL scheda HW-B di HW-I rete HW-I ora
RELATIVETIME-B
sj: ho NULL un NULL problema ACTION-B con
NULL la NULL scheda HW-B di HW-B rete HW-B ora
RELATIVETIME-B
</figure>
<bodyText confidence="0.9975006875">
where NULL, ACTION, RELATIVETIME,
and HW are the assigned concepts whereas B and
I are the usual begin and internal tags for concept
subparts. The second annotation is less accurate
than the first since problema is annotated as an ac-
tion and ”scheda di rete” is split in three different
concepts.
Given the above data, the sequence kernel
is used to evaluate the number of common n-
grams between si and sj. Since the string ker-
nel skips some elements of the target sequences,
the counted n-grams include: concept sequences,
word sequences and any subsequence of words
and concepts at any distance in the sentence.
Such counts are used in our re-ranking function
as follows: let ei be the pair (s1i, s2 � we evaluate
</bodyText>
<equation confidence="0.901439">
i
the kernel:
KR(e1, e2) = SK(s1 1, s12) + SK(s21, s22) (3)
− SK(s11, s22) − SK(s21, s12)
</equation>
<bodyText confidence="0.999886375">
This schema, consisting in summing four differ-
ent kernels, has been already applied in (Collins
and Duffy, 2002) for syntactic parsing re-ranking,
where the basic kernel was a tree kernel instead of
SK and in (Moschitti et al., 2006), where, to re-
rank Semantic Role Labeling annotations, a tree
kernel was used on a semantic tree similar to the
one introduced in the next section.
</bodyText>
<subsectionHeader confidence="0.993371">
3.8 Re-ranking models using trees
</subsectionHeader>
<bodyText confidence="0.999558888888889">
Since the aim in concept annotation re-ranking is
to exploit innovative and effective source of infor-
mation, we can use the power of tree kernels to
generate correlation between concepts and word
structures.
Fig. 2 describes the structural association be-
tween the concept and the word level. This kind of
trees allows us to engineer new kernels and con-
sequently new features (Moschitti et al., 2008),
</bodyText>
<equation confidence="0.995843714285714">
� �
µ λ2+
~I1,~I2,l(~I1)=l(
λd(~I1)+d(
l( ~I1)
~I2) �
j=1
</equation>
<page confidence="0.9989">
206
</page>
<figureCaption confidence="0.993824">
Figure 2: An example of the semantic tree used for STK or PTK
</figureCaption>
<table confidence="0.997994083333334">
Corpus Train set Test set
LUNA
words concepts words concepts
Dialogs WOZ 183 67
Dialogs HH 180 -
Turns WOZ 1.019 373
Turns HH 6.999 -
Tokens WOZ 8.512 2.887 2.888 984
Tokens WOZ 62.639 17.423 - -
Vocab. WOZ 1.172 34 - -
Vocab. HH 4.692 49 - -
OOV rate - - 3.2% 0.1%
</table>
<tableCaption confidence="0.998706">
Table 1: Statistics on the LUNA corpus
</tableCaption>
<table confidence="0.997501428571429">
Corpus Train set Test set
Media
words concepts words concepts
Turns 12,922 3,518
# of tokens 94,912 43,078 26,676 12,022
Vocabulary 5,307 80 - -
OOV rate - - 0.01% 0.0%
</table>
<tableCaption confidence="0.999479">
Table 2: Statistics on the MEDIA corpus
</tableCaption>
<bodyText confidence="0.948327909090909">
e.g. their subparts extracted by STK or PTK, like
the tree fragments in figures 1(a) and 1(b). These
can be used in SVMs to learn the classification of
words in concepts.
More specifically, in our approach, we use tree
fragments to establish the order of correctness
between two alternative annotations. Therefore,
given two trees associated with two annotations, a
re-ranker based on tree kernel, KR, can be built
in the same way of the sequence-based kernel by
substituting SK in Eq. 3 with STK or PTK.
</bodyText>
<sectionHeader confidence="0.999084" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999944625">
In this section, we describe the corpora, param-
eters, models and results of our experiments of
word chunking and concept classification. Our
baseline relates to the error rate of systems based
on only FST and SVMs. The re-ranking models
are built on the FST output. Different ways of
producing training data for the re-ranking models
determine different results.
</bodyText>
<subsectionHeader confidence="0.993676">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999986434782609">
We used two different speech corpora:
The corpus LUNA, produced in the homony-
mous European project is the first Italian corpus
of spontaneous speech on spoken dialog: it is
based on the help-desk conversation in the domain
of software/hardware repairing (Raymond et al.,
2007). The data are organized in transcriptions
and annotations of speech based on a new multi-
level protocol. Data acquisition is still in progress.
Currently, 250 dialogs acquired with a WOZ ap-
proach and 180 Human-Human (HH) dialogs are
available. Statistics on LUNA corpus are reported
in Table 1.
The corpus MEDIA was collected within
the French project MEDIA-EVALDA (Bonneau-
Maynard et al., 2005) for development and evalu-
ation of spoken understanding models and linguis-
tic studies. The corpus is composed of 1257 di-
alogs, from 250 different speakers, acquired with
a Wizard of Oz (WOZ) approach in the context
of hotel room reservations and tourist information.
Statistics on transcribed and conceptually anno-
tated data are reported in Table 2.
</bodyText>
<subsectionHeader confidence="0.992525">
4.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999836857142857">
We defined two different training sets in the
LUNA corpus: one using only the WOZ train-
ing dialogs and one merging them with the HH
dialogs. Given the small size of LUNA corpus, we
did not carried out parameterization on a develop-
ment set but we used default or a priori parameters.
We experimented with LUNA WOZ and six re-
rankers obtained with the combination of SVMs
and perceptron (PCT) with three different types
of kernels: Syntactic Tree Kernel (STK), Partial
Tree kernels (PTK) and the String Kernel (SK) de-
scribed in Section 3.3.
Given the high number and the cost of these ex-
periments, we ran only one model, i.e. the one
</bodyText>
<page confidence="0.993221">
207
</page>
<table confidence="0.999217125">
Corpus LUNA WOZ+HH MEDIA
Approach (STK)
MT ST MT
FST 18.2 18.2 12.6
SVM 23.4 23.4 13.7
RR-A 15.6 17.0 11.6
RR-B 16.2 16.5 11.8
RR-C 16.1 16.4 11.7
</table>
<tableCaption confidence="0.997999">
Table 3: Results of experiments (CER) using FST
</tableCaption>
<bodyText confidence="0.999034894736842">
and SVMs with the Sytntactic Tree Kernel (STK)
on two different corpora: LUNA WOZ + HH, and
MEDIA.
based on SVMs and STK3 , on the largest datasets,
i.e. WOZ merged with HH dialogs and Media.
We trained all the SCLMs used in our experiments
with the SRILM toolkit (Stolcke, 2002) and we
used an interpolated model for probability esti-
mation with the Kneser-Ney discount (Chen and
Goodman, 1998). We then converted the model in
an FST as described in Section 2.1.
The model used to obtain the SVM baseline
for concept classification was trained using Yam-
CHA (Kudo and Matsumoto, 2001). For the re-
ranking models based on structure kernels, SVMs
or perceptron, we used the SVM-Light-TK toolkit
(available at dit.unitn.it/moschitti). For A (see Sec-
tion 3.2), cost-factor and trade-off parameters, we
used, 0.4, 1 and 1, respectively.
</bodyText>
<subsectionHeader confidence="0.999628">
4.3 Training approaches
</subsectionHeader>
<bodyText confidence="0.998939578947369">
The FST model generates the m-best annotations,
i.e. the data used to train the re-ranker based
on SVMs and perceptron. Different training ap-
proaches can be carried out based on the use of the
corpus and the method to generate the m-best. We
apply two different methods for training: Mono-
lithic Training and Split Training.
In the former, FSTs are learned with the whole
training set. The m-best hypotheses generated by
such models are then used to train the re-ranker
classifier. In Split Training, the training data are
divided in two parts to avoid bias in the FST gen-
eration step. More in detail, we train FSTs on part
1 and generate the m-best hypotheses using part 2.
Then, we re-apply these procedures inverting part
1 with part 2. Finally, we train the re-ranker on the
merged m-best data. At the classification time, we
generate the m-best of the test set using the FST
trained on all training data.
</bodyText>
<footnote confidence="0.9451745">
3The number of parameters, models and training ap-
proaches make the exhaustive experimentation expensive in
terms of processing time, which approximately requires 2 or
3 months.
</footnote>
<table confidence="0.998694166666667">
WOZ Monolithic Training
SVM PCT
STK PTK SK STK PTK SK
RR-A 18.5 19.3 19.1 24.2 28.3 23.3
RR-B 18.5 19.3 19.0 29.4 23.7 20.3
RR-C 18.5 19.3 19.1 31.5 30.0 20.2
</table>
<tableCaption confidence="0.8789478">
Table 4: Results of experiments, in terms of Con-
cept Error Rate (CER), on the LUNA WOZ corpus
using Monolithic Training approach. The baseline
with FST and SVMs used separately are 23.2%
and 26.7% respectively.
</tableCaption>
<table confidence="0.999488">
WOZ Split Training
SVM PCT
STK PTK SK STK PTK SK
RR-A 20.0 18.0 16.1 28.4 29.8 27.8
RR-B 19.0 19.0 19.0 26.3 30.0 25.6
RR-C 19.0 18.4 16.6 27.1 26.2 30.3
</table>
<tableCaption confidence="0.99085">
Table 5: Results of experiments, in terms of Con-
</tableCaption>
<bodyText confidence="0.991053111111111">
cept Error Rate (CER), on the LUNA WOZ cor-
pus using Split Training approach. The baseline
with FST and SVMs used separately are 23.2%
and 26.7% respectively.
Regarding the generation of the training in-
stances �si, sj/, we set m to 10 and we choose one
of the 10-best hypotheses as the second element of
the pair, sj, thus generating 10 different pairs.
The first element instead can be selected accord-
ing to three different approaches:
(A): si is the manual annotation taken from the
corpus;
(B) si is the most accurate annotation, in terms
of the edit distance from the manual annotation,
among the 10-best hypotheses of the FST model;
(C) as above but si is selected among the 100-
best hypotheses. The pairs are also inverted to
generate negative examples.
</bodyText>
<subsectionHeader confidence="0.993525">
4.4 Re-ranking results
</subsectionHeader>
<bodyText confidence="0.999934692307692">
All the results of our experiments, expressed in
terms of concept error rate (CER), are reported in
Table 3, 4 and 5.
In Table 3, the corpora, i.e. LUNA (WOZ+HH)
and Media, and the training approaches, i.e.
Monolithic Training (MT) and Split Training (ST),
are reported in the first and second row. Column
1 shows the concept classification model used, i.e.
the baselines FST and SVMs, and the re-ranking
models (RR) applied to FST. A, B and C refer
to the three approaches for generating training in-
stances described above. As already mentioned
for these large datasets, SVMs only use STK.
</bodyText>
<page confidence="0.994019">
208
</page>
<bodyText confidence="0.99558666">
We note that our re-rankers relevantly improve
our baselines, i.e. the FST and SVM concept clas-
sifiers on both corpora. For example, SVM re-
ranker using STK, MT and RR-A improves FST
concept classifier of 23.2-15.6 = 7.6 points.
Moreover, the monolithic training seems the
most appropriate to train the re-rankers whereas
approach A is the best in producing training in-
stances for the re-rankers. This is not surprising
since method A considers the manual annotation
as a referent gold standard and it always allows
comparing candidate annotations with the perfect
one.
Tables 4 and 5 have a similar structure of Ta-
ble 3 but they only show experiments on LUNA
WOZ corpus with respect to the monolithic and
split training approach, respectively. In these ta-
bles, we also report the result for SVMs and per-
ceptron (PCT) using STK, PTK and SK. We note
that:
First, the small size of WOZ training set (only
1,019 turns) impacts on the accuracy of the sys-
tems, e.g. FST and SVMs, which achieved a
CER of 18.2% and 23.4%, respectively, using also
HH dialogs, with only the WOZ data, they obtain
23.2% and 26.7%, respectively.
Second, the perceptron algorithm appears to be
ineffective for re-ranking. This is mainly due to
the reduced size of the WOZ data, which clearly
prevents an on line algorithm like PCT to ade-
quately refine its model by observing many exam-
ples4.
Third, the kernels which produce higher number
of substructures, i.e. PTK and SK, improves the
kernel less rich in terms of features, i.e. STK. For
example, using split training and approach A, STK
is improved by 20.0-16.1=3.9. This is an interest-
ing result since it shows that (a) richer structures
do produce better ranking models and (b) kernel
methods give a remarkable help in feature design.
Next, although the training data is small, the re-
rankers based on kernels appear to be very effec-
tive. This may also alleviate the burden of anno-
tating a lot of data.
Finally, the experiments of MEDIA show a not
so high improvement using re-rankers. This is due
to: (a) the baseline, i.e. the FST model is very
accurate since MEDIA is a large corpus thus the
re-ranker can only ”correct” small number of er-
rors; and (b) we could only experiment with the
</bodyText>
<footnote confidence="0.522495">
4We use only one iteration of the algorithm.
</footnote>
<bodyText confidence="0.999871375">
less expensive but also less accurate models, i.e.
monolithic training and STK.
Media also offers the possibility to compare
with the state-of-the-art, which our re-rankers
seem to improve. However, we need to consider
that many Media corpus versions exist and this
makes such comparisons not completely reliable.
Future work on the paper research line appears
to be very interesting: the assessment of our best
models on Media and WOZ+HH as well as other
corpora is required. More importantly, the struc-
tures that we have proposed for re-ranking are
just two of the many possibilities to encode both
word/concept statistical distributions and linguis-
tic knowledge encoded in syntactic/semantic parse
trees.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999987666666667">
In this paper, we propose discriminative re-
ranking of concept annotation to capitalize from
the benefits of generative and discriminative ap-
proaches. Our generative approach is the state-
of-the-art in concept classification since we used
the same FST model used in (Raymond and Ric-
cardi, 2007). We could improve it by 1% point
in MEDIA and 7.6 points (until 30% of relative
improvement) on LUNA, where the more limited
availability of annotated data leaves a larger room
for improvement.
It should be noted that to design the re-ranking
model, we only used two different structures,
i.e. one sequence and one tree. Kernel meth-
ods show that combinations of feature vectors, se-
quence kernels and other structural kernels, e.g.
on shallow or deep syntactic parse trees, appear
to be a promising research line (Moschitti, 2008).
Also, the approach used in (Zanzotto and Mos-
chitti, 2006) to define cross pair relations may be
exploited to carry out a more effective pair re-
ranking. Finally, the experimentation with auto-
matic speech transcriptions is interesting to test the
robustness of our models to transcription errors.
</bodyText>
<sectionHeader confidence="0.999033" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.802112">
This work has been partially supported by the Eu-
ropean Commission - LUNA project, contract n.
33549.
</bodyText>
<page confidence="0.998427">
209
</page>
<sectionHeader confidence="0.993895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895524590164">
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of In-
terspeech2005, Lisbon, Portugal.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. In
Technical Report of Computer Science Group, Har-
vard, USA.
M. Collins and N. Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over
Discrete structures, and the voted perceptron. In
ACL02, pages 263–270.
Y. He and S. Young. 2005. Semantic processing us-
ing the hidden vector state model. Computer Speech
and Language, 19:85–106.
T. Kudo and Y. Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL2001, Pittsburg, USA.
A. Moschitti and C. Bejan. 2004. A semantic ker-
nel for predicate argument classification. In CoNLL-
2004, Boston, MA, USA.
A. Moschitti, D. Pighin, and R. Basili. 2006. Seman-
tic role labeling via tree kernel joint inference. In
Proceedings of CoNLL-X, New York City.
A. Moschitti, G. Riccardi, and C. Raymond. 2007.
Spoken language understanding with kernels for
syntactic/semantic structures. In Proceedings of
ASRU2007, Kyoto, Japan.
A. Moschitti, D. Pighin, and R. Basili. 2008. Tree
kernels for semantic role labeling. Computational
Linguistics, 34(2):193–224.
A. Moschitti. 2006. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees. In
Proceedings ofECML 2006, pages 318–329, Berlin,
Germany.
A. Moschitti. 2008. Kernel methods, syntax and se-
mantics for relational text categorization. In CIKM
’08: Proceeding of the 17th ACM conference on In-
formation and knowledge management, pages 253–
262, New York, NY, USA. ACM.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language un-
derstanding. In Proceedings of Interspeech2007,
Antwerp,Belgium.
C. Raymond, G. Riccardi, K. J. Rodrigez, and J. Wis-
niewska. 2007. The luna corpus: an annotation
scheme for a multi-domain multi-lingual dialogue
corpus. In Proceedings of Decalog2007, Trento,
Italy.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
A. Stolcke. 2002. Srilm: an extensible language mod-
eling toolkit. In Proceedings of SLP2002, Denver,
USA.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
F. M. Zanzotto and A. Moschitti. 2006. Automatic
learning of textual entailments with cross-pair simi-
larities. In Proceedings of the 21st Coling and 44th
ACL, pages 401–408, Sydney, Australia, July.
</reference>
<page confidence="0.998961">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885577">
<title confidence="0.99996">Re-Ranking Models For Spoken Language Understanding</title>
<author confidence="0.999959">Marco Dinarelli</author>
<affiliation confidence="0.999989">University of Trento</affiliation>
<address confidence="0.993058">Italy</address>
<email confidence="0.997815">dinarelli@disi.unitn.it</email>
<author confidence="0.999333">Alessandro Moschitti</author>
<affiliation confidence="0.999982">University of Trento</affiliation>
<address confidence="0.937121">Italy</address>
<email confidence="0.998708">moschitti@disi.unitn.it</email>
<author confidence="0.999666">Giuseppe Riccardi</author>
<affiliation confidence="0.999991">University of Trento</affiliation>
<address confidence="0.987065">Italy</address>
<email confidence="0.998931">riccardi@disi.unitn.it</email>
<abstract confidence="0.9987885">Spoken Language Understanding aims at mapping a natural language spoken sentence into a semantic representation. In the last decade two main approaches have been pursued: generative and discriminative models. The former is more robust to overfitting whereas the latter is more robust to many irrelevant features. Additionally, the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task. In this paper we describe a machine learning framework where both models are used: a generative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and Support Vector Machines, re-ranks such list. We tested our approach on the MEDIA corpus (human-machine dialogs) and on a new corpus (human-machine and humanhuman dialogs) produced in the European LUNA project. The results show a large improvement on the state-of-the-art in concept segmentation and labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Bonneau-Maynard</author>
<author>S Rosset</author>
<author>C Ayache</author>
<author>A Kuhn</author>
<author>D Mostefa</author>
</authors>
<title>Semantic annotation of the french media dialog corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech2005,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4459" citStr="Bonneau-Maynard et al., 2005" startWordPosition="680" endWordPosition="683">eece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 202 ing of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sections 2 and 3 show the generative and discriminative models, respectively. The experiments and results are reported in Section 4 whereas the conclusions are drawn in Section 5. 2 Generative approach for concept classification In the context of Spoken Language Understanding (SLU), co</context>
</contexts>
<marker>Bonneau-Maynard, Rosset, Ayache, Kuhn, Mostefa, 2005</marker>
<rawString>H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn, and D. Mostefa. 2005. Semantic annotation of the french media dialog corpus. In Proceedings of Interspeech2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. In</title>
<date>1998</date>
<tech>Technical Report of</tech>
<institution>Computer Science Group,</institution>
<location>Harvard, USA.</location>
<contexts>
<context position="23731" citStr="Chen and Goodman, 1998" startWordPosition="4148" endWordPosition="4151"> only one model, i.e. the one 207 Corpus LUNA WOZ+HH MEDIA Approach (STK) MT ST MT FST 18.2 18.2 12.6 SVM 23.4 23.4 13.7 RR-A 15.6 17.0 11.6 RR-B 16.2 16.5 11.8 RR-C 16.1 16.4 11.7 Table 3: Results of experiments (CER) using FST and SVMs with the Sytntactic Tree Kernel (STK) on two different corpora: LUNA WOZ + HH, and MEDIA. based on SVMs and STK3 , on the largest datasets, i.e. WOZ merged with HH dialogs and Media. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST as described in Section 2.1. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti). For A (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively. 4.3 Training approaches The FST model generates the m-best annotations, i.e. the data used to train the re-ranker based on SVMs and perceptron. Different training a</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Technical Report of Computer Science Group, Harvard, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL02,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="15407" citStr="Collins and Duffy, 2002" startWordPosition="2639" endWordPosition="2642">efined as: 1. if the productions at n1 and n2 are different then Δ(n1, n2) = 0; 2. if the productions at n1 and n2 are the same, and n1 and n2 have only leaf children (i.e. they are pre-terminals symbols) then Δ(n1, n2) = 1; 3. if the productions at n1 and n2 are the same, and n1 and n2 are not pre-terminals then nc(n1) Δ(n1, n2) = Y (σ + Δ(cjn1, cj n2)) (1) j=1 where σ ∈ {0, 1}, nc(n1) is the number of children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, nc(n1) = nc(n2). Δ(n1, n2) evaluates the number of STFs common to n1 and n2 as proved in (Collins and Duffy, 2002). Moreover, a decay factor λ can be added by modifying steps (2) and (3) as follows2: 2. Δ(n1, n2) = λ, 3. Δ(n1, n2) = λQnc(n1) j=1 (σ + Δ(cjn1, cjn2)). The computational complexity of Eq. 1 is O(|NT1 |× |NT2|) but as shown in (Moschitti, 2006), the average running time tends to be linear, i.e. O(|NT1 |+ |NT2|), for natural language syntactic trees. 3.6 The Partial Tree Kernel (PTK) PTFs have been defined in (Moschitti, 2006). Their computation is carried out by the following Δ function: 1. if the node labels of n1 and n2 are different then Δ(n1, n2) = 0; 2. else Δ(n1, n2) = 1+P Ql(~I1) ~I1,~I</context>
<context position="19228" citStr="Collins and Duffy, 2002" startWordPosition="3370" endWordPosition="3373">ts. Given the above data, the sequence kernel is used to evaluate the number of common ngrams between si and sj. Since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function as follows: let ei be the pair (s1i, s2 � we evaluate i the kernel: KR(e1, e2) = SK(s1 1, s12) + SK(s21, s22) (3) − SK(s11, s22) − SK(s21, s12) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK and in (Moschitti et al., 2006), where, to rerank Semantic Role Labeling annotations, a tree kernel was used on a semantic tree similar to the one introduced in the next section. 3.8 Re-ranking models using trees Since the aim in concept annotation re-ranking is to exploit innovative and effective source of information, we can use the power of tree kernels to generate correlation between concepts and word structures. Fig. 2 describes the structural association between the concept and the word level. This </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete structures, and the voted perceptron. In ACL02, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>Semantic processing using the hidden vector state model. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="2088" citStr="He and Young, 2005" startWordPosition="310" endWordPosition="313">ation and are often referred to as concepts. Concepts are instantiated by sequences of words, therefore a Spoken Language Understanding (SLU) module finds the association between words and concepts. In the last decade two major approaches have been proposed to find this correlation: (i) generative models, whose parameters refer to the joint probability of concepts and constituents; and (ii) discriminative models, which learn a classification function to map words into concepts based on geometric and statistical properties. An example of generative model is the Hidden Vector State model (HVS) (He and Young, 2005). This approach extends the discrete Markov model encoding the context of each state as a vector. State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label. In this way the model can capture semantic hierarchical structures without the use of tree-structured data. Another simpler but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vect</context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>Y. He and S. Young. 2005. Semantic processing using the hidden vector state model. Computer Speech and Language, 19:85–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL2001,</booktitle>
<location>Pittsburg, USA.</location>
<contexts>
<context position="23920" citStr="Kudo and Matsumoto, 2001" startWordPosition="4181" endWordPosition="4184">3: Results of experiments (CER) using FST and SVMs with the Sytntactic Tree Kernel (STK) on two different corpora: LUNA WOZ + HH, and MEDIA. based on SVMs and STK3 , on the largest datasets, i.e. WOZ merged with HH dialogs and Media. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST as described in Section 2.1. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti). For A (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively. 4.3 Training approaches The FST model generates the m-best annotations, i.e. the data used to train the re-ranker based on SVMs and perceptron. Different training approaches can be carried out based on the use of the corpus and the method to generate the m-best. We apply two different methods for training: Monolithic Training and Split Training. In th</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL2001, Pittsburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>C Bejan</author>
</authors>
<title>A semantic kernel for predicate argument classification.</title>
<date>2004</date>
<booktitle>In CoNLL2004,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="4203" citStr="Moschitti and Bejan, 2004" startWordPosition="645" endWordPosition="648">s. These exploit all possible word/concept subsequences (with gaps) of the spoken sentence as features (i.e. all possible n-grams). Gaps allow for the encodProceedings of the 12th Conference of the European Chapter of the ACL, pages 202–210, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 202 ing of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sections 2 and 3 show the gene</context>
</contexts>
<marker>Moschitti, Bejan, 2004</marker>
<rawString>A. Moschitti and C. Bejan. 2004. A semantic kernel for predicate argument classification. In CoNLL2004, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>D Pighin</author>
<author>R Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<location>New York City.</location>
<contexts>
<context position="19349" citStr="Moschitti et al., 2006" startWordPosition="3391" endWordPosition="3394"> string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function as follows: let ei be the pair (s1i, s2 � we evaluate i the kernel: KR(e1, e2) = SK(s1 1, s12) + SK(s21, s22) (3) − SK(s11, s22) − SK(s21, s12) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK and in (Moschitti et al., 2006), where, to rerank Semantic Role Labeling annotations, a tree kernel was used on a semantic tree similar to the one introduced in the next section. 3.8 Re-ranking models using trees Since the aim in concept annotation re-ranking is to exploit innovative and effective source of information, we can use the power of tree kernels to generate correlation between concepts and word structures. Fig. 2 describes the structural association between the concept and the word level. This kind of trees allows us to engineer new kernels and consequently new features (Moschitti et al., 2008), � � µ λ2+ ~I1,~I2</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>A. Moschitti, D. Pighin, and R. Basili. 2006. Semantic role labeling via tree kernel joint inference. In Proceedings of CoNLL-X, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>G Riccardi</author>
<author>C Raymond</author>
</authors>
<title>Spoken language understanding with kernels for syntactic/semantic structures.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU2007,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="6808" citStr="Moschitti et al., 2007" startWordPosition="1076" endWordPosition="1079"> class is the word itself. In the second step, classes are mapped into concepts. The mapping is not one-to-one: a class may be associated with more than one concept, i.e. more than one SLU hypothesis can be generated. In the third step, the best or the m-best hypotheses are selected among those produced in the previous step. They are chosen according to the maximum probability evaluated by the Conceptual Language Model, described in the next section. 2.1 Stochastic Conceptual Language Model (SCLM) An SCLM is an n-gram language model built on semantic tags. Using the same notation proposed in (Moschitti et al., 2007) and (Raymond and Riccardi, 2007), our SCLM trains joint probability P(W, C) of word and concept sequences from an annotated corpus: k P(W, C) = P(wi, ci|hi), i=1 where W = w1..wk, C = c1..ck and hi = wi−1ci−1..w1c1. Since we use a 3-gram conceptual language model, the history hi is {wi−1ci−1, wi−2ci−2}. All the steps of the translation process described here and above are implemented as Finite State Transducers (FST) using the AT&amp;T FSM/GRM tools and the SRILM (Stolcke, 2002) tools. In particular the SCLM is trained using SRILM tools and then converted to an FST. This allows the use of a wide </context>
</contexts>
<marker>Moschitti, Riccardi, Raymond, 2007</marker>
<rawString>A. Moschitti, G. Riccardi, and C. Raymond. 2007. Spoken language understanding with kernels for syntactic/semantic structures. In Proceedings of ASRU2007, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>D Pighin</author>
<author>R Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="19930" citStr="Moschitti et al., 2008" startWordPosition="3488" endWordPosition="3491">tead of SK and in (Moschitti et al., 2006), where, to rerank Semantic Role Labeling annotations, a tree kernel was used on a semantic tree similar to the one introduced in the next section. 3.8 Re-ranking models using trees Since the aim in concept annotation re-ranking is to exploit innovative and effective source of information, we can use the power of tree kernels to generate correlation between concepts and word structures. Fig. 2 describes the structural association between the concept and the word level. This kind of trees allows us to engineer new kernels and consequently new features (Moschitti et al., 2008), � � µ λ2+ ~I1,~I2,l(~I1)=l( λd(~I1)+d( l( ~I1) ~I2) � j=1 206 Figure 2: An example of the semantic tree used for STK or PTK Corpus Train set Test set LUNA words concepts words concepts Dialogs WOZ 183 67 Dialogs HH 180 - Turns WOZ 1.019 373 Turns HH 6.999 - Tokens WOZ 8.512 2.887 2.888 984 Tokens WOZ 62.639 17.423 - - Vocab. WOZ 1.172 34 - - Vocab. HH 4.692 49 - - OOV rate - - 3.2% 0.1% Table 1: Statistics on the LUNA corpus Corpus Train set Test set Media words concepts words concepts Turns 12,922 3,518 # of tokens 94,912 43,078 26,676 12,022 Vocabulary 5,307 80 - - OOV rate - - 0.01% 0.0% </context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>A. Moschitti, D. Pighin, and R. Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings ofECML 2006,</booktitle>
<pages>318--329</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="4221" citStr="Moschitti, 2006" startWordPosition="649" endWordPosition="650">le word/concept subsequences (with gaps) of the spoken sentence as features (i.e. all possible n-grams). Gaps allow for the encodProceedings of the 12th Conference of the European Chapter of the ACL, pages 202–210, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 202 ing of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sections 2 and 3 show the generative and discrim</context>
<context position="15651" citStr="Moschitti, 2006" startWordPosition="2688" endWordPosition="2689"> and n2 are the same, and n1 and n2 are not pre-terminals then nc(n1) Δ(n1, n2) = Y (σ + Δ(cjn1, cj n2)) (1) j=1 where σ ∈ {0, 1}, nc(n1) is the number of children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, nc(n1) = nc(n2). Δ(n1, n2) evaluates the number of STFs common to n1 and n2 as proved in (Collins and Duffy, 2002). Moreover, a decay factor λ can be added by modifying steps (2) and (3) as follows2: 2. Δ(n1, n2) = λ, 3. Δ(n1, n2) = λQnc(n1) j=1 (σ + Δ(cjn1, cjn2)). The computational complexity of Eq. 1 is O(|NT1 |× |NT2|) but as shown in (Moschitti, 2006), the average running time tends to be linear, i.e. O(|NT1 |+ |NT2|), for natural language syntactic trees. 3.6 The Partial Tree Kernel (PTK) PTFs have been defined in (Moschitti, 2006). Their computation is carried out by the following Δ function: 1. if the node labels of n1 and n2 are different then Δ(n1, n2) = 0; 2. else Δ(n1, n2) = 1+P Ql(~I1) ~I1,~I2,l(~I1)=l(~I2) j=1 Δ(cn1( 2To have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e.: K′(T1, T2) = TK(T1,T2) √T K (T1,T1) × T K (T2 ,T2) . ~I1j), cn2( ~I2j)) 205 D N a brought cat a a VP VP VP VP VP </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings ofECML 2006, pages 318–329, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>253--262</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Moschitti, 2008</marker>
<rawString>A. Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 253– 262, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond</author>
<author>G Riccardi</author>
</authors>
<title>Generative and discriminative algorithms for spoken language understanding.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech2007,</booktitle>
<location>Antwerp,Belgium.</location>
<contexts>
<context position="2763" citStr="Raymond and Riccardi, 2007" startWordPosition="422" endWordPosition="425">l encoding the context of each state as a vector. State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label. In this way the model can capture semantic hierarchical structures without the use of tree-structured data. Another simpler but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007). In this approach, data are mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Shawe-Taylor and Cristianini, 2004). Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrelevant features. Both approaches, used separately, have shown a good performance (Raymond and Riccardi, 2007), but they have very different characteristics and the way they encode prior knowledge is very different, thus designing models able to take into account characteristics of bot</context>
<context position="4176" citStr="Raymond and Riccardi, 2007" startWordPosition="641" endWordPosition="644">, which are re-ranked by SVMs. These exploit all possible word/concept subsequences (with gaps) of the spoken sentence as features (i.e. all possible n-grams). Gaps allow for the encodProceedings of the 12th Conference of the European Chapter of the ACL, pages 202–210, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 202 ing of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sec</context>
<context position="5579" citStr="Raymond and Riccardi, 2007" startWordPosition="867" endWordPosition="870"> 2 Generative approach for concept classification In the context of Spoken Language Understanding (SLU), concept classification is the task of associating the best sequence of concepts to a given sentence, i.e. word sequence. A concept is a class containing all the words carrying out the same semantic meaning with respect to the application domain. In SLU, concepts are used as semantic units and are represented with concept tags. The association between words and concepts is learned from an annotated corpus. The Generative model used in our work for concept classification is the same used in (Raymond and Riccardi, 2007). Given a sequence of words as input, a translation process based on FST is performed to output a sequence of concept tags. The translation process involves three steps: (1) the mapping of words into classes (2) the mapping of classes into concepts and (3) the selection of the best concept sequence. The first step is used to improve the generalization power of the model. The word classes at this level can be both domain-dependent, e.g. ”Hotel” in MEDIA or ”Software” in the LUNA corpus, or domain-independent, e.g. numbers, dates, months 1Contract n. 33549 etc. The class of a word not belonging </context>
<context position="6841" citStr="Raymond and Riccardi, 2007" startWordPosition="1081" endWordPosition="1085"> the second step, classes are mapped into concepts. The mapping is not one-to-one: a class may be associated with more than one concept, i.e. more than one SLU hypothesis can be generated. In the third step, the best or the m-best hypotheses are selected among those produced in the previous step. They are chosen according to the maximum probability evaluated by the Conceptual Language Model, described in the next section. 2.1 Stochastic Conceptual Language Model (SCLM) An SCLM is an n-gram language model built on semantic tags. Using the same notation proposed in (Moschitti et al., 2007) and (Raymond and Riccardi, 2007), our SCLM trains joint probability P(W, C) of word and concept sequences from an annotated corpus: k P(W, C) = P(wi, ci|hi), i=1 where W = w1..wk, C = c1..ck and hi = wi−1ci−1..w1c1. Since we use a 3-gram conceptual language model, the history hi is {wi−1ci−1, wi−2ci−2}. All the steps of the translation process described here and above are implemented as Finite State Transducers (FST) using the AT&amp;T FSM/GRM tools and the SRILM (Stolcke, 2002) tools. In particular the SCLM is trained using SRILM tools and then converted to an FST. This allows the use of a wide set of stochastic language models</context>
<context position="30501" citStr="Raymond and Riccardi, 2007" startWordPosition="5300" endWordPosition="5304">e assessment of our best models on Media and WOZ+HH as well as other corpora is required. More importantly, the structures that we have proposed for re-ranking are just two of the many possibilities to encode both word/concept statistical distributions and linguistic knowledge encoded in syntactic/semantic parse trees. 5 Conclusions In this paper, we propose discriminative reranking of concept annotation to capitalize from the benefits of generative and discriminative approaches. Our generative approach is the stateof-the-art in concept classification since we used the same FST model used in (Raymond and Riccardi, 2007). We could improve it by 1% point in MEDIA and 7.6 points (until 30% of relative improvement) on LUNA, where the more limited availability of annotated data leaves a larger room for improvement. It should be noted that to design the re-ranking model, we only used two different structures, i.e. one sequence and one tree. Kernel methods show that combinations of feature vectors, sequence kernels and other structural kernels, e.g. on shallow or deep syntactic parse trees, appear to be a promising research line (Moschitti, 2008). Also, the approach used in (Zanzotto and Moschitti, 2006) to define </context>
</contexts>
<marker>Raymond, Riccardi, 2007</marker>
<rawString>C. Raymond and G. Riccardi. 2007. Generative and discriminative algorithms for spoken language understanding. In Proceedings of Interspeech2007, Antwerp,Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond</author>
<author>G Riccardi</author>
<author>K J Rodrigez</author>
<author>J Wisniewska</author>
</authors>
<title>The luna corpus: an annotation scheme for a multi-domain multi-lingual dialogue corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of Decalog2007,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="4538" citStr="Raymond et al., 2007" startWordPosition="694" endWordPosition="698"> of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sections 2 and 3 show the generative and discriminative models, respectively. The experiments and results are reported in Section 4 whereas the conclusions are drawn in Section 5. 2 Generative approach for concept classification In the context of Spoken Language Understanding (SLU), concept classification is the task of associating the best sequence of concepts t</context>
<context position="21740" citStr="Raymond et al., 2007" startWordPosition="3806" endWordPosition="3809">he corpora, parameters, models and results of our experiments of word chunking and concept classification. Our baseline relates to the error rate of systems based on only FST and SVMs. The re-ranking models are built on the FST output. Different ways of producing training data for the re-ranking models determine different results. 4.1 Corpora We used two different speech corpora: The corpus LUNA, produced in the homonymous European project is the first Italian corpus of spontaneous speech on spoken dialog: it is based on the help-desk conversation in the domain of software/hardware repairing (Raymond et al., 2007). The data are organized in transcriptions and annotations of speech based on a new multilevel protocol. Data acquisition is still in progress. Currently, 250 dialogs acquired with a WOZ approach and 180 Human-Human (HH) dialogs are available. Statistics on LUNA corpus are reported in Table 1. The corpus MEDIA was collected within the French project MEDIA-EVALDA (BonneauMaynard et al., 2005) for development and evaluation of spoken understanding models and linguistic studies. The corpus is composed of 1257 dialogs, from 250 different speakers, acquired with a Wizard of Oz (WOZ) approach in the</context>
</contexts>
<marker>Raymond, Riccardi, Rodrigez, Wisniewska, 2007</marker>
<rawString>C. Raymond, G. Riccardi, K. J. Rodrigez, and J. Wisniewska. 2007. The luna corpus: an annotation scheme for a multi-domain multi-lingual dialogue corpus. In Proceedings of Decalog2007, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2937" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="448" endWordPosition="451">l. In this way the model can capture semantic hierarchical structures without the use of tree-structured data. Another simpler but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007). In this approach, data are mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Shawe-Taylor and Cristianini, 2004). Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrelevant features. Both approaches, used separately, have shown a good performance (Raymond and Riccardi, 2007), but they have very different characteristics and the way they encode prior knowledge is very different, thus designing models able to take into account characteristics of both approaches are particularly promising. In this paper we propose a method for SLU based on generative and discriminative models: the former uses FSTs to generate a list of S</context>
<context position="10302" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1680" endWordPosition="1684">+ b = 0, where x� is the feature vector representation of a classifying object o, w� E Rn (a vector space) and b E R are parameters (Vapnik, 1995). The classifying object o is mapped into x� by a feature function 0. The kernel trick allows us to rewrite the decision hyperplane as � i=1..lyiαi0(oi)0(o) + b = 0, where yi is equal to 1 for positive and -1 for negative examples, αi E R+, oiVi E {1..l} are the training instances and the product K(oi, o) = (0(oi)0(o)) is the kernel function associated with the mapping 0. Note that we do not need to apply the mapping 0, we can use K(oi, o) directly (Shawe-Taylor and Cristianini, 2004). For example, next section shows a kernel function that counts the number of word se3.2 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. Gaps modify the weight associated with the target substrings as shown in the following. Let E be a finite alphabet, E∗ = U∞n= 0 En is the set of all strings. Given a string s E E∗, |s |denotes the length of the strings and si its compounding symbols, i.e s = s1..s|s|, whereas s[i : j] selects the substring sisi+1..sj−1sj from</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of SLP2002,</booktitle>
<location>Denver, USA.</location>
<contexts>
<context position="7288" citStr="Stolcke, 2002" startWordPosition="1160" endWordPosition="1161"> Model (SCLM) An SCLM is an n-gram language model built on semantic tags. Using the same notation proposed in (Moschitti et al., 2007) and (Raymond and Riccardi, 2007), our SCLM trains joint probability P(W, C) of word and concept sequences from an annotated corpus: k P(W, C) = P(wi, ci|hi), i=1 where W = w1..wk, C = c1..ck and hi = wi−1ci−1..w1c1. Since we use a 3-gram conceptual language model, the history hi is {wi−1ci−1, wi−2ci−2}. All the steps of the translation process described here and above are implemented as Finite State Transducers (FST) using the AT&amp;T FSM/GRM tools and the SRILM (Stolcke, 2002) tools. In particular the SCLM is trained using SRILM tools and then converted to an FST. This allows the use of a wide set of stochastic language models (both back-off and interpolated models with several discounting techniques like Good-Turing, WittenBell, Natural, Kneser-Ney, Unchanged KneserNey etc). We represent the combination of all the translation steps as a transducer ASLU (Raymond and Riccardi, 2007) in terms of FST operations: ASLU = AW o AW2C o ASLM, where AW is the transducer representation of the input sentence, AW2C is the transducer mapping words to classes and ASLM is the Sema</context>
<context position="23616" citStr="Stolcke, 2002" startWordPosition="4132" endWordPosition="4133">ring Kernel (SK) described in Section 3.3. Given the high number and the cost of these experiments, we ran only one model, i.e. the one 207 Corpus LUNA WOZ+HH MEDIA Approach (STK) MT ST MT FST 18.2 18.2 12.6 SVM 23.4 23.4 13.7 RR-A 15.6 17.0 11.6 RR-B 16.2 16.5 11.8 RR-C 16.1 16.4 11.7 Table 3: Results of experiments (CER) using FST and SVMs with the Sytntactic Tree Kernel (STK) on two different corpora: LUNA WOZ + HH, and MEDIA. based on SVMs and STK3 , on the largest datasets, i.e. WOZ merged with HH dialogs and Media. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST as described in Section 2.1. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti). For A (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively. 4.3 Training approaches The FST model generates t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of SLP2002, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2721" citStr="Vapnik, 1995" startWordPosition="417" endWordPosition="418">nds the discrete Markov model encoding the context of each state as a vector. State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label. In this way the model can capture semantic hierarchical structures without the use of tree-structured data. Another simpler but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007). In this approach, data are mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Shawe-Taylor and Cristianini, 2004). Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrelevant features. Both approaches, used separately, have shown a good performance (Raymond and Riccardi, 2007), but they have very different characteristics and the way they encode prior knowledge is very different, thus designing models able t</context>
<context position="9813" citStr="Vapnik, 1995" startWordPosition="1590" endWordPosition="1592">pairs of conceptually annotated sentences. The classifiers learn to select which annotation has an error rate lower than the others so that the m-best annotations can be sorted based on their correctness. 3.1 SVMs and Kernel Methods Kernel Methods refer to a large class of learning algorithms based on inner product vector spaces, among which Support Vector Machines (SVMs) are one of the most well known algorithms. SVMs and perceptron learn a hyperplane H(x) = wx + b = 0, where x� is the feature vector representation of a classifying object o, w� E Rn (a vector space) and b E R are parameters (Vapnik, 1995). The classifying object o is mapped into x� by a feature function 0. The kernel trick allows us to rewrite the decision hyperplane as � i=1..lyiαi0(oi)0(o) + b = 0, where yi is equal to 1 for positive and -1 for negative examples, αi E R+, oiVi E {1..l} are the training instances and the product K(oi, o) = (0(oi)0(o)) is the kernel function associated with the mapping 0. Note that we do not need to apply the mapping 0, we can use K(oi, o) directly (Shawe-Taylor and Cristianini, 2004). For example, next section shows a kernel function that counts the number of word se3.2 String Kernels The Str</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<pages>401--408</pages>
<location>Sydney, Australia,</location>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>F. M. Zanzotto and A. Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, pages 401–408, Sydney, Australia, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>