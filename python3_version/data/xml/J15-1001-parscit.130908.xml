<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98894">
Towards Topic-to-Question Generation
</title>
<author confidence="0.995538">
Yllias Chali*
</author>
<affiliation confidence="0.992396">
University of Lethbridge
</affiliation>
<author confidence="0.566755">
Sadid A. Hasan**
</author>
<affiliation confidence="0.261494">
Philips Research North America
</affiliation>
<bodyText confidence="0.997798769230769">
This paper is concerned with automatic generation of all possible questions from a topic of
interest. Specifically, we consider that each topic is associated with a body of texts containing
useful information about the topic. Then, questions are generated by exploiting the named entity
information and the predicate argument structures of the sentences present in the body of texts.
The importance of the generated questions is measured using Latent Dirichlet Allocation by
identifying the subtopics (which are closely related to the original topic) in the given body of
texts and applying the Extended String Subsequence Kernel to calculate their similarity with
the questions. We also propose the use of syntactic tree kernels for the automatic judgment of
the syntactic correctness of the questions. The questions are ranked by considering both their
importance (in the context of the given body of texts) and syntactic correctness. To the best of
our knowledge, no previous study has accomplished this task in our setting. A series of exper-
iments demonstrate that the proposed topic-to-question generation approach can significantly
outperform the state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999991916666667">
We live in an information age where all kinds of information is easily accessible through
the Internet. The increasing demand for access to different types of information avail-
able online have interested researchers in a broad range of Information Retrieval–related
areas, such as question answering, topic detection and tracking, summarization, multi-
media retrieval, chemical and biological informatics, text structuring, and text mining.
Although search engines do a remarkable job in searching through a heap of informa-
tion, they have certain limitations, as they cannot satisfy the end users’ information
need to have more direct access to relevant documents. For example, if we ask for the
impact of the current global financial crisis in different parts of the world, we can expect
to sift through thousands of results for the answer. This fact can be more understandable
by the following scenario. When a user enters a query, they are served with a ranked list
of relevant documents by the standard document retrieval systems (i.e., search engines),
</bodyText>
<footnote confidence="0.721696">
* University of Lethbridge, 4401 University Drive West, Lethbridge, Alberta, T1K 3M4, Canada.
E-mail: chali®cs.uleth.ca.
** Philips Research North America, 345 Scarborough Rd, Briarcliff Manor, New York, 10510, USA.
E-mail: sadid.hasan®philips.com.
Submission received: 19 May, 2013; revised submission received: 26 May, 2014; accepted for publication:
22 June, 2014.
doi:10.1162/COLI a 00206
</footnote>
<note confidence="0.8812355">
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999501217391304">
and their search task is usually not over (Chali, Joty, and Hasan 2009). The next step
for the user is to look into the documents themselves and search for the precise piece of
information they were looking for. This method is time-consuming, and a correct
answer could easily be missed by either an incorrect query resulting in missing
documents or by careless reading. This is why Question Answering (QA) has received
immense attention from the information retrieval, information extraction, machine
learning, and natural language processing communities in the last 15 years (Hirschman
and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010).
The main goal of QA systems is to retrieve relevant answers to natural language
questions from a collection of documents rather than using keyword matching tech-
niques to extract documents. Automated QA research focuses on how to respond with
exact answers to a wide variety of questions, including: factoid, list, definition, how,
why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965;
Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang
2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA
system is that it must receive a well-formed question as input in order to come up
with the best possible correct answer as output. Available studies revealed that humans
are not very skilled in asking good questions about a topic of their interest. They are
forgetful in nature; this often restricts them to properly express whatever that is peeking
in their mind. Therefore, they would benefit from automated Question Generation (QG)
systems that can assist in meeting their inquiry needs (Lauer, Peacock, and Graesser
1992; Graesser et al. 2001; Rus and Graesser 2009; Ali, Chali, and Hasan 2010; Kotov
and Zhai 2010; Olney, Graesser, and Person 2012). Another benefit of QG is that it can
be a good tool to help improve the quality of the QA systems (Graesser et al. 2001; Rus
and Graesser 2009). These benefits of a QG system motivate us to address the important
problem of topic-to-question generation, where the main goal is to generate all possible
questions about a given topic. For example, given the topic Apple Inc. Logos, we would
like to generate questions such as What is Apple Inc.?, Where is Apple Inc. located?, Who
designed Apple’s Logo?, and so forth.
The problem of topic-to-question generation can be viewed as a generalization
of the problem of answering complex questions. Complex questions are essentially
broader information requests about a certain topic, whose answers could be obtained
from pieces of information scattered in multiple documents. For example, consider the
complex question:1 Describe steps taken and worldwide reaction prior to the introduction
of the Euro on January 1, 1999. Include predictions and expectations reported in the press.
This question is requesting an elaboration about the topic “Introduction of the Euro,”
which can be answered by following complex procedures such as question decom-
position or inferencing and synthesizing information from multiple documents (e.g.,
multi-document summarization). Answering complex questions is not easy as it is not
always understandable to which direction one should move to search for the answer
to a complex question. This situation arises because of the wider focus of the topic
that is inherent in the complex question in consideration. For example, a complex
question like Describe the tsunami disaster in Japan has a wider focus without a single
or well-defined information need. To narrow down the focus, this question can be
decomposed into a series of simple questions such as How many people were killed in the
tsunami?, How many people became homeless?, Which cities were mostly damaged?, and so on.
</bodyText>
<footnote confidence="0.9889895">
1 The example complex questions have been provided according to the guidelines of the Document
Understanding Conference (DUC, http://duc.nist.gov/) (2005–2007) tasks.
</footnote>
<page confidence="0.890307">
2
</page>
<note confidence="0.852612">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<bodyText confidence="0.999784675675676">
Decomposing a complex question automatically into simpler questions in this manner
such that each of them can be answered individually by using the state-of-the-art QA
systems, and then combining the individual answers to form a single answer to the
original complex question, has proven effective to deal with the complex question
answering problem (Harabagiu, Lacatusu, and Hickl 2006; Hickl et al. 2006; Chali,
Hasan, and Imam 2012). Moreover, the generated simple questions can be used as the
list of important aspects to act as a guide2 for selecting the most relevant sentences in
producing more focused and more accurate summaries as the output of a summariza-
tion system (Chali, Hasan, and Imam 2011, 2012). From this discussion, it is obvious
that the complex question decomposition problem can be generalized to the problem of
topic-to-question generation to help improve the complex question answering systems.
In this article3, we consider the task of automatically generating questions from
topics and assume that each topic is associated with a body of texts having useful
information about the topic. This assumption has been inherited from the process of
how a human asks questions based on their knowledge. For example, if a person knows
that a university is an educational institution, then they can ask a question about its
faculty and students. In this research, our main goal is to generate fact-based questions4
about a given topic from its associated content information. We generate questions by
exploiting the named entity information and the predicate argument structures of the
sentences (along with semantic roles) present in the given body of texts. The named
entities and the semantic role labels are used to identify relevant parts of a sentence in
order to form relevant questions about them. The importance of the generated questions
is measured in two steps. In the first step, we identify whether the question is asking
something about the topic or something that is very closely related to the topic. We call
this the measure of topic relevance. For this purpose, we use Latent Dirichlet Allocation
(LDA) (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to
the original topic) in the given body of texts and apply the Extended String Subsequence
Kernel (ESSK) (Hirao et al. 2003) to calculate their similarity with the questions. In the
second step, we judge the syntactic correctness of each generated question. We apply
the tree kernel functions (Collins and Duffy 2001) and re-implement the syntactic tree
kernel model according to Moschitti et al. (2007) for computing the syntactic similarity
of each question with the associated content information. We rank the questions by
considering their topic relevance and syntactic correctness scores. Experimental results
show the effectiveness of our approach for automatically generating topical questions.
The remainder of the article is organized as follows. Section 2 describes the related work.
Section 3 presents the description of our QG system. Section 4 explains the experiments
and shows evaluation results; Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.992572">
Recently, question generation has received immense attention from researchers and
different methods have been proposed to accomplish the task in different relevant fields
(Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build
a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed
</bodyText>
<footnote confidence="0.9746386">
2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html.
3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide
more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report
new results.
4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research.
</footnote>
<page confidence="0.99498">
3
</page>
<note confidence="0.836878">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999759914893617">
a method to automatically generate questions based on question templates (which are
created from training on medical articles). Brown, Frishkoff, and Eskenazi (2005) de-
scribed an approach to automatically generate questions to assess the user’s vocabulary
knowledge. Chen, Aist, and Mostow (2009) developed a method to generate questions
automatically from informational text to mimic the reader’s self-questioning strategy
during reading. On the other hand, Agarwal, Shah, and Mannem (2011) considered
the question generation problem beyond the sentence level and designed an approach
that uses discourse connectives to generate questions from a given text. Several other
QG models have been proposed over the years that deal with transforming answers
to questions and utilizing question generation as an intermediate step in the question
answering process (Echihabi and Marcu 2003; Hickl et al. 2005). There are some other
researchers who have approached the task of generating questions for educational
purposes (Mitkov and Ha 2003; Heilman and Smith 2010b).
Question asking and QG are important components in advanced learning technolo-
gies such as intelligent tutoring systems and inquiry-based environments (Graesser
et al. 2001). A QG system is useful for building better question-asking facilities in in-
telligent tutoring systems. The Natural Language Processing (NLP), Natural Language
Generation, Intelligent Tutoring System, and Information Retrieval communities have
currently identified the Text-to-Question generation task as promising candidates for
shared tasks5 (Rus and Graesser 2009; Boyer and Piwek 2010). In the Text-to-Question
generation task, a QG system is given a text, and the goal is to generate a set of questions
for which the text contains answers. The task of generating a question about a given text
can be typically decomposed into three subtasks. First, given the source text, a content
selection step is necessary to select a target to ask about, such as the desired answer.
Second, given a target answer, an appropriate question type is selected (i.e., the form of
question to ask is determined). Third, given the content and question type, the actual
question is constructed. Based on this principle, several approaches have been described
in Boyer and Piwek (2010) that use named entity information, syntactic knowledge, and
semantic structures of the sentences to perform the task of generating questions from
sentences and paragraphs (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010).
Inspired by these works, we perform the task of topic-to-question generation using
named entity information and semantic structures of the sentences. A task that is similar
to ours is the task of keywords-to-question generation that has been addressed recently
in Zheng et al. (2011). They propose a user model for jointly generating keywords and
questions. However, their approach is based on generating question templates from
existing questions, which requires a large set of English questions as training data. In
recent years, some other related researchers have proposed the tasks of high-quality
question generation (Ignatova, Bernhard, and Gurevych 2008) and generating ques-
tions from queries (Lin 2008). Fact-based question generation has been accomplished
previously (Rus, Cai, and Graesser 2007; Heilman and Smith 2010b). We also focus on
generating fact-based questions in this research.
Besides grammaticality, an effective QG system should focus deeply on the im-
portance of the generated questions (Vanderwende 2008). This motivates the use of
a question-ranking module in a typical QG system. Over-generated questions can be
ranked using different approaches, such as statistical ranking methods, dependency
parsing, identification of the presence of pronouns and named entities, and topic scoring
(Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010; McConnell et al. 2011).
</bodyText>
<footnote confidence="0.778624">
5 http://www.questiongeneration.org/QGSTEC2010.
</footnote>
<page confidence="0.913212">
4
</page>
<note confidence="0.400477">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<bodyText confidence="0.999924977272727">
However, most of these automatic ranking approaches ignore the aspects of complex
paraphrasing by not considering lexical semantic variations (e.g., synonymy) when
measuring the importance of the questions. In our work, we use LDA (Blei, Ng, and
Jordan 2003) to identify the subtopics (which are closely related to the original topic) in
the given body of texts. We choose LDA because in recent years it has become one of the
most popular topic modeling techniques and has been shown to be effective in several
text-related tasks, such as document classification, information retrieval, and question
answering (Wei and Croft 2006; Misra, Capp´e, and Yvon 2008; Celikyilmaz, Hakkani-
Tur, and Tur 2010).
Once we have the subtopics, we apply ESSK (Hirao et al. 2003) to calculate their sim-
ilarity with the generated questions. The choice of ESSK is motivated by its successful
use in different NLP tasks in recent years (Chali, Hasan, and Joty 2009, 2011; Chali and
Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses
of each word to perform their summarization task. Their method is effective. However,
the fact that they do not disambiguate word senses cannot be disregarded. In our task,
we apply ESSK to calculate the similarity between important topics (discovered using
LDA) and the generated questions in order to measure the importance of each question.
We use disambiguated word senses for this purpose.
Syntactic information has previously been used successfully in question answering
(Zhang and Lee 2003; Moschitti and Basili 2006; Moschitti et al. 2007; Chali, Hasan, and
Joty 2009, 2011). Pasca and Harabagiu (2001) argued that with the syntactic form of
a sentence one can see which words depend on other words. We also feel that there
should be a similarity between the words that are dependent in the sentences present
in the associated body of texts and the dependency between words of the generated
question. This motivates us to propose the use of syntactic kernels in judging the
syntactic correctness of the generated questions automatically.
The main goal of our work is to generate as many questions as possible related to the
topic. We use the named entity information and the predicate argument structures of the
sentences to accomplish this goal. Our approach is different from the set-up in shared
tasks (Rus and Graesser 2009; Boyer and Piwek 2010), as we generate a set of basic
questions that are useful to add variety in the question space. A paragraph associated
with each topic is used as the source of relevant information about the topic. We evaluate
our systems in terms of topic relevance, which is different from prior research (Heilman
and Smith 2010a; Mannem, Prasad, and Joshi 2010). Syntactic correctness is also an
important property of a good question. For this reason, we evaluate our system in
terms of syntactic correctness as well. The proposed system will be useful for generating
topic-related questions from the associated content information, which can be used to
incorporate a “question suggestions for a certain topic” facility in search systems (Kotov
and Zhai 2010). For example, if a user searches for some information related to a certain
topic, the search system could generate all possible topic-relevant questions from a pre-
existent related body of texts to provide suggestions. Kotov and Zhai (2010) approached
a similar task by proposing a technique to augment the standard ranked list presenta-
tion of search results with a question-based interface to refine user-given queries.
The major contributions of our work can be summarized as follows:
</bodyText>
<listItem confidence="0.9775456">
• We perform the task of topic-to-question generation, which can help users
in expressing their information needs. Questions are generated using a
set of general-purpose rules based on named entity information and the
predicate argument structures of the sentences (along with semantic roles)
present in the associated body of texts.
</listItem>
<page confidence="0.773881">
5
</page>
<figure confidence="0.228035">
Computational Linguistics Volume 41, Number 1
</figure>
<listItem confidence="0.962817428571429">
• We identify the subtopics (which are closely related to the original topic) in
the given body of texts by using LDA and calculate their similarity with
the questions by applying ESSK (with disambiguated word senses). This
helps us to measure the importance of each question.
• We compute the syntactic similarity of each question with its associated
content information by applying the tree kernel functions with the
re-implementation of the syntactic tree kernel model. In this way, we judge
the syntactic correctness of each generated question automatically.
• We evaluate the ESSK similarity scores and the syntactic similarity scores
in a ranking framework and show that the use of ESSK and syntactic
kernels improve the relevance and the syntactic correctness of the
top-ranked questions, respectively.
• We identify circumstances in which our approach performs well and show
that, using additional experiments by narrowing down the topic focus.
</listItem>
<bodyText confidence="0.8677925">
Experiments with the topics about people (biographical focus) reveal
improvements in the overall results.
</bodyText>
<sectionHeader confidence="0.945057" genericHeader="method">
3. Topic-to-Question Generation
</sectionHeader>
<bodyText confidence="0.999974416666667">
Our QG approach mainly builds on four steps. In the first step, complex sentences
(from the given body of texts) related to a topic are simplified, as it is easier to
generate questions from simple sentences. In the next step, named entity information
and predicate argument structures of the sentences are extracted and are then used
to generate questions. In the third step, LDA is used to identify important subtopics
from the given body of texts, and then ESSK is applied to find their similarity with
the generated questions. In the final step, a syntactic tree kernel is used and syntactic
similarity between the generated questions and the sentences present in the body of
texts determines the syntactic correctness of the questions. Questions are then ranked
by considering the ESSK similarity scores and the syntactic similarity scores. We present
an architectural diagram (Figure 1) to show the different components of our system
and describe the overall procedure in the following subsections.
</bodyText>
<subsectionHeader confidence="0.999758">
3.1 Sentence Simplification
</subsectionHeader>
<bodyText confidence="0.970073545454545">
Sentences may have complex grammatical structure with multiple embedded clauses.
Therefore, the first step of our proposed system is to simplify the complex sentences
with the intention of generating more accurate questions. We use the simplified fac-
tual statement extractor model6 of Heilman and Smith (2010a). Their model extracts
the simpler forms of the complex source sentence by altering lexical items, syntactic
structure, and semantics, as well as by removing phrase types such as leading conjunc-
tions, sentence-level modifying phrases, and appositives. For example, given a complex
sentence s, we get the corresponding simple sentences as follows:
Complex Sentence (s): Apple’s first logo, designed by Jobs and Wayne, depicts Sir Isaac
Newton sitting under an apple tree.
Simple Sentence (1): Apple’s first logo is designed by Jobs and Wayne.
</bodyText>
<page confidence="0.77812">
6 Availableathttp://www.ark.cs.cmu.edu/mheilman/questions/.
6
</page>
<note confidence="0.754996">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<figureCaption confidence="0.983316">
Figure 1
</figureCaption>
<bodyText confidence="0.812242666666667">
Architectural diagram of our system.
Simple Sentence (2): Apple’s first logo depicts Sir Isaac Newton sitting under an
apple tree.
</bodyText>
<subsectionHeader confidence="0.999694">
3.2 Named Entity Information and Semantic Role Labeling for QG
</subsectionHeader>
<bodyText confidence="0.999910222222222">
In the second step of our system, we at first process the simple sentences in order to
generate all possible questions from them. We use the Illinois Named Entity Tagger,7 a
state-of-the-art named entity (NE) tagger that tags plain text with named entities (peo-
ple, organizations, locations, miscellaneous) (Ratinov and Roth 2009). Once we tag the
topic under consideration and its associated body of texts, we use some general purpose
rules to create some basic questions even though the answer is not present in the body
of texts. For example, Apple Inc. is tagged as an organization, so we generate a question:
Where is Apple Inc. located?. The main motivation behind generating such questions is
to add variety to the generated question space. The basic questions are useful when
there is very little or no knowledge available for a certain topic in consideration. This
assumption is inherited from the scenario in the real world where a human can ask
questions having very limited background knowledge about the topic. For example,
if a person knows nothing about a university, they can ask What is it? or, if they at
least know that a university is an institution, then they can ask the question, Where is
it located?. In Table 1, we show some example rules for the basic questions generated
in this work.
Our next task is to generate specific questions from the sentences present in the
given body of texts. For this purpose, we parse the sentences semantically using a
</bodyText>
<footnote confidence="0.694889">
7 Available athttp://cogcomp.cs.illinois.edu/.
</footnote>
<page confidence="0.997817">
7
</page>
<table confidence="0.360922">
Computational Linguistics Volume 41, Number 1
</table>
<tableCaption confidence="0.993858">
Table 1
</tableCaption>
<table confidence="0.683597166666667">
Example basic question rules.
Tag Example Question
person Who is person?
organization Where is organization located?
location Where is location?
misc. What do you know about misc.?
</table>
<bodyText confidence="0.999188391304348">
Semantic Role Labeling (SRL) system (Kingsbury and Palmer 2002; Hacioglu et al. 2003),
ASSERT.8 ASSERT is an automatic statistical semantic role tagger that can annotate
naturally occuring text with semantic arguments. When presented with a sentence, it
performs a full syntactic analysis of the sentence, automatically identifies all the verb
predicates in that sentence, extracts features for all constituents in the parse tree relative
to the predicate, and identifies and tags the constituents with the appropriate semantic
arguments. For example, the output of the SRL system for the sentence Apple’s first
logo is designed by Jobs and Wayne is: [ARG1 Apple ’s first logo] is [TARGET designed ]
[ARG0 by Jobs and Wayne]. The output contains one verb (predicate) with its arguments
(i.e., semantic roles). These arguments are used to generate specific questions from the
sentences. For example, we can replace [ARG1 ..] with What and generate a question as:
What is designed by Jobs and Wayne?. Similarly, [ARG0 ..] can be replaced and the question:
Who designed Apple’s first logo? can be generated. The semantic roles ARG0...ARG5 are
called mandatory arguments. There are some additional arguments or semantic roles
that can be tagged by ASSERT. They are called optional arguments and they start with
the prefix ARGM. These are defined by the annotation guidelines set in Palmer, Gildea,
and Kingsbury (2005). A set of about 350 general-purpose rules are used to transform
the semantic-role labeled sentences into the questions. The rules were set up in a way
that we could use the semantic role information to find the potential answer words in a
sentence that would be replaced by suitable question words. In the case of a mandatory
argument, the choice of question word depends on the argument’s named entity tag
(who for a person, where for a location, etc.). Table 2 shows how different semantic roles
can be replaced by possible question words in order to generate a question.
</bodyText>
<subsectionHeader confidence="0.999913">
3.3 Importance of Generated Questions
</subsectionHeader>
<bodyText confidence="0.984241625">
In the third step of our proposed system, we pass the generated questions to the
importance judgment module that uses LDA and ESSK to assign a topic relevance score
to each question. The detailed procedure is discussed in the following subsections.
3.3.1 Latent Dirichlet Allocation (LDA). To measure the importance of the generated
questions, we use LDA (Blei, Ng, and Jordan 2003) to identify the important subtopics9
from the given body of texts. LDA is a probabilistic topic modeling technique where
the main principle is to view each document as a mixture of various topics. Here each
topic is a probability distribution over words. LDA assumes that documents are made
</bodyText>
<footnote confidence="0.982014666666667">
8 Available at http://cemantix.org/assert.html.
9 The term sub-topic is used in the LDA topic modeling sense, which represents a probability distribution
over words.
</footnote>
<page confidence="0.944074">
8
</page>
<note confidence="0.607389">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<tableCaption confidence="0.903562">
Table 2
</tableCaption>
<figure confidence="0.587969727272727">
Semantic roles with possible question words.
Arguments Question Words
ARG0...ARG5 who, where, what, which
ARGM-ADV in what circumstances
ARGM-CAU why
ARGM-DIS how
AGRM-EXT to what extent
ARGM-LOC where
ARGM-MNR how
ARGM-PNC why
ARGM-TMP when
</figure>
<bodyText confidence="0.998956333333333">
up of words and word ordering is not important (“bag-of-words” assumption) (Misra,
Capp´e, and Yvon 2008). The main idea is to choose a distribution over topics while
generating a new document. For each word in the new document, a topic is randomly
chosen according to this distribution and a word is drawn from that topic. LDA uses
a generative topic modeling approach to specify the following distribution over words
within a document:
</bodyText>
<equation confidence="0.999095666666667">
K
P(wi) = E P(wi|zi = j)P(zi = j) (1)
j=1
</equation>
<bodyText confidence="0.97440585">
where K is the number of topics, P(wi|zi = j) is the probability of word wi under topic j,
and P(zi = j) is the sampling probability of topic j for the ith word. The multinomial
distributions 4)(j) = P(wlzi = j) and 0(d) = P(z) are termed as topic-word distribution
and document-topic distribution, respectively (Blei, Ng, and Jordan 2003). A Dirichlet
(α) prior is placed on 0 and a Dirichlet (0) prior is set on 4) to refine this basic model
(Griffiths and Steyvers 2002; Blei, Ng, and Jordan 2003). Now the main goal is to
estimate the two parameters: 0 and 4). We apply this framework directly to solve our
problem by considering each topic-related body of texts as a document. We use a GUI-
based toolkit for topic modeling10 that uses the popular MALLET (McCallum 2002)
toolkit for the back-end. The LDA model is built on the development set11 (Section 4.2).
The process starts by removing a list of “stop words” from the document and runs 200
iterations of Gibbs sampling (Geman and Geman 1984) to estimate the parameters 0
and 4). From each body of texts, we discover K topics and choose the most frequent
words from the most likely unigrams as the desired subtopics. For example, from the
associated body of texts of the topic Apple Inc. Logos, we get these subtopics: janoff,
themes, logo, color, apple.
3.3.2 Extended String Subsequence Kernel (ESSK). Once we identify the subtopics, we
apply ESSK (Hirao et al. 2003) to measure their similarity with the generated questions.
In the general ESSK, each word in a sentence is considered an “alphabet,” and the
alternative is all its possible senses. However, our ESSK implementation considers the
</bodyText>
<footnote confidence="0.343333">
10 Available at http://code.google.com/p/topic-modeling-tool/.
11 The model was built and tested according to the guidelines of the topic modeling toolkit we used.
</footnote>
<page confidence="0.956721">
9
</page>
<note confidence="0.330561">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999904909090909">
alternative of each word as its disambiguated sense. We use a dictionary-based Word
Sense Disambiguation (WSD) system (Chali and Joty 2007) assuming one sense per
discourse. We use WordNet (Fellbaum 1998) to find the semantic relations (such as
repetition, synonym, hypernym and hyponym, holonym and meronym, and gloss)
for all the words in a text. We assign a weight to each semantic relation based on
heuristics and use all of them. Our WSD technique is decomposed into two steps: (1)
building a representation of all possible senses of the words and (2) disambiguating
the words based on the highest score. To be specific, each candidate word from the
context is expanded to all of its senses. A disambiguation graph is constructed as the
intermediate representation where the nodes denote word instances with their WordNet
senses, and the weighted edges (connecting the senses of two different words) represent
semantic relations. This graph is exploited to perform the WSD. We sum the weights of
all edges, leaving the nodes under their different senses. The sense with the highest
score is considered to be the most probable sense. In case of a tie between two or
more senses, we select the sense that comes first in WordNet, because WordNet orders
the senses of a word by decreasing order of their frequency. Our preliminary experi-
ments suggested that WSD has a positive impact on the performance of our proposed
system.
ESSK is used to measure the similarity between all possible subsequences of
the question words/senses and topic words/senses. We calculate the similarity score
Sim(Ti, Qj) using ESSK, where Ti denotes a topic/sub-topic word sequence and Qj
stands for a generated question. Formally, ESSK is defined as follows:12
</bodyText>
<equation confidence="0.975955833333333">
Kessk(T, Q) = d E E Km(ti, qj)
E tiET qjEQ
m=1
� val(ti, qj) if m = 1
Km(ti, qj) =
K�m−1(ti, qj) · val(ti, qj)
</equation>
<bodyText confidence="0.921391833333333">
Here, K&apos;m(ti, qj) is defined in the following. ti and qj are nodes of T and Q, respectively.
The function val(t, q) returns the number of common attributes (i.e., the number of
common words/senses) to the given nodes t and q.
� 0 if j = 1
Km (ti, qj) = λK; m(ti, qj−1) + Km(ti, qj−1)
Here, λ is the decay parameter for the number of skipped words. K~~m(ti, qj) is
defined as:
� 0 if i = 1
Km(ti, qj) = λK~~m(ti−1, qj) + Km(ti−1, qj)
12 The formulae denote a dynamic programming technique to compute the ESSK similarity score
where d is the vector space dimension (i.e., the number of all possible subsequences of up to
length d). More information about these formulae can be obtained from Hirao et al. (2003, 2004).
</bodyText>
<page confidence="0.990881">
10
</page>
<note confidence="0.860674">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<bodyText confidence="0.94643">
Finally, the similarity measure is defined after normalization:
</bodyText>
<equation confidence="0.977491666666667">
Kessk(T, Q)
siMessk(T, Q) =
\,IKessk(T, T)Kessk(Q, Q)
</equation>
<subsectionHeader confidence="0.993934">
3.4 Judging Syntactic Correctness
</subsectionHeader>
<bodyText confidence="0.999967391304348">
The next step of our system is to judge the syntactic correctness of the generated
questions. The generated questions might be syntactically incorrect due to the pro-
cess of automatic question generation. It is time-consuming and considerable human
intervention is necessary to check for the syntactically incorrect questions manually. We
strongly believe that a question should have a similar syntactic structure to a sentence
from which it is generated. For example, the sentence Apple’s first logo is designed by
Jobs and Wayne., and the generated question What is designed by Jobs and Wayne? are
syntactically similar. An example of an ungrammatical generated question that is not
very similar to its source is: Janoff presented Jobs What?. To judge the syntactic cor-
rectness of each generated question automatically, we apply the tree kernel functions
and re-implement the syntactic tree kernel model according to Moschitti et al. (2007)
for computing the syntactic similarity of each question with the associated content
information. We first parse the sentences and the questions into syntactic trees using the
Charniak parser13 (Charniak 1999). Then, we calculate the similarity between the two
corresponding trees using the tree kernel method (Collins and Duffy 2001). We convert
each parenthetic representation generated by the Charniak parser into its corresponding
tree and give the trees as input to the tree kernel functions for measuring the syntactic
similarity.
The tree kernel function computes the number of common subtrees between two
trees and gives the similarity score between each sentence in the given body of texts and
the generated question based on the syntactic structure. Each sentence14 contributes a
score to the questions and then the questions are ranked by considering the average of
similarity scores.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="method">
4. Experiments
</sectionHeader>
<subsectionHeader confidence="0.975688">
4.1 System Description
</subsectionHeader>
<bodyText confidence="0.9998932">
We consider the task of automatically generating questions from topics where each
topic is associated with a body of texts having a useful description about the topic.
The question-ranking module of the proposed QG system ranks the questions by com-
bining the topic relevance scores and the syntactic similarity scores of Section 3.3 and
Section 3.4 using the following formula:
</bodyText>
<equation confidence="0.645308">
w * ESSKscore + (1 − w) * SYNscore (2)
</equation>
<footnote confidence="0.888815333333333">
13 Availableathttps://github.com/BLLIP/bllip-parser.
14 We consider that a question is syntactically fluent as well as relevant to the topic if it has similar syntactic
subtrees to those of the most sentences in the body of texts.
</footnote>
<page confidence="0.993892">
11
</page>
<note confidence="0.557651">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999617">
Here, w is the importance parameter, which holds a value in [0, 1]. We kept w = 0.5 to
give equal importance15 to topic relevance and syntactic correctness.
</bodyText>
<subsectionHeader confidence="0.980118">
4.2 Corpus
</subsectionHeader>
<bodyText confidence="0.999983272727273">
To run our experiments, we use the data set provided in the Question Generation
Shared Task and Evaluation Challenge16 (2010) for the task of question generation
from paragraphs. This data set consists of 60 paragraphs about 60 topics that were
originally collected from several Wikipedia, OpenLearn, and Yahoo!Answers articles.
The paragraphs contain around 5–7 sentences for a total of 100–200 tokens (including
punctuation). This data set includes a diversity of topics of general interest. We consider
these topics and treat the paragraphs as their associated useful content information
in order to generate a set of questions using our proposed QG approach. We ran-
domly select 10 topics and their associated paragraphs as the development data.17
A total of 2,186 questions are generated from the remaining 50 topics (test data) to
be ranked.
</bodyText>
<subsectionHeader confidence="0.992882">
4.3 Evaluation Set-up
</subsectionHeader>
<bodyText confidence="0.995467083333333">
4.3.1 Methodology. We use a methodology derived from Boyer and Piwek (2010)
and Heilman and Smith (2010b) to evaluate the performance of our QG systems.
Three native English-speaking university graduate students judge the quality of the
top-ranked 20% questions using two criteria: topic relevance and syntactic correctness.
For topic relevance, the given score is an integer between 1 (very poor) and 5 (very
good) and is guided by the consideration of the following aspects: 1. Semantic
correctness (i.e., the question is meaningful and related to the topic), 2. Correctness of
question type (i.e., a correct question word is used), and 3. Referential clarity (i.e., it is
clearly possible to understand what the question refers to). For syntactic correctness,
the assigned score is also an integer between 1(very poor) and 5 (very good). Whether a
question is grammatically correct or not is checked here. The judges were asked to read
the topics with their associated body of texts and then rate the top-ranked questions
generated by different systems. For each question, we calculate the average of the
judges’ scores. The judges were provided with an annotation guideline and sample
judgments, according to the methodology derived from Boyer and Piwek (2010) and
Heilman and Smith (2010b). The same judges evaluated all the system outputs and
they were blind to the system identity when judging. No guidelines were provided on
the relative importance of the various aspects that made the judgment task subjective.
The inter-annotator agreement of Fleiss’s K = 0.41, 0.45, 0.62, and 0.33 are computed
for the three judges for the results in Tables 3–6, indicating moderate (for the first
two tables), and substantial and fair agreement (Landis and Koch 1977) between the
raters, respectively. These K values were shown to be acceptable in the literature for
the relevant NLP tasks (Dolan and Brockett 2005; Glickman, Dagan, and Koppel 2005;
Heilman and Smith 2010b).
</bodyText>
<footnote confidence="0.9147372">
15 A syntactically incorrect question is not useful even if it is relevant to the topic. This motivated us to
give equal importance to topic relevance and syntactic correctness. The parameter w can be tuned to
investigate its impact on the system performance.
16 http://www.questiongeneration.org/mediawiki.
17 We use these data to build necessary general purpose rules for our QG model.
</footnote>
<page confidence="0.988231">
12
</page>
<note confidence="0.840671">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<tableCaption confidence="0.996682">
Table 3
</tableCaption>
<table confidence="0.993654666666667">
Topic relevance and syntactic correctness scores.
Systems Topic Relevance Syntactic Correctness
Baseline1 (No Ranking) 2.15 2.63
Baseline2 (Topic Signature) 3.24 3.30
State-of-the-art (Heilman and Smith 2010b) 3.35 3.45
Proposed QG System 3.48 3.55
</table>
<listItem confidence="0.790706">
4.3.2 Systems for Comparison. We report the performance of the following systems in
order to do a meaningful comparison with our proposed QG system:
(1) Baseline1: This is our QG system without any question-ranking method applied to
it. Here, we randomly select top 20% questions and rate them.
(2) Baseline2: For our second baseline, we build a QG system using an alternative topic
modeling approach. Here, we use a topic signature model (instead of using LDA as
discussed in Section 3.3.1) (Lin and Hovy 2000) to identify the important subtopics from
</listItem>
<bodyText confidence="0.962334222222222">
the sentences present in the body of texts. The subtopics are the important words in the
context that are closely related to the topic and have significantly greater probability of
occurring in the given text compared with a large background corpus. We use a topic
signature computation tool18 for this purpose. The background corpus that is used in
this tool contains 5,000 documents from the English GigaWord Corpus. For example,
from the given body of texts of the topic Apple Inc. Logos, we get these subtopics: jobs,
logo, themes, rainbow, monochromatic. Then we use the same steps of Sections 3.3.2 and
3.4, and use Equation (2) to combine the scores. We evaluate the top-ranked 20%
questions and show the results.
</bodyText>
<listItem confidence="0.908254">
(3) State-of-the-art: We choose a publicly available state-of-the-art QG system19 to
generate questions from the sentences in the body of texts. This system was shown
</listItem>
<bodyText confidence="0.9280268125">
to achieve good performance in generating fact-based questions about the content of a
given article (Heilman and Smith 2010b). Their method ranks the questions automat-
ically using a logistic regression model. Given a paragraph as input, this system pro-
cesses each sentence and generates a set of ranked questions for the entire paragraph.
We evaluate the top-ranked 20% questions20 and report the results.
4.3.3 Results and Discussion. Table 3 shows the average topic relevance and syntactic
correctness scores for all the systems. From these results, we can see that the proposed QG
system improves the topic relevance and syntactic correctness scores over the Baseline1
system by 62% and 35%, respectively, and improves the topic relevance and syntactic
correctness scores over the Baseline2 system by 7%, and 8%, respectively. On the other
hand, the proposed QG system improves the topic relevance and syntactic correctness
scores over the state-of-the-art system by 4% and 3%, respectively. From these results, we
can clearly observe the effectiveness of our proposed QG system. The improvements in
the results are statistically significant21 (p &lt; 0.05).
The main goal of this work was to generate as many questions as possible related to
the topic. For this reason, we considered generating the basic questions. These questions
</bodyText>
<footnote confidence="0.94433125">
18 Available at http://www.cis.upenn.edu/∼lannie/topicS.html.
19 Available athttp://www.ark.cs.cmu.edu/mheilman/questions/.
20 We ignore the yes-no questions for our task.
21 We tested statistical significance using Student’s t test.
</footnote>
<page confidence="0.998348">
13
</page>
<tableCaption confidence="0.749979">
Computational Linguistics Volume 41, Number 1
Table 4
</tableCaption>
<table confidence="0.972941333333333">
Acceptability of the questions (in %).
Systems Top 15% Top 30%
Baseline1 (No Ranking) 35.2 32.6
Baseline2 (Topic Signature) 45.9 33.8
State-of-the-art (Heilman and Smith 2010b) 44.7 38.5
Proposed QG System 46.5 40.6
</table>
<tableCaption confidence="0.9968">
Table 5
</tableCaption>
<table confidence="0.974291666666667">
Topic relevance and syntactic correctness scores (narrowed focus).
Systems Topic Relevance Syntactic Correctness
Baseline1 (No Ranking) 2.84 2.75
Baseline2 (Topic Signature) 3.50 3.42
State-of-the-art (Heilman and Smith 2010b) 3.63 3.56
Proposed QG System 3.78 3.72
</table>
<bodyText confidence="0.999809583333333">
were also useful to provide variety in the question space. We generated these ques-
tions using the named entity information. As the performance of the NE taggers were
unsatisfactory, we had a few of these questions generated. In most cases, these questions
were outranked by other important questions, which included a combination of topics
and subtopics to show higher topic relevance score measured by ESSK. Therefore, they
do not have a considerable impact on the evaluation statistics. We claim that the overall
performance of our systems could be further improved if the accuracy of the NE tagger
and the semantic role labeler could be increased.
Acceptability Test. In another evaluation setting, the three annotators judge the questions
for their overall acceptability as a good question. If a question shows no deficiency
in terms of the criteria considered for topic relevance and syntactic correctness, it is
termed as acceptable. We evaluate the top 15% and top 30% questions separately for
each QG system and report the results indicating the percentage of questions rated as
acceptable in Table 4. The results indicate that the percentage of the questions rated
acceptable is reduced when we evaluate a greater number of questions—which proves
the effectiveness of our QG system.
Narrowing Down the Focus. We run further experiments by narrowing down the topic fo-
cus. We consider only the topics about people (biographical focus). We choose 50 people
as our topics from the list of the 20th century’s 100 most influential people, published
in Time magazine in 1999 and obtained the paragraphs containing their biographical
information from Wikipedia articles.22 We generate a total of 1, 845 questions from
the 50 topics considered and rank them using different ranking schemes as discussed
before. We evaluate the top 20% questions using the similar evaluation methodologies
and report the results in Table 5. From these results, we can see that the proposed QG
</bodyText>
<footnote confidence="0.487315">
22 http://en.wikipedia.org/wiki/Time 100.
</footnote>
<page confidence="0.998507">
14
</page>
<note confidence="0.893978">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<tableCaption confidence="0.993211">
Table 6
</tableCaption>
<table confidence="0.947813166666667">
Acceptability of the questions in % (narrowed focus).
Systems Top 15% Top 30%
Baseline1 (No Ranking) 38.6 31.5
Baseline2 (Topic Signature) 47.1 35.5
State-of-the-art (Heilman and Smith 2010b) 52.4 40.2
Proposed QG System 55.8 42.0
</table>
<bodyText confidence="0.943138162162162">
system improves the topic relevance and syntactic correctness scores over the Baseline1
system by 33% and 35%, respectively, and improves the topic relevance and syntactic
correctness scores over the Baseline2 system by 8% and 9%, respectively. Moreover,
the proposed QG system improves both the topic relevance and syntactic correctness
scores over the state-of-the-art system by 4%. From these results, we can clearly observe
the effectiveness of our proposed QG system when we narrow down the topic focus.
We also evaluate the top 15% and top 30% questions separately for each QG system
and report the results, indicating the percentage of questions rated as acceptable in
Table 6. From these tables, we can clearly see the improvements in all the scores for
all the QG approaches. This is reasonable because the accuracy of the NE tagger and
the semantic role labeler is increased for the biographical data.23 These results further
demonstrate that the proposed system is significantly better (at p &lt; 0.05) than the
other considered systems. We plan to make our created resources available to other
researchers.
4.3.4 An Input-Output Example. An input to our systems is, for instance,24 the topic Apple
Inc. Logos with the associated content information (body of texts):
Apple’s first logo, designed by Jobs and Wayne, depicts Sir Isaac Newton sitting under
an apple tree. Almost immediately, though, this was replaced by Rob Janoff’s “rainbow
Apple”, the now-familiar rainbow-colored silhouette of an apple with a bite taken
out of it. Janoff presented Jobs with several different monochromatic themes for the
“bitten” logo, and Jobs immediately took a liking to it. While Jobs liked the logo, he
insisted it be in color to humanize the company. The Apple logo was designed with a
bite so that it would be recognized as an apple rather than a cherry. The colored stripes
were conceived to make the logo more accessible, and to represent the fact the monitor
could reproduce images in color. In 1998, with the roll-out of the new iMac, Apple
discontinued the rainbow theme and began to use monochromatic themes, nearly
identical in shape to its previous rainbow incarnation.
The output of our systems is the ranked lists of questions. We show an example output
in Table 7. To provide a more detailed analysis of our results, the average output scores
of the example questions are presented in Table 8. From this table, we can understand
how different aspects of the evaluation criteria affected the performance of the different
systems. For example, Q1 of the proposed system was given a very good score due to
23 Although a few basic questions were generated compared with other important questions containing
topical words, we believe they did not have a considerable impact on the overall performance of our
system.
24 The example input text is provided from the Question Generation Shared Task and Evaluation Challenge
(QGSTEC 2010) data set that we used for our experiments (Section 4.2).
</bodyText>
<page confidence="0.984167">
15
</page>
<note confidence="0.463926">
Computational Linguistics Volume 41, Number 1
</note>
<tableCaption confidence="0.603458470588235">
Table 7
System output.
Systems Top-ranked questions
Baseline2 Q1: Who presented Jobs with several different monochromatic
themes for the bitten logo?
Q2: What were conceived to make the logo more accessible?
Q3: Who liked the logo?
State-of-the-art Q1: Whose first logo depicts Sir Isaac Newton sitting under an
apple tree?
Q2: What depicts Sir Isaac Newton sitting under an apple tree?
Q3: What did Janoff present Jobs with?
Proposed QG System Q1: Who designed Apple’s first logo?
Q2: What was replaced by Rob Janoff’s “rainbow Apple”?
Q3: What were conceived to make the logo more accessible?
Table 8
Judgment scores associated with example questions.
Systems Question Average score
</tableCaption>
<equation confidence="0.976447444444445">
Baseline2 Q1 3.65
Q2 3.42
Q3 3.38
State-of-the-art Q1 3.68
Q2 3.63
Q3 3.25
Proposed QG System Q1 4.34
Q2 3.50
Q3 3.42
</equation>
<bodyText confidence="0.9995795">
its relevance to the topic in consideration. On the other hand, Q3 of the state-of-the-art
was assigned a lower score due to its lack of clarity with respect to the topic.
</bodyText>
<sectionHeader confidence="0.997094" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.999975066666667">
In this article, we have considered the task of automatically generating questions from
topics where each topic is associated with a body of texts containing useful information.
The proposed method exploits named entity and semantic role labeling information
to accomplish the task. A key aspect of our approach was the use of latent Dirichlet
allocation (LDA) to automatically discover the hidden subtopics from the sentences.
We have proposed a novel method to rank the generated questions by considering: (1)
subtopical similarity determined using ESSK algorithm in combination with word sense
disambiguation, and (2) syntactic similarity determined using the syntactic tree kernel
based method. We have compared the proposed question generation (QG) system with
two baseline systems and one state-of-the-art system. The evaluation results show that
the proposed QG system significantly outperforms all other considered systems, as our
top-ranked system generated questions were found to be better in topic-relevance and
syntactic correctness than those of the other systems. Our results demonstrated that
judging syntactic correctness of the generated questions using the syntactic tree kernel
based model was suitable in our question generation setting. We would like to further
</bodyText>
<page confidence="0.996026">
16
</page>
<note confidence="0.914991">
Chali and Hasan Towards Topic-to-Question Generation
</note>
<bodyText confidence="0.999234444444444">
our research by using other available measures such as an n-gram language model
or a parser confidence score (Wagner, Foster, and van Genabith 2009) in order to see
how they would perform on the same task. In this article, we have also extended our
experiments by narrowing down the topic focus. In this experiment, we have considered
people as topics. A rigorous analysis of the evaluation results has revealed that the
performance of our proposed QG system can be enhanced if we narrow down the topic
focus. We hope to carry on these ideas and develop further mechanisms for question
generation based on the dependency features of the answers and answer finding (Li
and Roth 2006; Pinchak and Lin 2006).
</bodyText>
<sectionHeader confidence="0.99435" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982716">
We would like to thank the anonymous
reviewers for their useful comments.
The research reported in this article was
supported by the Natural Sciences and
Engineering Research Council (NSERC)
of Canada – discovery grant and the
University of Lethbridge. This work
was done when the second author was
at the University of Lethbridge.
</bodyText>
<sectionHeader confidence="0.993536" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.924994666666666">
Agarwal, M., R. Shah, and P. Mannem.
2011. Automatic question generation
using discourse cues. In Proceedings of the
6th Workshop on Innovative Use of NLP
for Building Educational Applications,
pages 1–9, Portland, OR.
Ali, H., Y. Chali, and S. A. Hasan. 2010.
Automation of question generation from
sentences. In Proceedings of QG2010: The
Third Workshop on Question Generation,
pages 58–67, Pittsburgh, PA.
Andrenucci, A. and E. Sneiders. 2005.
Automated question answering: Review of
the main approaches. In Proceedings of the
3rd International Conference on Information
Technology and Applications (ICITA’05),
pages 514–519, Sydney.
Blei, D. M., A. Y. Ng, and M. I. Jordan.
2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993–1022.
Boyer, K. E. and P. Piwek, eds. 2010.
Proceedings of QG2010: The Third Workshop
on Question Generation. Pittsburgh, PA:
questiongeneration.org.
Brown, J. C., G. A. Frishkoff, and
M. Eskenazi. 2005. Automatic question
generation for vocabulary assessment.
In Proceedings of the Conference on Human
Language Technology and Empirical
Methods in Natural Language Processing,
pages 819–826, Vancouver.
Celikyilmaz, A., D. Hakkani-Tur, and G. Tur.
2010. LDA based similarity modeling for
question answering. In Proceedings of the
NAACL HLT 2010 Workshop on Semantic
Search, SS ’10, pages 1–9, Los Angeles, CA.
Chali, Y. and S. A. Hasan. 2012a. On
the effectiveness of using sentence
compression models for query-focused
multi-document summarization.
In Proceedings of the 24th International
Conference on Computational Linguistics
(COLING 2012), pages 457–474, Mumbai.
Chali, Y. and S. A. Hasan. 2012b.
Query-focused multi-document
summarization: Automatic data
annotations and supervised learning
approaches. Journal of Natural Language
Engineering, 18(1):109–145.
Chali, Y. and S. A. Hasan. 2012c. Towards
automatic topical question generation.
In Proceedings of the 24th International
Conference on Computational Linguistics
(COLING 2012), pages 475–492, Mumbai.
Chali, Y., S. A. Hasan, and K. Imam. 2011.
An aspect-driven random walk model
for topic-focused multi-document
summarization. In Proceedings of the
7th Asian Information Retrieval Societies
Conference (AIRS 2011), pages 386–397,
Dubai.
Chali, Y., S. A. Hasan, and K. Imam. 2012.
Learning good decompositions of
complex questions. In Proceedings of
the 17th International Conference on
Applications of Natural Language Processing
to Information Systems (NLDB 2012),
pages 104–115, Groningen.
Chali, Y., S. A. Hasan, and S. R. Joty. 2009.
Do automatic annotation techniques
have any impact on supervised complex
question answering? In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the Association for Computational
Linguistics (ACL-IJCNLP 2009),
pages 329–332, Suntec.
Chali, Y., S. A. Hasan, and S. R. Joty. 2011.
Improving graph-based random walks
for complex question answering using
syntactic, shallow semantic and
</reference>
<page confidence="0.994681">
17
</page>
<note confidence="0.428839">
Computational Linguistics Volume 41, Number 1
</note>
<reference confidence="0.989971576271186">
extended string subsequence kernels.
Information Processing &amp; Management,
47(6):843–855.
Chali, Y. and S. R. Joty. 2007. Word sense
disambiguation using lexical cohesion.
In Proceedings of the 4th International
Conference on Semantic Evaluations,
pages 476–479, Prague.
Chali, Y., S. R. Joty, and S. A. Hasan.
2009. Complex question answering:
Unsupervised learning approaches
and experiments. Journal of Artificial
Intelligence Research, 35:1–47.
Charniak, E. 1999. A maximum-entropy-
inspired parser. Technical Report CS-99-12,
Brown University, Computer Science
Department, Providence, RI.
Chen, W., G. Aist, and J. Mostow. 2009.
Generating questions automatically from
informational text. In Proceedings of the
2nd Workshop on Question Generation
(AIED 2009), pages 17–24, Arlington, VA.
Collins, M. and N. Duffy. 2001. Convolution
Kernels for natural language. In
Proceedings of Neural Information Processing
Systems, pages 625–632, Vancouver.
Dolan, W. B. and C. Brockett. 2005.
Automatically constructing a corpus of
sentential paraphrases. In Proceedings of the
3rd International Workshop on Paraphrasing
(IWP2005), pages 9–16, Jeju Island.
Echihabi, A. and D. Marcu. 2003. A
noisy-channel approach to question
answering. In Proceedings of the 41st Annual
Meeting on Association for Computational
Linguistics - Volume 1, pages 16–23,
Sapporo.
Fellbaum, C. 1998. WordNet - An Electronic
Lexical Database. MIT Press.
Geman, S. and D. Geman. 1984. Stochastic
relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and
Machine Intelligence, 6:721–741.
Glickman, O., I. Dagan, and M. Koppel.
2005. A probabilistic classification
approach for lexical textual entailment. In
AAAI, pages 1,050–1,055, Pittsburgh, PA.
Graesser, A. C., K. VanLehn, C. P. Rose, P. W.
Jordan, and D. Harter. 2001. Intelligent
tutoring systems with conversational
dialogue. AI Magazine, 22(4):39–52.
Greenwood, M. A. 2005. Open-Domain
Question Answering. Ph.D. thesis,
Department of Computer Science,
University of Sheffield.
Griffiths, T. L. and M. Steyvers. 2002.
Prediction and semantic association.
In NIPS’02, pages 11–18, Cambridge, MA.
Hacioglu, K., S. Pradhan, W. Ward,
J. H. Martin, and D. Jurafsky. 2003.
Shallow semantic parsing using support
vector machines. In Technical Report
TR-CSLR-2003-03, University of Colorado,
Boulder.
Harabagiu, S., F. Lacatusu, and A. Hickl.
2006. Answering complex questions with
random walk models. In Proceedings of
the 29th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 220–227,
Seattle, WA.
Heilman, M. and N. A. Smith. 2010a.
Extracting simplified statements
for factual question generation. In
Proceedings of the Third Workshop on
Question Generation, pages 11–20,
Pittsburgh, PA.
Heilman, M. and N. A. Smith. 2010b.
Good question! Statistical ranking for
question generation. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 609–617, Los Angeles, CA.
Hickl, A., J. Lehmann, D. Moldovan, and
S. Harabagiu. 2005. Experiments with
interactive question-answering. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL’05), pages 205–214,
Ann Arbor, MI.
Hickl, A., P. Wang, J. Lehmann, and Sanda
Harabagiu. 2006. Ferret: Interactive
question-answering for real-world
environments. In Proceedings of the
COLING/ACL on Interactive Presentation
Sessions, pages 25–28, Sydney.
Hirao, T., J. Suzuki, H. Isozaki, and
E. Maeda. 2003. NTT’s multiple document
summarization system for DUC2003. In
Proceedings of the Document Understanding
Conference, Edmonton.
Hirao, T., J. Suzuki, H. Isozaki, and
E. Maeda. 2004. Dependency-based
sentence alignment for multiple document
summarization. In Proceedings of COLING
2004, pages 446–452, Geneva.
Hirschman, L. and R. Gaizauskas. 2001.
Natural language question answering:
The view from here. Natural Language
Engineering, 7(4):275–300.
Ignatova, K., D. Bernhard, and I. Gurevych.
2008. Generating high quality questions
from low quality questions. In Proceedings
of the Workshop on the Question Generation
Shared Task and Evaluation Challenge,
Arlington, VA.
</reference>
<page confidence="0.998755">
18
</page>
<note confidence="0.662918333333333">
Chali and Hasan Towards Topic-to-Question Generation
Kingsbury, P. and M. Palmer. 2002. From
Treebank to PropBank. In Proceedings of
the International Conference on Language
Resources and Evaluation, pages 1,989–1,993,
Las Palmas.
</note>
<reference confidence="0.998447371681416">
Kotov, A. and C. Zhai. 2010. Towards natural
question guided search. In Proceedings of
the 19th International Conference on the
World Wide Web, WWW ’10, pages 541–550,
Raleigh, NC.
Kupiec, J. 1993. MURAX: A robust linguistic
approach for question answering using
an on-line encyclopedia. In SIGIR,
pages 181–190, Pittsburgh, PA.
Landis, J. R. and G. G. Koch. 1977. The
measurement of observer agreement for
categorical data. Biometrics, 33(1):159–174.
Lauer, T. W., E. Peacock, and A. C. Graesser,
eds. 1992. Questions and Information
Systems. Erlbaum, Hillsdale, NJ.
Li, X. and D. Roth. 2006. Learning question
classifiers: The role of semantic
information. Journal of Natural Language
Engineering, 12(3):229–249.
Lin, C. Y. 2008. Automatic question
generation from queries. In Proceedings of
the Workshop on the Question Generation
Shared Task and Evaluation Challenge,
Arlington, VA.
Lin, C. Y. and E. H. Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings of
the 18th Conference on Computational
Linguistics, pages 495–501, Saarbr¨uken.
Mannem, P., R. Prasad, and A. Joshi. 2010.
Question generation from paragraphs
at UPenn: QGSTEC system description.
In Proceedings of the Third Workshop
on Question Generation, pages 84–91,
Pittsburgh, PA.
McCallum, A. K. 2002. MALLET: A
machine learning for language toolkit.
http://mallet.cs.umass.edu.
McConnell, C. C., P. Mannem, R. Prasad,
and A. Joshi. 2011. A new approach to
ranking over-generated questions. In
Proceedings of the AAAI Fall Symposium
on Question Generation, pages 45–48,
Arlington, VA.
McGough, J., J. Mortensen, J. Johnson,
and S. Fadali. 2001. A Web-based
testing system with dynamic question
generation. In ASEE/IEEE Frontiers in
Education Conference, pages S3C-23–28
(vol. 3), Reno, NV.
Misra, H., O. Capp´e, and F. Yvon. 2008.
Using LDA to detect semantically
incoherent documents. In Proceedings of
the Twelfth Conference on Computational
Natural Language Learning, CoNLL ’08,
pages 41–48, Manchester.
Mitkov, R. and L. A. Ha. 2003.
Computer-aided generation of
multiple-choice tests. In Proceedings of
the HLT-NAACL 03 Workshop on Building
Educational Applications Using Natural
Language Processing - Volume 2,
pages 17–22, Edmonton.
Moldovan, D., C. Clark, and M. Bowden.
2007. Lymba’s PowerAnswer 4 in TREC
2007. In Proceedings of the 16th Text
REtreival Conference, Gaithersburg, MD.
Moschitti, A. and R. Basili. 2006. A tree
kernel approach to question and
answer classification in question
answering systems. In Proceedings of
the 5th International Conference on
Language Resources and Evaluation,
pages 1,510–1,513, Genoa.
Moschitti, A., S. Quarteroni, R. Basili,
and S. Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels
for question/answer classification.
In Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 776–783, Prague.
Olney, A. M., A. C. Graesser, and N. K.
Person. 2012. Question generation from
concept maps. Dialogue and Discourse,
3(2):75–99.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The proposition bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71–106.
Pasca, M. and S. M. Harabagiu. 2001.
Answer mining from on-line documents.
In Proceedings of the Association for
Computational Linguistics 39th Annual
Meeting and 10th Conference of the European
Chapter Workshop on Open-Domain Question
Answering, pages 38–45, Toulouse.
Pinchak, C. and D. Lin. 2006. A probabilistic
answer type model. In Proceedings of the
11th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 393–400, Trento.
Ratinov, L. and D. Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning, pages 147–155,
Boulder, CO.
Rus, V., Z. Cai, and A. C. Graesser. 2007.
Experiments on generating questions
about facts. In Proceedings of the 8th
International Conference on Computational
Linguistics and Intelligent Text Processing,
pages 444–455, Mexico City.
</reference>
<page confidence="0.960945">
19
</page>
<reference confidence="0.989021581818182">
Computational Linguistics Volume 41, Number 1
Rus, V. and A. C. Graesser. 2009. The
question generation shared task and
evaluation challenge. In Workshop on
the Question Generation Shared Task and
Evaluation Challenge, Final Report,
pages 1–37, University of Memphis.
Simmons, R. F. 1965. Answering English
questions by computer: A survey.
Communications of the ACM, 8(1):53–70.
Strzalkowski, T. and S. Harabagiu, 2008.
Advances in Open Domain Question
Answering. Springer.
Vanderwende, L. 2008. The importance of
being important: Question generation.
In Proceedings of the Workshop on the
Question Generation Shared Task and
Evaluation Challenge, Arlington, VA.
Voorhees, E. M. 1999. Overview of the
TREC 1999 question answering track.
In Proceedings of the 8th Text REtreival
Conference, Gaithersburg, MD.
Wagner, J., J. Foster, and J. van Genabith.
2009. Judging grammaticality:
Experiments in sentence classification.
CALICO Journal, 26(3):474–490.
Wang, M. 2006. A survey of answer
extraction techniques in factoid
question answering. In CMU 11-762
Language and Statistics II, literature
review project.
Wang, W., H. Tianyong, and L. Wenyin.
2008. Automatic question generation
for learning evaluation in medicine.
In 6th International Conference on
Advances in Web Based Learning,
pages 242–251, Edinburgh.
Wei, X. and W. B. Croft. 2006. LDA-based
document models for ad-hoc retrieval. In
Proceedings of the 29th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval,
SIGIR ’06, pages 178–185, Seattle, WA.
Zhang, A. and W. Lee. 2003. Question
classification using support vector
machines. In Proceedings of the Special
Interest Group on Information Retrieval,
pages 26–32, Toronto.
Zheng, Z., X. Si, E. Y. Chang, and X. Zhu.
2011. K2Q: Generating natural language
questions from keywords with user
refinements. In Proceedings of the 5th
International Joint Conference on Natural
Language Processing, pages 947–955,
Chiang Mai.
</reference>
<page confidence="0.994833">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.442162">
<title confidence="0.91589">Towards Topic-to-Question Generation</title>
<affiliation confidence="0.685929">University of Lethbridge</affiliation>
<abstract confidence="0.952922066666667">A. Philips Research North America This paper is concerned with automatic generation of all possible questions from a topic of interest. Specifically, we consider that each topic is associated with a body of texts containing useful information about the topic. Then, questions are generated by exploiting the named entity information and the predicate argument structures of the sentences present in the body of texts. The importance of the generated questions is measured using Latent Dirichlet Allocation by identifying the subtopics (which are closely related to the original topic) in the given body of texts and applying the Extended String Subsequence Kernel to calculate their similarity with the questions. We also propose the use of syntactic tree kernels for the automatic judgment of the syntactic correctness of the questions. The questions are ranked by considering both their importance (in the context of the given body of texts) and syntactic correctness. To the best of our knowledge, no previous study has accomplished this task in our setting. A series of experiments demonstrate that the proposed topic-to-question generation approach can significantly outperform the state-of-the-art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Agarwal</author>
<author>R Shah</author>
<author>P Mannem</author>
</authors>
<title>Automatic question generation using discourse cues.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>1--9</pages>
<location>Portland, OR.</location>
<marker>Agarwal, Shah, Mannem, 2011</marker>
<rawString>Agarwal, M., R. Shah, and P. Mannem. 2011. Automatic question generation using discourse cues. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–9, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ali</author>
<author>Y Chali</author>
<author>S A Hasan</author>
</authors>
<title>Automation of question generation from sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of QG2010: The Third Workshop on Question Generation,</booktitle>
<pages>58--67</pages>
<location>Pittsburgh, PA.</location>
<marker>Ali, Chali, Hasan, 2010</marker>
<rawString>Ali, H., Y. Chali, and S. A. Hasan. 2010. Automation of question generation from sentences. In Proceedings of QG2010: The Third Workshop on Question Generation, pages 58–67, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Andrenucci</author>
<author>E Sneiders</author>
</authors>
<title>Automated question answering: Review of the main approaches.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Conference on Information Technology and Applications (ICITA’05),</booktitle>
<pages>514--519</pages>
<location>Sydney.</location>
<contexts>
<context position="10307" citStr="Andrenucci and Sneiders 2005" startWordPosition="1578" endWordPosition="1581">s by considering their topic relevance and syntactic correctness scores. Experimental results show the effectiveness of our approach for automatically generating topical questions. The remainder of the article is organized as follows. Section 2 describes the related work. Section 3 presents the description of our QG system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense attention from researchers and different methods have been proposed to accomplish the task in different relevant fields (Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed 2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html. 3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report new results. 4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research. 3 Computational Linguistics Volume 41, Number 1 a method </context>
</contexts>
<marker>Andrenucci, Sneiders, 2005</marker>
<rawString>Andrenucci, A. and E. Sneiders. 2005. Automated question answering: Review of the main approaches. In Proceedings of the 3rd International Conference on Information Technology and Applications (ICITA’05), pages 514–519, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>P Piwek</author>
<author>eds</author>
</authors>
<date>2010</date>
<booktitle>Proceedings of QG2010: The Third Workshop on Question Generation.</booktitle>
<pages>questiongeneration.org.</pages>
<location>Pittsburgh, PA:</location>
<marker>Boyer, Piwek, eds, 2010</marker>
<rawString>Boyer, K. E. and P. Piwek, eds. 2010. Proceedings of QG2010: The Third Workshop on Question Generation. Pittsburgh, PA: questiongeneration.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Brown</author>
<author>G A Frishkoff</author>
<author>M Eskenazi</author>
</authors>
<title>Automatic question generation for vocabulary assessment.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>819--826</pages>
<location>Vancouver.</location>
<marker>Brown, Frishkoff, Eskenazi, 2005</marker>
<rawString>Brown, J. C., G. A. Frishkoff, and M. Eskenazi. 2005. Automatic question generation for vocabulary assessment. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 819–826, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Celikyilmaz</author>
<author>D Hakkani-Tur</author>
<author>G Tur</author>
</authors>
<title>LDA based similarity modeling for question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS ’10,</booktitle>
<pages>1--9</pages>
<location>Los Angeles, CA.</location>
<marker>Celikyilmaz, Hakkani-Tur, Tur, 2010</marker>
<rawString>Celikyilmaz, A., D. Hakkani-Tur, and G. Tur. 2010. LDA based similarity modeling for question answering. In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS ’10, pages 1–9, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
</authors>
<title>On the effectiveness of using sentence compression models for query-focused multi-document summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>457--474</pages>
<location>Mumbai.</location>
<contexts>
<context position="10615" citStr="Chali and Hasan 2012" startWordPosition="1620" endWordPosition="1623"> system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense attention from researchers and different methods have been proposed to accomplish the task in different relevant fields (Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed 2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html. 3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report new results. 4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research. 3 Computational Linguistics Volume 41, Number 1 a method to automatically generate questions based on question templates (which are created from training on medical articles). Brown, Frishkoff, and Eskenazi (2005) described an approach to automatically generate questions to assess the user’s vocabulary knowledge. Chen, Aist, and Mostow (2009) developed a method t</context>
<context position="15901" citStr="Chali and Hasan 2012" startWordPosition="2413" endWordPosition="2416"> of texts. We choose LDA because in recent years it has become one of the most popular topic modeling techniques and has been shown to be effective in several text-related tasks, such as document classification, information retrieval, and question answering (Wei and Croft 2006; Misra, Capp´e, and Yvon 2008; Celikyilmaz, HakkaniTur, and Tur 2010). Once we have the subtopics, we apply ESSK (Hirao et al. 2003) to calculate their similarity with the generated questions. The choice of ESSK is motivated by its successful use in different NLP tasks in recent years (Chali, Hasan, and Joty 2009, 2011; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 20</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Chali, Y. and S. A. Hasan. 2012a. On the effectiveness of using sentence compression models for query-focused multi-document summarization. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 457–474, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
</authors>
<title>Query-focused multi-document summarization: Automatic data annotations and supervised learning approaches.</title>
<date>2012</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="10615" citStr="Chali and Hasan 2012" startWordPosition="1620" endWordPosition="1623"> system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense attention from researchers and different methods have been proposed to accomplish the task in different relevant fields (Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed 2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html. 3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report new results. 4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research. 3 Computational Linguistics Volume 41, Number 1 a method to automatically generate questions based on question templates (which are created from training on medical articles). Brown, Frishkoff, and Eskenazi (2005) described an approach to automatically generate questions to assess the user’s vocabulary knowledge. Chen, Aist, and Mostow (2009) developed a method t</context>
<context position="15901" citStr="Chali and Hasan 2012" startWordPosition="2413" endWordPosition="2416"> of texts. We choose LDA because in recent years it has become one of the most popular topic modeling techniques and has been shown to be effective in several text-related tasks, such as document classification, information retrieval, and question answering (Wei and Croft 2006; Misra, Capp´e, and Yvon 2008; Celikyilmaz, HakkaniTur, and Tur 2010). Once we have the subtopics, we apply ESSK (Hirao et al. 2003) to calculate their similarity with the generated questions. The choice of ESSK is motivated by its successful use in different NLP tasks in recent years (Chali, Hasan, and Joty 2009, 2011; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 20</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Chali, Y. and S. A. Hasan. 2012b. Query-focused multi-document summarization: Automatic data annotations and supervised learning approaches. Journal of Natural Language Engineering, 18(1):109–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
</authors>
<title>Towards automatic topical question generation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>475--492</pages>
<location>Mumbai.</location>
<contexts>
<context position="10615" citStr="Chali and Hasan 2012" startWordPosition="1620" endWordPosition="1623"> system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense attention from researchers and different methods have been proposed to accomplish the task in different relevant fields (Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed 2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html. 3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report new results. 4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research. 3 Computational Linguistics Volume 41, Number 1 a method to automatically generate questions based on question templates (which are created from training on medical articles). Brown, Frishkoff, and Eskenazi (2005) described an approach to automatically generate questions to assess the user’s vocabulary knowledge. Chen, Aist, and Mostow (2009) developed a method t</context>
<context position="15901" citStr="Chali and Hasan 2012" startWordPosition="2413" endWordPosition="2416"> of texts. We choose LDA because in recent years it has become one of the most popular topic modeling techniques and has been shown to be effective in several text-related tasks, such as document classification, information retrieval, and question answering (Wei and Croft 2006; Misra, Capp´e, and Yvon 2008; Celikyilmaz, HakkaniTur, and Tur 2010). Once we have the subtopics, we apply ESSK (Hirao et al. 2003) to calculate their similarity with the generated questions. The choice of ESSK is motivated by its successful use in different NLP tasks in recent years (Chali, Hasan, and Joty 2009, 2011; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 20</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Chali, Y. and S. A. Hasan. 2012c. Towards automatic topical question generation. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 475–492, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>K Imam</author>
</authors>
<title>An aspect-driven random walk model for topic-focused multi-document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 7th Asian Information Retrieval Societies Conference (AIRS 2011),</booktitle>
<pages>386--397</pages>
<location>Dubai.</location>
<marker>Chali, Hasan, Imam, 2011</marker>
<rawString>Chali, Y., S. A. Hasan, and K. Imam. 2011. An aspect-driven random walk model for topic-focused multi-document summarization. In Proceedings of the 7th Asian Information Retrieval Societies Conference (AIRS 2011), pages 386–397, Dubai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>K Imam</author>
</authors>
<title>Learning good decompositions of complex questions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 17th International Conference on Applications of Natural Language Processing to Information Systems (NLDB 2012),</booktitle>
<pages>104--115</pages>
<location>Groningen.</location>
<marker>Chali, Hasan, Imam, 2012</marker>
<rawString>Chali, Y., S. A. Hasan, and K. Imam. 2012. Learning good decompositions of complex questions. In Proceedings of the 17th International Conference on Applications of Natural Language Processing to Information Systems (NLDB 2012), pages 104–115, Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>S R Joty</author>
</authors>
<title>Do automatic annotation techniques have any impact on supervised complex question answering?</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP</booktitle>
<pages>329--332</pages>
<marker>Chali, Hasan, Joty, 2009</marker>
<rawString>Chali, Y., S. A. Hasan, and S. R. Joty. 2009. Do automatic annotation techniques have any impact on supervised complex question answering? In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP 2009), pages 329–332, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S A Hasan</author>
<author>S R Joty</author>
</authors>
<title>Improving graph-based random walks for complex question answering using syntactic, shallow semantic and extended string subsequence kernels.</title>
<date>2011</date>
<journal>Information Processing &amp; Management,</journal>
<volume>47</volume>
<issue>6</issue>
<marker>Chali, Hasan, Joty, 2011</marker>
<rawString>Chali, Y., S. A. Hasan, and S. R. Joty. 2011. Improving graph-based random walks for complex question answering using syntactic, shallow semantic and extended string subsequence kernels. Information Processing &amp; Management, 47(6):843–855.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S R Joty</author>
</authors>
<title>Word sense disambiguation using lexical cohesion.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Conference on Semantic Evaluations,</booktitle>
<pages>476--479</pages>
<location>Prague.</location>
<contexts>
<context position="29664" citStr="Chali and Joty 2007" startWordPosition="4605" endWordPosition="4608">topics, we apply ESSK (Hirao et al. 2003) to measure their similarity with the generated questions. In the general ESSK, each word in a sentence is considered an “alphabet,” and the alternative is all its possible senses. However, our ESSK implementation considers the 10 Available at http://code.google.com/p/topic-modeling-tool/. 11 The model was built and tested according to the guidelines of the topic modeling toolkit we used. 9 Computational Linguistics Volume 41, Number 1 alternative of each word as its disambiguated sense. We use a dictionary-based Word Sense Disambiguation (WSD) system (Chali and Joty 2007) assuming one sense per discourse. We use WordNet (Fellbaum 1998) to find the semantic relations (such as repetition, synonym, hypernym and hyponym, holonym and meronym, and gloss) for all the words in a text. We assign a weight to each semantic relation based on heuristics and use all of them. Our WSD technique is decomposed into two steps: (1) building a representation of all possible senses of the words and (2) disambiguating the words based on the highest score. To be specific, each candidate word from the context is expanded to all of its senses. A disambiguation graph is constructed as t</context>
</contexts>
<marker>Chali, Joty, 2007</marker>
<rawString>Chali, Y. and S. R. Joty. 2007. Word sense disambiguation using lexical cohesion. In Proceedings of the 4th International Conference on Semantic Evaluations, pages 476–479, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
<author>S R Joty</author>
<author>S A Hasan</author>
</authors>
<title>Complex question answering: Unsupervised learning approaches and experiments.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>35--1</pages>
<marker>Chali, Joty, Hasan, 2009</marker>
<rawString>Chali, Y., S. R. Joty, and S. A. Hasan. 2009. Complex question answering: Unsupervised learning approaches and experiments. Journal of Artificial Intelligence Research, 35:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>1999</date>
<tech>Technical Report CS-99-12,</tech>
<institution>Brown University, Computer Science Department,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="33451" citStr="Charniak 1999" startWordPosition="5234" endWordPosition="5235">the generated question What is designed by Jobs and Wayne? are syntactically similar. An example of an ungrammatical generated question that is not very similar to its source is: Janoff presented Jobs What?. To judge the syntactic correctness of each generated question automatically, we apply the tree kernel functions and re-implement the syntactic tree kernel model according to Moschitti et al. (2007) for computing the syntactic similarity of each question with the associated content information. We first parse the sentences and the questions into syntactic trees using the Charniak parser13 (Charniak 1999). Then, we calculate the similarity between the two corresponding trees using the tree kernel method (Collins and Duffy 2001). We convert each parenthetic representation generated by the Charniak parser into its corresponding tree and give the trees as input to the tree kernel functions for measuring the syntactic similarity. The tree kernel function computes the number of common subtrees between two trees and gives the similarity score between each sentence in the given body of texts and the generated question based on the syntactic structure. Each sentence14 contributes a score to the questi</context>
</contexts>
<marker>Charniak, 1999</marker>
<rawString>Charniak, E. 1999. A maximum-entropyinspired parser. Technical Report CS-99-12, Brown University, Computer Science Department, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>G Aist</author>
<author>J Mostow</author>
</authors>
<title>Generating questions automatically from informational text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd Workshop on Question Generation (AIED</booktitle>
<pages>17--24</pages>
<location>Arlington, VA.</location>
<marker>Chen, Aist, Mostow, 2009</marker>
<rawString>Chen, W., G. Aist, and J. Mostow. 2009. Generating questions automatically from informational text. In Proceedings of the 2nd Workshop on Question Generation (AIED 2009), pages 17–24, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<location>Vancouver.</location>
<contexts>
<context position="9474" citStr="Collins and Duffy 2001" startWordPosition="1460" endWordPosition="1463">entify whether the question is asking something about the topic or something that is very closely related to the topic. We call this the measure of topic relevance. For this purpose, we use Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts and apply the Extended String Subsequence Kernel (ESSK) (Hirao et al. 2003) to calculate their similarity with the questions. In the second step, we judge the syntactic correctness of each generated question. We apply the tree kernel functions (Collins and Duffy 2001) and re-implement the syntactic tree kernel model according to Moschitti et al. (2007) for computing the syntactic similarity of each question with the associated content information. We rank the questions by considering their topic relevance and syntactic correctness scores. Experimental results show the effectiveness of our approach for automatically generating topical questions. The remainder of the article is organized as follows. Section 2 describes the related work. Section 3 presents the description of our QG system. Section 4 explains the experiments and shows evaluation results; Secti</context>
<context position="33576" citStr="Collins and Duffy 2001" startWordPosition="5251" endWordPosition="5254"> generated question that is not very similar to its source is: Janoff presented Jobs What?. To judge the syntactic correctness of each generated question automatically, we apply the tree kernel functions and re-implement the syntactic tree kernel model according to Moschitti et al. (2007) for computing the syntactic similarity of each question with the associated content information. We first parse the sentences and the questions into syntactic trees using the Charniak parser13 (Charniak 1999). Then, we calculate the similarity between the two corresponding trees using the tree kernel method (Collins and Duffy 2001). We convert each parenthetic representation generated by the Charniak parser into its corresponding tree and give the trees as input to the tree kernel functions for measuring the syntactic similarity. The tree kernel function computes the number of common subtrees between two trees and gives the similarity score between each sentence in the given body of texts and the generated question based on the syntactic structure. Each sentence14 contributes a score to the questions and then the questions are ranked by considering the average of similarity scores. 4. Experiments 4.1 System Description </context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, M. and N. Duffy. 2001. Convolution Kernels for natural language. In Proceedings of Neural Information Processing Systems, pages 625–632, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
<author>C Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing (IWP2005),</booktitle>
<pages>9--16</pages>
<location>Jeju Island.</location>
<contexts>
<context position="37820" citStr="Dolan and Brockett 2005" startWordPosition="5924" endWordPosition="5927">(2010b). The same judges evaluated all the system outputs and they were blind to the system identity when judging. No guidelines were provided on the relative importance of the various aspects that made the judgment task subjective. The inter-annotator agreement of Fleiss’s K = 0.41, 0.45, 0.62, and 0.33 are computed for the three judges for the results in Tables 3–6, indicating moderate (for the first two tables), and substantial and fair agreement (Landis and Koch 1977) between the raters, respectively. These K values were shown to be acceptable in the literature for the relevant NLP tasks (Dolan and Brockett 2005; Glickman, Dagan, and Koppel 2005; Heilman and Smith 2010b). 15 A syntactically incorrect question is not useful even if it is relevant to the topic. This motivated us to give equal importance to topic relevance and syntactic correctness. The parameter w can be tuned to investigate its impact on the system performance. 16 http://www.questiongeneration.org/mediawiki. 17 We use these data to build necessary general purpose rules for our QG model. 12 Chali and Hasan Towards Topic-to-Question Generation Table 3 Topic relevance and syntactic correctness scores. Systems Topic Relevance Syntactic Co</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>Dolan, W. B. and C. Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the 3rd International Workshop on Paraphrasing (IWP2005), pages 9–16, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>16--23</pages>
<location>Sapporo.</location>
<contexts>
<context position="11780" citStr="Echihabi and Marcu 2003" startWordPosition="1792" endWordPosition="1795">wledge. Chen, Aist, and Mostow (2009) developed a method to generate questions automatically from informational text to mimic the reader’s self-questioning strategy during reading. On the other hand, Agarwal, Shah, and Mannem (2011) considered the question generation problem beyond the sentence level and designed an approach that uses discourse connectives to generate questions from a given text. Several other QG models have been proposed over the years that deal with transforming answers to questions and utilizing question generation as an intermediate step in the question answering process (Echihabi and Marcu 2003; Hickl et al. 2005). There are some other researchers who have approached the task of generating questions for educational purposes (Mitkov and Ha 2003; Heilman and Smith 2010b). Question asking and QG are important components in advanced learning technologies such as intelligent tutoring systems and inquiry-based environments (Graesser et al. 2001). A QG system is useful for building better question-asking facilities in intelligent tutoring systems. The Natural Language Processing (NLP), Natural Language Generation, Intelligent Tutoring System, and Information Retrieval communities have curr</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Echihabi, A. and D. Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, pages 16–23, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="29729" citStr="Fellbaum 1998" startWordPosition="4617" endWordPosition="4618">ith the generated questions. In the general ESSK, each word in a sentence is considered an “alphabet,” and the alternative is all its possible senses. However, our ESSK implementation considers the 10 Available at http://code.google.com/p/topic-modeling-tool/. 11 The model was built and tested according to the guidelines of the topic modeling toolkit we used. 9 Computational Linguistics Volume 41, Number 1 alternative of each word as its disambiguated sense. We use a dictionary-based Word Sense Disambiguation (WSD) system (Chali and Joty 2007) assuming one sense per discourse. We use WordNet (Fellbaum 1998) to find the semantic relations (such as repetition, synonym, hypernym and hyponym, holonym and meronym, and gloss) for all the words in a text. We assign a weight to each semantic relation based on heuristics and use all of them. Our WSD technique is decomposed into two steps: (1) building a representation of all possible senses of the words and (2) disambiguating the words based on the highest score. To be specific, each candidate word from the context is expanded to all of its senses. A disambiguation graph is constructed as the intermediate representation where the nodes denote word instan</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet - An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="28658" citStr="Geman and Geman 1984" startWordPosition="4448" endWordPosition="4451">and a Dirichlet (0) prior is set on 4) to refine this basic model (Griffiths and Steyvers 2002; Blei, Ng, and Jordan 2003). Now the main goal is to estimate the two parameters: 0 and 4). We apply this framework directly to solve our problem by considering each topic-related body of texts as a document. We use a GUIbased toolkit for topic modeling10 that uses the popular MALLET (McCallum 2002) toolkit for the back-end. The LDA model is built on the development set11 (Section 4.2). The process starts by removing a list of “stop words” from the document and runs 200 iterations of Gibbs sampling (Geman and Geman 1984) to estimate the parameters 0 and 4). From each body of texts, we discover K topics and choose the most frequent words from the most likely unigrams as the desired subtopics. For example, from the associated body of texts of the topic Apple Inc. Logos, we get these subtopics: janoff, themes, logo, color, apple. 3.3.2 Extended String Subsequence Kernel (ESSK). Once we identify the subtopics, we apply ESSK (Hirao et al. 2003) to measure their similarity with the generated questions. In the general ESSK, each word in a sentence is considered an “alphabet,” and the alternative is all its possible </context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Geman, S. and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Glickman</author>
<author>I Dagan</author>
<author>M Koppel</author>
</authors>
<title>A probabilistic classification approach for lexical textual entailment.</title>
<date>2005</date>
<booktitle>In AAAI,</booktitle>
<pages>1--050</pages>
<location>Pittsburgh, PA.</location>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Glickman, O., I. Dagan, and M. Koppel. 2005. A probabilistic classification approach for lexical textual entailment. In AAAI, pages 1,050–1,055, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>K VanLehn</author>
<author>C P Rose</author>
<author>P W Jordan</author>
<author>D Harter</author>
</authors>
<title>Intelligent tutoring systems with conversational dialogue.</title>
<date>2001</date>
<journal>AI Magazine,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="4651" citStr="Graesser et al. 2001" startWordPosition="697" endWordPosition="700"> Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) systems that can assist in meeting their inquiry needs (Lauer, Peacock, and Graesser 1992; Graesser et al. 2001; Rus and Graesser 2009; Ali, Chali, and Hasan 2010; Kotov and Zhai 2010; Olney, Graesser, and Person 2012). Another benefit of QG is that it can be a good tool to help improve the quality of the QA systems (Graesser et al. 2001; Rus and Graesser 2009). These benefits of a QG system motivate us to address the important problem of topic-to-question generation, where the main goal is to generate all possible questions about a given topic. For example, given the topic Apple Inc. Logos, we would like to generate questions such as What is Apple Inc.?, Where is Apple Inc. located?, Who designed Appl</context>
<context position="12132" citStr="Graesser et al. 2001" startWordPosition="1844" endWordPosition="1847">ives to generate questions from a given text. Several other QG models have been proposed over the years that deal with transforming answers to questions and utilizing question generation as an intermediate step in the question answering process (Echihabi and Marcu 2003; Hickl et al. 2005). There are some other researchers who have approached the task of generating questions for educational purposes (Mitkov and Ha 2003; Heilman and Smith 2010b). Question asking and QG are important components in advanced learning technologies such as intelligent tutoring systems and inquiry-based environments (Graesser et al. 2001). A QG system is useful for building better question-asking facilities in intelligent tutoring systems. The Natural Language Processing (NLP), Natural Language Generation, Intelligent Tutoring System, and Information Retrieval communities have currently identified the Text-to-Question generation task as promising candidates for shared tasks5 (Rus and Graesser 2009; Boyer and Piwek 2010). In the Text-to-Question generation task, a QG system is given a text, and the goal is to generate a set of questions for which the text contains answers. The task of generating a question about a given text ca</context>
</contexts>
<marker>Graesser, VanLehn, Rose, Jordan, Harter, 2001</marker>
<rawString>Graesser, A. C., K. VanLehn, C. P. Rose, P. W. Jordan, and D. Harter. 2001. Intelligent tutoring systems with conversational dialogue. AI Magazine, 22(4):39–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Greenwood</author>
</authors>
<title>Open-Domain Question Answering.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Sheffield.</institution>
<contexts>
<context position="4019" citStr="Greenwood 2005" startWordPosition="595" endWordPosition="596">ng communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) systems that can assist in meeting their inquiry needs (Lauer, Peacock, and Gra</context>
</contexts>
<marker>Greenwood, 2005</marker>
<rawString>Greenwood, M. A. 2005. Open-Domain Question Answering. Ph.D. thesis, Department of Computer Science, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Prediction and semantic association.</title>
<date>2002</date>
<booktitle>In NIPS’02,</booktitle>
<pages>11--18</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="28131" citStr="Griffiths and Steyvers 2002" startWordPosition="4356" endWordPosition="4359">DA uses a generative topic modeling approach to specify the following distribution over words within a document: K P(wi) = E P(wi|zi = j)P(zi = j) (1) j=1 where K is the number of topics, P(wi|zi = j) is the probability of word wi under topic j, and P(zi = j) is the sampling probability of topic j for the ith word. The multinomial distributions 4)(j) = P(wlzi = j) and 0(d) = P(z) are termed as topic-word distribution and document-topic distribution, respectively (Blei, Ng, and Jordan 2003). A Dirichlet (α) prior is placed on 0 and a Dirichlet (0) prior is set on 4) to refine this basic model (Griffiths and Steyvers 2002; Blei, Ng, and Jordan 2003). Now the main goal is to estimate the two parameters: 0 and 4). We apply this framework directly to solve our problem by considering each topic-related body of texts as a document. We use a GUIbased toolkit for topic modeling10 that uses the popular MALLET (McCallum 2002) toolkit for the back-end. The LDA model is built on the development set11 (Section 4.2). The process starts by removing a list of “stop words” from the document and runs 200 iterations of Gibbs sampling (Geman and Geman 1984) to estimate the parameters 0 and 4). From each body of texts, we discove</context>
</contexts>
<marker>Griffiths, Steyvers, 2002</marker>
<rawString>Griffiths, T. L. and M. Steyvers. 2002. Prediction and semantic association. In NIPS’02, pages 11–18, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines. In</title>
<date>2003</date>
<tech>Technical Report TR-CSLR-2003-03,</tech>
<institution>University of Colorado,</institution>
<location>Boulder.</location>
<contexts>
<context position="24092" citStr="Hacioglu et al. 2003" startWordPosition="3698" endWordPosition="3701">In Table 1, we show some example rules for the basic questions generated in this work. Our next task is to generate specific questions from the sentences present in the given body of texts. For this purpose, we parse the sentences semantically using a 7 Available athttp://cogcomp.cs.illinois.edu/. 7 Computational Linguistics Volume 41, Number 1 Table 1 Example basic question rules. Tag Example Question person Who is person? organization Where is organization located? location Where is location? misc. What do you know about misc.? Semantic Role Labeling (SRL) system (Kingsbury and Palmer 2002; Hacioglu et al. 2003), ASSERT.8 ASSERT is an automatic statistical semantic role tagger that can annotate naturally occuring text with semantic arguments. When presented with a sentence, it performs a full syntactic analysis of the sentence, automatically identifies all the verb predicates in that sentence, extracts features for all constituents in the parse tree relative to the predicate, and identifies and tags the constituents with the appropriate semantic arguments. For example, the output of the SRL system for the sentence Apple’s first logo is designed by Jobs and Wayne is: [ARG1 Apple ’s first logo] is [TAR</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Hacioglu, K., S. Pradhan, W. Ward, J. H. Martin, and D. Jurafsky. 2003. Shallow semantic parsing using support vector machines. In Technical Report TR-CSLR-2003-03, University of Colorado, Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>F Lacatusu</author>
<author>A Hickl</author>
</authors>
<title>Answering complex questions with random walk models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>220--227</pages>
<location>Seattle, WA.</location>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>Harabagiu, S., F. Lacatusu, and A. Hickl. 2006. Answering complex questions with random walk models. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 220–227, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Extracting simplified statements for factual question generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Workshop on Question Generation,</booktitle>
<pages>11--20</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="11956" citStr="Heilman and Smith 2010" startWordPosition="1820" endWordPosition="1823">ng. On the other hand, Agarwal, Shah, and Mannem (2011) considered the question generation problem beyond the sentence level and designed an approach that uses discourse connectives to generate questions from a given text. Several other QG models have been proposed over the years that deal with transforming answers to questions and utilizing question generation as an intermediate step in the question answering process (Echihabi and Marcu 2003; Hickl et al. 2005). There are some other researchers who have approached the task of generating questions for educational purposes (Mitkov and Ha 2003; Heilman and Smith 2010b). Question asking and QG are important components in advanced learning technologies such as intelligent tutoring systems and inquiry-based environments (Graesser et al. 2001). A QG system is useful for building better question-asking facilities in intelligent tutoring systems. The Natural Language Processing (NLP), Natural Language Generation, Intelligent Tutoring System, and Information Retrieval communities have currently identified the Text-to-Question generation task as promising candidates for shared tasks5 (Rus and Graesser 2009; Boyer and Piwek 2010). In the Text-to-Question generatio</context>
<context position="13395" citStr="Heilman and Smith 2010" startWordPosition="2037" endWordPosition="2040">asks. First, given the source text, a content selection step is necessary to select a target to ask about, such as the desired answer. Second, given a target answer, an appropriate question type is selected (i.e., the form of question to ask is determined). Third, given the content and question type, the actual question is constructed. Based on this principle, several approaches have been described in Boyer and Piwek (2010) that use named entity information, syntactic knowledge, and semantic structures of the sentences to perform the task of generating questions from sentences and paragraphs (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010). Inspired by these works, we perform the task of topic-to-question generation using named entity information and semantic structures of the sentences. A task that is similar to ours is the task of keywords-to-question generation that has been addressed recently in Zheng et al. (2011). They propose a user model for jointly generating keywords and questions. However, their approach is based on generating question templates from existing questions, which requires a large set of English questions as training data. In recent years, some other related researchers h</context>
<context position="14767" citStr="Heilman and Smith 2010" startWordPosition="2237" endWordPosition="2240">act-based question generation has been accomplished previously (Rus, Cai, and Graesser 2007; Heilman and Smith 2010b). We also focus on generating fact-based questions in this research. Besides grammaticality, an effective QG system should focus deeply on the importance of the generated questions (Vanderwende 2008). This motivates the use of a question-ranking module in a typical QG system. Over-generated questions can be ranked using different approaches, such as statistical ranking methods, dependency parsing, identification of the presence of pronouns and named entities, and topic scoring (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010; McConnell et al. 2011). 5 http://www.questiongeneration.org/QGSTEC2010. 4 Chali and Hasan Towards Topic-to-Question Generation However, most of these automatic ranking approaches ignore the aspects of complex paraphrasing by not considering lexical semantic variations (e.g., synonymy) when measuring the importance of the questions. In our work, we use LDA (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts. We choose LDA because in recent years it has become one of the most popular</context>
<context position="17651" citStr="Heilman and Smith 2010" startWordPosition="2698" endWordPosition="2701"> main goal of our work is to generate as many questions as possible related to the topic. We use the named entity information and the predicate argument structures of the sentences to accomplish this goal. Our approach is different from the set-up in shared tasks (Rus and Graesser 2009; Boyer and Piwek 2010), as we generate a set of basic questions that are useful to add variety in the question space. A paragraph associated with each topic is used as the source of relevant information about the topic. We evaluate our systems in terms of topic relevance, which is different from prior research (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010). Syntactic correctness is also an important property of a good question. For this reason, we evaluate our system in terms of syntactic correctness as well. The proposed system will be useful for generating topic-related questions from the associated content information, which can be used to incorporate a “question suggestions for a certain topic” facility in search systems (Kotov and Zhai 2010). For example, if a user searches for some information related to a certain topic, the search system could generate all possible topic-relevant questions from a preexis</context>
<context position="21384" citStr="Heilman and Smith (2010" startWordPosition="3272" endWordPosition="3275">ness of the questions. Questions are then ranked by considering the ESSK similarity scores and the syntactic similarity scores. We present an architectural diagram (Figure 1) to show the different components of our system and describe the overall procedure in the following subsections. 3.1 Sentence Simplification Sentences may have complex grammatical structure with multiple embedded clauses. Therefore, the first step of our proposed system is to simplify the complex sentences with the intention of generating more accurate questions. We use the simplified factual statement extractor model6 of Heilman and Smith (2010a). Their model extracts the simpler forms of the complex source sentence by altering lexical items, syntactic structure, and semantics, as well as by removing phrase types such as leading conjunctions, sentence-level modifying phrases, and appositives. For example, given a complex sentence s, we get the corresponding simple sentences as follows: Complex Sentence (s): Apple’s first logo, designed by Jobs and Wayne, depicts Sir Isaac Newton sitting under an apple tree. Simple Sentence (1): Apple’s first logo is designed by Jobs and Wayne. 6 Availableathttp://www.ark.cs.cmu.edu/mheilman/question</context>
<context position="36013" citStr="Heilman and Smith (2010" startWordPosition="5637" endWordPosition="5640">in around 5–7 sentences for a total of 100–200 tokens (including punctuation). This data set includes a diversity of topics of general interest. We consider these topics and treat the paragraphs as their associated useful content information in order to generate a set of questions using our proposed QG approach. We randomly select 10 topics and their associated paragraphs as the development data.17 A total of 2,186 questions are generated from the remaining 50 topics (test data) to be ranked. 4.3 Evaluation Set-up 4.3.1 Methodology. We use a methodology derived from Boyer and Piwek (2010) and Heilman and Smith (2010b) to evaluate the performance of our QG systems. Three native English-speaking university graduate students judge the quality of the top-ranked 20% questions using two criteria: topic relevance and syntactic correctness. For topic relevance, the given score is an integer between 1 (very poor) and 5 (very good) and is guided by the consideration of the following aspects: 1. Semantic correctness (i.e., the question is meaningful and related to the topic), 2. Correctness of question type (i.e., a correct question word is used), and 3. Referential clarity (i.e., it is clearly possible to understa</context>
<context position="37878" citStr="Heilman and Smith 2010" startWordPosition="5933" endWordPosition="5936">nd they were blind to the system identity when judging. No guidelines were provided on the relative importance of the various aspects that made the judgment task subjective. The inter-annotator agreement of Fleiss’s K = 0.41, 0.45, 0.62, and 0.33 are computed for the three judges for the results in Tables 3–6, indicating moderate (for the first two tables), and substantial and fair agreement (Landis and Koch 1977) between the raters, respectively. These K values were shown to be acceptable in the literature for the relevant NLP tasks (Dolan and Brockett 2005; Glickman, Dagan, and Koppel 2005; Heilman and Smith 2010b). 15 A syntactically incorrect question is not useful even if it is relevant to the topic. This motivated us to give equal importance to topic relevance and syntactic correctness. The parameter w can be tuned to investigate its impact on the system performance. 16 http://www.questiongeneration.org/mediawiki. 17 We use these data to build necessary general purpose rules for our QG model. 12 Chali and Hasan Towards Topic-to-Question Generation Table 3 Topic relevance and syntactic correctness scores. Systems Topic Relevance Syntactic Correctness Baseline1 (No Ranking) 2.15 2.63 Baseline2 (Topi</context>
<context position="40147" citStr="Heilman and Smith 2010" startWordPosition="6290" endWordPosition="6293"> the English GigaWord Corpus. For example, from the given body of texts of the topic Apple Inc. Logos, we get these subtopics: jobs, logo, themes, rainbow, monochromatic. Then we use the same steps of Sections 3.3.2 and 3.4, and use Equation (2) to combine the scores. We evaluate the top-ranked 20% questions and show the results. (3) State-of-the-art: We choose a publicly available state-of-the-art QG system19 to generate questions from the sentences in the body of texts. This system was shown to achieve good performance in generating fact-based questions about the content of a given article (Heilman and Smith 2010b). Their method ranks the questions automatically using a logistic regression model. Given a paragraph as input, this system processes each sentence and generates a set of ranked questions for the entire paragraph. We evaluate the top-ranked 20% questions20 and report the results. 4.3.3 Results and Discussion. Table 3 shows the average topic relevance and syntactic correctness scores for all the systems. From these results, we can see that the proposed QG system improves the topic relevance and syntactic correctness scores over the Baseline1 system by 62% and 35%, respectively, and improves t</context>
<context position="41822" citStr="Heilman and Smith 2010" startWordPosition="6538" endWordPosition="6541">this work was to generate as many questions as possible related to the topic. For this reason, we considered generating the basic questions. These questions 18 Available at http://www.cis.upenn.edu/∼lannie/topicS.html. 19 Available athttp://www.ark.cs.cmu.edu/mheilman/questions/. 20 We ignore the yes-no questions for our task. 21 We tested statistical significance using Student’s t test. 13 Computational Linguistics Volume 41, Number 1 Table 4 Acceptability of the questions (in %). Systems Top 15% Top 30% Baseline1 (No Ranking) 35.2 32.6 Baseline2 (Topic Signature) 45.9 33.8 State-of-the-art (Heilman and Smith 2010b) 44.7 38.5 Proposed QG System 46.5 40.6 Table 5 Topic relevance and syntactic correctness scores (narrowed focus). Systems Topic Relevance Syntactic Correctness Baseline1 (No Ranking) 2.84 2.75 Baseline2 (Topic Signature) 3.50 3.42 State-of-the-art (Heilman and Smith 2010b) 3.63 3.56 Proposed QG System 3.78 3.72 were also useful to provide variety in the question space. We generated these questions using the named entity information. As the performance of the NE taggers were unsatisfactory, we had a few of these questions generated. In most cases, these questions were outranked by other impo</context>
<context position="44408" citStr="Heilman and Smith 2010" startWordPosition="6937" endWordPosition="6940">rom Wikipedia articles.22 We generate a total of 1, 845 questions from the 50 topics considered and rank them using different ranking schemes as discussed before. We evaluate the top 20% questions using the similar evaluation methodologies and report the results in Table 5. From these results, we can see that the proposed QG 22 http://en.wikipedia.org/wiki/Time 100. 14 Chali and Hasan Towards Topic-to-Question Generation Table 6 Acceptability of the questions in % (narrowed focus). Systems Top 15% Top 30% Baseline1 (No Ranking) 38.6 31.5 Baseline2 (Topic Signature) 47.1 35.5 State-of-the-art (Heilman and Smith 2010b) 52.4 40.2 Proposed QG System 55.8 42.0 system improves the topic relevance and syntactic correctness scores over the Baseline1 system by 33% and 35%, respectively, and improves the topic relevance and syntactic correctness scores over the Baseline2 system by 8% and 9%, respectively. Moreover, the proposed QG system improves both the topic relevance and syntactic correctness scores over the state-of-the-art system by 4%. From these results, we can clearly observe the effectiveness of our proposed QG system when we narrow down the topic focus. We also evaluate the top 15% and top 30% question</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Heilman, M. and N. A. Smith. 2010a. Extracting simplified statements for factual question generation. In Proceedings of the Third Workshop on Question Generation, pages 11–20, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Good question! Statistical ranking for question generation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>609--617</pages>
<location>Los Angeles, CA.</location>
<marker>Heilman, Smith, 2010</marker>
<rawString>Heilman, M. and N. A. Smith. 2010b. Good question! Statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 609–617, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Lehmann</author>
<author>D Moldovan</author>
<author>S Harabagiu</author>
</authors>
<title>Experiments with interactive question-answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>205--214</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="11800" citStr="Hickl et al. 2005" startWordPosition="1796" endWordPosition="1799">ostow (2009) developed a method to generate questions automatically from informational text to mimic the reader’s self-questioning strategy during reading. On the other hand, Agarwal, Shah, and Mannem (2011) considered the question generation problem beyond the sentence level and designed an approach that uses discourse connectives to generate questions from a given text. Several other QG models have been proposed over the years that deal with transforming answers to questions and utilizing question generation as an intermediate step in the question answering process (Echihabi and Marcu 2003; Hickl et al. 2005). There are some other researchers who have approached the task of generating questions for educational purposes (Mitkov and Ha 2003; Heilman and Smith 2010b). Question asking and QG are important components in advanced learning technologies such as intelligent tutoring systems and inquiry-based environments (Graesser et al. 2001). A QG system is useful for building better question-asking facilities in intelligent tutoring systems. The Natural Language Processing (NLP), Natural Language Generation, Intelligent Tutoring System, and Information Retrieval communities have currently identified the</context>
</contexts>
<marker>Hickl, Lehmann, Moldovan, Harabagiu, 2005</marker>
<rawString>Hickl, A., J. Lehmann, D. Moldovan, and S. Harabagiu. 2005. Experiments with interactive question-answering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 205–214, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>P Wang</author>
<author>J Lehmann</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Ferret: Interactive question-answering for real-world environments.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive Presentation Sessions,</booktitle>
<pages>25--28</pages>
<location>Sydney.</location>
<contexts>
<context position="7340" citStr="Hickl et al. 2006" startWordPosition="1115" endWordPosition="1118">ple complex questions have been provided according to the guidelines of the Document Understanding Conference (DUC, http://duc.nist.gov/) (2005–2007) tasks. 2 Chali and Hasan Towards Topic-to-Question Generation Decomposing a complex question automatically into simpler questions in this manner such that each of them can be answered individually by using the state-of-the-art QA systems, and then combining the individual answers to form a single answer to the original complex question, has proven effective to deal with the complex question answering problem (Harabagiu, Lacatusu, and Hickl 2006; Hickl et al. 2006; Chali, Hasan, and Imam 2012). Moreover, the generated simple questions can be used as the list of important aspects to act as a guide2 for selecting the most relevant sentences in producing more focused and more accurate summaries as the output of a summarization system (Chali, Hasan, and Imam 2011, 2012). From this discussion, it is obvious that the complex question decomposition problem can be generalized to the problem of topic-to-question generation to help improve the complex question answering systems. In this article3, we consider the task of automatically generating questions from to</context>
</contexts>
<marker>Hickl, Wang, Lehmann, Harabagiu, 2006</marker>
<rawString>Hickl, A., P. Wang, J. Lehmann, and Sanda Harabagiu. 2006. Ferret: Interactive question-answering for real-world environments. In Proceedings of the COLING/ACL on Interactive Presentation Sessions, pages 25–28, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>NTT’s multiple document summarization system for DUC2003.</title>
<date>2003</date>
<booktitle>In Proceedings of the Document Understanding Conference,</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="9281" citStr="Hirao et al. 2003" startWordPosition="1430" endWordPosition="1433">e used to identify relevant parts of a sentence in order to form relevant questions about them. The importance of the generated questions is measured in two steps. In the first step, we identify whether the question is asking something about the topic or something that is very closely related to the topic. We call this the measure of topic relevance. For this purpose, we use Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts and apply the Extended String Subsequence Kernel (ESSK) (Hirao et al. 2003) to calculate their similarity with the questions. In the second step, we judge the syntactic correctness of each generated question. We apply the tree kernel functions (Collins and Duffy 2001) and re-implement the syntactic tree kernel model according to Moschitti et al. (2007) for computing the syntactic similarity of each question with the associated content information. We rank the questions by considering their topic relevance and syntactic correctness scores. Experimental results show the effectiveness of our approach for automatically generating topical questions. The remainder of the a</context>
<context position="15691" citStr="Hirao et al. 2003" startWordPosition="2377" endWordPosition="2380">.g., synonymy) when measuring the importance of the questions. In our work, we use LDA (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts. We choose LDA because in recent years it has become one of the most popular topic modeling techniques and has been shown to be effective in several text-related tasks, such as document classification, information retrieval, and question answering (Wei and Croft 2006; Misra, Capp´e, and Yvon 2008; Celikyilmaz, HakkaniTur, and Tur 2010). Once we have the subtopics, we apply ESSK (Hirao et al. 2003) to calculate their similarity with the generated questions. The choice of ESSK is motivated by its successful use in different NLP tasks in recent years (Chali, Hasan, and Joty 2009, 2011; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure </context>
<context position="29085" citStr="Hirao et al. 2003" startWordPosition="4520" endWordPosition="4523">model is built on the development set11 (Section 4.2). The process starts by removing a list of “stop words” from the document and runs 200 iterations of Gibbs sampling (Geman and Geman 1984) to estimate the parameters 0 and 4). From each body of texts, we discover K topics and choose the most frequent words from the most likely unigrams as the desired subtopics. For example, from the associated body of texts of the topic Apple Inc. Logos, we get these subtopics: janoff, themes, logo, color, apple. 3.3.2 Extended String Subsequence Kernel (ESSK). Once we identify the subtopics, we apply ESSK (Hirao et al. 2003) to measure their similarity with the generated questions. In the general ESSK, each word in a sentence is considered an “alphabet,” and the alternative is all its possible senses. However, our ESSK implementation considers the 10 Available at http://code.google.com/p/topic-modeling-tool/. 11 The model was built and tested according to the guidelines of the topic modeling toolkit we used. 9 Computational Linguistics Volume 41, Number 1 alternative of each word as its disambiguated sense. We use a dictionary-based Word Sense Disambiguation (WSD) system (Chali and Joty 2007) assuming one sense p</context>
<context position="32084" citStr="Hirao et al. (2003" startWordPosition="5028" endWordPosition="5031">ly. The function val(t, q) returns the number of common attributes (i.e., the number of common words/senses) to the given nodes t and q. � 0 if j = 1 Km (ti, qj) = λK; m(ti, qj−1) + Km(ti, qj−1) Here, λ is the decay parameter for the number of skipped words. K~~m(ti, qj) is defined as: � 0 if i = 1 Km(ti, qj) = λK~~m(ti−1, qj) + Km(ti−1, qj) 12 The formulae denote a dynamic programming technique to compute the ESSK similarity score where d is the vector space dimension (i.e., the number of all possible subsequences of up to length d). More information about these formulae can be obtained from Hirao et al. (2003, 2004). 10 Chali and Hasan Towards Topic-to-Question Generation Finally, the similarity measure is defined after normalization: Kessk(T, Q) siMessk(T, Q) = \,IKessk(T, T)Kessk(Q, Q) 3.4 Judging Syntactic Correctness The next step of our system is to judge the syntactic correctness of the generated questions. The generated questions might be syntactically incorrect due to the process of automatic question generation. It is time-consuming and considerable human intervention is necessary to check for the syntactically incorrect questions manually. We strongly believe that a question should have </context>
</contexts>
<marker>Hirao, Suzuki, Isozaki, Maeda, 2003</marker>
<rawString>Hirao, T., J. Suzuki, H. Isozaki, and E. Maeda. 2003. NTT’s multiple document summarization system for DUC2003. In Proceedings of the Document Understanding Conference, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>Dependency-based sentence alignment for multiple document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>446--452</pages>
<location>Geneva.</location>
<marker>Hirao, Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>Hirao, T., J. Suzuki, H. Isozaki, and E. Maeda. 2004. Dependency-based sentence alignment for multiple document summarization. In Proceedings of COLING 2004, pages 446–452, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>R Gaizauskas</author>
</authors>
<title>Natural language question answering: The view from here.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="3471" citStr="Hirschman and Gaizauskas 2001" startWordPosition="514" endWordPosition="517">s Volume 41, Number 1 and their search task is usually not over (Chali, Joty, and Hasan 2009). The next step for the user is to look into the documents themselves and search for the precise piece of information they were looking for. This method is time-consuming, and a correct answer could easily be missed by either an incorrect query resulting in missing documents or by careless reading. This is why Question Answering (QA) has received immense attention from the information retrieval, information extraction, machine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One </context>
</contexts>
<marker>Hirschman, Gaizauskas, 2001</marker>
<rawString>Hirschman, L. and R. Gaizauskas. 2001. Natural language question answering: The view from here. Natural Language Engineering, 7(4):275–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ignatova</author>
<author>D Bernhard</author>
<author>I Gurevych</author>
</authors>
<title>Generating high quality questions from low quality questions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge,</booktitle>
<location>Arlington, VA.</location>
<marker>Ignatova, Bernhard, Gurevych, 2008</marker>
<rawString>Ignatova, K., D. Bernhard, and I. Gurevych. 2008. Generating high quality questions from low quality questions. In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kotov</author>
<author>C Zhai</author>
</authors>
<title>Towards natural question guided search.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on the World Wide Web, WWW ’10,</booktitle>
<pages>541--550</pages>
<location>Raleigh, NC.</location>
<contexts>
<context position="3526" citStr="Kotov and Zhai 2010" startWordPosition="522" endWordPosition="525">(Chali, Joty, and Hasan 2009). The next step for the user is to look into the documents themselves and search for the precise piece of information they were looking for. This method is time-consuming, and a correct answer could easily be missed by either an incorrect query resulting in missing documents or by careless reading. This is why Question Answering (QA) has received immense attention from the information retrieval, information extraction, machine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must</context>
<context position="18083" citStr="Kotov and Zhai 2010" startWordPosition="2764" endWordPosition="2767">ach topic is used as the source of relevant information about the topic. We evaluate our systems in terms of topic relevance, which is different from prior research (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010). Syntactic correctness is also an important property of a good question. For this reason, we evaluate our system in terms of syntactic correctness as well. The proposed system will be useful for generating topic-related questions from the associated content information, which can be used to incorporate a “question suggestions for a certain topic” facility in search systems (Kotov and Zhai 2010). For example, if a user searches for some information related to a certain topic, the search system could generate all possible topic-relevant questions from a preexistent related body of texts to provide suggestions. Kotov and Zhai (2010) approached a similar task by proposing a technique to augment the standard ranked list presentation of search results with a question-based interface to refine user-given queries. The major contributions of our work can be summarized as follows: • We perform the task of topic-to-question generation, which can help users in expressing their information needs</context>
</contexts>
<marker>Kotov, Zhai, 2010</marker>
<rawString>Kotov, A. and C. Zhai. 2010. Towards natural question guided search. In Proceedings of the 19th International Conference on the World Wide Web, WWW ’10, pages 541–550, Raleigh, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>MURAX: A robust linguistic approach for question answering using an on-line encyclopedia.</title>
<date>1993</date>
<booktitle>In SIGIR,</booktitle>
<pages>181--190</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3957" citStr="Kupiec 1993" startWordPosition="587" endWordPosition="588">extraction, machine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) systems that can </context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Kupiec, J. 1993. MURAX: A robust linguistic approach for question answering using an on-line encyclopedia. In SIGIR, pages 181–190, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="37673" citStr="Landis and Koch 1977" startWordPosition="5900" endWordPosition="5903">ovided with an annotation guideline and sample judgments, according to the methodology derived from Boyer and Piwek (2010) and Heilman and Smith (2010b). The same judges evaluated all the system outputs and they were blind to the system identity when judging. No guidelines were provided on the relative importance of the various aspects that made the judgment task subjective. The inter-annotator agreement of Fleiss’s K = 0.41, 0.45, 0.62, and 0.33 are computed for the three judges for the results in Tables 3–6, indicating moderate (for the first two tables), and substantial and fair agreement (Landis and Koch 1977) between the raters, respectively. These K values were shown to be acceptable in the literature for the relevant NLP tasks (Dolan and Brockett 2005; Glickman, Dagan, and Koppel 2005; Heilman and Smith 2010b). 15 A syntactically incorrect question is not useful even if it is relevant to the topic. This motivated us to give equal importance to topic relevance and syntactic correctness. The parameter w can be tuned to investigate its impact on the system performance. 16 http://www.questiongeneration.org/mediawiki. 17 We use these data to build necessary general purpose rules for our QG model. 12 </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J. R. and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Lauer</author>
<author>E Peacock</author>
<author>A C Graesser</author>
<author>eds</author>
</authors>
<date>1992</date>
<booktitle>Questions and Information Systems. Erlbaum,</booktitle>
<location>Hillsdale, NJ.</location>
<marker>Lauer, Peacock, Graesser, eds, 1992</marker>
<rawString>Lauer, T. W., E. Peacock, and A. C. Graesser, eds. 1992. Questions and Information Systems. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers: The role of semantic information.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<marker>Li, Roth, 2006</marker>
<rawString>Li, X. and D. Roth. 2006. Learning question classifiers: The role of semantic information. Journal of Natural Language Engineering, 12(3):229–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>Automatic question generation from queries.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge,</booktitle>
<location>Arlington, VA.</location>
<contexts>
<context position="14142" citStr="Lin 2008" startWordPosition="2150" endWordPosition="2151">ation and semantic structures of the sentences. A task that is similar to ours is the task of keywords-to-question generation that has been addressed recently in Zheng et al. (2011). They propose a user model for jointly generating keywords and questions. However, their approach is based on generating question templates from existing questions, which requires a large set of English questions as training data. In recent years, some other related researchers have proposed the tasks of high-quality question generation (Ignatova, Bernhard, and Gurevych 2008) and generating questions from queries (Lin 2008). Fact-based question generation has been accomplished previously (Rus, Cai, and Graesser 2007; Heilman and Smith 2010b). We also focus on generating fact-based questions in this research. Besides grammaticality, an effective QG system should focus deeply on the importance of the generated questions (Vanderwende 2008). This motivates the use of a question-ranking module in a typical QG system. Over-generated questions can be ranked using different approaches, such as statistical ranking methods, dependency parsing, identification of the presence of pronouns and named entities, and topic scorin</context>
</contexts>
<marker>Lin, 2008</marker>
<rawString>Lin, C. Y. 2008. Automatic question generation from queries. In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="39094" citStr="Lin and Hovy 2000" startWordPosition="6120" endWordPosition="6123">pic Signature) 3.24 3.30 State-of-the-art (Heilman and Smith 2010b) 3.35 3.45 Proposed QG System 3.48 3.55 4.3.2 Systems for Comparison. We report the performance of the following systems in order to do a meaningful comparison with our proposed QG system: (1) Baseline1: This is our QG system without any question-ranking method applied to it. Here, we randomly select top 20% questions and rate them. (2) Baseline2: For our second baseline, we build a QG system using an alternative topic modeling approach. Here, we use a topic signature model (instead of using LDA as discussed in Section 3.3.1) (Lin and Hovy 2000) to identify the important subtopics from the sentences present in the body of texts. The subtopics are the important words in the context that are closely related to the topic and have significantly greater probability of occurring in the given text compared with a large background corpus. We use a topic signature computation tool18 for this purpose. The background corpus that is used in this tool contains 5,000 documents from the English GigaWord Corpus. For example, from the given body of texts of the topic Apple Inc. Logos, we get these subtopics: jobs, logo, themes, rainbow, monochromatic</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Lin, C. Y. and E. H. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th Conference on Computational Linguistics, pages 495–501, Saarbr¨uken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mannem</author>
<author>R Prasad</author>
<author>A Joshi</author>
</authors>
<title>Question generation from paragraphs at UPenn: QGSTEC system description.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Workshop on Question Generation,</booktitle>
<pages>84--91</pages>
<location>Pittsburgh, PA.</location>
<marker>Mannem, Prasad, Joshi, 2010</marker>
<rawString>Mannem, P., R. Prasad, and A. Joshi. 2010. Question generation from paragraphs at UPenn: QGSTEC system description. In Proceedings of the Third Workshop on Question Generation, pages 84–91, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="28432" citStr="McCallum 2002" startWordPosition="4411" endWordPosition="4412">d. The multinomial distributions 4)(j) = P(wlzi = j) and 0(d) = P(z) are termed as topic-word distribution and document-topic distribution, respectively (Blei, Ng, and Jordan 2003). A Dirichlet (α) prior is placed on 0 and a Dirichlet (0) prior is set on 4) to refine this basic model (Griffiths and Steyvers 2002; Blei, Ng, and Jordan 2003). Now the main goal is to estimate the two parameters: 0 and 4). We apply this framework directly to solve our problem by considering each topic-related body of texts as a document. We use a GUIbased toolkit for topic modeling10 that uses the popular MALLET (McCallum 2002) toolkit for the back-end. The LDA model is built on the development set11 (Section 4.2). The process starts by removing a list of “stop words” from the document and runs 200 iterations of Gibbs sampling (Geman and Geman 1984) to estimate the parameters 0 and 4). From each body of texts, we discover K topics and choose the most frequent words from the most likely unigrams as the desired subtopics. For example, from the associated body of texts of the topic Apple Inc. Logos, we get these subtopics: janoff, themes, logo, color, apple. 3.3.2 Extended String Subsequence Kernel (ESSK). Once we iden</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, A. K. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C McConnell</author>
<author>P Mannem</author>
<author>R Prasad</author>
<author>A Joshi</author>
</authors>
<title>A new approach to ranking over-generated questions.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Question Generation,</booktitle>
<pages>45--48</pages>
<location>Arlington, VA.</location>
<contexts>
<context position="14824" citStr="McConnell et al. 2011" startWordPosition="2246" endWordPosition="2249">ously (Rus, Cai, and Graesser 2007; Heilman and Smith 2010b). We also focus on generating fact-based questions in this research. Besides grammaticality, an effective QG system should focus deeply on the importance of the generated questions (Vanderwende 2008). This motivates the use of a question-ranking module in a typical QG system. Over-generated questions can be ranked using different approaches, such as statistical ranking methods, dependency parsing, identification of the presence of pronouns and named entities, and topic scoring (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010; McConnell et al. 2011). 5 http://www.questiongeneration.org/QGSTEC2010. 4 Chali and Hasan Towards Topic-to-Question Generation However, most of these automatic ranking approaches ignore the aspects of complex paraphrasing by not considering lexical semantic variations (e.g., synonymy) when measuring the importance of the questions. In our work, we use LDA (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts. We choose LDA because in recent years it has become one of the most popular topic modeling techniques and has been shown to be effec</context>
</contexts>
<marker>McConnell, Mannem, Prasad, Joshi, 2011</marker>
<rawString>McConnell, C. C., P. Mannem, R. Prasad, and A. Joshi. 2011. A new approach to ranking over-generated questions. In Proceedings of the AAAI Fall Symposium on Question Generation, pages 45–48, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McGough</author>
<author>J Mortensen</author>
<author>J Johnson</author>
<author>S Fadali</author>
</authors>
<title>A Web-based testing system with dynamic question generation.</title>
<date>2001</date>
<booktitle>In ASEE/IEEE Frontiers in Education Conference, pages S3C-23–28</booktitle>
<volume>3</volume>
<location>Reno, NV.</location>
<contexts>
<context position="10330" citStr="McGough et al. (2001)" startWordPosition="1582" endWordPosition="1585">levance and syntactic correctness scores. Experimental results show the effectiveness of our approach for automatically generating topical questions. The remainder of the article is organized as follows. Section 2 describes the related work. Section 3 presents the description of our QG system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense attention from researchers and different methods have been proposed to accomplish the task in different relevant fields (Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed 2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html. 3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report new results. 4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research. 3 Computational Linguistics Volume 41, Number 1 a method to automatically genera</context>
</contexts>
<marker>McGough, Mortensen, Johnson, Fadali, 2001</marker>
<rawString>McGough, J., J. Mortensen, J. Johnson, and S. Fadali. 2001. A Web-based testing system with dynamic question generation. In ASEE/IEEE Frontiers in Education Conference, pages S3C-23–28 (vol. 3), Reno, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Misra</author>
<author>O Capp´e</author>
<author>F Yvon</author>
</authors>
<title>Using LDA to detect semantically incoherent documents.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<pages>41--48</pages>
<location>Manchester.</location>
<marker>Misra, Capp´e, Yvon, 2008</marker>
<rawString>Misra, H., O. Capp´e, and F. Yvon. 2008. Using LDA to detect semantically incoherent documents. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 41–48, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
<author>L A Ha</author>
</authors>
<title>Computer-aided generation of multiple-choice tests.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using Natural Language Processing -</booktitle>
<volume>2</volume>
<pages>17--22</pages>
<location>Edmonton.</location>
<contexts>
<context position="11932" citStr="Mitkov and Ha 2003" startWordPosition="1816" endWordPosition="1819">trategy during reading. On the other hand, Agarwal, Shah, and Mannem (2011) considered the question generation problem beyond the sentence level and designed an approach that uses discourse connectives to generate questions from a given text. Several other QG models have been proposed over the years that deal with transforming answers to questions and utilizing question generation as an intermediate step in the question answering process (Echihabi and Marcu 2003; Hickl et al. 2005). There are some other researchers who have approached the task of generating questions for educational purposes (Mitkov and Ha 2003; Heilman and Smith 2010b). Question asking and QG are important components in advanced learning technologies such as intelligent tutoring systems and inquiry-based environments (Graesser et al. 2001). A QG system is useful for building better question-asking facilities in intelligent tutoring systems. The Natural Language Processing (NLP), Natural Language Generation, Intelligent Tutoring System, and Information Retrieval communities have currently identified the Text-to-Question generation task as promising candidates for shared tasks5 (Rus and Graesser 2009; Boyer and Piwek 2010). In the Te</context>
</contexts>
<marker>Mitkov, Ha, 2003</marker>
<rawString>Mitkov, R. and L. A. Ha. 2003. Computer-aided generation of multiple-choice tests. In Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using Natural Language Processing - Volume 2, pages 17–22, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
</authors>
<title>Lymba’s PowerAnswer 4 in TREC</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Text REtreival Conference,</booktitle>
<location>Gaithersburg, MD.</location>
<marker>Moldovan, Clark, Bowden, 2007</marker>
<rawString>Moldovan, D., C. Clark, and M. Bowden. 2007. Lymba’s PowerAnswer 4 in TREC 2007. In Proceedings of the 16th Text REtreival Conference, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>R Basili</author>
</authors>
<title>A tree kernel approach to question and answer classification in question answering systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1--510</pages>
<location>Genoa.</location>
<contexts>
<context position="16503" citStr="Moschitti and Basili 2006" startWordPosition="2504" endWordPosition="2507">1; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 2006; Moschitti et al. 2007; Chali, Hasan, and Joty 2009, 2011). Pasca and Harabagiu (2001) argued that with the syntactic form of a sentence one can see which words depend on other words. We also feel that there should be a similarity between the words that are dependent in the sentences present in the associated body of texts and the dependency between words of the generated question. This motivates us to propose the use of syntactic kernels in judging the syntactic correctness of the generated questions automatically. The main goal of our work is to generate as many questions as possible relate</context>
</contexts>
<marker>Moschitti, Basili, 2006</marker>
<rawString>Moschitti, A. and R. Basili. 2006. A tree kernel approach to question and answer classification in question answering systems. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 1,510–1,513, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>776--783</pages>
<location>Prague.</location>
<contexts>
<context position="9560" citStr="Moschitti et al. (2007)" startWordPosition="1473" endWordPosition="1476">ery closely related to the topic. We call this the measure of topic relevance. For this purpose, we use Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts and apply the Extended String Subsequence Kernel (ESSK) (Hirao et al. 2003) to calculate their similarity with the questions. In the second step, we judge the syntactic correctness of each generated question. We apply the tree kernel functions (Collins and Duffy 2001) and re-implement the syntactic tree kernel model according to Moschitti et al. (2007) for computing the syntactic similarity of each question with the associated content information. We rank the questions by considering their topic relevance and syntactic correctness scores. Experimental results show the effectiveness of our approach for automatically generating topical questions. The remainder of the article is organized as follows. Section 2 describes the related work. Section 3 presents the description of our QG system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense att</context>
<context position="16526" citStr="Moschitti et al. 2007" startWordPosition="2508" endWordPosition="2511">012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 2006; Moschitti et al. 2007; Chali, Hasan, and Joty 2009, 2011). Pasca and Harabagiu (2001) argued that with the syntactic form of a sentence one can see which words depend on other words. We also feel that there should be a similarity between the words that are dependent in the sentences present in the associated body of texts and the dependency between words of the generated question. This motivates us to propose the use of syntactic kernels in judging the syntactic correctness of the generated questions automatically. The main goal of our work is to generate as many questions as possible related to the topic. We use </context>
<context position="33242" citStr="Moschitti et al. (2007)" startWordPosition="5202" endWordPosition="5205">stions manually. We strongly believe that a question should have a similar syntactic structure to a sentence from which it is generated. For example, the sentence Apple’s first logo is designed by Jobs and Wayne., and the generated question What is designed by Jobs and Wayne? are syntactically similar. An example of an ungrammatical generated question that is not very similar to its source is: Janoff presented Jobs What?. To judge the syntactic correctness of each generated question automatically, we apply the tree kernel functions and re-implement the syntactic tree kernel model according to Moschitti et al. (2007) for computing the syntactic similarity of each question with the associated content information. We first parse the sentences and the questions into syntactic trees using the Charniak parser13 (Charniak 1999). Then, we calculate the similarity between the two corresponding trees using the tree kernel method (Collins and Duffy 2001). We convert each parenthetic representation generated by the Charniak parser into its corresponding tree and give the trees as input to the tree kernel functions for measuring the syntactic similarity. The tree kernel function computes the number of common subtrees</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Moschitti, A., S. Quarteroni, R. Basili, and S. Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Olney</author>
<author>A C Graesser</author>
<author>N K Person</author>
</authors>
<title>Question generation from concept maps.</title>
<date>2012</date>
<booktitle>Dialogue and Discourse,</booktitle>
<volume>3</volume>
<issue>2</issue>
<marker>Olney, Graesser, Person, 2012</marker>
<rawString>Olney, A. M., A. C. Graesser, and N. K. Person. 2012. Question generation from concept maps. Dialogue and Discourse, 3(2):75–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S M Harabagiu</author>
</authors>
<title>Answer mining from on-line documents.</title>
<date>2001</date>
<booktitle>In Proceedings of the Association for Computational Linguistics 39th Annual Meeting and 10th Conference of the European Chapter Workshop on Open-Domain Question Answering,</booktitle>
<pages>38--45</pages>
<location>Toulouse.</location>
<contexts>
<context position="16590" citStr="Pasca and Harabagiu (2001)" startWordPosition="2518" endWordPosition="2521">possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 2006; Moschitti et al. 2007; Chali, Hasan, and Joty 2009, 2011). Pasca and Harabagiu (2001) argued that with the syntactic form of a sentence one can see which words depend on other words. We also feel that there should be a similarity between the words that are dependent in the sentences present in the associated body of texts and the dependency between words of the generated question. This motivates us to propose the use of syntactic kernels in judging the syntactic correctness of the generated questions automatically. The main goal of our work is to generate as many questions as possible related to the topic. We use the named entity information and the predicate argument structur</context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Pasca, M. and S. M. Harabagiu. 2001. Answer mining from on-line documents. In Proceedings of the Association for Computational Linguistics 39th Annual Meeting and 10th Conference of the European Chapter Workshop on Open-Domain Question Answering, pages 38–45, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pinchak</author>
<author>D Lin</author>
</authors>
<title>A probabilistic answer type model.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>393--400</pages>
<location>Trento.</location>
<marker>Pinchak, Lin, 2006</marker>
<rawString>Pinchak, C. and D. Lin. 2006. A probabilistic answer type model. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 393–400, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>147--155</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="22576" citStr="Ratinov and Roth 2009" startWordPosition="3448" endWordPosition="3451">k.cs.cmu.edu/mheilman/questions/. 6 Chali and Hasan Towards Topic-to-Question Generation Figure 1 Architectural diagram of our system. Simple Sentence (2): Apple’s first logo depicts Sir Isaac Newton sitting under an apple tree. 3.2 Named Entity Information and Semantic Role Labeling for QG In the second step of our system, we at first process the simple sentences in order to generate all possible questions from them. We use the Illinois Named Entity Tagger,7 a state-of-the-art named entity (NE) tagger that tags plain text with named entities (people, organizations, locations, miscellaneous) (Ratinov and Roth 2009). Once we tag the topic under consideration and its associated body of texts, we use some general purpose rules to create some basic questions even though the answer is not present in the body of texts. For example, Apple Inc. is tagged as an organization, so we generate a question: Where is Apple Inc. located?. The main motivation behind generating such questions is to add variety to the generated question space. The basic questions are useful when there is very little or no knowledge available for a certain topic in consideration. This assumption is inherited from the scenario in the real wo</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Ratinov, L. and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>Z Cai</author>
<author>A C Graesser</author>
</authors>
<title>Experiments on generating questions about facts.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>444--455</pages>
<location>Mexico City.</location>
<marker>Rus, Cai, Graesser, 2007</marker>
<rawString>Rus, V., Z. Cai, and A. C. Graesser. 2007. Experiments on generating questions about facts. In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing, pages 444–455, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>A C Graesser</author>
</authors>
<title>The question generation shared task and evaluation challenge.</title>
<date>2009</date>
<booktitle>In Workshop on the Question Generation Shared Task and Evaluation Challenge, Final Report,</booktitle>
<pages>1--37</pages>
<institution>University of Memphis.</institution>
<contexts>
<context position="4674" citStr="Rus and Graesser 2009" startWordPosition="701" endWordPosition="704">Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) systems that can assist in meeting their inquiry needs (Lauer, Peacock, and Graesser 1992; Graesser et al. 2001; Rus and Graesser 2009; Ali, Chali, and Hasan 2010; Kotov and Zhai 2010; Olney, Graesser, and Person 2012). Another benefit of QG is that it can be a good tool to help improve the quality of the QA systems (Graesser et al. 2001; Rus and Graesser 2009). These benefits of a QG system motivate us to address the important problem of topic-to-question generation, where the main goal is to generate all possible questions about a given topic. For example, given the topic Apple Inc. Logos, we would like to generate questions such as What is Apple Inc.?, Where is Apple Inc. located?, Who designed Apple’s Logo?, and so forth</context>
<context position="12498" citStr="Rus and Graesser 2009" startWordPosition="1891" endWordPosition="1894"> questions for educational purposes (Mitkov and Ha 2003; Heilman and Smith 2010b). Question asking and QG are important components in advanced learning technologies such as intelligent tutoring systems and inquiry-based environments (Graesser et al. 2001). A QG system is useful for building better question-asking facilities in intelligent tutoring systems. The Natural Language Processing (NLP), Natural Language Generation, Intelligent Tutoring System, and Information Retrieval communities have currently identified the Text-to-Question generation task as promising candidates for shared tasks5 (Rus and Graesser 2009; Boyer and Piwek 2010). In the Text-to-Question generation task, a QG system is given a text, and the goal is to generate a set of questions for which the text contains answers. The task of generating a question about a given text can be typically decomposed into three subtasks. First, given the source text, a content selection step is necessary to select a target to ask about, such as the desired answer. Second, given a target answer, an appropriate question type is selected (i.e., the form of question to ask is determined). Third, given the content and question type, the actual question is </context>
<context position="17315" citStr="Rus and Graesser 2009" startWordPosition="2640" endWordPosition="2643">o feel that there should be a similarity between the words that are dependent in the sentences present in the associated body of texts and the dependency between words of the generated question. This motivates us to propose the use of syntactic kernels in judging the syntactic correctness of the generated questions automatically. The main goal of our work is to generate as many questions as possible related to the topic. We use the named entity information and the predicate argument structures of the sentences to accomplish this goal. Our approach is different from the set-up in shared tasks (Rus and Graesser 2009; Boyer and Piwek 2010), as we generate a set of basic questions that are useful to add variety in the question space. A paragraph associated with each topic is used as the source of relevant information about the topic. We evaluate our systems in terms of topic relevance, which is different from prior research (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010). Syntactic correctness is also an important property of a good question. For this reason, we evaluate our system in terms of syntactic correctness as well. The proposed system will be useful for generating topic-related questions</context>
</contexts>
<marker>Rus, Graesser, 2009</marker>
<rawString>Rus, V. and A. C. Graesser. 2009. The question generation shared task and evaluation challenge. In Workshop on the Question Generation Shared Task and Evaluation Challenge, Final Report, pages 1–37, University of Memphis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Simmons</author>
</authors>
<title>Answering English questions by computer: A survey.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="3944" citStr="Simmons 1965" startWordPosition="585" endWordPosition="586">, information extraction, machine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) syst</context>
</contexts>
<marker>Simmons, 1965</marker>
<rawString>Simmons, R. F. 1965. Answering English questions by computer: A survey. Communications of the ACM, 8(1):53–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>S Harabagiu</author>
</authors>
<date>2008</date>
<booktitle>Advances in Open Domain Question Answering.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="3504" citStr="Strzalkowski and Harabagiu 2008" startWordPosition="518" endWordPosition="521"> search task is usually not over (Chali, Joty, and Hasan 2009). The next step for the user is to look into the documents themselves and search for the precise piece of information they were looking for. This method is time-consuming, and a correct answer could easily be missed by either an incorrect query resulting in missing documents or by careless reading. This is why Question Answering (QA) has received immense attention from the information retrieval, information extraction, machine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA </context>
</contexts>
<marker>Strzalkowski, Harabagiu, 2008</marker>
<rawString>Strzalkowski, T. and S. Harabagiu, 2008. Advances in Open Domain Question Answering. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
</authors>
<title>The importance of being important: Question generation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge,</booktitle>
<location>Arlington, VA.</location>
<contexts>
<context position="14461" citStr="Vanderwende 2008" startWordPosition="2195" endWordPosition="2196"> templates from existing questions, which requires a large set of English questions as training data. In recent years, some other related researchers have proposed the tasks of high-quality question generation (Ignatova, Bernhard, and Gurevych 2008) and generating questions from queries (Lin 2008). Fact-based question generation has been accomplished previously (Rus, Cai, and Graesser 2007; Heilman and Smith 2010b). We also focus on generating fact-based questions in this research. Besides grammaticality, an effective QG system should focus deeply on the importance of the generated questions (Vanderwende 2008). This motivates the use of a question-ranking module in a typical QG system. Over-generated questions can be ranked using different approaches, such as statistical ranking methods, dependency parsing, identification of the presence of pronouns and named entities, and topic scoring (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010; McConnell et al. 2011). 5 http://www.questiongeneration.org/QGSTEC2010. 4 Chali and Hasan Towards Topic-to-Question Generation However, most of these automatic ranking approaches ignore the aspects of complex paraphrasing by not considering lexical semantic v</context>
</contexts>
<marker>Vanderwende, 2008</marker>
<rawString>Vanderwende, L. 2008. The importance of being important: Question generation. In Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>1999</date>
<booktitle>In Proceedings of the 8th Text REtreival Conference,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="3972" citStr="Voorhees 1999" startWordPosition="589" endWordPosition="590">achine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) systems that can assist in meeti</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Voorhees, E. M. 1999. Overview of the TREC 1999 question answering track. In Proceedings of the 8th Text REtreival Conference, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wagner</author>
<author>J Foster</author>
<author>J van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Wagner, J., J. Foster, and J. van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal, 26(3):474–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
</authors>
<title>A survey of answer extraction techniques in factoid question answering.</title>
<date>2006</date>
<booktitle>In CMU 11-762 Language and Statistics II, literature review project.</booktitle>
<contexts>
<context position="4030" citStr="Wang 2006" startWordPosition="597" endWordPosition="598">n the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, definition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would benefit from automated Question Generation (QG) systems that can assist in meeting their inquiry needs (Lauer, Peacock, and Graesser 1992;</context>
</contexts>
<marker>Wang, 2006</marker>
<rawString>Wang, M. 2006. A survey of answer extraction techniques in factoid question answering. In CMU 11-762 Language and Statistics II, literature review project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>H Tianyong</author>
<author>L Wenyin</author>
</authors>
<title>Automatic question generation for learning evaluation in medicine.</title>
<date>2008</date>
<booktitle>In 6th International Conference on Advances in Web Based Learning,</booktitle>
<pages>242--251</pages>
<location>Edinburgh.</location>
<contexts>
<context position="10439" citStr="Wang et al. (2008)" startWordPosition="1601" endWordPosition="1604">tically generating topical questions. The remainder of the article is organized as follows. Section 2 describes the related work. Section 3 presents the description of our QG system. Section 4 explains the experiments and shows evaluation results; Section 5 concludes. 2. Related Work Recently, question generation has received immense attention from researchers and different methods have been proposed to accomplish the task in different relevant fields (Andrenucci and Sneiders 2005). McGough et al. (2001) proposed an approach to build a Web-based testing system with the facility of dynamic QG. Wang et al. (2008) showed 2 http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html. 3 This article is a longer version of our previously published work (Chali and Hasan 2012c). We provide more theoretical descriptions and analyses, and conduct our experiments on a larger data set to report new results. 4 We mainly focus on generating who, what, where, which, when, why, and how questions in this research. 3 Computational Linguistics Volume 41, Number 1 a method to automatically generate questions based on question templates (which are created from training on medical articles). Brown, Frishk</context>
</contexts>
<marker>Wang, Tianyong, Wenyin, 2008</marker>
<rawString>Wang, W., H. Tianyong, and L. Wenyin. 2008. Automatic question generation for learning evaluation in medicine. In 6th International Conference on Advances in Web Based Learning, pages 242–251, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wei</author>
<author>W B Croft</author>
</authors>
<title>LDA-based document models for ad-hoc retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’06,</booktitle>
<pages>178--185</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="15558" citStr="Wei and Croft 2006" startWordPosition="2354" endWordPosition="2357">st of these automatic ranking approaches ignore the aspects of complex paraphrasing by not considering lexical semantic variations (e.g., synonymy) when measuring the importance of the questions. In our work, we use LDA (Blei, Ng, and Jordan 2003) to identify the subtopics (which are closely related to the original topic) in the given body of texts. We choose LDA because in recent years it has become one of the most popular topic modeling techniques and has been shown to be effective in several text-related tasks, such as document classification, information retrieval, and question answering (Wei and Croft 2006; Misra, Capp´e, and Yvon 2008; Celikyilmaz, HakkaniTur, and Tur 2010). Once we have the subtopics, we apply ESSK (Hirao et al. 2003) to calculate their similarity with the generated questions. The choice of ESSK is motivated by its successful use in different NLP tasks in recent years (Chali, Hasan, and Joty 2009, 2011; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we ap</context>
</contexts>
<marker>Wei, Croft, 2006</marker>
<rawString>Wei, X. and W. B. Croft. 2006. LDA-based document models for ad-hoc retrieval. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’06, pages 178–185, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zhang</author>
<author>W Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Special Interest Group on Information Retrieval,</booktitle>
<pages>26--32</pages>
<location>Toronto.</location>
<contexts>
<context position="16476" citStr="Zhang and Lee 2003" startWordPosition="2500" endWordPosition="2503">, and Joty 2009, 2011; Chali and Hasan 2012a, 2012b). Hirao et al. (2003) introduced ESSK considering all possible senses of each word to perform their summarization task. Their method is effective. However, the fact that they do not disambiguate word senses cannot be disregarded. In our task, we apply ESSK to calculate the similarity between important topics (discovered using LDA) and the generated questions in order to measure the importance of each question. We use disambiguated word senses for this purpose. Syntactic information has previously been used successfully in question answering (Zhang and Lee 2003; Moschitti and Basili 2006; Moschitti et al. 2007; Chali, Hasan, and Joty 2009, 2011). Pasca and Harabagiu (2001) argued that with the syntactic form of a sentence one can see which words depend on other words. We also feel that there should be a similarity between the words that are dependent in the sentences present in the associated body of texts and the dependency between words of the generated question. This motivates us to propose the use of syntactic kernels in judging the syntactic correctness of the generated questions automatically. The main goal of our work is to generate as many q</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Zhang, A. and W. Lee. 2003. Question classification using support vector machines. In Proceedings of the Special Interest Group on Information Retrieval, pages 26–32, Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zheng</author>
<author>X Si</author>
<author>E Y Chang</author>
<author>X Zhu</author>
</authors>
<title>K2Q: Generating natural language questions from keywords with user refinements.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>947--955</pages>
<location>Chiang Mai.</location>
<contexts>
<context position="13714" citStr="Zheng et al. (2011)" startWordPosition="2086" endWordPosition="2089">is constructed. Based on this principle, several approaches have been described in Boyer and Piwek (2010) that use named entity information, syntactic knowledge, and semantic structures of the sentences to perform the task of generating questions from sentences and paragraphs (Heilman and Smith 2010a; Mannem, Prasad, and Joshi 2010). Inspired by these works, we perform the task of topic-to-question generation using named entity information and semantic structures of the sentences. A task that is similar to ours is the task of keywords-to-question generation that has been addressed recently in Zheng et al. (2011). They propose a user model for jointly generating keywords and questions. However, their approach is based on generating question templates from existing questions, which requires a large set of English questions as training data. In recent years, some other related researchers have proposed the tasks of high-quality question generation (Ignatova, Bernhard, and Gurevych 2008) and generating questions from queries (Lin 2008). Fact-based question generation has been accomplished previously (Rus, Cai, and Graesser 2007; Heilman and Smith 2010b). We also focus on generating fact-based questions i</context>
</contexts>
<marker>Zheng, Si, Chang, Zhu, 2011</marker>
<rawString>Zheng, Z., X. Si, E. Y. Chang, and X. Zhu. 2011. K2Q: Generating natural language questions from keywords with user refinements. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 947–955, Chiang Mai.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>