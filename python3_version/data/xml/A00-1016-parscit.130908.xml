<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9981795">
A Compact Architecture for Dialogue Management Based on
Scripts and Meta-Outputs
</title>
<author confidence="0.768932">
Manny Rayner, Beth Ann Hockey, Frankie James
</author>
<note confidence="0.648115333333333">
Research Institute for Advanced Computer Science
Mail Stop 19-39, NASA Ames Research Center
Moffett Field, CA 94035-1000
</note>
<email confidence="0.993865">
Imanny,bahockey,fjamesl@riacs.edu
</email>
<sectionHeader confidence="0.993574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806545454546">
We describe an architecture for spoken dialogue
interfaces to semi-autonomous systems that trans-
forms speech signals through successive representa-
tions of linguistic, dialogue, and domain knowledge.
Each step produces an output, and a meta-output
describing the transformation, with an executable
program in a simple scripting language as the fi-
nal result. The output/meta-output distinction per-
mits perspicuous treatment of diverse tasks such as
resolving pronouns, correcting user misconceptions,
and optimizing scripts.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960535714286">
The basic task we consider in this paper is that of
using spoken language to give commands to a semi-
autonomous robot or other similar system. As ev-
idence of the importance of this task in the NLP
community note that the early, influential system
SHRDLU (Winograd, 1973) was intended to address
just this type of problem. More recent work on spo-
ken language interfaces to semi-autonomous robots
include SRI&apos;s Flakey robot (Konolige et al., 1993)
and NCARAI&apos;s InterBOT project (Perzanowski et
al., 1998; Perzanowski et al., 1999). A number of
other systems have addressed part of the task. Corn-
mandTalk (Moore et al., 1997), Circuit Fix-It Shop
(Smith, 1997) and TRAINS-96 (Traum and Allen,
1994; Traum and Andersen, 1999) are spoken lan-
guage systems but they interface to simulation or
help facilities rather than semi-autonomous agents.
Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text
rather than speech as natural language input and the
avatars being controlled are not semi-autonomous.
Other researchers have considered particular aspects
of the problem such as accounting for various aspects
of actions (Webber, 1995; Pym et al., 1995). In most
of this and other related work the treatment is some
variant of the following. If there is a speech inter-
face, the input speech signal is converted into text.
Text either from the recognizer or directly input by
the user is then converted into some kind of logi-
cal formula, which abstractly represents the user&apos;s
intended command; this formula is then fed into a
command interpreter, which executes the command.
We do not think the standard treatment outlined
above is in essence incorrect, but we do believe that,
as it stands, it is in need of some modification. This
paper will in particular make three points. First, we
suggest that the output representation should not be
regarded as a logical expression, but rather as a pro-
gram in some kind of scripting language. Second, we
argue that it is not merely the case that the process
of converting the input signal to the final represen-
tation can sometimes go wrong; rather, this is the
normal course of events, and the interpretation pro-
cess should be organized with that assumption in
mind. Third, we claim, perhaps surprisingly, that
the first and second points are related. These claims
are elaborated in Section 2.
The remainder of the paper describes an archi-
tecture which addresses the issues outlined above,
and which has been used to implement a prototype
speech interface to a simulated semi-autonomous
robot intended for deployment on the International
Space Station. Sections 3 and 4 present an overview
of the implemented interface, focussing on represen-
tational issues relevant to dialogue management. Il-
lustrative examples of interactions with the system
are provided in Section 5. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="introduction">
2 Theoretical Ideas
</sectionHeader>
<subsectionHeader confidence="0.99897">
2.1 Scripts vs Logical Forms
</subsectionHeader>
<bodyText confidence="0.999932">
Let&apos;s first look in a little more detail at the question
of what the output representation of a spoken lan-
guage interface to a semi-autonomous robot/agent
should be. In practice, there seem to be two main
choices: atheoretical representations, or some kind
of logic.
Logic is indeed an excellent way to think
about representing static relationships like database
queries, but it is much less clear that it is a good way
to represent commands. In real life, when people
wish to give a command to a computer, they usu-
ally do so via its operating system; a complex com-
mand is an expression in a scripting language like
CSHELL, Peri., or VBScript. These languages are
related to logical formalisms, but cannot be mapped
</bodyText>
<page confidence="0.994937">
112
</page>
<listItem confidence="0.953605421052632">
onto them in a simple way. Here are some of the
obvious differences:
• A scripting language is essentially imperative,
rather than relational.
• The notion of temporal sequence is fundamental
to the language. &amp;quot;Do P and then Q&amp;quot; is not the
same as &amp;quot;Make the goals P and Q true&amp;quot;; it is
explicitly stated that P is to be done first. Simi-
larly, &amp;quot;For each X in the list (A B C), do P(X)&amp;quot;
is not the same as &amp;quot;For all X, make P(X) true&amp;quot;;
once again, the scripting language defines an or--
der, but not the logical language&apos;.
• Scripting languages assume that commands do
not always succeed. For example, UNIX-based
scripting languages like CSHELL provide each
script with the three predefined streams stdin,
stdout and stderr. Input is read from stdin
and written to stdout; error messages, warn-
ings and other comments are sent to stderr.
</listItem>
<bodyText confidence="0.998864555555556">
We do not think that these properties of scripting
language are accidental. They have evolved as the
result of strong selectional pressure from real users
with real-world tasks that need to be carried out,
and represent a competitive way to meet said users&apos;
needs. We consequently think it is worth taking seri-
ously the idea that a target representation produced
by a spoken language interface should share many of
these properties.
</bodyText>
<subsectionHeader confidence="0.9986075">
2.2 Fallible Interpretation: Outputs and
Meta-outputs
</subsectionHeader>
<bodyText confidence="0.995639914893617">
We now move on to the question of modelling the in-
terpretation process, that is to say the process that
converts the input (speech) signal to the output (ex-
ecutable) representation. As already indicated, we
think it is important to realize that interpretation
is a process which, like any other process, may suc-
ceed more or less well in achieving its intended goals.
Users may express themselves unclearly or incom-
pletely, or the system may more or less seriously
fail to understand exactly what they mean. A good
interpretation architecture will keep these consider-
ations in mind.
Taking our lead from the description of scripting
languages sketched above, we adapt the notion of
the &amp;quot;error stream&amp;quot; to the interpretation process. In
the course of interpreting an utterance, the system
translates it into successively &amp;quot;deeper&amp;quot; levels of rep-
resentation. Each translation step has not only an
input (the representation consumed) and an output
&apos;In cases like these, the theorem prover or logic program-
ming interpreter used to evaluate the logical formula typically
assigns a conventional order to the conjuncts; note however
that this is part of the procedural semantics of the theorem
prover/interpreter, and does not follow from the declarative
semantics of the logical formalism.
(the representation produced), but also something
we will refer to as a &amp;quot;meta-output&amp;quot;: this provides in-
formation about how the translation was performed.
At a high level of abstraction, our architecture will
be as follows. Interpretation proceeds as a series
of non-deterministic translation steps, each produc-
ing a set of possible outputs and associated meta-
outputs. The final translation step produces an ex-
ecutable script. The interface attempts to simulate
execution of each possible script produced, in or-
der to determine what would happen if that script
were selected; simulated execution can itself produce
further meta-outputs. Finally, the system uses the
meta-output information to decide what to do with
the various possible interpretations it has produced.
Possible actions include selection and execution of
an output script, paraphrasing meta-output infor-
mation back to the user, or some combination of the
two.
In the following section, we present a more de-
tailed description showing how the output/meta-
output distinction works in a practical system.
</bodyText>
<sectionHeader confidence="0.991353" genericHeader="method">
3 A Prototype Implementation
</sectionHeader>
<bodyText confidence="0.99995575">
The ideas sketched out above have been realized as
a prototype spoken language dialogue interface to a
simulated version of the Personal Satellite Assistant
(PSA; (PSA, 2000)). This section gives an overview
of the implementation; in the following section, we
focus on the specific aspects of dialogue management
which are facilitated by the output/meta-output ar-
chitecture.
</bodyText>
<subsectionHeader confidence="0.999976">
3.1 Levels of Representation
</subsectionHeader>
<bodyText confidence="0.998250739130435">
The real PSA is a miniature robot currently being
developed at NASA Ames Research Center, which
is intended for deployment on the Space Shuttle
and/or International Space Station. It will be ca-
pable of free navigation in an indoor micro-gravity
environment, and will provide mobile sensory capac-
ity as a backup to a network of fixed sensors. The
PSA will primarily be controlled by voice commands
through a hand-held or head-mounted microphone,
with speech and language processing being handled
by an offboard processor. Since the speech process-
ing units are not in fact physically connected to the
PSA we envisage that they could also be used to con-
trol or monitor other environmental functions. In
particular, our simulation allows voice access to the
current and past values of the fixed sensor readings.
The initial PSA speech interface demo consists of
a simple simulation of the Shuttle. State parame-
ters include the PSA&apos;s current position, some envi-
ronmental variables such as local temperature, pres-
sure and carbon dioxide levels, and the status of the
Shuttle&apos;s doors (open/closed). A visual display gives
direct feedback on some of these parameters.
</bodyText>
<page confidence="0.995587">
113
</page>
<bodyText confidence="0.934191672413793">
The speech and language processing architecture
is based on that of the SRI CommandTalk sys-
tem (Moore et al., 1997; Stent et a., 1999). The sys-
tem comprises a suite of about 20 agents, connected
together using the SRI Open Agent Architecture
(OAA; (Martin et al., 1998)). Speech recognition
is performed using a version of the Nuance recog-
nizer (Nuance, 2000). Initial language processing is
carried out using the SRI Gemini system (Dowding
et al., 1993), using a domainAndependent unification
grammar and a domain-specific lexicon. The lan-
guage processing grammar is compiled into a recog-_
nition grammar using the methods of (Moore et al.,
1997); the net result is that only grammatically well-
formed utterances can be recognized. Output from
the initial language-processing step is represented
in a version of Quasi Logical Form (van Eijck and
Moore, 1992), and passed in that form to the dia-
logue manager. We refer to these as linguistic level
representations.
The aspects of the system which are of primary in-
terest here concern the dialogue manager (DM) and
related modules. Once a linguistic level represen-
tation has been produced, the following processing
steps occur:
• The linguistic level representation is converted
into a discourse level representation. This pri-
marily involves regularizing differences in sur-
face form: so, for example, &amp;quot;measure the pres-
sure&amp;quot; and &amp;quot;what is the pressure?&amp;quot; have differ-
ent representations at the linguistic level, but
the same representation at the discourse level.
• If necessary, the system attempts to resolve in-
stances of ellipsis and anaphoric reference. For
example, if the previous command was &amp;quot;mea-
sure temperature at flight deck&amp;quot;, then the new
command &amp;quot;lower deck&amp;quot; will be resolved to an
expression meaning &amp;quot;measure temperature at
lower deck&amp;quot;. Similarly, if the previous command
was &amp;quot;move to the crew hatch&amp;quot;, then the com-
mand &amp;quot;open it&amp;quot; will be resolved to &amp;quot;open the
crew hatch&amp;quot;. We call the output of this step a
resolved discourse level representation.
• The resolved discourse level representation is
converted into an executable script in a lan-
guage essentially equivalent to a subset of
CSHELL. This involves two sub-steps. First,
quantified variables are given scope: for exam-
ple, &amp;quot;go to the flight deck and lower deck and
measure pressure&amp;quot; becomes something approxi-
mately equivalent to the script
foreach x (flight_deck lower_deck)
go_to $x
measure pressure
end
The point to note here is that the foreach has
scope over both the go_to and the measure ac-
tions; an alternate (incorrect) scoping would be
</bodyText>
<table confidence="0.3280955">
foreach x (flight_deck lower_deck)
go_to $x
end
measure pressure
</table>
<bodyText confidence="0.999570285714286">
The second sub-step is to attempt to optimize
the plan. In the current example, this can
be done by reordering the list (flight_deck
lower_deck). For instance, if the PSA is al-
ready at the lower deck, reversing the list will
mean that the robot only makes one trip, in-
stead of two.
</bodyText>
<listItem confidence="0.9462593">
• The final step in the interpretation process is
plan evaluation: the system tries to work out
what will happen if it actually executes the
plan. (The relationship between plan evaluation
and plan execution is described in more detail
in Section 4.1). Among other things, this gives
the dialogue manager the possibility of compar-
ing different interpretations of the original com-
mand, and picking the one which is most effi-
cient.
</listItem>
<subsectionHeader confidence="0.997075">
3.2 How Meta-outputs Participate in the
Translation
</subsectionHeader>
<bodyText confidence="0.999834">
The above sketch shows how context-dependent
interpretation is arranged as a series of non-
deterministic translation steps; in each case, we have
described the input and the output for the step in
question. We now go back to the concerns of Sec-
tion 2. First, note that each translation step is in
general fallible. We give several examples:
</bodyText>
<listItem confidence="0.568077166666667">
• One of the most obvious cases arises when the
user simply issues an invalid command, such as
requesting the PSA to open a door D which is
already open. Here, one of the meta-outputs
issued by the plan evaluation step will be the
term
</listItem>
<bodyText confidence="0.915183923076923">
pre suppos it i on_f ailure ( already _open CD)) ;
the DM can decide to paraphrase this back to
the user as a surface string of the form &amp;quot;D is
already open&amp;quot;. Note that plan evaluation does
not involve actually executing the final script,
which can be important. For instance, if the
command is &amp;quot;go to the crew hatch and open it&amp;quot;
and the crew hatch is already open, the interface
has the option of informing the user that there
is a problem without first carrying out the &amp;quot;go
to&amp;quot; action.
• The resolution step can give rise to similar kinds
of meta-output. For example, a command may
</bodyText>
<page confidence="0.99544">
114
</page>
<bodyText confidence="0.940337130434783">
include a referring expression that has no deno-
tation, or an ambiguous denotation; for exam-
ple, the user might say &amp;quot;both decks&amp;quot;, presum-
ably being unaware that there are in fact three
of them. This time, the meta-output produced
is
presuppos it ion_failure (
incorrect_size_of _set (2,3) )
representing the user&apos;s incorrect belief about
the number of decks. The DM then has the pos-
sibility of informing the user of this misconcep:
tion by realizing the meta-output term as the
surface string &amp;quot;in fact there are three of them&amp;quot;.
Ambiguous denotation occurs when a descrip-
tion is under-specified. For instance, the user
might say &amp;quot;the deck&amp;quot; in a situation where there
is no clearly salient deck, either in the discourse
situation or in the simulated world: here, the
meta-output will be
presupposition_f ailure (
underspecified_definite(deck))
which can be realized as the clarification ques-
tion &amp;quot;which deck do you mean?&amp;quot;
</bodyText>
<listItem confidence="0.995864714285714">
• A slightly more complex case involves plan
costs. During plan evaluation, the system simu-
lates execution. of the output script while keep-
ing track of execution cost. (Currently, the cost
is just an estimate of the time required to exe-
cute the script). Execution costs are treated as
meta-outputs of the form
</listItem>
<subsectionHeader confidence="0.487902">
cost (C)
</subsectionHeader>
<bodyText confidence="0.62271">
and passed back through the interpreter so that
the plan optimization step can make use of
them.
</bodyText>
<listItem confidence="0.851709461538462">
• Finally, we consider what happens when the
system receives incorrect input from the speech
recognizer. Although the recognizer&apos;s language
model is constrained so that it can only pro-
duce grammatical utterances, it can still misrec-
ognize one grammatical string as another one.
Many of these cases fall into one of a small
number of syntactic patterns, which function as
fairly reliable indicators of bad recognition. A
typical example is conjunction involving a pro-
noun: if the system hears &amp;quot;it and flight deck&amp;quot;,
this is most likely a misrecognition of something
like &amp;quot;go to flight deck&amp;quot;.
</listItem>
<bodyText confidence="0.996579615384615">
During the processing phase which translates
linguistic level representations into discourse
level representations, the system attempts to
match each misrecognition pattern against the
input linguistic form, and if successful produces
a meta-output of the form
presuppos it ion_f ailure (
dubious_lf(&lt;Type&gt;))
These meta-outputs are passed down to the
DM, which in the absence of sufficiently com-
pelling contrary evidence will normally issue a
response of the form &amp;quot;I&apos;m sorry, I think I mis-
heard you&amp;quot;.
</bodyText>
<sectionHeader confidence="0.828595333333333" genericHeader="method">
4 A Compact Architecture for
Dialogue Management Based on
Scripts and Meta-Outputs
</sectionHeader>
<bodyText confidence="0.99994825">
None of the individual functionalities outlined above
are particularly novel in themselves. What we find
new and interesting is the fact that they can all
be expressed in a uniform way in terms of the
script output/meta-output architecture. This sec-
tion presents three examples illustrating how the ar-
chitecture can be used to simplify the overall orga-
nization of the system.
</bodyText>
<subsectionHeader confidence="0.957195">
4.1 Integration of plan evaluation, plan
execution and dialogue management
</subsectionHeader>
<bodyText confidence="0.99991536">
Recall that the DM simulates evaluation of the plan
before running it, in order to obtain relevant meta-
information. At plan execution time, plan actions
result in changes to the world; at plan evaluation
time, they result in simulated changes to the world
and/or produce meta-outputs.
Conceptualizing plans as scripts rather than log-
ical formulas permits an elegant treatment of the
execution/evaluation dichotomy. There is one script
interpreter, which functions both as a script exec-
utive and a script evaluator, and one set of rules
which defines the procedural semantics of script ac-
tions. Rules are parameterized by execution type
which is either &amp;quot;execute&amp;quot; or &amp;quot;evaluate&amp;quot;. In &amp;quot;evalu-
ate&amp;quot; mode, primitive actions modify a state vector
which is threaded through the interpreter; in &amp;quot;ex-
ecute&amp;quot; mode, they result in commands being sent
to (real or simulated) effector agents. Conversely,
&amp;quot;meta-information&amp;quot; actions, such as presupposition
failures, result in output being sent to the meta-
output stream in &amp;quot;evaluate&amp;quot; mode, and in a null ac-
tion in &amp;quot;execute&amp;quot; mode. The upshot is that a simple
semantics can be assigned to rules like the following
one, which defines the action of attempting to open
a door which may already be open:
</bodyText>
<construct confidence="0.8331364">
procedure(
open_door CD),
if _then_else (status (D, open_closed, open) ,
presupposition_failure (already_open(D) )
change_ status (D, open_closed, open) ) )
</construct>
<subsectionHeader confidence="0.850186">
4.2 Using meta-outputs to choose between
interpretations
</subsectionHeader>
<bodyText confidence="0.997134">
As described in the preceding section, the resolution
step is in general non-deterministic and gives rise to
</bodyText>
<page confidence="0.998037">
115
</page>
<bodyText confidence="0.9999932">
meta-outputs which describe the type of resolution
carried out. For example, consider a command in-
volving a definite description, like &amp;quot;open the door&amp;quot;.
Depending on the preceding context, resolution will
produce a number of possible interpretations; &amp;quot;the
door&amp;quot; may be resolved to one or more contextually
available doors, or the expression may be left un-
resolved. In each case, the type of resolution used
appears as a meta-output, and is available to the di-
alogue manager when it decides which interpretation
is most felicitous. By default, the DM&apos;s strategy is to
attempt to supply antecedents for referring expres-,.
sions, preferring the most recently occurring sortally
appropriate candidate. In some cases, however, it is
desirable to allow the default strategy to be over-
ridden: for instance, it may result in a script which
produces a presupposition failure during plan eval-
uation. Treating resolution choices and plan evalu-
ation problems as similar types of objects makes it
easy to implement this kind of idea.
</bodyText>
<subsectionHeader confidence="0.9531725">
4.3 Using meta-outputs to choose between
dialogue management moves
</subsectionHeader>
<bodyText confidence="0.999858517241379">
Perhaps the key advantage of our architecture is that
collecting together several types of information as a
bag of meta-outputs simplifies the top-level struc-
ture of the dialogue manager. In our application,
the critical choice of dialogue move comes after the
dialogue manager has selected the most plausible in-
terpretation. It now has to make two choices. First,
it must decide whether or not to paraphrase any of
the meta-outputs back to the user; for example, if
resolution was unable to fill some argument posi-
tion or find an antecedent for a pronoun, it may be
appropriate to paraphrase the corresponding meta-
output as a question, e.g. &amp;quot;where do you mean?&amp;quot;, or
&amp;quot;what do you mean by &apos;it&apos;?&amp;quot;. Having all the meta-
outputs available together means that the DM is
able to plan a coherent response: so if there are sev-
eral meta-outputs which could potentially be worth
paraphrasing, it typically realizes only the most im-
portant one.
Second, if interpretation was able to produce a
well-formed plan, the DM currently has the three
options of executing it, paraphrasing it back to the
user as a confirmation question, or doing nothing.
Once again, this decision often requires global infor-
mation about what has happened during the inter-
pretation process. For example, knowing that plan
evaluation showed that the plan would take signif-
icant time to execute makes it more plausible that
the user would prefer to receive a confirmation.
</bodyText>
<sectionHeader confidence="0.996405" genericHeader="method">
5 Examples
</sectionHeader>
<bodyText confidence="0.997314090909091">
This section presents a commented sample dialogue
with the implemented PSA interface demo chosen
to illustrate the points discussed in the previous sec-
tions. We focus in particular on the role that meta-
outputs play in interpretation. Text preceded by
USER represents spoken utterances from the user.
Text preceded by PSA represents synthesized spoken
responses from the PSA. Text inside square brackets
describes the actions carried out by the PSA.
1. The PSA can move to various locations and
measure environmental factors. In the first ex-
change, reference resolution is used to interpret
the phrase &amp;quot;all three decks&amp;quot; as referring to the
lower deck, mid deck and flight deck. Plan op-
timization finds the most efficient way to or-
der the locations, and paraphrases the resulting
plan back to the user.
USER: Go to all three decks and measure car-
bon dioxide.
PSA: I will move to flight deck, mid deck and
then lower deck and I will measure carbon
dioxide level, okay?
</bodyText>
<subsubsectionHeader confidence="0.522118">
USER: Okay. [PSA moves to flight deck]
</subsubsectionHeader>
<bodyText confidence="0.956644032258065">
PSA: The carbon dioxide level at the flight
deck is one percent. [PSA moves to mid
deck]
PSA: The carbon dioxide level at the mid deck
is one percent. [PSA moves to lower deck]
PSA: The carbon dioxide level at the lower
deck is one percent.
2. The next example involves resolution of ellip-
sis. The relevant context is recovered from the
preceding dialogue.
USER: Do the same for the pilot&apos;s seat.
PSA: I will move to the pilot&apos;s seat and I will
measure carbon dioxide level, okay?
USER: Right. [PSA moves to pilot&apos;s seat]
PSA: The carbon dioxide level at the pilot&apos;s
seat is one percent.
3. Resolution of ellipsis can involve filling in un-
specified semantic parameters to a syntactically
complete command. It is not meaningful to talk
about a sensor reading without reference to the
place at which that reading was carried out; in
the following example, resolution assumes that
the location in question is the one most recently
mentioned in the dialogue.
Note also that since the PSA does not have to
move in order to carry out the command, and
hence can execute it immediately, the DM rea-
sonably decides to skip the confirmation ques-
tion. This decision is made based on the meta-
output encoding the estimated cost of executing
the script.
</bodyText>
<page confidence="0.996687">
116
</page>
<bodyText confidence="0.971527428571428">
USER: What was the carbon dioxide level at
fifteen oh five according to the fixed sen-
sors?
PSA: According to the fixed sensors, at fifteen
oh five the carbon dioxide level at the pi-
lot&apos;s seat was one percent.
4. The system encounters a user misconception,
which results in a &amp;quot;presupposition failure&amp;quot;
meta-output being produced during reference
resolution. It consequently decides to alert the
user to this, rather than attempt to carry out
the command.
USER: Close both doors.
PSA: There are in fact three of them.
5. The user utterance includes a pronoun, &amp;quot;it&amp;quot;.
The system resolves &amp;quot;it&amp;quot; to &amp;quot;the crew hatch&amp;quot;,
and issues an appropriate paraphrase in order
to confirm that this is what the user meant.
USER: Go to crew hatch and close it.
PSA: I will go to crew hatch and I will close
crew hatch, okay?
</bodyText>
<construct confidence="0.5538905">
USER: Yeah. [PSA moves to crew hatch and
closes it]
</construct>
<bodyText confidence="0.97370504">
6. The user asks the system to close the door, but
it is unclear which door they are referring to.
An &amp;quot;ambiguous reference&amp;quot; meta-output is pro-
duced by reference resolution, resulting in the
DM issuing a clarification question.
USER: Close the door.
PSA: Which door do you mean?
7. The user response is successfully interpreted by
reference resolution as an instance of ellipsis.
Trying to close the crew hatch a second time
however causes a presupposition failure meta,
output to be generated during plan evaluation.
The system informs the user of this and takes
no action.
USER: The crew hatch.
PSA: The crew hatch is already closed.
8. The user presents three locations in a subopti-
mal order. Plan evaluation finds a more efficient
ordering.
USER: Move to storage lockers, commander&apos;s
seat and flight deck and measure tempera-
ture.
PSA: I will move to flight deck, commander&apos;s
seat and then storage lockers and I will
measure temperature, okay?
</bodyText>
<sectionHeader confidence="0.527316" genericHeader="method">
USER: Sure.
</sectionHeader>
<bodyText confidence="0.977305142857143">
[PSA moves to flight deck, commander&apos;s
seat and storage lockers in that order and
reports temperature at each location]
9. The user asks the system to repeat the previous
command. Since the PSA is now starting from
a different position, its most efficient plan is no
longer the same, and it informs the user of this.
</bodyText>
<sectionHeader confidence="0.613765" genericHeader="method">
USER: Do that again.
</sectionHeader>
<bodyText confidence="0.997426">
PSA: I will move to storage lockers, comman-
der&apos;s seat and then flight deck and I will
measure temperature, okay?
</bodyText>
<subsubsectionHeader confidence="0.464742">
USER: Yes. [PSA moves to storage lockers]
</subsubsectionHeader>
<bodyText confidence="0.98502325">
PSA: The temperature at the storage lockers
is 19.9 degrees Celsius. [PSA starts moving
to commander&apos;s seat]
10. The user interrupts the PSA in the middle of the
plan, and asks it to go back to its last stopping
place. Since &amp;quot;stop&amp;quot; may involve a potentially
hazardous condition, the interface obeys imme-
diately, without attempting to confirm first.
</bodyText>
<sectionHeader confidence="0.75793" genericHeader="method">
USER: Stop. [PSA stops]
</sectionHeader>
<reference confidence="0.2511305">
USER: Go back. [PSA returns to storage lock-
ers]
</reference>
<sectionHeader confidence="0.984056" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999994666666667">
We have described a generic architecture for spoken
language dialogue interfaces to semi-autonomous
agents, based on the standard notion of translating
to successively deeper levels of representation. The
novel aspects of our scheme center around two ideas:
first, that the final output representations are best
conceptualized not as logical expressions but rather
as programs in a scripting language; second, that
steps in the translation process should produce not
only a simple output, but also meta-information de-
scribing how the output was produced. We have pre-
sented examples suggesting how several apparently
diverse types of dialogue behavior can be captured
simply within our framework, and outlined a proto-
type implementation of the scheme.
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993051">
N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck,
J. Shi, and M. Palmer. 1999. Real time virtual
humans. In International Conference on Digital
Media Futures.
J. Dowding, M. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A nat-
ural language system for spoken language un-
derstanding. In Proceedings of the Thirty-First
Annual Meeting of the Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.981873">
117
</page>
<reference confidence="0.999706462686567">
K. Konolige, K. Myers, E. Ruspini, and A. Saf-
fiotti. 1993. Flakey in action: The 1992 AAAI
robot competition. Technical Report SRI Techni-
cal Note 528, SRI, Al Center, SRI International,
333 Ravenswood Ave., Menlo Park, CA 94025.
D. Martin, A. Cheyer, and D. Moran. 1998. Build-
ing distributed software systems with the open
agent architecture. In Proceedings of the Third
International Conference on the Practical Appli-
cation of Intelligent Agents and Multi-Agent Tech-
nology.
R. Moore, J. Dowding, H. Bratt, J. Gawron, •
Y. Gorfu, and A. Cheyer. 1997. CommandTalk:
A spoken-language interface for battlefield simu-
lations. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 1-7.
Nuance, 2000. Nuance Communications, Inc.
http://www.nuance.com. As of 9 March 2000.
D. Perzanowski, A. Schultz, and W. Adams. 1998.
Integrating natural language and gesture in a
robotics domain. In IEEE International Sympo-
sium on Intelligent Control: ISIC/CIRA/ISAS
Joint Conference, pages 247-252, Gaithersburg,
MD: National Institute of Standards and Tech-
nology.
D. Perzanowski, A. Schultz, W. Adams, and
E. Marsh. 1999. Goal tracking in a natural lan-
guage interface: Towards achieving adjustable au-
tonomy. In ISIS/CIRA99 Conference, Monterey,
CA. IEEE.
PSA, 2000. Personal Satellite Assistant (PSA)
Project. http://ic.arc.na,sa.gov/ic/psa/. As of 9
March 2000.
D. Pym, L. Pryor, and D. Murphy. 1995. Actions
as processes: a position on planning. In Working
Notes, AA AI Symposium on Extending Theories
of Action, pages 169-173.
R. W. Smith. 1997. An evaluation of strategies for
selective utterance verification for spoken natural
language dialog. In Proceedings of the Fifth Con-
ference on Applied Natural Language Processing,
pages 41-48.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and
R. Moore. 1999. The CommandTalk spoken di-
alogue system. In Proceedings of the Thirty-
Seventh Annual Meeting of the Association for
Computational Linguistics, pages 183-190.
D. R. Traum and J. Allen. 1994. Discourse obliga-
tions in dialogue processing. In Proceedings of the
Thirty-Second Annual Meeting of the Association
for Computational Linguistics, pages 1-8.
D. R. Traum and C. F. Andersen. 1999. Represen-
tations of dialogue state for domain and task inde-
pendent meta-dialogue. In Proceedings of the IJ-
CAI&apos;99 Workshop on Knowledge and Reasoning
in Practical Dialogue Systems, pages 113-120.
J. van Eijck and R. Moore. 1992. Semantic rules
for English. In H. Alshawi, editor, The Core Lan-
guage Engine. MIT Press.
B. Webber. 1995. Instructing animated agents:
Viewing language in behavioral terms. In Proceed-
ings of the International Conference on Coopera-
tive Multi-modal Communication.
T. A. Winograd. 1973. A procedural model of lan-
guage understanding. In R. C. Shank and K. M.
Colby, editors, Computer Models of Thought and
Language. Freeman, San Francisco, CA.
</reference>
<page confidence="0.996185">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896964">
<title confidence="0.9996435">A Compact Architecture for Dialogue Management Based on Scripts and Meta-Outputs</title>
<author confidence="0.999917">Manny Rayner</author>
<author confidence="0.999917">Beth Ann Hockey</author>
<author confidence="0.999917">Frankie James</author>
<affiliation confidence="0.959613">Research Institute for Advanced Computer Science Mail Stop 19-39, NASA Ames Research Center</affiliation>
<address confidence="0.99856">Moffett Field, CA 94035-1000</address>
<email confidence="0.985319">Imanny,bahockey,fjamesl@riacs.edu</email>
<abstract confidence="0.999346416666667">We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that transforms speech signals through successive representations of linguistic, dialogue, and domain knowledge. Each step produces an output, and a meta-output describing the transformation, with an executable program in a simple scripting language as the final result. The output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>USER Go back</author>
</authors>
<note>[PSA returns to storage lockers]</note>
<marker>back, </marker>
<rawString>USER: Go back. [PSA returns to storage lockers]</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Badler</author>
<author>R Bindiganavale</author>
<author>J Bourne</author>
<author>J Allbeck</author>
<author>J Shi</author>
<author>M Palmer</author>
</authors>
<title>Real time virtual humans.</title>
<date>1999</date>
<booktitle>In International Conference on Digital Media Futures.</booktitle>
<contexts>
<context position="1705" citStr="Badler et al., 1999" startWordPosition="250" endWordPosition="253">ograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user&apos;s intended com</context>
</contexts>
<marker>Badler, Bindiganavale, Bourne, Allbeck, Shi, Palmer, 1999</marker>
<rawString>N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck, J. Shi, and M. Palmer. 1999. Real time virtual humans. In International Conference on Digital Media Futures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>M Gawron</author>
<author>D Appelt</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D Moran</author>
</authors>
<title>Gemini: A natural language system for spoken language understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirty-First Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10134" citStr="Dowding et al., 1993" startWordPosition="1620" endWordPosition="1623">ture, pressure and carbon dioxide levels, and the status of the Shuttle&apos;s doors (open/closed). A visual display gives direct feedback on some of these parameters. 113 The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et a., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domainAndependent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recog-_ nition grammar using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager. We refer to these as linguistic level representations. The aspects of the system which are of primary interest here concern the dialogue manager</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, M. Gawron, D. Appelt, L. Cherny, R. Moore, and D. Moran. 1993. Gemini: A natural language system for spoken language understanding. In Proceedings of the Thirty-First Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Konolige</author>
<author>K Myers</author>
<author>E Ruspini</author>
<author>A Saffiotti</author>
</authors>
<title>Flakey in action: The</title>
<date>1993</date>
<booktitle>Note 528, SRI, Al Center, SRI International, 333 Ravenswood Ave.,</booktitle>
<tech>Technical Report SRI Technical</tech>
<pages>94025</pages>
<location>Menlo Park, CA</location>
<contexts>
<context position="1272" citStr="Konolige et al., 1993" startWordPosition="182" endWordPosition="185"> output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts. 1 Introduction The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects</context>
</contexts>
<marker>Konolige, Myers, Ruspini, Saffiotti, 1993</marker>
<rawString>K. Konolige, K. Myers, E. Ruspini, and A. Saffiotti. 1993. Flakey in action: The 1992 AAAI robot competition. Technical Report SRI Technical Note 528, SRI, Al Center, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martin</author>
<author>A Cheyer</author>
<author>D Moran</author>
</authors>
<title>Building distributed software systems with the open agent architecture.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third International Conference on the Practical Application of Intelligent Agents and Multi-Agent Technology.</booktitle>
<contexts>
<context position="9949" citStr="Martin et al., 1998" startWordPosition="1591" endWordPosition="1594">nitial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA&apos;s current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle&apos;s doors (open/closed). A visual display gives direct feedback on some of these parameters. 113 The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et a., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domainAndependent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recog-_ nition grammar using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and pass</context>
</contexts>
<marker>Martin, Cheyer, Moran, 1998</marker>
<rawString>D. Martin, A. Cheyer, and D. Moran. 1998. Building distributed software systems with the open agent architecture. In Proceedings of the Third International Conference on the Practical Application of Intelligent Agents and Multi-Agent Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
<author>H Bratt</author>
<author>J Gawron</author>
</authors>
<title>CommandTalk: A spoken-language interface for battlefield simulations.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1449" citStr="Moore et al., 1997" startWordPosition="211" endWordPosition="214">e basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the f</context>
<context position="9794" citStr="Moore et al., 1997" startWordPosition="1564" endWordPosition="1567">nitor other environmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA&apos;s current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle&apos;s doors (open/closed). A visual display gives direct feedback on some of these parameters. 113 The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et a., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domainAndependent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recog-_ nition grammar using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances </context>
</contexts>
<marker>Moore, Dowding, Bratt, Gawron, 1997</marker>
<rawString>R. Moore, J. Dowding, H. Bratt, J. Gawron, • Y. Gorfu, and A. Cheyer. 1997. CommandTalk: A spoken-language interface for battlefield simulations. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuance</author>
</authors>
<date>2000</date>
<journal>Nuance Communications, Inc. http://www.nuance.com. As of</journal>
<volume>9</volume>
<contexts>
<context position="10039" citStr="Nuance, 2000" startWordPosition="1607" endWordPosition="1608"> include the PSA&apos;s current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle&apos;s doors (open/closed). A visual display gives direct feedback on some of these parameters. 113 The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et a., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domainAndependent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recog-_ nition grammar using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager. We refer to these as linguistic level representat</context>
</contexts>
<marker>Nuance, 2000</marker>
<rawString>Nuance, 2000. Nuance Communications, Inc. http://www.nuance.com. As of 9 March 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perzanowski</author>
<author>A Schultz</author>
<author>W Adams</author>
</authors>
<title>Integrating natural language and gesture in a robotics domain.</title>
<date>1998</date>
<booktitle>In IEEE International Symposium on Intelligent Control: ISIC/CIRA/ISAS Joint Conference,</booktitle>
<pages>247--252</pages>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, MD:</location>
<contexts>
<context position="1328" citStr="Perzanowski et al., 1998" startWordPosition="190" endWordPosition="193">eatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts. 1 Introduction The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects o</context>
</contexts>
<marker>Perzanowski, Schultz, Adams, 1998</marker>
<rawString>D. Perzanowski, A. Schultz, and W. Adams. 1998. Integrating natural language and gesture in a robotics domain. In IEEE International Symposium on Intelligent Control: ISIC/CIRA/ISAS Joint Conference, pages 247-252, Gaithersburg, MD: National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perzanowski</author>
<author>A Schultz</author>
<author>W Adams</author>
<author>E Marsh</author>
</authors>
<title>Goal tracking in a natural language interface: Towards achieving adjustable autonomy.</title>
<date>1999</date>
<booktitle>In ISIS/CIRA99 Conference,</booktitle>
<publisher>IEEE.</publisher>
<location>Monterey, CA.</location>
<contexts>
<context position="1355" citStr="Perzanowski et al., 1999" startWordPosition="194" endWordPosition="197">uch as resolving pronouns, correcting user misconceptions, and optimizing scripts. 1 Introduction The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Py</context>
</contexts>
<marker>Perzanowski, Schultz, Adams, Marsh, 1999</marker>
<rawString>D. Perzanowski, A. Schultz, W. Adams, and E. Marsh. 1999. Goal tracking in a natural language interface: Towards achieving adjustable autonomy. In ISIS/CIRA99 Conference, Monterey, CA. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PSA</author>
</authors>
<title>Personal Satellite Assistant (PSA) Project.</title>
<date>2000</date>
<journal>http://ic.arc.na,sa.gov/ic/psa/. As of</journal>
<volume>9</volume>
<contexts>
<context position="8286" citStr="PSA, 2000" startWordPosition="1325" endWordPosition="1326">s the meta-output information to decide what to do with the various possible interpretations it has produced. Possible actions include selection and execution of an output script, paraphrasing meta-output information back to the user, or some combination of the two. In the following section, we present a more detailed description showing how the output/metaoutput distinction works in a practical system. 3 A Prototype Implementation The ideas sketched out above have been realized as a prototype spoken language dialogue interface to a simulated version of the Personal Satellite Assistant (PSA; (PSA, 2000)). This section gives an overview of the implementation; in the following section, we focus on the specific aspects of dialogue management which are facilitated by the output/meta-output architecture. 3.1 Levels of Representation The real PSA is a miniature robot currently being developed at NASA Ames Research Center, which is intended for deployment on the Space Shuttle and/or International Space Station. It will be capable of free navigation in an indoor micro-gravity environment, and will provide mobile sensory capacity as a backup to a network of fixed sensors. The PSA will primarily be co</context>
</contexts>
<marker>PSA, 2000</marker>
<rawString>PSA, 2000. Personal Satellite Assistant (PSA) Project. http://ic.arc.na,sa.gov/ic/psa/. As of 9 March 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pym</author>
<author>L Pryor</author>
<author>D Murphy</author>
</authors>
<title>Actions as processes: a position on planning.</title>
<date>1995</date>
<booktitle>In Working Notes, AA AI Symposium on Extending Theories of Action,</booktitle>
<pages>169--173</pages>
<contexts>
<context position="1970" citStr="Pym et al., 1995" startWordPosition="290" endWordPosition="293">9). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user&apos;s intended command; this formula is then fed into a command interpreter, which executes the command. We do not think the standard treatment outlined above is in essence incorrect, but we do believe that, as it stands, it is in need of some modification. This paper will in partic</context>
</contexts>
<marker>Pym, Pryor, Murphy, 1995</marker>
<rawString>D. Pym, L. Pryor, and D. Murphy. 1995. Actions as processes: a position on planning. In Working Notes, AA AI Symposium on Extending Theories of Action, pages 169-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Smith</author>
</authors>
<title>An evaluation of strategies for selective utterance verification for spoken natural language dialog.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="1484" citStr="Smith, 1997" startWordPosition="218" endWordPosition="219">that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech inte</context>
</contexts>
<marker>Smith, 1997</marker>
<rawString>R. W. Smith. 1997. An evaluation of strategies for selective utterance verification for spoken natural language dialog. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J Gawron</author>
<author>E Bratt</author>
<author>R Moore</author>
</authors>
<title>The CommandTalk spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the ThirtySeventh Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore. 1999. The CommandTalk spoken dialogue system. In Proceedings of the ThirtySeventh Annual Meeting of the Association for Computational Linguistics, pages 183-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Traum</author>
<author>J Allen</author>
</authors>
<title>Discourse obligations in dialogue processing.</title>
<date>1994</date>
<booktitle>In Proceedings of the Thirty-Second Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1521" citStr="Traum and Allen, 1994" startWordPosition="222" endWordPosition="225">e to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is con</context>
</contexts>
<marker>Traum, Allen, 1994</marker>
<rawString>D. R. Traum and J. Allen. 1994. Discourse obligations in dialogue processing. In Proceedings of the Thirty-Second Annual Meeting of the Association for Computational Linguistics, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Traum</author>
<author>C F Andersen</author>
</authors>
<title>Representations of dialogue state for domain and task independent meta-dialogue.</title>
<date>1999</date>
<booktitle>In Proceedings of the IJCAI&apos;99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1548" citStr="Traum and Andersen, 1999" startWordPosition="226" endWordPosition="229"> semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text eith</context>
</contexts>
<marker>Traum, Andersen, 1999</marker>
<rawString>D. R. Traum and C. F. Andersen. 1999. Representations of dialogue state for domain and task independent meta-dialogue. In Proceedings of the IJCAI&apos;99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems, pages 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Eijck</author>
<author>R Moore</author>
</authors>
<title>Semantic rules for English.</title>
<date>1992</date>
<booktitle>The Core Language Engine.</booktitle>
<editor>In H. Alshawi, editor,</editor>
<publisher>MIT Press.</publisher>
<marker>van Eijck, Moore, 1992</marker>
<rawString>J. van Eijck and R. Moore. 1992. Semantic rules for English. In H. Alshawi, editor, The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>Instructing animated agents: Viewing language in behavioral terms.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Cooperative Multi-modal Communication.</booktitle>
<contexts>
<context position="1951" citStr="Webber, 1995" startWordPosition="288" endWordPosition="289">ki et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user&apos;s intended command; this formula is then fed into a command interpreter, which executes the command. We do not think the standard treatment outlined above is in essence incorrect, but we do believe that, as it stands, it is in need of some modification. This p</context>
</contexts>
<marker>Webber, 1995</marker>
<rawString>B. Webber. 1995. Instructing animated agents: Viewing language in behavioral terms. In Proceedings of the International Conference on Cooperative Multi-modal Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Winograd</author>
</authors>
<title>A procedural model of language understanding. In</title>
<date>1973</date>
<booktitle>Computer Models of Thought and Language. Freeman,</booktitle>
<editor>R. C. Shank and K. M. Colby, editors,</editor>
<location>San Francisco, CA.</location>
<contexts>
<context position="1097" citStr="Winograd, 1973" startWordPosition="156" endWordPosition="157">ledge. Each step produces an output, and a meta-output describing the transformation, with an executable program in a simple scripting language as the final result. The output/meta-output distinction permits perspicuous treatment of diverse tasks such as resolving pronouns, correcting user misconceptions, and optimizing scripts. 1 Introduction The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI&apos;s Flakey robot (Konolige et al., 1993) and NCARAI&apos;s InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack&apos;s MOOse Lodge (Badler et al</context>
</contexts>
<marker>Winograd, 1973</marker>
<rawString>T. A. Winograd. 1973. A procedural model of language understanding. In R. C. Shank and K. M. Colby, editors, Computer Models of Thought and Language. Freeman, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>