<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.278188">
<title confidence="0.997675">
Behavioral Factors in Interactive Training of Text Classifiers
</title>
<author confidence="0.994362">
Burr Settles
</author>
<affiliation confidence="0.962266">
Machine Learning Department
Carnegie Mellon University
Pittsburgh PA 15213, USA
</affiliation>
<email confidence="0.996934">
bsettles@cs.cmu.edu
</email>
<author confidence="0.989382">
Xiaojin Zhu
</author>
<affiliation confidence="0.839242">
Computer Sciences Department
University of Wisconsin
Madison WI 53715, USA
</affiliation>
<email confidence="0.998891">
jerryzhu@cs.wisc.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999745">
This paper describes a user study where hu-
mans interactively train automatic text clas-
sifiers. We attempt to replicate previous re-
sults using multiple “average” Internet users
instead of a few domain experts as annotators.
We also analyze user annotation behaviors to
find that certain labeling actions have an im-
pact on classifier accuracy, drawing attention
to the important role these behavioral factors
play in interactive learning systems.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957923076923">
There is growing interest in methods that incorpo-
rate human domain knowledge in machine learning
algorithms, either as priors on model parameters or
as constraints in an objective function. Such ap-
proaches lend themselves well to natural language
tasks, where input features are often discrete vari-
ables that carry semantic meaning (e.g., words). A
feature label is a simple but expressive form of do-
main knowledge that has received considerable at-
tention recently (Druck et al., 2008; Melville et al.,
2009). For example, a single feature (word) can be
used to indicate a particular label or set of labels,
such as “excellent” ==&gt;. positive or “terrible” ==�- neg-
ative, which might be useful word-label rules for a
sentiment analysis task.
Contemporary work has also focused on mak-
ing such learning algorithms active, by enabling
them to pose “queries” in the form of feature-based
rules to be labeled by annotators in addition to —
and sometimes lieu of — data instances such as
documents (Attenberg et al., 2010; Druck et al.,
2009). These concepts were recently implemented
in a practical system for interactive training of text
classifiers called DUALIST1. Settles (2011) reports
that, in user experiments with real annotators, hu-
mans were able to train near state of the art classi-
fiers with only a few minutes of effort. However,
there were only five subjects, who were all com-
puter science researchers. It is possible that these
positive results can be attributed to the subjects’ im-
plicit familiarity with machine learning and natural
language processing algorithms.
This short paper sheds more light on previous ex-
periments by replicating them with many more hu-
man subjects, and of a different type: non-experts
recruited through the Amazon Mechanical Turk ser-
vice2. We also analyze the impact of annotator be-
havior on the resulting classifiers, and suggest rela-
tionships to recent work in curriculum learning.
</bodyText>
<sectionHeader confidence="0.997689" genericHeader="introduction">
2 DUALIST
</sectionHeader>
<bodyText confidence="0.999771727272727">
Figure 1 shows a screenshot of DUALIST, an inter-
active machine learning system for quickly build-
ing text classifiers. The annotator is allowed to
take three kinds of actions:10 label query docu-
ments (instances) by clicking class-label buttons in
the left panel, 20 label query words (features) by
selecting them from the class-label columns in the
right panel, or OO “volunteer” domain knowledge by
typing labeled words into a text box at the top of
each class column. The underlying classifier is a
naive Bayes variant combining informative priors,
</bodyText>
<footnote confidence="0.999989">
1http://code.google.com/p/dualist/
2http://mturk.com
</footnote>
<page confidence="0.920724333333333">
563
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 563–567,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<figureCaption confidence="0.999904">
Figure 1: The DUALIST user interface.
</figureCaption>
<bodyText confidence="0.999969666666667">
maximum likelihood estimation, and the EM algo-
rithm for fast semi-supervised training. When a
user performs action ➀ or ➁, she labels queries that
should help minimize the classifier’s uncertainty on
unlabeled documents (according to active learning
heuristics). For action ➂, the user is free to volun-
teer any relevant word, whether or not it appears in
a document or word column. For example, the user
might volunteer the labeled word “oscar” ==&gt;. posi-
tive in a sentiment analysis task for movie reviews
(leveraging her knowledge of domain), even if the
word “oscar” does not appear anywhere in the in-
terface. This flexibility goes beyond traditional ac-
tive learning, which restricts the user to feedback on
items queried by the learner (i.e., actions ➀ and ➁).
After a few labeling actions, the user submits her
feedback and receives the next set of queries in real
time. For more details, see Settles (2011).
</bodyText>
<sectionHeader confidence="0.99778" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999989723404255">
We recruited annotators through the crowdsourcing
marketplace Mechanical Turk. Subjects were shown
a tutorial page with a brief description of the clas-
sification task, as well as a cartoon of the interface
similar to Figure 1 explaining the various annotation
options. When they decided they were ready, users
followed a link to a web server running a customized
version of DUALIST, which is an open source web-
based application. At the end of each trial, subjects
were given a confirmation code to receive payment.
We conducted experiments using two corpora
from the original DUALIST study: Science (a subset
of the 20 Newsgroups benchmark: cryptography,
electronics, medicine, and space) and Movie Re-
views (a sentiment analysis collection). These are
not specialized domains, i.e., we could expect av-
erage Internet users to be knowledgable enough to
perform the annotations. While both are generally
accessible, these corpora represent different types of
tasks and vary both in number of categories (four
vs. two) and difficulty (Movie Reviews is known to
be harder for learning algorithms). We replicated
the same experimental conditions as previous work:
DUALIST (the full interface in Figure 1), active-doc
(the left-hand ➀ document panel only), and passive-
doc (the ➀ document panel only, but with texts se-
lected at random and not queried by active learning).
For each condition, we recruited 25 users for the
Science corpus (75 total) and 35 users for Movie Re-
views (105 total). We were careful to publish tasks
on MTurk in a way that no one user annotated more
than one condition. Some users experienced techni-
cal difficulties that nullified their work, and four ap-
peared to be spammers3. After removing these sub-
jects from the analysis, we were left with 23 users
for the Science DUALIST condition, 25 each for the
two document-only conditions (73 total), 32 users
for the Movie Reviews DUALIST condition, and
33 each for the document-only conditions (98 total).
DUALIST automatically logged data about user ac-
tions and model accuracies as training progressed,
although users could not see these statistics. Trials
lasted 6 minutes for the Science corpus and 10 min-
utes for Movie Reviews. We did advertise a “bonus”
for the user who trained the best classifier to encour-
age correctness, but otherwise offered no guidance
on how subjects should prioritize their time.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999647555555556">
Figure 2(a) shows learning curves aggregated across
all users in each experimental condition. Curves are
LOESS fits to classifier accuracy over time: locally-
weighted polynomial regressions (Cleveland et al.,
1992) ±1 standard error, with the actual user data
points omitted for clarity. For the Science task (top),
DUALIST users trained significantly better classi-
fiers after about four minutes of annotation time.
Document-only active learning also outperformed
</bodyText>
<footnote confidence="0.996326">
3A spammer was ruled to be one whose document error rate
(vs. the gold standard) was more than double the chance error,
and whose feature labels appeared to be arbitrary clicks.
</footnote>
<page confidence="0.99133">
564
</page>
<figure confidence="0.99992762745098">
0.70
0.60
0.50
0.40
0.30
0.20
DUALIST
active-doc
passive-doc
0 60 120 180 240 300 360
DUALIST active-doc passive-doc
0.70
0.60
0.50
0.40
0.30
0.20
DV++ (5)
DV+ (9)
DV- (9)
0 60 120 180 240 300 360
Science
Science
0.3 0.5 0.7
0 120 240 360 480 600
annotation time (sec)
DUALIST active-doc passive-doc
DV++ (8)
DV+ (13)
DV- (11)
0 120 240 360 480 600
annotation time (sec)
0.64
Movie Reviews
0.61
0.58
0.55
0.52
0.49
0.50 0.60 0.70
Movie Reviews
0.64
0.61
0.58
0.55
0.52
0.49
DUALIST
active-doc
passive-doc
(a) learning curves (b) final classifier accuracies (c) behavioral subgroup curves
</figure>
<figureCaption confidence="0.99873875">
Figure 2: (a) Learning curves plotting accuracy vs. actual annotation time for the three conditions. Curves are LOESS
fits (f1 SE) to all classifier accuracies at that point in time. (b) Box plots showing the distribution of final accuracies
under each condition. (c) Learning curves for three behavioral subgroups found in the DUALIST condition. The
DV++ group volunteered many labeled words (action OO ), DV+ volunteered some, and DV- volunteered none.
</figureCaption>
<bodyText confidence="0.999170925925926">
standard passive learning, which is consistent with
previous work. However, for Movie Reviews (bot-
tom), there is little difference among the three set-
tings, and in fact models trained with DUALIST ap-
pear to lag behind active learning with documents.
Figure 2(b) shows the distribution of final classi-
fier accuracies in each condition. For Science, the
DUALIST users are significantly better than either
of the baselines (two-sided KS test, p &lt; 0.005).
While the differences in DUALIST accuracies are
not significantly different, we can see that the top
quartile does much better than the two baselines.
Clearly some DUALIST users are making better use
of the interface and training better classifiers. How?
It is important to note that users in the active-
doc and passive-doc conditions can only choose ac-
tion10 (label documents), whereas those in the DU-
ALIST condition must allocate their time among
three kinds of actions. It turns out that the anno-
tators exhibited very non-uniform behavior in this
respect. In particular, activity of action 03 (volunteer
labeled words) follows a power law, and many sub-
jects volunteered no features at all. By inspecting
the distribution of these actions for natural break-
points, we identified three subgroups of DUALIST
users: DV++ (many volunteered words), DV+ (some
words), and DV- (none; labeled queries only). Note
</bodyText>
<table confidence="0.9993932">
Group Movie Reviews Science
# Words Users # Words Users
DV++ 21–62 8 24–42 5
DV+ 1–15 13 2–19 9
DV- 0 11 0 9
</table>
<tableCaption confidence="0.9914475">
Table 1: The range of volunteered words and number of
users in each behavioral subgroup of DUALIST subjects.
</tableCaption>
<bodyText confidence="0.997784473684211">
that DV- is not functionally equivalent to the active-
doc condition, as users in the DV- group could still
view and label word queries. The three behavioral
subgroups are summarized in Table 1.
Figure 2(c) shows learning curves for these three
groups. We can see that the DV++ and DV+ groups
ultimately train better classifiers than the DV- group,
and DV++ also dominates both the active and pas-
sive baselines from Figure 2(a). The DV++ group is
particularly effective on the Movie Reviews corpus.
This suggests that a user’s choice to volunteer more
labeled features — by occasionally side-stepping the
queries posed by the active learner and directly in-
jecting their domain knowledge — is a good predic-
tor of classifier accuracy on this task.
To tease apart the relative impact of other behav-
iors, we conducted an ordinary least-squares regres-
sion to predict classifier accuracy at the end of a trial.
We included the number of user events for each ac-
</bodyText>
<page confidence="0.996037">
565
</page>
<bodyText confidence="0.99997571875">
tion as independent variables, plus two controls: the
subject’s document error rate in [0,1] with respect to
the gold standard, and class entropy in [0, log C] of
all labeled words (where C is the number of classes).
The entropy variable is meant to capture how “bal-
anced” a user’s word-labeling activity was for ac-
tions ➁ and ➂, with the intuition that a skewed set of
words could confuse the learner, by biasing it away
from categories with fewer labeled words.
Table 2 summarizes these results. Surprisingly,
query-labeling actions (➀ and ➁) have a relatively
small impact on accuracy. The number of volun-
teered words and entropy among word labels appear
to be the only two factors that are somewhat signif-
icant: the former is strongest in the Movie Reviews
corpus, the latter in Science4. Interestingly, there is a
strong positive correlation between these two factors
in the Movie Reviews corpus (Spearman’s p = 0.51,
p = 0.02) but not in Science (p = 0.03). When we
consider change in word label entropy over time, the
Science DA++ group is balanced early on and be-
comes steadily more so on average , whereas
DA+ goes for several minutes before catching up
(and briefly overtaking) . This may account
for DA+’s early dip in accuracy in Figure 2(c). For
Movie Reviews, DA++ is more balanced than DA+
throughout the trial. DA++ labeled many words that
were also class-balanced, which may explain why
it is the best consistently-performing group. As is
common in behavior modeling with small samples,
the data are noisy and the regressions in Table 2 only
explain 33%–46% of the variance in accuracy.
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999921333333333">
We were able to partially replicate the results from
Settles (2011). That is, for two of the same data sets,
some of the subjects using DUALIST significantly
outperformed those using traditional document-only
interfaces. However, our results show that the
gains come not merely from the interface itself, but
from which labeling actions the users chose to per-
form. As interactive learning systems continue to
expand the palette of interactive options (e.g., la-
</bodyText>
<footnote confidence="0.9663608">
4Science has four labels and a larger entropy range, which
might explain the importance of the entropy factor here. Also,
labels are more related to natural clusterings in this corpus
(Nigam et al., 2000), so class-balanced priors might be key for
DUALIST’s semi-supervised EM procedure to work well.
</footnote>
<table confidence="0.9965137">
Action Movie Reviews Science
0 SE 0 SE
(intercept) 0.505 0.038 *** 0.473 0.147 **
➀ label query docs 0.001 0.001 0.005 0.005
➁ label query words -0.001 0.001 0.000 0.001
➂ volunteer words 0.002 0.001 * 0.000 0.002
human error rate -0.036 0.109 -0.328 0.230
word label entropy 0.053 0.051 0.201 0.102 .
R2 = 0.4608 ** R2 = 0.3342
*** p &lt; 0.001 ** p &lt; 0.01 * p &lt; 0.05 . p &lt; 0.1
</table>
<tableCaption confidence="0.9263215">
Table 2: Linear regressions estimating the accuracy of a
classifier as a function of annotator actions and behaviors.
</tableCaption>
<bodyText confidence="0.999971147058824">
beling and/or volunteering features), understanding
how these options impact learning becomes more
important. In particular, training a good classifier
in our experiments appears to be linked to (1) vol-
unteering more labeled words, and (2) maintaining
a class balance among them. Users who exhibited
both of these behaviors — which are possibly arti-
facts of their good intuitions — performed the best.
We posit that there is a conceptual connection be-
tween these insights and curriculum learning (Ben-
gio et al., 2009), the commonsense notion that learn-
ers perform better if they begin with clear and unam-
biguous examples before graduating to more com-
plex training data. A recent study found that some
humans use a curriculum strategy when teaching a
1D classification task to a robot (Khan et al., 2012).
About half of those subjects alternated between ex-
treme positive and negative instances in a relatively
class-balanced way. This behavior was explained by
showing that it is optimal under an assumption that,
in reality, the learning task has many input features
for which only one is relevant to the task.
Text classification exhibits similar properties:
there are many features (words), of which only a few
are relevant. We argue that labeling features can be
seen as a kind of training by curriculum. By volun-
teering labeled words in a class-balanced way (espe-
cially early on), a user provides clear, unambiguous
training signals that effectively perform feature se-
lection while biasing the classifier toward the user’s
hypothesis. Future research on mixed-initiative user
interfaces might try to detect and encourage these
kinds of annotator behaviors, and potentially im-
prove interactive machine learning outcomes.
</bodyText>
<page confidence="0.997471">
566
</page>
<sectionHeader confidence="0.998817" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995515666666667">
This work was funded in part by DARPA, the
National Science Foundation (under grants IIS-
0953219 and IIS-0968487), and Google.
</bodyText>
<sectionHeader confidence="0.995653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831975609756">
J. Attenberg, P. Melville, and F. Provost. 2010. A uni-
fied approach to active dual supervision for labeling
features and examples. In Proceedings of the Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 40–55. Springer.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 119–126. Omnipress.
W.S. Cleveland, E. Grosse, and W.M. Shyu. 1992. Lo-
cal regression models. In J.M. Chambers and T.J.
Hastie, editors, Statistical Models in S. Wadsworth &amp;
Brooks/Cole.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 595–602. ACM Press.
G. Druck, B. Settles, and A. McCallum. 2009. Ac-
tive learning by labeling features. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 81–90. ACL Press.
F. Khan, X. Zhu, and B. Mutlu. 2012. How do humans
teach: On curriculum learning and teaching dimen-
sion. In Advances in Neural Information Processing
Systems (NIPS), volume 24, pages 1449–1457. Mor-
gan Kaufmann.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sen-
timent analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 1275–1284. ACM Press.
K. Nigam, A.K. Mccallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using em. Machine Learning, 39:103–134.
B. Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1467–1478. ACL Press.
</reference>
<page confidence="0.997321">
567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342690">
<title confidence="0.999796">Behavioral Factors in Interactive Training of Text Classifiers</title>
<author confidence="0.987746">Burr</author>
<affiliation confidence="0.8788085">Machine Learning Carnegie Mellon</affiliation>
<address confidence="0.950209">Pittsburgh PA 15213,</address>
<email confidence="0.999143">bsettles@cs.cmu.edu</email>
<author confidence="0.486578">Xiaojin</author>
<affiliation confidence="0.998695">Computer Sciences University of</affiliation>
<address confidence="0.988056">Madison WI 53715,</address>
<email confidence="0.999891">jerryzhu@cs.wisc.edu</email>
<abstract confidence="0.999479909090909">This paper describes a user study where humans interactively train automatic text classifiers. We attempt to replicate previous results using multiple “average” Internet users instead of a few domain experts as annotators. We also analyze user annotation behaviors to find that certain labeling actions have an impact on classifier accuracy, drawing attention to the important role these behavioral factors play in interactive learning systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Attenberg</author>
<author>P Melville</author>
<author>F Provost</author>
</authors>
<title>A unified approach to active dual supervision for labeling features and examples.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD),</booktitle>
<pages>40--55</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1767" citStr="Attenberg et al., 2010" startWordPosition="268" endWordPosition="271">rm of domain knowledge that has received considerable attention recently (Druck et al., 2008; Melville et al., 2009). For example, a single feature (word) can be used to indicate a particular label or set of labels, such as “excellent” ==&gt;. positive or “terrible” ==�- negative, which might be useful word-label rules for a sentiment analysis task. Contemporary work has also focused on making such learning algorithms active, by enabling them to pose “queries” in the form of feature-based rules to be labeled by annotators in addition to — and sometimes lieu of — data instances such as documents (Attenberg et al., 2010; Druck et al., 2009). These concepts were recently implemented in a practical system for interactive training of text classifiers called DUALIST1. Settles (2011) reports that, in user experiments with real annotators, humans were able to train near state of the art classifiers with only a few minutes of effort. However, there were only five subjects, who were all computer science researchers. It is possible that these positive results can be attributed to the subjects’ implicit familiarity with machine learning and natural language processing algorithms. This short paper sheds more light on p</context>
</contexts>
<marker>Attenberg, Melville, Provost, 2010</marker>
<rawString>J. Attenberg, P. Melville, and F. Provost. 2010. A unified approach to active dual supervision for labeling features and examples. In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), pages 40–55. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J Louradour</author>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>119--126</pages>
<publisher>Omnipress.</publisher>
<contexts>
<context position="14479" citStr="Bengio et al., 2009" startWordPosition="2349" endWordPosition="2353"> regressions estimating the accuracy of a classifier as a function of annotator actions and behaviors. beling and/or volunteering features), understanding how these options impact learning becomes more important. In particular, training a good classifier in our experiments appears to be linked to (1) volunteering more labeled words, and (2) maintaining a class balance among them. Users who exhibited both of these behaviors — which are possibly artifacts of their good intuitions — performed the best. We posit that there is a conceptual connection between these insights and curriculum learning (Bengio et al., 2009), the commonsense notion that learners perform better if they begin with clear and unambiguous examples before graduating to more complex training data. A recent study found that some humans use a curriculum strategy when teaching a 1D classification task to a robot (Khan et al., 2012). About half of those subjects alternated between extreme positive and negative instances in a relatively class-balanced way. This behavior was explained by showing that it is optimal under an assumption that, in reality, the learning task has many input features for which only one is relevant to the task. Text c</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 119–126. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Cleveland</author>
<author>E Grosse</author>
<author>W M Shyu</author>
</authors>
<title>Local regression models.</title>
<date>1992</date>
<booktitle>Statistical Models in S.</booktitle>
<editor>In J.M. Chambers and T.J. Hastie, editors,</editor>
<publisher>Wadsworth &amp; Brooks/Cole.</publisher>
<contexts>
<context position="7084" citStr="Cleveland et al., 1992" startWordPosition="1111" endWordPosition="1114">IST automatically logged data about user actions and model accuracies as training progressed, although users could not see these statistics. Trials lasted 6 minutes for the Science corpus and 10 minutes for Movie Reviews. We did advertise a “bonus” for the user who trained the best classifier to encourage correctness, but otherwise offered no guidance on how subjects should prioritize their time. 4 Results Figure 2(a) shows learning curves aggregated across all users in each experimental condition. Curves are LOESS fits to classifier accuracy over time: locallyweighted polynomial regressions (Cleveland et al., 1992) ±1 standard error, with the actual user data points omitted for clarity. For the Science task (top), DUALIST users trained significantly better classifiers after about four minutes of annotation time. Document-only active learning also outperformed 3A spammer was ruled to be one whose document error rate (vs. the gold standard) was more than double the chance error, and whose feature labels appeared to be arbitrary clicks. 564 0.70 0.60 0.50 0.40 0.30 0.20 DUALIST active-doc passive-doc 0 60 120 180 240 300 360 DUALIST active-doc passive-doc 0.70 0.60 0.50 0.40 0.30 0.20 DV++ (5) DV+ (9) DV- </context>
</contexts>
<marker>Cleveland, Grosse, Shyu, 1992</marker>
<rawString>W.S. Cleveland, E. Grosse, and W.M. Shyu. 1992. Local regression models. In J.M. Chambers and T.J. Hastie, editors, Statistical Models in S. Wadsworth &amp; Brooks/Cole.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>595--602</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1237" citStr="Druck et al., 2008" startWordPosition="178" endWordPosition="181">ifier accuracy, drawing attention to the important role these behavioral factors play in interactive learning systems. 1 Introduction There is growing interest in methods that incorporate human domain knowledge in machine learning algorithms, either as priors on model parameters or as constraints in an objective function. Such approaches lend themselves well to natural language tasks, where input features are often discrete variables that carry semantic meaning (e.g., words). A feature label is a simple but expressive form of domain knowledge that has received considerable attention recently (Druck et al., 2008; Melville et al., 2009). For example, a single feature (word) can be used to indicate a particular label or set of labels, such as “excellent” ==&gt;. positive or “terrible” ==�- negative, which might be useful word-label rules for a sentiment analysis task. Contemporary work has also focused on making such learning algorithms active, by enabling them to pose “queries” in the form of feature-based rules to be labeled by annotators in addition to — and sometimes lieu of — data instances such as documents (Attenberg et al., 2010; Druck et al., 2009). These concepts were recently implemented in a p</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>G. Druck, G. Mann, and A. McCallum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 595–602. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>B Settles</author>
<author>A McCallum</author>
</authors>
<title>Active learning by labeling features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>81--90</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="1788" citStr="Druck et al., 2009" startWordPosition="272" endWordPosition="275">hat has received considerable attention recently (Druck et al., 2008; Melville et al., 2009). For example, a single feature (word) can be used to indicate a particular label or set of labels, such as “excellent” ==&gt;. positive or “terrible” ==�- negative, which might be useful word-label rules for a sentiment analysis task. Contemporary work has also focused on making such learning algorithms active, by enabling them to pose “queries” in the form of feature-based rules to be labeled by annotators in addition to — and sometimes lieu of — data instances such as documents (Attenberg et al., 2010; Druck et al., 2009). These concepts were recently implemented in a practical system for interactive training of text classifiers called DUALIST1. Settles (2011) reports that, in user experiments with real annotators, humans were able to train near state of the art classifiers with only a few minutes of effort. However, there were only five subjects, who were all computer science researchers. It is possible that these positive results can be attributed to the subjects’ implicit familiarity with machine learning and natural language processing algorithms. This short paper sheds more light on previous experiments b</context>
</contexts>
<marker>Druck, Settles, McCallum, 2009</marker>
<rawString>G. Druck, B. Settles, and A. McCallum. 2009. Active learning by labeling features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 81–90. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Khan</author>
<author>X Zhu</author>
<author>B Mutlu</author>
</authors>
<title>How do humans teach: On curriculum learning and teaching dimension.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>24</volume>
<pages>1449--1457</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14765" citStr="Khan et al., 2012" startWordPosition="2399" endWordPosition="2402">linked to (1) volunteering more labeled words, and (2) maintaining a class balance among them. Users who exhibited both of these behaviors — which are possibly artifacts of their good intuitions — performed the best. We posit that there is a conceptual connection between these insights and curriculum learning (Bengio et al., 2009), the commonsense notion that learners perform better if they begin with clear and unambiguous examples before graduating to more complex training data. A recent study found that some humans use a curriculum strategy when teaching a 1D classification task to a robot (Khan et al., 2012). About half of those subjects alternated between extreme positive and negative instances in a relatively class-balanced way. This behavior was explained by showing that it is optimal under an assumption that, in reality, the learning task has many input features for which only one is relevant to the task. Text classification exhibits similar properties: there are many features (words), of which only a few are relevant. We argue that labeling features can be seen as a kind of training by curriculum. By volunteering labeled words in a class-balanced way (especially early on), a user provides cl</context>
</contexts>
<marker>Khan, Zhu, Mutlu, 2012</marker>
<rawString>F. Khan, X. Zhu, and B. Mutlu. 2012. How do humans teach: On curriculum learning and teaching dimension. In Advances in Neural Information Processing Systems (NIPS), volume 24, pages 1449–1457. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Melville</author>
<author>W Gryc</author>
<author>R D Lawrence</author>
</authors>
<title>Sentiment analysis of blogs by combining lexical knowledge with text classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>1275--1284</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1261" citStr="Melville et al., 2009" startWordPosition="182" endWordPosition="185">ing attention to the important role these behavioral factors play in interactive learning systems. 1 Introduction There is growing interest in methods that incorporate human domain knowledge in machine learning algorithms, either as priors on model parameters or as constraints in an objective function. Such approaches lend themselves well to natural language tasks, where input features are often discrete variables that carry semantic meaning (e.g., words). A feature label is a simple but expressive form of domain knowledge that has received considerable attention recently (Druck et al., 2008; Melville et al., 2009). For example, a single feature (word) can be used to indicate a particular label or set of labels, such as “excellent” ==&gt;. positive or “terrible” ==�- negative, which might be useful word-label rules for a sentiment analysis task. Contemporary work has also focused on making such learning algorithms active, by enabling them to pose “queries” in the form of feature-based rules to be labeled by annotators in addition to — and sometimes lieu of — data instances such as documents (Attenberg et al., 2010; Druck et al., 2009). These concepts were recently implemented in a practical system for inte</context>
</contexts>
<marker>Melville, Gryc, Lawrence, 2009</marker>
<rawString>P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sentiment analysis of blogs by combining lexical knowledge with text classification. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 1275–1284. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A K Mccallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using em.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--103</pages>
<contexts>
<context position="13371" citStr="Nigam et al., 2000" startWordPosition="2159" endWordPosition="2162">ts from Settles (2011). That is, for two of the same data sets, some of the subjects using DUALIST significantly outperformed those using traditional document-only interfaces. However, our results show that the gains come not merely from the interface itself, but from which labeling actions the users chose to perform. As interactive learning systems continue to expand the palette of interactive options (e.g., la4Science has four labels and a larger entropy range, which might explain the importance of the entropy factor here. Also, labels are more related to natural clusterings in this corpus (Nigam et al., 2000), so class-balanced priors might be key for DUALIST’s semi-supervised EM procedure to work well. Action Movie Reviews Science 0 SE 0 SE (intercept) 0.505 0.038 *** 0.473 0.147 **  label query docs 0.001 0.001 0.005 0.005  label query words -0.001 0.001 0.000 0.001  volunteer words 0.002 0.001 * 0.000 0.002 human error rate -0.036 0.109 -0.328 0.230 word label entropy 0.053 0.051 0.201 0.102 . R2 = 0.4608 ** R2 = 0.3342 *** p &lt; 0.001 ** p &lt; 0.01 * p &lt; 0.05 . p &lt; 0.1 Table 2: Linear regressions estimating the accuracy of a classifier as a function of annotator actions and behaviors. beling an</context>
</contexts>
<marker>Nigam, Mccallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A.K. Mccallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machine Learning, 39:103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1467--1478</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="1929" citStr="Settles (2011)" startWordPosition="293" endWordPosition="294">indicate a particular label or set of labels, such as “excellent” ==&gt;. positive or “terrible” ==�- negative, which might be useful word-label rules for a sentiment analysis task. Contemporary work has also focused on making such learning algorithms active, by enabling them to pose “queries” in the form of feature-based rules to be labeled by annotators in addition to — and sometimes lieu of — data instances such as documents (Attenberg et al., 2010; Druck et al., 2009). These concepts were recently implemented in a practical system for interactive training of text classifiers called DUALIST1. Settles (2011) reports that, in user experiments with real annotators, humans were able to train near state of the art classifiers with only a few minutes of effort. However, there were only five subjects, who were all computer science researchers. It is possible that these positive results can be attributed to the subjects’ implicit familiarity with machine learning and natural language processing algorithms. This short paper sheds more light on previous experiments by replicating them with many more human subjects, and of a different type: non-experts recruited through the Amazon Mechanical Turk service2.</context>
<context position="4464" citStr="Settles (2011)" startWordPosition="694" endWordPosition="695">volunteer any relevant word, whether or not it appears in a document or word column. For example, the user might volunteer the labeled word “oscar” ==&gt;. positive in a sentiment analysis task for movie reviews (leveraging her knowledge of domain), even if the word “oscar” does not appear anywhere in the interface. This flexibility goes beyond traditional active learning, which restricts the user to feedback on items queried by the learner (i.e., actions  and ). After a few labeling actions, the user submits her feedback and receives the next set of queries in real time. For more details, see Settles (2011). 3 Experimental Setup We recruited annotators through the crowdsourcing marketplace Mechanical Turk. Subjects were shown a tutorial page with a brief description of the classification task, as well as a cartoon of the interface similar to Figure 1 explaining the various annotation options. When they decided they were ready, users followed a link to a web server running a customized version of DUALIST, which is an open source webbased application. At the end of each trial, subjects were given a confirmation code to receive payment. We conducted experiments using two corpora from the original D</context>
<context position="12774" citStr="Settles (2011)" startWordPosition="2066" endWordPosition="2067">ecomes steadily more so on average , whereas DA+ goes for several minutes before catching up (and briefly overtaking) . This may account for DA+’s early dip in accuracy in Figure 2(c). For Movie Reviews, DA++ is more balanced than DA+ throughout the trial. DA++ labeled many words that were also class-balanced, which may explain why it is the best consistently-performing group. As is common in behavior modeling with small samples, the data are noisy and the regressions in Table 2 only explain 33%–46% of the variance in accuracy. 5 Discussion We were able to partially replicate the results from Settles (2011). That is, for two of the same data sets, some of the subjects using DUALIST significantly outperformed those using traditional document-only interfaces. However, our results show that the gains come not merely from the interface itself, but from which labeling actions the users chose to perform. As interactive learning systems continue to expand the palette of interactive options (e.g., la4Science has four labels and a larger entropy range, which might explain the importance of the entropy factor here. Also, labels are more related to natural clusterings in this corpus (Nigam et al., 2000), s</context>
</contexts>
<marker>Settles, 2011</marker>
<rawString>B. Settles. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1467–1478. ACL Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>