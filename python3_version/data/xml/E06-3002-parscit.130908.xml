<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<note confidence="0.6737585">
What Humour Tells Us About Discourse Theories
Arjun Karande
Indian Institute of Technology Kanpur
Kanpur 208016, India
</note>
<email confidence="0.865999">
arjun@iitk.ac.in
</email>
<sectionHeader confidence="0.978754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999756608695652">
Many verbal jokes, like garden path sen-
tences, pose difficulties to models of dis-
course since the initially primed interpre-
tation needs to be discarded and a new one
created based on subsequent statements.
The effect of the joke depends on the
fact that the second (correct) interpretation
was not visible earlier. Existing models
of discourse semantics in principle gen-
erate all interpretations of discourse frag-
ments and carry these until contradicted,
and thus the dissonance criteria in humour
cannot be met. Computationally, main-
taining all possible worlds in a discourse
is very inefficient, thus computing only the
maximum-likelihood interpretation seems
to be a more efficient choice on average.
In this work we outline a probabilistic
lexicon based lexical semantics approach
which seems to be a reasonable construct
for discourse in general and use some ex-
amples from humour to demonstrate its
working.
</bodyText>
<sectionHeader confidence="0.987748" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984164756756757">
Consider the following :
(1) I still miss my ex-wife, but my aim is
improving.
(2) The horse raced past the barn fell.
In a discourse structure common to many jokes,
the first part of (1) has a default set of interpre-
tations, say P1, for which no consistent interpre-
tation can be found when the second part of the
joke is uttered. After a search, the listener reaches
Figure 1: Cognitive model of destructive disso-
nance as in joke (1). The initial sentence primes
the possible world P1 where miss is taken in an
emotional sense. After encountering the word aim
this is destroyed and eventually a new world P2
arises where miss is taken in the physical sense.
the alternate set of interpretations P2 (Figure 1).
A similar process holds for garden path sentences
such as (2), where the default interpretation cre-
ated in the first part (upto the word barn) has to be
discarded when the last part is heard. The search
involved in identifying the second interpretation
is an important indicator of human communica-
tion, and linguistic impairment such as autism of-
ten leads to difficulty in identifying jokes.
Yet, this aspect of discourse is not sufficiently
emphasized in most computational work. Cog-
nitively, this is a form of dissonance, a violation
of expectation. However, unlike some forms of
dissonance which may be constructive, leading to
metaphoric or implicature shifts, where part of the
original interpretation may be retained, these dis-
course structures are destructive, and the origi-
nal interpretation has to be completely abandoned,
and a new one searched out (Figure 2). Often
this is because the default interpretation involves
a sense-association that has very high coherence
in the immediate context, but is nullified by later
</bodyText>
<figure confidence="0.980358555555556">
J1
TP
J2
P1
search
gap
I still miss my ex-wife, but my aim is improving
time t
P2
</figure>
<page confidence="0.685615">
31
</page>
<figureCaption confidence="0.884097">
Figure 2: Cognitive Dissonance in Discourse (a-c)
</figureCaption>
<bodyText confidence="0.992663650793651">
can be Constructive, where the interpretation P1
does not disappear completely after the dissonant
utterance, or (d) Destructive, where P2 has to be
arrived at afresh and P1 is destroyed completely.
utterances.
While humour may involve a number of other
mechanisms such as allusion or stereotypes (Shi-
bles, 1989; Gruner, 1997), a wide class of ver-
bal humour exhibits destructive dissonance. For
a joke to work, the resulting interpretation must
result in an incongruity, what (Freud, 1960) calls
an ‘energy release’ that breaks the painful barriers
we have around forbidden thoughts.
Part of the difficulty in dealing with such shifts
is that it requires a rich model of discourse se-
mantics. Computational theories such as the
General Theory of Verbal Humour (Attardo and
Raskin, 1991) have avoided this difficult prob-
lem by adopting extra-linguistic knowledge in the
form of scripts, which encode different opposi-
tions that may arise in jokes. Others (Minsky,
1986) posit a general mechanism without con-
sidering specifics. Other models in computation
have attempted to generate jokes using templates
(Attardo and Raskin, 1994; Binsted and Ritchie,
1997) or recognize jokes using machine learning
models (Mihalcea and Strapparava, 2005).
Computationally, the fact that other less likely
interpretations such as P2 are not visible initially,
may also result in considerably efficiency in more
common situations, where ambiguities are not
generated to begin with. For example, in joke
(1) the interpretation after reading the first clause,
has the word miss referring to the abstract act of
missing a dear person. After hearing the punch
line, somewhere around the word aim, (the trigger
point TP), we have to revise our interpretation to
one where miss is used in a physical sense, as in
shooting a target. Then, the forbidden idea of hurt-
ing ex-wives generates the humour. By hiding this
meaning, the mechanism of destructive dissonance
enables the surprise which is the crux of the joke.
In the model proposed here, no extra-linguistic
sources of knowledge are appealed to. Lexical
Semantics proposes rich inter-relations encoding
knowledge within the lexicon itself (Pustejovksy,
1995; Jackendoff, 1990), and this work consid-
ers the possibility that such lexicons may eventu-
ally be able to carry discourse interpretations as
well, to the level of handling situations such as the
destructive transition from a possible-world P1 to
possible world P2. Clearly, a desideratum in such
a system would be that P1 would be the preferred
interpretation from the outset, so much so that P2,
which is in principle compatible with the joke, is
not even visible in the first part of the joke. It
would be reasonable to assume that such an inter-
pretation may be constructed as a “Winner Take
All” measure using probabilistic inter-relations in
the lexicon, built up based on usage frequencies.
This would differ from existing theories of dis-
course in several ways, as will be illustrated in the
following sections.
</bodyText>
<sectionHeader confidence="0.546181" genericHeader="method">
2 Models of Discourse
</sectionHeader>
<bodyText confidence="0.997498">
Formal semantics (Montague, 1973) looked at log-
ical structures, but it became evident that lan-
guage builds up on what is seemingly semantic
incompatibility, particularly in Gricean Implica-
ture (Grice, 1981). It became necessary to look
at the relations that describe interactions between
such structures. (Hobbs, 1985) introduces an early
theory of discourse and the notion of coherence
relations, which are applied recursively on dis-
course segments. Coherence relations, such as
Elaboration, Explanation and Contrast, are rela-
tions between discourse units that bind segments
of text into one global structure. (Grosz and Sid-
ner, 1986) incorporates two more important no-
tions into its model - the idea of intention and fo-
cus. The Rhetorical Structure Theory, introduced
in (Mann and Thompson, 1987), binds text spans
with rhetorical relations, which are discourse con-
nectives similar to coherence relations.
The Discourse Representation Theory (DRT)
(Kamp, 1984) computes inter-sentential anaphora
and attempts to maintain text cohesion through
sets of predicates, termed Discourse Representa-
tion Structures (DRSs), that represent discourse
</bodyText>
<figure confidence="0.978879153846154">
(a) (b)
(c)
P, P2
P, P2 P, P2
(d)
P, P2
32
Who supports Gorbachev?
Question-answer
pair
No one does
Explanation
He can still walk by himself
</figure>
<figureCaption confidence="0.997122">
Figure 3: Rhetorical Relations for joke (3)
</figureCaption>
<bodyText confidence="0.995944775862069">
units. A Principal DRS accumulates information
contained in the text, and forms the basis for re-
solving anaphora and discourse referents.
By marrying DRT to a rich set of rhetorical
relations, Segmented Discourse Representation
Theory (SDRT) (Lascarides and Asher, 2001)
attempts to to create a dynamic framework that
tries to bridge the semantic-pragmatic interface.
It consists of three components - Underspecified
Logical Formulae (ULF), Rhetorical Relations
and Glue Logic. Semantic representation in
the ULF acts as an interface to other levels.
Information in discourse units is represented by
a modified version of DRS, called Segmented
Discourse Representation Structures (SDRSs).
SDRSs are connected through rhetorical relations,
which posit relationships on SDRSs to bind them.
To illustrate, consider the discourse in (3):
(3) Who supports Gorbachev? No one does,
he can still walk by himself!
The rhetorical relations over the discourse are
shown in Figure 3. Here, Explanation induces
subordination and implies that the content of the
subordinate SDRSs work on further qualifying the
principal SDRS, while Question-Answer Pair in-
duces coordination. Rhetorical relations thus con-
nect semantic units together to formalize the flow
in a discourse. SDRT’s Glue Logic then runs se-
quentially on the ULF and rhetorical relations to
reduce underspecification and disambiguation and
derive inferences through the discourse. The way
inferencing is done is similar to DRT, with the ad-
ditional constraints that rhetorical relations spec-
ify.
A point to note is SDRT’s Maximum Dis-
course Coherence (MDC) Principle. This princi-
ple is used to resolve ambiguity in interpretation
by maximizing discourse coherence to obtain the
Pragmatically Preferred interpretation. There are
three conditions on which MDC works: (a) The
more rhetorical relations there are between two
units, the more coherent the discourse. (b) The
more anaphorae that are resolved, the more coher-
ent the discourse. (c) Some rhetorical relations
can be measured for coherence as well. For ex-
ample, the coherence of Contrast depends on how
dissimilar its connected prepositions are. SDRT
uses rhetorical relations and MDC to resolve lex-
ical and semantic ambiguities. For example, in
the utterance ‘John bought an apartment. But he
rented it’, the sense of rented is that of renting
out, and that is resolved in SDRT because the word
but cues the relation Contrast, which prefers an in-
terpretation that maximizes semantic contrast be-
tween its connectives.
Glue logic works by iteratively extracting sub-
sets of inferences through the flow of the dis-
course. This is discussed in more detail later.
</bodyText>
<subsectionHeader confidence="0.921814">
2.1 Lexicons for Discourse modeling
</subsectionHeader>
<bodyText confidence="0.999710444444444">
Pustejovsky’s Generative Lexicon (GL) model
(Pustejovksy, 1995) outlines an ambitious attempt
to formulate a lexical semantics framework that
can handle the unboundedness of linguistic ex-
pressions by providing a rich semantic structure,
a principled ontology of concepts (called qualia),
and a set of generative devices in which partici-
pants in a phrase or sentence can influence each
other’s semantic properties.
The ontology of concepts in GL is hierarchi-
cal, and concepts that exhibit similar behaviour
are grouped together into subsystems called Lexi-
cal Conceptual Paradigms (LCP). As an example,
the GL structure for door is an LCP that represents
both the use of door as a physical object such as in
‘he knocked on the door’, as well as an aperture
like in ‘he entered the door’.
In this work, we extend the GL structures to in-
corporate likelihood measures in the ontology and
the event structure relations. The Probabilistic
Qualia Structure, which outlines the ontological
hierarchy of a lexical item, also encodes frequency
information. Every time the target word appears
together with an ontologically connected concept,
the corresponding qualia features are strength-
ened. This results in a probabilistic model of
qualia features, which can in principle determine
</bodyText>
<page confidence="0.761533">
33
</page>
<bodyText confidence="0.999962517241379">
that a book has read as its maximally likely telic
role, but that in the context of the agent being the
author, write becomes more likely.
Generative mechanisms work on this semantic
structure to capture systematic polysemy in terms
of type shifts. Thus Type Coercion enforces se-
mantic constraints on the arguments of a predicate.
For example, ‘He enjoyed the book’ is coerced to
‘He enjoyed reading the book’ since enjoy requires
an activity, which is taken as the telic role of the
argument, i.e. that of book. Co-composition con-
strains the type-shifting of the predicate by its ar-
guments. An example is the difference between
‘bake a cake’ (creating a new object) versus ‘bake
beans’ (state change). Finally, Selective Binding
type-shifts a modifier based on the head. For ex-
ample, in ‘old man’ and ‘old book’, the property
being modified by old is shifted from physical-age
to information-recency.
To accommodate for likelihoods in generative
mechanisms, we need to incorporate conditional
probabilities between the lexical and ontological
entries that the mechanisms work on. These prob-
abilities can be stored within the lexicon itself or
integrated into the generative mechanisms. In ei-
ther case, mechanisms like Type Coercion should
no longer exhibit a default behaviour - the coer-
cion must change based on frequency of occur-
rence and context.
</bodyText>
<sectionHeader confidence="0.989934" genericHeader="method">
3 The Analysis of Humour
</sectionHeader>
<bodyText confidence="0.99917675">
The General Theory of Verbal Humour (GTVH),
introduced earlier, is a well-known computational
model of humour. It uses the notion of scripts
to account for the opposition in jokes. It models
humour as two opposing and overlapping scripts
put together in a discourse, one of which is
apparent and the other hidden from the reader till
a trigger point, when the hidden script suddenly
surfaces, generating humour. However, the notion
of scripts implies that there is a script for every
occasion, which severely limits the theory. On the
other hand, models of discourse are more general
and do not require scripts. However, they lack the
mechanism needed to capture such oppositions.
In addition to joke (3), consider:
(4) Two guys walked into a bar. The third
one ducked.
The humour in joke (4) results from the polyse-
mous use of the word bar. The first sentence leads
us to believe that bar is a place where one drinks,
but the second sentence forces us to revise our in-
terpretation to mean a solid object. GTVH would
use the DRINKING BAR script before the trigger
and the COLLISION script after. Joke (3), quoted
in Raskin’s work as well, contains an obvious op-
position. The first sentence invokes the sense of
support being that of political support. The second
sentence introduces the opposition, and the mean-
ing of support is changed to that of physical sup-
port.
In all examples discussed so far, the key
observations are that (i) a single inference is
primed by the reader, (ii) this primary inference
suppresses other inferences until (iii) a trigger
point is reached.
To formalize the unfolding of a joke, we re-
fer back to Figure 1. Let t be a point along the
timeline. When t &lt; TP, both P1 and P2 are com-
patible, and the possible world is P = P1 U P2.
P1 is the preferred interpretation and P2 is hidden.
When t = TP, J2 is introduced, and P1 becomes
incompatible with P2, and P1 may also lose
compatibility with J2. P2 now surfaces as the
preferred inference. The reader has to invoke a
search to find P2, which is represented by the
search gap.
A possible world Pz = {qz1, qz2, ... , qzk}
where qmn is an inference. Two worlds Pz and Pj
are incompatible if there exists any pair of sets of
inferences whose intersection is a contradiction.
i.e.
Pz is said to be incompatible with
</bodyText>
<equation confidence="0.906067">
Pj iff ] {qz1, qz2, . . . , qzk} C Pz n
I {qj1, qj2, ... , qjlI C Pj such that
{qz1 n qz2 n ... qzk n qj1 n qj2 n ... qjlI ==&gt;- F.
</equation>
<bodyText confidence="0.9993114">
They are said to be compatible if no such subsets
exist.
We now explore in detail why compositional
discourse models fail to handle the mechanisms of
humour.
</bodyText>
<subsectionHeader confidence="0.976578">
3.1 Beyond Scripts - Why Verbal Humour
Should Be Winner Take All
</subsectionHeader>
<bodyText confidence="0.999710666666667">
An argument against the approach of existing dis-
course models like SDRT concerns their iterative
inferencing. At each point in the process of infer-
</bodyText>
<equation confidence="0.779001">
34
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎣
</equation>
<bodyText confidence="0.9997916">
encing, SDRT’s Glue Logic carries over all inter-
pretations possible within its constraints as a set.
MDC ranks contending inferences, allowing less
preferred inferences to be discarded, and the result
of this process is a subset of the input to it. Con-
trasting inferences can coexist through underspec-
ification, and the contrast is resolved when one of
them loses compatibility. This is cognitively un-
likely; (Miller, 1956) has shown that the human
brain actively retains only around seven units of
information. With such a limited working mem-
ory, it is not cognitively feasible to model dis-
course analysis in this manner. Cognitive models
working with limited-capacity short-term memory
like in (Lewis, 1996) support the same intuition.
Thus, a better approach would be a Winner Take
All (WTA) approach, where the most likely inter-
pretation, called the winner, suppresses all other
interpretations as we move through the discourse.
The model must be revised to reflect new contexts
if they are incompatible with the existing model.
Let us now explore this with respect to joke
(3). There is a Question-Answer relation between
the first sentence and the next two. The semantic
representation for the first sentence alone is:
</bodyText>
<equation confidence="0.818634">
lx(support(x, Gorbachev)), x =?
</equation>
<bodyText confidence="0.999921666666666">
The x =? indicates a missing referent for
who. Using GL, it is not difficult to resolve the
sense of support to mean that of political support.
To elaborate, the lexical entry of Gorbachev
is an LCP of two senses - that of the head of
government and that of an animate, as shown:
</bodyText>
<equation confidence="0.998783857142857">
Gorbachev &amp;quot;
ARG1 =x: man #
ARGSTR = ARG2 =y: head of govt
&amp;quot; D-ARG3 =z: community
human.president lcp #
QUALIA = FORMAL = p(x, y)
TELIC = govern(y,z)
</equation>
<bodyText confidence="0.999818666666667">
The two senses of support applicable in this
context are that of physical support and of political
support. We use abstract support as a generaliza-
tion of the political sense. The analysis of the first
sentence alone would allow for both these possi-
bilities:
</bodyText>
<equation confidence="0.999688357142857">
supportabe � �
ARG1 =x: animate
ARGSTR =
ARG2 =y: abstract entity
EVENTSTR = [ E1 = e1 :process ]
r FORMAL = supportabe act(e1, x, y) 1
QUALIA = L AGENTIVE =... J
� �
ARG1 =x: physical entity
ARGSTR = ARG2 =y: physical entity
EVENTSTR = � E1 = e1 : process �
� �
FORMAL = supportphy act(e1, x, y)
QUALIA = AGENTIVE =...
</equation>
<bodyText confidence="0.9988424">
Thus, after the first sentence, the sense of
support includes both senses, i.e. support E
{supportabs, supportphy}.
We then come across the second sentence and
establish the semantic representation for it, as
well as establish rhetorical relations. We find
that the sentence contains walk(z). SDRT’s
Right Frontier Rule resolves the referent he to
Gorbachev. Also, the clause ‘no one does’
resolves the referent x to null. Thus, we get:
</bodyText>
<equation confidence="0.890594555555555">
walk(Gorbachev) n support(null, Gorbachev)
Now consider the lexical entry for walk:
walk
ARGSTR = [ ARG1 =x: animate ]
EVENTSTR = � E1 = e1 : process �
�
FORMAL = walk act(e1, x)
QUALIA =
AGENTIVE =walk begin(e1, x)
</equation>
<bodyText confidence="0.9998125">
The action walk requires an animate argument.
Since walk(Gorbachev) is true, the sense of sup-
port in the previous sentence is restricted to mean
physical support, i.e. support = supportphy,
since only supportphy can take an animate argu-
ment as its object - the abstract entity require-
ment of supportabs causes it to be ruled out, end-
ing at a final inference.
The change of sense for support is key to the
generation of humour, but SDRT fails to recog-
nize the shift since it neither has any priming
mechanism nor revision of models built into it.
It merely works by restricting the possible infer-
ences as more information becomes available. Re-
ferring to Figure 1 again, SDRT will only account
for the refinement of possible worlds from P1 UP2
to P2. It will not be able to account for the priming
of either Pi, which is required.
</bodyText>
<sectionHeader confidence="0.993364" genericHeader="method">
4 A Probabilistic Semantic Lexicon
</sectionHeader>
<bodyText confidence="0.999523">
We now introduce a WTA model under which
priming could be well accounted for. We would
like a model under which a single interpretation is
made at each point in the analysis. We want a set
</bodyText>
<figure confidence="0.968735142857143">
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎤
⎦⎥⎥⎥⎥⎥
⎤
⎦⎥⎥⎥⎥⎥
supportphy
⎤
⎦⎥⎥⎥⎥⎥
⎡
⎢ ⎢ ⎢ ⎣
⎤
⎦⎥⎥⎥
35
</figure>
<bodyText confidence="0.851276">
of possible worlds P such that:
</bodyText>
<equation confidence="0.756322833333333">
J1 )WTA P =
{p : p is a world consistent with J1}
WTA ensures that only the prime world P is
chosen by J1. When J2 is analyzed, no world
p E P can satisfy J2, i.e:
Vp E P,-J2 ) p
</equation>
<bodyText confidence="0.857343">
In this case, we need to backtrack and find
another set P&apos; that satisfies both J1 and J2, i.e:
</bodyText>
<equation confidence="0.855528">
(J1, J2) )WTA P&apos;
</equation>
<bodyText confidence="0.998046285714286">
In Figure 1, P = P1 and P&apos; = P2.
The most appropriate way to achieve this is
to include the priming in the lexicon itself. We
present a lexical structure where senses of com-
positional units are attributed with a probability of
occurrence approximated by its frequency count.
The probability of a composition can then be cal-
culated from the individual probabilities. The
highest probability is primed. Thus, at every point
in the discourse, only one inference emerges as
primary and suppresses all other inferences. As
an example, the proposed structure for Gorbachev
is presented below:
Instead of using the concept of an LCP as in
classical GL, we assign probabilities to each sense
encountered. These probabilities can then facili-
tate priming.
To add weight to the argument with empirical
data, we use WordNet (Fellbaum, 1998), built on
the British National Corpus, as an approximation
for frequency counts. We find that
</bodyText>
<equation confidence="0.9965555">
P(supportabs) = 0.59 and
P(supportphy) = 0.36.
</equation>
<bodyText confidence="0.994081666666667">
Similarly, for the notion of Gorbachev, it is
plausible to assume that Gorbachev as head of
government is more meaningful for most of us,
rather than just another old man. In order to make
an inference after the first sentence, we need to
search for the correct interpretation, i.e. we need
to find argmaxi,j(P(supporti/Gorbachevj)),
which intuitively should be
P(supportabs/head of govt). Making a
similar analysis as in the previous section,
the second sentence should violate the first
assumption, since walk(Gorbachev) can-
not be true (since P(abstract entity) = 0).
Thus, we need to revise our inference, mov-
ing back to the first sentence and choosing
max(P(supporti/Gorbachevj)) that is compati-
ble with the second sentence. This turns out to be
P(supportphy/animate). Thus, the distinct shift
between inferences is captured in the course of
analysis. Cognitive studies such as the studies on
Garden Path Sentences strengthen this approach
to analysis. (Lewis, 1996), for example, presents
a model that predicts cognitive observations with
very limited working memory.
Storing the inter-lexical conditional proba-
bilities is also an issue, as mentioned ear-
lier. Where, for example, do we store
P(supporti/Gorbachevj)? One possible ap-
proach would be to store them with either lexical
item. A better approach would be to bestow the re-
sponsibility of calculating these probabilities upon
the generative mechanisms of the semantic lexicon
whenever possible.
Let us now analyze joke (1) under the prob-
abilistic framework. Again, approximations for
probability of occurrence will be taken from
WordNet. The entry for wife in WordNet lists just
one sense, and so we assign a probability of 1 to it
in its lexical entry:
</bodyText>
<equation confidence="0.988497285714286">
wife
� �
ARG1 =x: woman
ARGSTR = D-ARG2 =y: man
� FORMAL = husband(x) = y
QUALIA = AGENTIVE = marriage(x, y)
p(woman) = 1
</equation>
<bodyText confidence="0.999778">
The humour is generated due to the lexical am-
biguity of miss. We list the lexical entries of the
two senses of miss that apply in this context - the
first being an abstract emotional state and the other
being a physical process.
</bodyText>
<equation confidence="0.994986142857143">
Gorbachev �
ARG1 =x: man
ARGSTR = ARG2 =y: head of govt
D-ARG3 =z: community
FORMAL = p(x, y)
p(man) = p1
p(head_of_govt) = p2
</equation>
<figure confidence="0.996332333333333">
...
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎢
QUALIA = ⎣
⎤
⎥ ⎥ ⎥
1 ⎦
⎡
⎢ ⎢ ⎢ ⎢ ⎣
⎤
⎦⎥⎥⎥⎥
36
∃x(δgoodness(aim(x)) &gt; 0)
⎤
⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥
I still miss my ex-wife
</figure>
<figureCaption confidence="0.998903">
Figure 4: Rhetorical relations for joke (1)
</figureCaption>
<equation confidence="0.997878529411765">
missabe � �
ARG1 =x: animate
ARGSTR = ARG2 =y: entity
EVENTSTRr= I E1 = e1 : state
I FORMAL = missabe act(e1, x, y) 1
QUALIA = L AGENTIVE =... J
missihy �
ARG1 =x: physical entity
ARGSTR = ARG2 =y: physical entity
D-ARG1 =z: trajector
E1 = e1 : process
E2 = e2 : state
RESTR2 =&lt;α
HEAD2 = e2
�
FORMAL = missedihy act(e2, x, y, z)
QUALIA = AGENTIVE = shoot(e1, x, y, z)
</equation>
<bodyText confidence="0.999073666666667">
The Rhetorical Relations for joke (1) are
presented in Figure 4. After parsing the first
sentence, the logical representation obtained is:
</bodyText>
<equation confidence="0.9709635">
∃e1∃e2∃e3∃x∃y(wife(e1, x, y) ∧
divorce(e2, x, y)∧miss(e3, x, y)∧e1 &lt; e2 &lt; e3)
</equation>
<bodyText confidence="0.99948">
To arrive at a prime inference, note that the
semantic types of the arguments of both
senses of miss are exclusive, and hence
</bodyText>
<equation confidence="0.9596005">
P(physical entity/missphy) = 1 and
P(entity/missabs) = 1. Thus, using Bayes
</equation>
<bodyText confidence="0.52479675">
Theorem, to compare P(missabs/entity) and
P(missphy/physical entity), it is sufficient to
compare P(missabs) and P(missphy). From
WordNet,
</bodyText>
<equation confidence="0.9688335">
P(missabs) = 0.22 and
P(missphy) = 0.06.
</equation>
<bodyText confidence="0.997045227272727">
Thus, the primed inference has miss =
missabs. The second sentence has the following
logical representation:
This simply means that a measure of the
aim, called goodness, is undergoing a positive
change. The word but is a cue for a Contrast
relation between the two sentences, while the
discourse suggests Parallelism. The two senses of
aim compatible with the first sentence are aimabs,
which is synonymous to goal, and aimphy,
referring to the physical sense of missing. We
now need to consider P(aimabs/missabs) and
P(aimphy/missphy). The semantic constraints
of the rhetorical relation Contrast ensures that
the second is more coherent, i.e. it is more
probable that the contrast of physical aim get-
ting better is more coherent with the physical
sense of miss, and we expect this to be re-
flected in usage frequency as well. Therefore
P(aimabs/missabs) &lt; P(aimphy/missphy),
and we need to shift our inference and make
miss = missphy.
As a final assertion of the probabilistic ap-
proach, consider:
(5) You can lead a child to college, but you
cannot make him think.
The incongruity in joke (5) does not result from
a syntactical or semantic ambiguity at all, and yet
it induces dissonance. The dissonance is not a
result of compositionality, but due to the access
of a whole linguistic structure, i.e. we recall the
familiar proverb ‘You can lead a horse to water
but you cannot make it drink’, and the deviation
from the recognizable structure causes the viola-
tion of our expectations. Thus, access is not re-
stricted to the lexical level; we seem to store and
access bigger units of discourse if encountered fre-
quently enough. The only way to do justice to this
joke would be to encode the entire sentential struc-
ture directly into the lexicon. Our model will now
also consider these larger chunks, whose meaning
is specified atomically. The dissonance will now
come from the semantic difference between the
accessed expression and the one under analysis.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99985175">
We have examined the mechanisms behind verbal
humour and shown how existing discourse mod-
els are inadequate at capturing the mechanisms
of humour. We have proposed a probabilistic
</bodyText>
<figure confidence="0.964510357142857">
Contrast, Parallel
But my aim is improving
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
⎢
EVENTSTR = ⎣
⎤
⎦⎥
⎤
⎦⎥⎥⎥⎥⎥
37
</figure>
<bodyText confidence="0.99943625">
WTA model based on lexical frequency distribu-
tions that is more capable at handling humour, and
is based on the notion of expectation and disso-
nance.
It would be interesting now to find necessary
and sufficient conditions under this framework for
humour to be generated. Although the above
framework can identify incongruity in humour dis-
course, the same mechanisms are used and indeed
are often integral to other forms of literature. Po-
ems, for example, often rely on such mechanisms.
Are Freudian thoughts the key to separating hu-
mour from the rest, or is it a result of the inten-
tional misleading done by the speaker of a joke?
Also, it would be very interesting to find an empir-
ical link between the extent of incongruity in jokes
in our framework and the way people respond to
them.
Finally, a very interesting question is the acqui-
sition of the lexicon under such a model. How are
lexical semantic models learned by the language
acquirer probabilistically? An exploration of the
question might result in a cognitively sound com-
putational model for acquisition.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987">
Salvatore Attardo and Victor Raskin. 1991. Script the-
ory revis(it)ed: Joke similarity and joke representa-
tion model. 4(3):293–347.
Savatore Attardo and Victor Raskin. 1994. Non-
literalness and non-bona-fide in language: An ap-
proach to formal and computational treatments of
humor. volume 2, pages 31–69.
Kim Binsted and Graeme Ritchie. 1997. Computa-
tional rules for generating punning riddles. HU-
MOR - International Journal of Humor Research,
10(1):25–76.
Christiane Fellbaum. 1998. WordNet - An Electronic
Lexical Database. MIT Press.
Sigmund Freud. 1960. Jokes and their relation to the
unconscious. The Standard Edition of the Complete
Psychological Works of Sigmund Freud.
Herbet Paul Grice. 1981. Presupposition and conver-
sational implicature. In Radical Pragmatics, pages
183–197. New York: Academic Press.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175–204.
Charles R. Gruner. 1997. The Game of Humor:
A Comprehensive Theory of Why We Laugh. NJ:
Transaction Publishers.
Jerry R Hobbs. 1985. On the coherence and structure
of discourse. Technical Report CSLI-85-37, Center
for the Study of Language and Information, Stanford
University.
Ray S. Jackendoff. 1990. Semantic Structures. MIT
Press.
H. Kamp. 1984. A theory of truth and semantic rep-
resentation. In J. Groenendijk, T. M. V. Janssen,
and M. Stokhof, editors, Truth, Interpretation and
Information: Selected Papers from the Third Ams-
terdam Colloquium, pages 1–41. Foris Publications,
Dordrecht.
Asher Lascarides and Nicolas Asher. 2001. Seg-
mented discourse representation theory: Dynamic
semantics with discourse structure. Computing
Meaning, 3.
Richard L. Lewis. 1996. Interference in short-term
memory: The magical number two (or three) in sen-
tence processing. Journal of Psycholinguistic Re-
search, 25(1):93 – 115.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, Center
for the Study of Language and Information, Stan-
ford University.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Joint Conference on Human Lan-
guage Technology / Empirical Methods in Natural
Language Processing (HLT/EMNLP).
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81–97.
Marvin Minsky. 1986. The Society of Mind. Simon
and Schuster.
Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Approaches
to Natural Language, pages 221–242. D. Reidel.
James Pustejovksy. 1995. The Generative Lexicon.
MIT Press.
Warren Shibles. 1989. Humor reference guide.
http://facstaff.uww.edu/shiblesw/humorbook.
</reference>
<page confidence="0.953591">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.736414">
<title confidence="0.999768">What Humour Tells Us About Discourse Theories</title>
<author confidence="0.823676">Arjun Karande</author>
<affiliation confidence="0.91681">Indian Institute of Technology Kanpur</affiliation>
<address confidence="0.994427">Kanpur 208016, India</address>
<email confidence="0.978052">arjun@iitk.ac.in</email>
<abstract confidence="0.996519583333333">Many verbal jokes, like garden path sentences, pose difficulties to models of discourse since the initially primed interpretation needs to be discarded and a new one created based on subsequent statements. The effect of the joke depends on the fact that the second (correct) interpretation was not visible earlier. Existing models of discourse semantics in principle generate all interpretations of discourse fragments and carry these until contradicted, and thus the dissonance criteria in humour cannot be met. Computationally, maintaining all possible worlds in a discourse is very inefficient, thus computing only the maximum-likelihood interpretation seems to be a more efficient choice on average. In this work we outline a probabilistic lexicon based lexical semantics approach which seems to be a reasonable construct for discourse in general and use some examples from humour to demonstrate its working.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salvatore Attardo</author>
<author>Victor Raskin</author>
</authors>
<title>Script theory revis(it)ed: Joke similarity and joke representation model.</title>
<date>1991</date>
<pages>4--3</pages>
<contexts>
<context position="3734" citStr="Attardo and Raskin, 1991" startWordPosition="603" endWordPosition="606">afresh and P1 is destroyed completely. utterances. While humour may involve a number of other mechanisms such as allusion or stereotypes (Shibles, 1989; Gruner, 1997), a wide class of verbal humour exhibits destructive dissonance. For a joke to work, the resulting interpretation must result in an incongruity, what (Freud, 1960) calls an ‘energy release’ that breaks the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that may arise in jokes. Others (Minsky, 1986) posit a general mechanism without considering specifics. Other models in computation have attempted to generate jokes using templates (Attardo and Raskin, 1994; Binsted and Ritchie, 1997) or recognize jokes using machine learning models (Mihalcea and Strapparava, 2005). Computationally, the fact that other less likely interpretations such as P2 are not visible initially, may also result in considerably efficiency i</context>
</contexts>
<marker>Attardo, Raskin, 1991</marker>
<rawString>Salvatore Attardo and Victor Raskin. 1991. Script theory revis(it)ed: Joke similarity and joke representation model. 4(3):293–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Savatore Attardo</author>
<author>Victor Raskin</author>
</authors>
<title>Nonliteralness and non-bona-fide in language: An approach to formal and computational treatments of humor.</title>
<date>1994</date>
<volume>2</volume>
<pages>31--69</pages>
<contexts>
<context position="4075" citStr="Attardo and Raskin, 1994" startWordPosition="654" endWordPosition="657">ergy release’ that breaks the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that may arise in jokes. Others (Minsky, 1986) posit a general mechanism without considering specifics. Other models in computation have attempted to generate jokes using templates (Attardo and Raskin, 1994; Binsted and Ritchie, 1997) or recognize jokes using machine learning models (Mihalcea and Strapparava, 2005). Computationally, the fact that other less likely interpretations such as P2 are not visible initially, may also result in considerably efficiency in more common situations, where ambiguities are not generated to begin with. For example, in joke (1) the interpretation after reading the first clause, has the word miss referring to the abstract act of missing a dear person. After hearing the punch line, somewhere around the word aim, (the trigger point TP), we have to revise our interpr</context>
</contexts>
<marker>Attardo, Raskin, 1994</marker>
<rawString>Savatore Attardo and Victor Raskin. 1994. Nonliteralness and non-bona-fide in language: An approach to formal and computational treatments of humor. volume 2, pages 31–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Binsted</author>
<author>Graeme Ritchie</author>
</authors>
<title>Computational rules for generating punning riddles. HUMOR -</title>
<date>1997</date>
<journal>International Journal of Humor Research,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="4103" citStr="Binsted and Ritchie, 1997" startWordPosition="658" endWordPosition="661">the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that may arise in jokes. Others (Minsky, 1986) posit a general mechanism without considering specifics. Other models in computation have attempted to generate jokes using templates (Attardo and Raskin, 1994; Binsted and Ritchie, 1997) or recognize jokes using machine learning models (Mihalcea and Strapparava, 2005). Computationally, the fact that other less likely interpretations such as P2 are not visible initially, may also result in considerably efficiency in more common situations, where ambiguities are not generated to begin with. For example, in joke (1) the interpretation after reading the first clause, has the word miss referring to the abstract act of missing a dear person. After hearing the punch line, somewhere around the word aim, (the trigger point TP), we have to revise our interpretation to one where miss is</context>
</contexts>
<marker>Binsted, Ritchie, 1997</marker>
<rawString>Kim Binsted and Graeme Ritchie. 1997. Computational rules for generating punning riddles. HUMOR - International Journal of Humor Research, 10(1):25–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20663" citStr="Fellbaum, 1998" startWordPosition="3447" endWordPosition="3448">robability of occurrence approximated by its frequency count. The probability of a composition can then be calculated from the individual probabilities. The highest probability is primed. Thus, at every point in the discourse, only one inference emerges as primary and suppresses all other inferences. As an example, the proposed structure for Gorbachev is presented below: Instead of using the concept of an LCP as in classical GL, we assign probabilities to each sense encountered. These probabilities can then facilitate priming. To add weight to the argument with empirical data, we use WordNet (Fellbaum, 1998), built on the British National Corpus, as an approximation for frequency counts. We find that P(supportabs) = 0.59 and P(supportphy) = 0.36. Similarly, for the notion of Gorbachev, it is plausible to assume that Gorbachev as head of government is more meaningful for most of us, rather than just another old man. In order to make an inference after the first sentence, we need to search for the correct interpretation, i.e. we need to find argmaxi,j(P(supporti/Gorbachevj)), which intuitively should be P(supportabs/head of govt). Making a similar analysis as in the previous section, the second sen</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet - An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sigmund Freud</author>
</authors>
<title>Jokes and their relation to the unconscious. The Standard Edition of the Complete Psychological Works of Sigmund Freud.</title>
<date>1960</date>
<contexts>
<context position="3438" citStr="Freud, 1960" startWordPosition="557" endWordPosition="558">search gap I still miss my ex-wife, but my aim is improving time t P2 31 Figure 2: Cognitive Dissonance in Discourse (a-c) can be Constructive, where the interpretation P1 does not disappear completely after the dissonant utterance, or (d) Destructive, where P2 has to be arrived at afresh and P1 is destroyed completely. utterances. While humour may involve a number of other mechanisms such as allusion or stereotypes (Shibles, 1989; Gruner, 1997), a wide class of verbal humour exhibits destructive dissonance. For a joke to work, the resulting interpretation must result in an incongruity, what (Freud, 1960) calls an ‘energy release’ that breaks the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that may arise in jokes. Others (Minsky, 1986) posit a general mechanism without considering specifics. Other models in computation have attempted to generate jokes usin</context>
</contexts>
<marker>Freud, 1960</marker>
<rawString>Sigmund Freud. 1960. Jokes and their relation to the unconscious. The Standard Edition of the Complete Psychological Works of Sigmund Freud.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbet Paul Grice</author>
</authors>
<title>Presupposition and conversational implicature.</title>
<date>1981</date>
<booktitle>In Radical Pragmatics,</booktitle>
<pages>183--197</pages>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="6180" citStr="Grice, 1981" startWordPosition="993" endWordPosition="994">h the joke, is not even visible in the first part of the joke. It would be reasonable to assume that such an interpretation may be constructed as a “Winner Take All” measure using probabilistic inter-relations in the lexicon, built up based on usage frequencies. This would differ from existing theories of discourse in several ways, as will be illustrated in the following sections. 2 Models of Discourse Formal semantics (Montague, 1973) looked at logical structures, but it became evident that language builds up on what is seemingly semantic incompatibility, particularly in Gricean Implicature (Grice, 1981). It became necessary to look at the relations that describe interactions between such structures. (Hobbs, 1985) introduces an early theory of discourse and the notion of coherence relations, which are applied recursively on discourse segments. Coherence relations, such as Elaboration, Explanation and Contrast, are relations between discourse units that bind segments of text into one global structure. (Grosz and Sidner, 1986) incorporates two more important notions into its model - the idea of intention and focus. The Rhetorical Structure Theory, introduced in (Mann and Thompson, 1987), binds </context>
</contexts>
<marker>Grice, 1981</marker>
<rawString>Herbet Paul Grice. 1981. Presupposition and conversational implicature. In Radical Pragmatics, pages 183–197. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="6609" citStr="Grosz and Sidner, 1986" startWordPosition="1054" endWordPosition="1058">tics (Montague, 1973) looked at logical structures, but it became evident that language builds up on what is seemingly semantic incompatibility, particularly in Gricean Implicature (Grice, 1981). It became necessary to look at the relations that describe interactions between such structures. (Hobbs, 1985) introduces an early theory of discourse and the notion of coherence relations, which are applied recursively on discourse segments. Coherence relations, such as Elaboration, Explanation and Contrast, are relations between discourse units that bind segments of text into one global structure. (Grosz and Sidner, 1986) incorporates two more important notions into its model - the idea of intention and focus. The Rhetorical Structure Theory, introduced in (Mann and Thompson, 1987), binds text spans with rhetorical relations, which are discourse connectives similar to coherence relations. The Discourse Representation Theory (DRT) (Kamp, 1984) computes inter-sentential anaphora and attempts to maintain text cohesion through sets of predicates, termed Discourse Representation Structures (DRSs), that represent discourse (a) (b) (c) P, P2 P, P2 P, P2 (d) P, P2 32 Who supports Gorbachev? Question-answer pair No one</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles R Gruner</author>
</authors>
<title>The Game of Humor: A Comprehensive Theory of Why We Laugh. NJ:</title>
<date>1997</date>
<publisher>Transaction Publishers.</publisher>
<contexts>
<context position="3275" citStr="Gruner, 1997" startWordPosition="531" endWordPosition="532">his is because the default interpretation involves a sense-association that has very high coherence in the immediate context, but is nullified by later J1 TP J2 P1 search gap I still miss my ex-wife, but my aim is improving time t P2 31 Figure 2: Cognitive Dissonance in Discourse (a-c) can be Constructive, where the interpretation P1 does not disappear completely after the dissonant utterance, or (d) Destructive, where P2 has to be arrived at afresh and P1 is destroyed completely. utterances. While humour may involve a number of other mechanisms such as allusion or stereotypes (Shibles, 1989; Gruner, 1997), a wide class of verbal humour exhibits destructive dissonance. For a joke to work, the resulting interpretation must result in an incongruity, what (Freud, 1960) calls an ‘energy release’ that breaks the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that m</context>
</contexts>
<marker>Gruner, 1997</marker>
<rawString>Charles R. Gruner. 1997. The Game of Humor: A Comprehensive Theory of Why We Laugh. NJ: Transaction Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse.</title>
<date>1985</date>
<tech>Technical Report CSLI-85-37,</tech>
<institution>Center for the Study of Language and Information, Stanford University.</institution>
<contexts>
<context position="6292" citStr="Hobbs, 1985" startWordPosition="1009" endWordPosition="1010">erpretation may be constructed as a “Winner Take All” measure using probabilistic inter-relations in the lexicon, built up based on usage frequencies. This would differ from existing theories of discourse in several ways, as will be illustrated in the following sections. 2 Models of Discourse Formal semantics (Montague, 1973) looked at logical structures, but it became evident that language builds up on what is seemingly semantic incompatibility, particularly in Gricean Implicature (Grice, 1981). It became necessary to look at the relations that describe interactions between such structures. (Hobbs, 1985) introduces an early theory of discourse and the notion of coherence relations, which are applied recursively on discourse segments. Coherence relations, such as Elaboration, Explanation and Contrast, are relations between discourse units that bind segments of text into one global structure. (Grosz and Sidner, 1986) incorporates two more important notions into its model - the idea of intention and focus. The Rhetorical Structure Theory, introduced in (Mann and Thompson, 1987), binds text spans with rhetorical relations, which are discourse connectives similar to coherence relations. The Discou</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R Hobbs. 1985. On the coherence and structure of discourse. Technical Report CSLI-85-37, Center for the Study of Language and Information, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray S Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5154" citStr="Jackendoff, 1990" startWordPosition="823" endWordPosition="824"> missing a dear person. After hearing the punch line, somewhere around the word aim, (the trigger point TP), we have to revise our interpretation to one where miss is used in a physical sense, as in shooting a target. Then, the forbidden idea of hurting ex-wives generates the humour. By hiding this meaning, the mechanism of destructive dissonance enables the surprise which is the crux of the joke. In the model proposed here, no extra-linguistic sources of knowledge are appealed to. Lexical Semantics proposes rich inter-relations encoding knowledge within the lexicon itself (Pustejovksy, 1995; Jackendoff, 1990), and this work considers the possibility that such lexicons may eventually be able to carry discourse interpretations as well, to the level of handling situations such as the destructive transition from a possible-world P1 to possible world P2. Clearly, a desideratum in such a system would be that P1 would be the preferred interpretation from the outset, so much so that P2, which is in principle compatible with the joke, is not even visible in the first part of the joke. It would be reasonable to assume that such an interpretation may be constructed as a “Winner Take All” measure using probab</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>Ray S. Jackendoff. 1990. Semantic Structures. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
</authors>
<title>A theory of truth and semantic representation.</title>
<date>1984</date>
<booktitle>Truth, Interpretation and Information: Selected Papers from the Third Amsterdam Colloquium,</booktitle>
<pages>1--41</pages>
<editor>In J. Groenendijk, T. M. V. Janssen, and M. Stokhof, editors,</editor>
<publisher>Foris Publications,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="6936" citStr="Kamp, 1984" startWordPosition="1106" endWordPosition="1107">scourse and the notion of coherence relations, which are applied recursively on discourse segments. Coherence relations, such as Elaboration, Explanation and Contrast, are relations between discourse units that bind segments of text into one global structure. (Grosz and Sidner, 1986) incorporates two more important notions into its model - the idea of intention and focus. The Rhetorical Structure Theory, introduced in (Mann and Thompson, 1987), binds text spans with rhetorical relations, which are discourse connectives similar to coherence relations. The Discourse Representation Theory (DRT) (Kamp, 1984) computes inter-sentential anaphora and attempts to maintain text cohesion through sets of predicates, termed Discourse Representation Structures (DRSs), that represent discourse (a) (b) (c) P, P2 P, P2 P, P2 (d) P, P2 32 Who supports Gorbachev? Question-answer pair No one does Explanation He can still walk by himself Figure 3: Rhetorical Relations for joke (3) units. A Principal DRS accumulates information contained in the text, and forms the basis for resolving anaphora and discourse referents. By marrying DRT to a rich set of rhetorical relations, Segmented Discourse Representation Theory (</context>
</contexts>
<marker>Kamp, 1984</marker>
<rawString>H. Kamp. 1984. A theory of truth and semantic representation. In J. Groenendijk, T. M. V. Janssen, and M. Stokhof, editors, Truth, Interpretation and Information: Selected Papers from the Third Amsterdam Colloquium, pages 1–41. Foris Publications, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Lascarides</author>
<author>Nicolas Asher</author>
</authors>
<title>Segmented discourse representation theory: Dynamic semantics with discourse structure.</title>
<date>2001</date>
<journal>Computing Meaning,</journal>
<volume>3</volume>
<contexts>
<context position="7570" citStr="Lascarides and Asher, 2001" startWordPosition="1201" endWordPosition="1204">es inter-sentential anaphora and attempts to maintain text cohesion through sets of predicates, termed Discourse Representation Structures (DRSs), that represent discourse (a) (b) (c) P, P2 P, P2 P, P2 (d) P, P2 32 Who supports Gorbachev? Question-answer pair No one does Explanation He can still walk by himself Figure 3: Rhetorical Relations for joke (3) units. A Principal DRS accumulates information contained in the text, and forms the basis for resolving anaphora and discourse referents. By marrying DRT to a rich set of rhetorical relations, Segmented Discourse Representation Theory (SDRT) (Lascarides and Asher, 2001) attempts to to create a dynamic framework that tries to bridge the semantic-pragmatic interface. It consists of three components - Underspecified Logical Formulae (ULF), Rhetorical Relations and Glue Logic. Semantic representation in the ULF acts as an interface to other levels. Information in discourse units is represented by a modified version of DRS, called Segmented Discourse Representation Structures (SDRSs). SDRSs are connected through rhetorical relations, which posit relationships on SDRSs to bind them. To illustrate, consider the discourse in (3): (3) Who supports Gorbachev? No one d</context>
</contexts>
<marker>Lascarides, Asher, 2001</marker>
<rawString>Asher Lascarides and Nicolas Asher. 2001. Segmented discourse representation theory: Dynamic semantics with discourse structure. Computing Meaning, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
</authors>
<title>Interference in short-term memory: The magical number two (or three) in sentence processing.</title>
<date>1996</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>25</volume>
<issue>1</issue>
<pages>115</pages>
<contexts>
<context position="16178" citStr="Lewis, 1996" startWordPosition="2636" endWordPosition="2637">et. MDC ranks contending inferences, allowing less preferred inferences to be discarded, and the result of this process is a subset of the input to it. Contrasting inferences can coexist through underspecification, and the contrast is resolved when one of them loses compatibility. This is cognitively unlikely; (Miller, 1956) has shown that the human brain actively retains only around seven units of information. With such a limited working memory, it is not cognitively feasible to model discourse analysis in this manner. Cognitive models working with limited-capacity short-term memory like in (Lewis, 1996) support the same intuition. Thus, a better approach would be a Winner Take All (WTA) approach, where the most likely interpretation, called the winner, suppresses all other interpretations as we move through the discourse. The model must be revised to reflect new contexts if they are incompatible with the existing model. Let us now explore this with respect to joke (3). There is a Question-Answer relation between the first sentence and the next two. The semantic representation for the first sentence alone is: lx(support(x, Gorbachev)), x =? The x =? indicates a missing referent for who. Using</context>
<context position="21775" citStr="Lewis, 1996" startWordPosition="3619" endWordPosition="3620"> be P(supportabs/head of govt). Making a similar analysis as in the previous section, the second sentence should violate the first assumption, since walk(Gorbachev) cannot be true (since P(abstract entity) = 0). Thus, we need to revise our inference, moving back to the first sentence and choosing max(P(supporti/Gorbachevj)) that is compatible with the second sentence. This turns out to be P(supportphy/animate). Thus, the distinct shift between inferences is captured in the course of analysis. Cognitive studies such as the studies on Garden Path Sentences strengthen this approach to analysis. (Lewis, 1996), for example, presents a model that predicts cognitive observations with very limited working memory. Storing the inter-lexical conditional probabilities is also an issue, as mentioned earlier. Where, for example, do we store P(supporti/Gorbachevj)? One possible approach would be to store them with either lexical item. A better approach would be to bestow the responsibility of calculating these probabilities upon the generative mechanisms of the semantic lexicon whenever possible. Let us now analyze joke (1) under the probabilistic framework. Again, approximations for probability of occurrenc</context>
</contexts>
<marker>Lewis, 1996</marker>
<rawString>Richard L. Lewis. 1996. Interference in short-term memory: The magical number two (or three) in sentence processing. Journal of Psycholinguistic Research, 25(1):93 – 115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/RS-87-190,</tech>
<institution>Center for the Study of Language and Information, Stanford University.</institution>
<contexts>
<context position="6772" citStr="Mann and Thompson, 1987" startWordPosition="1082" endWordPosition="1085">ricean Implicature (Grice, 1981). It became necessary to look at the relations that describe interactions between such structures. (Hobbs, 1985) introduces an early theory of discourse and the notion of coherence relations, which are applied recursively on discourse segments. Coherence relations, such as Elaboration, Explanation and Contrast, are relations between discourse units that bind segments of text into one global structure. (Grosz and Sidner, 1986) incorporates two more important notions into its model - the idea of intention and focus. The Rhetorical Structure Theory, introduced in (Mann and Thompson, 1987), binds text spans with rhetorical relations, which are discourse connectives similar to coherence relations. The Discourse Representation Theory (DRT) (Kamp, 1984) computes inter-sentential anaphora and attempts to maintain text cohesion through sets of predicates, termed Discourse Representation Structures (DRSs), that represent discourse (a) (b) (c) P, P2 P, P2 P, P2 (d) P, P2 32 Who supports Gorbachev? Question-answer pair No one does Explanation He can still walk by himself Figure 3: Rhetorical Relations for joke (3) units. A Principal DRS accumulates information contained in the text, an</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report ISI/RS-87-190, Center for the Study of Language and Information, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Making computers laugh: Investigations in automatic humor recognition.</title>
<date>2005</date>
<booktitle>In Joint Conference on Human Language Technology / Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="4185" citStr="Mihalcea and Strapparava, 2005" startWordPosition="669" endWordPosition="672">y in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that may arise in jokes. Others (Minsky, 1986) posit a general mechanism without considering specifics. Other models in computation have attempted to generate jokes using templates (Attardo and Raskin, 1994; Binsted and Ritchie, 1997) or recognize jokes using machine learning models (Mihalcea and Strapparava, 2005). Computationally, the fact that other less likely interpretations such as P2 are not visible initially, may also result in considerably efficiency in more common situations, where ambiguities are not generated to begin with. For example, in joke (1) the interpretation after reading the first clause, has the word miss referring to the abstract act of missing a dear person. After hearing the punch line, somewhere around the word aim, (the trigger point TP), we have to revise our interpretation to one where miss is used in a physical sense, as in shooting a target. Then, the forbidden idea of hu</context>
</contexts>
<marker>Mihalcea, Strapparava, 2005</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In Joint Conference on Human Language Technology / Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>The magical number seven, plus or minus two: Some limits on our capacity for processing information.</title>
<date>1956</date>
<journal>Psychological Review,</journal>
<pages>63--81</pages>
<contexts>
<context position="15892" citStr="Miller, 1956" startWordPosition="2591" endWordPosition="2592">er Take All An argument against the approach of existing discourse models like SDRT concerns their iterative inferencing. At each point in the process of infer34 ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ encing, SDRT’s Glue Logic carries over all interpretations possible within its constraints as a set. MDC ranks contending inferences, allowing less preferred inferences to be discarded, and the result of this process is a subset of the input to it. Contrasting inferences can coexist through underspecification, and the contrast is resolved when one of them loses compatibility. This is cognitively unlikely; (Miller, 1956) has shown that the human brain actively retains only around seven units of information. With such a limited working memory, it is not cognitively feasible to model discourse analysis in this manner. Cognitive models working with limited-capacity short-term memory like in (Lewis, 1996) support the same intuition. Thus, a better approach would be a Winner Take All (WTA) approach, where the most likely interpretation, called the winner, suppresses all other interpretations as we move through the discourse. The model must be revised to reflect new contexts if they are incompatible with the existi</context>
</contexts>
<marker>Miller, 1956</marker>
<rawString>George A. Miller. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63:81–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Minsky</author>
</authors>
<title>The Society of Mind.</title>
<date>1986</date>
<publisher>Simon and Schuster.</publisher>
<contexts>
<context position="3915" citStr="Minsky, 1986" startWordPosition="633" endWordPosition="634">mour exhibits destructive dissonance. For a joke to work, the resulting interpretation must result in an incongruity, what (Freud, 1960) calls an ‘energy release’ that breaks the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different oppositions that may arise in jokes. Others (Minsky, 1986) posit a general mechanism without considering specifics. Other models in computation have attempted to generate jokes using templates (Attardo and Raskin, 1994; Binsted and Ritchie, 1997) or recognize jokes using machine learning models (Mihalcea and Strapparava, 2005). Computationally, the fact that other less likely interpretations such as P2 are not visible initially, may also result in considerably efficiency in more common situations, where ambiguities are not generated to begin with. For example, in joke (1) the interpretation after reading the first clause, has the word miss referring </context>
</contexts>
<marker>Minsky, 1986</marker>
<rawString>Marvin Minsky. 1986. The Society of Mind. Simon and Schuster.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>In Approaches to Natural Language,</booktitle>
<pages>221--242</pages>
<note>D. Reidel.</note>
<contexts>
<context position="6007" citStr="Montague, 1973" startWordPosition="966" endWordPosition="967">rld P2. Clearly, a desideratum in such a system would be that P1 would be the preferred interpretation from the outset, so much so that P2, which is in principle compatible with the joke, is not even visible in the first part of the joke. It would be reasonable to assume that such an interpretation may be constructed as a “Winner Take All” measure using probabilistic inter-relations in the lexicon, built up based on usage frequencies. This would differ from existing theories of discourse in several ways, as will be illustrated in the following sections. 2 Models of Discourse Formal semantics (Montague, 1973) looked at logical structures, but it became evident that language builds up on what is seemingly semantic incompatibility, particularly in Gricean Implicature (Grice, 1981). It became necessary to look at the relations that describe interactions between such structures. (Hobbs, 1985) introduces an early theory of discourse and the notion of coherence relations, which are applied recursively on discourse segments. Coherence relations, such as Elaboration, Explanation and Contrast, are relations between discourse units that bind segments of text into one global structure. (Grosz and Sidner, 198</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The proper treatment of quantification in ordinary English. In Approaches to Natural Language, pages 221–242. D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovksy</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5135" citStr="Pustejovksy, 1995" startWordPosition="821" endWordPosition="822">the abstract act of missing a dear person. After hearing the punch line, somewhere around the word aim, (the trigger point TP), we have to revise our interpretation to one where miss is used in a physical sense, as in shooting a target. Then, the forbidden idea of hurting ex-wives generates the humour. By hiding this meaning, the mechanism of destructive dissonance enables the surprise which is the crux of the joke. In the model proposed here, no extra-linguistic sources of knowledge are appealed to. Lexical Semantics proposes rich inter-relations encoding knowledge within the lexicon itself (Pustejovksy, 1995; Jackendoff, 1990), and this work considers the possibility that such lexicons may eventually be able to carry discourse interpretations as well, to the level of handling situations such as the destructive transition from a possible-world P1 to possible world P2. Clearly, a desideratum in such a system would be that P1 would be the preferred interpretation from the outset, so much so that P2, which is in principle compatible with the joke, is not even visible in the first part of the joke. It would be reasonable to assume that such an interpretation may be constructed as a “Winner Take All” m</context>
<context position="10058" citStr="Pustejovksy, 1995" startWordPosition="1583" endWordPosition="1584">s are. SDRT uses rhetorical relations and MDC to resolve lexical and semantic ambiguities. For example, in the utterance ‘John bought an apartment. But he rented it’, the sense of rented is that of renting out, and that is resolved in SDRT because the word but cues the relation Contrast, which prefers an interpretation that maximizes semantic contrast between its connectives. Glue logic works by iteratively extracting subsets of inferences through the flow of the discourse. This is discussed in more detail later. 2.1 Lexicons for Discourse modeling Pustejovsky’s Generative Lexicon (GL) model (Pustejovksy, 1995) outlines an ambitious attempt to formulate a lexical semantics framework that can handle the unboundedness of linguistic expressions by providing a rich semantic structure, a principled ontology of concepts (called qualia), and a set of generative devices in which participants in a phrase or sentence can influence each other’s semantic properties. The ontology of concepts in GL is hierarchical, and concepts that exhibit similar behaviour are grouped together into subsystems called Lexical Conceptual Paradigms (LCP). As an example, the GL structure for door is an LCP that represents both the u</context>
</contexts>
<marker>Pustejovksy, 1995</marker>
<rawString>James Pustejovksy. 1995. The Generative Lexicon. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren Shibles</author>
</authors>
<date>1989</date>
<note>Humor reference guide. http://facstaff.uww.edu/shiblesw/humorbook.</note>
<contexts>
<context position="3260" citStr="Shibles, 1989" startWordPosition="528" endWordPosition="530">ure 2). Often this is because the default interpretation involves a sense-association that has very high coherence in the immediate context, but is nullified by later J1 TP J2 P1 search gap I still miss my ex-wife, but my aim is improving time t P2 31 Figure 2: Cognitive Dissonance in Discourse (a-c) can be Constructive, where the interpretation P1 does not disappear completely after the dissonant utterance, or (d) Destructive, where P2 has to be arrived at afresh and P1 is destroyed completely. utterances. While humour may involve a number of other mechanisms such as allusion or stereotypes (Shibles, 1989; Gruner, 1997), a wide class of verbal humour exhibits destructive dissonance. For a joke to work, the resulting interpretation must result in an incongruity, what (Freud, 1960) calls an ‘energy release’ that breaks the painful barriers we have around forbidden thoughts. Part of the difficulty in dealing with such shifts is that it requires a rich model of discourse semantics. Computational theories such as the General Theory of Verbal Humour (Attardo and Raskin, 1991) have avoided this difficult problem by adopting extra-linguistic knowledge in the form of scripts, which encode different opp</context>
</contexts>
<marker>Shibles, 1989</marker>
<rawString>Warren Shibles. 1989. Humor reference guide. http://facstaff.uww.edu/shiblesw/humorbook.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>