<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000646">
<title confidence="0.993425">
The Best of Both Worlds – A Graph-based Completion Model for
Transition-based Parsers
</title>
<author confidence="0.996901">
Bernd Bohnet and Jonas Kuhn
</author>
<affiliation confidence="0.9831375">
University of Stuttgart
Institute for Natural Language Processing
</affiliation>
<email confidence="0.995407">
{bohnet,jonas}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.99733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998395">
Transition-based dependency parsers are
often forced to make attachment deci-
sions at a point when only partial infor-
mation about the relevant graph configu-
ration is available. In this paper, we de-
scribe a model that takes into account com-
plete structures as they become available
to rescore the elements of a beam, com-
bining the advantages of transition-based
and graph-based approaches. We also pro-
pose an efficient implementation that al-
lows for the use of sophisticated features
and show that the completion model leads
to a substantial increase in accuracy. We
apply the new transition-based parser on ty-
pologically different languages such as En-
glish, Chinese, Czech, and German and re-
port competitive labeled and unlabeled at-
tachment scores.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99207420754717">
Background. A considerable amount of recent
research has gone into data-driven dependency
parsing, and interestingly throughout the continu-
ous process of improvements, two classes of pars-
ing algorithms have stayed at the centre of at-
tention, the transition-based (Nivre, 2003) vs. the
graph-based approach (Eisner, 1996; McDonald
et al., 2005).1 The two approaches apply funda-
mentally different strategies to solve the task of
finding the optimal labeled dependency tree over
the words of an input sentence (where supervised
machine learning is used to estimate the scoring
parameters on a treebank).
The transition-based approach is based on the
conceptually (and cognitively) compelling idea
1More references will be provided in sec. 2.
that machine learning, i.e., a model of linguis-
tic experience, is used in exactly those situations
when there is an attachment choice in an other-
wise deterministic incremental left-to-right pars-
ing process. As a new word is processed, the
parser has to decide on one out of a small num-
ber of possible transitions (adding a dependency
arc pointing to the left or right and/or pushing or
popping a word on/from a stack representation).
Obviously, the learning can be based on the fea-
ture information available at a particular snapshot
in incremental processing, i.e., only surface in-
formation for the unparsed material to the right,
but full structural information for the parts of the
string already processed. For the completely pro-
cessed parts, there are no principled limitations as
regards the types of structural configurations that
can be checked in feature functions.
The graph-based approach in contrast empha-
sizes the objective of exhaustive search over all
possible trees spanning the input words. Com-
monly, dynamic programming techniques are
used to decide on the optimal tree for each par-
ticular word span, considering all candidate splits
into subspans, successively building longer spans
in a bottom-up fashion (similar to chart-based
constituent parsing). Machine learning drives
the process of deciding among alternative can-
didate splits, i.e., feature information can draw
on full structural information for the entire ma-
terial in the span under consideration. However,
due to the dynamic programming approach, the
features cannot use arbitrarily complex structural
configurations: otherwise the dynamic program-
ming chart would have to be split into exponen-
tially many special states. The typical feature
models are based on combinations of edges (so-
</bodyText>
<page confidence="0.986738">
77
</page>
<note confidence="0.976375">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 77–87,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999844777777778">
called second-order factors) that closely follow
the bottom-up combination of subspans in the
parsing algorithm, i.e., the feature functions de-
pend on the presence of two specific dependency
edges. Configurations not directly supported by
the bottom-up building of larger spans are more
cumbersome to integrate into the model (since the
combination algorithm has to be adjusted), in par-
ticular for third-order factors or higher.
Empirically, i.e., when applied in supervised
machine learning experiments based on existing
treebanks for various languages, both strategies
(and further refinements of them not mentioned
here) turn out roughly equal in their capability
of picking up most of the relevant patterns well;
some subtle strengths and weaknesses are com-
plementary, such that stacking of two parsers rep-
resenting both strategies yields the best results
(Nivre and McDonald, 2008): in training and ap-
plication, one of the parsers is run on each sen-
tence prior to the other, providing additional fea-
ture information for the other parser. Another suc-
cessful technique to combine parsers is voting as
carried out by Sagae and Lavie (2006).
The present paper addresses the question if
and how a more integrated combination of the
strengths of the two strategies can be achieved
and implemented efficiently to warrant competi-
tive results.
The main issue and solution strategy. In or-
der to preserve the conceptual (and complexity)
advantages of the transition-based strategy, the
integrated algorithm we are looking for has to
be transition-based at the top level. The advan-
tages of the graph-based approach – a more glob-
ally informed basis for the decision among dif-
ferent attachment options – have to be included
as part of the scoring procedure. As a prerequi-
site, our algorithm will require a memory for stor-
ing alternative analyses among which to choose.
This has been previously introduced in transition-
based approaches in the form of a beam (Johans-
son and Nugues, 2006): rather than representing
only the best-scoring history of transitions, the k
best-scoring alternative histories are kept around.
As we will indicate in the following, the mere
addition of beam search does not help overcome
a representational key issue of transition-based
parsing: in many situations, a transition-based
parser is forced to make an attachment decision
for a given input word at a point where no or only
partial information about the word’s own depen-
dents (and further decendents) is available. Fig-
ure 1 illustrates such a case.
</bodyText>
<figureCaption confidence="0.9399418">
Figure 1: The left set of brackets indicates material
that has been processed or is under consideration; on
the right is the input, still to be processed. Access to in-
formation that is yet unavailable would help the parser
to decide on the correct transition.
</figureCaption>
<bodyText confidence="0.99954775">
Here, the parser has to decide whether to create an
edge between house and with or between bought
and with (which is technically achieved by first
popping house from the stack and then adding the
edge). At this time, no information about the ob-
ject of with is available; with fails to provide what
we call a complete factor for the calculation of the
scores of the alternative transitions under consid-
eration. In other words, the model cannot make
use of any evidence to distinguish between the
two examples in Figure 1, and it is bound to get
one of the two cases wrong.
</bodyText>
<figureCaption confidence="0.914207">
Figure 2 illustrates the same case from the per-
spective of a graph-based parser.
Figure 2: A second order model as used in graph-based
parsers has access to the crucial information to build
the correct tree. In this case, the parser condsiders the
word friend (as opposed to garden, for instance) as it
introduces the bold-face edge.
</figureCaption>
<bodyText confidence="0.999896125">
Here, the combination of subspans is performed
at a point when their internal structure has been
finalized, i.e., the attachment of with (to bought
or house) is not decided until it is clear that friend
is the object of with; hence, the semantically im-
portant lexicalization of with’s object informs the
higher-level attachment decision through a so-
called second order factor in the feature model.
</bodyText>
<page confidence="0.99677">
78
</page>
<bodyText confidence="0.999951848484848">
Given a suitable amount of training data, the
model can thus learn to make the correct deci-
sion. The dynamic-programming based graph-
based parser is designed in such a way that any
score calculation is based on complete factors for
the subspans that are combined at this point.
Note that the problem for the transition-based
parser cannot be remedied by beam search alone.
If we were to keep the two options for attach-
ing with around in a beam (say, with a slightly
higher score for attachment to house, but with
bought following narrowly behind), there would
be no point in the further processing of the sen-
tence at which the choice could be corrected: the
transition-based parser still needs to make the de-
cision that friend is attached to with, but this will
not lead the parser to reconsider the decision made
earlier on.
The strategy we describe in this paper applies
in this very type of situation: whenever infor-
mation is added in the transition-based parsing
process, the scores of all the histories stored in
the beam are recalculated based on a scoring
model inspired by the graph-based parsing ap-
proach, i.e., taking complete factors into account
as they become incrementally available. As a con-
sequence the beam is reordered, and hence, the
incorrect preference of an attachment of with to
house (based on incomplete factors) can later be
corrected as friend is processed and the complete
second-order factor becomes available.2
The integrated transition-based parsing strategy
has a number of advantages:
</bodyText>
<listItem confidence="0.998082615384616">
(1) We can integrate and investigate a number of
third order factors, without the need to implement
a more complex parsing model each time anew to
explore the properties of such distinct model.
(2) The parser with completion model main-
tains the favorable complexity of transition-based
parsers.
(3) The completion model compensates for the
lower accuracy of cases when only incomplete in-
formation is available.
(4) The parser combines the two leading pars-
ing paradigms in a single efficient parser with-
out stacking the two approaches. Therefore the
</listItem>
<footnote confidence="0.9390264">
2Since search is not exhaustive, there is of course a slight
danger that the correct history drops out of the beam before
complete information becomes available. But as our experi-
ments show, this does not seem to be a serious issue empiri-
cally.
</footnote>
<bodyText confidence="0.998568">
parser requires only one training phase (without
jackknifing) and it uses only a single transition-
based decoder.
The structure of this paper is as follows. In Sec-
tion 2, we discuss related work. In Section 3, we
introduce our transition-based parser and in Sec-
tion 4 the completion model as well as the im-
plementation of third order models. In Section 5,
we describe experiments and provide evaluation
results on selected data sets.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999980256410256">
Kudo and Matsumoto (2002) and Yamada and
Matsumoto (2003) carried over the idea for de-
terministic parsing by chunks from Abney (1991)
to dependency parsing. Nivre (2003) describes
in a more strict sense the first incremental parser
that tries to find the most appropriate dependency
tree by a sequence of local transitions. In order
to optimize the results towards a more globally
optimal solution, Johansson and Nugues (2006)
first applied beam search, which leads to a sub-
stantial improvment of the results (cf. also (Titov
and Henderson, 2007)). Zhang and Clark (2008)
augment the beam-search algorithm, adapting the
early update strategy of Collins and Roark (2004)
to dependency parsing. In this approach, the
parser stops and updates the model when the or-
acle transition sequence drops out of the beam.
In contrast to most other approaches, the training
procedure of Zhang and Clark (2008) takes the
complete transition sequence into account as it is
calculating the update. Zhang and Clark compare
aspects of transition-based and graph-based pars-
ing, and end up using a transition-based parser
with a combined transition-based/second-order
graph-based scoring model (Zhang and Clark,
2008, 567), which is similar to the approach we
describe in this paper. However, their approach
does not involve beam rescoring as the partial
structures built by the transition-based parser are
subsequently augmented; hence, there are cases in
which our approach is able to differentiate based
on higher-order factors that go unnoticed by the
combined model of (Zhang and Clark, 2008, 567).
One step beyond the use of a beam is a dynamic
programming approach to carry out a full search
in the state space, cf. (Huang and Sagae, 2010;
Kuhlmann et al., 2011). However, in this case
one has to restrict the employed features to a set
which fits to the elements composed by the dy-
</bodyText>
<page confidence="0.99144">
79
</page>
<bodyText confidence="0.999967424242424">
namic programming approach. This is a trade-off
between an exhaustive search and a unrestricted
(rich) feature set and the question which provides
a higher accuracy is still an open research ques-
tion, cf. (Kuhlmann et al., 2011).
Parsing of non-projective dependency trees is
an important feature for many languages. At
first most algorithms were restricted to projec-
tive dependency trees and used pseudo-projective
parsing (Kahane et al., 1998; Nivre and Nilsson,
2005). Later, additional transitions were intro-
duced to handle non-projectivity (Attardi, 2006;
Nivre, 2009). The most common strategy uses
the swap transition (Nivre, 2009; Nivre et al.,
2009), an alternative solution uses two planes
and a switch transition to switch between the two
planes (G´omez-Rodriguez and Nivre, 2010).
Since we use the scoring model of a graph-
based parser, we briefly review releated work
on graph-based parsing. The most well known
graph-based parser is the MST (maximum span-
ning tree) parser, cf. (McDonald et al., 2005; Mc-
Donald and Pereira, 2006). The idea of the MST
parser is to find the highest scoring tree in a graph
that contains all possible edges. Eisner (1996)
introduced a dynamic programming algorithm to
solve this problem efficiently. Carreras (2007) in-
troduced the left-most and right-most grandchild
as factors. We use the factor model of Carreras
(2007) as starting point for our experiments, cf.
Section 4. We extend Carreras (2007) graph-
based model with factors involving three edges
similar to that of Koo and Collins (2010).
</bodyText>
<sectionHeader confidence="0.971858" genericHeader="method">
3 Transition-based Parser with a Beam
</sectionHeader>
<bodyText confidence="0.99940434375">
This section specifies the transition-based beam-
search parser underlying the combined approach
more formally. Sec. 4 will discuss the graph-
based scoring model that we are adding.
The input to the parser is a word string x,
the goal is to find the optimal set y of labeled
edges xi —*, xj forming a dependency tree over x
U{root}. We characterize the state of a transition-
based parser as πi=(σi, βi, yi, hi), πi E H, the set
of possible states. σi is a stack of words from x
that are still under consideration; βi is the input
buffer, the suffix of x yet to be processed; yi the
set of labeled edges already assigned (a partial la-
beled dependency tree); hi is a sequence record-
ing the history of transitions (from the set of op-
erations Q = {shift, left-arc,, right-arc,, reduce,
swap}) taken up to this point.
(1) The initial state π0 has an empty stack, the
input buffer is the full input string x, and the edge
set is empty. (2) The (partial) transition function
τ(πi, t) : H x Q —* H maps a state and an opera-
tion t to a new state πi+1. (3) Final states πf are
characterized by an empty input buffer and stack;
no further transitions can be taken.
The transition function is informally defined as
follows: The shift transition removes the first ele-
ment of the input buffer and pushes it to the stack.
The left-arc, transition adds an edge with label l
from the first word in the buffer to the word on
top of the stack, removes the top element from
the stack and pushes the first element of the input
buffer to the stack.
The right-arc, transition adds an edge from word
on top of the stack to the first word in the input
buffer and removes the top element of the input
buffer and pushes that element onto the stack.
The reduce transition pops the top word from the
stack.
The swap changes the order of the two top el-
ements on the stack (possibly generating non-
projkective trees).
When more than one operation is applicable, a
scoring function assigns a numerical value (based
on a feature vector and a weight vector trained
by supervised machine learning) to each possi-
ble continuation. When using a beam search ap-
proach with beam size k, the highest-scoring k al-
ternative states with the same length n of transi-
tion history h are kept in a set “beamn”.
In the beam-based parsing algorithm (cf. the
pseudo code in Algorithm 1), all candidate states
for the next set “beamn+1” are determined using
the transition function τ, but based on the scor-
ing function, only the best k are preserved. (Fi-
nal) states to which no more transitions apply are
copied to the next state set. This means that once
all transition paths have reached a final state, the
overall best-scoring states can be read off the fi-
nal “beamn”. The y of the top-scoring state is the
predicted parse.
Under the plain transition-based scoring
regime scorer, the score for a state π is the sum
of the “local” scores for the transitions ti in the
state’s history sequence:
</bodyText>
<equation confidence="0.857884777777778">
scorer(π) = �i�0 w f(πi, ti)
80
Algorithm 1: Transition-based parser
// x is the input sentence, k is the beam size
u0 = ∅,O0 = x,y0 = ∅,h = ∅
7r0 ← hu0, O0, y0, h0i // initial parts of a state
beam0 ← {7r0} // create initial state
n ← 0 // iteration
repeat
n ← n + 1
for all 7rj ∈ beamn−1 do
transitions ← possible-applicable-transition (7rj)
// if no transition is applicable keep state 7rj:
if transitions = ∅ then beamn ← beamn ∪ {7rj}
else for all ti ∈ transitions do
// apply the transition i to state j
7r ← T(7rj, ti)
beamn ← beamn ∪ {7r}
</equation>
<bodyText confidence="0.768452833333333">
// end for
// end for
sort beamn due to the score(7rj)
beamn ← sublist (beamn, 0, k)
until beamn−1 = beamn // beam changed?
w is the weight vector. Note that the features
f(7ri, ti) can take into account all structural and
labeling information available prior to taking tran-
sition ti, i.e., the graph built so far, the words (and
their part of speech etc.) on the stack and in the
input buffer, etc. But if a larger graph configu-
ration involving the next word evolves only later,
as in Figure 1, this information is not taken into
account in scoring. For instance, if the feature
extraction uses the subcategorization frame of a
word under consideration to compute a score, it is
quite possible that some dependents are still miss-
ing and will only be attached in a future transition.
</bodyText>
<sectionHeader confidence="0.999372" genericHeader="method">
4 Completion Model
</sectionHeader>
<bodyText confidence="0.997141862068965">
We define an augmented scoring function which
can be used in the same beam-search algorithm in
order to ensure that in the scoring of alternative
transition paths, larger configurations can be ex-
ploited as they are completed in the incremental
process. The feature configurations can be largely
taken from graph-based approaches. Here, spans
from the string are assembled in a bottom-up fash-
ion, and the scoring for an edge can be based on
structurally completed subspans (“factors”).
Our completion model for scoring a state 7rn
incorporates factors for all configurations (match-
ing the extraction scheme that is applied) that are
present in the partial dependency graph yn built
up to this point, which is continuously augmented.
This means if at a given point n in the transition
path, complete information for a particular config-
uration (e.g., a third-order factor involving a head,
its dependent and its grand-child dependent) is
unavailable, scoring will ignore this factor at time
n, but the configuration will inform the scoring
later on, maybe at point n + 4, when the complete
information for this factor has entered the partial
graph yn+4.
We present results for a number of different
second-order and third-order feature models.
Second Order Factors. We start with the
model introduced by Carreras (2007). Figure 3
illustrates the factors used.
</bodyText>
<figureCaption confidence="0.997424333333333">
Figure 3: Model 2a. Second order factors of Carreras
(2007). We omit the right-headed cases, which are
mirror images. The model comprises a factoring into
one first order part and three second order factors (2-
4): 1) The head (h) and the dependent (c); 2) the head,
the dependent and the left-most (or right-most) grand-
child in between (cmi); 3) the head, the dependent and
the right-most (or left-most) grandchild away from the
head (cmo). 4) the head, the dependent and between
those words the right-most (or left-most) sibling (ci).
Figure 4: 2b. The left-most dependent of the head or
the right-most dependent in the right-headed case.
</figureCaption>
<bodyText confidence="0.9965341">
Figure 4 illustrates a new type of factor we use,
which includes the left-most dependent in the left-
headed case and symmetricaly the right-most sib-
ling in the right-head case.
Third Order Factors. In addition to the second
order factors, we investigate combinations of third
order factors. Figure 5 and 6 illustrate the third
order factors, which are similar to the factors of
Koo and Collins (2010). They restrict the factor
to the innermost sibling pair for the tri-siblings
</bodyText>
<page confidence="0.993478">
81
</page>
<bodyText confidence="0.999254222222222">
and the outermost pair for the grand-siblings. We
use the first two siblings of the dependent from
the left side of the head for the tri-siblings and
the first two dependents of the child for the grand-
siblings. With these factors, we aim to capture
non-projective edges and subcategorization infor-
mation. Figure 7 illustrates a factor of a sequence
of four nodes. All the right headed variants are
symmetrically and left out for brevity.
</bodyText>
<figureCaption confidence="0.999556">
Figure 5: 3a. The first two children of the head, which
do not include the edge between the head and the de-
pendent.
Figure 6: 3b. The first two children of the dependent.
Figure 7: 3c. The right-most dependent of the right-
most dependent.
</figureCaption>
<bodyText confidence="0.999889857142857">
Integrated approach. To obtain an integrated
system for the various feature models, the scoring
function of the transition-based parser from Sec-
tion 3 is augmented by a family of scoring func-
tions scoreG. for the completion model, where m
is from 2a, 2b, 3a etc., x is the input string, and y
is the (partial) dependency tree built so far:
</bodyText>
<equation confidence="0.999019">
scoreT.(7r) = scoreT(7r) + scoreG.(x, y)
</equation>
<bodyText confidence="0.999929">
The scoring function of the completion model
depends on the selected factor model Gm. The
model G2a comprises the edge factoring of Fig-
ure 3. With this model, we obtain the following
scoring function.
</bodyText>
<equation confidence="0.6357935">
scoreG..(x, y) = E(h,c)Ey w · ffirst(x,h,c)
+E(h,c,ci)Ey w · fsib(x,h,c,ci)
+E(h,c,cmo)Ey w · fgra(x,h,c,cmo)
+E(h,c,cmi)Ey w · fgra(x,h,c,cmi)
</equation>
<bodyText confidence="0.999258333333333">
The function f maps the input sentence x, and
a subtree y defined by the indexes to a feature-
vector. Again, w is the corresponding weight vec-
tor. In order to add the factor of Figure 4 to our
model, we have to add the scoring function (2a)
the sum:
</bodyText>
<equation confidence="0.980118">
(2b) scoreG.,(x, y) = scoreG..(x, y)
+ E(h,c,cmi)Ey w · fgra(x,h,c,cmi)
</equation>
<bodyText confidence="0.99944475">
In order to build a scoring function for combi-
nation of the factors shown in Figure 5 to 7, we
have to add to the equation 2b one or more of the
following sums:
</bodyText>
<listItem confidence="0.992494666666667">
(3a) E(h,c,ch1,ch2)Ey w · fgra(x,h,c,ch1,ch2)
(3b) E(h,c,cm1,cm2)Ey w · fgra(x,h,c,cm1,cm2)
(3c) E(h,c,cmo,tmo)Ey w · fgra(x,h,c,cmo,tmo)
</listItem>
<bodyText confidence="0.999710555555556">
Feature Set. The feature set of the transition
model is similar to that of Zhang and Nivre
(2011). In addition, we use the cross product of
morphologic features between the head and the
dependent since we apply also the parser on mor-
phologic rich languages.
The feature sets of the completion model de-
scribed above are mostly based on previous work
(McDonald et al., 2005; McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010).
The models denoted with + use all combinations
of words before and after the head, dependent,
sibling, grandchilrden, etc. These are respectively
three-, and four-grams for the first order and sec-
ond order. The algorithm includes these features
only the words left and right do not overlap with
the factor (e.g. the head, dependent, etc.). We use
feature extraction procedure for second order, and
third order factors. Each feature extracted in this
procedure includes information about the position
of the nodes relative to the other nodes of the part
and a factor identifier.
Training. For the training of our parser, we use
a variant of the perceptron algorithm that uses the
Passive-Aggressive update function, cf. (Freund
and Schapire, 1998; Collins, 2002; Crammer et
al., 2006). The Passive-Aggressive perceptron
uses an aggressive update strategy by modifying
the weight vector by as much as needed to clas-
sify correctly the current example, cf. (Crammer
et al., 2006). We apply a random function (hash
function) to retrieve the weights from the weight
vector instead of a table. Bohnet (2010) showed
that the Hash Kernel improves parsing speed and
accuracy since the parser uses additionaly nega-
tive features. Ganchev and Dredze (2008) used
</bodyText>
<page confidence="0.997061">
82
</page>
<bodyText confidence="0.9999854">
this technique for structured prediction in NLP to
reduce the needed space, cf. (Shi et al., 2009).
We use as weight vector size 800 million. After
the training, we counted 65 millions non zero
weights for English (penn2malt), 83 for Czech
and 87 millions for German. The feature vectors
are the union of features originating from the
transition sequence of a sentence and the features
of the factors over all edges of a dependency tree
(e.g. G2a, etc.). To prevent over-fitting, we use
averaging to cope with this problem, cf. (Freund
and Schapire, 1998; Collins, 2002). We calculate
the error e as the sum of all attachment errors and
label errors both weighted by 0.5. We use the
following equations to compute the update.
</bodyText>
<equation confidence="0.990235">
loss: lt = e-(scoreT (xt , yt )-scoreT (xt, yt))
PA-update: Tt = l�
||f9�fp||2
</equation>
<bodyText confidence="0.974646434782609">
We train the model to select the transitions and
the completion model together and therefore, we
use one parameter space. In order to compute the
weight vector, we employ standard online learn-
ing with 25 training iterations, and carry out early
updates, cf. Collins and Roark (2004; Zhang and
Clark (2008).
Efficient Implementation. Keeping the scoring
with the completion model tractable with millions
of feature weights and for second- and third-order
factors requires careful bookkeeping and a num-
ber of specialized techniques from recent work on
dependency parsing.
We use two variables to store the scores (a)
for complete factors and (b) for incomplete fac-
tors. The complete factors (first-order factors and
higher-order factors for which further augmenta-
tion is structurally excluded) need to be calculated
only once and can then be stored with the tree fac-
tors. The incomplete factors (higher-order factors
whose node elements may still receive additional
descendants) need to be dynamically recomputed
while the tree is built.
The parsing algorithm only has to compute the
scores of the factored model when the transition-
based parser selects a left-arc or right-arc transi-
tion and the beam has to be sorted. The parser
sorts the beam when it exceeds the maximal beam
size, in order to discard superfluous parses or
when the parsing algorithm terminates in order to
select the best parse tree. The complexity of the
transition-based parser is quadratic due to swap
operation in the worse case, which is rare, and
O(n) in the best case, cf. (Nivre, 2009). The
beam size B is constant. Hence, the complexity
is in the worst case O(n2).
The parsing time is to a large degree deter-
mined by the feature extraction, the score calcu-
lation and the implementation, cf. also (Goldberg
and Elhadad, 2010). The transition-based parser
is able to parse 30 sentences per second. The
parser with completion model processes about 5
sentences per second with a beam size of 80.
Note, we use a rich feature set, a completion
model with third order factors, negative features,
and a large beam. 3
We implemented the following optimizations:
(1) We use a parallel feature extraction for the
beam elements. Each process extracts the fea-
tures, scores the possible transitions and computes
the score of the completion model. After the ex-
tension step, the beam is sorted and the best ele-
ments are selected according to the beam size.
(2) The calculation of each score is optimized (be-
yond the distinction of a static and a dynamic
component): We calculate for each location de-
termined by the last element sl E QZ and the first
element of b0 E QZ a numeric feature representa-
tion. This is kept fix and we add only the numeric
value for each of the edge labels plus a value for
the transition left-arc or right-arc. In this way, we
create the features incrementally. This has some
similarity to Goldberg and Elhadad (2010).
(3) We apply edge filtering as it is used in graph-
based dependency parsing, cf. (Johansson and
Nugues, 2008), i.e., we calculate the edge weights
only for the labels that were found for the part-of-
speech combination of the head and dependent in
the training data.
</bodyText>
<sectionHeader confidence="0.723419" genericHeader="method">
5 Parsing Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.9995485">
The results of different parsing systems are of-
ten hard to compare due to differences in phrase
structure to dependency conversions, corpus ver-
sion, and experimental settings. For better com-
parison, we provide results on English for two
commonly used data sets, based on two differ-
ent conversions of the Penn Treebank. The first
uses the Penn2Malt conversion based on the head-
</bodyText>
<page confidence="0.715377">
36 core, 3.33 Ghz Intel Nehalem
83
</page>
<table confidence="0.999817">
Section Sentences PoS Acc.
Training 2-21 39.832 97.08
Dev 24 1.394 97.18
Test 23 2.416 97.30
</table>
<tableCaption confidence="0.8059332">
Table 1: Overview of the training, development and
test data split converted to dependency graphs with
head-finding rules of (Yamada and Matsumoto, 2003).
The last column shows the accuracy of Part-of-Speech
tags.
</tableCaption>
<bodyText confidence="0.992031052631579">
finding rules of Yamada and Matsumoto (2003).
Table 1 gives an overview of the properties of the
corpus. The annotation of the corpus does not
contain non-projective links. The training data
was 10-fold jackknifed with our own tagger.4. Ta-
ble 1 shows the tagging accuracy.
Table 2 lists the accuracy of our transition-
based parser with completion model together with
results from related work. All results use pre-
dicted PoS tags. As a baseline, we present in ad-
dition results without the completion model and
a graph-based parser with second order features
(G2a). For the Graph-based parser, we used 10
training iterations. The following rows denoted
with Ta, T2a, T2ab, T2ab3a, T2ab3b, T2ab3bc, and
T2a3abc present the result for the parser with com-
pletion model. The subscript letters denote the
used factors of the completion model as shown
in Figure 3 to 7. The parsers with subscribed plus
(e.g. G2a+) in addition use feature templates that
contain one word left or right of the head, depen-
dent, siblings, and grandchildren. We left those
feature in our previous models out as they may in-
terfere with the second and third order factors. As
in previous work, we exclude punctuation marks
for the English data converted with Penn2Malt in
the evaluation, cf. (McDonald et al., 2005; Koo
and Collins, 2010; Zhang and Nivre, 2011).5 We
optimized the feature model of our parser on sec-
tion 24 and used section 23 for evaluation. We use
a beam size of 80 for our transition-based parser
and 25 training iterations.
The second English data set was obtained by
using the LTH conversion schema as used in the
CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009).
This corpus preserves the non-projectivity of the
phrase structure annotation, it has a rich edge
label set, and provides automatic assigned PoS
</bodyText>
<footnote confidence="0.984460666666667">
4http://code.google.com/p/mate-tools/
5We follow Koo and Collins (2010) and ignore any token
whose POS tag is one of the following tokens ‘‘ ’’:,.
</footnote>
<table confidence="0.999885736842105">
Parser UAS LAS
(McDonald et al., 2005) 90.9
(McDonald and Pereira, 2006) 91.5
(Huang and Sagae, 2010) 92.1
(Zhang and Nivre, 2011) 92.9
(Koo and Collins, 2010) 93.04
(Martins et al., 2010) 93.26
T (baseline) 92.7
G2a (baseline) 92.89
T2a 92.94 91.87
T2ab 93.16 92.08
T2ab3a 93.20 92.10
T2ab3b 93.23 92.15
T2ab3c 93.17 92.10
T2ab3abc+ 93.39 92.38
G2a+ 93.1
(Koo et al., 2008) † 93.16
(Carreras et al., 2008) † 93.5
(Suzuki et al., 2009) † 93.79
</table>
<tableCaption confidence="0.968734">
Table 2: English Attachment Scores for the
Penn2Malt conversion of the Penn Treebank for the
test set. Punctuation is excluded from the evaluation.
The results marked with † are not directly comparable
to our work as they depend on additional sources of
information (Brown Clusters).
</tableCaption>
<bodyText confidence="0.999967464285714">
tags. From the same data set, we selected the
corpora for Czech and German. In all cases, we
used the provided training, development, and test
data split, cf. (Hajiˇc et al., 2009). In contrast
to the evaluation of the Penn2Malt conversion,
we include punctuation marks for these corpora
and follow in that the evaluation schema of the
CoNLL Shared Task 2009. Table 3 presents the
results as obtained for these data set.
The transition-based parser obtains higher ac-
curacy scores for Czech but still lower scores for
English and German. For Czech, the result of T
is 1.59 percentage points higher than the top la-
beled score in the CoNLL shared task 2009. The
reason is that T includes already third order fea-
tures that are needed to determine some edge la-
bels. The transition-based parser with completion
model T2a has even 2.62 percentage points higher
accuracy and it could improve the results of the
parser T by additional 1.03 percentage points.
The results of the parser T are lower for English
and German compared to the results of the graph-
based parser G2a. The completion model T2a can
reach a similar accuracy level for these two lan-
guages. The third order features let the transition-
based parser reach higher scores than the graph-
based parser. The third order features contribute
for each language a relatively small improvement
</bodyText>
<page confidence="0.996558">
84
</page>
<table confidence="0.999543538461538">
Parser Eng. Czech German
(Gesmundo 88.79/- 80.38 87.29
et al., 2009)†
(Bohnet, 2009) 89.88/- 80.11 87.48
T (Baseline) 89.52/92.10 81.97/87.26 87.53/89.86
G2a (Baseline) 90.14/92.36 81.13/87.65 87.79/90.12
T2a 90.20/92.55 83.01/88.12 88.22/90.36
T2ab 90.26/92.56 83.22/88.34 88.31/90.24
T2ab3a 90.20/90.51 83.21.88.30 88.14/90.23
T2ab3b 90.26/92.57 83.22/88.35 88.50/90.59
T2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45
G2a+ 90.39/92.8 81.43/88.0 88.26/90.50
T2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62
</table>
<tableCaption confidence="0.99989675">
Table 3: Labeled Attachment Scores of parsers that
use the data sets of the CoNLL shared task 2009. In
line with previous work, punctuation is included. The
parsers marked with † used a joint model for syntactic
parsing and semantic role labelling. We provide more
parsing results for the languages of CoNLL-X Shared
Task at http://code.google.com/p/mate-tools/.
Table 4: Chinese Attachment Scores for the conver-
</tableCaption>
<bodyText confidence="0.998376095238095">
sion of CTB 5 with head rules of Zhang and Clark
(2008). We take the standard split of CTB 5 and use
in line with previous work gold segmentation, POS-
tags and exclude punctuation marks for the evaluation.
of the score. Small and statistically significant im-
provements provides the additional second order
factor (2b).6 We tried to determine the best third
order factors or set of factors but we cannot denote
such a factor which is the best for all languages.
For German, we obtained a significant improve-
ment with the factor (3b). We believe that this is
due to the flat annotation of PPs in the German
corpus. If we combine all third order factors we
obtain for the Penn2Malt conversion a small im-
provement of 0.2 percentage points over the re-
sults of (2ab). We think that a more deep feature
selection for third order factors may help to im-
prove the actuary further.
In Table 4, we present results on the Chinese
Treebank. To our knowledge, we obtain the best
published results so far.
</bodyText>
<footnote confidence="0.9592165">
6The results of the baseline T compared to T2ab3abc are
statistically significant (p &lt; 0.01).
</footnote>
<sectionHeader confidence="0.972687" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999978666666666">
The parser introduced in this paper combines
advantageous properties from the two major
paradigms in data-driven dependency parsing,
in particular worst case quadratic complexity of
transition-based parsing with a swap operation
and the consideration of complete second and
third order factors in the scoring of alternatives.
While previous work using third order factors, cf.
Koo and Collins (2010), was restricted to unla-
beled and projective trees, our parser can produce
labeled and non-projective dependency trees.
In contrast to parser stacking, which involves
running two parsers in training and application,
we use only the feature model of a graph-based
parser but not the graph-based parsing algorithm.
This is not only conceptually superior, but makes
training much simpler, since no jackknifing has
to be carried out. Zhang and Clark (2008) pro-
posed a similar combination, without the rescor-
ing procedure. Our implementation allows for the
use of rich feature sets in the combined scoring
functions, and our experimental results show that
the “graph-based” completion model leads to an
increase of between 0.4 (for English) and about
1 percentage points (for Czech). The scores go
beyond the current state of the art results for ty-
pologically different languages such as Chinese,
Czech, English, and German. For Czech, English
(Penn2Malt) and German, these are to our knowl-
ege the highest reported scores of a dependency
parser that does not use additional sources of in-
formation (such as extra unlabeled training data
for clustering). Note that the efficient techniques
and implementation such as the Hash Kernel, the
incremental calculation of the scores of the com-
pletion model, and the parallel feature extraction
as well as the parallelized transition-based pars-
ing strategy play an important role in carrying out
this idea in practice.
</bodyText>
<sectionHeader confidence="0.999325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993564375">
S. Abney. 1991. Parsing by chunks. In Principle-
Based Parsing, pages 257–278. Kluwer Academic
Publishers.
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Tenth
Conference on Computational Natural Language
Learning (CoNLL-X).
B. Bohnet. 2009. Efficient Parsing of Syntactic and
</reference>
<figure confidence="0.984822615384615">
Parser
UAS
LAS
(Zhang and Clark, 2008)
(Huang and Sagae, 2010)
(Zhang and Nivre, 2011)
T2ab3abc+
84.3
85.2
86.0
87.5
84.4
85.9
</figure>
<page confidence="0.990038">
85
</page>
<reference confidence="0.9993774">
Semantic Dependency Structures. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009).
B. Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 89–97, Beijing,
China, August. Coling 2010 Organizing Commit-
tee.
X. Carreras, M. Collins, and T. Koo. 2008. Tag,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ’08, pages 9–16, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
X. Carreras. 2007. Experiments with a Higher-order
Projective Dependency Parser. In EMNLP/CoNLL.
M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In ACL, pages 111–
118.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551–585.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340–
345, Copenhaen.
Y. Freund and R. E. Schapire. 1998. Large margin
classification using the perceptron algorithm. In
11th Annual Conference on Computational Learn-
ing Theory, pages 209–217, New York, NY. ACM
Press.
K. Ganchev and M. Dredze. 2008. Small statisti-
cal models by random feature mixing. In Proceed-
ings of the ACL-2008 Workshop on Mobile Lan-
guage Processing. Association for Computational
Linguistics.
A. Gesmundo, J. Henderson, P. Merlo, and I. Titov.
2009. A Latent Variable Model of Syn-
chronous Syntactic-Semantic Parsing for Multiple
Languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), Boulder, Colorado, USA., June 4-5.
Y. Goldberg and M. Elhadad. 2010. An efficient al-
gorithm for easy-first non-directional dependency
parsing. In HLT-NAACL, pages 742–750.
C. G´omez-Rodr´ıguez and J. Nivre. 2010. A
Transition-Based Parser for 2-Planar Dependency
Structures. In ACL, pages 1492–1501.
J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara,
M. Ant`onia Marti, L. M`arquez, A. Meyers, J. Nivre,
S. Pad´o, J. ˇStˇep´anek, P. Straˇn´ak, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1–18, Boulder, United States, June.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077–1086, Up-
psala, Sweden, July. Association for Computational
Linguistics.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings
of the Shared Task Session of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 206–210, New York City, United
States, June 8-9.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic–Semantic Analysis with PropBank
and NomBank. In Proceedings of the Shared Task
Session of CoNLL-2008, Manchester, UK.
S. Kahane, A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-ACL,
pages 646–652.
T. Koo and M. Collins. 2010. Efficient third-order
dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1–11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
pages 595–603.
T. Kudo and Y. Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In
proceedings of the 6th conference on Natural lan-
guage learning - Volume 20, COLING-02, pages 1–
7, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Kuhlmann, C. G´omez-Rodr´ıguez, and G. Satta.
2011. Dynamic programming algorithms for
transition-based dependency parsers. In ACL, pages
673–682.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: De-
pendency parsing by approximate variational infer-
ence. pages 34–44.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms.
In In Proc. of EACL, pages 81–88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-margin Training of Dependency Parsers.
In Proc. ACL, pages 91–98.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers.
In ACL-08, pages 950–958, Columbus, Ohio.
</reference>
<page confidence="0.969315">
86
</page>
<reference confidence="0.999817875">
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In ACL.
J. Nivre, M. Kuhlmann, and J. Hall. 2009. An im-
proved oracle for dependency parsing with online
reordering. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, IWPT
’09, pages 73–76, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
J. Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In 8th International
Workshop on Parsing Technologies, pages 149–160,
Nancy, France.
J. Nivre. 2009. Non-Projective Dependency Parsing
in Expected Linear Time. In Proceedings of the
47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP, pages 351–359, Suntec, Singa-
pore.
K. Sagae and A. Lavie. 2006. Parser combina-
tion by reparsing. In NAACL ’06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 129–132, Morristown, NJ, USA. Association
for Computational Linguistics.
Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola,
and S.V.N. Vishwanathan. 2009. Hash Kernels for
Structured Data. In Journal of Machine Learning.
J. Suzuki, H. Isozaki, X. Carreras, and M Collins.
2009. An empirical study of semi-supervised struc-
tured conditional models for dependency parsing.
In EMNLP, pages 551–560.
I. Titov and J. Henderson. 2007. A Latent Variable
Model for Generative Dependency Parsing. In Pro-
ceedings of IWPT, pages 144–155.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of IWPT, pages 195–206.
Y. Zhang and S. Clark. 2008. A tale of two
parsers: investigating and combining graph-based
and transition-based dependency parsing using
beam-search. In Proceedings of EMNLP, Hawaii,
USA.
Y. Zhang and J. Nivre. 2011. Transition-based de-
pendency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 188–193, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
</reference>
<page confidence="0.999468">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892088">
<title confidence="0.997829">The Best of Both Worlds – A Graph-based Completion Model Transition-based Parsers</title>
<author confidence="0.984989">Bohnet</author>
<affiliation confidence="0.969875">University of Institute for Natural Language</affiliation>
<abstract confidence="0.99809845">Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available. In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining the advantages of transition-based and graph-based approaches. We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>In PrincipleBased Parsing,</booktitle>
<pages>257--278</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="10685" citStr="Abney (1991)" startWordPosition="1717" endWordPosition="1718">e a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and u</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In PrincipleBased Parsing, pages 257–278. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Attardi</author>
</authors>
<title>Experiments with a Multilanguage Non-Projective Dependency Parser.</title>
<date>2006</date>
<booktitle>In Tenth Conference on Computational Natural Language Learning (CoNLL-X).</booktitle>
<contexts>
<context position="12982" citStr="Attardi, 2006" startWordPosition="2081" endWordPosition="2082">a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>G. Attardi. 2006. Experiments with a Multilanguage Non-Projective Dependency Parser. In Tenth Conference on Computational Natural Language Learning (CoNLL-X).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Efficient Parsing of Syntactic and Semantic Dependency Structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009).</booktitle>
<contexts>
<context position="33311" citStr="Bohnet, 2009" startWordPosition="5553" endWordPosition="5554">a has even 2.62 percentage points higher accuracy and it could improve the results of the parser T by additional 1.03 percentage points. The results of the parser T are lower for English and German compared to the results of the graphbased parser G2a. The completion model T2a can reach a similar accuracy level for these two languages. The third order features let the transitionbased parser reach higher scores than the graphbased parser. The third order features contribute for each language a relatively small improvement 84 Parser Eng. Czech German (Gesmundo 88.79/- 80.38 87.29 et al., 2009)† (Bohnet, 2009) 89.88/- 80.11 87.48 T (Baseline) 89.52/92.10 81.97/87.26 87.53/89.86 G2a (Baseline) 90.14/92.36 81.13/87.65 87.79/90.12 T2a 90.20/92.55 83.01/88.12 88.22/90.36 T2ab 90.26/92.56 83.22/88.34 88.31/90.24 T2ab3a 90.20/90.51 83.21.88.30 88.14/90.23 T2ab3b 90.26/92.57 83.22/88.35 88.50/90.59 T2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45 G2a+ 90.39/92.8 81.43/88.0 88.26/90.50 T2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62 Table 3: Labeled Attachment Scores of parsers that use the data sets of the CoNLL shared task 2009. In line with previous work, punctuation is included. The parsers marked with † used</context>
</contexts>
<marker>Bohnet, 2009</marker>
<rawString>B. Bohnet. 2009. Efficient Parsing of Syntactic and Semantic Dependency Structures. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="24329" citStr="Bohnet (2010)" startWordPosition="4045" endWordPosition="4046">the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a, </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>B. Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31563" citStr="Carreras et al., 2008" startWordPosition="5256" endWordPosition="5259">n, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following tokens ‘‘ ’’:,. Parser UAS LAS (McDonald et al., 2005) 90.9 (McDonald and Pereira, 2006) 91.5 (Huang and Sagae, 2010) 92.1 (Zhang and Nivre, 2011) 92.9 (Koo and Collins, 2010) 93.04 (Martins et al., 2010) 93.26 T (baseline) 92.7 G2a (baseline) 92.89 T2a 92.94 91.87 T2ab 93.16 92.08 T2ab3a 93.20 92.10 T2ab3b 93.23 92.15 T2ab3c 93.17 92.10 T2ab3abc+ 93.39 92.38 G2a+ 93.1 (Koo et al., 2008) † 93.16 (Carreras et al., 2008) † 93.5 (Suzuki et al., 2009) † 93.79 Table 2: English Attachment Scores for the Penn2Malt conversion of the Penn Treebank for the test set. Punctuation is excluded from the evaluation. The results marked with † are not directly comparable to our work as they depend on additional sources of information (Brown Clusters). tags. From the same data set, we selected the corpora for Czech and German. In all cases, we used the provided training, development, and test data split, cf. (Hajiˇc et al., 2009). In contrast to the evaluation of the Penn2Malt conversion, we include punctuation marks for thes</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a Higher-order Projective Dependency Parser.</title>
<date>2007</date>
<booktitle>In EMNLP/CoNLL.</booktitle>
<contexts>
<context position="13681" citStr="Carreras (2007)" startWordPosition="2195" endWordPosition="2196">e et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The input to the parser is a word string x, the goal is to find the optimal set y of labeled edges xi</context>
<context position="19604" citStr="Carreras (2007)" startWordPosition="3254" endWordPosition="3255">ich is continuously augmented. This means if at a given point n in the transition path, complete information for a particular configuration (e.g., a third-order factor involving a head, its dependent and its grand-child dependent) is unavailable, scoring will ignore this factor at time n, but the configuration will inform the scoring later on, maybe at point n + 4, when the complete information for this factor has entered the partial graph yn+4. We present results for a number of different second-order and third-order feature models. Second Order Factors. We start with the model introduced by Carreras (2007). Figure 3 illustrates the factors used. Figure 3: Model 2a. Second order factors of Carreras (2007). We omit the right-headed cases, which are mirror images. The model comprises a factoring into one first order part and three second order factors (2- 4): 1) The head (h) and the dependent (c); 2) the head, the dependent and the left-most (or right-most) grandchild in between (cmi); 3) the head, the dependent and the right-most (or left-most) grandchild away from the head (cmo). 4) the head, the dependent and between those words the right-most (or left-most) sibling (ci). Figure 4: 2b. The left</context>
<context position="23203" citStr="Carreras, 2007" startWordPosition="3865" endWordPosition="3866">to add to the equation 2b one or more of the following sums: (3a) E(h,c,ch1,ch2)Ey w · fgra(x,h,c,ch1,ch2) (3b) E(h,c,cm1,cm2)Ey w · fgra(x,h,c,cm1,cm2) (3c) E(h,c,cmo,tmo)Ey w · fgra(x,h,c,cmo,tmo) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifi</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a Higher-order Projective Dependency Parser. In EMNLP/CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="11221" citStr="Collins and Roark (2004)" startWordPosition="1799" endWordPosition="1802">umoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order graph-based scoring model (Zhang and Clark, 2008, 567), which is similar to the approach we describe in this paper. Howeve</context>
<context position="25567" citStr="Collins and Roark (2004" startWordPosition="4253" endWordPosition="4256">nt over-fitting, we use averaging to cope with this problem, cf. (Freund and Schapire, 1998; Collins, 2002). We calculate the error e as the sum of all attachment errors and label errors both weighted by 0.5. We use the following equations to compute the update. loss: lt = e-(scoreT (xt , yt )-scoreT (xt, yt)) PA-update: Tt = l� ||f9�fp||2 We train the model to select the transitions and the completion model together and therefore, we use one parameter space. In order to compute the weight vector, we employ standard online learning with 25 training iterations, and carry out early updates, cf. Collins and Roark (2004; Zhang and Clark (2008). Efficient Implementation. Keeping the scoring with the completion model tractable with millions of feature weights and for second- and third-order factors requires careful bookkeeping and a number of specialized techniques from recent work on dependency parsing. We use two variables to store the scores (a) for complete factors and (b) for incomplete factors. The complete factors (first-order factors and higher-order factors for which further augmentation is structurally excluded) need to be calculated only once and can then be stored with the tree factors. The incompl</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL, pages 111– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="23989" citStr="Collins, 2002" startWordPosition="3990" endWordPosition="3991">hree-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use a</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="24012" citStr="Crammer et al., 2006" startWordPosition="3992" endWordPosition="3995">-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 80</context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer. 2006. Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="1307" citStr="Eisner, 1996" startWordPosition="194" endWordPosition="195">ed features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left</context>
<context position="13587" citStr="Eisner (1996)" startWordPosition="2183" endWordPosition="2184">di, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The inp</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340– 345, Copenhaen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1998</date>
<booktitle>In 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>209--217</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="23974" citStr="Freund and Schapire, 1998" startWordPosition="3986" endWordPosition="3989">c. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., </context>
</contexts>
<marker>Freund, Schapire, 1998</marker>
<rawString>Y. Freund and R. E. Schapire. 1998. Large margin classification using the perceptron algorithm. In 11th Annual Conference on Computational Learning Theory, pages 209–217, New York, NY. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>M Dredze</author>
</authors>
<title>Small statistical models by random feature mixing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-2008 Workshop on Mobile Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24472" citStr="Ganchev and Dredze (2008)" startWordPosition="4065" endWordPosition="4068">, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a, etc.). To prevent over-fitting, we use averaging to cope with this problem, cf. (Freund and Schapire, 1998; Collins, 2002). We calculate the er</context>
</contexts>
<marker>Ganchev, Dredze, 2008</marker>
<rawString>K. Ganchev and M. Dredze. 2008. Small statistical models by random feature mixing. In Proceedings of the ACL-2008 Workshop on Mobile Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gesmundo</author>
<author>J Henderson</author>
<author>P Merlo</author>
<author>I Titov</author>
</authors>
<title>A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for Multiple Languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.,</location>
<marker>Gesmundo, Henderson, Merlo, Titov, 2009</marker>
<rawString>A. Gesmundo, J. Henderson, P. Merlo, and I. Titov. 2009. A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for Multiple Languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), Boulder, Colorado, USA., June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>742--750</pages>
<contexts>
<context position="27087" citStr="Goldberg and Elhadad, 2010" startWordPosition="4501" endWordPosition="4504"> transition and the beam has to be sorted. The parser sorts the beam when it exceeds the maximal beam size, in order to discard superfluous parses or when the parsing algorithm terminates in order to select the best parse tree. The complexity of the transition-based parser is quadratic due to swap operation in the worse case, which is rare, and O(n) in the best case, cf. (Nivre, 2009). The beam size B is constant. Hence, the complexity is in the worst case O(n2). The parsing time is to a large degree determined by the feature extraction, the score calculation and the implementation, cf. also (Goldberg and Elhadad, 2010). The transition-based parser is able to parse 30 sentences per second. The parser with completion model processes about 5 sentences per second with a beam size of 80. Note, we use a rich feature set, a completion model with third order factors, negative features, and a large beam. 3 We implemented the following optimizations: (1) We use a parallel feature extraction for the beam elements. Each process extracts the features, scores the possible transitions and computes the score of the completion model. After the extension step, the beam is sorted and the best elements are selected according t</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Y. Goldberg and M. Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In HLT-NAACL, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G´omez-Rodr´ıguez</author>
<author>J Nivre</author>
</authors>
<title>A Transition-Based Parser for 2-Planar Dependency Structures. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>1492--1501</pages>
<marker>G´omez-Rodr´ıguez, Nivre, 2010</marker>
<rawString>C. G´omez-Rodr´ıguez and J. Nivre. 2010. A Transition-Based Parser for 2-Planar Dependency Structures. In ACL, pages 1492–1501.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M Ant`onia Marti</author>
<author>L M`arquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Pad´o</author>
<author>J ˇStˇep´anek</author>
<author>P Straˇn´ak</author>
<author>M Surdeanu</author>
</authors>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, </marker>
<rawString>J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M. Ant`onia Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, P. Straˇn´ak, M. Surdeanu,</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>Y Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>1--18</pages>
<location>Boulder, United States,</location>
<marker>Xue, Zhang, 2009</marker>
<rawString>N. Xue, and Y. Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 1–18, Boulder, United States, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12276" citStr="Huang and Sagae, 2010" startWordPosition="1968" endWordPosition="1971">th a combined transition-based/second-order graph-based scoring model (Zhang and Clark, 2008, 567), which is similar to the approach we describe in this paper. However, their approach does not involve beam rescoring as the partial structures built by the transition-based parser are subsequently augmented; hence, there are cases in which our approach is able to differentiate based on higher-order factors that go unnoticed by the combined model of (Zhang and Clark, 2008, 567). One step beyond the use of a beam is a dynamic programming approach to carry out a full search in the state space, cf. (Huang and Sagae, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre a</context>
<context position="31258" citStr="Huang and Sagae, 2010" startWordPosition="5204" endWordPosition="5207">. We use a beam size of 80 for our transition-based parser and 25 training iterations. The second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009). This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following tokens ‘‘ ’’:,. Parser UAS LAS (McDonald et al., 2005) 90.9 (McDonald and Pereira, 2006) 91.5 (Huang and Sagae, 2010) 92.1 (Zhang and Nivre, 2011) 92.9 (Koo and Collins, 2010) 93.04 (Martins et al., 2010) 93.26 T (baseline) 92.7 G2a (baseline) 92.89 T2a 92.94 91.87 T2ab 93.16 92.08 T2ab3a 93.20 92.10 T2ab3b 93.23 92.15 T2ab3c 93.17 92.10 T2ab3abc+ 93.39 92.38 G2a+ 93.1 (Koo et al., 2008) † 93.16 (Carreras et al., 2008) † 93.5 (Suzuki et al., 2009) † 93.79 Table 2: English Attachment Scores for the Penn2Malt conversion of the Penn Treebank for the test set. Punctuation is excluded from the evaluation. The results marked with † are not directly comparable to our work as they depend on additional sources of inf</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Investigating multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Shared Task Session of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>206--210</pages>
<location>New York City, United States,</location>
<contexts>
<context position="5680" citStr="Johansson and Nugues, 2006" startWordPosition="874" endWordPosition="878">issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level. The advantages of the graph-based approach – a more globally informed basis for the decision among different attachment options – have to be included as part of the scoring procedure. As a prerequisite, our algorithm will require a memory for storing alternative analyses among which to choose. This has been previously introduced in transitionbased approaches in the form of a beam (Johansson and Nugues, 2006): rather than representing only the best-scoring history of transitions, the k best-scoring alternative histories are kept around. As we will indicate in the following, the mere addition of beam search does not help overcome a representational key issue of transition-based parsing: in many situations, a transition-based parser is forced to make an attachment decision for a given input word at a point where no or only partial information about the word’s own dependents (and further decendents) is available. Figure 1 illustrates such a case. Figure 1: The left set of brackets indicates material </context>
<context position="10978" citStr="Johansson and Nugues (2006)" startWordPosition="1761" endWordPosition="1764">er and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition</context>
</contexts>
<marker>Johansson, Nugues, 2006</marker>
<rawString>R. Johansson and P. Nugues. 2006. Investigating multilingual dependency parsing. In Proceedings of the Shared Task Session of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 206–210, New York City, United States, June 8-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Dependencybased Syntactic–Semantic Analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task Session of CoNLL-2008,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="28300" citStr="Johansson and Nugues, 2008" startWordPosition="4713" endWordPosition="4716">ed according to the beam size. (2) The calculation of each score is optimized (beyond the distinction of a static and a dynamic component): We calculate for each location determined by the last element sl E QZ and the first element of b0 E QZ a numeric feature representation. This is kept fix and we add only the numeric value for each of the edge labels plus a value for the transition left-arc or right-arc. In this way, we create the features incrementally. This has some similarity to Goldberg and Elhadad (2010). (3) We apply edge filtering as it is used in graphbased dependency parsing, cf. (Johansson and Nugues, 2008), i.e., we calculate the edge weights only for the labels that were found for the part-ofspeech combination of the head and dependent in the training data. 5 Parsing Experiments and Discussion The results of different parsing systems are often hard to compare due to differences in phrase structure to dependency conversions, corpus version, and experimental settings. For better comparison, we provide results on English for two commonly used data sets, based on two different conversions of the Penn Treebank. The first uses the Penn2Malt conversion based on the head36 core, 3.33 Ghz Intel Nehalem</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>R. Johansson and P. Nugues. 2008. Dependencybased Syntactic–Semantic Analysis with PropBank and NomBank. In Proceedings of the Shared Task Session of CoNLL-2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>646--652</pages>
<contexts>
<context position="12867" citStr="Kahane et al., 1998" startWordPosition="2064" endWordPosition="2067">, cf. (Huang and Sagae, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). T</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar. In COLING-ACL, pages 646–652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="13961" citStr="Koo and Collins (2010)" startWordPosition="2239" endWordPosition="2242">l known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The input to the parser is a word string x, the goal is to find the optimal set y of labeled edges xi —*, xj forming a dependency tree over x U{root}. We characterize the state of a transitionbased parser as πi=(σi, βi, yi, hi), πi E H, the set of possible states. σi is a stack of words from x that are still under consideration; βi is the input buffer, the suffix of x yet to be </context>
<context position="20685" citStr="Koo and Collins (2010)" startWordPosition="3432" endWordPosition="3435">ay from the head (cmo). 4) the head, the dependent and between those words the right-most (or left-most) sibling (ci). Figure 4: 2b. The left-most dependent of the head or the right-most dependent in the right-headed case. Figure 4 illustrates a new type of factor we use, which includes the left-most dependent in the leftheaded case and symmetricaly the right-most sibling in the right-head case. Third Order Factors. In addition to the second order factors, we investigate combinations of third order factors. Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). They restrict the factor to the innermost sibling pair for the tri-siblings 81 and the outermost pair for the grand-siblings. We use the first two siblings of the dependent from the left side of the head for the tri-siblings and the first two dependents of the child for the grandsiblings. With these factors, we aim to capture non-projective edges and subcategorization information. Figure 7 illustrates a factor of a sequence of four nodes. All the right headed variants are symmetrically and left out for brevity. Figure 5: 3a. The first two children of the head, which do not include the edge b</context>
<context position="23227" citStr="Koo and Collins, 2010" startWordPosition="3867" endWordPosition="3870">uation 2b one or more of the following sums: (3a) E(h,c,ch1,ch2)Ey w · fgra(x,h,c,ch1,ch2) (3b) E(h,c,cm1,cm2)Ey w · fgra(x,h,c,cm1,cm2) (3c) E(h,c,cmo,tmo)Ey w · fgra(x,h,c,cmo,tmo) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the tr</context>
<context position="30516" citStr="Koo and Collins, 2010" startWordPosition="5080" endWordPosition="5083">, T2ab3bc, and T2a3abc present the result for the parser with completion model. The subscript letters denote the used factors of the completion model as shown in Figure 3 to 7. The parsers with subscribed plus (e.g. G2a+) in addition use feature templates that contain one word left or right of the head, dependent, siblings, and grandchildren. We left those feature in our previous models out as they may interfere with the second and third order factors. As in previous work, we exclude punctuation marks for the English data converted with Penn2Malt in the evaluation, cf. (McDonald et al., 2005; Koo and Collins, 2010; Zhang and Nivre, 2011).5 We optimized the feature model of our parser on section 24 and used section 23 for evaluation. We use a beam size of 80 for our transition-based parser and 25 training iterations. The second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009). This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag</context>
<context position="35651" citStr="Koo and Collins (2010)" startWordPosition="5911" endWordPosition="5914">ent results on the Chinese Treebank. To our knowledge, we obtain the best published results so far. 6The results of the baseline T compared to T2ab3abc are statistically significant (p &lt; 0.01). 6 Conclusion and Future Work The parser introduced in this paper combines advantageous properties from the two major paradigms in data-driven dependency parsing, in particular worst case quadratic complexity of transition-based parsing with a swap operation and the consideration of complete second and third order factors in the scoring of alternatives. While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. In contrast to parser stacking, which involves running two parsers in training and application, we use only the feature model of a graph-based parser but not the graph-based parsing algorithm. This is not only conceptually superior, but makes training much simpler, since no jackknifing has to be carried out. Zhang and Clark (2008) proposed a similar combination, without the rescoring procedure. Our implementation allows for the use of rich feature sets in the combined scoring</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<pages>595--603</pages>
<contexts>
<context position="31531" citStr="Koo et al., 2008" startWordPosition="5250" endWordPosition="5253"> phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following tokens ‘‘ ’’:,. Parser UAS LAS (McDonald et al., 2005) 90.9 (McDonald and Pereira, 2006) 91.5 (Huang and Sagae, 2010) 92.1 (Zhang and Nivre, 2011) 92.9 (Koo and Collins, 2010) 93.04 (Martins et al., 2010) 93.26 T (baseline) 92.7 G2a (baseline) 92.89 T2a 92.94 91.87 T2ab 93.16 92.08 T2ab3a 93.20 92.10 T2ab3b 93.23 92.15 T2ab3c 93.17 92.10 T2ab3abc+ 93.39 92.38 G2a+ 93.1 (Koo et al., 2008) † 93.16 (Carreras et al., 2008) † 93.5 (Suzuki et al., 2009) † 93.79 Table 2: English Attachment Scores for the Penn2Malt conversion of the Penn Treebank for the test set. Punctuation is excluded from the evaluation. The results marked with † are not directly comparable to our work as they depend on additional sources of information (Brown Clusters). tags. From the same data set, we selected the corpora for Czech and German. In all cases, we used the provided training, development, and test data split, cf. (Hajiˇc et al., 2009). In contrast to the evaluation of the Penn2Malt conversion, we in</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In proceedings of the 6th conference on Natural language learning - Volume 20, COLING-02,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10577" citStr="Kudo and Matsumoto (2002)" startWordPosition="1697" endWordPosition="1700">drops out of the beam before complete information becomes available. But as our experiments show, this does not seem to be a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>T. Kudo and Y. Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In proceedings of the 6th conference on Natural language learning - Volume 20, COLING-02, pages 1– 7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
<author>C G´omez-Rodr´ıguez</author>
<author>G Satta</author>
</authors>
<title>Dynamic programming algorithms for transition-based dependency parsers.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>673--682</pages>
<marker>Kuhlmann, G´omez-Rodr´ıguez, Satta, 2011</marker>
<rawString>M. Kuhlmann, C. G´omez-Rodr´ıguez, and G. Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In ACL, pages 673–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
<author>Pedro Aguiar</author>
<author>Mario Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<pages>34--44</pages>
<contexts>
<context position="31345" citStr="Martins et al., 2010" startWordPosition="5219" endWordPosition="5222">he second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009). This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following tokens ‘‘ ’’:,. Parser UAS LAS (McDonald et al., 2005) 90.9 (McDonald and Pereira, 2006) 91.5 (Huang and Sagae, 2010) 92.1 (Zhang and Nivre, 2011) 92.9 (Koo and Collins, 2010) 93.04 (Martins et al., 2010) 93.26 T (baseline) 92.7 G2a (baseline) 92.89 T2a 92.94 91.87 T2ab 93.16 92.08 T2ab3a 93.20 92.10 T2ab3b 93.23 92.15 T2ab3c 93.17 92.10 T2ab3abc+ 93.39 92.38 G2a+ 93.1 (Koo et al., 2008) † 93.16 (Carreras et al., 2008) † 93.5 (Suzuki et al., 2009) † 93.79 Table 2: English Attachment Scores for the Penn2Malt conversion of the Penn Treebank for the test set. Punctuation is excluded from the evaluation. The results marked with † are not directly comparable to our work as they depend on additional sources of information (Brown Clusters). tags. From the same data set, we selected the corpora for Cz</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. pages 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms. In</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="13464" citStr="McDonald and Pereira, 2006" startWordPosition="2157" endWordPosition="2161">e parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser u</context>
<context position="23187" citStr="McDonald and Pereira, 2006" startWordPosition="3861" endWordPosition="3864">n in Figure 5 to 7, we have to add to the equation 2b one or more of the following sums: (3a) E(h,c,ch1,ch2)Ey w · fgra(x,h,c,ch1,ch2) (3b) E(h,c,cm1,cm2)Ey w · fgra(x,h,c,cm1,cm2) (3c) E(h,c,cmo,tmo)Ey w · fgra(x,h,c,cmo,tmo) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a</context>
<context position="31229" citStr="McDonald and Pereira, 2006" startWordPosition="5199" endWordPosition="5202">and used section 23 for evaluation. We use a beam size of 80 for our transition-based parser and 25 training iterations. The second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009). This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following tokens ‘‘ ’’:,. Parser UAS LAS (McDonald et al., 2005) 90.9 (McDonald and Pereira, 2006) 91.5 (Huang and Sagae, 2010) 92.1 (Zhang and Nivre, 2011) 92.9 (Koo and Collins, 2010) 93.04 (Martins et al., 2010) 93.26 T (baseline) 92.7 G2a (baseline) 92.89 T2a 92.94 91.87 T2ab 93.16 92.08 T2ab3a 93.20 92.10 T2ab3b 93.23 92.15 T2ab3c 93.17 92.10 T2ab3abc+ 93.39 92.38 G2a+ 93.1 (Koo et al., 2008) † 93.16 (Carreras et al., 2008) † 93.5 (Suzuki et al., 2009) † 93.79 Table 2: English Attachment Scores for the Penn2Malt conversion of the Penn Treebank for the test set. Punctuation is excluded from the evaluation. The results marked with † are not directly comparable to our work as they depend</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In In Proc. of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online Large-margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1331" citStr="McDonald et al., 2005" startWordPosition="196" endWordPosition="199">d show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left-to-right parsing proces</context>
<context position="13435" citStr="McDonald et al., 2005" startWordPosition="2153" endWordPosition="2156">d used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transit</context>
<context position="23159" citStr="McDonald et al., 2005" startWordPosition="3857" endWordPosition="3860">ion of the factors shown in Figure 5 to 7, we have to add to the equation 2b one or more of the following sums: (3a) E(h,c,ch1,ch2)Ey w · fgra(x,h,c,ch1,ch2) (3b) E(h,c,cm1,cm2)Ey w · fgra(x,h,c,cm1,cm2) (3c) E(h,c,cmo,tmo)Ey w · fgra(x,h,c,cmo,tmo) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the o</context>
<context position="30493" citStr="McDonald et al., 2005" startWordPosition="5076" endWordPosition="5079">a, T2ab, T2ab3a, T2ab3b, T2ab3bc, and T2a3abc present the result for the parser with completion model. The subscript letters denote the used factors of the completion model as shown in Figure 3 to 7. The parsers with subscribed plus (e.g. G2a+) in addition use feature templates that contain one word left or right of the head, dependent, siblings, and grandchildren. We left those feature in our previous models out as they may interfere with the second and third order factors. As in previous work, we exclude punctuation marks for the English data converted with Penn2Malt in the evaluation, cf. (McDonald et al., 2005; Koo and Collins, 2010; Zhang and Nivre, 2011).5 We optimized the feature model of our parser on section 24 and used section 23 for evaluation. We use a beam size of 80 for our transition-based parser and 25 training iterations. The second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009). This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online Large-margin Training of Dependency Parsers. In Proc. ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating GraphBased and Transition-Based Dependency Parsers. In</title>
<date>2008</date>
<booktitle>ACL-08,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4590" citStr="Nivre and McDonald, 2008" startWordPosition="692" endWordPosition="695"> of larger spans are more cumbersome to integrate into the model (since the combination algorithm has to be adjusted), in particular for third-order factors or higher. Empirically, i.e., when applied in supervised machine learning experiments based on existing treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results (Nivre and McDonald, 2008): in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser. Another successful technique to combine parsers is voting as carried out by Sagae and Lavie (2006). The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results. The main issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integr</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating GraphBased and Transition-Based Dependency Parsers. In ACL-08, pages 950–958, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12893" citStr="Nivre and Nilsson, 2005" startWordPosition="2068" endWordPosition="2071">e, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser </context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Kuhlmann</author>
<author>J Hall</author>
</authors>
<title>An improved oracle for dependency parsing with online reordering.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies, IWPT ’09,</booktitle>
<pages>73--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13081" citStr="Nivre et al., 2009" startWordPosition="2095" endWordPosition="2098">rade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007)</context>
</contexts>
<marker>Nivre, Kuhlmann, Hall, 2009</marker>
<rawString>J. Nivre, M. Kuhlmann, and J. Hall. 2009. An improved oracle for dependency parsing with online reordering. In Proceedings of the 11th International Conference on Parsing Technologies, IWPT ’09, pages 73–76, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An Efficient Algorithm for Projective Dependency Parsing.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>149--160</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1264" citStr="Nivre, 2003" startWordPosition="188" endWordPosition="189">tion that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in </context>
<context position="10721" citStr="Nivre (2003)" startWordPosition="1722" endWordPosition="1723">r requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle tra</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An Efficient Algorithm for Projective Dependency Parsing. In 8th International Workshop on Parsing Technologies, pages 149–160, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Non-Projective Dependency Parsing in Expected Linear Time.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>351--359</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="12996" citStr="Nivre, 2009" startWordPosition="2083" endWordPosition="2084">s to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodriguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduc</context>
<context position="26847" citStr="Nivre, 2009" startWordPosition="4461" endWordPosition="4462">ive additional descendants) need to be dynamically recomputed while the tree is built. The parsing algorithm only has to compute the scores of the factored model when the transitionbased parser selects a left-arc or right-arc transition and the beam has to be sorted. The parser sorts the beam when it exceeds the maximal beam size, in order to discard superfluous parses or when the parsing algorithm terminates in order to select the best parse tree. The complexity of the transition-based parser is quadratic due to swap operation in the worse case, which is rare, and O(n) in the best case, cf. (Nivre, 2009). The beam size B is constant. Hence, the complexity is in the worst case O(n2). The parsing time is to a large degree determined by the feature extraction, the score calculation and the implementation, cf. also (Goldberg and Elhadad, 2010). The transition-based parser is able to parse 30 sentences per second. The parser with completion model processes about 5 sentences per second with a beam size of 80. Note, we use a rich feature set, a completion model with third order factors, negative features, and a large beam. 3 We implemented the following optimizations: (1) We use a parallel feature e</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>J. Nivre. 2009. Non-Projective Dependency Parsing in Expected Linear Time. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351–359, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX,</booktitle>
<pages>129--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4845" citStr="Sagae and Lavie (2006)" startWordPosition="737" endWordPosition="740">ng treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results (Nivre and McDonald, 2008): in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser. Another successful technique to combine parsers is voting as carried out by Sagae and Lavie (2006). The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results. The main issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level. The advantages of the graph-based approach – a more globally informed basis for the decision among different attachment options – have to be included as part of the scoring pro</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>K. Sagae and A. Lavie. 2006. Parser combination by reparsing. In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX, pages 129–132, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Shi</author>
<author>J Petterson</author>
<author>G Dror</author>
<author>J Langford</author>
<author>A Smola</author>
<author>S V N Vishwanathan</author>
</authors>
<title>Hash Kernels for Structured Data. In</title>
<date>2009</date>
<journal>Journal of Machine Learning.</journal>
<contexts>
<context position="24579" citStr="Shi et al., 2009" startWordPosition="4084" endWordPosition="4087">hapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a, etc.). To prevent over-fitting, we use averaging to cope with this problem, cf. (Freund and Schapire, 1998; Collins, 2002). We calculate the error e as the sum of all attachment errors and label errors both weighted by 0.5. We use the following equat</context>
</contexts>
<marker>Shi, Petterson, Dror, Langford, Smola, Vishwanathan, 2009</marker>
<rawString>Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, and S.V.N. Vishwanathan. 2009. Hash Kernels for Structured Data. In Journal of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing. In</title>
<date>2009</date>
<booktitle>EMNLP,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="31592" citStr="Suzuki et al., 2009" startWordPosition="5262" endWordPosition="5265">, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following tokens ‘‘ ’’:,. Parser UAS LAS (McDonald et al., 2005) 90.9 (McDonald and Pereira, 2006) 91.5 (Huang and Sagae, 2010) 92.1 (Zhang and Nivre, 2011) 92.9 (Koo and Collins, 2010) 93.04 (Martins et al., 2010) 93.26 T (baseline) 92.7 G2a (baseline) 92.89 T2a 92.94 91.87 T2ab 93.16 92.08 T2ab3a 93.20 92.10 T2ab3b 93.23 92.15 T2ab3c 93.17 92.10 T2ab3abc+ 93.39 92.38 G2a+ 93.1 (Koo et al., 2008) † 93.16 (Carreras et al., 2008) † 93.5 (Suzuki et al., 2009) † 93.79 Table 2: English Attachment Scores for the Penn2Malt conversion of the Penn Treebank for the test set. Punctuation is excluded from the evaluation. The results marked with † are not directly comparable to our work as they depend on additional sources of information (Brown Clusters). tags. From the same data set, we selected the corpora for Czech and German. In all cases, we used the provided training, development, and test data split, cf. (Hajiˇc et al., 2009). In contrast to the evaluation of the Penn2Malt conversion, we include punctuation marks for these corpora and follow in that </context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>J. Suzuki, H. Isozaki, X. Carreras, and M Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In EMNLP, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>A Latent Variable Model for Generative Dependency Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="11098" citStr="Titov and Henderson, 2007" startWordPosition="1781" endWordPosition="1784">xperiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I. Titov and J. Henderson. 2007. A Latent Variable Model for Generative Dependency Parsing. In Proceedings of IWPT, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="10609" citStr="Yamada and Matsumoto (2003)" startWordPosition="1702" endWordPosition="1705">omplete information becomes available. But as our experiments show, this does not seem to be a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and </context>
<context position="29150" citStr="Yamada and Matsumoto, 2003" startWordPosition="4853" endWordPosition="4856">ng systems are often hard to compare due to differences in phrase structure to dependency conversions, corpus version, and experimental settings. For better comparison, we provide results on English for two commonly used data sets, based on two different conversions of the Penn Treebank. The first uses the Penn2Malt conversion based on the head36 core, 3.33 Ghz Intel Nehalem 83 Section Sentences PoS Acc. Training 2-21 39.832 97.08 Dev 24 1.394 97.18 Test 23 2.416 97.30 Table 1: Overview of the training, development and test data split converted to dependency graphs with head-finding rules of (Yamada and Matsumoto, 2003). The last column shows the accuracy of Part-of-Speech tags. finding rules of Yamada and Matsumoto (2003). Table 1 gives an overview of the properties of the corpus. The annotation of the corpus does not contain non-projective links. The training data was 10-fold jackknifed with our own tagger.4. Table 1 shows the tagging accuracy. Table 2 lists the accuracy of our transitionbased parser with completion model together with results from related work. All results use predicted PoS tags. As a baseline, we present in addition results without the completion model and a graph-based parser with secon</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of IWPT, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Hawaii, USA.</location>
<contexts>
<context position="11123" citStr="Zhang and Clark (2008)" startWordPosition="1785" endWordPosition="1788">tion results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order graph-based scoring mode</context>
<context position="25591" citStr="Zhang and Clark (2008)" startWordPosition="4257" endWordPosition="4260">veraging to cope with this problem, cf. (Freund and Schapire, 1998; Collins, 2002). We calculate the error e as the sum of all attachment errors and label errors both weighted by 0.5. We use the following equations to compute the update. loss: lt = e-(scoreT (xt , yt )-scoreT (xt, yt)) PA-update: Tt = l� ||f9�fp||2 We train the model to select the transitions and the completion model together and therefore, we use one parameter space. In order to compute the weight vector, we employ standard online learning with 25 training iterations, and carry out early updates, cf. Collins and Roark (2004; Zhang and Clark (2008). Efficient Implementation. Keeping the scoring with the completion model tractable with millions of feature weights and for second- and third-order factors requires careful bookkeeping and a number of specialized techniques from recent work on dependency parsing. We use two variables to store the scores (a) for complete factors and (b) for incomplete factors. The complete factors (first-order factors and higher-order factors for which further augmentation is structurally excluded) need to be calculated only once and can then be stored with the tree factors. The incomplete factors (higher-orde</context>
<context position="34195" citStr="Zhang and Clark (2008)" startWordPosition="5668" endWordPosition="5671">7 83.22/88.35 88.50/90.59 T2ab3abc 90.31/92.58 83.31/88.30 88.33/90.45 G2a+ 90.39/92.8 81.43/88.0 88.26/90.50 T2ab3ab+ 90.36/92.66 83.48/88.47 88.51/90.62 Table 3: Labeled Attachment Scores of parsers that use the data sets of the CoNLL shared task 2009. In line with previous work, punctuation is included. The parsers marked with † used a joint model for syntactic parsing and semantic role labelling. We provide more parsing results for the languages of CoNLL-X Shared Task at http://code.google.com/p/mate-tools/. Table 4: Chinese Attachment Scores for the conversion of CTB 5 with head rules of Zhang and Clark (2008). We take the standard split of CTB 5 and use in line with previous work gold segmentation, POStags and exclude punctuation marks for the evaluation. of the score. Small and statistically significant improvements provides the additional second order factor (2b).6 We tried to determine the best third order factors or set of factors but we cannot denote such a factor which is the best for all languages. For German, we obtained a significant improvement with the factor (3b). We believe that this is due to the flat annotation of PPs in the German corpus. If we combine all third order factors we ob</context>
<context position="36103" citStr="Zhang and Clark (2008)" startWordPosition="5981" endWordPosition="5984">and the consideration of complete second and third order factors in the scoring of alternatives. While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. In contrast to parser stacking, which involves running two parsers in training and application, we use only the feature model of a graph-based parser but not the graph-based parsing algorithm. This is not only conceptually superior, but makes training much simpler, since no jackknifing has to be carried out. Zhang and Clark (2008) proposed a similar combination, without the rescoring procedure. Our implementation allows for the use of rich feature sets in the combined scoring functions, and our experimental results show that the “graph-based” completion model leads to an increase of between 0.4 (for English) and about 1 percentage points (for Czech). The scores go beyond the current state of the art results for typologically different languages such as Chinese, Czech, English, and German. For Czech, English (Penn2Malt) and German, these are to our knowlege the highest reported scores of a dependency parser that does no</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Y. Zhang and S. Clark. 2008. A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam-search. In Proceedings of EMNLP, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="22885" citStr="Zhang and Nivre (2011)" startWordPosition="3810" endWordPosition="3813">ctor. Again, w is the corresponding weight vector. In order to add the factor of Figure 4 to our model, we have to add the scoring function (2a) the sum: (2b) scoreG.,(x, y) = scoreG..(x, y) + E(h,c,cmi)Ey w · fgra(x,h,c,cmi) In order to build a scoring function for combination of the factors shown in Figure 5 to 7, we have to add to the equation 2b one or more of the following sums: (3a) E(h,c,ch1,ch2)Ey w · fgra(x,h,c,ch1,ch2) (3b) E(h,c,cm1,cm2)Ey w · fgra(x,h,c,cm1,cm2) (3c) E(h,c,cmo,tmo)Ey w · fgra(x,h,c,cmo,tmo) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the wo</context>
<context position="30540" citStr="Zhang and Nivre, 2011" startWordPosition="5084" endWordPosition="5087">present the result for the parser with completion model. The subscript letters denote the used factors of the completion model as shown in Figure 3 to 7. The parsers with subscribed plus (e.g. G2a+) in addition use feature templates that contain one word left or right of the head, dependent, siblings, and grandchildren. We left those feature in our previous models out as they may interfere with the second and third order factors. As in previous work, we exclude punctuation marks for the English data converted with Penn2Malt in the evaluation, cf. (McDonald et al., 2005; Koo and Collins, 2010; Zhang and Nivre, 2011).5 We optimized the feature model of our parser on section 24 and used section 23 for evaluation. We use a beam size of 80 for our transition-based parser and 25 training iterations. The second English data set was obtained by using the LTH conversion schema as used in the CoNLL Shared Task 2009, cf. (Hajiˇc et al., 2009). This corpus preserves the non-projectivity of the phrase structure annotation, it has a rich edge label set, and provides automatic assigned PoS 4http://code.google.com/p/mate-tools/ 5We follow Koo and Collins (2010) and ignore any token whose POS tag is one of the following</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Y. Zhang and J. Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>