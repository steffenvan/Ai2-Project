<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.942011">
Searching for Topics in a Large Collection of Texts
</title>
<author confidence="0.995344">
Martin Holub Jiˇr´ı Semeck´y Jiˇr´ı Diviˇs
</author>
<affiliation confidence="0.8888885">
Center for Computational Linguistics
Charles University, Prague
</affiliation>
<email confidence="0.9700755">
holub|semecky @ufal.mff.cuni.cz
jiri.divis@atlas.cz
</email>
<sectionHeader confidence="0.99554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945545454546">
We describe an original method that
automatically finds specific topics in a
large collection of texts. Each topic is
first identified as a specific cluster of
texts and then represented as a virtual
concept, which is a weighted mixture of
words. Our intention is to employ these
virtual concepts in document indexing.
In this paper we show some preliminary
experimental results and discuss direc-
tions of future work.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980395348837">
In the field of information retrieval (for a detailed
survey see e.g. (Baeza-Yates and Ribeiro-Neto,
1999)), document indexing and representing doc-
uments as vectors belongs among the most suc-
cessful techniques. Within the framework of the
well known vector model, the indexed elements
are usually individual words, which leads to high
dimensional vectors. However, there are several
approaches that try to reduce the high dimension-
ality of the vectors in order to improve the effec-
tivity of retrieving. The most famous is probably
the method called Latent Semantic Indexing (LSI),
introduced by Deerwester et al. (1990), which em-
ploys a specific linear transformation of original
word-based vectors using a system of “latent se-
mantic concepts”. Other two approaches which
inspired us, namely (Dhillon and Modha, 2001)
and (Torkkola, 2002), are similar to LSI but dif-
ferent in the way how they project the vectors of
documents into a space of a lower dimension.
Our idea is to establish a system of “virtual
concepts”, which are linear functions represented
by vectors, extracted from automatically discov-
ered “concept-formative clusters” of documents.
Shortly speaking, concept-formative clusters are
semantically coherent and specific sets of docu-
ments, which represent specific topics. This idea
was originally proposed by Holub (2003), who
hypothesizes that concept-oriented vector models
of documents based on indexing virtual concepts
could improve the effectiveness of both automatic
comparison of documents and their matching with
queries.
The paper is organized as follows. In section 2
we formalize the notion of concept-formative clus-
ters and give a heuristic method of finding them.
Section 3 first introduces virtual concepts in a
formal way and shows an algorithm to construct
them. Then, some experiments are shown. In sec-
tions 4 we compare our model with another ap-
proach and give a brief survey of some open ques-
tions. Finally, a short summary is given in sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.993647" genericHeader="method">
2 Concept-formative clusters
</sectionHeader>
<subsectionHeader confidence="0.998974">
2.1 Graph of a text collection
</subsectionHeader>
<bodyText confidence="0.854182">
Let be a collection of text
documents; is the size of the collection. Now
suppose that we have a function
, which gives a degree of
document similarity for each pair of documents.
Then we represent the collection as a graph.
Definition: A labeled graph is called graph of
collection if where
and each edge is labeled by
number , called weight of;
is a given document similarity threshold
(i.e. a threshold weight of edge).
Now we introduce some terminology and neces-
sary notation. Let be a graph of col-
lection . Each subset is called a cut of;
stands for the complement . If
are disjoint cuts then
. Both functions
are positive.
Thus, the specificity of cut can be formalized
by the following formula
— the greater this value, the more specific the
cut ; and are positive parameters, which
are used for balancing the two factors.
The extensity of cut is defined as a positive
function where is a
threshold size of cut.
is a set of edges
within cut;
Definition: The total quality of cut is a pos-
itive real function composed of all factors men-
tioned above and is defined as
is called weight of
cut;
is a set
of edges between cuts and;
is called weight
of the connection between cuts and;
is the expected weight of
edge in graph;
is the expected weight of
cut ;
is the expected
weight of the connection between cut X and
the rest of the collection;
each cut naturally splits the collection into
three disjoint subsets
where
and .
</bodyText>
<subsectionHeader confidence="0.998098">
2.2 Quality of cuts
</subsectionHeader>
<bodyText confidence="0.9998526875">
Now we formalize the property of “being concept-
-formative” by a positive real function called qual-
ity of cut. A high value of quality means that a cut
must be specific and extensive.
A cut is called specific if (i) the weight
is relatively high and (ii) the connec-
tion between and the rest of the collection
is relatively small. The first prop-
erty is called compactness of cut, and is defined
as , while the other is
called exhaustivity of cut, which is defined as
where the three lambdas are parameters whose
purpose is balancing the three factors.
To be concept-formative, a cut (i) must have a
sufficiently high quality and (ii) must be locally
optimal.
</bodyText>
<subsectionHeader confidence="0.998707">
2.3 Local optimization of cuts
</subsectionHeader>
<bodyText confidence="0.974633214285714">
A cut is called locally optimal regarding
quality function if each cut which is
only a small modification of the original does
not have greater quality, i.e. .
Now we describe a local search procedure
whose purpose is to optimize any input cut ;
if is not locally optimal, the output of the
Local Search procedure is a locally optimal
cut which results from the original as its lo-
cal modification. First we need the following def-
inition:
Definition: Potential of document with re-
spect to cut is a real function
: defined as
</bodyText>
<figureCaption confidence="0.5960475">
The Local Search procedure is described in
Fig. 1. Note that
</figureCaption>
<listItem confidence="0.961482">
1. Local Search gradually generates a se-
quence of cuts so that
</listItem>
<figureCaption confidence="0.770864">
Figure 1: The Local Search Algorithm
</figureCaption>
<listItem confidence="0.8904605">
(i) for , and
(ii) cut always arises from by
adding or taking away one document
into/from it;
2. since the quality of modified cuts cannot in-
crease infinitely, a finite necessarily
exists so that is locally optimal and con-
sequently the program stops at least after the
-th iteration;
3. each output cut is locally optimal.
</listItem>
<bodyText confidence="0.988147923076923">
Now we are ready to precisely define concept-
-formative clusters:
Definition: A cut is called a concept-
-formative cluster if
(ii) where is the output of the
Local Search algorithm.
The whole procedure for finding concept-
formative clusters consists of two basic stages:
first, a set of initial cuts is found within the whole
collection, and then each of them is used as a seed
for the Local Search algorithm, which locally
optimizes the quality function.
Note that are crucial parameters,
which strongly affect the whole process of search-
ing and consequently also the character of re-
sulting concept-formative clusters. We have op-
timized their values by a sort of machine learn-
ing, using a small manually annotated collection
of texts. When optimized-parameters are used,
the Local Search procedure tries to simulate
the behavior of human annotator who finds topi-
cally coherent clusters in a training collection. The
task of-optimization leads to a system of linear
inequalities, which we solve via linear program-
ming. As there is no scope for this issue here, we
cannot go into details.
</bodyText>
<sectionHeader confidence="0.997598" genericHeader="method">
3 Virtual concepts
</sectionHeader>
<bodyText confidence="0.981035344827586">
In this section we first show that concept-
-formative clusters can be viewed as fuzzy sets. In
this sense, each concept-formative cluster can be
characterized by a membership function. Fuzzy
clustering allows for some ambiguity in the data,
and its main advantage over hard clustering is
that it yields much more detailed information
on the structure of the data (cf. (Kaufman and
Rousseeuw, 1990), chapter 4).
Then we define virtual concepts as linear func-
tions which estimate degree of membership of
documents in concept-formative clusters. Since
virtual concepts are weighted mixtures of words
represented as vectors, they can also be seen as
virtual documents representing specific topics that
emerge in the analyzed collection.
Definition: Degree of membership of a document
in a concept-formative cluster
is a function : . For
we define
where is a constant. For we define
.
The following holds true for any concept-
-formative cluster and any document :
iff ;
iff .
Now we formalize the notion of virtual con-
cepts. Let be vector rep-
resentations of documents , where
</bodyText>
<figure confidence="0.986022333333333">
Algorithm:
loop:
Input: the graph of text collection ;
an initial cut .
Output: locally optimal cut .
if then
if then
end
goto loop
goto loop
(i) where is a threshold quality
and
</figure>
<figureCaption confidence="0.99998">
Figure 2: The Greedy Regression Algorithm
</figureCaption>
<bodyText confidence="0.964183375">
is the number of indexed terms. We look for
such a vector so that
approximately holds for any . This
vector is then called virtual concept corre-
sponding to concept-formative cluster .
The task of finding virtual concepts can be
solved using the Greedy Regression Algorithm
(GRA), originally suggested by Semeck´y (2003).
</bodyText>
<subsectionHeader confidence="0.998273">
3.1 Greedy Regression Algorithm
</subsectionHeader>
<bodyText confidence="0.960645">
The GRA is directly based on multiple linear re-
gression (see e.g. (Rice, 1994)). The GRA works
in iterations and gradually increases the number of
non-zero elements in the resulting vector, i.e. the
number of words with non-zero weight in the re-
sulting mixture. So this number can be explicitly
restricted by a parameter. This feature of the GRA
has been designed for the sake of generalization,
in order to not overfit the input sample.
The input of the GRA consists of (i) a sam-
ple set of document vectors with the correspond-
ing values of , (ii) a maximum number of
non-zero elements, and (iii) an error threshold.
The GRA, which is described in Fig. 2, re-
quires a procedure for solving multiple linear re-
gression (MLR) with a limited number of non-
zero elements in the resulting vector. Formally,
gets on input
a set of vectors ;
a corresponding set of values to be
approximated; and
a set of indexes of the ele-
ments which are allowed to be non-zero in
the output vector.
The output of the MLR is a vector
.
</bodyText>
<subsectionHeader confidence="0.302165">
Implementation and time complexity
</subsectionHeader>
<bodyText confidence="0.974513739130435">
For solving multiple linear regression we use a
public-domain Java package JAMA (2004), devel-
oped by the MathWorks and NIST. The computa-
tion of inverse matrix is based on the LU decom-
position, which makes it faster (Press et al., 1992).
As for the asymptotic time complexity of the
GRA, it is in complexity of the MLR
since the outer loop runs times at maximum and
the inner loop always runs nearly times. The
MLR substantially consists of matrix multiplica-
tions in dimension and a matrix inversion
in dimension . Thus the complexity of the
MLR is in because
. So the total complexity of the GRA is in
.
To reduce this high computational complexity,
we make a term pre-selection using a heuristic
method based on linear programming. Then, the
GRA does not need to deal with high-dimensional
vectors in , but works with vectors in dimen-
sion . Although the acceleration is only
linear, the required time has been reduced more
than ten times, which is practically significant.
</bodyText>
<subsectionHeader confidence="0.948324">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.998347">
The experiments reported here were done on a
small experimental collection of
</bodyText>
<figure confidence="0.945114684210526">
... output concept;
... quadratic residual error;
... number of words in the output concept.
Algorithm:
,
while do
Input:
pairs
where ;
... maximal number of words in output concept;
... quadratic residual error threshold.
Output:
end
for each do
output of MLR
if then
, ,
where each considered must
fulfill for any
</figure>
<bodyText confidence="0.998733285714286">
Czech documents. The texts were articles from
two different newspapers and one journal. Each
document was morphologically analyzed and lem-
matized (Hajiˇc, 2000) and then indexed and rep-
resented as a vector. We indexed only lemmas
of nouns, adjectives, verbs, adverbs and numer-
als whose document frequency was greater than
and less than . Then the number of indexed
terms was . The cosine similarity was
used to compute the document similarity; thresh-
old was . There were edges in
the graph of the collection.
We had computed a set of concept-formative
clusters and then approximated the corresponding
membership functions by virtual concepts.
The first thing we have observed was that the
quadratic residual error systematically and progre-
sivelly decreases in each GRA iteration. More-
over, the words in virtual concepts are obviously
intelligible for humans and strongly suggest the
topic. An example is given in Table 1.
</bodyText>
<table confidence="0.992871384615385">
words in the concept the weights
Czech lemma literally transl.
bosensk´y Bosnian
Srb Serb
UNPROFOR UNPROFOR
OSN UN
Sarajevo Sarajevo —
muslimsk´y Muslim (adj) —
odvolat withdraw —
srbsk´y Serbian —
gener´al general (n) —
list paper
quadratic residual error:
</table>
<tableCaption confidence="0.7603985">
Table 1: Two virtual concepts ( and )
corresponding to cluster #318.
</tableCaption>
<bodyText confidence="0.908593923076923">
Another example is cluster #19 focused on
“pension funds”, which was approximated
( ) by the following words (literally trans-
lated):
pension (adj), pension (n), fund , additional insurance ,
inheritance , payment , interest (n), dealer , regulation ,
lawsuit , August (adj), measure (n), approve ,
increase (v), appreciation , property , trade (adj),
attentively , improve , coupon (adj).
(The signs after the words indicate their positive
or negative weights in the concept.) Figure 3
shows the approximation of this cluster by virtual
concept.
</bodyText>
<figureCaption confidence="0.8259595">
Figure 3: The approximation of membership func-
tion corresponding to cluster #19 by a virtual con-
</figureCaption>
<bodyText confidence="0.938982">
cept (the number of words in the concept ).
</bodyText>
<sectionHeader confidence="0.988398" genericHeader="method">
4 Discussion
4.1 Related work
</sectionHeader>
<bodyText confidence="0.999986611111111">
A similar approach to searching for topics and em-
ploying them for document retrieval has been re-
cently suggested by Xu and Croft (2000), who,
however, try to employ the topics in the area of
distributed retrieval.
They use document clustering, treat each clus-
ter as a topic, and then define topics as probabil-
ity distributions of words. They use the Kullback-
-Leibler divergence with some modification as a
distance metric to determine the closeness of a
document to a cluster. Although our virtual con-
cepts cannot be interpreted as probability distribu-
tions, in this point both approaches are quite simi-
lar.
The substantial difference is in the clustering
method used. Xu and Croft have chosen the
K-Means algorithm, “for its efficiency”. In con-
trast to this hard clustering algorithm, (i) our
method is consistently based on empirical analysis
of a text collection and does not require an a priori
given number of topics; (ii) in order to induce per-
meable topics, our concept-formative clusters are
not disjoint; (iii) the specificity of our clusters is
driven by training samples given by human.
Xu and Croft suggest that retrieval based on
topics may be more robust in comparison with
the classic vector technique: Document ranking
against a query is based on statistical correlation
between query words and words in a document.
Since a document is a small sample of text, the
statistics in a document are often too sparse to re-
liably predict how likely the document is relevant
to a query. In contrast, we have much more texts
for a topic and the statistics are more stable. By
excluding clearly unrelated topics, we can avoid
retrieving many of the non-relevant documents.
</bodyText>
<sectionHeader confidence="0.512297" genericHeader="method">
4.2 Future work
</sectionHeader>
<bodyText confidence="0.999994285714286">
As our work is still in progress, there are some
open questions, which we will concentrate on in
the near future. Three main issues are (i) evalua-
tion, (ii) parameters setting (which is closely con-
nected to the previous one), and (iii) an effective
implementation of crucial algorithms (the current
implementation is still experimental).
As for the evaluation, we are building a manu-
ally annotated test collection using which we want
to test the capability of our model to estimate inter-
-document similarity in comparison with the clas-
sic vector model and the LSI model. So far, we
have been working with a Czech collection for we
also test the impact of morphology and some other
NLP methods developed for Czech. Next step will
be the evaluation on the English TREC collec-
tions, which will enable us to rigorously evaluate
if our model really helps to improve IR tasks.
The evaluation will also give us criteria for pa-
rameters setting. We expect that a positive value
ofwill significantly accelerate the computation
without loss of quality, but finding the right value
must be based on the evaluation. As for the most
important parameters of the GRA (i.e. the size of
the sample set and the number of words in con-
cept ), these should be set so that the resulting
concept is a good membership estimator also for
documents not included in the sample set.
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999962833333333">
We have designed and implemented a system that
automatically discovers specific topics in a text
collection. We try to employ it in document index-
ing. The main directions for our future work are
thorough evaluation of the model and optimization
of the parameters.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999626">
This work has been supported by the Ministry of
Education, project Center for Computational Lin-
guistics (project LN00A063).
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999568880952381">
Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto.
1999. Modern Information Retrieval. ACM Press /
Addison-Wesley.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Inderjit S. Dhillon and D. S. Modha. 2001. Concept
decompositions for large sparse text data using clus-
tering. Machine Learning, 42(1/2):143–175.
Jan Hajiˇc. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of the 6th ANLP Confer-
ence, 1st NAACL Meeting, pages 94–101, Seattle.
Martin Holub. 2003. A new approach to concep-
tual document indexing: Building a hierarchical sys-
tem of concepts based on document clusters. In
M. Aleksy et al. (eds.): ISICT 2003, Proceedings
of the International Symposium on Information and
Communication Technologies, pages 311–316. Trin-
ity College Dublin, Ireland.
JAMA. 2004. JAMA: A Java Matrix Package. Public-
domain, http://math.nist.gov/javanumerics/jama/.
Leonard Kaufman and Peter J. Rousseeuw. 1990.
Finding Groups in Data. John Wiley &amp; Sons.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 1992. Numerical Recipes in C. Second
edition, Cambridge University Press, Cambridge.
John A. Rice. 1994. Mathematical Statistics and Data
Analysis. Second edition, Duxbury Press, Califor-
nia.
Jiˇr´ı Semeck´y. 2003. Semantic word classes extrac-
ted from text clusters. In 12th Annual Confer-
ence WDS 2003, Proceeding of Contributed Papers.
MATFYZPRESS, Prague.
Kari Torkkola. 2002. Discriminative features for doc-
ument classification. In Proceedings of the Interna-
tional Conference on Pattern Recognition, Quebec
City, Canada, August 11–15.
Jinxi Xu and W. Bruce Croft. 2000. Topic-based lan-
guage models for distributed retrieval. In W. Bruce
Croft (ed.): Advances in Information Retrieval,
pages 151–172. Kluwer Academic Publishers.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514319">
<title confidence="0.999596">Searching for Topics in a Large Collection of Texts</title>
<author confidence="0.995768">Martin Holub Jiˇr´ı Semeck´y Jiˇr´ı Diviˇs</author>
<affiliation confidence="0.977742">Center for Computational Linguistics Charles University, Prague</affiliation>
<abstract confidence="0.938575785714286">holub|semecky @ufal.mff.cuni.cz jiri.divis@atlas.cz Abstract We describe an original method that automatically finds specific topics in a large collection of texts. Each topic is first identified as a specific cluster of texts and then represented as a virtual concept, which is a weighted mixture of words. Our intention is to employ these virtual concepts in document indexing. In this paper we show some preliminary experimental results and discuss directions of future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Berthier A Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>ACM Press / Addison-Wesley.</publisher>
<contexts>
<context position="759" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="108" endWordPosition="111">uistics Charles University, Prague holub|semecky @ufal.mff.cuni.cz jiri.divis@atlas.cz Abstract We describe an original method that automatically finds specific topics in a large collection of texts. Each topic is first identified as a specific cluster of texts and then represented as a virtual concept, which is a weighted mixture of words. Our intention is to employ these virtual concepts in document indexing. In this paper we show some preliminary experimental results and discuss directions of future work. 1 Introduction In the field of information retrieval (for a detailed survey see e.g. (Baeza-Yates and Ribeiro-Neto, 1999)), document indexing and representing documents as vectors belongs among the most successful techniques. Within the framework of the well known vector model, the indexed elements are usually individual words, which leads to high dimensional vectors. However, there are several approaches that try to reduce the high dimensionality of the vectors in order to improve the effectivity of retrieving. The most famous is probably the method called Latent Semantic Indexing (LSI), introduced by Deerwester et al. (1990), which employs a specific linear transformation of original word-based vectors using a</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press / Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1272" citStr="Deerwester et al. (1990)" startWordPosition="188" endWordPosition="191">tion In the field of information retrieval (for a detailed survey see e.g. (Baeza-Yates and Ribeiro-Neto, 1999)), document indexing and representing documents as vectors belongs among the most successful techniques. Within the framework of the well known vector model, the indexed elements are usually individual words, which leads to high dimensional vectors. However, there are several approaches that try to reduce the high dimensionality of the vectors in order to improve the effectivity of retrieving. The most famous is probably the method called Latent Semantic Indexing (LSI), introduced by Deerwester et al. (1990), which employs a specific linear transformation of original word-based vectors using a system of “latent semantic concepts”. Other two approaches which inspired us, namely (Dhillon and Modha, 2001) and (Torkkola, 2002), are similar to LSI but different in the way how they project the vectors of documents into a space of a lower dimension. Our idea is to establish a system of “virtual concepts”, which are linear functions represented by vectors, extracted from automatically discovered “concept-formative clusters” of documents. Shortly speaking, concept-formative clusters are semantically coher</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>D S Modha</author>
</authors>
<title>Concept decompositions for large sparse text data using clustering.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--1</pages>
<contexts>
<context position="1470" citStr="Dhillon and Modha, 2001" startWordPosition="218" endWordPosition="221">ssful techniques. Within the framework of the well known vector model, the indexed elements are usually individual words, which leads to high dimensional vectors. However, there are several approaches that try to reduce the high dimensionality of the vectors in order to improve the effectivity of retrieving. The most famous is probably the method called Latent Semantic Indexing (LSI), introduced by Deerwester et al. (1990), which employs a specific linear transformation of original word-based vectors using a system of “latent semantic concepts”. Other two approaches which inspired us, namely (Dhillon and Modha, 2001) and (Torkkola, 2002), are similar to LSI but different in the way how they project the vectors of documents into a space of a lower dimension. Our idea is to establish a system of “virtual concepts”, which are linear functions represented by vectors, extracted from automatically discovered “concept-formative clusters” of documents. Shortly speaking, concept-formative clusters are semantically coherent and specific sets of documents, which represent specific topics. This idea was originally proposed by Holub (2003), who hypothesizes that concept-oriented vector models of documents based on ind</context>
</contexts>
<marker>Dhillon, Modha, 2001</marker>
<rawString>Inderjit S. Dhillon and D. S. Modha. 2001. Concept decompositions for large sparse text data using clustering. Machine Learning, 42(1/2):143–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Morphological tagging: Data vs. dictionaries.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ANLP Conference, 1st NAACL Meeting,</booktitle>
<pages>94--101</pages>
<location>Seattle.</location>
<marker>Hajiˇc, 2000</marker>
<rawString>Jan Hajiˇc. 2000. Morphological tagging: Data vs. dictionaries. In Proceedings of the 6th ANLP Conference, 1st NAACL Meeting, pages 94–101, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Holub</author>
</authors>
<title>A new approach to conceptual document indexing: Building a hierarchical system of concepts based on document clusters.</title>
<date>2003</date>
<booktitle>ISICT 2003, Proceedings of the International Symposium on Information and Communication Technologies,</booktitle>
<pages>311--316</pages>
<editor>In M. Aleksy et al. (eds.):</editor>
<location>Trinity College Dublin, Ireland.</location>
<contexts>
<context position="1990" citStr="Holub (2003)" startWordPosition="298" endWordPosition="299">semantic concepts”. Other two approaches which inspired us, namely (Dhillon and Modha, 2001) and (Torkkola, 2002), are similar to LSI but different in the way how they project the vectors of documents into a space of a lower dimension. Our idea is to establish a system of “virtual concepts”, which are linear functions represented by vectors, extracted from automatically discovered “concept-formative clusters” of documents. Shortly speaking, concept-formative clusters are semantically coherent and specific sets of documents, which represent specific topics. This idea was originally proposed by Holub (2003), who hypothesizes that concept-oriented vector models of documents based on indexing virtual concepts could improve the effectiveness of both automatic comparison of documents and their matching with queries. The paper is organized as follows. In section 2 we formalize the notion of concept-formative clusters and give a heuristic method of finding them. Section 3 first introduces virtual concepts in a formal way and shows an algorithm to construct them. Then, some experiments are shown. In sections 4 we compare our model with another approach and give a brief survey of some open questions. Fi</context>
</contexts>
<marker>Holub, 2003</marker>
<rawString>Martin Holub. 2003. A new approach to conceptual document indexing: Building a hierarchical system of concepts based on document clusters. In M. Aleksy et al. (eds.): ISICT 2003, Proceedings of the International Symposium on Information and Communication Technologies, pages 311–316. Trinity College Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JAMA</author>
</authors>
<title>JAMA: A Java Matrix Package.</title>
<date>2004</date>
<note>Publicdomain, http://math.nist.gov/javanumerics/jama/.</note>
<contexts>
<context position="9717" citStr="JAMA (2004)" startWordPosition="1636" endWordPosition="1637">responding values of , (ii) a maximum number of non-zero elements, and (iii) an error threshold. The GRA, which is described in Fig. 2, requires a procedure for solving multiple linear regression (MLR) with a limited number of nonzero elements in the resulting vector. Formally, gets on input a set of vectors ; a corresponding set of values to be approximated; and a set of indexes of the elements which are allowed to be non-zero in the output vector. The output of the MLR is a vector . Implementation and time complexity For solving multiple linear regression we use a public-domain Java package JAMA (2004), developed by the MathWorks and NIST. The computation of inverse matrix is based on the LU decomposition, which makes it faster (Press et al., 1992). As for the asymptotic time complexity of the GRA, it is in complexity of the MLR since the outer loop runs times at maximum and the inner loop always runs nearly times. The MLR substantially consists of matrix multiplications in dimension and a matrix inversion in dimension . Thus the complexity of the MLR is in because . So the total complexity of the GRA is in . To reduce this high computational complexity, we make a term pre-selection using a</context>
</contexts>
<marker>JAMA, 2004</marker>
<rawString>JAMA. 2004. JAMA: A Java Matrix Package. Publicdomain, http://math.nist.gov/javanumerics/jama/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Kaufman</author>
<author>Peter J Rousseeuw</author>
</authors>
<title>Finding Groups in Data.</title>
<date>1990</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="7339" citStr="Kaufman and Rousseeuw, 1990" startWordPosition="1225" endWordPosition="1228">nt clusters in a training collection. The task of-optimization leads to a system of linear inequalities, which we solve via linear programming. As there is no scope for this issue here, we cannot go into details. 3 Virtual concepts In this section we first show that concept-formative clusters can be viewed as fuzzy sets. In this sense, each concept-formative cluster can be characterized by a membership function. Fuzzy clustering allows for some ambiguity in the data, and its main advantage over hard clustering is that it yields much more detailed information on the structure of the data (cf. (Kaufman and Rousseeuw, 1990), chapter 4). Then we define virtual concepts as linear functions which estimate degree of membership of documents in concept-formative clusters. Since virtual concepts are weighted mixtures of words represented as vectors, they can also be seen as virtual documents representing specific topics that emerge in the analyzed collection. Definition: Degree of membership of a document in a concept-formative cluster is a function : . For we define where is a constant. For we define . The following holds true for any concept-formative cluster and any document : iff ; iff . Now we formalize the notion</context>
</contexts>
<marker>Kaufman, Rousseeuw, 1990</marker>
<rawString>Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding Groups in Data. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical Recipes in C. Second edition,</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="9866" citStr="Press et al., 1992" startWordPosition="1662" endWordPosition="1665">res a procedure for solving multiple linear regression (MLR) with a limited number of nonzero elements in the resulting vector. Formally, gets on input a set of vectors ; a corresponding set of values to be approximated; and a set of indexes of the elements which are allowed to be non-zero in the output vector. The output of the MLR is a vector . Implementation and time complexity For solving multiple linear regression we use a public-domain Java package JAMA (2004), developed by the MathWorks and NIST. The computation of inverse matrix is based on the LU decomposition, which makes it faster (Press et al., 1992). As for the asymptotic time complexity of the GRA, it is in complexity of the MLR since the outer loop runs times at maximum and the inner loop always runs nearly times. The MLR substantially consists of matrix multiplications in dimension and a matrix inversion in dimension . Thus the complexity of the MLR is in because . So the total complexity of the GRA is in . To reduce this high computational complexity, we make a term pre-selection using a heuristic method based on linear programming. Then, the GRA does not need to deal with high-dimensional vectors in , but works with vectors in dimen</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. 1992. Numerical Recipes in C. Second edition, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Rice</author>
</authors>
<title>Mathematical Statistics and Data Analysis. Second edition,</title>
<date>1994</date>
<publisher>Duxbury Press,</publisher>
<location>California.</location>
<contexts>
<context position="8664" citStr="Rice, 1994" startWordPosition="1448" endWordPosition="1449"> collection ; an initial cut . Output: locally optimal cut . if then if then end goto loop goto loop (i) where is a threshold quality and Figure 2: The Greedy Regression Algorithm is the number of indexed terms. We look for such a vector so that approximately holds for any . This vector is then called virtual concept corresponding to concept-formative cluster . The task of finding virtual concepts can be solved using the Greedy Regression Algorithm (GRA), originally suggested by Semeck´y (2003). 3.1 Greedy Regression Algorithm The GRA is directly based on multiple linear regression (see e.g. (Rice, 1994)). The GRA works in iterations and gradually increases the number of non-zero elements in the resulting vector, i.e. the number of words with non-zero weight in the resulting mixture. So this number can be explicitly restricted by a parameter. This feature of the GRA has been designed for the sake of generalization, in order to not overfit the input sample. The input of the GRA consists of (i) a sample set of document vectors with the corresponding values of , (ii) a maximum number of non-zero elements, and (iii) an error threshold. The GRA, which is described in Fig. 2, requires a procedure f</context>
</contexts>
<marker>Rice, 1994</marker>
<rawString>John A. Rice. 1994. Mathematical Statistics and Data Analysis. Second edition, Duxbury Press, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇr´ı Semeck´y</author>
</authors>
<title>Semantic word classes extracted from text clusters.</title>
<date>2003</date>
<booktitle>In 12th Annual Conference WDS 2003, Proceeding of Contributed Papers. MATFYZPRESS,</booktitle>
<location>Prague.</location>
<marker>Semeck´y, 2003</marker>
<rawString>Jiˇr´ı Semeck´y. 2003. Semantic word classes extracted from text clusters. In 12th Annual Conference WDS 2003, Proceeding of Contributed Papers. MATFYZPRESS, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kari Torkkola</author>
</authors>
<title>Discriminative features for document classification.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Pattern Recognition,</booktitle>
<location>Quebec City, Canada,</location>
<contexts>
<context position="1491" citStr="Torkkola, 2002" startWordPosition="223" endWordPosition="224">ramework of the well known vector model, the indexed elements are usually individual words, which leads to high dimensional vectors. However, there are several approaches that try to reduce the high dimensionality of the vectors in order to improve the effectivity of retrieving. The most famous is probably the method called Latent Semantic Indexing (LSI), introduced by Deerwester et al. (1990), which employs a specific linear transformation of original word-based vectors using a system of “latent semantic concepts”. Other two approaches which inspired us, namely (Dhillon and Modha, 2001) and (Torkkola, 2002), are similar to LSI but different in the way how they project the vectors of documents into a space of a lower dimension. Our idea is to establish a system of “virtual concepts”, which are linear functions represented by vectors, extracted from automatically discovered “concept-formative clusters” of documents. Shortly speaking, concept-formative clusters are semantically coherent and specific sets of documents, which represent specific topics. This idea was originally proposed by Holub (2003), who hypothesizes that concept-oriented vector models of documents based on indexing virtual concept</context>
</contexts>
<marker>Torkkola, 2002</marker>
<rawString>Kari Torkkola. 2002. Discriminative features for document classification. In Proceedings of the International Conference on Pattern Recognition, Quebec City, Canada, August 11–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Topic-based language models for distributed retrieval.</title>
<date>2000</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>151--172</pages>
<editor>In W. Bruce Croft (ed.):</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="13115" citStr="Xu and Croft (2000)" startWordPosition="2201" endWordPosition="2204">ler , regulation , lawsuit , August (adj), measure (n), approve , increase (v), appreciation , property , trade (adj), attentively , improve , coupon (adj). (The signs after the words indicate their positive or negative weights in the concept.) Figure 3 shows the approximation of this cluster by virtual concept. Figure 3: The approximation of membership function corresponding to cluster #19 by a virtual concept (the number of words in the concept ). 4 Discussion 4.1 Related work A similar approach to searching for topics and employing them for document retrieval has been recently suggested by Xu and Croft (2000), who, however, try to employ the topics in the area of distributed retrieval. They use document clustering, treat each cluster as a topic, and then define topics as probability distributions of words. They use the Kullback-Leibler divergence with some modification as a distance metric to determine the closeness of a document to a cluster. Although our virtual concepts cannot be interpreted as probability distributions, in this point both approaches are quite similar. The substantial difference is in the clustering method used. Xu and Croft have chosen the K-Means algorithm, “for its efficienc</context>
</contexts>
<marker>Xu, Croft, 2000</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 2000. Topic-based language models for distributed retrieval. In W. Bruce Croft (ed.): Advances in Information Retrieval, pages 151–172. Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>