<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000119">
<title confidence="0.871801">
Analysis of Source Identified Text Corpora:
Exploring the Statistics of the Reused Text and Authorship
</title>
<author confidence="0.989795">
Akiko Aizawa
</author>
<affiliation confidence="0.998337">
National Institute of Informatics
</affiliation>
<address confidence="0.9793635">
2-1-2 Hitotsubashi, Chiyoda-ku
Tokyo, 101-8430, Japan
</address>
<email confidence="0.999546">
akiko@nii.ac.jp
</email>
<sectionHeader confidence="0.996635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999849846153846">
This paper aims at providing a view
of text recycled, within a short time,
by the authors themselves. We
first present a simple and general
method for extracting reused term
sequences, and then analyze several
author-identified text collections to
compare the statistical quantities. The
ratio of recycling is also measured for
each collection. Finally, related re-
search topics are introduced together
with some discussion of future research
directions.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999230780487805">
In conventional information retrieval studies,
the similarity between two documents is calcu-
lated based on the distribution of terms that ap-
pear in each document. However, in document
databases, or on the Web, there exist numbers
of documents that literally contain the same
phrases. These documents not only maintain
a good statistical resemblance but also share
a long section of terms, sometimes spread over
sentences.
When the degree of the match is beyond the
level of a simple coincidence, it is a natural con-
sequence that these sections of terms are dupli-
cated and reused by the authors. Furthermore,
we can assume that, in this digital age, this type
of &apos;recycling&apos; is an ordinal practice when author-
ing text-based products because texts are easily
copied and reused. Another important aspect
is that the reused texts are often semantically
meaningful; their survival across documents it-
self is an evidence of their usefulness. For exam-
ple, some expressions contain the definitions of
named entities that are shared between the two
documents.
It should be emphasized here that the statis-
tical similarity and the term sequence match-
ing are strongly associated, but essentially dif-
ferent, phenomena. The former is derived from
the topical relationship between the two docu-
ments, whereas the author&apos;s editing, revising,
or quoting a document, indicating some form
of &apos;social&apos; relatedness, causes the latter. How-
ever, there have been few attempts, to date, to
analyze text corpora explicitly focusing on the
reuse and reusability issues.
Based on the above observations, this paper
aims at establishing a methodological basis for
extracting featured term sequences reused in a
group of documents. First, we define the follow-
ing three types that correspond to distinctive
reuse patterns of term sequences.
</bodyText>
<listItem confidence="0.999750555555555">
(1) Compounds and phrases. Permanent lexi-
con and idiomatic expressions that are fre-
quently and universally used in texts.
(2) Instantly lexiconized texts. Passages and
conventional expressions that are only tem-
porarily and locally reused. Reusable with-
out credits to the authors, also referred to
as instant lexicon.
(3) Quoted texts. Passages that are attributed
</listItem>
<bodyText confidence="0.963707">
to a particular author. When used by other
authors, usually copied with credits, also
referred to as authored texts.
Note that we consider only the designated
&apos;writer&apos; of the target text here. Issues in iden-
tifying a copyright holder of a specific text are
outside of the scope of this paper.
While terms and compounds have long been
a central issue of natural language processing
studies, little attention has been paid to the
extraction and utilization of longer passages,
namely, the instant lexicon and the authored
texts as previously defined. Nevertheless, these
are the featured text elements that are most
strongly related to particular topics or authors,
and therefore could be useful resources in vari-
ous text processing applications, such as author-
ship identification, duplication checking, docu-
ment clustering and summarization.
Because the exploration in this direction has
just started, in this paper we limit our focus
to the following three issues. First, in section
2, we present an efficient method for extracting
reused term sequences together with the corre-
sponding document subsets. Special attention
is paid to make the method simple and general
so that it is easily applicable to wide variety of
text resources. Next, in section 3, some ana-
lytical results are reported where the proposed
method was applied to several text collections
and the statistical natures were compared. Fi-
nally, in section 4, we introduce related research
topics and discuss the utilization of the proposed
method in connection with existing text retrieval
applications.
</bodyText>
<sectionHeader confidence="0.771279" genericHeader="method">
2 Suffix Tree based Clustering
</sectionHeader>
<subsectionHeader confidence="0.864591">
2.1 Definition of ST-Clusters
</subsectionHeader>
<bodyText confidence="0.75142525">
Denote all the documents in the target corpus as
D, all the terms in the target corpus as W. The
word n-gram (n &gt; 1) is a sequence of n terms
given by:
</bodyText>
<equation confidence="0.946338">
= euil, • • •
(wi e W) . (1)
</equation>
<bodyText confidence="0.821589333333333">
Next, consider a suffix tree, each node of which
corresponds to a distinctive word n-gram ob-
served in D. For every node on the tree, there
exists an uniquely determined subset Sdeuin
(c D) given as a subset of all the documents
that contain w. In other words, wri&apos; is a se-
quence of terms that is shared between Sd(aT).
Noting that multiple nodes may refer to the
same document subset, we define a suffix tree
based cluster (ST-cluster) as a subset of nodes
on the suffix tree that is mapped to the same
document set. Namely,
</bodyText>
<equation confidence="0.97656725">
(definition) A ST-cluster is defined as a
pair (S, D) such that S is a subset of n-grams,
D is a subset of documents, and Vs c S,
Sd(s) = D, Vs S, sd(s) D.
</equation>
<bodyText confidence="0.610094666666667">
For example, in Figure 1, nodes A and B both
refer to subset { DOC#10, DOC#13 } and are
therefore merged into a single ST-cluster.
</bodyText>
<equation confidence="0.89872">
&lt;maintain&gt;
DOC#1, 3, 5, 7
8, 10, 13, ...
&lt;substantic &lt;ec&apos;l°rnic&gt;&lt;gr°wth&gt;
D0C#8, 10.13 DOC#3, 5.10.
20 13, 28, 35
oar re, ign:t cc ah prt10(&gt;g y
0 DOOM 0, 13
&lt;post&gt;&lt;nine&gt;
DOC#10, 13
level 11
</equation>
<figureCaption confidence="0.8500005">
Figure 1: Example of a ST-cluster
2.2 Procedure for ST-Clustering
</figureCaption>
<bodyText confidence="0.998191333333333">
The basic procedure for extracting ST-clusters
is similar to that used by Zamir &amp; Etzioni (1998)
and is summarized as follows:
</bodyText>
<listItem confidence="0.999221">
(1) Convert the target collection into sequences
of terms, each of which corresponds to a sin-
gle document. Apply morphological analy-
sis or other pre-processing methods when it
is necessary to determine the word bound-
aries. Neither stemming nor normalization
is applied.
(2) Generate a suffix array by a single sort.
Suffix tree nodes, together with their cor-
responding document subset lists, are then
</listItem>
<figure confidence="0.957442714285714">
&lt;attracted&gt;
DOC#8 10.13
20,33 34
Df&gt;&lt;over&gt;&lt;eight&gt;
&lt;percent&gt;&lt;for&gt;&lt;the&gt;
D0C#10, 13,
28
</figure>
<bodyText confidence="0.94630021875">
level 6
Suffix Tree Cluster = nodes 0 + ®
term sequences
&amp;quot;maintain economic growth of over
eight percent for the past nine&amp;quot;
&amp;quot;attracted substantial foreign capital
and technology&amp;quot;
documents DOC# 10, DOC# 1 3
identified as adjacent members of the suffix
array. For each node, sort the document list
according to some pre-determined order.
Sort all the suffix tree nodes using the
sorted document list as a key. Then, the
adjacent members of the node list with the
same key constitute a single ST-cluster.
The computation time of the above procedure
is basically determined by the sort operation in
step (3) (0 (n log (n))), and would be feasible
with the power of today&apos;s computers. What we
found more problematic is the cost of memory
to store all the suffix tree nodes and the corre-
sponding document lists at step (2). Figure 2
shows the count statistics for different levels of
the suffix tree generated from the Reuters col-
lection, which is also used in our later exper-
iments. Based on the figure, it becomes clear
that short length n-grams are the most memory
consuming. Because our focus is restricted to
longer n-grams, in this paper we consider only
the suffix tree nodes with longer than four term
sequences.
number of the pointers
</bodyText>
<equation confidence="0.989036166666667">
. c, CJO C ▪ bO ▪ c% ▪ GO ▪ 0
0 0 0 0 0 0 ▪ 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
</equation>
<figureCaption confidence="0.986795">
Figure 2: Numbers of pointers at level k
</figureCaption>
<subsectionHeader confidence="0.508611">
2.3 Measures for ST-Clusters
</subsectionHeader>
<bodyText confidence="0.997095875">
The ST-clusters generated are evaluated using
the following two measures. First is the term se-
quence coincidence that quantifies the strength
of the coincidence of the extracted term se-
quences. Second is the term distribution sim-
ilarity that calculates the divergence of the doc-
uments in the cluster based on the conventional
document similarity measure.
</bodyText>
<listItem confidence="0.664436">
(1) Term sequence coincidence
</listItem>
<bodyText confidence="0.954738666666667">
The coincidence score of term sequence wri&apos;
is calculated as the specific mutual information
Weil given, by definition, as:
</bodyText>
<equation confidence="0.982625">
P(w)
Well) =log P(w)) • • • P(llik) •
</equation>
<bodyText confidence="0.996454941176471">
That is, Weil is the difference between (i)
the entropy calculated based on the assumption
that the k terms (w1, , wk) occurred indepen-
dently, and (ii) the entropy calculated based on
the actual observation.
Intuitively, Weil becomes greater for longer
sequences. However, the scheme is different
from simply counting the length of the sequence
because it puts more weight on low frequency
terms. In our preliminary experiments, we com-
pared two different rankings of the ST-clusters
using M(w) and the sequence length, and ob-
served the former has a better correlation with
the term distribution similarity.
The occurrence probability P(w) in Eq. (2)
is simply determined by freq(aT), the frequency
of wri&apos; in D, and the overall total frequency F,
</bodyText>
<equation confidence="0.7927475">
given as F = freq(wi), as follows:
P(w) = freq(4) • (3)
</equation>
<bodyText confidence="0.986585538461538">
The occurrence probability of wi is also deter-
mined by Eq. (3), considering that wi is a unit
length sequence of terms. Because probability
estimation of unobserved terms is not an issue
here, we have not applied any discounting or
smoothing methods for simplicity, unlike many
language-modeling studies.
The coincidence score is calculated for every
term sequence in the ST-cluster, and then, ei-
ther the maximum or the total value is used as
an overall evaluation, depending on the purpose
of the analysis. In this paper, we consistently
use the maximum values.
</bodyText>
<listItem confidence="0.942566">
(2) Term distribution similarity
</listItem>
<bodyText confidence="0.998439375">
The document similarity of the ST-cluster is
defined using the cosine similarity commonly
used in information retrieval studies. For each
document d in the cluster, index terms are first
extracted by applying standard methods, such
as morphological analysis, stemming and stop
word removal. Then, the term vector d is gen-
erated for each document using tf-idf weighting
</bodyText>
<figure confidence="0.998922722222222">
(3)
a) 3
r 4
&gt;8
1 9
10
11
12
13
14
15
a)
x °
• number of the pointers to store
the suffix tree nodes at level k
11 number of the pointers to store
the suffix tree clusters at level k
(2)
</figure>
<tableCaption confidence="0.998661">
Table 1: Data source used in the experiments
</tableCaption>
<table confidence="0.948440888888889">
Data source Period Lang #Docs #ST Execution M(w) #Words
cluster time per sent. per sent.
Reuters 1996.8.20 — 1997.8.19 Eng 109,433 1,338,735 2644 sec. 330 24.5
San Jose Mercury 1991.1.1 — 1991.12.31 Eng 72,947 320,457 595 sec. 361 30.1
Mainichi 1998.1.1 — 1998.12.31 Jpn 10,855 111,406 78 sec. 394 33.6
Nikkei 1996.1.1 — 1996.12.31 Jpn 911 19,745 10 sec. 274 27.5
ntc-IPSJ 1988.5.19 — 1997.7.25 Jpn 26,796 226,640 99 sec. 420 32.4
ntc-JSCE 1991.9.17 — 1996.9.17 Jpn 21,259 180,538 70 sec. 434 35.3
scheme. In addition, the central vector of the
</table>
<bodyText confidence="0.999559428571429">
cluster, denoted as is calculated as an aver-
age of all the term vectors. Next, the cosine
similarities between the central and each term
vector are obtained. Finally, the averaged pair-
wise similarity values becomes the overall eval-
uation of the term distribution similarity of the
ST-cluster:
</bodyText>
<equation confidence="0.9963375">
Sim(D) = E . (4)
dED
</equation>
<bodyText confidence="0.999453">
Note that 0 &lt; Sim(D) &lt; 1, and the value be-
comes closer to one for the ST-cluster where the
documents are statistically similar to each other.
</bodyText>
<sectionHeader confidence="0.997302" genericHeader="conclusions">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.718729">
3.1 Target Corpora
</subsectionHeader>
<bodyText confidence="0.9999629375">
The six text collections used in our experiments
are shown in Table 1. We used two sets of
English newspaper articles extracted from ei-
ther Reuters (Reuters, 2000) or San Jose Mer-
cury (SJM) (Harman Sz Mark, 1993), two sets
of Japanese newspaper articles extracted from
either Mainichi (Mainichi, 2001) or NIKKEI
(Nikkei, 2001), and two sets of Japanese aca-
demic papers&apos; abstracts both extracted from
NTCIR-1 (NTCIR, 2001), one presented at the
Information Processing Society in Japan (ntc-
IPSJ), and the other at the Japan Society of
Civil Engineers (ntc-JSCE). For the newspaper
articles, we selected only articles with their au-
thors specified, either in the &apos;byline&apos; (in the case
of Reuters and SJM) or embedded in the text
in a particular form (in the case of Mainichi and
NIKKEI). Morphological analyzer ChaSen was
used for Japanese text (Matsumoto, 2001).
For each collection, ST-clusters with term se-
quences longer than four were enumerated using
the method described in 3.2. The numbers of re-
sulting clusters and the corresponding execution
time measured on 2.8GMHz Xeon/Linux are
also shown in Table 1. (Note that for compari-
son purpose, the execution time does not include
the time for morphological analysis and word
dictionary generation.) For reference, we have
also segmented the target collections into sen-
tences, and calculated the average coincidence
score together with the average number of terms
per a sentence.
</bodyText>
<subsectionHeader confidence="0.579028">
3.2 Experiment 1: Measuring the
</subsectionHeader>
<bodyText confidence="0.98402952">
instantly lexiconized texts
In our first experiment, we examine the distribu-
tion of the coincidence score of term sequences
that were reproduced accidentally without refer-
ring to the original document.
For this purpose, we first made mixtures of
the two sources: (a) Reuters and SJM, (b)
Mainichi and Nikkei, and (c) ntc-IPSJ and ntc-
JSCE. Next, we applied the ST-clustering to the
generated mixtures. Then, ST-clusters that con-
tain documents from both collections were se-
lected and term sequences with the maximum
coincidence score were examined. Note that the
pairs were arranged so that both collections be-
long to the same type (i.e., either newspaper
stories or academic papers&apos; abstracts) but orig-
inating from different publication sources (i.e.,
different newspaper companies or academic so-
cieties). The topical overlap was also kept small,
either by choosing collections of different years,
in the case of newspaper articles, or by focus-
ing on different academic fields, in the case of
papers&apos; abstracts.
Figure 3 shows the normalized histograms
(i.e., empirical p.d.f.) of the coincidence score
</bodyText>
<figure confidence="0.999455057471264">
0�.07
0�.06
0�.05
0�.04
0�.03
0�.02
0�.01
0�
0 5✑0 100 150 200
1
0.8
0.6
0.4
0.2
0
10 100 1000 10000
1
0.8
0.6
0.4
0.2
0
10 100 1000 10000
1
0.8
0.6
0.4
0.2
0
10 100 1000 10000
1
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
1
0.8
0.6
0.4
0.2
1
0
10 100 1000 10000
0
10 100 1000 10000
0
10 100 1000 10000
1
0.8
0.6
0.4
0.2
0
10 100 1000 10000
1
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
1
0
10 100 1000 10000
0
10 100 1000 10000
0 5 25 �3 0
10 15 20
0 200 �3 00 400 �5 00 �6 00
100
1000
100
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
</figure>
<bodyText confidence="0.998689181818182">
tection (Broder et.al., 1997; Chowdhury, et.al.,
2002). The second is similarity measure cal-
culation where the term distribution similarity
is used to detect potential duplicates (Molina,
et.al, 1996; Sanderson, 1997). Although most
studies allow minor syntactic variations, the du-
plication is detected for entire documents or
Web sites. Because the proposed scheme is fo-
cused on partial duplications, it could be used as
a complementary measure to improve the flexi-
bility of the duplication check.
</bodyText>
<listItem confidence="0.547337">
(3) Document clustering
</listItem>
<bodyText confidence="0.999980309523809">
There also exist studies that generate clusters
based on phrases shared between documents.
Suffix Tree Clustering (STC), proposed for on-
the-fly reorganization of the search results on the
Web, is an example close to our approach (Zamir
Etzioni, 1998). Although both STC and our
methods exploit suffix tree structure to realize
efficient clustering, the adaptations are slightly
different. Because the objective of STC is to
create semantically associated document clus-
ters, stemming and sentence level segmentation
were applied at the pre-processing stage, term
sequences longer than six were penalized with
equal weights, and the extracted &apos;base clusters&apos;
are further integrated into larger clusters. Be-
cause our focus is on the exact term sequence
match, we analyze directly the &apos;base clusters&apos;
extracted from the entire text collections.
Finally, future research directions are as fol-
lows. First, the issue of quantifying the au-
thorship of anonymous texts should be further
explored, because the interpretation may de-
pend on various factors including the language,
the media, the editing policy, or the subject
field. The proposed analytical method could
be a promising tool to explore different types
of textual resources, including Web documents,
XML-based databases, or program source codes.
The second potential research topic is the rapid
detection of partially duplicated texts as well
as the automatic generation of embedded text
anchors using the proposed clustering method.
The third issue concerns the extraction of event-
specific expressions that can be utilized further
in summarizing the contents of the cluster. The
last issue also requires such techniques as re-
segmentation and interpolation of terms, and
automatic detection of media-specific expres-
sions. In addition, it is important to develop
a refined language-based method of identifying
and classifying quoted descriptions that appear
in the texts.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99951123255814">
Reuters. Reuters Corpus, Volume 1, English language,
1996-08-20 to 1997-08-19 2000.
Donna Harman and Mark Liberman. TIPSTAR Com-
plete. 1993. Linguistic Data Consortium.
Mainichi Interactive. 1999. 1998 Mainichi Daily News
CD-ROM Version.
Nihon Keizai Shinbun. 2001. 1996-2000 Nikkei Full-text
Database.
National Center for Science Information Systems. 1999.
NTCIR Test Collection 1.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imauchi, and Tomoaki Iwa-
mura. 1997. Japanese Morphological Analysis System
ChaSen Manual. NAIST Technical Report, NAIST-
IS-TR97007.
Tony McEnery and Michael Oakes. 2000. Author-
ship Identification and Computational Stylometry. in
Handbook of Natural Language Processing, Marcel
Dekker Inc., 545-562.
Yuta Tsuboi and Yuji Matsumoto. 2002. Author-
ship Identification for Heterogeneous Documents. SIG
Notes of Information Processing Society in Japan,
SIG-NL-148, 17-24.
Abdur Chowdhury, Ophir Frieder, David Grossman, and
Mary Catherine McCabe. 2002. Collection Statistics
for Fast Duplicate Document Detection. ACM Trans.
on Information Systems, 20(2), 171-191.
Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse,
and Geoffrey Zweig. 1997. Syntactic Clustering of the
Web. Proc. of the Sixth International World Wide
Web Conference, 391-404.
Hector Garcia-Molina, Luis Gravano, and Narayanan
Shivakumar. 1996. dSCAM: Finding Document
Copies Across Multiple Databases. Proc. of Fourth
International Conference on Parallel and Distributed
Information System, 68-79.
Mark Sanderson. 1997. Duplicate Detection in the
Reuters Collection. Technical Report of the Depart-
ment of Computing Science at the University of Glas-
gow, TR-1997-5.
Oren Zamir and Oren Etzioni. 1998. Web Document
Clustering: A Feasibility Demonstration. Proc. of SI-
GIR&apos;98, 46-54.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875908">
<title confidence="0.986809">Analysis of Source Identified Text Corpora: Exploring the Statistics of the Reused Text and Authorship</title>
<author confidence="0.968511">Akiko Aizawa</author>
<affiliation confidence="0.999495">National Institute of Informatics</affiliation>
<address confidence="0.9824075">2-1-2 Hitotsubashi, Chiyoda-ku Tokyo, 101-8430, Japan</address>
<email confidence="0.974102">akiko@nii.ac.jp</email>
<abstract confidence="0.997869571428571">This paper aims at providing a view of text recycled, within a short time, by the authors themselves. We first present a simple and general method for extracting reused term sequences, and then analyze several author-identified text collections to compare the statistical quantities. The ratio of recycling is also measured for each collection. Finally, related research topics are introduced together with some discussion of future research directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Reuters Corpus</author>
</authors>
<date>2000</date>
<volume>1</volume>
<pages>1996--08</pages>
<marker>Corpus, 2000</marker>
<rawString>Reuters. Reuters Corpus, Volume 1, English language, 1996-08-20 to 1997-08-19 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIPSTAR Complete</author>
</authors>
<title>Linguistic Data Consortium.</title>
<date>1993</date>
<marker>Complete, 1993</marker>
<rawString>Donna Harman and Mark Liberman. TIPSTAR Complete. 1993. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mainichi Interactive</author>
</authors>
<date>1999</date>
<journal>Mainichi Daily News CD-ROM Version.</journal>
<marker>Interactive, 1999</marker>
<rawString>Mainichi Interactive. 1999. 1998 Mainichi Daily News CD-ROM Version.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nihon Keizai Shinbun</author>
</authors>
<date>2001</date>
<institution>Nikkei Full-text Database.</institution>
<marker>Shinbun, 2001</marker>
<rawString>Nihon Keizai Shinbun. 2001. 1996-2000 Nikkei Full-text Database.</rawString>
</citation>
<citation valid="true">
<title>Information Systems.</title>
<date>1999</date>
<journal>NTCIR Test Collection</journal>
<volume>1</volume>
<institution>National Center for Science</institution>
<marker>1999</marker>
<rawString>National Center for Science Information Systems. 1999. NTCIR Test Collection 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imauchi, and Tomoaki Iwamura.</title>
<date>1997</date>
<tech>NAIST Technical Report, NAISTIS-TR97007.</tech>
<marker>Matsumoto, 1997</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imauchi, and Tomoaki Iwamura. 1997. Japanese Morphological Analysis System ChaSen Manual. NAIST Technical Report, NAISTIS-TR97007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony McEnery</author>
<author>Michael Oakes</author>
</authors>
<title>Authorship Identification and Computational Stylometry.</title>
<date>2000</date>
<booktitle>in Handbook of Natural Language Processing,</booktitle>
<pages>545--562</pages>
<publisher>Marcel Dekker Inc.,</publisher>
<marker>McEnery, Oakes, 2000</marker>
<rawString>Tony McEnery and Michael Oakes. 2000. Authorship Identification and Computational Stylometry. in Handbook of Natural Language Processing, Marcel Dekker Inc., 545-562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Authorship Identification for Heterogeneous Documents.</title>
<date>2002</date>
<booktitle>SIG Notes of Information Processing Society in Japan, SIG-NL-148,</booktitle>
<pages>17--24</pages>
<marker>Tsuboi, Matsumoto, 2002</marker>
<rawString>Yuta Tsuboi and Yuji Matsumoto. 2002. Authorship Identification for Heterogeneous Documents. SIG Notes of Information Processing Society in Japan, SIG-NL-148, 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdur Chowdhury</author>
<author>Ophir Frieder</author>
<author>David Grossman</author>
<author>Mary Catherine McCabe</author>
</authors>
<title>Collection Statistics for Fast Duplicate Document Detection.</title>
<date>2002</date>
<journal>ACM Trans. on Information Systems,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>171--191</pages>
<marker>Chowdhury, Frieder, Grossman, McCabe, 2002</marker>
<rawString>Abdur Chowdhury, Ophir Frieder, David Grossman, and Mary Catherine McCabe. 2002. Collection Statistics for Fast Duplicate Document Detection. ACM Trans. on Information Systems, 20(2), 171-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
<author>Steven C Glassman</author>
<author>Mark S Manasse</author>
<author>Geoffrey Zweig</author>
</authors>
<date>1997</date>
<booktitle>Syntactic Clustering of the Web. Proc. of the Sixth International World Wide Web Conference,</booktitle>
<pages>391--404</pages>
<marker>Broder, Glassman, Manasse, Zweig, 1997</marker>
<rawString>Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. 1997. Syntactic Clustering of the Web. Proc. of the Sixth International World Wide Web Conference, 391-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Garcia-Molina</author>
<author>Luis Gravano</author>
<author>Narayanan Shivakumar</author>
</authors>
<title>dSCAM: Finding Document Copies Across Multiple Databases.</title>
<date>1996</date>
<booktitle>Proc. of Fourth International Conference on Parallel and Distributed Information System,</booktitle>
<pages>68--79</pages>
<marker>Garcia-Molina, Gravano, Shivakumar, 1996</marker>
<rawString>Hector Garcia-Molina, Luis Gravano, and Narayanan Shivakumar. 1996. dSCAM: Finding Document Copies Across Multiple Databases. Proc. of Fourth International Conference on Parallel and Distributed Information System, 68-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
</authors>
<title>Duplicate Detection in the Reuters Collection.</title>
<date>1997</date>
<tech>Technical Report of the</tech>
<pages>1997--5</pages>
<institution>Department of Computing Science at the University of Glasgow,</institution>
<contexts>
<context position="14671" citStr="Sanderson, 1997" startWordPosition="2495" endWordPosition="2496"> 0.6 0.4 0.2 0 10 100 1000 10000 1 0.8 0.6 0.4 0.2 0 10 100 1000 10000 1 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1 0.8 0.6 0.4 0.2 1 0 10 100 1000 10000 0 10 100 1000 10000 0 10 100 1000 10000 1 0.8 0.6 0.4 0.2 0 10 100 1000 10000 1 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1 0 10 100 1000 10000 0 10 100 1000 10000 0 5 25 �3 0 10 15 20 0 200 �3 00 400 �5 00 �6 00 100 1000 100 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 tection (Broder et.al., 1997; Chowdhury, et.al., 2002). The second is similarity measure calculation where the term distribution similarity is used to detect potential duplicates (Molina, et.al, 1996; Sanderson, 1997). Although most studies allow minor syntactic variations, the duplication is detected for entire documents or Web sites. Because the proposed scheme is focused on partial duplications, it could be used as a complementary measure to improve the flexibility of the duplication check. (3) Document clustering There also exist studies that generate clusters based on phrases shared between documents. Suffix Tree Clustering (STC), proposed for onthe-fly reorganization of the search results on the Web, is an example close to our approach (Zamir Etzioni, 1998). Although both STC and our methods exploit </context>
</contexts>
<marker>Sanderson, 1997</marker>
<rawString>Mark Sanderson. 1997. Duplicate Detection in the Reuters Collection. Technical Report of the Department of Computing Science at the University of Glasgow, TR-1997-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Zamir</author>
<author>Oren Etzioni</author>
</authors>
<title>Web Document Clustering: A Feasibility Demonstration.</title>
<date>1998</date>
<booktitle>Proc. of SIGIR&apos;98,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="5846" citStr="Zamir &amp; Etzioni (1998)" startWordPosition="953" endWordPosition="956">-cluster is defined as a pair (S, D) such that S is a subset of n-grams, D is a subset of documents, and Vs c S, Sd(s) = D, Vs S, sd(s) D. For example, in Figure 1, nodes A and B both refer to subset { DOC#10, DOC#13 } and are therefore merged into a single ST-cluster. &lt;maintain&gt; DOC#1, 3, 5, 7 8, 10, 13, ... &lt;substantic &lt;ec&apos;l°rnic&gt;&lt;gr°wth&gt; D0C#8, 10.13 DOC#3, 5.10. 20 13, 28, 35 oar re, ign:t cc ah prt10(&gt;g y 0 DOOM 0, 13 &lt;post&gt;&lt;nine&gt; DOC#10, 13 level 11 Figure 1: Example of a ST-cluster 2.2 Procedure for ST-Clustering The basic procedure for extracting ST-clusters is similar to that used by Zamir &amp; Etzioni (1998) and is summarized as follows: (1) Convert the target collection into sequences of terms, each of which corresponds to a single document. Apply morphological analysis or other pre-processing methods when it is necessary to determine the word boundaries. Neither stemming nor normalization is applied. (2) Generate a suffix array by a single sort. Suffix tree nodes, together with their corresponding document subset lists, are then &lt;attracted&gt; DOC#8 10.13 20,33 34 Df&gt;&lt;over&gt;&lt;eight&gt; &lt;percent&gt;&lt;for&gt;&lt;the&gt; D0C#10, 13, 28 level 6 Suffix Tree Cluster = nodes 0 + ® term sequences &amp;quot;maintain economic growth </context>
</contexts>
<marker>Zamir, Etzioni, 1998</marker>
<rawString>Oren Zamir and Oren Etzioni. 1998. Web Document Clustering: A Feasibility Demonstration. Proc. of SIGIR&apos;98, 46-54.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>