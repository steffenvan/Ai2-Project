<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001277">
<note confidence="0.9514495">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 199-206.
</note>
<title confidence="0.991254">
Entropy Rate Constancy in Text
</title>
<author confidence="0.97759">
Dmitriy Genzel and Eugene Charniak
</author>
<affiliation confidence="0.981669333333333">
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University
</affiliation>
<address confidence="0.97267">
Providence, RI, USA, 02912
</address>
<email confidence="0.999418">
{dg,ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983818181818">
We present a constancy rate princi-
ple governing language generation. We
show that this principle implies that lo-
cal measures of entropy (ignoring con-
text) should increase with the sentence
number. We demonstrate that this is
indeed the case by measuring entropy
in three different ways. We also show
that this effect has both lexical (which
words are used) and non-lexical (how
the words are used) causes.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966033333333">
It is well-known from Information Theory that
the most efficient way to send information
through noisy channels is at a constant rate. If
humans try to communicate in the most efficient
way, then they must obey this principle. The
communication medium we examine in this pa-
per is text, and we present some evidence that
this principle holds here.
Entropy is a measure of information first pro-
posed by Shannon (1948). Informally, entropy
of a random variable is proportional to the diffi-
culty of correctly guessing the value of this vari-
able (when the distribution is known). Entropy
is the highest when all values are equally prob-
able, and is lowest (equal to 0) when one of the
choices has probability of 1, i.e. deterministi-
cally known in advance.
In this paper we are concerned with entropy
of English as exhibited through written text,
though these results can easily be extended to
speech as well. The random variable we deal
with is therefore a unit of text (a word, for our
purposes1) that a random person who has pro-
duced all the previous words in the text stream
is likely to produce next. We have as many ran-
dom variables as we have words in a text. The
distributions of these variables are obviously dif-
ferent and depend on all previous words pro-
duced. We claim, however, that the entropy of
these random variables is on average the same2.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998088375">
There has been work in the speech community
inspired by this constancy rate principle. In
speech, distortion of the audio signal is an extra
source of uncertainty, and this principle can by
applied in the following way:
A given word in one speech context might be
common, while in another context it might be
rare. To keep the entropy rate constant over
time, it would be necessary to take more time
(i.e., pronounce more carefully) in less common
situations. Aylett (1999) shows that this is in-
deed the case.
It has also been suggested that the principle
of constant entropy rate agrees with biological
evidence of how human language processing has
evolved (Plotkin and Nowak, 2000).
</bodyText>
<footnote confidence="0.851852666666667">
Kontoyiannis (1996) also reports results on 5
consecutive blocks of characters from the works
1It may seem like an arbitrary choice, but a word is a
natural unit of length, after all when one is asked to give
the length of an essay one typically chooses the number
of words as a measure.
2Strictly speaking, we want the cross-entropy between
all words in the sentences number n and the true model
of English to be the same for all n.
</footnote>
<bodyText confidence="0.980622">
of Jane Austen which are in agreement with our
principle and, in particular, with its corollary as
derived in the following section.
</bodyText>
<sectionHeader confidence="0.977447" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.98559295">
Let {Xi}, i = 1... n be a sequence of random
variables, with Xi corresponding to word wi in
the corpus. Let us consider i to be fixed. The
random variable we are interested in is Yi, a ran-
dom variable that has the same distribution as
Xi|X1 = w1, ... , Xi−1 = wi−1 for some fixed
words w1 ... wi−1. For each word wi there will
be some word wj, (j &lt; i) which is the start-
ing word of the sentence wi belongs to. We will
combine random variables X1 ... Xi−1 into two
sets. The first, which we call Ci (for context),
contains X1 through Xj−1, i.e. all the words
from the preceding sentences. The remaining
set, which we call Li (for local), will contain
words Xj through Xi−1 . Both Li and Ci could
be empty sets. We can now write our variable
Yi as Xi|Ci, Li.
Our claim is that the entropy of Yi , H(Yi)
stays constant for all i. By the definition of rel-
ative mutual information between Xi and Ci,
</bodyText>
<equation confidence="0.998317">
H(Yi) = H(Xi|Ci,Li)
= H(Xi|Li) − I(Xi|Ci,Li)
</equation>
<bodyText confidence="0.969250090909091">
where the last term is the mutual information
between the word and context given the sen-
tence. As i increases, so does the set Ci. Li, on
the other hand, increases until we reach the end
of the sentence, and then becomes small again.
Intuitively, we expect the mutual information
at, say, word k of each sentence (where Li has
the same size for all i) to increase as the sen-
tence number is increasing. By our hypothesis
we then expect H(Xi|Li) to increase with the
sentence number as well.
Current techniques are not very good at es-
timating H(Yi), because we do not have a
very good model of context, since this model
must be mostly semantic in nature. We have
shown, however, that if we can instead estimate
H(Xi|Li) and show that it increases with the
sentence number, we will provide evidence to
support the constancy rate principle.
The latter expression is much easier to esti-
mate, because it involves only words from the
beginning of the sentence whose relationship
is largely local and can be successfully cap-
tured through something as simple as an n-gram
model.
We are only interested in the mean value of
the H(Xj|Lj) for wj E Si, where Si is the ith
sentence. This number is equal to  |1 H(Si),
Si|
which reduces the problem to the one of esti-
mating the entropy of a sentence.
We use three different ways to estimate the
entropy:
</bodyText>
<listItem confidence="0.9622305">
• Estimate H(Si) using an n-gram probabilis-
tic model
• Estimate H(Si) using a probabilistic model
induced by a statistical parser
• Estimate H(Xi) directly, using a non-para-
metric estimator. We estimate the entropy
for the beginning of each sentence. This
approach estimates H(Xi), not H(Xi|Li),
i.e. ignores not only the context, but also
the local syntactic information.
</listItem>
<sectionHeader confidence="0.999462" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.997149">
4.1 N-gram
</subsectionHeader>
<bodyText confidence="0.9998034">
N-gram models make the simplifying assump-
tion that the current word depends on a con-
stant number of the preceding words (we use
three). The probability model for sentence S
thus looks as follows:
</bodyText>
<equation confidence="0.99816">
P(S) = P(w1)P(w2|w1)P(w3|w2w1)
n
× 11 P(wn|wn−1wn−2wn−3)
i=4
</equation>
<bodyText confidence="0.999977">
To estimate the entropy of the sentence S, we
compute log P(S). This is in fact an estimate of
cross entropy between our model and true distri-
bution. Thus we are overestimating the entropy,
but if we assume that the overestimation error is
more or less uniform, we should still see our esti-
mate increase as the sentence number increases.
Penn Treebank corpus (Marcus et al., 1993)
sections 0-20 were used for training, sections 21-
24 for testing. Each article was treated as a sep-
arate text, results for each sentence number were
grouped together, and the mean value reported
on Figure 1 (dashed line). Since most articles
are short, there are fewer sentences available for
larger sentence numbers, thus results for large
sentence numbers are less reliable.
The trend is fairly obvious, especially for
small sentence numbers: sentences (with no con-
text used) get harder as sentence number in-
creases, i.e. the probability of the sentence given
the model decreases.
</bodyText>
<subsectionHeader confidence="0.717811">
4.2 Parser Model
</subsectionHeader>
<bodyText confidence="0.99941175">
We also computed the log-likelihood of the sen-
tence using a statistical parser described in
Charniak (2001)3. The probability model for
sentence S with parse tree T is (roughly):
</bodyText>
<equation confidence="0.9952105">
P(S) = � P (x|parents(x))
xET
</equation>
<bodyText confidence="0.999992">
where parents(x) are words which are parents
of node x in the the tree T. This model takes
into account syntactic information present in
the sentence which the previous model does not.
The entropy estimate is again log P(S). Overall,
these estimates are lower (closer to the true en-
tropy) in this model because the model is closer
to the true probability distribution. The same
corpus, training and testing sets were used. The
results are reported on Figure 1 (solid line). The
estimates are lower (better), but follow the same
trend as the n-gram estimates.
</bodyText>
<subsectionHeader confidence="0.994202">
4.3 Non-parametric Estimator
</subsectionHeader>
<bodyText confidence="0.993381714285714">
Finally we compute the entropy using the esti-
mator described in (Kontoyiannis et al., 1998).
The estimation is done as follows. Let T be our
training corpus. Let S = {w1 ... wn} be the test
sentence. We find the largest k &lt; n, such that
sequence of words w1 ... wk occurs in T. Then
log S is an estimate of the entropy at the word
</bodyText>
<equation confidence="0.41536">
k
</equation>
<bodyText confidence="0.873641333333333">
w1. We compute such estimates for many first
sentences, second sentences, etc., and take the
average.
</bodyText>
<footnote confidence="0.461256">
3This parser does not proceed in a strictly left-to-right
fashion, but this is not very important since we estimate
entropy for the whole sentence, rather than individual
words
</footnote>
<bodyText confidence="0.999916545454545">
For this experiment we used 3 million words of
the Wall Street Journal (year 1988) as the train-
ing set and 23 million words (full year 1987) as
the testing set4. The results are shown on Fig-
ure 2. They demonstrate the expected behavior,
except for the strong abnormality on the second
sentence. This abnormality is probably corpus-
specific. For example, 1.5% of the second sen-
tences in this corpus start with words “the terms
were not disclosed”, which makes such sentences
easy to predict and decreases entropy.
</bodyText>
<subsectionHeader confidence="0.999042">
4.4 Causes of Entropy Increase
</subsectionHeader>
<bodyText confidence="0.999965566666667">
We have shown that the entropy of a sentence
(taken without context) tends to increase with
the sentence number. We now examine the
causes of this effect.
These causes may be split into two categories:
lexical (which words are used) and non-lexical
(how the words are used). If the effects are en-
tirely lexical, we would expect the per-word en-
tropy of the closed-class words not to increase
with sentence number, since presumably the
same set of words gets used in each sentence.
For this experiment we use our n-gram estima-
tor as described in Section 4.2. We evaluate
the per-word entropy for nouns, verbs, deter-
miners, and prepositions. The results are given
in Figure 3 (solid lines). The results indicate
that entropy of the closed class words increases
with sentence number, which presumably means
that non-lexical effects (e.g. usage) are present.
We also want to check for presence of lexical
effects. It has been shown by Kuhn and Mohri
(1990) that lexical effects can be easily captured
by caching. In its simplest form, caching in-
volves keeping track of words occurring in the
previous sentences and assigning for each word
w a caching probability Pc(w) = C(C(w), where
E.C(w) is the number of times w occurs in the
previous sentences. This probability is then
mixed with the regular probability (in our case
- smoothed trigram) as follows:
</bodyText>
<equation confidence="0.985686">
Pmixed(w) = (1 − A)Pngram(w) + APc(w)
</equation>
<footnote confidence="0.953912666666667">
4This is not the same training set as the one used in
two previous experiments. For this experiment we needed
a larger, but similar data set
</footnote>
<figure confidence="0.9955575">
0 5 10 15 20 25
sentence number
</figure>
<figureCaption confidence="0.995144">
Figure 1: N-gram and parser estimates of entropy (in bits per word)
</figureCaption>
<figure confidence="0.9964528125">
8.4
8.2
8
7.2
7
6.8
7.8
entropy estimate
7.6
7.4
parser
n−gram
9
8
0 5 10 15 20 25
sentence number
</figure>
<figureCaption confidence="0.659165">
Figure 2: Non-parametric estimate of entropy
</figureCaption>
<figure confidence="0.9235283">
entropy estimate
8.9
8.8
8.7
8.6
8.5
8.4
8.3
8.2
8.1
</figure>
<bodyText confidence="0.999983421052632">
where A was picked to be 0.1. This new prob-
ability model is known to have lower entropy.
More complex caching techniques are possible
(Goodman, 2001), but are not necessary for this
experiment.
Thus, if lexical effects are present, we expect
the model that uses caching to provide lower
entropy estimates. The results are given in Fig-
ure 3 (dashed lines). We can see that caching
gives a significant improvement for nouns and a
small one for verbs, and gives no improvement
for the closed-class parts of speech. This shows
that lexical effects are present for the open-class
parts of speech and (as we assumed in the previ-
ous experiment) are absent for the closed-class
parts of speech. Since we have proven the pres-
ence of the non-lexical effects in the previous
experiment, we can see that both lexical and
non-lexical effects are present.
</bodyText>
<sectionHeader confidence="0.965214" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999980078947368">
We have proposed a fundamental principle of
language generation, namely the entropy rate
constancy principle. We have shown that en-
tropy of the sentences taken without context in-
creases with the sentence number, which is in
agreement with the above principle. We have
also examined the causes of this increase and
shown that they are both lexical (primarily for
open-class parts of speech) and non-lexical.
These results are interesting in their own
right, and may have practical implications as
well. In particular, they suggest that language
modeling may be a fruitful way to approach is-
sues of contextual influence in text.
Of course, to some degree language-modeling
caching work has always recognized this, but
this is rather a crude use of context and does
not address the issues which one normally thinks
of when talking about context. We have seen,
however, that entropy measurements can pick
up much more subtle influences, as evidenced
by the results for determiners and prepositions
where we see no caching influence at all, but nev-
ertheless observe increasing entropy as a func-
tion of sentence number. This suggests that
such measurements may be able to pick up more
obviously semantic contextual influences than
simply the repeating words captured by caching
models. For example, sentences will differ in
how much useful contextual information they
carry. Are there useful generalizations to be
made? E.g., might the previous sentence always
be the most useful, or, perhaps, for newspa-
per articles, the first sentence? Can these mea-
surements detect such already established con-
textual relations as the given-new distinction?
What about other pragmatic relations? All of
these deserve further study.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999908777777778">
We would like to acknowledge the members of
the Brown Laboratory for Linguistic Informa-
tion Processing and particularly Mark Johnson
for many useful discussions. Also thanks to
Daniel Jurafsky who early on suggested the in-
terpretation of our data that we present here.
This research has been supported in part by
NSF grants IIS 0085940, IIS 0112435, and DGE
9870676.
</bodyText>
<sectionHeader confidence="0.9977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99348452173913">
M. P. Aylett. 1999. Stochastic suprasegmentals: Re-
lationships between redundancy, prosodic struc-
ture and syllabic duration. In Proceedings of
ICPhS–99, San Francisco.
E. Charniak. 2001. A maximum-entropy-inspired
parser. In Proceedings of ACL–2001, Toulouse.
J. T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15:403–434.
I. Kontoyiannis, P. H. Algoet, Yu. M. Suhov, and
A.J. Wyner. 1998. Nonparametric entropy esti-
mation for stationary processes and random fields,
with applications to English text. IEEE Trans.
Inform. Theory, 44:1319–1327, May.
I. Kontoyiannis. 1996. The complexity and en-
tropy of literary styles. NSF Technical Report No.
97, Department of Statistics, Stanford University,
June. [unpublished, can be found at the author’s
web page].
R. Kuhn and R. De Mori. 1990. A cache-based
natural language model for speech reproduction.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12(6):570–583.
</reference>
<figure confidence="0.997666">
Nouns
9
8.5
8
9.5
normal
caching
2 4 6 8 10
Prepositions
4.8
4.6
5.4
5.2
5
normal
caching
2 4 6 8 10
Verbs
11
normal
caching
10.5
10
9.5
2 4 6 8 10
Determiners
4.4
4.3
4.2
4.1
3.9
3.8
3.7
4
normal
caching
2 4 6 8 10
</figure>
<figureCaption confidence="0.997993">
Figure 3: Comparing Parts of Speech
</figureCaption>
<reference confidence="0.9990729">
M. P. Marcus, B. Santorini, and M. A. Marcin-
kiewicz. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19:313–330.
J. B. Plotkin and M. A. Nowak. 2000. Language
evolution and information theory. Journal of The-
oretical Biology, pages 147–159.
C. E. Shannon. 1948. A mathematical theory of
communication. The Bell System Technical Jour-
nal, 27:379–423, 623–656, July, October.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799972">
<note confidence="0.998264">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 199-206.</note>
<title confidence="0.983327">Entropy Rate Constancy in Text</title>
<author confidence="0.851313">Genzel Charniak</author>
<affiliation confidence="0.954090333333333">Brown Laboratory for Linguistic Information Processing Department of Computer Science Brown University</affiliation>
<address confidence="0.999449">Providence, RI, USA, 02912</address>
<abstract confidence="0.999253916666667">present a rate princilanguage generation. We show that this principle implies that local measures of entropy (ignoring context) should increase with the sentence number. We demonstrate that this is indeed the case by measuring entropy in three different ways. We also show this effect has both lexical are used) and non-lexical the words are used) causes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M P Aylett</author>
</authors>
<title>Stochastic suprasegmentals: Relationships between redundancy, prosodic structure and syllabic duration.</title>
<date>1999</date>
<booktitle>In Proceedings of ICPhS–99,</booktitle>
<location>San Francisco.</location>
<contexts>
<context position="2619" citStr="Aylett (1999)" startWordPosition="436" endWordPosition="437">epend on all previous words produced. We claim, however, that the entropy of these random variables is on average the same2. 2 Related Work There has been work in the speech community inspired by this constancy rate principle. In speech, distortion of the audio signal is an extra source of uncertainty, and this principle can by applied in the following way: A given word in one speech context might be common, while in another context it might be rare. To keep the entropy rate constant over time, it would be necessary to take more time (i.e., pronounce more carefully) in less common situations. Aylett (1999) shows that this is indeed the case. It has also been suggested that the principle of constant entropy rate agrees with biological evidence of how human language processing has evolved (Plotkin and Nowak, 2000). Kontoyiannis (1996) also reports results on 5 consecutive blocks of characters from the works 1It may seem like an arbitrary choice, but a word is a natural unit of length, after all when one is asked to give the length of an essay one typically chooses the number of words as a measure. 2Strictly speaking, we want the cross-entropy between all words in the sentences number n and the tr</context>
</contexts>
<marker>Aylett, 1999</marker>
<rawString>M. P. Aylett. 1999. Stochastic suprasegmentals: Relationships between redundancy, prosodic structure and syllabic duration. In Proceedings of ICPhS–99, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL–2001,</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="7439" citStr="Charniak (2001)" startWordPosition="1289" endWordPosition="1290">s a separate text, results for each sentence number were grouped together, and the mean value reported on Figure 1 (dashed line). Since most articles are short, there are fewer sentences available for larger sentence numbers, thus results for large sentence numbers are less reliable. The trend is fairly obvious, especially for small sentence numbers: sentences (with no context used) get harder as sentence number increases, i.e. the probability of the sentence given the model decreases. 4.2 Parser Model We also computed the log-likelihood of the sentence using a statistical parser described in Charniak (2001)3. The probability model for sentence S with parse tree T is (roughly): P(S) = � P (x|parents(x)) xET where parents(x) are words which are parents of node x in the the tree T. This model takes into account syntactic information present in the sentence which the previous model does not. The entropy estimate is again log P(S). Overall, these estimates are lower (closer to the true entropy) in this model because the model is closer to the true probability distribution. The same corpus, training and testing sets were used. The results are reported on Figure 1 (solid line). The estimates are lower </context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. A maximum-entropy-inspired parser. In Proceedings of ACL–2001, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<pages>15--403</pages>
<contexts>
<context position="11260" citStr="Goodman, 2001" startWordPosition="1957" endWordPosition="1958"> (1 − A)Pngram(w) + APc(w) 4This is not the same training set as the one used in two previous experiments. For this experiment we needed a larger, but similar data set 0 5 10 15 20 25 sentence number Figure 1: N-gram and parser estimates of entropy (in bits per word) 8.4 8.2 8 7.2 7 6.8 7.8 entropy estimate 7.6 7.4 parser n−gram 9 8 0 5 10 15 20 25 sentence number Figure 2: Non-parametric estimate of entropy entropy estimate 8.9 8.8 8.7 8.6 8.5 8.4 8.3 8.2 8.1 where A was picked to be 0.1. This new probability model is known to have lower entropy. More complex caching techniques are possible (Goodman, 2001), but are not necessary for this experiment. Thus, if lexical effects are present, we expect the model that uses caching to provide lower entropy estimates. The results are given in Figure 3 (dashed lines). We can see that caching gives a significant improvement for nouns and a small one for verbs, and gives no improvement for the closed-class parts of speech. This shows that lexical effects are present for the open-class parts of speech and (as we assumed in the previous experiment) are absent for the closed-class parts of speech. Since we have proven the presence of the non-lexical effects i</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. T. Goodman. 2001. A bit of progress in language modeling. Computer Speech and Language, 15:403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Suhov</author>
<author>A J Wyner</author>
</authors>
<title>Nonparametric entropy estimation for stationary processes and random fields, with applications to English text.</title>
<date>1998</date>
<journal>IEEE Trans. Inform. Theory,</journal>
<pages>44--1319</pages>
<marker>Suhov, Wyner, 1998</marker>
<rawString>I. Kontoyiannis, P. H. Algoet, Yu. M. Suhov, and A.J. Wyner. 1998. Nonparametric entropy estimation for stationary processes and random fields, with applications to English text. IEEE Trans. Inform. Theory, 44:1319–1327, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kontoyiannis</author>
</authors>
<title>The complexity and entropy of literary styles.</title>
<date>1996</date>
<tech>NSF Technical Report No. 97,</tech>
<institution>Department of Statistics, Stanford University,</institution>
<note>unpublished, can be found</note>
<contexts>
<context position="2850" citStr="Kontoyiannis (1996)" startWordPosition="473" endWordPosition="474">. In speech, distortion of the audio signal is an extra source of uncertainty, and this principle can by applied in the following way: A given word in one speech context might be common, while in another context it might be rare. To keep the entropy rate constant over time, it would be necessary to take more time (i.e., pronounce more carefully) in less common situations. Aylett (1999) shows that this is indeed the case. It has also been suggested that the principle of constant entropy rate agrees with biological evidence of how human language processing has evolved (Plotkin and Nowak, 2000). Kontoyiannis (1996) also reports results on 5 consecutive blocks of characters from the works 1It may seem like an arbitrary choice, but a word is a natural unit of length, after all when one is asked to give the length of an essay one typically chooses the number of words as a measure. 2Strictly speaking, we want the cross-entropy between all words in the sentences number n and the true model of English to be the same for all n. of Jane Austen which are in agreement with our principle and, in particular, with its corollary as derived in the following section. 3 Problem Formulation Let {Xi}, i = 1... n be a sequ</context>
</contexts>
<marker>Kontoyiannis, 1996</marker>
<rawString>I. Kontoyiannis. 1996. The complexity and entropy of literary styles. NSF Technical Report No. 97, Department of Statistics, Stanford University, June. [unpublished, can be found at the author’s web page].</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>A cache-based natural language model for speech reproduction.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>R. Kuhn and R. De Mori. 1990. A cache-based natural language model for speech reproduction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6):570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--313</pages>
<contexts>
<context position="6730" citStr="Marcus et al., 1993" startWordPosition="1173" endWordPosition="1176">e the simplifying assumption that the current word depends on a constant number of the preceding words (we use three). The probability model for sentence S thus looks as follows: P(S) = P(w1)P(w2|w1)P(w3|w2w1) n × 11 P(wn|wn−1wn−2wn−3) i=4 To estimate the entropy of the sentence S, we compute log P(S). This is in fact an estimate of cross entropy between our model and true distribution. Thus we are overestimating the entropy, but if we assume that the overestimation error is more or less uniform, we should still see our estimate increase as the sentence number increases. Penn Treebank corpus (Marcus et al., 1993) sections 0-20 were used for training, sections 21- 24 for testing. Each article was treated as a separate text, results for each sentence number were grouped together, and the mean value reported on Figure 1 (dashed line). Since most articles are short, there are fewer sentences available for larger sentence numbers, thus results for large sentence numbers are less reliable. The trend is fairly obvious, especially for small sentence numbers: sentences (with no context used) get harder as sentence number increases, i.e. the probability of the sentence given the model decreases. 4.2 Parser Mode</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Plotkin</author>
<author>M A Nowak</author>
</authors>
<title>Language evolution and information theory.</title>
<date>2000</date>
<journal>Journal of Theoretical Biology,</journal>
<pages>147--159</pages>
<contexts>
<context position="2829" citStr="Plotkin and Nowak, 2000" startWordPosition="469" endWordPosition="472">s constancy rate principle. In speech, distortion of the audio signal is an extra source of uncertainty, and this principle can by applied in the following way: A given word in one speech context might be common, while in another context it might be rare. To keep the entropy rate constant over time, it would be necessary to take more time (i.e., pronounce more carefully) in less common situations. Aylett (1999) shows that this is indeed the case. It has also been suggested that the principle of constant entropy rate agrees with biological evidence of how human language processing has evolved (Plotkin and Nowak, 2000). Kontoyiannis (1996) also reports results on 5 consecutive blocks of characters from the works 1It may seem like an arbitrary choice, but a word is a natural unit of length, after all when one is asked to give the length of an essay one typically chooses the number of words as a measure. 2Strictly speaking, we want the cross-entropy between all words in the sentences number n and the true model of English to be the same for all n. of Jane Austen which are in agreement with our principle and, in particular, with its corollary as derived in the following section. 3 Problem Formulation Let {Xi},</context>
</contexts>
<marker>Plotkin, Nowak, 2000</marker>
<rawString>J. B. Plotkin and M. A. Nowak. 2000. Language evolution and information theory. Journal of Theoretical Biology, pages 147–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>The Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="1195" citStr="Shannon (1948)" startWordPosition="184" endWordPosition="185">is indeed the case by measuring entropy in three different ways. We also show that this effect has both lexical (which words are used) and non-lexical (how the words are used) causes. 1 Introduction It is well-known from Information Theory that the most efficient way to send information through noisy channels is at a constant rate. If humans try to communicate in the most efficient way, then they must obey this principle. The communication medium we examine in this paper is text, and we present some evidence that this principle holds here. Entropy is a measure of information first proposed by Shannon (1948). Informally, entropy of a random variable is proportional to the difficulty of correctly guessing the value of this variable (when the distribution is known). Entropy is the highest when all values are equally probable, and is lowest (equal to 0) when one of the choices has probability of 1, i.e. deterministically known in advance. In this paper we are concerned with entropy of English as exhibited through written text, though these results can easily be extended to speech as well. The random variable we deal with is therefore a unit of text (a word, for our purposes1) that a random person wh</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. E. Shannon. 1948. A mathematical theory of communication. The Bell System Technical Journal, 27:379–423, 623–656, July, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>