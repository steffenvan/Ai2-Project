<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9830395">
Tagging English Text with a Probabilistic
Model
</title>
<author confidence="0.961481">
Bernard Merialdo*t
</author>
<bodyText confidence="0.869556428571429">
Institut EURECOM
In this paper we present some experiments on the use of a probabilistic model to tag English text,
i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main
novelty of these experiments is the use of untagged text in the training of the model. We have
used a simple triclass Markov model and are looking for the best way to estimate the parameters
of this model, depending on the kind and amount of training data provided. Two approaches in
particular are compared and combined:
</bodyText>
<listItem confidence="0.862788">
• using text that has been tagged by hand and computing relative frequency counts,
• using text without tags and training the model as a hidden Markov process,
according to a Maximum Likelihood principle.
</listItem>
<bodyText confidence="0.898456">
Experiments show that the best training is obtained by using as much tagged text as possible. They
also show that Maximum Likelihood training, the procedure that is routinely used to estimate
hidden Markov models parameters from training data, will not necessarily improve the tagging
accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of
hand-tagged text is available.
</bodyText>
<sectionHeader confidence="0.992125" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999475666666667">
A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning
to each word the correct tag (part of speech) in the context of the sentence. Two main
approaches have generally been considered:
</bodyText>
<listItem confidence="0.97812325">
• rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and
Martin 1992; Brill et al. 1990)
• probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and
Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983;
</listItem>
<bodyText confidence="0.8328335">
Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988;
Marcken 1990; Merialdo 1991; Cutting et al. 1992).
More recently, some work has been proposed using neural networks (Benello,
Mackie, and Anderson 1989; Nakamura and Shikano 1989).
</bodyText>
<footnote confidence="0.828234">
* Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904
Valbonne Cedex France; merialdo@eurecom.fr.
</footnote>
<note confidence="0.9479995">
This work was carried out while the author was a visitor of the Continuous Speech Recognition group,
IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this
work has been presented at the IEEE International Conference on Acoustics, Speech and Signal Processing,
Toronto (Canada), May 1991.
© 1994 Association for Computational Linguistics
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.984311">
Through these different approaches, some common points have emerged:
</bodyText>
<listItem confidence="0.992806">
• For any given word, only a few tags are possible, a list of which can be
found either in the dictionary or through a morphological analysis of the
word.
• When a word has several possible tags, the correct tag can generally be
chosen from the local context, using contextual rules that define the valid
sequences of tags. These rules may be given priorities so that a selection
can be made even when several rules apply.
</listItem>
<bodyText confidence="0.598745">
These kinds of considerations fit nicely inside a probabilistic formulation of the
problem (Beale 1985; Garside and Leech 1985), which offers the following advantages:
</bodyText>
<listItem confidence="0.99988975">
• a sound theoretical framework is provided
• the approximations are clear
• the probabilities provide a straightforward way to disambiguate
• the probabilities can be estimated automatically from data.
</listItem>
<bodyText confidence="0.9985646">
In this paper we present a particular probabilistic model, the triclass model, and
results from experiments involving different ways to estimate its parameters, with the
intention of maximizing the ability of the model to tag text accurately. In particular,
we are interested in a way to make the best use of untagged text in the training of the
model.
</bodyText>
<sectionHeader confidence="0.989663" genericHeader="method">
2. The Problem of Tagging
</sectionHeader>
<bodyText confidence="0.946495272727273">
We suppose that the user has defined a set of tags (attached to words). Consider a
sentence W = w1w2 wn, and a sequence of tags T =-- tit2 tn, of the same length.
We call the pair (W, T) an alignment. We say that word w, has been assigned the tag t,
in this alignment.
We assume that the tags have some linguistic meaning for the user, so that among
all possible alignments for a sentence there is a single one that is correct from a
grammatical point of view.
A tagging procedure is a procedure 0 that selects a sequence of tags (and so defines
an alignment) for each sentence.
0:W--+T=0(W)
There are (at least) two measures for the quality of a tagging procedure:
</bodyText>
<listItem confidence="0.919825">
• at sentence level
perfs(0) percentage of sentences correctly tagged
• at word level
perfw(0) = percentage of words correctly tagged
</listItem>
<page confidence="0.995466">
156
</page>
<note confidence="0.46975">
Bernard Merialdo Tagging English Text with a Probabilistic Model
</note>
<bodyText confidence="0.9998168">
In practice, performance at sentence level is generally lower than performance at word
level, since all the words have to be tagged correctly for the sentence to be tagged
correctly.
The standard measure used in the literature is performance at word level, and this
is the one considered here.
</bodyText>
<sectionHeader confidence="0.80851" genericHeader="method">
3. Probabilistic Formulation
</sectionHeader>
<bodyText confidence="0.9131506">
In the probabilistic formulation of the tagging problem we assume that the alignments
are generated by a probabilistic model according to a probability distribution:
p(W, T)
In this case, depending on the criterion that we choose for evaluation, the optimal
tagging procedure is as follows:
</bodyText>
<listItem confidence="0.9953165">
• for evaluation at sentence level, choose the most probable sequence of
tags for the sentence
</listItem>
<equation confidence="0.696237">
0(w) _ argTmax p(Tiw) argTmax p(w,T)
</equation>
<bodyText confidence="0.834138">
We call this procedure Viterbi tagging. It is achieved using a dynamic
programming scheme.
</bodyText>
<listItem confidence="0.967041">
• for evaluation at word level, choose the most probable tag for each word
in the sentence
</listItem>
<equation confidence="0.931848333333333">
0(w)i argmt ax p(t. _ t/w) _ argmax
t 2_, P(W, T)
T:t,=t
</equation>
<bodyText confidence="0.999864">
where 0(W), is the tag assigned to word w, by the tagging procedure
in the context of the sentence W. We call this procedure Maximum
</bodyText>
<subsectionHeader confidence="0.66931">
Likelihood (ML) tagging.
</subsectionHeader>
<bodyText confidence="0.991617333333333">
It is interesting to note that the most commonly used method is Viterbi tagging
(see DeRose 1988; Church 1989) although it is not the optimal method for evaluation
at word level. The reasons for this preference are presumably that:
</bodyText>
<listItem confidence="0.995148875">
• Viterbi tagging is simpler to implement than ML tagging and requires
less computation (although they both have the same asymptotic
complexity)
• Viterbi tagging provides the best interpretation for the sentence, which is
linguistically appealing
• ML tagging may produce sequences of tags that are linguistically
impossible (because the choice of a tag depends on all contexts taken
together).
</listItem>
<bodyText confidence="0.9973645">
However, in our experiments, we will show that Viterbi and ML tagging result in very
similar performance.
</bodyText>
<page confidence="0.991449">
157
</page>
<note confidence="0.563575">
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.997015">
Of course, the real tags have not been generated by a probabilistic model and,
even if they had been, we would not be able to determine this model exactly be-
cause of practical limitations. Therefore the models that we construct will only be
approximations of an ideal model that does not exist. It so happens that despite these
assumptions and approximations, these models are still able to perform reasonably
well.
</bodyText>
<sectionHeader confidence="0.910522" genericHeader="method">
4. The Triclass Model
</sectionHeader>
<bodyText confidence="0.968473">
We have the mathematical expression:
</bodyText>
<equation confidence="0.742901">
p(WIT) =
</equation>
<bodyText confidence="0.923893">
The triclass (or tri-POS Perouault 19861, or tri-Ggram Kodogno et al. 19871, or
HK) model is based on the following approximations:
</bodyText>
<listItem confidence="0.98201975">
• The probability of the tag given the past depends only on the last two
tags
p(ti/witi • • • - h(ti/ti-2ti-i)
• The probability of the word given the past depends only on its tag
</listItem>
<equation confidence="0.744203">
P(wi/witi ...wi_iti_iti) = k(wi/ti)
</equation>
<bodyText confidence="0.67016025">
(the name HK model comes from the notation chosen for these probabilities).
In order to define the model completely we have to specify the values of all h and
k probabilities. If Nw is the size of the vocabulary and NT the number of different tags,
then there are:
</bodyText>
<listItem confidence="0.9764164">
• NT.NT.NT values for the h probabilities
• Nw.NT values for the k probabilities.
Also, since all probability distributions have to sum to one, there are:
• NT.NT equations to constrain the values for the h probabilities
• NT equations to constrain the values for the k probabilities.
</listItem>
<bodyText confidence="0.756144">
The total number of free parameters is then:
</bodyText>
<equation confidence="0.863482">
(Nw -1).NT + (NT -1).NT.NT.
</equation>
<bodyText confidence="0.999844166666667">
Note that this number grows only linearly with respect to the size of the vocabulary,
which makes this model attractive for vocabularies of a very large size.
The triclass model by itself allows any word to have any tag. However, if we
have a dictionary that specifies the list of possible tags for each word, we can use this
information to constrain the model: if t is not a valid tag for the word w, then we are
sure that
</bodyText>
<equation confidence="0.827141">
k(w/t) = 0.
</equation>
<bodyText confidence="0.9581205">
There are thus at most as many nonzero values for the k probabilities as there are
possible pairs (word, tag) allowed in the dictionary.
</bodyText>
<page confidence="0.990786">
158
</page>
<author confidence="0.332509">
Bernard Merialdo Tagging English Text with a Probabilistic Model
</author>
<listItem confidence="0.8722938">
5. Training the Triclass Model
We consider two different types of training:
• Relative Frequency (RF) training
• Maximum Likelihood (ML) training which is done via the
Forward-Backward (FB) algorithm.
</listItem>
<subsectionHeader confidence="0.998939">
5.1 Relative Frequency Training
</subsectionHeader>
<bodyText confidence="0.999971">
If we have some tagged text available we can compute the number of times N(w,t)
a given word w appears with the tag t, and the number of times N(ti, t2, t3) the se-
quence (t1, t2, t3) appears in this text. We can then estimate the probabilities h and k
by computing the relative frequencies of the corresponding events on this data:
</bodyText>
<equation confidence="0.999069333333333">
N(ti,t2, t3)
hrf(t3/ti, t2) =f (6/4, t2) = N(ti, t2)
krf (w I t) = f (w It) = N(w,t) N(t)
</equation>
<bodyText confidence="0.999770875">
These estimates assign a probability of zero to any sequence of tags that did not
occur in the training data. But such sequences may occur if we consider other texts.
A probability of zero for a sequence creates problems because any alignment that
contains this sequence will get a probability of zero. Therefore, it may happen that,
for some sequences of words, all alignments get a probability of zero and the model
becomes useless for such sentences.
To avoid this, we interpolate these distributions with uniform distributions, i.e.
:onsider the interpolated model defined by:
</bodyText>
<equation confidence="0.9918805">
hinter(t3/ti , t2) = A.hrf (t3 I , t2) + (1 – A) f (t3 I ti.,t2)
kinter (Wt) = A.k,f(w I t) + (1 –
</equation>
<bodyText confidence="0.569053">
where
</bodyText>
<equation confidence="0.998755">
hunif (t3/ti , t2) = —
/cum/. (w I t) –
</equation>
<bodyText confidence="0.955296">
number of words that have the tag t
The interpolation coefficient A is computed using the deleted interpolation algorithm
(Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one for
the interpolation on h, one for the interpolation on k). The value of this coefficient
is expected to increase if we increase the size of the training text, since the rela-
tive frequencies should be more reliable. This interpolation procedure is also called
&amp;quot;smoothing.&amp;quot;
Smoothing is performed as follows:
</bodyText>
<listItem confidence="0.9527915">
• Some quantity of tagged text from the training data is not used in the
computation of the relative frequencies. It is called the &amp;quot;held-out&amp;quot; data.
• The coefficient A is chosen to maximize the probability of emission of the
held-out data by the interpolated model.
</listItem>
<figure confidence="0.6051182">
1
NT
1
159
Computational Linguistics Volume 20, Number 2
</figure>
<listItem confidence="0.936193">
• This maximization can be performed by the standard Forward-Backward
(FB) or Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976;
Bahl, Jelinek, and Mercer 1983; Poritz 1988), by considering A and 1 — A
as the transition probabilities of a Markov model.
</listItem>
<bodyText confidence="0.999667857142857">
It can be noted that more complicated interpolation schemes are possible. For
example, different coefficients can be used depending on the count of (t1, t2), with the
intuition that relative frequencies can be trusted more when this count is high. Another
possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or
hrf (t3).
Smoothing can also be achieved with procedures other than interpolation. One
example is the &amp;quot;backing-off&amp;quot; strategy proposed by Katz (1987).
</bodyText>
<subsectionHeader confidence="0.999711">
5.2 Maximum Likelihood Training
</subsectionHeader>
<bodyText confidence="0.999749">
Using a triclass model M it is possible to compute the probability of any sequence of
words W according to this model:
</bodyText>
<equation confidence="0.999022">
pm(w) = EPA4(W,T)
T
</equation>
<bodyText confidence="0.9999575">
where the sum is taken over all possible alignments. The Maximum Likelihood (ML)
training finds the model M that maximizes the probability of the training text:
</bodyText>
<equation confidence="0.370409">
max TT pm( vv)
M Ai-
W
</equation>
<bodyText confidence="0.99997">
where the product is taken over all the sentences W in the training text. This is the
problem of training a hidden Markov model (it is hidden because the sequence of tags
is hidden). A well-known solution to this problem is the Forward-Backward (FB) or
Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer
1983), which iteratively constructs a sequence of models that improve the probability
of the training data.
The advantage of this approach is that it does not require any tagging of the text,
but makes the assumption that the correct model is the one in which tags are used to
best predict the word sequence.
</bodyText>
<sectionHeader confidence="0.652731" genericHeader="method">
6. Tagging Algorithms
</sectionHeader>
<bodyText confidence="0.999928857142857">
The Viterbi algorithm is easily implemented using a dynamic programming scheme
(Bellman 1957). The Maximum Likelihood algorithm appears more complex at first
glance, because it involves computing the sum of the probabilities of a large number
of alignments. However, in the case of a hidden Markov model, these computations
can be arranged in a way similar to the one used during the FB algorithm, so that the
overall amount of computation needed becomes linear in the length of the sentence
(Baum and Eagon 1967).
</bodyText>
<sectionHeader confidence="0.926718" genericHeader="evaluation">
7. Experiments
</sectionHeader>
<bodyText confidence="0.999883333333333">
The main objective of this paper is to compare RF and ML training. This is done in
Section 7.2. We also take advantage of the environment that we have set up to perform
other experiments, described in Section 7.3, that have some theoretical interest, but did
</bodyText>
<page confidence="0.987382">
160
</page>
<note confidence="0.303747">
Bernard Merialdo Tagging English Text with a Probabilistic Model
</note>
<tableCaption confidence="0.997163">
Table 1
</tableCaption>
<table confidence="0.9716774">
RF training on N sentences, Viterbi tagging.
Training data Interpolation Nb of errors `)/0 correct
(sentences) coefficient A (words) tags
0 .0 10498 77.0
100 .48 4568 90.0
2000 .77 2110 95.4
5000 .85 1744 96.2
10000 .90 1555 96.6
20000 .92 1419 96.9
all .94 1365 97.0
</table>
<bodyText confidence="0.98540025">
not bring any improvement in practice. One concerns the difference between Viterbi
and ML tagging, and the other concerns the use of constraints during training.
We shall begin by describing the textual data that we are using, before presenting
the different tagging experiments using these various training and tagging methods.
</bodyText>
<subsectionHeader confidence="0.999784">
7.1 Text Data
</subsectionHeader>
<bodyText confidence="0.9997871875">
We use the &amp;quot;treebank&amp;quot; data described in Beale (1988). It contains 42,186 sentences
(about one million words) from the Associated Press. These sentences have been tagged
manually at the Unit for Computer Research on the English Language (University of
Lancaster, U.K.), in collaboration with IBM U.K. (Winchester) and the IBM Speech
Recognition group in Yorktown Heights (USA). In fact, these sentences are not only
tagged but also parsed. However, we do not use the information contained in the
parse.
In the treebank 159 different tags are used. These tags were projected on a smaller
system of 76 tags designed by Evelyne Tzoukermann and Peter Brown (see Appendix).
The results quoted in this paper all refer to this smaller system.
We built a dictionary that indicates the list of possible tags for each word, by
taking all the words that occur in this text and, for each word, all the tags that are
assigned to it somewhere in the text. In some sense, this is an optimal dictionary for
this data, since a word will not have all its possible tags (in the language), but only
the tags that it actually had within the text.
We separated this data into two parts:
</bodyText>
<listItem confidence="0.9989595">
• a set of 40,186 tagged sentences, the training data, which is used to build
the models
• a set of 2,000 tagged sentences (45,583 words), the test data, which is
used to test the quality of the models.
</listItem>
<subsectionHeader confidence="0.8810755">
7.2 Basic Experiments
RF training, Viterbi tagging
</subsectionHeader>
<bodyText confidence="0.999023833333333">
In this experiment, we extracted N tagged sentences from the training data. We then
computed the relative frequencies on these sentences and built a &amp;quot;smoothed&amp;quot; model
using the procedure previously described. This model was then used to tag the 2,000
test sentences. We experimented with different values of N, for each of which we
indicate the value of the interpolation coefficient and the number and percentage of
correctly tagged words. Results are indicated in Table 1.
</bodyText>
<page confidence="0.989384">
161
</page>
<note confidence="0.73928">
Computational Linguistics Volume 20, Number 2
</note>
<bodyText confidence="0.999640166666667">
As expected, as the size of the training increases, the interpolation coefficient in-
creases and the quality of the tagging improves.
When N = 0, the model is made up of uniform distributions. In this case, all
alignments for a sentence are equally probable, so that the choice of the correct tag
is just a choice at random. However, the percentage of correct tags is relatively high
(more than three out of four) because:
</bodyText>
<listItem confidence="0.93083425">
• almost half of the words of the text have a single possible tag, so that no
mistake can be made on these words
• about a quarter of the words of the text have only two possible tags so
that, on the average, a random choice is correct every other time.
</listItem>
<bodyText confidence="0.985493736842105">
Note that this behavior is obviously very dependent on the system of tags that is used.
It can be noted that reasonable results are obtained quite rapidly. Using 2,000
tagged sentences (less than 50,000 words), the tagging error rate is already less than 5%.
Using 10 times as much data (20,000 tagged sentences) provides an improvement of
only 1.5%.
ML training, Viterbi tagging
In ML training we take all the training data available (40,186 sentences) but we only
use the word sequences, not the associated tags (except to compute the initial model,
as will be described later). This is possible since the FB algorithm is able to train the
model using the word sequence only.
In the first experiment we took the model made up of uniform distributions as the
initial one. The only constraints in this model came from the values k(w It) that were
set to zero when the tag t was not possible for the word w (as found in the dictionary).
We then ran the FB algorithm and evaluated the quality of the tagging. The results
are shown in Figure 1. (Perplexity is a measure of the average branching factor for
probabilistic models.)
This figure shows that ML training both improves the perplexity of the model and
reduces the tagging error rate. However, this error rate remains at a relatively high
level—higher than that obtained with a RF training on 100 tagged sentences.
Having shown that ML training is able to improve the uniform model, we then
wanted to know if it was also able to improve more accurate models. We therefore
took as the initial model each of the models obtained previously by RF training and,
for each one, performed ML training using all of the training word sequences. The
results are shown graphically in Figure 2 and numerically in Table 2.
These results show that, when we use few tagged data, the model obtained by
relative frequency is not very good and Maximum Likelihood training is able to im-
prove it. However, as the amount of tagged data increases, the models obtained by
Relative Frequency are more accurate and Maximum Likelihood training improves
on the initial iterations only, but after deteriorates. If we use more than 5,000 tagged
sentences, even the first iteration of ML training degrades the tagging. (This number
is of course dependent on both the particular system of tags and the kind of text used
in this experiment).
These results call for some comments. ML training is a theoretically sound pro-
cedure, and one that is routinely and successfully used in speech recognition to es-
timate the parameters of hidden Markov models that describe the relations between
sequences of phonemes and the speech signal. Although ML training is guaranteed
to improve perplexity, perplexity is not necessarily related to tagging accuracy, and
it is possible to improve one while degrading the other. Also, in the case of tagging,
</bodyText>
<page confidence="0.953829">
162
</page>
<figure confidence="0.998289136363636">
Bernard Merialdo Tagging English Text with a Probabilistic Model
24
22
20
16
14
12
10
Error rate
10 15 20 25 30 35 90 45 50
Iterations
600
580
560
540
520
500
980
460
440
5 10 15 20 25 30 35 40 45 50
Iterations_
</figure>
<figureCaption confidence="0.97565">
Figure 1
</figureCaption>
<tableCaption confidence="0.8139835">
ML training from uniform distributions.
Table 2
</tableCaption>
<table confidence="0.976654466666667">
ML training from various initial points.
Number of tagged sentences used for the initial model
0 100 2000 5000 10000 20000 all
Iter Correct Lags (% words) after ML on 1M words
0 77.0 90.0 95.4 96.2 96.6 96.9 97.0
1 80.5 92.6 95.8 96.3 96.6 96.7 96.8
2 81.8 93.0 95.7 96.1 96.3 96.4 96.4
3 83.0 93.1 95.4 95.8 96.1 96.2 96.2
4 84.0 93.0 95.2 95.5 95.8 96.0 96.0
5 84.8 92.9 95.1 95.4 95.6 95.8 95.8
6 85.3 92.8 94.9 95.2 95.5 95.6 95.7
7 85.8 92.8 94.7 95.1 95.3 95.5 95.5
8 86.1 92.7 94.6 95.0 95.2 95.4 95.4
9 86.3 92.6 945 94.9 95.1 95.3 95.3
10 86.6 92.6 94.4 94.8 95.0 95.2 95.2
</table>
<page confidence="0.994267">
163
</page>
<figure confidence="0.998529111111111">
Computational Linguistics
Volume 20, Number 2
12
10
8-
43. ......... 49. ......... ......... ......... ......... .0. ......... .........
2 -
4 6 8 10
Iterations
</figure>
<figureCaption confidence="0.947766">
Figure 2
</figureCaption>
<bodyText confidence="0.8665176">
ML training from various initial points (top line corresponds to N=100, bottom line to N=a11).
the relations between words and tags are much more precise than the relations be-
tween phonemes and speech signals (where the correct correspondence is harder to
define precisely). Some characteristics of ML training, such as the effect of smoothing
probabilities, are probably more suited to speech than to tagging.
</bodyText>
<subsectionHeader confidence="0.985475">
7.3 Extra Experiments
Viterbi versus ML tagging
</subsectionHeader>
<bodyText confidence="0.999989111111111">
For this experiment we considered the initial model built by RF training over the whole
training data and all the successive models created by the iterations of ML training.
For each of these models we performed Viterbi tagging and ML tagging on the same
test data, then evaluated and compared the number of tagging errors produced by
these two methods. The results are shown in Table 3.
The models obtained at different iterations are related, so one should not draw
strong conclusions about the definite superiority of one tagging procedure. However,
the difference in error rate is very small, and shows that the choice of the tagging
procedure is not as critical as the kind of training material.
</bodyText>
<subsectionHeader confidence="0.934309">
Constrained ML training
</subsectionHeader>
<bodyText confidence="0.999601571428571">
Following a suggestion made by F. Jelinek, we investigated the effect of constraining
the ML training by imposing constraints on the probabilities. This idea comes from
the observation that the amount of training data needed to properly estimate the
model increases with the number of free parameters of the model. In the case of
little training data, adding reasonable constraints on the shape of the models that are
looked for reduces the number of free parameters and should improve the quality of
the estimates.
</bodyText>
<figure confidence="0.865307555555555">
Error rate
6 -
164
Bernard Merialdo Tagging English Text with a Probabilistic Model
Table 3
Viterbi vs. ML tagging.
Tagging errors out of 45,583 words
Iter. Viterbi ML Vit. - ML
0 % nb % nb nb
0 97.01 1365 97.01 1362 3
1 96.76 1477 96.75 1480 -3
2 96.44 1623 96.47 1607 16
3 96.23 1718 96.23 1719 - 1
4 96.00 1824 96.02 1812 12
5 95.82 1906 95.85 1892 14
6 95.66 1978 95.68 1970 8
7 95.51 2046 95.54 2031 15
8 95.39 2100 95.42 2087 13
9 95.30 2144 95.31 2140 4
10 95.21 2183 95.22 2177 6
Table 4
Standard ML vs. tw-constrained ML training.
Tagging errors out of 45,583 words
Iter. ML tw-c. ML
0 % nb % nb
0 97.01 1365 97.01 1365
1 96.76 1477 96.87 1427
2 96.44 1623 96.71 1501
3 96.23 1718 96.57 1562
4 96.00 1824 96.43 1626
5 95.82 1906 96.36 1661
6 95.66 1978 96.29 1690
7 95.51 2046 96.22 1723
8 95.39 2100 96.18 1741
9 95.30 2144 96.12 1768
10 95.21 2183 96.09 1784
</figure>
<bodyText confidence="0.9624">
We tried two different constraints:
</bodyText>
<listItem confidence="0.9880765">
• The first one keeps p(t/w) fixed if w is a frequent word, in our case one
of the 1,000 most frequent words. We call it tw-constraint. The rationale is
that if w is frequent, the relative frequency provides a good estimate for
p(t/w) and the training should not change it.
• The second one keeps the marginal distribution p(t) constant and is
based on a similar reasoning. We call it t-constraint.
</listItem>
<subsectionHeader confidence="0.632902">
tw-constraint
</subsectionHeader>
<bodyText confidence="0.9993594">
The tw-constrained ML training is similar to the standard ML training, except that the
probabilities p(t/w) are not changed at the end of an iteration.
The results in Table 4 show the number of tagging errors when the model is trained
with the standard or tw-constrained ML training. They show that the tw-constrained
ML training still degrades the RF training, but not as quickly as the standard ML. We
</bodyText>
<page confidence="0.997614">
165
</page>
<note confidence="0.62811">
Computational Linguistics Volume 20, Number 2
</note>
<tableCaption confidence="0.994851">
Table 5
</tableCaption>
<table confidence="0.9616458">
Standard ML vs. constrained ML training.
Tagging errors out of 45,583 words (biclass model)
Iter. ML t-c. ML
U 0/0 nb nb
0 96.87 1429 96.87 1429
1 96.51 1592 96.54 1576
2 96.18 1743 96.23 1718
3 96.00 1824 96.03 1810
4 95.84 1896 95.90 1871
5 95.67 1972 95.77 1928
6 95.52 2044 95.59 2009
7 95.42 2087 95.50 2051
8 95.33 2129 95.42 2087
9 95.24 2171 95.34 2126
10 95.18 2196 95.30 2141
</table>
<bodyText confidence="0.995037333333333">
have not tested what happens when smaller training data is used to build the initial
model.
t-constraint
This constraint is more difficult to implement than the previous one because the prob-
abilities p(t) are not the parameters of the model, but a combination of these parame-
ters. With the help of R. Polyak we have designed an iterative procedure that allows
the likelihood to be improved while preserving the values of p(t). We do not have
sufficient space to describe this procedure here. Because of its greater computational
complexity, we have only applied it to a biclass model, i.e. a model where
</bodyText>
<equation confidence="0.869356">
p(tdwiti • • • =
</equation>
<bodyText confidence="0.9595790625">
The initial model is estimated by relative frequency on the whole training data and
Viterbi tagging is used.
As in the previous experiment, the results in Table 5 show the number of tagging
errors when the model is trained with the standard or t-constrained ML training.
They show that the t-constrained ML training still degrades the RF training, but not
as quickly as the standard ML. Again, we have not tested what happens when smaller
training data is used to build the initial model.
8. Conclusion
The results presented in this paper show that estimating the parameters of the model
by counting relative frequencies over a very large amount of hand-tagged text lead to
the best tagging accuracy.
Maximum Likelihood training is guaranteed to improve perplexity, but will not
necessarily improve tagging accuracy. In our experiments, ML training degrades the
performance unless the initial model is already very bad.
The preceding results suggest that the optimal strategy to build the best possible
model for tagging is the following:
</bodyText>
<listItem confidence="0.891524">
• get as much tagged (by hand) text as you can afford
</listItem>
<page confidence="0.984749">
166
</page>
<author confidence="0.341271">
Bernard Merialdo Tagging English Text with a Probabilistic Model
</author>
<listItem confidence="0.966103428571429">
• compute the relative frequencies from this data to build an initial model
Mo
• get as much untagged text as you can afford
• starting from Mo, perform the Forward-Backward iterations. At each
iteration, evaluate the tagging quality of the new model M, on some
held-out tagged text. Stop either when you have reached a preset
number of iterations or the model M, performs worse than
</listItem>
<bodyText confidence="0.853388">
whichever occurs first.
</bodyText>
<sectionHeader confidence="0.929523" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998823875">
I would like to thank Peter Brown, Fred
Jelinek, John Lafferty, Robert Mercer, Salim
Roukos, and other members of the
Continuous Speech Recognition group for
the fruitful discussions I had with them
throughout this work. I also want to thank
one of the referees for his judicious
comments.
</bodyText>
<sectionHeader confidence="0.580187" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.859609105263158">
Bahl, Lalit R., and Mercer, Robert L. (1976).
&amp;quot;Part of speech assignment by a statistical
decision algorithm.&amp;quot; In IEEE International
Symposium on Information Theory, 88-89.
Ronneby.
Bahl, Lalit R.; Jelinek, Frederick and Mercer,
Robert L. (1983). &amp;quot;A maximum likelihood
approach to continuous speech
recognition,&amp;quot; In IEEE Transactions on
PAMI, 5(2), 179-190.
Baum, L. E., and Eagon, J. A. (1967). &amp;quot;An
inequality with application to statistical
estimation for probabilistic functions of
Markov processes and to a model for
ecology.&amp;quot; Bulletin of the American
Mathematicians Society, 73,360-363.
Beale, A. D. (1985). &amp;quot;A probabilistic
approach to grammatical analysis of
written English by computer.&amp;quot; In
</bodyText>
<reference confidence="0.925683634920635">
Proceedings, Second Conference of the
European Chapter of the ACL, Geneva,
Switzerland, 159-165.
Beale, A. D. (1988). &amp;quot;Lexicon and grammar
in probabilistic tagging of written
English.&amp;quot; In Proceedings, 26th Annual
Meeting of the Association for Computational
Linguistics, Buffalo NY: 211-216.
Bellman, R. E. (1957). Dynamic Programming.
Princeton University Press.
Benello, J.; Mackie, A. W.; and Anderson,
J. A. (1989). &amp;quot;Syntactic category
disambiguation with neural networks.&amp;quot;
Computer Speech and Language, 3,203-217.
Brill, E.; Magerman, D.; Marcus, M.; and
Santorini, B. (1990). &amp;quot;Deducing linguistic
structure from the statistics of large
corpora.&amp;quot; In Proceedings, DARPA Speech
and Natural Language Workshop, Hidden
Valley PA. 275-282.
Brodda, Benny (1982). &amp;quot;Problems with
tagging and a solution.&amp;quot; Nordic Journal of
Linguistics, 93-116.
Church, Kenneth W. (1989). &amp;quot;A stochastic
parts program noun phrase parser for
unrestricted text.&amp;quot; In IEEE Proceedings of
the ICASSP, Glasgow, 695-698.
Codogno, M.; Fissore, L.; Martelli, A.; Pirani,
G.; and Volpi, G. (1987). &amp;quot;Experimental
evaluation of Italian language models for
large-dictionary speech recognition.&amp;quot; In
Proceedings, European Conference on Speech
Technology, Edinburgh, 159-162.
Cutting, D.; Kupiec, J.; Pedersen, J.; and
Sibun, P. (1992). &amp;quot;A practical
part-of-speech tagger.&amp;quot; In Proceedings,
Third Conference on Applied Language
Processing, Trento, Italy, 133-140.
Debili, Fathi (1977). &amp;quot;Traitements
syntaxiques utilisant des matrices de
precedence frequentielles construites
automatiquement par apprentissage.&amp;quot;
Doctoral dissertation, Engineering
Department, Universite Paris 7, France.
DeRose, S. (1988). &amp;quot;Grammatical category
disambiguation by statistical
optimization.&amp;quot; Computational Linguistics,
14(1), 31-39.
Derouault, Anne-Marie, and Merialdo,
Bernard (1986). &amp;quot;Natural language
modeling for phoneme-to-text
transcription.&amp;quot; In IEEE Transactions on
Pattern Analysis and Machine Intelligence,
8(6), 742-749.
Garside, R., and Leech, F. (1985). &amp;quot;A
probabilistic parser.&amp;quot; In Proceedings, Second
Conference of the European Chapter of the
ACL, Geneva, Switzerland, 166-170.
Jelinek, Frederick (1976). &amp;quot;Continuous
speech recognition by statistical
methods.&amp;quot; In Proceedings of the IEEE, 64,
532-556.
Jelinek, Frederick, and Mercer, Robert L.
</reference>
<page confidence="0.991418">
167
</page>
<note confidence="0.738335">
Computational Linguistics Volume 20, Number 2
</note>
<reference confidence="0.991941872340426">
(1980). &amp;quot;Interpolated estimation of
Markov source parameters from sparse
data.&amp;quot; In Proceedings, Workshop on Pattern
Recognition in Practice, Amsterdam,
381-397.
Katz, S. (1987). &amp;quot;Estimation of probabilities
from sparse data for the language model
component of a speech recognizer.&amp;quot; IEEE
Transactions on ASSP, 34(3), 400-401.
Klein, S., and Simmons, R. F. (1963). &amp;quot;A
grammatical approach to grammatical
coding of English words.&amp;quot; JACM, 10,
334-347.
Leech, G.; Garside, R.; and Atwell, E. (1983).
&amp;quot;The automatic grammatical tagging of
the LOB corpus.&amp;quot; Newsletter of the
International Computer Archive of Modern
English, 7,13-33.
de Marcken, C. G. (1990). &amp;quot;Parsing the LOB
corpus.&amp;quot; In Proceedings, ACL Annual
Meeting, Pittsburg PA, 243-251.
Marshall, Ian (1983). &amp;quot;Choice of
grammatical word-class without global
syntactic analysis: Tagging words in the
LOB corpus.&amp;quot; Computers and the
Humanities, 139-150.
Merialdo, Bernard (1991). &amp;quot;Tagging text with
a probabilistic model.&amp;quot; In IEEE Proceedings
of the ICASSP, Toronto, 809-812.
Nakamura, M., and Shikano, K. (1989). &amp;quot;A
study of English word category
prediction based on neural networks.&amp;quot; In
IEEE Proceedings of the ICASSP, Glasgow,
731-734.
Paulussen, H., and Martin, W. (1992).
&amp;quot;Dilemma-2: A lemmatizer-tagger for
medical abstracts.&amp;quot; In Proceedings, Third
Conference on Applied Language Processing,
Trento, Italy, 141-146.
Poritz, Alan B. (1988). &amp;quot;Hidden Markov
models: A guided tour.&amp;quot; In IEEE
Proceedings of the ICASSP, New York, 7-13.
Stolz, W. S.; Tannenbaum, P. H.; and
Carstensen F. V. (1965). &amp;quot;A stochastic
approach to the grammatical coding of
English.&amp;quot; Communications of the ACM, 8,
399-405.
</reference>
<page confidence="0.988575">
168
</page>
<figure confidence="0.73855834375">
Bernard Merialdo Tagging English Text with a Probabilistic Model
Appendix A: List of Tags Used
$* possessive marker (&apos;s, &apos;)
APP$* possessive adjectives (my, your, our)
AT* article (the, a, no)
BOUNDARY_TAG end-of-sentence marker
CCF* coordinating conjunction (and, or, but, so, yet, then)
CS* subordinating conjunction (if, because, unless)
CT* that or whether as subordinating conjunctions
D* determiner (all, any, enough)
D*Q wh-determiner (which, what, whose)
D*R comparative plural after-determiner (less, more)
D*1 determiner singular (this, that, little, much, former)
D*2 determiner plural (these, few, several, many)
DAT* superlative determiner (least, most)
EX* existential there
FW* foreign words (ipso, facto)
I* preposition (general)
ICS* preposition that can also be used as a conjunction (since, after)
IF* the preposition for
I0* the preposition of
J* adjective (small, pretty)
J*R comparative adjective (smaller, prettier)
J*T superlative adjective (prettiest, nicest)
LE* leading coordinator (both, either, neither)
M* cardinal number
MD* ordinal number (first, second)
N* noun without number (english)
N*1 singular noun (cat, man)
N*2 plural noun (cats, men)
NPR* proper noun (paris, fred)
NR* noun/adverb of direction (south, west) or time (now, tomorrow, tuesday)
P* non-nominative pronoun (none, anyone, oneself)
P*() who, whom, whoever, whomever
PNX1* personal pronoun reflexive (himself)
169
Computational Linguistics Volume 20, Number 2
PN1* indefinite pronoun (anyone, anybody)
PP$* possessive pronoun (mine, yours)
Pro personal pronoun object (me, him)
PP*S personal pronoun subject (I, you, we)
PP*S3 personal pronoun subject 3rd person singular (he, she)
PUNCT1* end of sentence (. ! ? —)
PUNCT2* non terminal punctuation (, : ;)
QUOT* quote
R* adverb (here, slowly)
R*Q wh-adverb (where, when, why, how, whenever, wherever)
R*R comparative adverb (better, longer)
RG* degree adverb (very, so, too, enough, indeed)
RGQ* wh-degree adverb (how)
RGR* comparative degree adverb (more, less, worse)
RP* adverb that can also serve as a preposition
SIGN* sign ($, c., ct, %)
TO* to as pre-infinitive
UH* interjection (gee)
VBDR* were
VBDZ* was
VBG* being
VBP infinitive form of be and imperative
VBM* am
VBN* been
VBR* are
VBZ* is
VDG* doing
</figure>
<bodyText confidence="0.919887857142857">
VDN* past participial form of do (did)
VDPAST* past form of do (did)
VDO* do as a conjugated form and infinitive
VDOZ* does as a conjugated form
VHG* having
VHN* past participial form of have (had)
VHPAST* past form of have (had)
</bodyText>
<page confidence="0.967377">
170
</page>
<author confidence="0.371523">
Bernard Merialdo Tagging English Text with a Probabilistic Model
</author>
<bodyText confidence="0.986090777777778">
VHO* have as a conjugated form
VHOZ* has as a conjugated form
VM* modals (can, would, ought, used)
VVG* non-aux verb in -ing
VVN* past participial form of non-aux verb
VVPAST* preterit of non-aux verb
VVO* non-third-person-singular form of non-aux verb and infinitive
VVOZ* third-person-singular form of non-aux verb
XX* not
</bodyText>
<page confidence="0.997014">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896212">
<title confidence="0.9906405">Tagging English Text with a Probabilistic Model</title>
<author confidence="0.995698">Bernard Merialdot</author>
<affiliation confidence="0.954556">Institut EURECOM</affiliation>
<abstract confidence="0.996774571428572">In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined: • using text that has been tagged by hand and computing relative frequency counts, • using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle. Experiments show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings, Second Conference of the European Chapter of the ACL,</booktitle>
<pages>159--165</pages>
<location>Geneva, Switzerland,</location>
<marker></marker>
<rawString>Proceedings, Second Conference of the European Chapter of the ACL, Geneva, Switzerland, 159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Beale</author>
</authors>
<title>Lexicon and grammar in probabilistic tagging of written English.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>211--216</pages>
<location>Buffalo NY:</location>
<contexts>
<context position="1765" citStr="Beale 1988" startWordPosition="289" endWordPosition="290">, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE International Conference on Acoustics, </context>
<context position="14177" citStr="Beale (1988)" startWordPosition="2382" endWordPosition="2383">ng data Interpolation Nb of errors `)/0 correct (sentences) coefficient A (words) tags 0 .0 10498 77.0 100 .48 4568 90.0 2000 .77 2110 95.4 5000 .85 1744 96.2 10000 .90 1555 96.6 20000 .92 1419 96.9 all .94 1365 97.0 not bring any improvement in practice. One concerns the difference between Viterbi and ML tagging, and the other concerns the use of constraints during training. We shall begin by describing the textual data that we are using, before presenting the different tagging experiments using these various training and tagging methods. 7.1 Text Data We use the &amp;quot;treebank&amp;quot; data described in Beale (1988). It contains 42,186 sentences (about one million words) from the Associated Press. These sentences have been tagged manually at the Unit for Computer Research on the English Language (University of Lancaster, U.K.), in collaboration with IBM U.K. (Winchester) and the IBM Speech Recognition group in Yorktown Heights (USA). In fact, these sentences are not only tagged but also parsed. However, we do not use the information contained in the parse. In the treebank 159 different tags are used. These tags were projected on a smaller system of 76 tags designed by Evelyne Tzoukermann and Peter Brown </context>
</contexts>
<marker>Beale, 1988</marker>
<rawString>Beale, A. D. (1988). &amp;quot;Lexicon and grammar in probabilistic tagging of written English.&amp;quot; In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics, Buffalo NY: 211-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="12741" citStr="Bellman 1957" startWordPosition="2141" endWordPosition="2142">se the sequence of tags is hidden). A well-known solution to this problem is the Forward-Backward (FB) or Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983), which iteratively constructs a sequence of models that improve the probability of the training data. The advantage of this approach is that it does not require any tagging of the text, but makes the assumption that the correct model is the one in which tags are used to best predict the word sequence. 6. Tagging Algorithms The Viterbi algorithm is easily implemented using a dynamic programming scheme (Bellman 1957). The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large number of alignments. However, in the case of a hidden Markov model, these computations can be arranged in a way similar to the one used during the FB algorithm, so that the overall amount of computation needed becomes linear in the length of the sentence (Baum and Eagon 1967). 7. Experiments The main objective of this paper is to compare RF and ML training. This is done in Section 7.2. We also take advantage of the environment that we have set up to pe</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Benello</author>
<author>A W Mackie</author>
<author>J A Anderson</author>
</authors>
<title>Syntactic category disambiguation with neural networks.&amp;quot;</title>
<date>1989</date>
<journal>Computer Speech and Language,</journal>
<pages>3--203</pages>
<marker>Benello, Mackie, Anderson, 1989</marker>
<rawString>Benello, J.; Mackie, A. W.; and Anderson, J. A. (1989). &amp;quot;Syntactic category disambiguation with neural networks.&amp;quot; Computer Speech and Language, 3,203-217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>D Magerman</author>
<author>M Marcus</author>
<author>B Santorini</author>
</authors>
<title>Deducing linguistic structure from the statistics of large corpora.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, DARPA Speech and Natural Language Workshop,</booktitle>
<pages>275--282</pages>
<location>Hidden Valley PA.</location>
<contexts>
<context position="1559" citStr="Brill et al. 1990" startWordPosition="257" endWordPosition="260">ood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Contin</context>
</contexts>
<marker>Brill, Magerman, Marcus, Santorini, 1990</marker>
<rawString>Brill, E.; Magerman, D.; Marcus, M.; and Santorini, B. (1990). &amp;quot;Deducing linguistic structure from the statistics of large corpora.&amp;quot; In Proceedings, DARPA Speech and Natural Language Workshop, Hidden Valley PA. 275-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benny Brodda</author>
</authors>
<title>Problems with tagging and a solution.&amp;quot;</title>
<date>1982</date>
<journal>Nordic Journal of Linguistics,</journal>
<pages>93--116</pages>
<contexts>
<context position="1512" citStr="Brodda 1982" startWordPosition="251" endWordPosition="252">ble. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried o</context>
</contexts>
<marker>Brodda, 1982</marker>
<rawString>Brodda, Benny (1982). &amp;quot;Problems with tagging and a solution.&amp;quot; Nordic Journal of Linguistics, 93-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1989</date>
<booktitle>In IEEE Proceedings of the ICASSP,</booktitle>
<pages>695--698</pages>
<location>Glasgow,</location>
<contexts>
<context position="1753" citStr="Church 1989" startWordPosition="287" endWordPosition="288">this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE International Conference on</context>
<context position="5882" citStr="Church 1989" startWordPosition="969" endWordPosition="970">e level, choose the most probable sequence of tags for the sentence 0(w) _ argTmax p(Tiw) argTmax p(w,T) We call this procedure Viterbi tagging. It is achieved using a dynamic programming scheme. • for evaluation at word level, choose the most probable tag for each word in the sentence 0(w)i argmt ax p(t. _ t/w) _ argmax t 2_, P(W, T) T:t,=t where 0(W), is the tag assigned to word w, by the tagging procedure in the context of the sentence W. We call this procedure Maximum Likelihood (ML) tagging. It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level. The reasons for this preference are presumably that: • Viterbi tagging is simpler to implement than ML tagging and requires less computation (although they both have the same asymptotic complexity) • Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing • ML tagging may produce sequences of tags that are linguistically impossible (because the choice of a tag depends on all contexts taken together). However, in our experiments, we will show that Viterbi and ML tagging result in ve</context>
</contexts>
<marker>Church, 1989</marker>
<rawString>Church, Kenneth W. (1989). &amp;quot;A stochastic parts program noun phrase parser for unrestricted text.&amp;quot; In IEEE Proceedings of the ICASSP, Glasgow, 695-698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Codogno</author>
<author>L Fissore</author>
<author>A Martelli</author>
<author>G Pirani</author>
<author>G Volpi</author>
</authors>
<title>Experimental evaluation of Italian language models for large-dictionary speech recognition.&amp;quot; In</title>
<date>1987</date>
<booktitle>Proceedings, European Conference on Speech Technology,</booktitle>
<pages>159--162</pages>
<location>Edinburgh,</location>
<marker>Codogno, Fissore, Martelli, Pirani, Volpi, 1987</marker>
<rawString>Codogno, M.; Fissore, L.; Martelli, A.; Pirani, G.; and Volpi, G. (1987). &amp;quot;Experimental evaluation of Italian language models for large-dictionary speech recognition.&amp;quot; In Proceedings, European Conference on Speech Technology, Edinburgh, 159-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>A practical part-of-speech tagger.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Third Conference on Applied Language Processing,</booktitle>
<pages>133--140</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1816" citStr="Cutting et al. 1992" startWordPosition="295" endWordPosition="298">nd-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto (Canada), May</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Cutting, D.; Kupiec, J.; Pedersen, J.; and Sibun, P. (1992). &amp;quot;A practical part-of-speech tagger.&amp;quot; In Proceedings, Third Conference on Applied Language Processing, Trento, Italy, 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fathi Debili</author>
</authors>
<title>Traitements syntaxiques utilisant des matrices de precedence frequentielles construites automatiquement par apprentissage.&amp;quot; Doctoral dissertation,</title>
<date>1977</date>
<institution>Engineering Department, Universite Paris</institution>
<contexts>
<context position="1610" citStr="Debili 1977" startWordPosition="267" endWordPosition="268">mate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Res</context>
</contexts>
<marker>Debili, 1977</marker>
<rawString>Debili, Fathi (1977). &amp;quot;Traitements syntaxiques utilisant des matrices de precedence frequentielles construites automatiquement par apprentissage.&amp;quot; Doctoral dissertation, Engineering Department, Universite Paris 7, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>31--39</pages>
<contexts>
<context position="1740" citStr="DeRose 1988" startWordPosition="285" endWordPosition="286">ally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE International </context>
<context position="5868" citStr="DeRose 1988" startWordPosition="967" endWordPosition="968">on at sentence level, choose the most probable sequence of tags for the sentence 0(w) _ argTmax p(Tiw) argTmax p(w,T) We call this procedure Viterbi tagging. It is achieved using a dynamic programming scheme. • for evaluation at word level, choose the most probable tag for each word in the sentence 0(w)i argmt ax p(t. _ t/w) _ argmax t 2_, P(W, T) T:t,=t where 0(W), is the tag assigned to word w, by the tagging procedure in the context of the sentence W. We call this procedure Maximum Likelihood (ML) tagging. It is interesting to note that the most commonly used method is Viterbi tagging (see DeRose 1988; Church 1989) although it is not the optimal method for evaluation at word level. The reasons for this preference are presumably that: • Viterbi tagging is simpler to implement than ML tagging and requires less computation (although they both have the same asymptotic complexity) • Viterbi tagging provides the best interpretation for the sentence, which is linguistically appealing • ML tagging may produce sequences of tags that are linguistically impossible (because the choice of a tag depends on all contexts taken together). However, in our experiments, we will show that Viterbi and ML taggin</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>DeRose, S. (1988). &amp;quot;Grammatical category disambiguation by statistical optimization.&amp;quot; Computational Linguistics, 14(1), 31-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne-Marie Derouault</author>
<author>Bernard Merialdo</author>
</authors>
<title>Natural language modeling for phoneme-to-text transcription.&amp;quot;</title>
<date>1986</date>
<journal>In IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>8</volume>
<issue>6</issue>
<pages>742--749</pages>
<contexts>
<context position="1727" citStr="Derouault and Merialdo 1986" startWordPosition="281" endWordPosition="284">uracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE I</context>
</contexts>
<marker>Derouault, Merialdo, 1986</marker>
<rawString>Derouault, Anne-Marie, and Merialdo, Bernard (1986). &amp;quot;Natural language modeling for phoneme-to-text transcription.&amp;quot; In IEEE Transactions on Pattern Analysis and Machine Intelligence, 8(6), 742-749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>F Leech</author>
</authors>
<title>A probabilistic parser.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, Second Conference of the European Chapter of the ACL,</booktitle>
<pages>166--170</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="3137" citStr="Garside and Leech 1985" startWordPosition="501" endWordPosition="504">r 2 Through these different approaches, some common points have emerged: • For any given word, only a few tags are possible, a list of which can be found either in the dictionary or through a morphological analysis of the word. • When a word has several possible tags, the correct tag can generally be chosen from the local context, using contextual rules that define the valid sequences of tags. These rules may be given priorities so that a selection can be made even when several rules apply. These kinds of considerations fit nicely inside a probabilistic formulation of the problem (Beale 1985; Garside and Leech 1985), which offers the following advantages: • a sound theoretical framework is provided • the approximations are clear • the probabilities provide a straightforward way to disambiguate • the probabilities can be estimated automatically from data. In this paper we present a particular probabilistic model, the triclass model, and results from experiments involving different ways to estimate its parameters, with the intention of maximizing the ability of the model to tag text accurately. In particular, we are interested in a way to make the best use of untagged text in the training of the model. 2. </context>
</contexts>
<marker>Garside, Leech, 1985</marker>
<rawString>Garside, R., and Leech, F. (1985). &amp;quot;A probabilistic parser.&amp;quot; In Proceedings, Second Conference of the European Chapter of the ACL, Geneva, Switzerland, 166-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Continuous speech recognition by statistical methods.&amp;quot;</title>
<date>1976</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<volume>64</volume>
<pages>532--556</pages>
<contexts>
<context position="11003" citStr="Jelinek 1976" startWordPosition="1851" endWordPosition="1852">g text, since the relative frequencies should be more reliable. This interpolation procedure is also called &amp;quot;smoothing.&amp;quot; Smoothing is performed as follows: • Some quantity of tagged text from the training data is not used in the computation of the relative frequencies. It is called the &amp;quot;held-out&amp;quot; data. • The coefficient A is chosen to maximize the probability of emission of the held-out data by the interpolated model. 1 NT 1 159 Computational Linguistics Volume 20, Number 2 • This maximization can be performed by the standard Forward-Backward (FB) or Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983; Poritz 1988), by considering A and 1 — A as the transition probabilities of a Markov model. It can be noted that more complicated interpolation schemes are possible. For example, different coefficients can be used depending on the count of (t1, t2), with the intuition that relative frequencies can be trusted more when this count is high. Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or hrf (t3). Smoothing can also be achieved with procedures other than interpolation. One example is the &amp;quot;backing-off&amp;quot; strategy </context>
<context position="12289" citStr="Jelinek 1976" startWordPosition="2067" endWordPosition="2068">model M it is possible to compute the probability of any sequence of words W according to this model: pm(w) = EPA4(W,T) T where the sum is taken over all possible alignments. The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text: max TT pm( vv) M AiW where the product is taken over all the sentences W in the training text. This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden). A well-known solution to this problem is the Forward-Backward (FB) or Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983), which iteratively constructs a sequence of models that improve the probability of the training data. The advantage of this approach is that it does not require any tagging of the text, but makes the assumption that the correct model is the one in which tags are used to best predict the word sequence. 6. Tagging Algorithms The Viterbi algorithm is easily implemented using a dynamic programming scheme (Bellman 1957). The Maximum Likelihood algorithm appears more complex at first glance, because it involves computing the sum of the probabilities of a large numbe</context>
</contexts>
<marker>Jelinek, 1976</marker>
<rawString>Jelinek, Frederick (1976). &amp;quot;Continuous speech recognition by statistical methods.&amp;quot; In Proceedings of the IEEE, 64, 532-556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.&amp;quot;</title>
<date>1980</date>
<booktitle>In Proceedings, Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<location>Amsterdam,</location>
<contexts>
<context position="10181" citStr="Jelinek and Mercer 1980" startWordPosition="1713" endWordPosition="1716">ontains this sequence will get a probability of zero. Therefore, it may happen that, for some sequences of words, all alignments get a probability of zero and the model becomes useless for such sentences. To avoid this, we interpolate these distributions with uniform distributions, i.e. :onsider the interpolated model defined by: hinter(t3/ti , t2) = A.hrf (t3 I , t2) + (1 – A) f (t3 I ti.,t2) kinter (Wt) = A.k,f(w I t) + (1 – where hunif (t3/ti , t2) = — /cum/. (w I t) – number of words that have the tag t The interpolation coefficient A is computed using the deleted interpolation algorithm (Jelinek and Mercer 1980) (it would also be possible to use two coefficients, one for the interpolation on h, one for the interpolation on k). The value of this coefficient is expected to increase if we increase the size of the training text, since the relative frequencies should be more reliable. This interpolation procedure is also called &amp;quot;smoothing.&amp;quot; Smoothing is performed as follows: • Some quantity of tagged text from the training data is not used in the computation of the relative frequencies. It is called the &amp;quot;held-out&amp;quot; data. • The coefficient A is chosen to maximize the probability of emission of the held-out </context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, Frederick, and Mercer, Robert L. (1980). &amp;quot;Interpolated estimation of Markov source parameters from sparse data.&amp;quot; In Proceedings, Workshop on Pattern Recognition in Practice, Amsterdam, 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.&amp;quot;</title>
<date>1987</date>
<journal>IEEE Transactions on ASSP,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="11626" citStr="Katz (1987)" startWordPosition="1951" endWordPosition="1952">nek, and Mercer 1983; Poritz 1988), by considering A and 1 — A as the transition probabilities of a Markov model. It can be noted that more complicated interpolation schemes are possible. For example, different coefficients can be used depending on the count of (t1, t2), with the intuition that relative frequencies can be trusted more when this count is high. Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or hrf (t3). Smoothing can also be achieved with procedures other than interpolation. One example is the &amp;quot;backing-off&amp;quot; strategy proposed by Katz (1987). 5.2 Maximum Likelihood Training Using a triclass model M it is possible to compute the probability of any sequence of words W according to this model: pm(w) = EPA4(W,T) T where the sum is taken over all possible alignments. The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text: max TT pm( vv) M AiW where the product is taken over all the sentences W in the training text. This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden). A well-known solution to this problem is the Forward-Backward </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. (1987). &amp;quot;Estimation of probabilities from sparse data for the language model component of a speech recognizer.&amp;quot; IEEE Transactions on ASSP, 34(3), 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Klein</author>
<author>R F Simmons</author>
</authors>
<title>A grammatical approach to grammatical coding of English words.&amp;quot;</title>
<date>1963</date>
<journal>JACM,</journal>
<volume>10</volume>
<pages>334--347</pages>
<contexts>
<context position="1499" citStr="Klein and Simmons 1963" startWordPosition="247" endWordPosition="250">uch tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work </context>
</contexts>
<marker>Klein, Simmons, 1963</marker>
<rawString>Klein, S., and Simmons, R. F. (1963). &amp;quot;A grammatical approach to grammatical coding of English words.&amp;quot; JACM, 10, 334-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
<author>R Garside</author>
<author>E Atwell</author>
</authors>
<title>The automatic grammatical tagging of the LOB corpus.&amp;quot;</title>
<date>1983</date>
<journal>Newsletter of the International Computer Archive of Modern English,</journal>
<pages>7--13</pages>
<marker>Leech, Garside, Atwell, 1983</marker>
<rawString>Leech, G.; Garside, R.; and Atwell, E. (1983). &amp;quot;The automatic grammatical tagging of the LOB corpus.&amp;quot; Newsletter of the International Computer Archive of Modern English, 7,13-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Parsing the LOB corpus.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, ACL Annual Meeting,</booktitle>
<pages>243--251</pages>
<location>Pittsburg PA,</location>
<marker>de Marcken, 1990</marker>
<rawString>de Marcken, C. G. (1990). &amp;quot;Parsing the LOB corpus.&amp;quot; In Proceedings, ACL Annual Meeting, Pittsburg PA, 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Marshall</author>
</authors>
<title>Choice of grammatical word-class without global syntactic analysis:</title>
<date>1983</date>
<booktitle>Tagging words in the LOB corpus.&amp;quot; Computers and the Humanities,</booktitle>
<pages>139--150</pages>
<contexts>
<context position="1665" citStr="Marshall 1983" startWordPosition="274" endWordPosition="275">ta, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the m</context>
</contexts>
<marker>Marshall, 1983</marker>
<rawString>Marshall, Ian (1983). &amp;quot;Choice of grammatical word-class without global syntactic analysis: Tagging words in the LOB corpus.&amp;quot; Computers and the Humanities, 139-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.&amp;quot;</title>
<date>1991</date>
<booktitle>In IEEE Proceedings of the ICASSP,</booktitle>
<pages>809--812</pages>
<location>Toronto,</location>
<contexts>
<context position="1794" citStr="Merialdo 1991" startWordPosition="293" endWordPosition="294">ed amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE International Conference on Acoustics, Speech and Signal Processing,</context>
</contexts>
<marker>Merialdo, 1991</marker>
<rawString>Merialdo, Bernard (1991). &amp;quot;Tagging text with a probabilistic model.&amp;quot; In IEEE Proceedings of the ICASSP, Toronto, 809-812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nakamura</author>
<author>K Shikano</author>
</authors>
<title>A study of English word category prediction based on neural networks.&amp;quot;</title>
<date>1989</date>
<booktitle>In IEEE Proceedings of the ICASSP,</booktitle>
<pages>731--734</pages>
<location>Glasgow,</location>
<contexts>
<context position="1946" citStr="Nakamura and Shikano 1989" startWordPosition="314" endWordPosition="317">e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a visitor of the Continuous Speech Recognition group, IBM T. J. Watson Research Center, Yorktown Heights, NY (USA). Part of the material included in this work has been presented at the IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto (Canada), May 1991. © 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 2 Through these different appr</context>
</contexts>
<marker>Nakamura, Shikano, 1989</marker>
<rawString>Nakamura, M., and Shikano, K. (1989). &amp;quot;A study of English word category prediction based on neural networks.&amp;quot; In IEEE Proceedings of the ICASSP, Glasgow, 731-734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paulussen</author>
<author>W Martin</author>
</authors>
<title>Dilemma-2: A lemmatizer-tagger for medical abstracts.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Third Conference on Applied Language Processing,</booktitle>
<pages>141--146</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1539" citStr="Paulussen and Martin 1992" startWordPosition="253" endWordPosition="256">o show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. 1. Introduction A lot of effort has been devoted in the past to the problem of tagging text, i.e. assigning to each word the correct tag (part of speech) in the context of the sentence. Two main approaches have generally been considered: • rule-based (Klein and Simmons 1963; Brodda 1982; Paulussen and Martin 1992; Brill et al. 1990) • probabilistic (Bahl and Mercer 1976; Debili 1977; Stolz, Tannenbaum, and Carstensen 1965; Marshall 1983; Leech, Garside, and Atwell 1983; Derouault and Merialdo 1986; DeRose 1988; Church 1989; Beale 1988; Marcken 1990; Merialdo 1991; Cutting et al. 1992). More recently, some work has been proposed using neural networks (Benello, Mackie, and Anderson 1989; Nakamura and Shikano 1989). * Multimedia Communications Department, Institut EURECOM, 2229 Route des Cretes, B.F. 193, 06904 Valbonne Cedex France; merialdo@eurecom.fr. This work was carried out while the author was a v</context>
</contexts>
<marker>Paulussen, Martin, 1992</marker>
<rawString>Paulussen, H., and Martin, W. (1992). &amp;quot;Dilemma-2: A lemmatizer-tagger for medical abstracts.&amp;quot; In Proceedings, Third Conference on Applied Language Processing, Trento, Italy, 141-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan B Poritz</author>
</authors>
<title>Hidden Markov models: A guided tour.&amp;quot;</title>
<date>1988</date>
<booktitle>In IEEE Proceedings of the ICASSP,</booktitle>
<pages>7--13</pages>
<location>New York,</location>
<contexts>
<context position="11049" citStr="Poritz 1988" startWordPosition="1858" endWordPosition="1859">be more reliable. This interpolation procedure is also called &amp;quot;smoothing.&amp;quot; Smoothing is performed as follows: • Some quantity of tagged text from the training data is not used in the computation of the relative frequencies. It is called the &amp;quot;held-out&amp;quot; data. • The coefficient A is chosen to maximize the probability of emission of the held-out data by the interpolated model. 1 NT 1 159 Computational Linguistics Volume 20, Number 2 • This maximization can be performed by the standard Forward-Backward (FB) or Baum—Welch algorithm (Baum and Eagon 1967; Jelinek 1976; Bahl, Jelinek, and Mercer 1983; Poritz 1988), by considering A and 1 — A as the transition probabilities of a Markov model. It can be noted that more complicated interpolation schemes are possible. For example, different coefficients can be used depending on the count of (t1, t2), with the intuition that relative frequencies can be trusted more when this count is high. Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or hrf (t3). Smoothing can also be achieved with procedures other than interpolation. One example is the &amp;quot;backing-off&amp;quot; strategy proposed by Katz (1987). 5.2 Maximum Likelihoo</context>
</contexts>
<marker>Poritz, 1988</marker>
<rawString>Poritz, Alan B. (1988). &amp;quot;Hidden Markov models: A guided tour.&amp;quot; In IEEE Proceedings of the ICASSP, New York, 7-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Stolz</author>
<author>P H Tannenbaum</author>
<author>F V Carstensen</author>
</authors>
<title>A stochastic approach to the grammatical coding of English.&amp;quot;</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<pages>399--405</pages>
<marker>Stolz, Tannenbaum, Carstensen, 1965</marker>
<rawString>Stolz, W. S.; Tannenbaum, P. H.; and Carstensen F. V. (1965). &amp;quot;A stochastic approach to the grammatical coding of English.&amp;quot; Communications of the ACM, 8, 399-405.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>