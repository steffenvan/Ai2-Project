<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.813147">
Semantic Enrichment of Text with Background Knowledge
</title>
<note confidence="0.71159325">
Anselmo Perias Eduard Hovy
UNED NLP &amp; IR Group USC Information Sciences Institute
Juan del Rosal, 16 4676 Admiralty Way
28040 Madrid, Spain Marina del Rey, CA 90292-6695
</note>
<email confidence="0.981791">
anselmo@lsi.uned.es hovy@isi.edu
</email>
<sectionHeader confidence="0.993366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867">
Texts are replete with gaps, information omit-
ted since authors assume a certain amount of
background knowledge. We describe the kind
of information (the formalism and methods to
derive the content) useful for automated fill-
ing of such gaps. We describe a stepwise pro-
cedure with a detailed example.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906655172414">
Automated understanding of connected text re-
mains an unsolved challenge in NLP. In contrast
to systems that harvest information from large col-
lections of text, or that extract only certain pre-
specified kinds of information from single texts,
the task of extracting and integrating all informa-
tion from a single text, and building a coherent and
relatively complete representation of its full con-
tent, is still beyond current capabilities.
A significant obstacle is the fact that text always
omits information that is important, but that people
recover effortlessly. Authors leave out information
that they assume is known to their readers, since its
inclusion (under the Gricean maxim of minimality)
would carry an additional, often pragmatic, import.
The problem is that systems cannot perform the
recovery since they lack the requisite background
knowledge and inferential machinery to use it.
In this research we address the problem of
automatically recovering such omitted information
to ‘plug the gaps’ in text. To do so, we describe
the background knowledge required as well as a
procedure for recognizing where gaps exist and
determining which kinds of background knowl-
edge are needed.
We are looking for the synchronization between
the text representation achievable by current NLP
and a knowledge representation (KR) scheme that
can permit further inference for text interpretation.
</bodyText>
<page confidence="0.955184">
15
</page>
<subsectionHeader confidence="0.947467">
1.1 Vision
</subsectionHeader>
<bodyText confidence="0.999977296296296">
Clearly, producing a rich text interpretation re-
quires both NLP and KR capabilities. The strategy
we explore is the enablement of bidirectional
communication between the two sides from the
very beginning of the text processing. We assume
that the KR system doesn’t require a full represen-
tation of the text meaning, but can work with a par-
tial interpretation, namely of the material explicitly
present in the text, and can then flesh out this in-
terpretation as required for its specific task. Al-
though the NLP system initially provides simpler
representations (even possibly ambiguous or
wrong ones), the final result contains the semantics
of the text according to the working domain.
In this model, the following questions arise:
How much can we simplify our initial text repre-
sentation and still permit the attachment of back-
ground knowledge for further inference and
interpretation? How should background knowl-
edge be represented for use by the KR system?
How can the incompleteness and brittleness typical
of background knowledge (its representational in-
flexibility, or limitation to a single viewpoint or
expressive phrasing) (Barker 2007) be overcome?
In what sequence can a KR system enrich an initial
and/or impoverished reading, and how can the en-
richment benefit subsequent text processing?
</bodyText>
<subsectionHeader confidence="0.918655">
1.2 Approach
</subsectionHeader>
<bodyText confidence="0.999799333333333">
Although we are working toward it, we do not yet
have such a system. The aim of our current work
is to rapidly assemble some necessary pieces and
explore how to (i) attach background knowledge to
flesh out a simple text representation and (ii) there
by make explicit the meanings attached to some of
its syntactic relations. We begin with an initial
simple text representation, a background knowl-
edge base corresponding to the text, and a simple
</bodyText>
<note confidence="0.984512">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 15–23,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942107142857">
formalized procedure to attach elements from the
background knowledge to the entities and implicit
relations present in the initial text representation.
Surprisingly, we find that some quite simple
processing can be effective if we are able to con-
textualize the text under interpretation.
For our exploratory experiments, we are work-
ing with a collection of 30,000 documents in the
domain of US football. We parsed the collection
using a standard dependency parser (Marneffe and
Manning, 2008; Klein and Maning, 2003) and, af-
ter collapsing some syntactic dependencies, ob-
tained the simple textual representations shown in
Section 2. From them, we built a Background
Knowledge Base by automatically harvesting
propositions expressed in the collection (Section
3). Their frequency in the collection lead the en-
richment process: given a new text in the same
domain, we build exactly the same kind of repre-
sentation, and attach the background knowledge
propositions as related to the text (Section 4).
Since this is an exploratory sketch, we cannot
provide a quantitative evaluation yet, but the quali-
tative study over some examples suggest that this
simple framework is promising enough to start a
long term research (Section 5). Finally, we con-
clude with the next steps we want to follow and the
kind of evaluation we plan to do.
</bodyText>
<sectionHeader confidence="0.967125" genericHeader="method">
2 Text Representation
</sectionHeader>
<bodyText confidence="0.999670608695652">
The starting text representation must capture the
first shot of what’s going on in the text, taking
some excerpts into account and (unfortunately)
losing others. After the first shot, in accord with
the purpose of the reading, we will “contextualize”
each sentence, expanding its initial representation
with the relevant related background knowledge in
our base.
During this process of making explicit the im-
plicit semantic relations (which we call contextu-
alization or interpretation) it will become apparent
whether we need to recover some of the discarded
elements, whether we need to expand some others,
etc. So the process of interpretation is identified
with the growing of the context (according to the
KB) until the interpretation is possible. This is re-
lated to some well-known theories such as the
Theory of Relevance (Sperber and Wilson, 1995).
The particular method we envisage is related to
Interpretation as Abduction (Hobbs et al. 1993).
How can the initial information be represented
so as to enable the context to grow into an interpre-
tation? We hypothesize that:
</bodyText>
<listItem confidence="0.998443166666667">
1. Behind certain syntactic dependencies there
are semantic relations.
2. In the case of dependencies between nouns,
this semantic relation can be made more ex-
plicit using verbs and/or prepositions. The
knowledge base must help us find them.
</listItem>
<bodyText confidence="0.9339562">
We look for a semantic representation close
enough to the syntactic representation we can ob-
tain from the dependency graph. The main syntac-
tic dependencies we want to represent in order to
enable enrichment are:
</bodyText>
<listItem confidence="0.918215074074074">
1. Dependencies between nouns such as noun-
noun compounds (nn) or possessive (poss).
2. Dependencies between nouns and verbs,
such as subject and object relations.
3. Prepositions having two nouns as argu-
ments. Then the preposition becomes the la-
bel for the relation between the two nouns,
being the object of the preposition the target
of the relation.
For these selected elements, we produce two very
simple transformations of the syntactic dependency
graph:
1. Invert the direction of the syntactic depend-
ency for the modifiers. Since we work with
the hypothesis that behind a syntactic de-
pendency there is a semantic relation, we re-
cord the direction of the semantic relation.
2. Collapse the syntactic dependencies be-
tween verb, subject, and object into a single
semantic relation. Since we are assuming
that the verb is the more explicit expression
of a semantic relation, we fix this in the ini-
tial representation. The subject will be the
source of the relation and the object will be
the target of the relation. When the verb has
more arguments we consider its expansion
as a new node as referred in Section 4.4.
</listItem>
<bodyText confidence="0.996123333333333">
Figure 1 shows the initial minimal representa-
tion for the sentence we will use for our discus-
sion:
</bodyText>
<subsectionHeader confidence="0.549391333333333">
San_Francisco&apos;s Eric_Davis intercepted
a Steve_Walsh pass on the next series to
set_up a seven-yard Young touchdown pass
</subsectionHeader>
<bodyText confidence="0.8527245">
to Brent_Jones.
Notice that some pieces of the text are lost in the
initial representation of the text as for example “on
the next series” or “seven-yard”.
</bodyText>
<page confidence="0.995086">
16
</page>
<figureCaption confidence="0.968061">
Figure 1. Representation of the sentence: San_FranciscoIs Eric_Davis intercepted a Steve_Walsh
pass on the next series to set_up a seven-yard Young touchdown pass to Brent_Jones.
</figureCaption>
<sectionHeader confidence="0.930615" genericHeader="method">
3 Background Knowledge Base
</sectionHeader>
<bodyText confidence="0.999547818181818">
The Background Knowledge Base (BKB) is built
from a collection in the domain of the texts we
want to semanticize. The collection consists of
30,826 New York Times news about American
football, similar to the kind of texts we want to
interpret. The elements in the BKB (3,022,305 in
total) are obtained as a result of applying general
patterns over dependency trees. We take advantage
of the typed dependencies (Marneffe and Manning,
2008) produced by the Stanford parser (Klein and
Maning, 2003).
</bodyText>
<subsectionHeader confidence="0.999895">
3.1 Types of elements in the BKB
</subsectionHeader>
<bodyText confidence="0.9999765">
We distinguish three elements in our Background
Knowledge Base: Entities, Propositions, and Lexi-
cal relations. All of them have associated their fre-
quency in the reference collection.
</bodyText>
<subsectionHeader confidence="0.532783">
Entities
</subsectionHeader>
<bodyText confidence="0.997502">
We distinguish between entity classes and entity
instances:
</bodyText>
<listItem confidence="0.637755866666667">
1. Entity classes: Entity classes are denoted by
the nouns that participate in a copulative rela-
tion or as noun modifier. In addition, we intro-
duce two special classes: Person and Group.
These two classes are related to the use of pro-
nouns in text. Pronouns “I”, “he” and “she” are
linked to class Person. Pronouns “we” and
“they” are linked to class Group. For example,
the occurrence of the pronoun “he” in “He
threw a pass” would produce an additional
count of the proposition “person:throw:pass”.
2. Entity Instances: Entity instances are indicated
by proper nouns. Proper nouns are identified
by the part of speech tagging. Some of these
instances will participate in the “has-instance”
</listItem>
<bodyText confidence="0.927529666666667">
relation (see below). When they participate in
a proposition they produce proposition in-
stances.
</bodyText>
<subsectionHeader confidence="0.516179">
Propositions
</subsectionHeader>
<bodyText confidence="0.999767666666667">
Following Clark and Harrison (2009) we call
propositions the tuples of words that have some
determined pattern of syntactic relations among
them. We focus on NVN, NVNPN and NPN
proposition types. For example, a NVNPN propo-
sition is a full instantiation of:
</bodyText>
<subsectionHeader confidence="0.852639">
Subject:Verb:Object:Prep:Complement
</subsectionHeader>
<bodyText confidence="0.997886285714286">
The first three elements are the subject, the verb
and the direct object. Fourth is the preposition that
attaches the PP complement to the verb. For sim-
plicity, indirect objects are considered as a Com-
plement with the preposition “to”.
The following are the most frequent NVN
propositions in the BKB ordered by frequency.
</bodyText>
<equation confidence="0.975502769230769">
NVN 2322 &apos;NNP&apos;:&apos;beat&apos;:&apos;NNP&apos;
NVN 2231 &apos;NNP&apos;:&apos;catch&apos;:&apos;pass&apos;
NVN 2093 &apos;NNP&apos;:&apos;throw&apos;:&apos;pass&apos;
NVN 1799 &apos;NNP&apos;:&apos;score&apos;:&apos;touchdown&apos;
NVN 1792 &apos;NNP&apos;:&apos;lead&apos;:&apos;NNP&apos;
NVN 1571 &apos;NNP&apos;:&apos;play&apos;:&apos;NNP&apos;
NVN 1534 &apos;NNP&apos;:&apos;win&apos;:&apos;game&apos;
NVN 1355 &apos;NNP&apos;:&apos;coach&apos;:&apos;NNP&apos;
NVN 1330 &apos;NNP&apos;:&apos;replace&apos;:&apos;NNP&apos;
NVN 1322 &apos;NNP&apos;:&apos;kick&apos;:&apos;goal&apos;
NVN 1195 &apos;NNP&apos;:&apos;win&apos;:&apos;NNP&apos;
NVN 1155 &apos;NNP&apos;:&apos;defeat&apos;:&apos;NNP&apos;
NVN 1103 &apos;NNP&apos;:&apos;gain&apos;:&apos;yard&apos;
</equation>
<bodyText confidence="0.98676375">
The ‘NNP’ tag replaces specific proper nouns
found in the proposition.
When a sentence has more than one comple-
ment, a new occurrence is counted for each com-
plement. For example, given the sentence
“Steve_Walsh threw a pass to Brent_Jones
in the first quarter”, we would add a count to
each of the following propositions:
</bodyText>
<page confidence="0.989596">
17
</page>
<bodyText confidence="0.430654">
Steve_Walsh:throw:pass
</bodyText>
<subsectionHeader confidence="0.411415">
Steve_Walsh:throw:pass:to:Brent_Jones
</subsectionHeader>
<bodyText confidence="0.982120647058824">
Steve_Walsh:throw:pass:in:quarter
Notice that right now we include only the heads
of the noun phrases in the propositions.
We call proposition classes the propositions that
only involve instance classes (e.g., “per-
son:throw:pass”), and proposition instances
those that involve at least one entity instance (e.g.,
“Steve_Walsh:throw:pass”).
Proposition instances are useful for the tracking
of a entity instance. For example,
“&apos;Steve_Walsh&apos;:&apos;supplant&apos;:&apos;John_Fourcade&apos;:
&apos;as&apos;:&apos;quarterback&apos;”. When a proposition in-
stance is found, it is stored also as a proposition
class replacing the proper nouns by a special word
(NNP) to indicate the presence of a entity instance.
The enrichment of the text is based on the use of
most frequent proposition classes.
</bodyText>
<subsectionHeader confidence="0.867028">
Lexical relations
</subsectionHeader>
<bodyText confidence="0.998119333333333">
At the moment, we make use of the copulative
verbs (detected by the Stanford’s parser) in order
to extract “is”, and “has-instance” relations:
</bodyText>
<listItem confidence="0.96974875">
1. Is: between two entity classes. They denote a
kind of identity between both entity classes,
but not in any specific hierarchical relation
such as hyponymy. Neither is a relation of
synonymy. As a result, is somehow a kind of
underspecified relation that groups those more
specific. For example, if we ask the BKB what
a “receiver” is, the most frequent relations are:
</listItem>
<figure confidence="0.62641575">
290 &apos;person&apos;:is:&apos;receiver&apos;
29 &apos;player&apos;:is:&apos;receiver&apos;
16 &apos;pick&apos;:is:&apos;receiver&apos;
15 &apos;one&apos;:is:&apos;receiver&apos;
14 &apos;receiver&apos;:is:&apos;target&apos;
8 &apos;end&apos;:is:&apos;receiver&apos;
7 &apos;back&apos;:is:&apos;receiver&apos;
6 &apos;position&apos;:is:&apos;receiver&apos;
</figure>
<bodyText confidence="0.9969025">
The number indicates the number of times the
relation appears explicitly in the collection.
</bodyText>
<listItem confidence="0.95225">
2. Has-instance: between an entity class and an
</listItem>
<bodyText confidence="0.748635666666667">
entity instance. For example, if we ask for in-
stances of team, the top 10 instances with more
support in the collection are:
</bodyText>
<table confidence="0.572329083333333">
192 &apos;team&apos;:has-instance:&apos;Jets&apos;
189 &apos;team&apos;:has-instance:&apos;Giants&apos;
43 &apos;team&apos;:has-instance:&apos;Eagles&apos;
40 &apos;team&apos;:has-instance:&apos;Bills&apos;
36 &apos;team&apos;:has-instance:&apos;Colts&apos;
35 &apos;team&apos;:has-instance:&apos;Miami&apos;
35 &apos;team&apos;:has-instance:&apos;Vikings&apos;
34 &apos;team&apos;:has-instance:&apos;Cowboys&apos;
32 &apos;team&apos;:has-instance:&apos;Patriots&apos;
31 &apos;team&apos;:has-instance:&apos;Dallas&apos;
But we can ask also for the possible classes of
an instance. For example, all the entity classes for
“Eric_Davis” are:
12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos;
1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos;
1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos;
There are other lexical relations as “part-of” and
“is-value-of” in which we are still working. For
example, the most frequent “is-value-of” relations
are:
5178 &apos;[0- 9]- [0-9]&apos;:is-value-of:&apos;lead&apos;
3996 &apos;[0- 9]- [0-9]&apos;:is-value-of:&apos;record&apos;
2824 &apos;[0- 9]- [0-9]&apos;:is-value-of:&apos;loss&apos;
1225 &apos;[0- 9]- [0-9]&apos;:is-value-of:&apos;season&apos;
</table>
<sectionHeader confidence="0.964179" genericHeader="method">
4 Enrichment procedure
</sectionHeader>
<bodyText confidence="0.99998">
The goal of the enrichment procedure is to deter-
mine what kind of events and entities are involved
in the text, and what semantic relations are hidden
by some syntactic dependencies such as noun-noun
compound or some prepositions.
</bodyText>
<subsectionHeader confidence="0.999331">
4.1 Fusion of nodes
</subsectionHeader>
<bodyText confidence="0.999982086956522">
Sometimes, the syntactic dependency ties two or
more words that form a single concept. This is the
case with multiword terms such as “tight end”,
“field goal”, “running back”, etc. In these cases,
the meaning of the compound is beyond the syn-
tactic dependency. Thus, we shouldn’t look for its
explicit meaning. Instead, we activate the fusion of
the nodes into a single one.
However, there are some open issues related to
the cases were fusion is not preferred. Otherwise,
the process could be done with standard measures
like mutual information, before the parsing step
(and possibly improving its results).
The question is whether the fusion of the words
into a single expression allows or not the consid-
eration of possible paraphrases. For example, in
the case of “field:nn:goal”, we don’t find other
ways to express the concept in the BKB. However,
in the case of “touchdown:nn:pass” we can find,
for example, “pass:for:touchdown” a significant
amount of times, and we want to identify them as
equivalent expressions. For this reason, we find not
convenient to fuse these cases.
</bodyText>
<page confidence="0.994889">
18
</page>
<subsectionHeader confidence="0.994132">
4.2 Building context for instances
</subsectionHeader>
<bodyText confidence="0.998916428571428">
Suppose we wish to determine what kind of entity
“Steve Walsh” is in the context of the syntactic
dependency “Steve_Walsh:nn:pass”. First, we
look into the BKB for the possible entity classes of
Steve_Walsh previously found in the collection. In
this particular case, the most frequent class is
“quarterback”:
</bodyText>
<equation confidence="0.9436915">
40 &apos;quarterback&apos;:has-instance:&apos;Steve_Walsh&apos;
2 &apos;junior&apos;:has-instance:&apos;Steve_Walsh&apos;
</equation>
<bodyText confidence="0.999762">
But, what happens if we see “Steve_Walsh” for
the first time? Then we need to find evidence from
other entities in the same syntactic context. We
found that “Marino”, “Kelly”, “Elway”,
“Dan_Marino”, etc. appear in the same kind of
proposition (“N:nn:pass”) where we found
“Steve_Walsh”, each of them supported by 24, 17,
15 and 10 occurrences respectively. However,
some of the names can be ambiguous. For exam-
ple, searching for “Kelly” in our BKB yields:
</bodyText>
<figure confidence="0.992584454545455">
153 &apos;quarterback&apos;:has-instance:&apos;Jim_Kelly&apos;
19 &apos;linebacker&apos;:has-instance:&apos;Joe_Kelly&apos;
17 &apos;quarterback&apos;:has-instance:&apos;Kelly&apos;
14 &apos;quarterback&apos;:has-instance:&apos;Kelly_Stouffer&apos;
10 &apos;quarterback&apos;:has-instance:&apos;Kelly_Ryan&apos;
8 &apos;quarterback&apos;:has-instance:&apos;Kelly_Holcomb&apos;
7 &apos;cornerback&apos;:has-instance:&apos;Brian_Kelly&apos;
Whereas others are not so ambiguous:
113 &apos;quarterback&apos;:has-instance:&apos;Dan_Marino&apos;
6 &apos;passer&apos;:has-instance:&apos;Dan_Marino&apos;
5 &apos;player&apos;:has-instance:&apos;Dan_Marino&apos;
</figure>
<bodyText confidence="0.953461333333333">
Taking this into account, we are able to infer that
the most plausible class for an entity involved in a
“NNP:nn:pass” proposition is a quarterback.
</bodyText>
<subsectionHeader confidence="0.999685">
4.3 Building context for dependencies
</subsectionHeader>
<bodyText confidence="0.9999575">
Now we want to determine the meaning behind
such syntactic dependencies as
“Steve_Walsh:nn:pass”, “touchdown:nn:pass“,
“Young:nn:pass” or “pass:to:Brent_Jones”.
We have two ways for adding more meaning to
these syntactic dependencies: find the most appro-
priate prepositions to describe them, and find the
most appropriate verbs. Whether one, the other or
both is more useful has to be determined during the
reasoning system development.
</bodyText>
<subsectionHeader confidence="0.501627">
Finding the prepositions
</subsectionHeader>
<bodyText confidence="0.9999706">
There are several types of propositions in the
BKB that involve prepositions. The most relevant
are NPN and NVNPN. In the case of “touch-
down:nn:pass”, preposition “for” is clearly the best
interpretation for the “nn” dependency:
</bodyText>
<equation confidence="0.751231">
NPN 712 &apos;pass&apos;:&apos;for&apos;:&apos;touchdown&apos;
NPN 24 &apos;pass&apos;:&apos;include&apos;:&apos;touchdown&apos;
NPN 3 &apos;pass&apos;:&apos;with&apos;:&apos;touchdown&apos;
NPN 2 &apos;pass&apos;:&apos;of&apos;:&apos;touchdown&apos;
NPN 1 &apos;pass&apos;:&apos;in&apos;:&apos;touchdown&apos;
NPN 1 &apos;pass&apos;:&apos;follow&apos;:&apos;touchdown&apos;
NPN 1 &apos;pass&apos;:&apos;to&apos;:&apos;touchdown&apos;
</equation>
<bodyText confidence="0.9821195">
In the case of “Steve_Walsh:nn:pass” and
“Young:nn:pass”, assuming they are quarterbacks,
we can ask for all the prepositions between “pass”
and “quarterback”:
</bodyText>
<equation confidence="0.9842842">
NPN 23 &apos;pass&apos;:&apos;from&apos;:&apos;quarterback&apos;
NPN 14 &apos;pass&apos;:&apos;by&apos;:&apos;quarterback&apos;
NPN 2 &apos;pass&apos;:&apos;of&apos;:&apos;quarterback&apos;
NPN 1 &apos;pass&apos;:&apos;than&apos;:&apos;quarterback&apos;
NPN 1 &apos;pass&apos;:&apos;to&apos;:&apos;quarterback&apos;
</equation>
<bodyText confidence="0.972319">
Notice how lower frequencies involve more
noisy options.
If we don’t have any evidence on the instance
class, and we know only that they are instances,
the pertinent query to the BKB obtains:
</bodyText>
<equation confidence="0.98529775">
NPN 1305 &apos;pass&apos;:&apos;to&apos;:&apos;NNP&apos;
NPN 1085 &apos;pass&apos;:&apos;from&apos;:&apos;NNP&apos;
NPN 147 &apos;pass&apos;:&apos;by&apos;:&apos;NNP&apos;
NPN 144 &apos;pass&apos;:&apos;for&apos;:&apos;NNP&apos;
</equation>
<bodyText confidence="0.9989050625">
In the case of “Young:nn:pass” (in “Young
pass to Brent Jones”), there exists already the
preposition “to” (“pass:to:Brent_Jones”), so the
most promising choice become the second,
“pass:from:Young”, which has one order of magni-
tude more occurrences than the following.
In the case of “Steve_Walsh:nn:pass” (in “Eric
Davis intercepted a Steve Walsh pass”) we can use
additional information: we know that
“Eric_Davis:intercept:pass”. So, we can try to
find the appropriate preposition using NVNPN
propositions in the following way:
Eric_Davis:intercept:pass:P:Steve_Walsh”
Asking the BKB about the propositions that in-
volve two instances with “intercept” and “pass” we
get:
</bodyText>
<footnote confidence="0.643623111111111">
NVNPN 48 &apos;NNP&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;by&apos;:&apos;NNP&apos;
NVNPN 26 &apos;NNP&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;at&apos;:&apos;NNP&apos;
NVNPN 12 &apos;NNP&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;from&apos;:&apos;NNP&apos;
We could also query the BKB with the classes
we already found for “Eric_Davis” (cornerback,
player, person):
NVNPN 11 &apos;person&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;by&apos;:&apos;NNP&apos;
NVNPN 4 &apos;person&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;at&apos;:&apos;NNP&apos;
NVNPN 2 &apos;person&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;in&apos;:&apos;NNP&apos;
</footnote>
<page confidence="0.994436">
19
</page>
<equation confidence="0.2426295">
NVNPN 2 &apos;person&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;against&apos;:&apos;NNP&apos;
NVNPN 1 &apos;cornerback&apos;:&apos;intercept&apos;:&apos;pass&apos;:&apos;by&apos;:&apos;NNP&apos;
</equation>
<bodyText confidence="0.9996495">
All these queries accumulate evidence over a cor-
rect preposition “by” (“pass:by:Steve_Walsh”).
However, an explicit entity classification would
make the procedure more robust.
</bodyText>
<subsectionHeader confidence="0.517563">
Finding the verbs
</subsectionHeader>
<bodyText confidence="0.999943166666667">
Now the exercise is to find a verb able to give
meaning to the syntactic dependencies such as
“Steve_Walsh:nn:pass”, “touchdown:nn:pass“,
“Young:nn:pass” or “pass:to:Brent_Jones”.
We can ask the BKB what instances (NNP) do
with passes. The most frequent propositions are:
</bodyText>
<equation confidence="0.918044">
NVN 2241 &apos;NNP&apos;:&apos;catch&apos;:&apos;pass&apos;
NVN 2106 &apos;NNP&apos;:&apos;throw&apos;:&apos;pass&apos;
NVN 844 &apos;NNP&apos;:&apos;complete&apos;:&apos;pass&apos;
NVN 434 &apos;NNP&apos;:&apos;intercept&apos;:&apos;pass&apos;
NVNPN 758 &apos;NNP&apos;:&apos;throw&apos;:&apos;pass&apos;:&apos;to&apos;:&apos;NNP&apos;
NVNPN 562 &apos;NNP&apos;:&apos;catch&apos;:&apos;pass&apos;:&apos;for&apos;:&apos;yard&apos;
NVNPN 338 &apos;NNP&apos;:&apos;complete&apos;:&apos;pass&apos;:&apos;to&apos;:&apos;NNP&apos;
NVNPN 255 &apos;NNP&apos;:&apos;catch&apos;:&apos;pass&apos;:&apos;from&apos;:&apos;NNP&apos;
</equation>
<bodyText confidence="0.99967075">
Considering the evidence of “Brent_Jones” be-
ing instance of “end” (tight end), if we ask the
BKB about the most frequent relations between
“end” and “pass” we find:
</bodyText>
<equation confidence="0.4660865">
NVN 28 &apos;end&apos;:&apos;catch&apos;:&apos;pass&apos;
NVN 6 &apos;end&apos;:&apos;drop&apos;:&apos;pass&apos;
</equation>
<bodyText confidence="0.999887777777778">
So, in this case, the BKB suggests that the syn-
tactic dependency “pass:to:Brent_Jones” means
“Brent_Jones is an end catching a pass”. Or in
other words, that “Brent_Jones” has a role of
“catch-ER” with respect to “pass”.
If we want to accumulate more evidence on this
we can consider NVNPN propositions including
touchdown. We only find evidence for the most
general classes (NNP and person):
</bodyText>
<equation confidence="0.9545315">
NVNPN 189 &apos;NNP&apos;:&apos;catch&apos;:&apos;pass&apos;:&apos;for&apos;:&apos;touchdown&apos;
NVNPN 26 &apos;NNP&apos;:&apos;complete&apos;:&apos;pass&apos;:&apos;for&apos;:&apos;touchdown&apos;
NVNPN 84 &apos;person&apos;:&apos;catch&apos;:&apos;pass&apos;:&apos;for&apos;:&apos;touchdown&apos;
NVNPN 18 &apos;person&apos;:&apos;complete&apos;:&apos;pass&apos;:&apos;for&apos;:&apos;touchdown&apos;
</equation>
<bodyText confidence="0.999101714285714">
This means, that when we have “touchdown”,
we don’t have counting for the second option
“Brent_Jones:drop:pass”, while “catch” becomes
stronger.
In the case of “Steve_Walsh:nn:pass” we hy-
pothesize that “Steve_Walsh” is a quarterback.
Asking the BKB about the most plausible relation
</bodyText>
<figureCaption confidence="0.975638">
Figure 2. Graphical representation of the enriched
</figureCaption>
<bodyText confidence="0.9075895">
between a quarterback and a pass we find:
text.
</bodyText>
<page confidence="0.939924">
20
</page>
<note confidence="0.575728">
NVN 98 &apos;quarterback&apos;:&apos;throw&apos;:&apos;pass&apos;
NVN 27 &apos;quarterback&apos;:&apos;complete&apos;:&apos;pass&apos;
</note>
<bodyText confidence="0.997362111111111">
Again, if we take into account that it is a
“touchdown:nn:pass”, then only the second op-
tion “Steve_Walsh:complete:pass” is consistent
with the NVNPN propositions.
So, in this case, the BKB suggests that the syn-
tactic dependency “Steve_Walsh:nn:pass” means
“Steve_Walsh is a quarterback completing a pass”.
Finally, with respect to “touchdown:nn:pass“,
we can ask about the verbs that relate them:
</bodyText>
<footnote confidence="0.648198">
NVN 14 &apos;pass&apos;:&apos;set_up&apos;:&apos;touchdown&apos;
NVN 6 &apos;pass&apos;:&apos;score&apos;:&apos;touchdown&apos;
NVN 5 &apos;pass&apos;:&apos;produce&apos;:&apos;touchdown&apos;
</footnote>
<figureCaption confidence="0.815189">
Figure 2 shows the graphical representation of
the sentence after some enrichment.
</figureCaption>
<subsectionHeader confidence="0.999127">
4.4 Expansion of relations
</subsectionHeader>
<bodyText confidence="0.9998297">
Sometimes, the sentence shows a verb with several
arguments. In our example, we have
“Eric_David:intercept:pass:on:series”. In
these cases, the relation can be expanded and be-
come a node.
In our example, the new node is the eventuality
of “intercept” (let’s say “intercept-ION”),
“Eric_Davis” is the “intercept-ER” and “pass” is
the “intercept-ED”. Then, we can attach the miss-
ing information to the new node (see Figure 3).
</bodyText>
<figureCaption confidence="0.996016">
Figure 3. Expansion of the &amp;quot;intercept&amp;quot; relation.
</figureCaption>
<bodyText confidence="0.999925411764706">
In addition, we can proceed with the expansion
of the context considering this new node. For ex-
ample, we are working with the hypothesis that
“Steve_Walsh” is an instance of quarterback and
thus, its most plausible relations with pass are
“throw” and “complete”. However, now we can
ask about the most frequent relation between
“quarterback” and “interception”. The most fre-
quent is “quarterback:throw:interception”
supported 35 times in the collection. From this,
two actions can be done: reinforce the hypothesis
of “throw:pass” instead of “complete:pass”, and
add the hypothesis that
“Steve_Walsh:throw:interception”.
Finally, notice that since “set_up” doesn’t need
to accommodate more arguments, we can maintain
the collapsed edge.
</bodyText>
<subsectionHeader confidence="0.997138">
4.5 Constraining the interpretations
</subsectionHeader>
<bodyText confidence="0.999588538461538">
Some of the inferences being performed are local
in the sense that they involve only an entity and a
relation. However, these local inferences must be
coherent both with the sentence and the complete
document.
To ensure this coherence we can use additional
information as a way to constrain different hy-
potheses. In section 4.3 we showed the use of
NVNPN propositions to constrain NVN ones.
Another example is the case of
“Eric_Davis:intercept:pass”. We can ask the
BKB for the entity classes that participate in such
kind of proposition:
</bodyText>
<equation confidence="0.788923166666667">
NVN 75 &apos;person&apos;:&apos;intercept&apos;:&apos;pass&apos;
NVN 14 &apos;cornerback&apos;:&apos;intercept&apos;:&apos;pass&apos;
NVN 11 &apos;defense&apos;:&apos;intercept&apos;:&apos;pass&apos;
NVN 8 &apos;safety&apos;:&apos;intercept&apos;:&apos;pass&apos;
NVN 7 &apos;group&apos;:&apos;intercept&apos;:&apos;pass&apos;
NVN 5 &apos;linebacker&apos;:&apos;intercept&apos;:&apos;pass&apos;
</equation>
<bodyText confidence="0.999956333333333">
So the local inference for the kind of entity
“Eric_Davis” is (cornerback) must be coherent
with the fact that it intercepted a pass. In this case
“cornerback” and “person” are properly reinforced.
In some sense, we are using these additional con-
strains as shallow selectional preferences.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999880357142857">
The evaluation of the enrichment process is a chal-
lenge by itself. Eventually, we will use extrinsic
measures such as system performance on a QA
task, applied first after reading a text, and then a
second time after the enrichment process. This will
measure the ability of the system to absorb and use
knowledge across texts to enrich the interpretation
of the target text. In the near term, however, it re-
mains unclear which intrinsic evaluation measures
to apply. It is not informative simply to count the
number of additional relations one can attach to
representation elements, or to count the increase in
degree of interlinking of the nodes in the represen-
tation of a paragraph.
</bodyText>
<page confidence="0.998848">
21
</page>
<sectionHeader confidence="0.99992" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9431469">
To build the knowledge base we take an approach
closely related to DART (Clark and Harrison,
2009) which in turn is related to KNEXT (Van
Durme and Schubert, 2008). It is also more dis-
tantly related to TextRunner (Banko et al. 2007).
Like DART, we make use of a dependency
parser instead of partial parsing. So we capture
phrase heads instead complete phrases. The main
differences between the generation of our BKB
and the generation of DART are:
1. We use the dependencies involving copula-
tive verbs as a source of evidence for “is”
and “has-instance” relations.
2. Instead of replacing proper nouns by “per-
son”, “place”, or “organization”, we con-
sider all of them just as instances in our
BKB. Furthermore, when a proposition con-
tains a proper noun, we count it twice: one
as the original proposition instance, and a
second replacing the proper nouns with a
generic tag indicating that there was a name.
3. We make use of the modifiers that involve
an instance (proper noun) to add counting to
the “has-instance” relation.
4. Instead of replacing pronouns by “person”
or “thing”, we replace them by “person”,
“group” or “thing”, taking advantage of the
preposition number. This is particular useful
for the domain of football where players and
teams are central.
</bodyText>
<listItem confidence="0.543553375">
5. We add a new set of propositions that relate
two clauses in the same sentence (e.g.,
Floyd:break:takle:add:touchdown). We
tagged these propositions NVV, NVNV,
NVVN and NVNVN.
6. Instead of an unrestricted domain collection,
we consider documents closely related to the
domain in which we want to interpret texts.
</listItem>
<bodyText confidence="0.999960666666667">
The consideration of a specific domain collec-
tion seems a very powerful option. Ambiguity is
reduced inside a domain so the counting for propo-
sitions is more robust. Also frequency distribution
of propositions is different from one domain into
another. For example, the list of the most frequent
NVN propositions in our BKB (see Section 3.1) is,
by itself, an indication of the most salient and im-
portant events in the American football domain.
</bodyText>
<sectionHeader confidence="0.974544" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999971235294118">
The task of inferring omitted but necessary infor-
mation is a significant part of automated text inter-
pretation. In this paper we show that even simple
kinds of information, gleaned relatively straight-
forwardly from a parsed corpus, can be quite use-
ful. Though they are still lexical and not even
starting to be semantic, propositions consisting of
verbs as relations between nouns seem to provide a
surprising amount of utility. It remains a research
problem to determine what kinds and levels of
knowledge are most useful in the long run.
In the paper, we discuss only the propositions
that are grounded in instantial statements about
players and events. But for true learning by read-
ing, a system has to be able to recognize when the
input expresses general rules, and to formulate
such input as axioms or inferences. In addition,
augmenting that is the significant challenge of
generalizing certain kinds of instantial propositions
to produce inferences. At which point, for exam-
ple, should the system decide that “all football
players have teams”, and how should it do so?
How to do so remains a topic for future work.
A further topic of investigation is the time at
which expansion should occur. Doing so at ques-
tion time, in the manner of traditional task-oriented
back-chaining inference, is the obvious choice, but
some limited amount of forward chaining at read-
ing time seems appropriate too, especially if it can
significantly assist with text processing tasks, in
the manner of expectation-driven understanding.
Finally, as discussed above, the evaluation of
our reading augmentation procedures remains to be
developed.
</bodyText>
<sectionHeader confidence="0.998359" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987275">
We are grateful to Hans Chalupsky and David
Farwell for their comments and input along this
work. This work has been partially supported by
the Spanish Government through the &amp;quot;Programa
Nacional de Movilidad de Recursos Humanos del
Plan Nacional de I+D+i 2008-2011 (Grant
PR2009-0020). We acknowledge the support of
DARPA contract number: FA8750-09-C-0172.
</bodyText>
<page confidence="0.994702">
22
</page>
<sectionHeader confidence="0.995905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999963848484848">
1. Banko, M., Cafarella, M., Soderland, S.,
Broadhead, M., Etzioni, O. 2007. Open Infor-
mation Extraction from the Web. IJCAI 2007.
2. Barker, K. 2007. Building Models by Reading
Texts. Invited talk at the AAAI 2007 Spring
Symposium on Machine Reading, Stanford
University.
3. Clark, P. and Harrison, P. 2009. Large-scale
extraction and use of knowledge from text.
The Fifth International Conference on Knowl-
edge Capture (K-CAP 2009).
http://www.cs.utexas.edu/users/pclark/dart/
4. Hobbs, J.R., Stickel, M., Appelt, D. and Mar-
tin, P., 1993. Interpretation as Abduction. Arti-
ficial Intelligence, Vol. 63, Nos. 1-2, pp. 69-
142.
http://www.isi.edu/~hobbs/interp-abduct-ai.pdf
5. Klein, D. and Manning, C.D. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational
Linguistics, pp. 423-430
6. Marneffe, M. and Manning, C.D. 2008. The
Stanford typed dependencies representation. In
COLING 2008 Workshop on Cross-framework
and Cross-domain Parser Evaluation.
7. Sperber, D. and Wilson, D. 1995. Relevance:
Communication and cognition (2nd ed.) Ox-
ford, Blackwell.
8. Van Durme, B., Schubert, L. 2008. Open
Knowledge Extraction through Compositional
Language Processing. Symposium on Seman-
tics in Systems for Text Processing, STEP
2008.
</reference>
<page confidence="0.998937">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936917">
<title confidence="0.999182">Semantic Enrichment of Text with Background Knowledge</title>
<author confidence="0.994512">Anselmo Perias Eduard Hovy</author>
<affiliation confidence="0.997305">UNED NLP &amp; IR Group USC Information Sciences Institute</affiliation>
<address confidence="0.9931785">Juan del Rosal, 16 4676 Admiralty Way 28040 Madrid, Spain Marina del Rey, CA 90292-6695</address>
<email confidence="0.996972">anselmo@lsi.uned.eshovy@isi.edu</email>
<abstract confidence="0.994684125">Texts are replete with gaps, information omitted since authors assume a certain amount of background knowledge. We describe the kind of information (the formalism and methods to derive the content) useful for automated filling of such gaps. We describe a stepwise procedure with a detailed example.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<date>2007</date>
<booktitle>Open Information Extraction from the Web. IJCAI</booktitle>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>1.</marker>
<rawString>Banko, M., Cafarella, M., Soderland, S., Broadhead, M., Etzioni, O. 2007. Open Information Extraction from the Web. IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barker</author>
</authors>
<title>Building Models by Reading Texts.</title>
<date>2007</date>
<booktitle>Invited talk at the AAAI 2007 Spring Symposium on Machine Reading,</booktitle>
<institution>Stanford University.</institution>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>2.</marker>
<rawString>Barker, K. 2007. Building Models by Reading Texts. Invited talk at the AAAI 2007 Spring Symposium on Machine Reading, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
<author>P Harrison</author>
</authors>
<title>Large-scale extraction and use of knowledge from text.</title>
<date>2009</date>
<booktitle>The Fifth International Conference on Knowledge Capture (K-CAP</booktitle>
<note>http://www.cs.utexas.edu/users/pclark/dart/</note>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>3.</marker>
<rawString>Clark, P. and Harrison, P. 2009. Large-scale extraction and use of knowledge from text. The Fifth International Conference on Knowledge Capture (K-CAP 2009). http://www.cs.utexas.edu/users/pclark/dart/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>M Stickel</author>
<author>D Appelt</author>
<author>P Martin</author>
</authors>
<title>Interpretation as Abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>63</volume>
<pages>69</pages>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>4.</marker>
<rawString>Hobbs, J.R., Stickel, M., Appelt, D. and Martin, P., 1993. Interpretation as Abduction. Artificial Intelligence, Vol. 63, Nos. 1-2, pp. 69-</rawString>
</citation>
<citation valid="false">
<note>http://www.isi.edu/~hobbs/interp-abduct-ai.pdf</note>
<marker>142.</marker>
<rawString> http://www.isi.edu/~hobbs/interp-abduct-ai.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>5.</marker>
<rawString>Klein, D. and Manning, C.D. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marneffe</author>
<author>C D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</booktitle>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>6.</marker>
<rawString>Marneffe, M. and Manning, C.D. 2008. The Stanford typed dependencies representation. In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sperber</author>
<author>D Wilson</author>
</authors>
<title>Relevance: Communication and cognition (2nd ed.)</title>
<date>1995</date>
<location>Oxford, Blackwell.</location>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>7.</marker>
<rawString>Sperber, D. and Wilson, D. 1995. Relevance: Communication and cognition (2nd ed.) Oxford, Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>L Schubert</author>
</authors>
<title>Open Knowledge Extraction through Compositional Language Processing.</title>
<date>2008</date>
<booktitle>Symposium on Semantics in Systems for Text Processing, STEP</booktitle>
<contexts>
<context position="14020" citStr="[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]" startWordPosition="2138" endWordPosition="2138">lls&apos; 36 &apos;team&apos;:has-instance:&apos;Colts&apos; 35 &apos;team&apos;:has-instance:&apos;Miami&apos; 35 &apos;team&apos;:has-instance:&apos;Vikings&apos; 34 &apos;team&apos;:has-instance:&apos;Cowboys&apos; 32 &apos;team&apos;:has-instance:&apos;Patriots&apos; 31 &apos;team&apos;:has-instance:&apos;Dallas&apos; But we can ask also for the possible classes of an instance. For example, all the entity classes for “Eric_Davis” are: 12 &apos;cornerback&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;hand&apos;:has-instance:&apos;Eric_Davis&apos; 1 &apos;back&apos;:has-instance:&apos;Eric_Davis&apos; There are other lexical relations as “part-of” and “is-value-of” in which we are still working. For example, the most frequent “is-value-of” relations are: 5178 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;lead&apos; 3996 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;record&apos; 2824 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;loss&apos; 1225 &apos;[0- 9]- [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&apos;:is-value-of:&apos;season&apos; 4 Enrichment procedure The goal of the enrichment procedure is to determine what kind of events and entities are involved in the text, and what semantic relations are hidden by some syntactic dependencies such as noun-noun compound or some prepositions. 4.1 Fusion of nodes Sometimes, the syntactic dependency ties two or more words that form a single concept. This is the case wi</context>
</contexts>
<marker>8.</marker>
<rawString>Van Durme, B., Schubert, L. 2008. Open Knowledge Extraction through Compositional Language Processing. Symposium on Semantics in Systems for Text Processing, STEP</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>