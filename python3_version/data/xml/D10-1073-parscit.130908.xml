<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000188">
<title confidence="0.997334">
Word Sense Induction &amp; Disambiguation Using
Hierarchical Random Graphs
</title>
<author confidence="0.968583">
Ioannis P. Klapaftis
</author>
<affiliation confidence="0.998146">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.66728">
United Kingdom
</address>
<email confidence="0.996547">
giannis@cs.york.ac.uk
</email>
<author confidence="0.990145">
Suresh Manandhar
</author>
<affiliation confidence="0.998377">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.668217">
United Kingdom
</address>
<email confidence="0.99811">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99892535">
Graph-based methods have gained attention in
many areas of Natural Language Processing
(NLP) including Word Sense Disambiguation
(WSD), text summarization, keyword extrac-
tion and others. Most of the work in these ar-
eas formulate their problem in a graph-based
setting and apply unsupervised graph cluster-
ing to obtain a set of clusters. Recent studies
suggest that graphs often exhibit a hierarchi-
cal structure that goes beyond simple flat clus-
tering. This paper presents an unsupervised
method for inferring the hierarchical group-
ing of the senses of a polysemous word. The
inferred hierarchical structures are applied to
the problem of word sense disambiguation,
where we show that our method performs sig-
nificantly better than traditional graph-based
methods and agglomerative clustering yield-
ing improvements over state-of-the-art WSD
systems based on sense induction.
</bodyText>
<sectionHeader confidence="0.999123" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814782608696">
A number of NLP problems can be cast into a graph-
based framework, in which entities are represented
as vertices in a graph and relations between them are
depicted by weighted or unweighted edges. For in-
stance, in unsupervised WSD a number of methods
(Widdows and Dorow, 2002; V´eronis, 2004; Agirre
et al., 2006) have constructed word co-occurrence
graphs for a target polysemous word and applied
graph-clustering to obtain the clusters (senses) of
that word.
Similarly in text summarization, Mihalcea (2004)
developed a method, in which sentences are rep-
resented as vertices in a graph and edges between
them are drawn according to their common tokens
or words of a given POS category, e.g. nouns.
Graph-based ranking algorithms, such as PageRank
(Brin and Page, 1998), were then applied in order
to determine the significance of sentences. In the
same vein, graph-based methods have been applied
to other problems such as determining semantic sim-
ilarity of text (Ramage et al., 2009).
Recent studies (Clauset et al., 2006; Clauset et
al., 2008) suggest that graphs exhibit a hierarchi-
cal structure (e.g. a binary tree), in which vertices
are divided into groups that are further subdivided
into groups of groups, and so on, until we reach the
leaves. This hierarchical structure provides addi-
tional information as opposed to flat clustering by
explicitly including organisation at all scales of a
graph (Clauset et al., 2008). In this paper, we present
an unsupervised method for inferring the hierarchi-
cal structure (binary tree) of a graph, in which ver-
tices are the contexts of a polysemous word and
edges represent the similarity between contexts. The
method that we use to infer that hierarchical struc-
ture is the Hierarchical Random Graphs (HRGs) al-
gorithm due to Clauset et al. (2008).
The binary tree produced by our method groups
the contexts of a polysemous word at different
heights of the tree. Thus, it induces the senses of
that word at different levels of sense granularity. To
evaluate our method, we apply it to the problem of
noun sense disambiguation showing that inferring
the hierarchical structure using HRGs provides ad-
ditional information from the observed graph lead-
ing to improved WSD performance compared to: (1)
</bodyText>
<page confidence="0.976568">
745
</page>
<note confidence="0.9210355">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745–755,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999897">
Figure 1: Stages of the proposed method.
</figureCaption>
<bodyText confidence="0.999733">
simple flat clustering, and (2) traditional agglomera-
tive clustering. Finally, we compare our results with
state-of-the-art sense induction systems and show
that our method yields improvements. Figure 1
shows the different stages of the proposed method
that we describe in the following sections.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99826696969697">
Typically, graph-based methods, when applied to
unsupervised sense disambiguation represent each
word wi co-occurring with the target word tw as a
vertex. Two vertices are connected via an edge if
they co-occur in one or more contexts of tw. Once
the co-occurrence graph of tw has been constructed,
different graph clustering algorithms are applied to
induce the senses. Each cluster (induced sense) con-
sists of a set of words that are semantically related to
the particular sense. Figure 2 shows an example of
a graph for the target word paper that appears with
two different senses scholarly article and newspa-
per.
V´eronis (2004) has shown that co-occurrence
graphs are small-world networks that contain highly
dense subgraphs representing the different clusters
(senses) of the target word (V´eronis, 2004). To iden-
tify these dense regions V´eronis’s algorithm itera-
tively finds their hubs, where a hub is a vertex with a
very high degree. The degree of a vertex is defined to
be the number of edges incident to that vertex. The
identified hub is then deleted along with its direct
neighbours from the graph producing a new cluster.
For example, in Figure 2 the highest degree ver-
tex, news, is the first hub, which would be deleted
along with its direct neighbours. The deleted re-
gion corresponds to the newspaper sense of the tar-
get word paper. V´eronis (2004) further processed
the identified clusters (senses), in order to assign the
rest of graph vertices to the identified clusters by
utilising the minimum spanning tree of the original
graph.
In Agirre et al. (2006), the algorithm of V´eronis
(2004) is analysed and assessed on the SensEval-3
dataset (Snyder and Palmer, 2004), after optimis-
ing its parameters on the SensEval-2 dataset (Ed-
monds and Dorow, 2001). The results show that the
WSD F-Score outperforms the Most Frequent Sense
(MFS) baseline by approximately 10%, while induc-
ing a large number of clusters (with averages of 60
to 70).
Another graph-based method is presented in
(Dorow and Widdows, 2003). They extract only
noun neighbours that appear in conjunctions or dis-
junctions with the target word. Additionally, they
extract second-order co-occurrences. Nouns are rep-
resented as vertices, while edges between vertices
are drawn, if their associated nouns co-occur in con-
junctions or disjunctions more than a given num-
ber of times. This co-occurrence frequency is also
used to weight the edges. The resulting graph is
then pruned by removing the target word and ver-
tices with a low degree. Finally, the MCL algorithm
(Dongen, 2000) is used to cluster the graph and pro-
duce a set of clusters (senses) each one consisting of
a set of contextually related words.
Chinese Whispers (CW) (Biemann, 2006) is a
parameter-free1 graph clustering method that has
been applied in sense induction to cluster the co-
occurrence graph of a target word (Biemann, 2006),
as well as a graph of collocations related to the tar-
get word (Klapaftis and Manandhar, 2008). The
evaluation of the collocational-graph method in the
SemEval-2007 sense induction task (Agirre and
Soroa, 2007) showed promising results.
All the described methods for sense induction ap-
</bodyText>
<footnote confidence="0.818415">
1One needs to specify only the number of iterations. The
number of clusters is generated automatically.
</footnote>
<page confidence="0.996913">
746
</page>
<figureCaption confidence="0.999131333333333">
Figure 2: Graph of words for the target word paper.
Numbers inside vertices correspond to their degree.
Figure 3: Running example of graph creation
</figureCaption>
<bodyText confidence="0.999924">
ply flat graph clustering methods to derive the clus-
ters (senses) of a target word. As a result, they ne-
glect the fact that their constructed graphs often ex-
hibit a hierarchical structure that is useful in several
tasks including word sense disambiguation.
</bodyText>
<sectionHeader confidence="0.622118" genericHeader="method">
3 Building a graph of contexts
</sectionHeader>
<bodyText confidence="0.999989285714286">
This section describes the process of creating a
graph of contexts for a polysemous target word. Fig-
ure 3 provides a running example of the different
stages of our method. In the example, the target
word paper appears with the scholarly article sense
in the contexts A, B, and with the newspaper sense
in the contexts C and D.
</bodyText>
<subsectionHeader confidence="0.999827">
3.1 Corpus preprocessing
</subsectionHeader>
<bodyText confidence="0.999968235294118">
Let bc denote the base corpus consisting of the con-
texts containing the target word tw. In our work,
a context is defined as a paragraph2 containing the
target word.
The aim of this stage is to capture nouns contex-
tually related to tw. Initially, the target word is re-
moved from bc and part-of-speech tagging is applied
to each context. Following the work in (V´eronis,
2004; Agirre et al., 2006) only nouns are kept and
lemmatised. In the next step, the distribution of each
noun in the base corpus is compared to the distri-
bution of the same noun in a reference corpus3 us-
ing the log-likelihood ratio (G2) (Dunning, 1993).
Nouns with a G2 below a pre-specified threshold
(parameter p1) are removed from each paragraph of
the base corpus. The upper left part of Figure 3
shows the words kept as a result of this stage.
</bodyText>
<subsectionHeader confidence="0.99895">
3.2 Graph creation
</subsectionHeader>
<bodyText confidence="0.9990574">
Graph vertices: To create the graph of vertices, we
represent each context ci as a vertex in a graph G.
Graph edges: Edges between the vertices of the
graph are drawn based on their similarity, defined
in Equation 1, where simcl(ci, cj) is the colloca-
tional weight of contexts ci, cj and simwd(ci, cj)
is their bag-of-words weight. If the edge weight
W (ci, cj) is above a prespecified threshold (param-
eter p3), then an edge is drawn between the corre-
sponding vertices in the graph.
</bodyText>
<equation confidence="0.998359">
1
W (ci, cj) = 2(simcl(ci, cj) + simwd(ci, cj)) (1)
</equation>
<bodyText confidence="0.998656">
Collocational weight: The limited polysemy of col-
locations can be exploited to compute the similarity
between contexts ci and cj. In our setting, a colloca-
tion is a juxtaposition of two nouns within the same
context. Thus, given a context ci, each of its nouns
is combined with any other noun yielding a total of
collocations for a context with N nouns. Each
collocation, clij is weighted using the log-likelihood
ratio (G2) (Dunning, 1993) and is filtered out if the
G2 is below a prespecified threshold (parameter p2).
At the end of this process, each context ci of tw is
associated with a vector of collocations (vi). The
upper right part of Figure 3 shows the collocations
associated with each context of our example.
</bodyText>
<footnote confidence="0.9302716">
2Our definition of context is equivalent to an instance of the
target word in the SemEval-2007 sense induction task dataset
(Agirre and Soroa, 2007).
3The British National Corpus, 2001, Distributed by Oxford
University Computing Services.
</footnote>
<equation confidence="0.764882">
(N )
2
</equation>
<page confidence="0.982417">
747
</page>
<bodyText confidence="0.981736170731707">
Given two contexts ci and cj, we calculate their
collocational weight using the Jaccard coefficient
on the collocational vectors, i.e. simj(ci, cj) =
|vi∩vj|The selection of Jaccard is based on the work
vi∪vj |
of Weeds et al. (2004), who analyzed the variation
in a word’s distributionally nearest neighbours with
respect to a variety of similarity measures. Their
analysis showed that there are three classes of mea-
sures, i.e. those selecting distributionally more gen-
eral neighbours (e.g. cosine), those selecting distri-
butionally less general neighbours (e.g. AMCRM-
Precision (Weeds et al., 2004)) and those without a
bias towards the distributional generality of a neigh-
bour (e.g. Jaccard). In our setting, we are interested
in calculating the similarity between two contexts
without any bias. We selected Jaccard, since the rest
of that class’s measures are based on pointwise mu-
tual information that assigns high weights to infre-
quent events.
Bag-of-words weight: Estimating context similar-
ity using collocations may provide reliable estimates
regarding the existence of an edge in the graph, how-
ever, it also suffers from data sparsity. For this rea-
son, we also employ a bag-of-words model. Specif-
ically, each context ci is associated with a vector gi
that contains the nouns kept as result of the corpus
preprocessing stage. The upper left part of Figure
3 shows the words associated with each context of
our example. Given two contexts ci and cj, we cal-
culate their bag-of-words weight using the Jaccard
coefficient on the word vectors, i.e. sim,,,d(ci, cj) =
|gi∩gj|
|gi∪gj|.
The collocational weight and bag-of-words
weight are averaged to derive the edge weight be-
tween two contexts as defined in Equation 1. The
resulting graph of our running example is shown on
the bottom of Figure 3. This graph is the input to the
hierarchical random graphs method (Clauset et al.,
2008) described in the next section.
</bodyText>
<sectionHeader confidence="0.9898725" genericHeader="method">
4 Hierarchical Random Graphs for sense
induction
</sectionHeader>
<bodyText confidence="0.998443">
In this section, we describe the process of inferring
the hierarchical structure of the graph of contexts
using hierarchical random graphs (Clauset et al.,
2008).
</bodyText>
<figureCaption confidence="0.990561">
Figure 4: Two dendrograms for the graph in Figure 3.
</figureCaption>
<subsectionHeader confidence="0.996243">
4.1 The Hierarchical Random Graph model
</subsectionHeader>
<bodyText confidence="0.988150592592593">
A dendrogram is a binary tree with n leaves and
n − 1 parents. Figure 4 shows an example of two
dendrograms with 4 leaves and 3 parents. Given a
set of n contexts that we need to arrange hierarchi-
cally, let us denote by G = (V, E) the graph of con-
texts, where V = {v0, v1 ... vn} is the set of ver-
tices, E = {e0, e1 ... em} is the set of edges and
ek = {vi, vj}.
Given an undirected graph G, each of its n ver-
tices is a leaf in a dendrogram, while the internal
nodes of that dendrogram indicate the hierarchical
relationships among the leaves. We denote this or-
ganisation by D = {D1, D2,... Dn−1}, where each
Dk is an internal node. Every pair of nodes (vi, vj)
is associated with a unique Dk, which is their low-
est common ancestor in the tree. In this manner D
partitions the edges that exist in G.
The primary assumption in the hierarchical ran-
dom graph model is that edges in G exist indepen-
dently, but with a probability that is not identically
distributed. In particular, the probability that an edge
{vi, vj} exists in G is given by a parameter Bk asso-
ciated with Dk, the lowest common ancestor of vi
and vj in D. In this manner, the topological struc-
�
ture D and the vector of probabilities 0 define the
HRG given by H(D, 0) (Clauset et al., 2008).
</bodyText>
<page confidence="0.984753">
748
</page>
<subsectionHeader confidence="0.984406">
4.2 HRG parameterisation
</subsectionHeader>
<bodyText confidence="0.993827">
Assuming a uniform prior over all HRGs, the target
is to identify the parameters of D and B, so that the
chosen HRG is statistically similar to G. Let Dk be
an internal node of dendrogram D and f(Dk) be the
number of edges between the vertices of the subtrees
of the subtree rooted at Dk that actually exist in G.
For example, in Figure 4(A), f(D2) = 1, because
there is one edge in G connecting vertices B and C.
Let l(Dk) be the number of leaves in the left subtree
of Dk, and r(Dk) be the number of leaves in the
right subtree. For example in Figure 4(A), l(D2) =
2 and r(D2) = 2. The likelihood of the hierarchical
random graph (D, 0) is defined in Equation 2, where
</bodyText>
<equation confidence="0.996931">
A(Dk) = l(Dk)r(Dk) − f(Dk).
Bf(Dk)
k (1 − Bk)A(Dk) (2)
</equation>
<bodyText confidence="0.981905666666667">
The probabilities Bk that maximise the likelihood
of a dendrogram D can be easily estimated using
the method of MLE i.e Bk = f(Dk)
</bodyText>
<equation confidence="0.441012">
l(Dk)r(Dk). Substi-
</equation>
<bodyText confidence="0.989367">
tuting this into Equation 2 yields Equation 3. For
numerical reasons, it is more convenient to work
with the logarithm of the likelihood which is defined
in Equation 4, where h(Bk) = −Bk log Bk − (1 −
Bk) log (1 − Bk).
</bodyText>
<subsectionHeader confidence="0.687503">
4.2.1 MCMC sampling
</subsectionHeader>
<bodyText confidence="0.999987384615385">
Finding the values of Bk using the MLE method
is straightforward. However, this is not the case
for maximising the likelihood function over the
space of all possible dendrograms. Given a graph
with n vertices, i.e. n leaves in each dendrogram,
the total number of different dendrograms is super-
exponential ((2n − 3)!! pz� \/2(2n)n−1e−n) (Clauset
et al., 2006).
To deal with this problem, we use a Markov
Chain Monte Carlo (MCMC) method that samples
dendrograms from the space of dendrogram mod-
els with probability proportional to their likelihood.
Each time MCMC samples a dendrogram with a
new highest likelihood, that dendrogram is stored.
Hence, our goal is to choose the highest likelihood
dendrogram once MCMC has converged.
Following the work in (Clauset et al., 2008),
we pick a set of transitions between dendrograms,
where a transition is a re-arrangement of the sub-
trees of a dendrogram. In particular, given a current
dendrogram Dcurr, each internal node Dk of Dcurr
is associated with three subtrees of Dcurr. For in-
stance, in Figure 5A, the subtrees st1 and st2 are
derived from the two children of Dk and the third
st3 from its sibling. Given a current dendrogram,
Dcurr, the algorithm proceeds as follows:
</bodyText>
<equation confidence="0.970538714285714">
L(D, �B) = �
DkED
� [Bθk 1. Choose an internal node, Dk E Dcurr uni-
L(D) = k (1 − Bk)1−θk]l(Dk)r(Dk) (3) formly.
DkED
log L(D) = − � h(Bk)l(Dk)r(Dk) (4)
DkED
</equation>
<bodyText confidence="0.9842612">
As can be observed, each term −l(Dk)r(Dk)h(Bk)
is maximised when Bk approaches 0 or 1. This
means that high-likelihood dendrograms partition
vertices into subtrees, such that the connections
among their vertices in the observed graph are either
very rare or very common (Clauset et al., 2008). For
example, consider the two dendrograms in Figures
4(A) and 4(B). We observe that 4(A) is more likely
than 4(B), since it provides a better division of the
network leaves. Particularly, the likelihood of 4(A)
</bodyText>
<equation confidence="0.550109">
is L(D1) = (11 · (1 − 1)1) · (11 · (1 − 1)1) · (0.251 ·
(1 − 0.25)3) = 0.105, while the likelihood of 4(B)
is L(D2) = (00 · (1 − 0)1) · (11 · (1 − 1)1) · (0.52 ·
(1 − 0.5)2) = 0.062.
</equation>
<listItem confidence="0.923287727272727">
2. Generate two possible new configurations of
the subtrees of Dk (See Figure 5).
3. Choose one of the configurations uniformly to
generate a new dendrogram, Dnext.
4. Accept or reject Dnext according to
Metropolis-Hastings (MH) rule.
5. If transition is accepted, then Dcurr = Dnext.
6. GOTO 1.
According to MH rule (Newman and Barkema,
1999), a transition is accepted if log L(Dnext) &gt;
log L(Dcurr); otherwise the transition is accepted
</listItem>
<bodyText confidence="0.92895725">
with probability L(Dnext)
L(D����). These transitions define
an ergodic Markov chain, hence its stationary distri-
bution can be reached (Clauset et al., 2008).
</bodyText>
<page confidence="0.995856">
749
</page>
<figureCaption confidence="0.999671">
Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration,
(C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this
configuration is excluded.
</figureCaption>
<bodyText confidence="0.972592">
In our experiments, we noticed that the algorithm
converged relatively quickly. The same behaviour
(roughly O(n2) steps) was also noticed in Clauset et
al. (2008), when considering graphs with thousands
of vertices.
</bodyText>
<sectionHeader confidence="0.998265" genericHeader="method">
5 HRGs for sense disambiguation
</sectionHeader>
<subsectionHeader confidence="0.968873">
5.1 Sense mapping
</subsectionHeader>
<bodyText confidence="0.9999585">
The output of HRG learning is a dendrogram D with
n leaves (contexts) and n −1 internal nodes. To per-
form sense disambiguation, we mapped the internal
nodes to gold standard senses using a sense-tagged
corpus. Such a sense-tagged corpus is needed when
induced word senses need to be mapped to a gold
standard sense inventory.
Instead of using a hard mapping from the den-
drogram internal nodes to the Gold Standard (GS)
senses, we use a soft probabilistic mapping and cal-
culate P(sk|Di), i.e the probability of sense sk given
node Di. Let F(Di) be the set of training contexts
grouped by internal node Di. Let F&apos;(sk) be the set
of training contexts that are tagged with sense sk.
Then the conditional probability, P(sk|Di), is de-
fined in Equation 5.
</bodyText>
<equation confidence="0.9943125">
P(sk|Di) = |F(Di) ∩ F&apos;(sk) |(5)
|F(Di)|
</equation>
<bodyText confidence="0.9990204">
Table 1 provides a sense-tagged corpus for the
running example of Figure 3. Using this corpus
and the tree in Figure 4(A), P(s1|D2) = 23 and
P(s2|D2) = 1�. In Figure 4(A) the rest of the calcu-
lated conditional probabilities are given.
</bodyText>
<subsectionHeader confidence="0.999029">
5.2 Sense tagging
</subsectionHeader>
<bodyText confidence="0.9976605">
For evaluation we compared the proposed method
against the current state-of-the-art sense induction
</bodyText>
<table confidence="0.998939428571428">
GS sense Context ID Context words
s1 A journal, scholar, observation
science, paper
s1 B scholar, scholar, author,
publication, paper
s2 D times, guardian,
journalist, paper
</table>
<tableCaption confidence="0.999966">
Table 1: Sense-tagged corpus for the example in Figure 3
</tableCaption>
<bodyText confidence="0.999677777777778">
systems in the WSD task. We followed the setting
of SemEval-2007 sense induction task (Agirre and
Soroa, 2007). In this setting, the base corpus (bc)
(Section 3.1) for a target word consists both of the
training and testing corpus. As a result, a testing
context cj of tw is a leaf in the generated dendro-
gram. The process of disambiguating cj is straight-
forward exploiting the structural information pro-
vided by HRGs.
</bodyText>
<equation confidence="0.999976666666667">
�w(sk,cj) = P(sk|Di) · Bi (6)
DiEH(cj)
w(s*,cj) = argmaxsk(w(sk,cj)) (7)
</equation>
<bodyText confidence="0.938263923076923">
Let H(cj) denote the set of parents for context cj.
Then, the weight assigned to sense sk is the sum of
weighted scores provided by each identified parent.
This is shown in Equation 6, where Bi is the proba-
bility associated with each internal node Di from the
hierarchical random graph (see Figure 4(A)). This
probability reflects the discriminating ability of in-
ternal nodes.
Finally, the highest weight determines the win-
ning sense for context cj (Equation 7). In our ex-
ample (Figure 4(A)), w(s1, C) = (0·1+ 2 3·0.25) =
0.16 and w(s2, C) = (1·1+13·0.25) = 1.08. Hence,
s2 is the winning sense.
</bodyText>
<page confidence="0.992133">
750
</page>
<table confidence="0.9974775">
Parameter Range
G2 word threshold (p1) 15,25,35,45
G2 collocation threshold (p2) 10,15,20
Edge similarity threshold (p3) 0.05,0.09,0.13
</table>
<tableCaption confidence="0.996932">
Table 2: Parameter values used in the evaluation.
</tableCaption>
<sectionHeader confidence="0.998179" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999057">
6.1 Evaluation setting &amp; baselines
</subsectionHeader>
<bodyText confidence="0.999973243243243">
We evaluate our method on the nouns of the
SemEval-2007 word sense induction task (Agirre
and Soroa, 2007) under the second evaluation setting
of that task, i.e. supervised evaluation. Specifically,
we use the standard WSD measures of precision and
recall in order to produce their harmonic mean (F-
Score). The official scoring software of that task has
been used in our evaluation. Note that the unsuper-
vised measures of that task are not directly applica-
ble to our induced hierarchies, since they focus on
assessing flat clustering methods.
The first aim of our evaluation is to test whether
inferring the hierarchical structure of the constructed
graphs improves WSD performance. For that reason
our first baseline, Chinese Whispers Unweighted
version (CWU), takes as input the same unweighted
graph of contexts as HRGs in order to produce a
flat clustering. The set of produced clusters is then
mapped to GS senses using the training dataset and
performance is then measured on the testing dataset.
We followed the same sense mapping method as in
the SemEval-2007 sense induction task (Agirre and
Soroa, 2007).
Our second baseline, Chinese Whispers Weighted
version (CWW), is similar to the previous one, with
the difference that the edges of the input graph
are weighted using Equation 1. For clustering the
graphs of CWU and CWW we employ, Chinese
Whispers4 (Biemann, 2006).
The second aim of our evaluation is to assess
whether the hierarchical structure inferred by HRGs
is more informative than the hierarchical struc-
ture inferred by traditional Hierarchical Clustering
(HAC). Hence, our third baseline, takes as input a
similarity matrix of the graph vertices and performs
bottom-up clustering with average-linkage, which
has already been used in WSI in (Pantel and Lin,
</bodyText>
<footnote confidence="0.71806">
4The number of iterations for CW was set to 200.
</footnote>
<bodyText confidence="0.997878705882353">
2003) and was shown to have superior or similar per-
formance to single-linkage and complete-linkage in
the related problem of learning a taxonomy of senses
(Klapaftis and Manandhar, 2010).
To calculate the similarity matrix of vertices we
follow a process similar to the one used in Sec-
tion 4.2 for calculating the probability of an inter-
nal node. The similarity between two vertices is
calculated according to the degree of connected-
ness among their direct neighbours. Specifically,
we would like to assign high similarity to pairs of
vertices, whose neighbours are close to forming a
clique.
Given two vertices (contexts) ci and cj, let
N(ci7 cj) be the set of their neighbours and K(ci7 cj)
be the set of edges between the vertices in N(ci7 cj).
The maximum number of edges that could exist be-
</bodyText>
<equation confidence="0.778305">
tween vertices in N(ci7 cj) is (|N(ci,cj)|�. Thus, the
2
</equation>
<bodyText confidence="0.998668666666667">
similarity of ci, cj is set equal to the number of
edges that actually exist in that neighbourhood di-
vided by the total number of edges that could exist
</bodyText>
<equation confidence="0.928606">
YIN(-i,cj)
I YJ) ).
</equation>
<subsectionHeader confidence="0.948772">
6.2 Results &amp; discussion
</subsectionHeader>
<bodyText confidence="0.999729533333333">
Table 2 shows the parameter values used in the eval-
uation. Figure 6(A) shows the performance of the
proposed method against the baselines for p3 = 0.05
and different p1 and p2 values. Figure 6(B) il-
lustrates the results of the same experiment using
p3 = 0.09. In both figures, we observe that HRGs
outperform the CWU baseline under all parameter
combinations. In particular, all of the 12 perfor-
mance differences for p3 = 0.09 are statistically
significant using McNemar’s test at 95% confidence
level, while for p3 = 0.05 only 2 out of the 12 per-
formance differences were not judged as significant
from the test.
The picture is the same for p3 = 0.13, where
CWU performs significantly worse than for p3 =
</bodyText>
<page confidence="0.578965">
2
</page>
<bodyText confidence="0.933679222222222">
The disambiguation process using the HAC tree
is identical to the one presented in Section 5.2 with
the only difference that the internal probability, Bi,
in Equation 6 does not exist for HAC. Hence, we re-
placed it with the factor 1 , where H(Di) is the
|H(Di
set of children of internal node Di. This factor pro-
vides lower weights for nodes high in the tree, since
their discriminating ability will possibly be lower.
</bodyText>
<page confidence="0.997717">
751
</page>
<figureCaption confidence="0.995912">
Figure 6: Performance analysis of HRGs, CWU, CWW &amp; HAC for different parameter combinations (Table 2). (A)
All combinations of p1, p2 and p3 = 0.05. (B) All combinations of p1, p2 and p3 = 0.09.
</figureCaption>
<bodyText confidence="0.999938467741936">
0.05 and p3 = 0.09. Specifically, the largest perfor-
mance difference between HRGs and CWU is 9.4%
at p1 = 25, p2 = 10 and p3 = 0.13. Setting the ver-
tex similarity threshold (p3) equal to 0.13 leads to
more sparse and disconnected graphs, which causes
Chinese Whispers to produce a large number of clus-
ters. This leads to sparsity problems and unreliable
mapping of clusters to GS senses due to the lack of
adequate training data. In contrast, HRGs suffer less
at this high threshold, although their performance
when p3&lt;0.13 is better.
This picture does not change for the weighted ver-
sion of Chinese Whispers (CWW) which performs
worse than CWU. This is because CWW produces
a smaller number of clusters than CWU that con-
flate the target word senses. It seems that using
weighted edges creates a bias towards the MFS, in
effect missing rare senses of a target word. This
means that a number of words in the bag-of-words
context vectors and collocations in the collocational
context vectors (Section 3.2) are associated to more
than one sense of the target word and most strongly
associated to the MFS. As a result, increasing the p1
threshold to 25 and 35 leads to a higher performance
for CWW, since many of these words and colloca-
tions are filtered out.
Overall, the comparison of HRGs against the
CWU and CWW baselines has shown that inferring
the hierarchical structure of observed graphs leads
to improved WSD performance as opposed to using
flat clustering. This is because HRGs are able to in-
fer both the hierarchical structure of the graph and
include the probabilities, Bk, associated with each
internal node. These probabilities reflect the dis-
criminating ability of each node, offering informa-
tion missed by flat clustering.
In Figures 6(A) and 6(B) we observe that HRGs
perform significantly better than HAC. In particular,
all of their performance differences are statistically
significant for these parameter values. The largest
performance difference is 6.0% at p1 = 45, p2 = 10
and p3 = 0.05. However, this picture is not the same
when considering a higher context similarity thresh-
old (p3 = 0.13) as Figure 7 shows. In particular,
HRGs and HAC perform similarly for p3 = 0.13,
while the majority of performance differences are
not statistically significant.
The similar behaviour of HRGs and HAC at this
threshold is caused both by the worse performance
of HRGs and the improved performance of HAC as
opposed to lower p3 values. As it has been men-
tioned, setting p3 = 0.13 leads to sparse and dis-
connected graphs. Additionally, the likelihood func-
tion (Equation 3) is maximised when the probabil-
ity, Bk, of an internal node, Dk, approaches 0 or 1.
This creates a bias towards dendrograms, in which a
large number of internal nodes have zero probabil-
ity. These dendrograms might be a good-fit to the
observed graph, but not to the GS.
In contrast, HAC is less affected, because it never
considers creating an internal node, when the max-
imum similarity among any pair of two candidate
</bodyText>
<page confidence="0.99502">
752
</page>
<figureCaption confidence="0.975681333333333">
Figure 7: Performance of HRGs and HAC for different
parameter combinations (Table 2). All combinations of
p1, p2 and p3 &gt; 0.13.
</figureCaption>
<bodyText confidence="0.997879589743589">
subtrees is zero. Additionally, our experiments show
that HAC is unable to deal with noise when con-
sidering sparse graphs (p3&lt;0.13). For that reason,
the F-Score of HAC increases as the edge similarity
threshold decreases.
To further investigate this issue and test whether
HAC is able to achieve a higher F-Score than HRGs
in higher p3 values, we executed two more experi-
ments for HAC and HRGs increasing p3 to 0.17 and
0.21 respectively. In the first case we observed that
the performance of HAC remained relatively stable
compared to p3 = 0.13, while in the second case the
performance of HAC decreased as Figure 7 shows.
In both cases, HAC performed significantly better
than HRGs.
Overall, the comparison of HRGs against HAC
has shown that HRGs perform significantly better
than HAC when considering connected or less sparse
graphs (p3&lt;0.13). This is due to the fact that HAC
creates dendrograms, in which connections within
the clusters are dense, while connections between
the clusters are sparse, i.e. it only considers assorta-
tive structures. In contrast, HRGs also consider dis-
assortative dendrograms, i.e. dendrograms in which
vertices are less likely to be connected on small
scales than on large ones, as well as mixtures of
assortative and disassortative (Clauset et al., 2008).
This is achieved by allowing the probability Bk of
a node k to vary arbitrarily throughout the dendro-
gram.
HAC performs similarly or better than HRGs for
largely disconnected and sparse graphs, because
HRGs become biased towards disassortative trees
which are not a good fit to the GS (Figure 7). De-
spite that, our evaluation has also shown that the best
performance of HAC (F-Score = 86.0% at p1 = 15,
p2 = 10, p3 = 0.13) is significantly lower than
the best performance of HRGs (F-Score = 87.6% at
p1 = 35, p2 = 10, p3 = 0.09).
</bodyText>
<subsectionHeader confidence="0.999473">
6.3 Comparison to state-of-the-art methods
</subsectionHeader>
<bodyText confidence="0.9999745">
Table 3 compares the best performing parameter
combination of our method against state-of-the-art
methods. Table 3 also includes the best performance
of our baselines, i.e HAC, CWU and CWW.
Brody &amp; Lapata (2009) presented a sense induc-
tion method that is related to Latent Dirichlet Al-
location (Blei et al., 2003). In their work, they
model the target word instances as samples from a
multinomial distribution over senses which are suc-
cessively characterized as distributions over words
(Brody and Lapata, 2009). A significant advantage
of their method is the inclusion of more than one
layer in the LDA setting, where each layer corre-
sponds to a different feature type e.g. dependency
relations, bigrams, etc. The inclusion of different
feature types as separate models in the sense in-
duction process can easily be modeled in our set-
ting, by inferring a different hierarchy of target word
instances according to each feature type, and then
combining all of them to a consensus tree. In this
work, we have focused on extracting a single hierar-
chy combining word co-occurrence and bigram fea-
tures.
Niu et al. (2007) developed a vector-based
method that performs sense induction by group-
ing the contexts of a target word using three types
of features, i.e. POS tags of neighbouring words,
word co-occurrences and local collocations. The se-
quential information bottleneck algorithm (Slonim
et al., 2002) is applied for clustering. HRGs perform
slightly better than the methods of Brody &amp; Lap-
ata (2009) and Niu et al. (2007), although the dif-
ferences are not significant (McNemar’s test at 95%
confidence level).
Klapaftis &amp; Manandhar (2008) developed a
graph-based sense induction method, in which ver-
tices correspond to collocations related to the tar-
get word and edges between vertices are drawn ac-
</bodyText>
<page confidence="0.996415">
753
</page>
<table confidence="0.9974523">
System Performance (%)
HRGs 87.6
(Brody and Lapata, 2009) 87.3
(Niu et al., 2007) 86.8
(Klapaftis and Manandhar, 2008) 86.4
HAC 86.0
CWU 85.1
CWW 84.7
(Pedersen, 2007) 84.5
MFS 80.9
</table>
<tableCaption confidence="0.99996">
Table 3: HRGs against recent methods &amp; baselines.
</tableCaption>
<bodyText confidence="0.999927384615385">
cording to the co-occurrence frequency of the cor-
responding collocations. The constructed graph is
smoothed to identify more edges between vertices
and then clustered using Chinese Whispers (Bie-
mann, 2006). This method is related to the basic
inputs of our presented method. Despite that, it is
a flat clustering method that ignores the hierarchical
structure exhibited by observed graphs. The previ-
ous section has shown that inferring the hierarchical
structure of graphs leads to superior WSD perfor-
mance.
Pedersen (2007) presented SenseClusters, a
vector-based method that clusters second order co-
occurrence vectors using k-means, where k is auto-
matically determined using the Adapted Gap Statis-
tic (Pedersen and Kulkarni, 2006). As can be ob-
served, HRGs perform significantly better than the
methods of Pedersen (2007) and Klapaftis &amp; Man-
andhar (2008) (McNemar’s test at 95% confidence
level).
Finally, Table 3 shows that the best performing
parameter combination of HRGs achieves a signifi-
cantly higher F-Score than the best performing pa-
rameter combination of HAC, CWU and CWW. Fur-
thermore, HRGs outperform the most frequent sense
baseline by 6.7%.
</bodyText>
<sectionHeader confidence="0.987088" genericHeader="conclusions">
7 Conclusion &amp; future work
</sectionHeader>
<bodyText confidence="0.999985782608696">
We presented an unsupervised method for inferring
the hierarchical grouping of the senses of a polyse-
mous word. Our method creates a graph, in which
vertices correspond to contexts of a polysemous tar-
get word and edges between them are drawn ac-
cording to their similarity. The hierarchical random
graphs algorithm (Clauset et al., 2008) was applied
to the constructed graph in order to infer its hierar-
chical structure, i.e. binary tree.
The learned tree provides an induction of the
senses of a given word at different levels of sense
granularity and was applied to the problem of WSD.
The WSD process mapped the tree’s internal nodes
to GS senses using a sense tagged corpus, and then
tagged new instances by exploiting the structural in-
formation provided by the tree.
Our experimental results have shown that our
graphs exhibit hierarchical organisation that can
be captured by HRGs, in effect providing im-
proved WSD performance compared to flat cluster-
ing. Additionally, our comparison against hierarchi-
cal agglomerative clustering with average-linkage
has shown that HRGs perform significantly better
than HAC when the graphs do not suffer from spar-
sity (disconnected graphs). The comparison with
state-of-the-art sense induction systems has shown
that our method yields improvements.
Our future work focuses on using different feature
types, e.g. dependency relations, second-order co-
occurrences, named entities and others to construct
our undirected graphs and then applying HRGs, in
order to measure the impact of each feature type
on the induced hierarchical structures within a WSD
setting. Moreover, following the work in (Clauset et
al., 2008), we are also working on using MCMC in
order to sample more than one dendrogram at equi-
librium, and then combine them to a consensus tree.
This consensus tree might be able to express a larger
amount of topological features of the initial undi-
rected graph.
Finally in terms of evaluation, our future work
also focuses on evaluating HRGs using a fine-
grained sense inventory, extending the evaluation on
the SemEval-2010 WSI task dataset (Manandhar et
al., 2010) as well as applying HRGs to other related
tasks such as taxonomy learning.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999686333333333">
This work is supported by the European Com-
mission via the EU FP7 INDECT project, Grant
No.218086, Research area: SEC-2007-1.2-01 Intel-
ligent Urban Environment Observation System. The
authors would like to thank the anonymous review-
ers for their useful comments.
</bodyText>
<page confidence="0.998085">
754
</page>
<sectionHeader confidence="0.995891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999522">
Eneko Agirre and Aitor. Soroa. 2007. Semeval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7–12, Prague, Czech Republic.
Eneko Agirre, David Martinez, Oier L´opez de Lacalle,
and Aitor Soroa. 2006. Two Graph-based Algorithms
for State-of-the-art WSD. In Proceedings of EMNLP-
2006, pages 585–593, Sydney, Australia.
Chris Biemann. 2006. Chinese Whispers - An Efficient
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proceed-
ings of TextGraphs, pages 73–80, New York, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993–1022.
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search Engine.
Comput. Netw. ISDNSyst., 30(1-7):107–117.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proceedings of EACL-2009, pages
103–111, Athens, Greece. ACL.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-
man. 2006. Structural Inference of Hierarchies in Net-
works. In Proceedings of the ICML-2006 Workshop
on Social Network Analysis, pages 1–13, Pittsburgh,
USA.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-
man. 2008. Hierarchical Structure and the Prediction
of Missing Links in Networks. Nature, 453(7191):98–
101.
Stijn Dongen. 2000. Performance Criteria for Graph
Clustering and Markov Cluster Experiments. Tech-
nical report, CWI (Centre for Mathematics and Com-
puter Science), Amsterdam, The Netherlands.
Beate Dorow and Dominic Widdows. 2003. Discovering
Corpus-specific Word Senses. In Proceedings of the
EACL-2003, pages 79–82, Budapest, Hungary.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61–74.
Phil Edmonds and Beate Dorow. 2001. Senseval-2:
Overview. In Proceedings of SensEval-2, pages 1–5,
Toulouse, France.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
Sense Induction Using Graphs of Collocations. In
Proceedings of ECAI-2008, pages 298–302, Patras,
Greece.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-
onomy Learning Using Word Sense Induction. In Pro-
ceedings of NAACL-HLT-2010, pages 82–90, Los An-
geles, California, June. ACL.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
Task 14: Word Sense Induction &amp; Disambiguation. In
Proceedings of SemEval-2, Uppsala, Sweden. ACL.
Rada Mihalcea. 2004. Graph-based Ranking Algorithms
for Sentence Extraction, Applied to Text Summariza-
tion. In Proceedings of the ACL 2004 on Interactive
poster and demonstration sessions, page 20, Morris-
town, NJ, USA.
Mark Newman and Gerard Barkema. 1999. Monte Carlo
Methods in Statistical Physics. Oxford: Clarendon
Press, New York, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2R: Three Systems for Word Sense Discrimination,
Chinese Word Sense Disambiguation, and English
Word Sense Disambiguation. In Proceedings of
SemEval-2007, pages 177–182, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2003. Automatically
Discovering Word Senses. In Proceedings of NAACL-
HLT-2003, pages 21–22, Morristown, NJ, USA.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
Cluster Stopping With Criterion Functions and the gap
Statistic. In Proceedings of the 2006 Conference of the
North American Chapter of the ACL on Human Lan-
guage Technology, pages 276–279, Morristown, NJ,
USA.
Ted Pedersen. 2007. UMND2 : Senseclusters Applied to
the Sense Induction Task of Senseval-4. In Proceed-
ings of SemEval-2007, pages 394–397, Prague, Czech
Republic.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random Walks for Text Semantic
Similarity. In Proceedings of TextGraphs-4, Suntec,
Singapore, August.
Noam Slonim, Nir Friedman, and Naftali Tishby. 2002.
Unsupervised Document Classification Using Sequen-
tial Information Maximization. In SIGIR 2002, pages
129–136, New York, NY, USA. ACM.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish All-words Task. In Rada Mihalcea and Phil Ed-
monds, editors, In Proceedings of Senseval-3, pages
41–43, Barcelona, Spain.
Jean V´eronis. 2004. Hyperlex: Lexical Cartography for
Information Retrieval. Computer Speech &amp; Language,
18(3):223–252.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of COLING-2004, pages
10–15, Morristown, NJ, USA.
Dominic Widdows and Beate Dorow. 2002. A Graph
Model for Unsupervised Lexical Acquisition. In Pro-
ceedings of Coling-2002, pages 1–7, Morristown, NJ,
USA.
</reference>
<page confidence="0.998709">
755
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.387748">
<title confidence="0.999541">Word Sense Induction &amp; Disambiguation Hierarchical Random Graphs</title>
<author confidence="0.975255">P Ioannis</author>
<affiliation confidence="0.935163333333333">Department of Computer University of United</affiliation>
<email confidence="0.972068">giannis@cs.york.ac.uk</email>
<author confidence="0.617398">Suresh</author>
<affiliation confidence="0.940276666666667">Department of Computer University of United</affiliation>
<email confidence="0.983131">suresh@cs.york.ac.uk</email>
<abstract confidence="0.999364095238095">Graph-based methods have gained attention in many areas of Natural Language Processing (NLP) including Word Sense Disambiguation (WSD), text summarization, keyword extraction and others. Most of the work in these areas formulate their problem in a graph-based setting and apply unsupervised graph clustering to obtain a set of clusters. Recent studies suggest that graphs often exhibit a hierarchical structure that goes beyond simple flat clustering. This paper presents an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs significantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Soroa</author>
</authors>
<title>Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval2007,</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7062" citStr="Soroa, 2007" startWordPosition="1115" endWordPosition="1116">get word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free1 graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008). The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. All the described methods for sense induction ap1One needs to specify only the number of iterations. The number of clusters is generated automatically. 746 Figure 2: Graph of words for the target word paper. Numbers inside vertices correspond to their degree. Figure 3: Running example of graph creation ply flat graph clustering methods to derive the clusters (senses) of a target word. As a result, they neglect the fact that their constructed graphs often exhibit a hierarchical structure that is useful in several tasks including word sense disambiguation. 3 Building a</context>
<context position="10277" citStr="Soroa, 2007" startWordPosition="1674" endWordPosition="1675">ns is combined with any other noun yielding a total of collocations for a context with N nouns. Each collocation, clij is weighted using the log-likelihood ratio (G2) (Dunning, 1993) and is filtered out if the G2 is below a prespecified threshold (parameter p2). At the end of this process, each context ci of tw is associated with a vector of collocations (vi). The upper right part of Figure 3 shows the collocations associated with each context of our example. 2Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). 3The British National Corpus, 2001, Distributed by Oxford University Computing Services. (N ) 2 747 Given two contexts ci and cj, we calculate their collocational weight using the Jaccard coefficient on the collocational vectors, i.e. simj(ci, cj) = |vi∩vj|The selection of Jaccard is based on the work vi∪vj | of Weeds et al. (2004), who analyzed the variation in a word’s distributionally nearest neighbours with respect to a variety of similarity measures. Their analysis showed that there are three classes of measures, i.e. those selecting distributionally more general neighbours (e.g. cosine</context>
<context position="19684" citStr="Soroa, 2007" startWordPosition="3306" endWordPosition="3307">this corpus and the tree in Figure 4(A), P(s1|D2) = 23 and P(s2|D2) = 1�. In Figure 4(A) the rest of the calculated conditional probabilities are given. 5.2 Sense tagging For evaluation we compared the proposed method against the current state-of-the-art sense induction GS sense Context ID Context words s1 A journal, scholar, observation science, paper s1 B scholar, scholar, author, publication, paper s2 D times, guardian, journalist, paper Table 1: Sense-tagged corpus for the example in Figure 3 systems in the WSD task. We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). In this setting, the base corpus (bc) (Section 3.1) for a target word consists both of the training and testing corpus. As a result, a testing context cj of tw is a leaf in the generated dendrogram. The process of disambiguating cj is straightforward exploiting the structural information provided by HRGs. �w(sk,cj) = P(sk|Di) · Bi (6) DiEH(cj) w(s*,cj) = argmaxsk(w(sk,cj)) (7) Let H(cj) denote the set of parents for context cj. Then, the weight assigned to sense sk is the sum of weighted scores provided by each identified parent. This is shown in Equation 6, where Bi is the probability assoc</context>
<context position="21006" citStr="Soroa, 2007" startWordPosition="3527" endWordPosition="3528">ts the discriminating ability of internal nodes. Finally, the highest weight determines the winning sense for context cj (Equation 7). In our example (Figure 4(A)), w(s1, C) = (0·1+ 2 3·0.25) = 0.16 and w(s2, C) = (1·1+13·0.25) = 1.08. Hence, s2 is the winning sense. 750 Parameter Range G2 word threshold (p1) 15,25,35,45 G2 collocation threshold (p2) 10,15,20 Edge similarity threshold (p3) 0.05,0.09,0.13 Table 2: Parameter values used in the evaluation. 6 Evaluation 6.1 Evaluation setting &amp; baselines We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task, i.e. supervised evaluation. Specifically, we use the standard WSD measures of precision and recall in order to produce their harmonic mean (FScore). The official scoring software of that task has been used in our evaluation. Note that the unsupervised measures of that task are not directly applicable to our induced hierarchies, since they focus on assessing flat clustering methods. The first aim of our evaluation is to test whether inferring the hierarchical structure of the constructed graphs improves WSD performance. For that reason our firs</context>
</contexts>
<marker>Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor. Soroa. 2007. Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems. In Proceedings of SemEval2007, pages 7–12, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
<author>Oier L´opez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Two Graph-based Algorithms for State-of-the-art WSD.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP2006,</booktitle>
<pages>585--593</pages>
<location>Sydney, Australia.</location>
<marker>Agirre, Martinez, de Lacalle, Soroa, 2006</marker>
<rawString>Eneko Agirre, David Martinez, Oier L´opez de Lacalle, and Aitor Soroa. 2006. Two Graph-based Algorithms for State-of-the-art WSD. In Proceedings of EMNLP2006, pages 585–593, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese Whispers - An Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs,</booktitle>
<pages>73--80</pages>
<location>New York, USA.</location>
<contexts>
<context position="6698" citStr="Biemann, 2006" startWordPosition="1057" endWordPosition="1058">he target word. Additionally, they extract second-order co-occurrences. Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times. This co-occurrence frequency is also used to weight the edges. The resulting graph is then pruned by removing the target word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free1 graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008). The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. All the described methods for sense induction ap1One needs to specify only the number of iterations. The number of clusters is generated automatically. 746 Figure 2: Graph of words for the target word paper. N</context>
<context position="22278" citStr="Biemann, 2006" startWordPosition="3732" endWordPosition="3733">es as input the same unweighted graph of contexts as HRGs in order to produce a flat clustering. The set of produced clusters is then mapped to GS senses using the training dataset and performance is then measured on the testing dataset. We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). Our second baseline, Chinese Whispers Weighted version (CWW), is similar to the previous one, with the difference that the edges of the input graph are weighted using Equation 1. For clustering the graphs of CWU and CWW we employ, Chinese Whispers4 (Biemann, 2006). The second aim of our evaluation is to assess whether the hierarchical structure inferred by HRGs is more informative than the hierarchical structure inferred by traditional Hierarchical Clustering (HAC). Hence, our third baseline, takes as input a similarity matrix of the graph vertices and performs bottom-up clustering with average-linkage, which has already been used in WSI in (Pantel and Lin, 4The number of iterations for CW was set to 200. 2003) and was shown to have superior or similar performance to single-linkage and complete-linkage in the related problem of learning a taxonomy of s</context>
<context position="32341" citStr="Biemann, 2006" startWordPosition="5434" endWordPosition="5436">andhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs against recent methods &amp; baselines. cording to the co-occurrence frequency of the corresponding collocations. The constructed graph is smoothed to identify more edges between vertices and then clustered using Chinese Whispers (Biemann, 2006). This method is related to the basic inputs of our presented method. Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs. The previous section has shown that inferring the hierarchical structure of graphs leads to superior WSD performance. Pedersen (2007) presented SenseClusters, a vector-based method that clusters second order cooccurrence vectors using k-means, where k is automatically determined using the Adapted Gap Statistic (Pedersen and Kulkarni, 2006). As can be observed, HRGs perform significantly better than the methods o</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese Whispers - An Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems. In Proceedings of TextGraphs, pages 73–80, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="30416" citStr="Blei et al., 2003" startWordPosition="5122" endWordPosition="5125">od fit to the GS (Figure 7). Despite that, our evaluation has also shown that the best performance of HAC (F-Score = 86.0% at p1 = 15, p2 = 10, p3 = 0.13) is significantly lower than the best performance of HRGs (F-Score = 87.6% at p1 = 35, p2 = 10, p3 = 0.09). 6.3 Comparison to state-of-the-art methods Table 3 compares the best performing parameter combination of our method against state-of-the-art methods. Table 3 also includes the best performance of our baselines, i.e HAC, CWU and CWW. Brody &amp; Lapata (2009) presented a sense induction method that is related to Latent Dirichlet Allocation (Blei et al., 2003). In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). A significant advantage of their method is the inclusion of more than one layer in the LDA setting, where each layer corresponds to a different feature type e.g. dependency relations, bigrams, etc. The inclusion of different feature types as separate models in the sense induction process can easily be modeled in our setting, by inferring a different hierarchy of target word instances according to </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The Anatomy of a Large-Scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<journal>Comput. Netw. ISDNSyst.,</journal>
<pages>30--1</pages>
<contexts>
<context position="1948" citStr="Brin and Page, 1998" startWordPosition="288" endWordPosition="291"> are depicted by weighted or unweighted edges. For instance, in unsupervised WSD a number of methods (Widdows and Dorow, 2002; V´eronis, 2004; Agirre et al., 2006) have constructed word co-occurrence graphs for a target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns. Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences. In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text (Ramage et al., 2009). Recent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves. This hierarchical structure provides additional information as opposed to flat clustering by explicitly including or</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Comput. Netw. ISDNSyst., 30(1-7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian Word Sense Induction.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-2009,</booktitle>
<pages>103--111</pages>
<publisher>ACL.</publisher>
<location>Athens, Greece.</location>
<contexts>
<context position="30614" citStr="Brody and Lapata, 2009" startWordPosition="5152" endWordPosition="5155">erformance of HRGs (F-Score = 87.6% at p1 = 35, p2 = 10, p3 = 0.09). 6.3 Comparison to state-of-the-art methods Table 3 compares the best performing parameter combination of our method against state-of-the-art methods. Table 3 also includes the best performance of our baselines, i.e HAC, CWU and CWW. Brody &amp; Lapata (2009) presented a sense induction method that is related to Latent Dirichlet Allocation (Blei et al., 2003). In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). A significant advantage of their method is the inclusion of more than one layer in the LDA setting, where each layer corresponds to a different feature type e.g. dependency relations, bigrams, etc. The inclusion of different feature types as separate models in the sense induction process can easily be modeled in our setting, by inferring a different hierarchy of target word instances according to each feature type, and then combining all of them to a consensus tree. In this work, we have focused on extracting a single hierarchy combining word co-occurrence and bigram features. Niu et al. (20</context>
<context position="31961" citStr="Brody and Lapata, 2009" startWordPosition="5373" endWordPosition="5376">s of features, i.e. POS tags of neighbouring words, word co-occurrences and local collocations. The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs against recent methods &amp; baselines. cording to the co-occurrence frequency of the corresponding collocations. The constructed graph is smoothed to identify more edges between vertices and then clustered using Chinese Whispers (Biemann, 2006). This method is related to the basic inputs of our presented method. Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs. The previous section has shown that</context>
<context position="30314" citStr="Brody &amp; Lapata (2009)" startWordPosition="5104" endWordPosition="5107">isconnected and sparse graphs, because HRGs become biased towards disassortative trees which are not a good fit to the GS (Figure 7). Despite that, our evaluation has also shown that the best performance of HAC (F-Score = 86.0% at p1 = 15, p2 = 10, p3 = 0.13) is significantly lower than the best performance of HRGs (F-Score = 87.6% at p1 = 35, p2 = 10, p3 = 0.09). 6.3 Comparison to state-of-the-art methods Table 3 compares the best performing parameter combination of our method against state-of-the-art methods. Table 3 also includes the best performance of our baselines, i.e HAC, CWU and CWW. Brody &amp; Lapata (2009) presented a sense induction method that is related to Latent Dirichlet Allocation (Blei et al., 2003). In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). A significant advantage of their method is the inclusion of more than one layer in the LDA setting, where each layer corresponds to a different feature type e.g. dependency relations, bigrams, etc. The inclusion of different feature types as separate models in the sense induction process can easil</context>
<context position="31601" citStr="Brody &amp; Lapata (2009)" startWordPosition="5314" endWordPosition="5318"> target word instances according to each feature type, and then combining all of them to a consensus tree. In this work, we have focused on extracting a single hierarchy combining word co-occurrence and bigram features. Niu et al. (2007) developed a vector-based method that performs sense induction by grouping the contexts of a target word using three types of features, i.e. POS tags of neighbouring words, word co-occurrences and local collocations. The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs against recent methods &amp; baselines. cording to the co-occurrence frequency of the corresponding collo</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian Word Sense Induction. In Proceedings of EACL-2009, pages 103–111, Athens, Greece. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Cristopher Moore</author>
<author>Mark E J Newman</author>
</authors>
<title>Structural Inference of Hierarchies in Networks.</title>
<date>2006</date>
<booktitle>In Proceedings of the ICML-2006 Workshop on Social Network Analysis,</booktitle>
<pages>1--13</pages>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="2202" citStr="Clauset et al., 2006" startWordPosition="329" endWordPosition="332">clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns. Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences. In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text (Ramage et al., 2009). Recent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves. This hierarchical structure provides additional information as opposed to flat clustering by explicitly including organisation at all scales of a graph (Clauset et al., 2008). In this paper, we present an unsupervised method for inferring the hierarchical structure (binary tree) of a graph, in which vertices are the contexts of a polysemous word and edges represent th</context>
<context position="15361" citStr="Clauset et al., 2006" startWordPosition="2573" endWordPosition="2576">(Dk) l(Dk)r(Dk). Substituting this into Equation 2 yields Equation 3. For numerical reasons, it is more convenient to work with the logarithm of the likelihood which is defined in Equation 4, where h(Bk) = −Bk log Bk − (1 − Bk) log (1 − Bk). 4.2.1 MCMC sampling Finding the values of Bk using the MLE method is straightforward. However, this is not the case for maximising the likelihood function over the space of all possible dendrograms. Given a graph with n vertices, i.e. n leaves in each dendrogram, the total number of different dendrograms is superexponential ((2n − 3)!! pz� \/2(2n)n−1e−n) (Clauset et al., 2006). To deal with this problem, we use a Markov Chain Monte Carlo (MCMC) method that samples dendrograms from the space of dendrogram models with probability proportional to their likelihood. Each time MCMC samples a dendrogram with a new highest likelihood, that dendrogram is stored. Hence, our goal is to choose the highest likelihood dendrogram once MCMC has converged. Following the work in (Clauset et al., 2008), we pick a set of transitions between dendrograms, where a transition is a re-arrangement of the subtrees of a dendrogram. In particular, given a current dendrogram Dcurr, each interna</context>
</contexts>
<marker>Clauset, Moore, Newman, 2006</marker>
<rawString>Aaron Clauset, Cristopher Moore, and Mark E. J. Newman. 2006. Structural Inference of Hierarchies in Networks. In Proceedings of the ICML-2006 Workshop on Social Network Analysis, pages 1–13, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Cristopher Moore</author>
<author>Mark E J Newman</author>
</authors>
<title>Hierarchical Structure and the Prediction of Missing Links in Networks.</title>
<date>2008</date>
<journal>Nature,</journal>
<volume>453</volume>
<issue>7191</issue>
<pages>101</pages>
<contexts>
<context position="2225" citStr="Clauset et al., 2008" startWordPosition="333" endWordPosition="336">he clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns. Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences. In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text (Ramage et al., 2009). Recent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves. This hierarchical structure provides additional information as opposed to flat clustering by explicitly including organisation at all scales of a graph (Clauset et al., 2008). In this paper, we present an unsupervised method for inferring the hierarchical structure (binary tree) of a graph, in which vertices are the contexts of a polysemous word and edges represent the similarity between co</context>
<context position="12266" citStr="Clauset et al., 2008" startWordPosition="1992" endWordPosition="1995">ntains the nouns kept as result of the corpus preprocessing stage. The upper left part of Figure 3 shows the words associated with each context of our example. Given two contexts ci and cj, we calculate their bag-of-words weight using the Jaccard coefficient on the word vectors, i.e. sim,,,d(ci, cj) = |gi∩gj| |gi∪gj|. The collocational weight and bag-of-words weight are averaged to derive the edge weight between two contexts as defined in Equation 1. The resulting graph of our running example is shown on the bottom of Figure 3. This graph is the input to the hierarchical random graphs method (Clauset et al., 2008) described in the next section. 4 Hierarchical Random Graphs for sense induction In this section, we describe the process of inferring the hierarchical structure of the graph of contexts using hierarchical random graphs (Clauset et al., 2008). Figure 4: Two dendrograms for the graph in Figure 3. 4.1 The Hierarchical Random Graph model A dendrogram is a binary tree with n leaves and n − 1 parents. Figure 4 shows an example of two dendrograms with 4 leaves and 3 parents. Given a set of n contexts that we need to arrange hierarchically, let us denote by G = (V, E) the graph of contexts, where V =</context>
<context position="13857" citStr="Clauset et al., 2008" startWordPosition="2295" endWordPosition="2298">Every pair of nodes (vi, vj) is associated with a unique Dk, which is their lowest common ancestor in the tree. In this manner D partitions the edges that exist in G. The primary assumption in the hierarchical random graph model is that edges in G exist independently, but with a probability that is not identically distributed. In particular, the probability that an edge {vi, vj} exists in G is given by a parameter Bk associated with Dk, the lowest common ancestor of vi and vj in D. In this manner, the topological struc� ture D and the vector of probabilities 0 define the HRG given by H(D, 0) (Clauset et al., 2008). 748 4.2 HRG parameterisation Assuming a uniform prior over all HRGs, the target is to identify the parameters of D and B, so that the chosen HRG is statistically similar to G. Let Dk be an internal node of dendrogram D and f(Dk) be the number of edges between the vertices of the subtrees of the subtree rooted at Dk that actually exist in G. For example, in Figure 4(A), f(D2) = 1, because there is one edge in G connecting vertices B and C. Let l(Dk) be the number of leaves in the left subtree of Dk, and r(Dk) be the number of leaves in the right subtree. For example in Figure 4(A), l(D2) = 2 </context>
<context position="15776" citStr="Clauset et al., 2008" startWordPosition="2640" endWordPosition="2643">all possible dendrograms. Given a graph with n vertices, i.e. n leaves in each dendrogram, the total number of different dendrograms is superexponential ((2n − 3)!! pz� \/2(2n)n−1e−n) (Clauset et al., 2006). To deal with this problem, we use a Markov Chain Monte Carlo (MCMC) method that samples dendrograms from the space of dendrogram models with probability proportional to their likelihood. Each time MCMC samples a dendrogram with a new highest likelihood, that dendrogram is stored. Hence, our goal is to choose the highest likelihood dendrogram once MCMC has converged. Following the work in (Clauset et al., 2008), we pick a set of transitions between dendrograms, where a transition is a re-arrangement of the subtrees of a dendrogram. In particular, given a current dendrogram Dcurr, each internal node Dk of Dcurr is associated with three subtrees of Dcurr. For instance, in Figure 5A, the subtrees st1 and st2 are derived from the two children of Dk and the third st3 from its sibling. Given a current dendrogram, Dcurr, the algorithm proceeds as follows: L(D, �B) = � DkED � [Bθk 1. Choose an internal node, Dk E Dcurr uniL(D) = k (1 − Bk)1−θk]l(Dk)r(Dk) (3) formly. DkED log L(D) = − � h(Bk)l(Dk)r(Dk) (4) D</context>
<context position="17664" citStr="Clauset et al., 2008" startWordPosition="2975" endWordPosition="2978">2 · (1 − 0.5)2) = 0.062. 2. Generate two possible new configurations of the subtrees of Dk (See Figure 5). 3. Choose one of the configurations uniformly to generate a new dendrogram, Dnext. 4. Accept or reject Dnext according to Metropolis-Hastings (MH) rule. 5. If transition is accepted, then Dcurr = Dnext. 6. GOTO 1. According to MH rule (Newman and Barkema, 1999), a transition is accepted if log L(Dnext) &gt; log L(Dcurr); otherwise the transition is accepted with probability L(Dnext) L(D����). These transitions define an ergodic Markov chain, hence its stationary distribution can be reached (Clauset et al., 2008). 749 Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration, (C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this configuration is excluded. In our experiments, we noticed that the algorithm converged relatively quickly. The same behaviour (roughly O(n2) steps) was also noticed in Clauset et al. (2008), when considering graphs with thousands of vertices. 5 HRGs for sense disambiguation 5.1 Sense mapping The output of HRG learning is a dendrogram D with n leaves (con</context>
<context position="29528" citStr="Clauset et al., 2008" startWordPosition="4967" endWordPosition="4970">tter than HRGs. Overall, the comparison of HRGs against HAC has shown that HRGs perform significantly better than HAC when considering connected or less sparse graphs (p3&lt;0.13). This is due to the fact that HAC creates dendrograms, in which connections within the clusters are dense, while connections between the clusters are sparse, i.e. it only considers assortative structures. In contrast, HRGs also consider disassortative dendrograms, i.e. dendrograms in which vertices are less likely to be connected on small scales than on large ones, as well as mixtures of assortative and disassortative (Clauset et al., 2008). This is achieved by allowing the probability Bk of a node k to vary arbitrarily throughout the dendrogram. HAC performs similarly or better than HRGs for largely disconnected and sparse graphs, because HRGs become biased towards disassortative trees which are not a good fit to the GS (Figure 7). Despite that, our evaluation has also shown that the best performance of HAC (F-Score = 86.0% at p1 = 15, p2 = 10, p3 = 0.13) is significantly lower than the best performance of HRGs (F-Score = 87.6% at p1 = 35, p2 = 10, p3 = 0.09). 6.3 Comparison to state-of-the-art methods Table 3 compares the best</context>
<context position="33654" citStr="Clauset et al., 2008" startWordPosition="5641" endWordPosition="5644"> Finally, Table 3 shows that the best performing parameter combination of HRGs achieves a significantly higher F-Score than the best performing parameter combination of HAC, CWU and CWW. Furthermore, HRGs outperform the most frequent sense baseline by 6.7%. 7 Conclusion &amp; future work We presented an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. Our method creates a graph, in which vertices correspond to contexts of a polysemous target word and edges between them are drawn according to their similarity. The hierarchical random graphs algorithm (Clauset et al., 2008) was applied to the constructed graph in order to infer its hierarchical structure, i.e. binary tree. The learned tree provides an induction of the senses of a given word at different levels of sense granularity and was applied to the problem of WSD. The WSD process mapped the tree’s internal nodes to GS senses using a sense tagged corpus, and then tagged new instances by exploiting the structural information provided by the tree. Our experimental results have shown that our graphs exhibit hierarchical organisation that can be captured by HRGs, in effect providing improved WSD performance comp</context>
<context position="34973" citStr="Clauset et al., 2008" startWordPosition="5845" endWordPosition="5848">g with average-linkage has shown that HRGs perform significantly better than HAC when the graphs do not suffer from sparsity (disconnected graphs). The comparison with state-of-the-art sense induction systems has shown that our method yields improvements. Our future work focuses on using different feature types, e.g. dependency relations, second-order cooccurrences, named entities and others to construct our undirected graphs and then applying HRGs, in order to measure the impact of each feature type on the induced hierarchical structures within a WSD setting. Moreover, following the work in (Clauset et al., 2008), we are also working on using MCMC in order to sample more than one dendrogram at equilibrium, and then combine them to a consensus tree. This consensus tree might be able to express a larger amount of topological features of the initial undirected graph. Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a finegrained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. Acknowledgements This work is supported by the European Commissi</context>
</contexts>
<marker>Clauset, Moore, Newman, 2008</marker>
<rawString>Aaron Clauset, Cristopher Moore, and Mark E. J. Newman. 2008. Hierarchical Structure and the Prediction of Missing Links in Networks. Nature, 453(7191):98– 101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn Dongen</author>
</authors>
<title>Performance Criteria for Graph Clustering and Markov Cluster Experiments.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>CWI (Centre for Mathematics and Computer Science),</institution>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="6532" citStr="Dongen, 2000" startWordPosition="1028" endWordPosition="1029">60 to 70). Another graph-based method is presented in (Dorow and Widdows, 2003). They extract only noun neighbours that appear in conjunctions or disjunctions with the target word. Additionally, they extract second-order co-occurrences. Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times. This co-occurrence frequency is also used to weight the edges. The resulting graph is then pruned by removing the target word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free1 graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008). The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. All the described methods for sense inducti</context>
</contexts>
<marker>Dongen, 2000</marker>
<rawString>Stijn Dongen. 2000. Performance Criteria for Graph Clustering and Markov Cluster Experiments. Technical report, CWI (Centre for Mathematics and Computer Science), Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Dominic Widdows</author>
</authors>
<title>Discovering Corpus-specific Word Senses.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL-2003,</booktitle>
<pages>79--82</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="5998" citStr="Dorow and Widdows, 2003" startWordPosition="941" endWordPosition="944">lusters (senses), in order to assign the rest of graph vertices to the identified clusters by utilising the minimum spanning tree of the original graph. In Agirre et al. (2006), the algorithm of V´eronis (2004) is analysed and assessed on the SensEval-3 dataset (Snyder and Palmer, 2004), after optimising its parameters on the SensEval-2 dataset (Edmonds and Dorow, 2001). The results show that the WSD F-Score outperforms the Most Frequent Sense (MFS) baseline by approximately 10%, while inducing a large number of clusters (with averages of 60 to 70). Another graph-based method is presented in (Dorow and Widdows, 2003). They extract only noun neighbours that appear in conjunctions or disjunctions with the target word. Additionally, they extract second-order co-occurrences. Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times. This co-occurrence frequency is also used to weight the edges. The resulting graph is then pruned by removing the target word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (sense</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>Beate Dorow and Dominic Widdows. 2003. Discovering Corpus-specific Word Senses. In Proceedings of the EACL-2003, pages 79–82, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8656" citStr="Dunning, 1993" startWordPosition="1390" endWordPosition="1391">note the base corpus consisting of the contexts containing the target word tw. In our work, a context is defined as a paragraph2 containing the target word. The aim of this stage is to capture nouns contextually related to tw. Initially, the target word is removed from bc and part-of-speech tagging is applied to each context. Following the work in (V´eronis, 2004; Agirre et al., 2006) only nouns are kept and lemmatised. In the next step, the distribution of each noun in the base corpus is compared to the distribution of the same noun in a reference corpus3 using the log-likelihood ratio (G2) (Dunning, 1993). Nouns with a G2 below a pre-specified threshold (parameter p1) are removed from each paragraph of the base corpus. The upper left part of Figure 3 shows the words kept as a result of this stage. 3.2 Graph creation Graph vertices: To create the graph of vertices, we represent each context ci as a vertex in a graph G. Graph edges: Edges between the vertices of the graph are drawn based on their similarity, defined in Equation 1, where simcl(ci, cj) is the collocational weight of contexts ci, cj and simwd(ci, cj) is their bag-of-words weight. If the edge weight W (ci, cj) is above a prespecifie</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Edmonds</author>
<author>Beate Dorow</author>
</authors>
<title>Senseval-2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of SensEval-2,</booktitle>
<pages>1--5</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="5746" citStr="Edmonds and Dorow, 2001" startWordPosition="899" endWordPosition="903">, in Figure 2 the highest degree vertex, news, is the first hub, which would be deleted along with its direct neighbours. The deleted region corresponds to the newspaper sense of the target word paper. V´eronis (2004) further processed the identified clusters (senses), in order to assign the rest of graph vertices to the identified clusters by utilising the minimum spanning tree of the original graph. In Agirre et al. (2006), the algorithm of V´eronis (2004) is analysed and assessed on the SensEval-3 dataset (Snyder and Palmer, 2004), after optimising its parameters on the SensEval-2 dataset (Edmonds and Dorow, 2001). The results show that the WSD F-Score outperforms the Most Frequent Sense (MFS) baseline by approximately 10%, while inducing a large number of clusters (with averages of 60 to 70). Another graph-based method is presented in (Dorow and Widdows, 2003). They extract only noun neighbours that appear in conjunctions or disjunctions with the target word. Additionally, they extract second-order co-occurrences. Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times. This co-occur</context>
</contexts>
<marker>Edmonds, Dorow, 2001</marker>
<rawString>Phil Edmonds and Beate Dorow. 2001. Senseval-2: Overview. In Proceedings of SensEval-2, pages 1–5, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis P Klapaftis</author>
<author>Suresh Manandhar</author>
</authors>
<title>Word Sense Induction Using Graphs of Collocations.</title>
<date>2008</date>
<booktitle>In Proceedings of ECAI-2008,</booktitle>
<pages>298--302</pages>
<location>Patras, Greece.</location>
<contexts>
<context position="6946" citStr="Klapaftis and Manandhar, 2008" startWordPosition="1097" endWordPosition="1100">number of times. This co-occurrence frequency is also used to weight the edges. The resulting graph is then pruned by removing the target word and vertices with a low degree. Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free1 graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008). The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. All the described methods for sense induction ap1One needs to specify only the number of iterations. The number of clusters is generated automatically. 746 Figure 2: Graph of words for the target word paper. Numbers inside vertices correspond to their degree. Figure 3: Running example of graph creation ply flat graph clustering methods to derive the clusters (senses) of a target word. As a result, they neglect the fact that their constructed graphs ofte</context>
<context position="32022" citStr="Klapaftis and Manandhar, 2008" startWordPosition="5383" endWordPosition="5386">rd co-occurrences and local collocations. The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs against recent methods &amp; baselines. cording to the co-occurrence frequency of the corresponding collocations. The constructed graph is smoothed to identify more edges between vertices and then clustered using Chinese Whispers (Biemann, 2006). This method is related to the basic inputs of our presented method. Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs. The previous section has shown that inferring the hierarchical structure of graphs leads to supe</context>
<context position="31740" citStr="Klapaftis &amp; Manandhar (2008)" startWordPosition="5337" endWordPosition="5340">used on extracting a single hierarchy combining word co-occurrence and bigram features. Niu et al. (2007) developed a vector-based method that performs sense induction by grouping the contexts of a target word using three types of features, i.e. POS tags of neighbouring words, word co-occurrences and local collocations. The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs against recent methods &amp; baselines. cording to the co-occurrence frequency of the corresponding collocations. The constructed graph is smoothed to identify more edges between vertices and then clustered using Chinese Whispers (Biemann, 2006</context>
<context position="32991" citStr="Klapaftis &amp; Manandhar (2008)" startWordPosition="5533" endWordPosition="5537">ed to the basic inputs of our presented method. Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs. The previous section has shown that inferring the hierarchical structure of graphs leads to superior WSD performance. Pedersen (2007) presented SenseClusters, a vector-based method that clusters second order cooccurrence vectors using k-means, where k is automatically determined using the Adapted Gap Statistic (Pedersen and Kulkarni, 2006). As can be observed, HRGs perform significantly better than the methods of Pedersen (2007) and Klapaftis &amp; Manandhar (2008) (McNemar’s test at 95% confidence level). Finally, Table 3 shows that the best performing parameter combination of HRGs achieves a significantly higher F-Score than the best performing parameter combination of HAC, CWU and CWW. Furthermore, HRGs outperform the most frequent sense baseline by 6.7%. 7 Conclusion &amp; future work We presented an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. Our method creates a graph, in which vertices correspond to contexts of a polysemous target word and edges between them are drawn according to their similarity. </context>
</contexts>
<marker>Klapaftis, Manandhar, 2008</marker>
<rawString>Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word Sense Induction Using Graphs of Collocations. In Proceedings of ECAI-2008, pages 298–302, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis P Klapaftis</author>
<author>Suresh Manandhar</author>
</authors>
<title>Taxonomy Learning Using Word Sense Induction.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT-2010,</booktitle>
<pages>82--90</pages>
<publisher>ACL.</publisher>
<location>Los Angeles, California,</location>
<contexts>
<context position="22915" citStr="Klapaftis and Manandhar, 2010" startWordPosition="3830" endWordPosition="3833">econd aim of our evaluation is to assess whether the hierarchical structure inferred by HRGs is more informative than the hierarchical structure inferred by traditional Hierarchical Clustering (HAC). Hence, our third baseline, takes as input a similarity matrix of the graph vertices and performs bottom-up clustering with average-linkage, which has already been used in WSI in (Pantel and Lin, 4The number of iterations for CW was set to 200. 2003) and was shown to have superior or similar performance to single-linkage and complete-linkage in the related problem of learning a taxonomy of senses (Klapaftis and Manandhar, 2010). To calculate the similarity matrix of vertices we follow a process similar to the one used in Section 4.2 for calculating the probability of an internal node. The similarity between two vertices is calculated according to the degree of connectedness among their direct neighbours. Specifically, we would like to assign high similarity to pairs of vertices, whose neighbours are close to forming a clique. Given two vertices (contexts) ci and cj, let N(ci7 cj) be the set of their neighbours and K(ci7 cj) be the set of edges between the vertices in N(ci7 cj). The maximum number of edges that could</context>
</contexts>
<marker>Klapaftis, Manandhar, 2010</marker>
<rawString>Ioannis P. Klapaftis and Suresh Manandhar. 2010. Taxonomy Learning Using Word Sense Induction. In Proceedings of NAACL-HLT-2010, pages 82–90, Los Angeles, California, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 Task 14: Word Sense Induction &amp; Disambiguation. In Proceedings of SemEval-2,</booktitle>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach, and Sameer S. Pradhan. 2010. Semeval-2010 Task 14: Word Sense Induction &amp; Disambiguation. In Proceedings of SemEval-2, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1687" citStr="Mihalcea (2004)" startWordPosition="246" endWordPosition="247">lustering yielding improvements over state-of-the-art WSD systems based on sense induction. 1 Introduction A number of NLP problems can be cast into a graphbased framework, in which entities are represented as vertices in a graph and relations between them are depicted by weighted or unweighted edges. For instance, in unsupervised WSD a number of methods (Widdows and Dorow, 2002; V´eronis, 2004; Agirre et al., 2006) have constructed word co-occurrence graphs for a target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns. Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences. In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text (Ramage et al., 2009). Recent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a </context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 20, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Newman</author>
<author>Gerard Barkema</author>
</authors>
<title>Monte Carlo Methods in Statistical Physics.</title>
<date>1999</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford:</location>
<contexts>
<context position="17411" citStr="Newman and Barkema, 1999" startWordPosition="2937" endWordPosition="2940">), since it provides a better division of the network leaves. Particularly, the likelihood of 4(A) is L(D1) = (11 · (1 − 1)1) · (11 · (1 − 1)1) · (0.251 · (1 − 0.25)3) = 0.105, while the likelihood of 4(B) is L(D2) = (00 · (1 − 0)1) · (11 · (1 − 1)1) · (0.52 · (1 − 0.5)2) = 0.062. 2. Generate two possible new configurations of the subtrees of Dk (See Figure 5). 3. Choose one of the configurations uniformly to generate a new dendrogram, Dnext. 4. Accept or reject Dnext according to Metropolis-Hastings (MH) rule. 5. If transition is accepted, then Dcurr = Dnext. 6. GOTO 1. According to MH rule (Newman and Barkema, 1999), a transition is accepted if log L(Dnext) &gt; log L(Dcurr); otherwise the transition is accepted with probability L(Dnext) L(D����). These transitions define an ergodic Markov chain, hence its stationary distribution can be reached (Clauset et al., 2008). 749 Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration, (C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this configuration is excluded. In our experiments, we noticed that the algorithm converged relatively quick</context>
</contexts>
<marker>Newman, Barkema, 1999</marker>
<rawString>Mark Newman and Gerard Barkema. 1999. Monte Carlo Methods in Statistical Physics. Oxford: Clarendon Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew-Lim Tan</author>
</authors>
<title>I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense Disambiguation, and English Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>177--182</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="31217" citStr="Niu et al. (2007)" startWordPosition="5255" endWordPosition="5258"> Lapata, 2009). A significant advantage of their method is the inclusion of more than one layer in the LDA setting, where each layer corresponds to a different feature type e.g. dependency relations, bigrams, etc. The inclusion of different feature types as separate models in the sense induction process can easily be modeled in our setting, by inferring a different hierarchy of target word instances according to each feature type, and then combining all of them to a consensus tree. In this work, we have focused on extracting a single hierarchy combining word co-occurrence and bigram features. Niu et al. (2007) developed a vector-based method that performs sense induction by grouping the contexts of a target word using three types of features, i.e. POS tags of neighbouring words, word co-occurrences and local collocations. The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond</context>
</contexts>
<marker>Niu, Ji, Tan, 2007</marker>
<rawString>Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense Disambiguation, and English Word Sense Disambiguation. In Proceedings of SemEval-2007, pages 177–182, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Automatically Discovering Word Senses.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACLHLT-2003,</booktitle>
<pages>21--22</pages>
<location>Morristown, NJ, USA.</location>
<marker>Pantel, Lin, 2003</marker>
<rawString>Patrick Pantel and Dekang Lin. 2003. Automatically Discovering Word Senses. In Proceedings of NAACLHLT-2003, pages 21–22, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Automatic Cluster Stopping With Criterion Functions and the gap Statistic.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference of the North American Chapter of the ACL on Human Language Technology,</booktitle>
<pages>276--279</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="32867" citStr="Pedersen and Kulkarni, 2006" startWordPosition="5513" endWordPosition="5516">thed to identify more edges between vertices and then clustered using Chinese Whispers (Biemann, 2006). This method is related to the basic inputs of our presented method. Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs. The previous section has shown that inferring the hierarchical structure of graphs leads to superior WSD performance. Pedersen (2007) presented SenseClusters, a vector-based method that clusters second order cooccurrence vectors using k-means, where k is automatically determined using the Adapted Gap Statistic (Pedersen and Kulkarni, 2006). As can be observed, HRGs perform significantly better than the methods of Pedersen (2007) and Klapaftis &amp; Manandhar (2008) (McNemar’s test at 95% confidence level). Finally, Table 3 shows that the best performing parameter combination of HRGs achieves a significantly higher F-Score than the best performing parameter combination of HAC, CWU and CWW. Furthermore, HRGs outperform the most frequent sense baseline by 6.7%. 7 Conclusion &amp; future work We presented an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. Our method creates a graph, in which </context>
</contexts>
<marker>Pedersen, Kulkarni, 2006</marker>
<rawString>Ted Pedersen and Anagha Kulkarni. 2006. Automatic Cluster Stopping With Criterion Functions and the gap Statistic. In Proceedings of the 2006 Conference of the North American Chapter of the ACL on Human Language Technology, pages 276–279, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>UMND2 : Senseclusters Applied to the Sense Induction Task of Senseval-4.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="32071" citStr="Pedersen, 2007" startWordPosition="5394" endWordPosition="5395">tion bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs against recent methods &amp; baselines. cording to the co-occurrence frequency of the corresponding collocations. The constructed graph is smoothed to identify more edges between vertices and then clustered using Chinese Whispers (Biemann, 2006). This method is related to the basic inputs of our presented method. Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs. The previous section has shown that inferring the hierarchical structure of graphs leads to superior WSD performance. Pedersen (2007) presented S</context>
</contexts>
<marker>Pedersen, 2007</marker>
<rawString>Ted Pedersen. 2007. UMND2 : Senseclusters Applied to the Sense Induction Task of Senseval-4. In Proceedings of SemEval-2007, pages 394–397, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Random Walks for Text Semantic Similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of TextGraphs-4,</booktitle>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2164" citStr="Ramage et al., 2009" startWordPosition="323" endWordPosition="326">get polysemous word and applied graph-clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns. Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences. In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text (Ramage et al., 2009). Recent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves. This hierarchical structure provides additional information as opposed to flat clustering by explicitly including organisation at all scales of a graph (Clauset et al., 2008). In this paper, we present an unsupervised method for inferring the hierarchical structure (binary tree) of a graph, in which vertices are the contexts of a </context>
</contexts>
<marker>Ramage, Rafferty, Manning, 2009</marker>
<rawString>Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning. 2009. Random Walks for Text Semantic Similarity. In Proceedings of TextGraphs-4, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Slonim</author>
<author>Nir Friedman</author>
<author>Naftali Tishby</author>
</authors>
<title>Unsupervised Document Classification Using Sequential Information Maximization.</title>
<date>2002</date>
<booktitle>In SIGIR 2002,</booktitle>
<pages>129--136</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31503" citStr="Slonim et al., 2002" startWordPosition="5298" endWordPosition="5301">nse induction process can easily be modeled in our setting, by inferring a different hierarchy of target word instances according to each feature type, and then combining all of them to a consensus tree. In this work, we have focused on extracting a single hierarchy combining word co-occurrence and bigram features. Niu et al. (2007) developed a vector-based method that performs sense induction by grouping the contexts of a target word using three types of features, i.e. POS tags of neighbouring words, word co-occurrences and local collocations. The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering. HRGs perform slightly better than the methods of Brody &amp; Lapata (2009) and Niu et al. (2007), although the differences are not significant (McNemar’s test at 95% confidence level). Klapaftis &amp; Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac753 System Performance (%) HRGs 87.6 (Brody and Lapata, 2009) 87.3 (Niu et al., 2007) 86.8 (Klapaftis and Manandhar, 2008) 86.4 HAC 86.0 CWU 85.1 CWW 84.7 (Pedersen, 2007) 84.5 MFS 80.9 Table 3: HRGs aga</context>
</contexts>
<marker>Slonim, Friedman, Tishby, 2002</marker>
<rawString>Noam Slonim, Nir Friedman, and Naftali Tishby. 2002. Unsupervised Document Classification Using Sequential Information Maximization. In SIGIR 2002, pages 129–136, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English All-words Task.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3,</booktitle>
<pages>41--43</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<location>Barcelona, Spain.</location>
<contexts>
<context position="5661" citStr="Snyder and Palmer, 2004" startWordPosition="886" endWordPosition="889"> along with its direct neighbours from the graph producing a new cluster. For example, in Figure 2 the highest degree vertex, news, is the first hub, which would be deleted along with its direct neighbours. The deleted region corresponds to the newspaper sense of the target word paper. V´eronis (2004) further processed the identified clusters (senses), in order to assign the rest of graph vertices to the identified clusters by utilising the minimum spanning tree of the original graph. In Agirre et al. (2006), the algorithm of V´eronis (2004) is analysed and assessed on the SensEval-3 dataset (Snyder and Palmer, 2004), after optimising its parameters on the SensEval-2 dataset (Edmonds and Dorow, 2001). The results show that the WSD F-Score outperforms the Most Frequent Sense (MFS) baseline by approximately 10%, while inducing a large number of clusters (with averages of 60 to 70). Another graph-based method is presented in (Dorow and Widdows, 2003). They extract only noun neighbours that appear in conjunctions or disjunctions with the target word. Additionally, they extract second-order co-occurrences. Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-o</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English All-words Task. In Rada Mihalcea and Phil Edmonds, editors, In Proceedings of Senseval-3, pages 41–43, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Hyperlex: Lexical Cartography for Information Retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. Hyperlex: Lexical Cartography for Information Retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising Measures of Lexical Distributional Similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-2004,</booktitle>
<pages>10--15</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10612" citStr="Weeds et al. (2004)" startWordPosition="1726" endWordPosition="1729">ith a vector of collocations (vi). The upper right part of Figure 3 shows the collocations associated with each context of our example. 2Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). 3The British National Corpus, 2001, Distributed by Oxford University Computing Services. (N ) 2 747 Given two contexts ci and cj, we calculate their collocational weight using the Jaccard coefficient on the collocational vectors, i.e. simj(ci, cj) = |vi∩vj|The selection of Jaccard is based on the work vi∪vj | of Weeds et al. (2004), who analyzed the variation in a word’s distributionally nearest neighbours with respect to a variety of similarity measures. Their analysis showed that there are three classes of measures, i.e. those selecting distributionally more general neighbours (e.g. cosine), those selecting distributionally less general neighbours (e.g. AMCRMPrecision (Weeds et al., 2004)) and those without a bias towards the distributional generality of a neighbour (e.g. Jaccard). In our setting, we are interested in calculating the similarity between two contexts without any bias. We selected Jaccard, since the rest</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising Measures of Lexical Distributional Similarity. In Proceedings of COLING-2004, pages 10–15, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of Coling-2002,</booktitle>
<pages>1--7</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1453" citStr="Widdows and Dorow, 2002" startWordPosition="211" endWordPosition="214">he senses of a polysemous word. The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs significantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction. 1 Introduction A number of NLP problems can be cast into a graphbased framework, in which entities are represented as vertices in a graph and relations between them are depicted by weighted or unweighted edges. For instance, in unsupervised WSD a number of methods (Widdows and Dorow, 2002; V´eronis, 2004; Agirre et al., 2006) have constructed word co-occurrence graphs for a target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word. Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns. Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences. In the same vein, graph-based me</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In Proceedings of Coling-2002, pages 1–7, Morristown, NJ, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>