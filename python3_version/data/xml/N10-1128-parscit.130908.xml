<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.969201">
Context-free reordering, finite-state translation
</title>
<author confidence="0.990159">
Chris Dyer and Philip Resnik
</author>
<affiliation confidence="0.997195333333333">
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
</affiliation>
<email confidence="0.641548">
redpony, resnik AT umd.edu
</email>
<sectionHeader confidence="0.995759" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999648526315789">
We describe a class of translation model in
which a set of input variants encoded as a
context-free forest is translated using a finite-
state translation model. The forest structure of
the input is well-suited to representing word
order alternatives, making it straightforward to
model translation as a two step process: (1)
tree-based source reordering and (2) phrase
transduction. By treating the reordering pro-
cess as a latent variable in a probabilistic trans-
lation model, we can learn a long-range source
reordering model without example reordered
sentences, which are problematic to construct.
The resulting model has state-of-the-art trans-
lation performance, uses linguistically moti-
vated features to effectively model long range
reordering, and is significantly smaller than a
comparable hierarchical phrase-based transla-
tion model.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999181181818182">
Translation models based on synchronous context-
free grammars (SCFGs) have become widespread in
recent years (Wu, 1997; Chiang, 2007). Compared
to phrase-based models, which can be represented as
finite-state transducers (FSTs, Kumar et al. (2006)),
one important benefit that SCFG models have is the
ability to process long range reordering patterns in
space and time that is polynomial in the length of
the displacement, whereas an FST must generally
explore a number of states that is exponential in
this length.1 As one would expect, for language
</bodyText>
<footnote confidence="0.849417333333333">
1Our interest here is the reordering made possible by varying
the arrangement of the translation units, not the local word order
differences captured inside memorized phrase pairs.
</footnote>
<bodyText confidence="0.999335714285714">
pairs with substantial structural differences (and thus
requiring long-range reordering during translation),
SCFG models have come to outperform the best FST
models (Zollmann et al., 2008).
In this paper, we explore a new way to take advan-
tage of the computational benefits of CFGs during
translation. Rather than using a single SCFG to both
reorder and translate a source sentence into the target
language, we break the translation process into a two
step pipeline where (1) the source language is re-
ordered into a target-like order, with alternatives en-
coded in a context-free forest, and (2) the reordered
source is transduced into the target language using
an FST that represents phrasal correspondences.
While multi-step decompositions of the transla-
tion problem have been proposed before (Kumar et
al., 2006), they are less practical with the rise of
SCFG models, since the context-free languages are
not closed under intersection (Hopcroft and Ullman,
1979). However, the CFLs are closed under intersec-
tion with regular languages. By restricting ourselves
to a finite-state phrase transducer and representing
reorderings of the source in a context-free forest, ex-
act inference over the composition of the two models
is possible.
The paper proceeds as follows. We first ex-
plore reordering forests and describe how to trans-
late them with an FST (§2). Since we would like our
reordering model to discriminate between good re-
orderings of the source and bad ones, we show how
to train our reordering component as a latent variable
in an end-to-end translation model (§3). We then
presents experimental results on language pairs re-
quiring small amounts and large amounts of reorder-
ing (§4). We conclude with a discussion of related
</bodyText>
<page confidence="0.969528">
858
</page>
<note confidence="0.7882325">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 858–866,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.505945">
work (§6) and possible extensions (§7).
</bodyText>
<sectionHeader confidence="0.956473" genericHeader="method">
2 Reordering forests and translation
</sectionHeader>
<bodyText confidence="0.999973888888889">
In this section, we describe source reordering
forests, a context-free representation of source lan-
guage word order alternatives.2 The basic idea is
that for the source sentence, f, that is to be trans-
lated, we want to create a (monolingual) context-free
grammar F that generates strings (f) of words in the
source language that are permutations of the origi-
nal sentence. Specifically, this forest should contain
derivations that put the source words into an order
that approximates how they will be ordered in the
grammar of the target language.
For a concrete example, let us consider the task of
English-Japanese translation.3 Our input sentence
is John ate an apple. Japanese is a head-final lan-
guage, where the heads of phrases (such as the verb
in a verb phrase) typically come last, and English
is a head-initial language, where heads come first.
As a result, the usual order for a declarative sen-
tence in English is SVO (subject-verb-object), but
in Japanese, it is SOV, and the desired translation
is John-ga ringo-o [an apple] tabeta [ate]. In sum-
mary, when translating from English into Japanese,
it is usually necessary to move verbs from their po-
sition between the subject and object to the end of
the sentence.
This reordering can happen in two ways, which
we depict in Figure 1. In the derivation on the left,
a memorized phrase pair captures the movement of
the verb (Koehn et al., 2003). In the other deriva-
tion, the source is first reordered into target word
order and then translated, using smaller translation
units. In addition, we have assumed that the phrase
translations were learned from a parallel corpus that
is in the original ordering, so the reordering forest F
should include derivations of phrase-size units in the
source order as well as the target order.
</bodyText>
<footnote confidence="0.9557117">
2Note that forests are isomorphic to context-free grammars.
For example, what is referred to as the ‘parse forest’, and un-
derstood to encode all derivations of a sentence s under some
grammar, can also be understood as being a context-free gram-
mar itself that exactly generates s. We therefore refer to a forest
as a grammar sometimes, or vice versa, depending on which
characterization is clearer in context.
3We use English as the source language since we expect the
parse structure of English sentences will be more familiar to
many readers.
</footnote>
<figureCaption confidence="0.997846666666667">
Figure 2: A fragment of a phrase-based English-Japanese
translation model, represented as an FST. Japanese ro-
manization is given in brackets.
</figureCaption>
<bodyText confidence="0.9997515">
A minimal reordering forest that supports the
derivations depicted needs to include both an SOV
and SVO version of the source. This could be ac-
complished trivially with the following grammar:
</bodyText>
<subsectionHeader confidence="0.2971725">
S → John ate an apple
S → John an apple ate
</subsectionHeader>
<bodyText confidence="0.999759">
However, this grammar misses the opportunity to
take advantage of the regularities in the permuted
structure. A better alternative might be:
</bodyText>
<equation confidence="0.94489225">
S → John VP
VP → ate NP
VP → NP ate
NP → an apple
</equation>
<bodyText confidence="0.99938">
In this grammar, the phrases John and an apple are
fixed and only the VP contains ordering ambiguity.
</bodyText>
<subsectionHeader confidence="0.982882">
2.1 Reordering forests based on source parses
</subsectionHeader>
<bodyText confidence="0.999967909090909">
Many kinds of reordering forests are possible; in
general, the best one for a particular language pair
will be one that is easiest to create given the re-
sources available in the source language. It will
also be the one that most compactly expresses the
source reorderings that are most likely to be use-
ful for translation. In this paper, we consider a
particular kind of reordering forest that is inspired
by the reordering model of Yamada and Knight
(2001).4 These are generated by taking a source lan-
guage parse tree and ‘expanding’ each node so that it
</bodyText>
<footnote confidence="0.987399833333333">
4One important difference is that our translation model is not
restricted by the structure of the source parse tree; i.e., phrases
used in transduction need not correspond to constituents in the
source reordering forest. However, if a phrase does cross a con-
stituent boundary between constituents A and B, then transla-
tions that use that phrase will have A and B adjacent.
</footnote>
<equation confidence="0.428535583333333">
an : ε ate : ε
an : ε
John : pieibt
[John-ga]
3 2
0 1
apple : リンゴを :R^Zt
[ringo-o tabeta]
apple : リンゴを
[ringo-o]
[tabeta]
ate : :R^�t
</equation>
<page confidence="0.643139">
859
</page>
<figure confidence="0.999537111111111">
John ate an apple
ate an apple
John ate an apple
John an apple ate
f
f&apos;
John
pie�lbt
e
John-ga
&apos;J%7;Lr :ft/`�t
ringo-o tabeta
pie�lbt
John-ga
&apos;J%7;Lr
ringo-o
:ftl,�t
tabeta
</figure>
<figureCaption confidence="0.999986">
Figure 1: Two possible derivations of a Japanese translation of an English source sentence.
</figureCaption>
<bodyText confidence="0.997814769230769">
rewrites with different permutations of its children.5
For an illustration using our example sentence, re-
fer to Figure 3 for the forest representation and Fig-
ure 4 for its isomorphic CFG representation. It is
easy to see that this forest generates the two ‘good’
order variants from Figure 1; however, the forest in-
cludes many other derivations that will probably not
lead to good translations. For this reason, it is help-
ful to associate the edges in the forest (that is, the
rules in the CFG) with weights reflecting how likely
that rule is to lead to a good translation. We discuss
how these weights can be learned automatically in
§3.
</bodyText>
<subsectionHeader confidence="0.999409">
2.2 Translating reordering forests with FSTs
</subsectionHeader>
<bodyText confidence="0.999530571428571">
Having described how to construct a context-free re-
ordering forest for the source sentence, we now turn
to the problem of how to translate the source forest
into the target language using a phrase-based trans-
lation model encoded as an FST, e.g. Figure 2. The
process is quite similar to the one used when trans-
lating a source sentence with an SCFG, but with a
twist: rather than representing the translation model
as a grammar and parsing the source sentence, we
represent the source sentence as a grammar (i.e. its
reordering forest), and we use it to ‘parse’ the trans-
lation model (i.e. the FST representation of the
phrase-based model). The end result (either way!)
is a translation forest containing all possible target-
language translations of the source.
Parsing can be understood as a means of comput-
ing the intersection of an FSA and a CFG (Grune and
Jacobs, 2008). Since we are dealing with FSTs that
define binary relations over strings, not FSAs defin-
ing strings, this operation is more properly compo-
sition. However, since CFG/FSA intersection is less
</bodyText>
<footnote confidence="0.971941">
5For computational tractability, we only consider all permu-
tations only when the number of children is less than 5, other-
wise we exclude permutations where a child moves more than
4 positions away from where it starts.
</footnote>
<bodyText confidence="0.994946347826087">
cumbersome to describe, we present the algorithm
in terms of intersection.
To compute the composition of a reordering for-
est, !g, with an FSA, F, we will make use of a variant
of Earley’s algorithm (Earley, 1970). Let weighted
finite-state automaton F = (E, Q, q0, qfinal, S, w).
E is a finite alphabet; Q is a set of states; q0 and
qfinal E Q are start and accept states, respectively,6 S
is the transition function Q x E 2Q, and w is the
transition cost function Q x Q R. We use vari-
ables that refer to states in the FSA with the letters
q, r, and s. We use x to represent a variable that is
an element of E. Variables u and v represent costs.
X and Y are non-terminals. Lowercase Greek let-
ters are strings of terminals and non-terminals. The
function S(q, x) returns the state(s) that are reach-
able from state q by taking a transition labeled with
x in the FSA.
Figure 5 provides the inference rules for a
top-down intersection algorithm in the form of a
weighted logic program; the three inference rules
correspond to Earley’s SCAN, PREDICT, and COM-
PLETE, respectively.
</bodyText>
<sectionHeader confidence="0.964084" genericHeader="method">
3 Reordering and translation model
</sectionHeader>
<bodyText confidence="0.999933272727273">
As pointed out in §2.1, our reordering forests may
contain many paths, some of which when translated
will lead to good translations and others that will be
bad. We would like a model to distinguish the two.
If we had a parallel corpus of source language
sentences paired with ‘reference reorderings‘, such
a model could be learned directly as a supervised
learning task. However, creating the optimal target-
language reordering fffor some f is a nontrivial
task.7 Instead of trying to solve this problem, we
opt to treat the reordered from of the source, f, as a
</bodyText>
<footnote confidence="0.77667475">
6Other FSA definitions permit sets of start and final states.
We use the more restricted definition for simplicity and because
in our FSTs qo = qfinal.
7For a discussion of methods for generating reference re-
</footnote>
<page confidence="0.989521">
860
</page>
<figure confidence="0.97946075">
Original parse:
John ate an apple
Reordering forest:
John ate an apple
</figure>
<figureCaption confidence="0.99529975">
Figure 3: Example of a reordering forest. Linearization
order of non-terminals is indicated by the index at the tail
of each edge. The isomorphic CFG is shown in Figure 4;
dashed edges correspond to reordering-specific rules.
</figureCaption>
<bodyText confidence="0.999877625">
latent variable in a probabilistic translation model.
By doing this, we only require a parallel corpus of
translations to learn the reordering model. Not only
does this make our lives easier, since ‘reference re-
orderings’ are not necessary, but it is also intuitively
satisfying because from a task perspective, we are
not concerned with values of f0, but only with pro-
ducing a good translation e.
</bodyText>
<subsectionHeader confidence="0.996381">
3.1 A probabilistic translation model with a
latent reordering variable
</subsectionHeader>
<bodyText confidence="0.99449625">
The translation model we use is a two phase process.
First, source sentence f is reordered into a target-
like word order f0 according to a reordering model
r(f0|f). The reordered source is then transduced into
the target language according to a translation model
t(e|f0). We require that r(f0|f) can be represented by
orderings from word aligned parallel corpora, refer to Tromble
and Eisner (2009).
</bodyText>
<figure confidence="0.82878425">
Original parse grammar: S → NPsubj VP
VP → V NPobj NPobj → DT NN
NPsubj → John V → ate
DT → an NN → apple
Additional reordering grammar rules:
S → VP NPsubj
VP → NPobj V
NPobj → NN DT
</figure>
<figureCaption confidence="0.9950954">
Figure 4: Context-free grammar representation of the for-
est in Figure 3. The reordering grammar contains the
parse grammar, plus the reordering-specific rules.
Figure 5: Weighted logic program for computing the in-
tersection of a weighted FSA and a weighted CFG.
</figureCaption>
<bodyText confidence="0.998706444444444">
a recursion-free probabilistic context-free grammar,
i.e. a forest as in §2.1, and that t(e|f0) is represented
by a (cyclic) finite-state transducer, as in Figure 2.
Since the reordering forest may define multiple
derivations a from f to a particular f0, and the trans-
ducer may define multiple derivations d from f0 to
a particular translation e, we marginalize over these
nuisance variables as follows to define the probabil-
ity of a translation given the source:
</bodyText>
<equation confidence="0.994091">
p(e|f) = 1: 1: t(e, d|f0) r(f0, a|f) (1)
</equation>
<bodyText confidence="0.949515333333333">
d f� a
Crucially, since we have restricted r(f0|f) to have
the form of a weighted CFG and t(e|f0) to be an
</bodyText>
<figure confidence="0.943130807692308">
NPsubj
1 1
V DT 1 NN
S
2
VP
2
NPobj
2
2
2
2
2
1 1
S
2
1
VP
2
1
NPobj
NPsubj
V DT 1 1 NN
Initialization:
[S&apos; → •S, qo, qo] : 1
Inference rules:
</figure>
<equation confidence="0.92555825">
[X → α • x0, q, r] : u
[X → αx • 0, q, S(r, x)] : u ⊗ w(S(r, x))
[X → α • Y0, q, r] Y U→ y ∈ G
[Y → •y, r, r] : u
[X → α • Y 0, q, s] : u [Y → y•, s, r] : v
[X → αY • 0, q, r] : u ⊗ v
Goal state:
[S&apos; → S•, qo, qfinal]
</equation>
<page confidence="0.958275">
861
</page>
<bodyText confidence="0.99997675">
FST, the quantity (1), which sums over all reorder-
ings (and derivations), can be computed in polyno-
mial time with dynamic programming composition,
as described in §2.2.
</bodyText>
<subsectionHeader confidence="0.999433">
3.2 Conditional training
</subsectionHeader>
<bodyText confidence="0.9803052">
While it is straightforward to use expectation maxi-
mization to optimize the joint likelihood of the paral-
lel training data with a latent variable model, instead
we use a log-linear parameterization and maximize
conditional likelihood (Blunsom et al., 2008; Petrov
and Klein, 2008). This enables us to employ a rich
set of (possibly overlapping, non-independent) fea-
tures to discriminate among translations. The proba-
bility of a derivation from source to reordered source
to target is thus written in terms of model parameters
</bodyText>
<equation confidence="0.988511666666667">
Λ = {λi} as:
p(e, d, f; a|f; Λ) = exp Ei λi · Hi(e, d, f�,a,f)
Z(f; Λ)
</equation>
<bodyText confidence="0.947859666666667">
�where Hi(e, d, f, a, f) = hi(f�, r) + � hi(f, s)
rEd sEa
The derivation probability is globally normalized by
the partition Z(f; Λ), which is just the sum of the
numerator for all derivations of f (corresponding to
any e). The Hi (written below without their argu-
ments) are real-valued feature functions that may
be overlapping and non-independent. For compu-
tational tractability, we assume that the feature func-
tions Hi decompose with the derivations of fannd e
in terms of local feature functions hi. We also de-
fine Z(e, f; λ) to be the sum of the numerator over all
derivations that yield the sentence pair he, fi. Rather
than training purely to optimize conditional likeli-
hood, we also make use of a spherical Gaussian prior
on the value of Λ with mean 0 and variance σ2,
which helps prevent overfitting of the model (Chen
and Rosenfeld, 1998). Our objective is thus to select
Λ minimizing:
The gradient of L with respect to the feature weights
has a parallel form; it is the difference in feature ex-
pectations under the reference distribution and the
translation distribution with a penalty term due to
the prior:
</bodyText>
<equation confidence="0.9188345">
∂L
∂λi
</equation>
<bodyText confidence="0.999868111111111">
The form of the objective and gradient are quite sim-
ilar to the traditional fully observed training scenario
for CRFs (Sha and Pereira, 2003). However, rather
than matching the feature expectations in the model
to an observable feature value, we have to sum over
the latent structure that remains after observing our
target e, which makes the form of the first summand
an expectation rather than just a feature function
value.
</bodyText>
<subsectionHeader confidence="0.843115">
3.2.1 Computing the objective and gradient
</subsectionHeader>
<bodyText confidence="0.9997802">
The objective and gradient that were just introduced
can be computed in two steps. Given a training pair
he, fi, we generate the forest of reorderings F from f
as described in §2.1. We then compose this grammar
with T, the FST representing the translation model,
which yields F ◦ T, a translation forest that contains
all possible translations of f into the target language,
as described in §2.2. Running the inside algorithm
on the translation forest computes Z(f; Λ), the first
term in the objective, and the inside-outside algo-
rithm can be used to compute E° p(e,d,a|f)[hi]. Next,
to compute Z(e, f; Λ) and the first expectation in the
gradient, we need to find the subset of the transla-
tion forest F ◦ T that exactly derives the reference
translation e. To do this, we again rely on the fact
that F ◦ T is a forest and therefore itself a context-
free grammar. So, we use this grammar to parse
the target reference string e. The resulting forest,
F ◦ T ◦ e, contains all and only derivations that yield
the pair he, fi. Here, the inside algorithm computes
Z(e, f; Λ) and the inside-outside algorithm can be
used to compute E° p(e,d,a|f)[hi].
Once we have an objective and gradient, we can
apply any first-order numerical optimization tech-
nique.8 Although the conditional likelihood surface
of this model is non-convex (on account of the la-
tent variables), we did not find a significant initial-
ization effect. For the experiments below, we ini-
tialized Λ = 0 and set σ2 = 1. Training generally
converged in fewer than 1500 function evaluations.
</bodyText>
<footnote confidence="0.8998415">
8For our experiments we used L-BFGS (Liu and Nocedal,
1989).
</footnote>
<figure confidence="0.807708">
HL = − log p(e|f; Λ) − ||Λ||2
(e,f� 2σ2
�= − [log Z(e, f; Λ) − log Z(f; Λ)] − ||Λ||2
(e,f� 2σ2
�= λi
(e,f� E° p(d,a|e,f;Λ)[hi] − E° p(e,d,a|f;Λ)[hi] − σ2
</figure>
<page confidence="0.992016">
862
</page>
<sectionHeader confidence="0.998176" genericHeader="method">
4 Experimental setup
</sectionHeader>
<bodyText confidence="0.999427622222222">
We now turn to an experimental validation of the
models we have introduced. We define three con-
ditions: a small data scenario consisting of a trans-
lation task based on the BTEC Chinese-English cor-
pus (Takezawa et al., 2002), a large data Chinese-
English condition designed to be more comparable
to conditions in a NIST MT evaluation, and a large
data Arabic-English task.
For each condition, phrase tables were extracted
as described in Koehn et al. (2003) with a maxi-
mum phrase size of 5. The parallel training data
was aligned using the Giza++ implementation of
IBM Model 4 (Och and Ney, 2003). The Chinese
text was segmented using a CRF-based word seg-
menter (Tseng et al., 2005). The Arabic text was
segmented using the technique described in Lee et
al. (2003). The Stanford parser was used to generate
source parses for all conditions, and these were then
used to generate the reordering forests as described
in §2.1.
Table 1 summarizes statistics about the cor-
pora used. The reachability statistic indicates
what percentage of sentence pairs in the train-
ing data could be regenerated using our reorder-
ing/translation model.9 To train the reordering
model, we used all of the reachable sentence pairs
from BTEC, 20% of the reachable set in the
Chinese-English condition, and all reachable sen-
tence pairs under 40 words (source) in length in the
Arabic-English condition.
Error analysis indicates that a substantial portion
of unreachable sentence pairs are due to alignment
(word or sentence) or parse errors; however, in some
cases the reordering forests did not contain an ad-
equate source reordering to produce the necessary
target. For example, in Arabic, which is a VSO lan-
guage, the treebank annotation is to place the sub-
ject NP as the ‘middle child’ between the V and the
object constituent. This can be reordered into an En-
glish SVO order using our child-permutation rules;
however, if the source VP is modified by a modal
particle, the parser makes the particle the parent of
the VP, and it is no longer possible to move the sub-
ject to the first position in the sentence. Richer re-
ordering rules are needed to address this problem.
</bodyText>
<footnote confidence="0.9495055">
9Only sentences that can be generated by the model can be
used in training.
</footnote>
<bodyText confidence="0.9995904">
Other solutions to the reachability problem include
targeting reachable oracles instead of the reference
translation (Li and Khudanpur, 2009) or making use
of alternative training criteria, such as minimum risk
training (Li and Eisner, 2009).
</bodyText>
<subsectionHeader confidence="0.856516">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.99999505">
We briefly describe the feature functions we used
in our model. These include the typical dense fea-
tures used in translation: relative phrase translation
frequencies p(e|f) and p(f|e), ‘lexically smoothed’
translation probabilities plex(e|f) and plex(f|e), and
a phrase count feature. For the reordering model, we
used a binary feature for each kind of rule used, for
example oVP→V NP(a) would fire once for each time
the rule VP —* V NP was used in a derivation, a.
For the Arabic-English condition, we observed that
the parse trees tended to be quite flat, with many re-
peated non-terminal types in one rule, so we aug-
mented the non-terminal types with an index indi-
cating where they were located in the original parse
tree. This resulted in a total of 6.7k features for
IWSLT, 18k features for the large Chinese-English
condition, and 516k features for Arabic-English.10
A target language model was not used during the
training of the source reordering model, but it was
used during the translation experiments (see below).
</bodyText>
<subsectionHeader confidence="0.995583">
4.2 Qualitative assessment of reordering model
</subsectionHeader>
<bodyText confidence="0.91242935">
Before looking at the translation results, we exam-
ine what the model learns during training. Figure 6
lists the 10 most highly weighted reordering features
learned by the BTEC model (above) and shows an
example reordering using this model (below), with
the most English-like reordering indicated with a
star.11 Keep in mind, we expect these features to
reflect what the best English-like order of the input
should be. All are almost surprisingly intuitive, but
this is not terribly surprising since Chinese and En-
glish have very similar large-scale structures (both
are head initial, both have adjectives and quanti-
fiers that precede nouns). However, we see two en-
tries in the list (starred) that correspond to an En-
10The large number of features in the Arabic system was due
to the relative flatness of the Arabic parse trees.
11The italicized symbols in the English gloss are functional
elements with no precise translation. Q is an interrogative parti-
cle, and DE marks a variety of attributive roles and is used here
as the head of a relative clause.
</bodyText>
<page confidence="0.999312">
863
</page>
<tableCaption confidence="0.999569">
Table 1: Corpus statistics
</tableCaption>
<table confidence="0.9995725">
Condition Sentences Source words Target words Reachability
BTEC 44k 0.33M 0.36M 81%
Chinese-English 400k 9.4M 10.9M 25%
Arabic-English 120k 3.3M 3.6M 66%
</table>
<bodyText confidence="0.996586454545454">
glish word order that is ungrammatical in Chinese:
PP modifiers in Chinese typically precede the VPs
they modify, and CPs (relative clauses) also typi-
cally precede the nouns they modify. In English, the
reverse is true, and we see that the model has indeed
learned to prefer this ordering. It was not necessary
that this be the case: since our model makes use
of phrases memorized from a non-reordered training
set, it could hav relied on those for all its reordering.
Yet these results provide evidence that it is learning
large-scale reordering successfully.
</bodyText>
<table confidence="0.993693545454545">
Feature A
VP → VE NP 0.995
VP → VV VP 0.939
VP → VV NP 0.895
VP → VP PP* 0.803
VP → VV NP IP 0.763
PP → P NP 0.753
IP → NP VP PU 0.728
VP → VC NP 0.598
NP → DP NP 0.538
NP → NP CP* 0.537
</table>
<figure confidence="0.9622139">
Input:
a MG kf±_ A, 9qrM顿 饭店 &amp;quot; L:L 吗 ?
I CAN CATCH [NP[CP GO HILTON HOTEL DE] BUS] Q ?
(Can I catch a bus that goes to the Hilton Hotel ?)
5-best reordering:
I CAN CATCH [NP BUS [CP GO HILTON HOTEL DE]] Q ?
I CAN CATCH [NP BUS [CP DE GO HILTON HOTEL]] Q ?
I CAN CATCH [NP BUS [CP GO HOTEL HILTON DE]] Q ?
I CATCH [NP BUS [CP GO HILTON HOTEL DE]] CAN Q ?
I CAN CATCH [NP BUS [CP DE GO HOTEL HILTON]] Q ?
</figure>
<figureCaption confidence="0.6652205">
Figure 6: (Above) The 10 most highly-weighted features
in a Chinese-English reordering model. (Below) Exam-
ple reordering of a Chinese sentence (with English gloss,
translation, and partial syntactic information).
</figureCaption>
<sectionHeader confidence="0.958568" genericHeader="method">
5 Translation experiments
</sectionHeader>
<bodyText confidence="0.9999715">
We now consider how to apply this model to a trans-
lation task. The training we described in §3.2 is
suboptimal for state-of-the-art translation systems,
since (1) it optimizes likelihood rather than an MT
metric and (2) it does not include a language model.
We describe how we addressed these problems here,
and then present our results in the three conditions
defined above.
</bodyText>
<subsectionHeader confidence="0.940501">
5.1 Training for Viterbi decoding
</subsectionHeader>
<bodyText confidence="0.999977083333333">
A language model was incorporated using cube
pruning (Huang and Chiang, 2007), using a 200-
best limit at each node during LM integration. To
improve the ability of the phrase model to match
reordered phrases, we extracted the 1-best reorder-
ing of the training data under the learned reordering
model and generated the phrase translation model so
that it contained phrases from both the original order
and the 1-best reordering.
To be competitive with other state-of-the-art sys-
tems, we would like to use Och’s minimum error
training algorithm for training; however, we can-
not tune the model as described with it, since it has
far too many features. To address this, we con-
verted the coefficients on the reordering features into
a single reordering feature which then had a coef-
ficient assigned to it. This technique is similar to
what is done with logarithmic opinion pools, only
the learned model is not a probability distribution
(Smith et al., 2005). Once we collapsed the reorder-
ing weights into a single feature, we used the tech-
niques described by Kumar et al. (2009) to optimize
the feature weights to maximize corpus BLEU on a
held-out development set.
</bodyText>
<subsectionHeader confidence="0.9995">
5.2 Translation results
</subsectionHeader>
<bodyText confidence="0.99903775">
Scores on a held-out test set are reported in Table 2
using case-insensitive BLEU with 4 reference trans-
lations (16 for BTEC) using the original definition
of the brevity penalty. We report the results of our
</bodyText>
<figure confidence="0.9633056">
note
modal + VP
PP modifier of VP
PU = punctuation
rel. clauses follow
</figure>
<page confidence="0.99563">
864
</page>
<bodyText confidence="0.999278">
model along with three baseline conditions, one with
no-reordering at all (mono), the performance of a
phrase-based translation model with distance-based
distortion, the performance of our implementation of
a hierarchical phrase-based translation model (Chi-
ang, 2007), and then our model.
</bodyText>
<tableCaption confidence="0.98825">
Table 2: Translation results (BLEU)
</tableCaption>
<table confidence="0.99926725">
Condition Mono PB Hiero Forest
BTEC 47.4 51.8 52.4 54.1
Chinese-Eng. 29.0 30.9 32.1 32.4
Arabic-Eng. 41.2 45.8 46.6 44.9
</table>
<sectionHeader confidence="0.999949" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.994316977777778">
A variety of translation processes can be formalized
as the composition of a finite-state representation of
input (typically just a sentence, but often a more
complex structure, like a word lattice) with an SCFG
(Wu, 1997; Chiang, 2007; Zollmann and Venugopal,
2006). Like these, our work uses parsing algorithms
to perform the composition operation. But this is the
first time that the input to a finite-state transducer has
a context-free structure.12 Although not described
in terms of operations over formal languages, the
model of Yamada and Knight (2001) can be under-
stood as an instance of our class of models with a
specific input forest and phrases restricted to match
syntactic constituents.
In terms of formal similarity, Mi et al. (2008) use
forests as input to a tree-to-string transducer pro-
cess, but the forests are used to recover from 1-
best parsing errors (as such, all derivations yield
the same source string). Iglesias et al. (2009) use
a SCFG-based translation model, but implement it
using FSTs, although they use non-regular exten-
sions that make FSTs equivalent to recursive tran-
sition networks. Galley and Manning (2008) use
a context-free reordering model to score a phrase-
based (exponential) search space.
Syntax-based preprocessing approaches that have
relied on hand-written rules to restructure source
trees for particular translation tasks have been quite
widely used (Collins et al., 2005; Wang et al., 2007;
Xu et al., 2009; Chang et al., 2009). Discrimina-
tively trained reordering models have been exten-
sively explored. A widely used approach has been to
12Satta (submitted) discusses the theoretical possibility of
this sort of model but provides no experimental results.
use a classifier to predict the orientation of phrases
during decoding (Zens and Ney, 2006; Chang et al.,
2009). These classifiers must be trained indepen-
dently from the translation model using training ex-
amples extracted from the training data. A more am-
bitious approach is described by Tromble and Eisner
(2009), who build a global reordering model that is
learned automatically from reordered training data.
The latent variable discriminative training ap-
proach we describe is similar to the one originally
proposed by Blunsom et al. (2008).
</bodyText>
<sectionHeader confidence="0.993332" genericHeader="discussions">
7 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.9999828">
We have described a new model of translation that
takes advantage of the strengths of context-free
modeling, but splits reordering and phrase transduc-
tion into two separate models. This lets the context-
free part handle what it does well, mid-to-long range
reordering, and lets the finite-state part handle lo-
cal phrasal correspondences. We have further shown
that the reordering component can be trained effec-
tively as a latent variable in a discriminative transla-
tion model using only conventional parallel training
data.
This model holds considerable promise for fu-
ture improvement. Not only does it already achieve
quite reasonable performance (performing particu-
larly well in Chinese-English, where mid-range re-
ordering is often required), but we have only begun
to scratch the surface in terms of the kinds of fea-
tures that can be included to predict reordering, as
well as the kinds of reordering forests used. Fur-
thermore, by reintroducing the concept of a cascade
of transducers into the context-free model space, it
should be possible to develop new and more effec-
tive rescoring mechanisms. Finally, unlike SCFG
and phrase-based models, our model does not im-
pose any distortion limits.
</bodyText>
<sectionHeader confidence="0.99733" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.603018666666667">
The authors gratefully acknowledge partial support from
the GALE program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do not
necessarily reflect the views of the sponsors. Thanks
to Hendra Setiawan, Vlad Eidelman, Zhifei Li, Chris
Callison-Burch, Brian Dillon and the anonymous review-
ers for insightful comments.
</bodyText>
<page confidence="0.998466">
865
</page>
<sectionHeader confidence="0.996349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998532742857143">
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proceedings of ACL-HLT.
P.-C. Chang, D. Jurafsky, and C. D. Manning. 2009. Dis-
ambiguating “DE” for Chinese-English machine trans-
lation,. In Proc. WMT.
S. F. Chen and R. Rosenfeld. 1998. A Gaussian prior
for smoothing maximum entropy models. Technical
Report TR-10-98, Computer Science Group, Harvard
University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL 2005.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94–102.
M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc.
EMNLP.
D. Grune and C. J. H. Jacobs. 2008. Parsing as intersec-
tion. In D. Gries and F. B. Schneider, editors, Parsing
Techniques, pages 425–442. Springer, New York.
J. E. Hopcroft and J. D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Computa-
tion. Addison-Wesley.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL.
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In Proc. NAACL.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of NAACL, pages 48–54.
S. Kumar, Y. Deng, and W. Byrne. 2006. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 12(1):35–75.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. ACL.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In Proc. ACL.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. EMNLP.
Z. Li and S. Khudanpur. 2009. Efficient extraction of
oracle-best translations from hypergraphs. In Proc.
NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503–528.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based transla-
tion. In Proceedings ofACL-08: HLT, pages 192–199,
Columbus, Ohio, June. Association for Computational
Linguistics.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In Advances in Neu-
ral Information Processing Systems 20 (NIPS), pages
1153–1160.
G. Satta. submitted. Translation algorithms by means of
language intersection.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213–220.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In Proc.
ACL.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings of LREC 2002,
pages 147–152, Las Palmas, Spain.
R. Tromble and J. Eisner. 2009. Learning linear or-
der problems for better translation. In Proceedings of
EMNLP 2009.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chinese Lan-
guage Processing.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proc. EMNLP.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–404.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In Proc. NAACL, pages 245–253.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In Proc. of
the Workshop on SMT.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc.
of the Workshop on SMT.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte. 2008.
A systematic comparison of phrase-based, hierarchical
and syntax-augmented statistical MT. In Proc. Coling.
</reference>
<page confidence="0.99878">
866
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829923">
<title confidence="0.998185">Context-free reordering, finite-state translation</title>
<author confidence="0.99516">Chris Dyer</author>
<author confidence="0.99516">Philip</author>
<affiliation confidence="0.956259">UMIACS Laboratory for Computational Linguistics and Information Department of University of Maryland, College Park, MD 20742,</affiliation>
<email confidence="0.979416">redpony,resnikATumd.edu</email>
<abstract confidence="0.99891505">We describe a class of translation model in which a set of input variants encoded as a context-free forest is translated using a finitestate translation model. The forest structure of the input is well-suited to representing word order alternatives, making it straightforward to model translation as a two step process: (1) tree-based source reordering and (2) phrase transduction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="15136" citStr="Blunsom et al., 2008" startWordPosition="2576" endWordPosition="2579"> ⊗ w(S(r, x)) [X → α • Y0, q, r] Y U→ y ∈ G [Y → •y, r, r] : u [X → α • Y 0, q, s] : u [Y → y•, s, r] : v [X → αY • 0, q, r] : u ⊗ v Goal state: [S&apos; → S•, qo, qfinal] 861 FST, the quantity (1), which sums over all reorderings (and derivations), can be computed in polynomial time with dynamic programming composition, as described in §2.2. 3.2 Conditional training While it is straightforward to use expectation maximization to optimize the joint likelihood of the parallel training data with a latent variable model, instead we use a log-linear parameterization and maximize conditional likelihood (Blunsom et al., 2008; Petrov and Klein, 2008). This enables us to employ a rich set of (possibly overlapping, non-independent) features to discriminate among translations. The probability of a derivation from source to reordered source to target is thus written in terms of model parameters Λ = {λi} as: p(e, d, f; a|f; Λ) = exp Ei λi · Hi(e, d, f�,a,f) Z(f; Λ) �where Hi(e, d, f, a, f) = hi(f�, r) + � hi(f, s) rEd sEa The derivation probability is globally normalized by the partition Z(f; Λ), which is just the sum of the numerator for all derivations of f (corresponding to any e). The Hi (written below without thei</context>
<context position="29629" citStr="Blunsom et al. (2008)" startWordPosition="5041" endWordPosition="5044">etical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the finite-state part handle local phrasal correspondences. We have further shown that the reordering component can be trained effectively as a latent variable in a discriminative translation model using only conventional parallel training data. This model holds considerable promise for futur</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-C Chang</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Disambiguating “DE” for Chinese-English machine translation,.</title>
<date>2009</date>
<booktitle>In Proc. WMT.</booktitle>
<contexts>
<context position="28857" citStr="Chang et al., 2009" startWordPosition="4922" endWordPosition="4925"> parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned aut</context>
</contexts>
<marker>Chang, Jurafsky, Manning, 2009</marker>
<rawString>P.-C. Chang, D. Jurafsky, and C. D. Manning. 2009. Disambiguating “DE” for Chinese-English machine translation,. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="16326" citStr="Chen and Rosenfeld, 1998" startWordPosition="2789" endWordPosition="2792"> The Hi (written below without their arguments) are real-valued feature functions that may be overlapping and non-independent. For computational tractability, we assume that the feature functions Hi decompose with the derivations of fannd e in terms of local feature functions hi. We also define Z(e, f; λ) to be the sum of the numerator over all derivations that yield the sentence pair he, fi. Rather than training purely to optimize conditional likelihood, we also make use of a spherical Gaussian prior on the value of Λ with mean 0 and variance σ2, which helps prevent overfitting of the model (Chen and Rosenfeld, 1998). Our objective is thus to select Λ minimizing: The gradient of L with respect to the feature weights has a parallel form; it is the difference in feature expectations under the reference distribution and the translation distribution with a penalty term due to the prior: ∂L ∂λi The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). However, rather than matching the feature expectations in the model to an observable feature value, we have to sum over the latent structure that remains after observing our targ</context>
</contexts>
<marker>Chen, Rosenfeld, 1998</marker>
<rawString>S. F. Chen and R. Rosenfeld. 1998. A Gaussian prior for smoothing maximum entropy models. Technical Report TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1255" citStr="Chiang, 2007" startWordPosition="175" endWordPosition="176">uction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs. p</context>
<context position="25525" citStr="Chiang, 2007" startWordPosition="4383" endWordPosition="4384">rdering of a Chinese sentence (with English gloss, translation, and partial syntactic information). 5 Translation experiments We now consider how to apply this model to a translation task. The training we described in §3.2 is suboptimal for state-of-the-art translation systems, since (1) it optimizes likelihood rather than an MT metric and (2) it does not include a language model. We describe how we addressed these problems here, and then present our results in the three conditions defined above. 5.1 Training for Viterbi decoding A language model was incorporated using cube pruning (Huang and Chiang, 2007), using a 200- best limit at each node during LM integration. To improve the ability of the phrase model to match reordered phrases, we extracted the 1-best reordering of the training data under the learned reordering model and generated the phrase translation model so that it contained phrases from both the original order and the 1-best reordering. To be competitive with other state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we convert</context>
<context position="27185" citStr="Chiang, 2007" startWordPosition="4655" endWordPosition="4657">s BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our note modal + VP PP modifier of VP PU = punctuation rel. clauses follow 864 model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition Mono PB Hiero Forest BTEC 47.4 51.8 52.4 54.1 Chinese-Eng. 29.0 30.9 32.1 32.4 Arabic-Eng. 41.2 45.8 46.6 44.9 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-s</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="28800" citStr="Collins et al., 2005" startWordPosition="4910" endWordPosition="4913"> process, but the forests are used to recover from 1- best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009)</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="10427" citStr="Earley, 1970" startWordPosition="1711" endWordPosition="1712"> are dealing with FSTs that define binary relations over strings, not FSAs defining strings, this operation is more properly composition. However, since CFG/FSA intersection is less 5For computational tractability, we only consider all permutations only when the number of children is less than 5, otherwise we exclude permutations where a child moves more than 4 positions away from where it starts. cumbersome to describe, we present the algorithm in terms of intersection. To compute the composition of a reordering forest, !g, with an FSA, F, we will make use of a variant of Earley’s algorithm (Earley, 1970). Let weighted finite-state automaton F = (E, Q, q0, qfinal, S, w). E is a finite alphabet; Q is a set of states; q0 and qfinal E Q are start and accept states, respectively,6 S is the transition function Q x E 2Q, and w is the transition cost function Q x Q R. We use variables that refer to states in the FSA with the letters q, r, and s. We use x to represent a variable that is an element of E. Variables u and v represent costs. X and Y are non-terminals. Lowercase Greek letters are strings of terminals and non-terminals. The function S(q, x) returns the state(s) that are reachable from state</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="28525" citStr="Galley and Manning (2008)" startWordPosition="4871" endWordPosition="4874">, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1- best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orient</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Grune</author>
<author>C J H Jacobs</author>
</authors>
<title>Parsing as intersection.</title>
<date>2008</date>
<booktitle>Parsing Techniques,</booktitle>
<pages>425--442</pages>
<editor>In D. Gries and F. B. Schneider, editors,</editor>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="9804" citStr="Grune and Jacobs, 2008" startWordPosition="1605" endWordPosition="1608">ure 2. The process is quite similar to the one used when translating a source sentence with an SCFG, but with a twist: rather than representing the translation model as a grammar and parsing the source sentence, we represent the source sentence as a grammar (i.e. its reordering forest), and we use it to ‘parse’ the translation model (i.e. the FST representation of the phrase-based model). The end result (either way!) is a translation forest containing all possible targetlanguage translations of the source. Parsing can be understood as a means of computing the intersection of an FSA and a CFG (Grune and Jacobs, 2008). Since we are dealing with FSTs that define binary relations over strings, not FSAs defining strings, this operation is more properly composition. However, since CFG/FSA intersection is less 5For computational tractability, we only consider all permutations only when the number of children is less than 5, otherwise we exclude permutations where a child moves more than 4 positions away from where it starts. cumbersome to describe, we present the algorithm in terms of intersection. To compute the composition of a reordering forest, !g, with an FSA, F, we will make use of a variant of Earley’s a</context>
</contexts>
<marker>Grune, Jacobs, 2008</marker>
<rawString>D. Grune and C. J. H. Jacobs. 2008. Parsing as intersection. In D. Gries and F. B. Schneider, editors, Parsing Techniques, pages 425–442. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hopcroft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="2818" citStr="Hopcroft and Ullman, 1979" startWordPosition="418" endWordPosition="421">oth reorder and translate a source sentence into the target language, we break the translation process into a two step pipeline where (1) the source language is reordered into a target-like order, with alternatives encoded in a context-free forest, and (2) the reordered source is transduced into the target language using an FST that represents phrasal correspondences. While multi-step decompositions of the translation problem have been proposed before (Kumar et al., 2006), they are less practical with the rise of SCFG models, since the context-free languages are not closed under intersection (Hopcroft and Ullman, 1979). However, the CFLs are closed under intersection with regular languages. By restricting ourselves to a finite-state phrase transducer and representing reorderings of the source in a context-free forest, exact inference over the composition of the two models is possible. The paper proceeds as follows. We first explore reordering forests and describe how to translate them with an FST (§2). Since we would like our reordering model to discriminate between good reorderings of the source and bad ones, we show how to train our reordering component as a latent variable in an end-to-end translation mo</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>J. E. Hopcroft and J. D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25525" citStr="Huang and Chiang, 2007" startWordPosition="4381" endWordPosition="4384">xample reordering of a Chinese sentence (with English gloss, translation, and partial syntactic information). 5 Translation experiments We now consider how to apply this model to a translation task. The training we described in §3.2 is suboptimal for state-of-the-art translation systems, since (1) it optimizes likelihood rather than an MT metric and (2) it does not include a language model. We describe how we addressed these problems here, and then present our results in the three conditions defined above. 5.1 Training for Viterbi decoding A language model was incorporated using cube pruning (Huang and Chiang, 2007), using a 200- best limit at each node during LM integration. To improve the ability of the phrase model to match reordered phrases, we extracted the 1-best reordering of the training data under the learned reordering model and generated the phrase translation model so that it contained phrases from both the original order and the 1-best reordering. To be competitive with other state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we convert</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Iglesias</author>
<author>A de Gispert</author>
<author>E R Banga</author>
<author>W Byrne</author>
</authors>
<title>Hierarchical phrase-based translation with weighted finite state transducers. In</title>
<date>2009</date>
<booktitle>Proc. NAACL.</booktitle>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. 2009. Hierarchical phrase-based translation with weighted finite state transducers. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="5269" citStr="Koehn et al., 2003" startWordPosition="821" endWordPosition="824">last, and English is a head-initial language, where heads come first. As a result, the usual order for a declarative sentence in English is SVO (subject-verb-object), but in Japanese, it is SOV, and the desired translation is John-ga ringo-o [an apple] tabeta [ate]. In summary, when translating from English into Japanese, it is usually necessary to move verbs from their position between the subject and object to the end of the sentence. This reordering can happen in two ways, which we depict in Figure 1. In the derivation on the left, a memorized phrase pair captures the movement of the verb (Koehn et al., 2003). In the other derivation, the source is first reordered into target word order and then translated, using smaller translation units. In addition, we have assumed that the phrase translations were learned from a parallel corpus that is in the original ordering, so the reordering forest F should include derivations of phrase-size units in the source order as well as the target order. 2Note that forests are isomorphic to context-free grammars. For example, what is referred to as the ‘parse forest’, and understood to encode all derivations of a sentence s under some grammar, can also be understoo</context>
<context position="19315" citStr="Koehn et al. (2003)" startWordPosition="3310" endWordPosition="3313">f; Λ) − ||Λ||2 (e,f� 2σ2 �= − [log Z(e, f; Λ) − log Z(f; Λ)] − ||Λ||2 (e,f� 2σ2 �= λi (e,f� E° p(d,a|e,f;Λ)[hi] − E° p(e,d,a|f;Λ)[hi] − σ2 862 4 Experimental setup We now turn to an experimental validation of the models we have introduced. We define three conditions: a small data scenario consisting of a translation task based on the BTEC Chinese-English corpus (Takezawa et al., 2002), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task. For each condition, phrase tables were extracted as described in Koehn et al. (2003) with a maximum phrase size of 5. The parallel training data was aligned using the Giza++ implementation of IBM Model 4 (Och and Ney, 2003). The Chinese text was segmented using a CRF-based word segmenter (Tseng et al., 2005). The Arabic text was segmented using the technique described in Lee et al. (2003). The Stanford parser was used to generate source parses for all conditions, and these were then used to generate the reordering forests as described in §2.1. Table 1 summarizes statistics about the corpora used. The reachability statistic indicates what percentage of sentence pairs in the tr</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. of NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>Y Deng</author>
<author>W Byrne</author>
</authors>
<title>A weighted finite state transducer translation template model for statistical machine translation.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="1369" citStr="Kumar et al. (2006)" startWordPosition="189" endWordPosition="192"> learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs. pairs with substantial structural differences (and thus requiring long-range reordering during translation), SCFG m</context>
<context position="2668" citStr="Kumar et al., 2006" startWordPosition="395" endWordPosition="398">is paper, we explore a new way to take advantage of the computational benefits of CFGs during translation. Rather than using a single SCFG to both reorder and translate a source sentence into the target language, we break the translation process into a two step pipeline where (1) the source language is reordered into a target-like order, with alternatives encoded in a context-free forest, and (2) the reordered source is transduced into the target language using an FST that represents phrasal correspondences. While multi-step decompositions of the translation problem have been proposed before (Kumar et al., 2006), they are less practical with the rise of SCFG models, since the context-free languages are not closed under intersection (Hopcroft and Ullman, 1979). However, the CFLs are closed under intersection with regular languages. By restricting ourselves to a finite-state phrase transducer and representing reorderings of the source in a context-free forest, exact inference over the composition of the two models is possible. The paper proceeds as follows. We first explore reordering forests and describe how to translate them with an FST (§2). Since we would like our reordering model to discriminate b</context>
</contexts>
<marker>Kumar, Deng, Byrne, 2006</marker>
<rawString>S. Kumar, Y. Deng, and W. Byrne. 2006. A weighted finite state transducer translation template model for statistical machine translation. Journal of Natural Language Engineering, 12(1):35–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Macherey</author>
<author>C Dyer</author>
<author>F Och</author>
</authors>
<title>Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="26522" citStr="Kumar et al. (2009)" startWordPosition="4550" endWordPosition="4553">er state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we converted the coefficients on the reordering features into a single reordering feature which then had a coefficient assigned to it. This technique is similar to what is done with logarithmic opinion pools, only the learned model is not a probability distribution (Smith et al., 2005). Once we collapsed the reordering weights into a single feature, we used the techniques described by Kumar et al. (2009) to optimize the feature weights to maximize corpus BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our note modal + VP PP modifier of VP PU = punctuation rel. clauses follow 864 model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation </context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-S Lee</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Language model based Arabic word segmentation.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="19622" citStr="Lee et al. (2003)" startWordPosition="3365" endWordPosition="3368">nslation task based on the BTEC Chinese-English corpus (Takezawa et al., 2002), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task. For each condition, phrase tables were extracted as described in Koehn et al. (2003) with a maximum phrase size of 5. The parallel training data was aligned using the Giza++ implementation of IBM Model 4 (Och and Ney, 2003). The Chinese text was segmented using a CRF-based word segmenter (Tseng et al., 2005). The Arabic text was segmented using the technique described in Lee et al. (2003). The Stanford parser was used to generate source parses for all conditions, and these were then used to generate the reordering forests as described in §2.1. Table 1 summarizes statistics about the corpora used. The reachability statistic indicates what percentage of sentence pairs in the training data could be regenerated using our reordering/translation model.9 To train the reordering model, we used all of the reachable sentence pairs from BTEC, 20% of the reachable set in the Chinese-English condition, and all reachable sentence pairs under 40 words (source) in length in the Arabic-English</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan. 2003. Language model based Arabic word segmentation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="21321" citStr="Li and Eisner, 2009" startWordPosition="3646" endWordPosition="3649">VO order using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence. Richer reordering rules are needed to address this problem. 9Only sentences that can be generated by the model can be used in training. Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation (Li and Khudanpur, 2009) or making use of alternative training criteria, such as minimum risk training (Li and Eisner, 2009). 4.1 Features We briefly describe the feature functions we used in our model. These include the typical dense features used in translation: relative phrase translation frequencies p(e|f) and p(f|e), ‘lexically smoothed’ translation probabilities plex(e|f) and plex(f|e), and a phrase count feature. For the reordering model, we used a binary feature for each kind of rule used, for example oVP→V NP(a) would fire once for each time the rule VP —* V NP was used in a derivation, a. For the Arabic-English condition, we observed that the parse trees tended to be quite flat, with many repeated non-ter</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Z. Li and J. Eisner. 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>S Khudanpur</author>
</authors>
<title>Efficient extraction of oracle-best translations from hypergraphs.</title>
<date>2009</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="21221" citStr="Li and Khudanpur, 2009" startWordPosition="3630" endWordPosition="3633">as the ‘middle child’ between the V and the object constituent. This can be reordered into an English SVO order using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence. Richer reordering rules are needed to address this problem. 9Only sentences that can be generated by the model can be used in training. Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation (Li and Khudanpur, 2009) or making use of alternative training criteria, such as minimum risk training (Li and Eisner, 2009). 4.1 Features We briefly describe the feature functions we used in our model. These include the typical dense features used in translation: relative phrase translation frequencies p(e|f) and p(f|e), ‘lexically smoothed’ translation probabilities plex(e|f) and plex(f|e), and a phrase count feature. For the reordering model, we used a binary feature for each kind of rule used, for example oVP→V NP(a) would fire once for each time the rule VP —* V NP was used in a derivation, a. For the Arabic-Eng</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Z. Li and S. Khudanpur. 2009. Efficient extraction of oracle-best translations from hypergraphs. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="18679" citStr="Liu and Nocedal, 1989" startWordPosition="3195" endWordPosition="3198">derivations that yield the pair he, fi. Here, the inside algorithm computes Z(e, f; Λ) and the inside-outside algorithm can be used to compute E° p(e,d,a|f)[hi]. Once we have an objective and gradient, we can apply any first-order numerical optimization technique.8 Although the conditional likelihood surface of this model is non-convex (on account of the latent variables), we did not find a significant initialization effect. For the experiments below, we initialized Λ = 0 and set σ2 = 1. Training generally converged in fewer than 1500 function evaluations. 8For our experiments we used L-BFGS (Liu and Nocedal, 1989). HL = − log p(e|f; Λ) − ||Λ||2 (e,f� 2σ2 �= − [log Z(e, f; Λ) − log Z(f; Λ)] − ||Λ||2 (e,f� 2σ2 �= λi (e,f� E° p(d,a|e,f;Λ)[hi] − E° p(e,d,a|f;Λ)[hi] − σ2 862 4 Experimental setup We now turn to an experimental validation of the models we have introduced. We define three conditions: a small data scenario consisting of a translation task based on the BTEC Chinese-English corpus (Takezawa et al., 2002), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task. For each condition, phrase tables were extracted</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Forest-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>192--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="28128" citStr="Mi et al. (2008)" startWordPosition="4807" endWordPosition="4810">entence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1- best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for partic</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>H. Mi, L. Huang, and Q. Liu. 2008. Forest-based translation. In Proceedings ofACL-08: HLT, pages 192–199, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19454" citStr="Och and Ney, 2003" startWordPosition="3336" endWordPosition="3339">62 4 Experimental setup We now turn to an experimental validation of the models we have introduced. We define three conditions: a small data scenario consisting of a translation task based on the BTEC Chinese-English corpus (Takezawa et al., 2002), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task. For each condition, phrase tables were extracted as described in Koehn et al. (2003) with a maximum phrase size of 5. The parallel training data was aligned using the Giza++ implementation of IBM Model 4 (Och and Ney, 2003). The Chinese text was segmented using a CRF-based word segmenter (Tseng et al., 2005). The Arabic text was segmented using the technique described in Lee et al. (2003). The Stanford parser was used to generate source parses for all conditions, and these were then used to generate the reordering forests as described in §2.1. Table 1 summarizes statistics about the corpora used. The reachability statistic indicates what percentage of sentence pairs in the training data could be regenerated using our reordering/translation model.9 To train the reordering model, we used all of the reachable sente</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20 (NIPS),</booktitle>
<pages>1153--1160</pages>
<contexts>
<context position="15161" citStr="Petrov and Klein, 2008" startWordPosition="2580" endWordPosition="2583"> Y0, q, r] Y U→ y ∈ G [Y → •y, r, r] : u [X → α • Y 0, q, s] : u [Y → y•, s, r] : v [X → αY • 0, q, r] : u ⊗ v Goal state: [S&apos; → S•, qo, qfinal] 861 FST, the quantity (1), which sums over all reorderings (and derivations), can be computed in polynomial time with dynamic programming composition, as described in §2.2. 3.2 Conditional training While it is straightforward to use expectation maximization to optimize the joint likelihood of the parallel training data with a latent variable model, instead we use a log-linear parameterization and maximize conditional likelihood (Blunsom et al., 2008; Petrov and Klein, 2008). This enables us to employ a rich set of (possibly overlapping, non-independent) features to discriminate among translations. The probability of a derivation from source to reordered source to target is thus written in terms of model parameters Λ = {λi} as: p(e, d, f; a|f; Λ) = exp Ei λi · Hi(e, d, f�,a,f) Z(f; Λ) �where Hi(e, d, f, a, f) = hi(f�, r) + � hi(f, s) rEd sEa The derivation probability is globally normalized by the partition Z(f; Λ), which is just the sum of the numerator for all derivations of f (corresponding to any e). The Hi (written below without their arguments) are real-val</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>S. Petrov and D. Klein. 2008. Discriminative log-linear grammars with latent variables. In Advances in Neural Information Processing Systems 20 (NIPS), pages 1153–1160.</rawString>
</citation>
<citation valid="false">
<authors>
<author>submitted</author>
</authors>
<title>Translation algorithms by means of language intersection.</title>
<marker>submitted, </marker>
<rawString>G. Satta. submitted. Translation algorithms by means of language intersection.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="16746" citStr="Sha and Pereira, 2003" startWordPosition="2861" endWordPosition="2864"> to optimize conditional likelihood, we also make use of a spherical Gaussian prior on the value of Λ with mean 0 and variance σ2, which helps prevent overfitting of the model (Chen and Rosenfeld, 1998). Our objective is thus to select Λ minimizing: The gradient of L with respect to the feature weights has a parallel form; it is the difference in feature expectations under the reference distribution and the translation distribution with a penalty term due to the prior: ∂L ∂λi The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). However, rather than matching the feature expectations in the model to an observable feature value, we have to sum over the latent structure that remains after observing our target e, which makes the form of the first summand an expectation rather than just a feature function value. 3.2.1 Computing the objective and gradient The objective and gradient that were just introduced can be computed in two steps. Given a training pair he, fi, we generate the forest of reorderings F from f as described in §2.1. We then compose this grammar with T, the FST representing the translation model, which yi</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smith</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Logarithmic opinion pools for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="26401" citStr="Smith et al., 2005" startWordPosition="4528" endWordPosition="4531">ion model so that it contained phrases from both the original order and the 1-best reordering. To be competitive with other state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we converted the coefficients on the reordering features into a single reordering feature which then had a coefficient assigned to it. This technique is similar to what is done with logarithmic opinion pools, only the learned model is not a probability distribution (Smith et al., 2005). Once we collapsed the reordering weights into a single feature, we used the techniques described by Kumar et al. (2009) to optimize the feature weights to maximize corpus BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our note modal + VP PP modifier of VP PU = punctuation rel. clauses follow 864 model along with three baseline conditions, one with no-reordering at all (mono), t</context>
</contexts>
<marker>Smith, Cohn, Osborne, 2005</marker>
<rawString>A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opinion pools for conditional random fields. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>H Yamamoto</author>
<author>S Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC 2002,</booktitle>
<pages>147--152</pages>
<location>Las Palmas,</location>
<contexts>
<context position="19083" citStr="Takezawa et al., 2002" startWordPosition="3272" endWordPosition="3275">t initialization effect. For the experiments below, we initialized Λ = 0 and set σ2 = 1. Training generally converged in fewer than 1500 function evaluations. 8For our experiments we used L-BFGS (Liu and Nocedal, 1989). HL = − log p(e|f; Λ) − ||Λ||2 (e,f� 2σ2 �= − [log Z(e, f; Λ) − log Z(f; Λ)] − ||Λ||2 (e,f� 2σ2 �= λi (e,f� E° p(d,a|e,f;Λ)[hi] − E° p(e,d,a|f;Λ)[hi] − σ2 862 4 Experimental setup We now turn to an experimental validation of the models we have introduced. We define three conditions: a small data scenario consisting of a translation task based on the BTEC Chinese-English corpus (Takezawa et al., 2002), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task. For each condition, phrase tables were extracted as described in Koehn et al. (2003) with a maximum phrase size of 5. The parallel training data was aligned using the Giza++ implementation of IBM Model 4 (Och and Ney, 2003). The Chinese text was segmented using a CRF-based word segmenter (Tseng et al., 2005). The Arabic text was segmented using the technique described in Lee et al. (2003). The Stanford parser was used to generate source parses for </context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and S. Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world. In Proceedings of LREC 2002, pages 147–152, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>J Eisner</author>
</authors>
<title>Learning linear order problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="13262" citStr="Tromble and Eisner (2009)" startWordPosition="2198" endWordPosition="2201">ut it is also intuitively satisfying because from a task perspective, we are not concerned with values of f0, but only with producing a good translation e. 3.1 A probabilistic translation model with a latent reordering variable The translation model we use is a two phase process. First, source sentence f is reordered into a targetlike word order f0 according to a reordering model r(f0|f). The reordered source is then transduced into the target language according to a translation model t(e|f0). We require that r(f0|f) can be represented by orderings from word aligned parallel corpora, refer to Tromble and Eisner (2009). Original parse grammar: S → NPsubj VP VP → V NPobj NPobj → DT NN NPsubj → John V → ate DT → an NN → apple Additional reordering grammar rules: S → VP NPsubj VP → NPobj V NPobj → NN DT Figure 4: Context-free grammar representation of the forest in Figure 3. The reordering grammar contains the parse grammar, plus the reordering-specific rules. Figure 5: Weighted logic program for computing the intersection of a weighted FSA and a weighted CFG. a recursion-free probabilistic context-free grammar, i.e. a forest as in §2.1, and that t(e|f0) is represented by a (cyclic) finite-state transducer, as</context>
<context position="29400" citStr="Tromble and Eisner (2009)" startWordPosition="5006" endWordPosition="5009">sed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the finite-state part handle local phrasal correspondences. We h</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>R. Tromble and J. Eisner. 2009. Learning linear order problems for better translation. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>P Chang</author>
<author>G Andrew</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="19540" citStr="Tseng et al., 2005" startWordPosition="3351" endWordPosition="3354">ve introduced. We define three conditions: a small data scenario consisting of a translation task based on the BTEC Chinese-English corpus (Takezawa et al., 2002), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task. For each condition, phrase tables were extracted as described in Koehn et al. (2003) with a maximum phrase size of 5. The parallel training data was aligned using the Giza++ implementation of IBM Model 4 (Och and Ney, 2003). The Chinese text was segmented using a CRF-based word segmenter (Tseng et al., 2005). The Arabic text was segmented using the technique described in Lee et al. (2003). The Stanford parser was used to generate source parses for all conditions, and these were then used to generate the reordering forests as described in §2.1. Table 1 summarizes statistics about the corpora used. The reachability statistic indicates what percentage of sentence pairs in the training data could be regenerated using our reordering/translation model.9 To train the reordering model, we used all of the reachable sentence pairs from BTEC, 20% of the reachable set in the Chinese-English condition, and al</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="28819" citStr="Wang et al., 2007" startWordPosition="4914" endWordPosition="4917">sts are used to recover from 1- best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a globa</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and P. Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1240" citStr="Wu, 1997" startWordPosition="173" endWordPosition="174">ase transduction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized </context>
<context position="27600" citStr="Wu, 1997" startWordPosition="4724" endWordPosition="4725">(mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition Mono PB Hiero Forest BTEC 47.4 51.8 52.4 54.1 Chinese-Eng. 29.0 30.9 32.1 32.4 Arabic-Eng. 41.2 45.8 46.6 44.9 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the fo</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Xu</author>
<author>J Kang</author>
<author>M Ringgaard</author>
<author>F Och</author>
</authors>
<title>Using a dependency parser to improve SMT for subject-objectverb languages. In</title>
<date>2009</date>
<booktitle>Proc. NAACL,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="28836" citStr="Xu et al., 2009" startWordPosition="4918" endWordPosition="4921">over from 1- best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering mode</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a dependency parser to improve SMT for subject-objectverb languages. In Proc. NAACL, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7374" citStr="Yamada and Knight (2001)" startWordPosition="1181" endWordPosition="1184"> ate NP → an apple In this grammar, the phrases John and an apple are fixed and only the VP contains ordering ambiguity. 2.1 Reordering forests based on source parses Many kinds of reordering forests are possible; in general, the best one for a particular language pair will be one that is easiest to create given the resources available in the source language. It will also be the one that most compactly expresses the source reorderings that are most likely to be useful for translation. In this paper, we consider a particular kind of reordering forest that is inspired by the reordering model of Yamada and Knight (2001).4 These are generated by taking a source language parse tree and ‘expanding’ each node so that it 4One important difference is that our translation model is not restricted by the structure of the source parse tree; i.e., phrases used in transduction need not correspond to constituents in the source reordering forest. However, if a phrase does cross a constituent boundary between constituents A and B, then translations that use that phrase will have A and B adjacent. an : ε ate : ε an : ε John : pieibt [John-ga] 3 2 0 1 apple : リンゴを :R^Zt [ringo-o tabeta] apple : リンゴを [ringo-o] [tabeta] ate : </context>
<context position="27939" citStr="Yamada and Knight (2001)" startWordPosition="4774" endWordPosition="4777">0 30.9 32.1 32.4 Arabic-Eng. 41.2 45.8 46.6 44.9 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1- best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on SMT.</booktitle>
<contexts>
<context position="29177" citStr="Zens and Ney, 2006" startWordPosition="4970" endWordPosition="4973">el to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but split</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. Discriminative reordering models for statistical machine translation. In Proc. of the Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on SMT.</booktitle>
<contexts>
<context position="27645" citStr="Zollmann and Venugopal, 2006" startWordPosition="4728" endWordPosition="4731">of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition Mono PB Hiero Forest BTEC 47.4 51.8 52.4 54.1 Chinese-Eng. 29.0 30.9 32.1 32.4 Arabic-Eng. 41.2 45.8 46.6 44.9 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1- best parsin</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of the Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
<author>F Och</author>
<author>J Ponte</author>
</authors>
<title>A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT. In</title>
<date>2008</date>
<booktitle>Proc. Coling.</booktitle>
<contexts>
<context position="2042" citStr="Zollmann et al., 2008" startWordPosition="293" endWordPosition="296">he ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs. pairs with substantial structural differences (and thus requiring long-range reordering during translation), SCFG models have come to outperform the best FST models (Zollmann et al., 2008). In this paper, we explore a new way to take advantage of the computational benefits of CFGs during translation. Rather than using a single SCFG to both reorder and translate a source sentence into the target language, we break the translation process into a two step pipeline where (1) the source language is reordered into a target-like order, with alternatives encoded in a context-free forest, and (2) the reordered source is transduced into the target language using an FST that represents phrasal correspondences. While multi-step decompositions of the translation problem have been proposed b</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>A. Zollmann, A. Venugopal, F. Och, and J. Ponte. 2008. A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT. In Proc. Coling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>