<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000594">
<title confidence="0.9887445">
Combining Lexical, Syntactic, and Semantic Features with
Maximum Entropy Models for Extracting Relations
</title>
<author confidence="0.78605">
Nanda Kambhatla
</author>
<note confidence="0.579634">
IBM T. J. Watson Research Center
</note>
<address confidence="0.9300395">
1101 Kitchawan Road Route 134
Yorktown Heights, NY 10598
</address>
<email confidence="0.996159">
nanda@us.ibm.com
</email>
<sectionHeader confidence="0.993828" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999813777777778">
Extracting semantic relationships between entities
is challenging because of a paucity of annotated
data and the errors induced by entity detection mod-
ules. We employ Maximum Entropy models to
combine diverse lexical, syntactic and semantic fea-
tures derived from the text. Our system obtained
competitive results in the Automatic Content Ex-
traction (ACE) evaluation. Here we present our gen-
eral approach and describe our ACE results.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999837029850746">
Extraction of semantic relationships between en-
tities can be very useful for applications such as
biography extraction and question answering, e.g.
to answer queries such as “Where is the Taj Ma-
hal?”. Several prior approaches to relation extrac-
tion have focused on using syntactic parse trees.
For the Template Relations task of MUC-7, BBN
researchers (Miller et al., 2000) augmented syn-
tactic parse trees with semantic information corre-
sponding to entities and relations and built genera-
tive models for the augmented trees. More recently,
(Zelenko et al., 2003) have proposed extracting rela-
tions by computing kernel functions between parse
trees and (Culotta and Sorensen, 2004) have ex-
tended this work to estimate kernel functions be-
tween augmented dependency trees.
We build Maximum Entropy models for extract-
ing relations that combine diverse lexical, syntactic
and semantic features. Our results indicate that us-
ing a variety of information sources can result in
improved recall and overall F measure. Our ap-
proach can easily scale to include more features
from a multitude of sources–e.g. WordNet, gazat-
teers, output of other semantic taggers etc.–that can
be brought to bear on this task. In this paper, we
present our general approach, describe the features
we currently use and show the results of our partic-
ipation in the ACE evaluation.
Automatic Content Extraction (ACE, 2004) is an
evaluation conducted by NIST to measure Entity
Detection and Tracking (EDT) and relation detec-
tion and characterization (RDC). The EDT task en-
tails the detection of mentions of entities and chain-
ing them together by identifying their coreference.
In ACE vocabulary, entities are objects, mentions
are references to them, and relations are explic-
itly or implicitly stated relationships among enti-
ties. Entities can be of five types: persons, organiza-
tions, locations, facilities, and geo-political entities
(geographically defined regions that define a politi-
cal boundary, e.g. countries, cities, etc.). Mentions
have levels: they can be names, nominal expressions
or pronouns.
The RDC task detects implicit and explicit rela-
tions&apos; between entities identified by the EDT task.
Here is an example:
The American Medical Association
voted yesterday to install the heir ap-
parent as its president-elect, rejecting a
strong, upstart challenge by a District
doctor who argued that the nation’s
largest physicians’ group needs stronger
ethics and new leadership.
In electing Thomas R. Reardon, an
Oregon general practitioner who had
been the chairman of its board, ...
In this fragment, all the underlined phrases are men-
tions referring to the American Medical Associa-
tion, or to Thomas R. Reardon or the board (an or-
ganization) of the American Medical Association.
Moreover, there is an explicit management rela-
tion between chairman and board, which are ref-
erences to Thomas R. Reardon and the board of the
American Medical Association respectively. Rela-
tion extraction is hard, since successful extraction
implies correctly detecting both the argument men-
tions, correctly chaining these mentions to their re-
</bodyText>
<footnote confidence="0.87633">
&apos;Explict relations occur in text with explicit evidence sug-
gesting the relationship. Implicit relations need not have ex-
plicit supporting evidence in text, though they should be evi-
dent from a reading of the document.
</footnote>
<table confidence="0.99959456">
Type Subtype Count
AT based-In 496
located 2879
residence 395
NEAR relative-location 288
PART other 6
part-Of 1178
subsidiary 366
ROLE affiliate-partner 219
citizen-Of 450
client 159
founder 37
general-staff 1507
management 1559
member 1404
other 174
owner 274
SOCIAL associate 119
grandparent 10
other-personal 108
other-professional 415
other-relative 86
parent 149
sibling 23
spouse 89
</table>
<tableCaption confidence="0.999141">
Table 1: The list of relation types and subtypes used
</tableCaption>
<bodyText confidence="0.959856363636364">
in the ACE 2003 evaluation.
spective entities, and correctly determining the type
of relation that holds between them.
This paper focuses on the relation extraction
component of our ACE system. The reader is re-
ferred to (Florian et al., 2004; Ittycheriah et al.,
2003; Luo et al., 2004) for more details of our men-
tion detection and mention chaining modules. In the
next section, we describe our extraction system. We
present results in section 3, and we conclude after
making some general observations in section 4.
</bodyText>
<sectionHeader confidence="0.8017445" genericHeader="method">
2 Maximum Entropy models for
extracting relations
</sectionHeader>
<bodyText confidence="0.998729438596491">
We built Maximum Entropy models for predicting
the type of relation (if any) between every pair of
mentions within each sentence. We only model
explicit relations, because of poor inter-annotator
agreement in the annotation of implicit relations.
Table 1 lists the types and subtypes of relations
for the ACE RDC task, along with their frequency
of occurence in the ACE training data2. Note that
only 6 of these 24 relation types are symmetric:
“relative-location”, “associate”, “other-relative”,
“other-professional”, “sibling”, and “spouse”. We
only model the relation subtypes, after making them
unique by concatenating the type where appropri-
ate (e.g. “OTHER” became “OTHER-PART” and
“OTHER-ROLE”). We explicitly model the argu-
ment order of mentions. Thus, when comparing
mentions and , we distinguish between the case
where -citizen-Of- and -citizen-Of- . We
thus model the extraction as a classification problem
with 49 classes, two for each relation subtype and a
“NONE” class for the case where the two mentions
are not related.
For each pair of mentions, we compute several
feature streams shown below. All the syntactic fea-
tures are derived from the syntactic parse tree and
the dependency tree that we compute using a statis-
tical parser trained on the PennTree Bank using the
Maximum Entropy framework (Ratnaparkhi, 1999).
The feature streams are:
Words The words of both the mentions and all the
words in between.
Entity Type The entity type (one of PERSON,
ORGANIZATION, LOCATION, FACILITY,
Geo-Political Entity or GPE) of both the men-
tions.
Mention Level The mention level (one of NAME,
NOMINAL, PRONOUN) of both the men-
tions.
Overlap The number of words (if any) separating
the two mentions, the number of other men-
tions in between, flags indicating whether the
two mentions are in the same noun phrase, verb
phrase or prepositional phrase.
Dependency The words and part-of-speech and
chunk labels of the words on which the men-
tions are dependent in the dependency tree de-
rived from the syntactic parse tree.
Parse Tree The path of non-terminals (removing
duplicates) connecting the two mentions in the
parse tree, and the path annotated with head
words.
Here is an example. For the sentence fragment,
been the chairman of its board ...
the corresponding syntactic parse tree is shown in
Figure 1 and the dependency tree is shown in Figure
2. For the pair of mentions chairman and board,
the feature streams are shown below.
</bodyText>
<footnote confidence="0.976093">
2The reader is referred to (Strassel et al., 2003) or LDC’s
web site for more details of the data.
</footnote>
<bodyText confidence="0.34776">
Words
, , , .
</bodyText>
<figureCaption confidence="0.99549275">
Figure 1: The syntactic parse tree for the fragment
“chairman of its board”.
Figure 2: The dependency tree for the fragment
“chairman of its board”.
</figureCaption>
<figure confidence="0.61109725">
Entity Type (for “chairman”),
(for “board”).
Mention Level
.
</figure>
<bodyText confidence="0.944330714285714">
Overlap one-mention-in-between (the word “its”),
two-words-apart, in-same-noun-phrase.
Dependency (word on which
is depedent), (POS of word
on which is dependent),
(chunk label of word on which is de-
pendent),
Parse Tree PERSON-NP-PP-ORGANIZATION,
PERSON-NP-PP:of-ORGANIZATION (both
derived from the path shown in bold in Figure
1).
We trained Maximum Entropy models using fea-
tures derived from the feature streams described
above.
</bodyText>
<sectionHeader confidence="0.998217" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.999644833333333">
We divided the ACE training data provided by LDC
into separate training and development sets. The
training set contained around 300K words, and 9752
instances of relations and the development set con-
tained around 46K words, and 1679 instances of re-
lations.
</bodyText>
<table confidence="0.999757714285714">
Features P R F Value
Words 81.9 17.4 28.6 8.0
+ Entity Type 71.1 27.5 39.6 19.3
+ Mention Level 71.6 28.6 40.9 20.2
+ Overlap 61.4 38.8 47.6 34.7
+ Dependency 63.4 44.3 52.1 40.2
+ Parse Tree 63.5 45.2 52.8 40.9
</table>
<tableCaption confidence="0.811980666666667">
Table 2: The Precision, Recall, F-measure and the
ACE Value on the development set with true men-
tions and entities.
</tableCaption>
<bodyText confidence="0.999779378378378">
We report results in two ways. To isolate the
perfomance of relation extraction, we measure the
performance of relation extraction models on “true”
mentions with “true” chaining (i.e. as annotated by
LDC annotators). We also measured performance
of models run on the deficient output of mention de-
tection and mention chaining modules.
We report both the F-measure&apos; and the ACE
value of relation extraction. The ACE value is a
NIST metric that assigns 0% value for a system
which produces no output and 100% value for a sys-
tem that extracts all the relations and produces no
false alarms. We count the misses; the true relations
not extracted by the system, and the false alarms;
the spurious relations extracted by the system, and
obtain the ACE value by subtracting from 1.0, the
normalized weighted cost of the misses and false
alarms. The ACE value counts each relation only
once, even if it was expressed many times in a doc-
ument in different ways. The reader is referred to
the ACE web site (ACE, 2004) for more details.
We built several models to compare the relative
utility of the feature streams described in the previ-
ous section. Table 2 shows the results we obtained
when running on “truth” for the development set
and Table 3 shows the results we obtained when run-
ning on the output of mention detection and mention
chaining modules. Note that a model trained with
only words as features obtains a very high precision
and a very low recall. For example, for the men-
tion pair his and wife with no words in between, the
lexical features together with the fact that there are
no words in between is sufficient (though not nec-
essary) to extract the relationship between the two
entities. The addition of entity types, mention levels
and especially, the word proximity features (“over-
lap”) boosts the recall at the expense of the very
</bodyText>
<footnote confidence="0.98910975">
3The F-measure is the harmonic mean of the precision, de-
fined as the percentage of extracted relations that are valid, and
the recall, defined as the percentage of valid relations that are
extracted.
</footnote>
<figure confidence="0.949381454545455">
NP
NN
DT NN IN PRP
PP
NP NP
... been the chairman of its board ...
VBN DT NN IN PRP NN
been the chairman of its board ...
...
,
, , ,
</figure>
<figureCaption confidence="0.360296666666667">
m1-m2-dependent-in-second-level(number of
links traversed in dependency tree to go from
one mention to another in Figure 2).
</figureCaption>
<table confidence="0.999542571428571">
Features P R F Value
Words 58.4 11.1 18.6 5.9
+ Entity Type 43.6 14.0 21.1 12.5
+ Mention Level 43.6 14.5 21.7 13.4
+ Overlap 35.6 17.6 23.5 21.0
+ Dependency 35.0 19.1 24.7 24.6
+ Parse Tree 35.5 19.8 25.4 25.2
</table>
<tableCaption confidence="0.978981">
Table 3: The Precision, Recall, F-measure, and
ACE Value on the development set with system out-
put mentions and entities.
</tableCaption>
<table confidence="0.99991525">
Eval Value F Value F
Set (T) (T) (S) (S)
Feb’02 31.3 52.4 17.3 24.9
Sept’03 39.4 55.2 18.3 23.6
</table>
<tableCaption confidence="0.966695">
Table 4: The F-measure and ACE Value for the test
</tableCaption>
<bodyText confidence="0.963946611111111">
sets with true (T) and system output (S) mentions
and entities.
high precision. Adding the parse tree and depen-
dency tree based features gives us our best result
by exploiting the consistent syntactic patterns ex-
hibited between mentions for some relations. Note
that the trends of contributions from different fea-
ture streams is consistent for the “truth” and system
output runs. As expected, the numbers are signifi-
cantly lower for the system output runs due to errors
made by the mention detection and mention chain-
ing modules.
We ran the best model on the official ACE
Feb’2002 and ACE Sept’2003 evaluation sets. We
obtained competitive results shown in Table 4. The
rules of the ACE evaluation prohibit us from dis-
closing our final ranking and the results of other par-
ticipants.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999988105263158">
We have presented a statistical approach for extract-
ing relations where we combine diverse lexical, syn-
tactic, and semantic features. We obtained compet-
itive results on the ACE RDC task.
Several previous relation extraction systems have
focused almost exclusively on syntactic parse trees.
We believe our approach of combining many kinds
of evidence can potentially scale better to problems
(like ACE), where we have a lot of relation types
with relatively small amounts of annotated data.
Our system certainly benefits from features derived
from parse trees, but it is not inextricably linked to
them. Even using very simple lexical features, we
obtained high precision extractors that can poten-
tially be used to annotate large amounts of unlabeled
data for semi-supervised or unsupervised learning,
without having to parse the entire data. We obtained
our best results when we combined a variety of fea-
tures.
</bodyText>
<sectionHeader confidence="0.990287" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997873666666667">
We thank Salim Roukos for several invaluable sugges-
tions and the entire ACE team at IBM for help with var-
ious components, feature suggestions and guidance.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999441177777778">
ACE. 2004. The nist ace evaluation website.
http://www.nist.gov/speech/tests/ace/.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, Barcelona, Spain, July 21–July
26.
Radu Florian, Hany Hassan, Hongyan Jing, Nanda
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and
Salim Roukos. 2004. A statistical model for multilin-
gual entity detection and tracking. In Proceedings of
the Human Language Technologies Conference (HLT-
NAACL’04), Boston, Mass., May 27 – June 1.
Abraham Ittycheriah, Lucian Lita, Nanda Kambhatla,
Nicolas Nicolov, Salim Roukos, and Margo Stys.
2003. Identifying and tracking entity mentions in
a maximum entropy framework. In Proceedings of
the Human Language Technologies Conference (HLT-
NAACL’03), pages 40–42, Edmonton, Canada, May
27 – June 1.
Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing,
Nanda Kambhatla, and Salim Roukos. 2004. A
mention-synchronous coreference resolution algo-
rithm based on the bell tree. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics, Barcelona, Spain, July 21–July
26.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 226–233, Seattle, Wash-
ington, April 29–May 4.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy. Machine Learning
(Special Issue on Natural Language Learning), 34(1-
3):151–176.
Stephanie Strassel, Alexis Mitchell, and Shudong
Huang. 2003. Multilingual resources for entity de-
tection. In Proceedings of the ACL 2003 Workshop on
Multilingual Resources for Entity Detection.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083–1106.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.786109">
<title confidence="0.9993315">Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Extracting Relations</title>
<author confidence="0.999416">Nanda Kambhatla</author>
<affiliation confidence="0.999931">IBM T. J. Watson Research Center</affiliation>
<address confidence="0.9833195">1101 Kitchawan Road Route 134 Yorktown Heights, NY 10598</address>
<email confidence="0.99939">nanda@us.ibm.com</email>
<abstract confidence="0.9805038">Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>The nist ace evaluation website.</title>
<date>2004</date>
<note>http://www.nist.gov/speech/tests/ace/.</note>
<contexts>
<context position="2075" citStr="ACE, 2004" startWordPosition="317" endWordPosition="318">Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features. Our results indicate that using a variety of information sources can result in improved recall and overall F measure. Our approach can easily scale to include more features from a multitude of sources–e.g. WordNet, gazatteers, output of other semantic taggers etc.–that can be brought to bear on this task. In this paper, we present our general approach, describe the features we currently use and show the results of our participation in the ACE evaluation. Automatic Content Extraction (ACE, 2004) is an evaluation conducted by NIST to measure Entity Detection and Tracking (EDT) and relation detection and characterization (RDC). The EDT task entails the detection of mentions of entities and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are explicitly or implicitly stated relationships among entities. Entities can be of five types: persons, organizations, locations, facilities, and geo-political entities (geographically defined regions that define a political boundary, e.g. countries, citie</context>
<context position="9819" citStr="ACE, 2004" startWordPosition="1567" endWordPosition="1568">traction. The ACE value is a NIST metric that assigns 0% value for a system which produces no output and 100% value for a system that extracts all the relations and produces no false alarms. We count the misses; the true relations not extracted by the system, and the false alarms; the spurious relations extracted by the system, and obtain the ACE value by subtracting from 1.0, the normalized weighted cost of the misses and false alarms. The ACE value counts each relation only once, even if it was expressed many times in a document in different ways. The reader is referred to the ACE web site (ACE, 2004) for more details. We built several models to compare the relative utility of the feature streams described in the previous section. Table 2 shows the results we obtained when running on “truth” for the development set and Table 3 shows the results we obtained when running on the output of mention detection and mention chaining modules. Note that a model trained with only words as features obtains a very high precision and a very low recall. For example, for the mention pair his and wife with no words in between, the lexical features together with the fact that there are no words in between is</context>
</contexts>
<marker>ACE, 2004</marker>
<rawString>ACE. 2004. The nist ace evaluation website. http://www.nist.gov/speech/tests/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1366" citStr="Culotta and Sorensen, 2004" startWordPosition="201" endWordPosition="204"> can be very useful for applications such as biography extraction and question answering, e.g. to answer queries such as “Where is the Taj Mahal?”. Several prior approaches to relation extraction have focused on using syntactic parse trees. For the Template Relations task of MUC-7, BBN researchers (Miller et al., 2000) augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees. More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features. Our results indicate that using a variety of information sources can result in improved recall and overall F measure. Our approach can easily scale to include more features from a multitude of sources–e.g. WordNet, gazatteers, output of other semantic taggers etc.–that can be brought to bear on this task. In this paper, we present our general approach, describe the features we currently </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, July 21–July 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technologies Conference (HLTNAACL’04),</booktitle>
<volume>27</volume>
<location>Boston, Mass.,</location>
<contexts>
<context position="4684" citStr="Florian et al., 2004" startWordPosition="718" endWordPosition="721">AR relative-location 288 PART other 6 part-Of 1178 subsidiary 366 ROLE affiliate-partner 219 citizen-Of 450 client 159 founder 37 general-staff 1507 management 1559 member 1404 other 174 owner 274 SOCIAL associate 119 grandparent 10 other-personal 108 other-professional 415 other-relative 86 parent 149 sibling 23 spouse 89 Table 1: The list of relation types and subtypes used in the ACE 2003 evaluation. spective entities, and correctly determining the type of relation that holds between them. This paper focuses on the relation extraction component of our ACE system. The reader is referred to (Florian et al., 2004; Ittycheriah et al., 2003; Luo et al., 2004) for more details of our mention detection and mention chaining modules. In the next section, we describe our extraction system. We present results in section 3, and we conclude after making some general observations in section 4. 2 Maximum Entropy models for extracting relations We built Maximum Entropy models for predicting the type of relation (if any) between every pair of mentions within each sentence. We only model explicit relations, because of poor inter-annotator agreement in the annotation of implicit relations. Table 1 lists the types and</context>
</contexts>
<marker>Florian, Hassan, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Hongyan Jing, Nanda Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of the Human Language Technologies Conference (HLTNAACL’04), Boston, Mass., May 27 – June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Lucian Lita</author>
<author>Nanda Kambhatla</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
<author>Margo Stys</author>
</authors>
<title>Identifying and tracking entity mentions in a maximum entropy framework.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technologies Conference (HLTNAACL’03),</booktitle>
<volume>27</volume>
<pages>40--42</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="4710" citStr="Ittycheriah et al., 2003" startWordPosition="722" endWordPosition="725">88 PART other 6 part-Of 1178 subsidiary 366 ROLE affiliate-partner 219 citizen-Of 450 client 159 founder 37 general-staff 1507 management 1559 member 1404 other 174 owner 274 SOCIAL associate 119 grandparent 10 other-personal 108 other-professional 415 other-relative 86 parent 149 sibling 23 spouse 89 Table 1: The list of relation types and subtypes used in the ACE 2003 evaluation. spective entities, and correctly determining the type of relation that holds between them. This paper focuses on the relation extraction component of our ACE system. The reader is referred to (Florian et al., 2004; Ittycheriah et al., 2003; Luo et al., 2004) for more details of our mention detection and mention chaining modules. In the next section, we describe our extraction system. We present results in section 3, and we conclude after making some general observations in section 4. 2 Maximum Entropy models for extracting relations We built Maximum Entropy models for predicting the type of relation (if any) between every pair of mentions within each sentence. We only model explicit relations, because of poor inter-annotator agreement in the annotation of implicit relations. Table 1 lists the types and subtypes of relations for</context>
</contexts>
<marker>Ittycheriah, Lita, Kambhatla, Nicolov, Roukos, Stys, 2003</marker>
<rawString>Abraham Ittycheriah, Lucian Lita, Nanda Kambhatla, Nicolas Nicolov, Salim Roukos, and Margo Stys. 2003. Identifying and tracking entity mentions in a maximum entropy framework. In Proceedings of the Human Language Technologies Conference (HLTNAACL’03), pages 40–42, Edmonton, Canada, May 27 – June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4729" citStr="Luo et al., 2004" startWordPosition="726" endWordPosition="729">78 subsidiary 366 ROLE affiliate-partner 219 citizen-Of 450 client 159 founder 37 general-staff 1507 management 1559 member 1404 other 174 owner 274 SOCIAL associate 119 grandparent 10 other-personal 108 other-professional 415 other-relative 86 parent 149 sibling 23 spouse 89 Table 1: The list of relation types and subtypes used in the ACE 2003 evaluation. spective entities, and correctly determining the type of relation that holds between them. This paper focuses on the relation extraction component of our ACE system. The reader is referred to (Florian et al., 2004; Ittycheriah et al., 2003; Luo et al., 2004) for more details of our mention detection and mention chaining modules. In the next section, we describe our extraction system. We present results in section 3, and we conclude after making some general observations in section 4. 2 Maximum Entropy models for extracting relations We built Maximum Entropy models for predicting the type of relation (if any) between every pair of mentions within each sentence. We only model explicit relations, because of poor inter-annotator agreement in the annotation of implicit relations. Table 1 lists the types and subtypes of relations for the ACE RDC task, </context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, July 21–July 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>226--233</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="1059" citStr="Miller et al., 2000" startWordPosition="155" endWordPosition="158">iverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results. 1 Introduction Extraction of semantic relationships between entities can be very useful for applications such as biography extraction and question answering, e.g. to answer queries such as “Where is the Taj Mahal?”. Several prior approaches to relation extraction have focused on using syntactic parse trees. For the Template Relations task of MUC-7, BBN researchers (Miller et al., 2000) augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees. More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features. Our results indicate that using a variety of information sources can result in impr</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 226–233, Seattle, Washington, April 29–May 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy.</title>
<date>1999</date>
<journal>Machine Learning (Special Issue on Natural Language Learning),</journal>
<pages>34--1</pages>
<contexts>
<context position="6342" citStr="Ratnaparkhi, 1999" startWordPosition="980" endWordPosition="981">ly model the argument order of mentions. Thus, when comparing mentions and , we distinguish between the case where -citizen-Of- and -citizen-Of- . We thus model the extraction as a classification problem with 49 classes, two for each relation subtype and a “NONE” class for the case where the two mentions are not related. For each pair of mentions, we compute several feature streams shown below. All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework (Ratnaparkhi, 1999). The feature streams are: Words The words of both the mentions and all the words in between. Entity Type The entity type (one of PERSON, ORGANIZATION, LOCATION, FACILITY, Geo-Political Entity or GPE) of both the mentions. Mention Level The mention level (one of NAME, NOMINAL, PRONOUN) of both the mentions. Overlap The number of words (if any) separating the two mentions, the number of other mentions in between, flags indicating whether the two mentions are in the same noun phrase, verb phrase or prepositional phrase. Dependency The words and part-of-speech and chunk labels of the words on whi</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy. Machine Learning (Special Issue on Natural Language Learning), 34(1-3):151–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
<author>Alexis Mitchell</author>
<author>Shudong Huang</author>
</authors>
<title>Multilingual resources for entity detection.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual Resources for Entity Detection.</booktitle>
<contexts>
<context position="7500" citStr="Strassel et al., 2003" startWordPosition="1175" endWordPosition="1178">he words and part-of-speech and chunk labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic parse tree. Parse Tree The path of non-terminals (removing duplicates) connecting the two mentions in the parse tree, and the path annotated with head words. Here is an example. For the sentence fragment, been the chairman of its board ... the corresponding syntactic parse tree is shown in Figure 1 and the dependency tree is shown in Figure 2. For the pair of mentions chairman and board, the feature streams are shown below. 2The reader is referred to (Strassel et al., 2003) or LDC’s web site for more details of the data. Words , , , . Figure 1: The syntactic parse tree for the fragment “chairman of its board”. Figure 2: The dependency tree for the fragment “chairman of its board”. Entity Type (for “chairman”), (for “board”). Mention Level . Overlap one-mention-in-between (the word “its”), two-words-apart, in-same-noun-phrase. Dependency (word on which is depedent), (POS of word on which is dependent), (chunk label of word on which is dependent), Parse Tree PERSON-NP-PP-ORGANIZATION, PERSON-NP-PP:of-ORGANIZATION (both derived from the path shown in bold in Figure</context>
</contexts>
<marker>Strassel, Mitchell, Huang, 2003</marker>
<rawString>Stephanie Strassel, Alexis Mitchell, and Shudong Huang. 2003. Multilingual resources for entity detection. In Proceedings of the ACL 2003 Workshop on Multilingual Resources for Entity Detection.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="1248" citStr="Zelenko et al., 2003" startWordPosition="184" endWordPosition="187">eral approach and describe our ACE results. 1 Introduction Extraction of semantic relationships between entities can be very useful for applications such as biography extraction and question answering, e.g. to answer queries such as “Where is the Taj Mahal?”. Several prior approaches to relation extraction have focused on using syntactic parse trees. For the Template Relations task of MUC-7, BBN researchers (Miller et al., 2000) augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees. More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features. Our results indicate that using a variety of information sources can result in improved recall and overall F measure. Our approach can easily scale to include more features from a multitude of sources–e.g. WordNet, gazatteers, output of other semantic taggers etc.–that ca</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>