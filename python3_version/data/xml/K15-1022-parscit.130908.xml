<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9544325">
Opinion Holder and Target Extraction based on the Induction of Verbal
Categories
</title>
<author confidence="0.985368">
Michael Wiegand
</author>
<affiliation confidence="0.9762535">
Spoken Language Systems
Saarland University
</affiliation>
<address confidence="0.759303">
D-66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.995929">
michael.wiegand@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.993827" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902764705882">
We present an approach for opinion role
induction for verbal predicates. Our model
rests on the assumption that opinion verbs
can be divided into three different types
where each type is associated with a char-
acteristic mapping between semantic roles
and opinion holders and targets. In sev-
eral experiments, we demonstrate the rel-
evance of those three categories for the
task. We show that verbs can easily be
categorized with semi-supervised graph-
based clustering and some appropriate
similarity metric. The seeds are obtained
through linguistic diagnostics. We evalu-
ate our approach against a new manually-
compiled opinion role lexicon and perform
in-context classification.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997927">
While there has been much research in senti-
ment analysis on subjectivity detection and po-
larity classification, there has been less work on
the extraction of opinion roles, i.e. entities that
express an opinion (opinion holders), and enti-
ties or propositions at which sentiment is directed
(opinion targets). Previous research relies on large
amounts of labeled training data or leverages gen-
eral semantic resources which are expensive to
construct, e.g. FrameNet (Baker et al., 1998).
In this paper, we present an approach to induce
opinion roles of verbal predicates. The input is a
set of opinion verbs that can be found in a com-
mon sentiment lexicon. Our model rests on the
assumption that those verbs can be divided into
three different types. Each type has a character-
istic mapping between semantic roles and opinion
holders and targets. Thus, the problem of opinion
role induction is reduced to automatically catego-
rizing opinion verbs.
</bodyText>
<author confidence="0.55114">
Josef Ruppenhofer
</author>
<affiliation confidence="0.742525333333333">
Dept. of Information Science
and Language Technology
Hildesheim University
</affiliation>
<address confidence="0.393547">
D-31141 Hildesheim, Germany
</address>
<email confidence="0.406359">
ruppenho@uni-hildesheim.de
</email>
<bodyText confidence="0.999991926829269">
We frame the task of opinion role extraction as
a triple (pred, const, role) where pred is a predi-
cate evoking an opinion (we exclusively focus on
opinion verbs), const is some constituent bearing
a semantic role assigned by pred, and role is the
opinion role that is assigned to const.
Our work assumes the knowledge of opinion
words. We do not cover polarity classification.
Many lexicons with that kind of information al-
ready exist. Our sole interest is the assignment of
opinion holder and target given some opinion verb.
There does not exist any publicly available lexical
resource specially designed for this task.
For the induction of opinion verb types, we con-
sider semi-supervised graph clustering with some
appropriate similarity metric. We also propose an
effective method for deriving seeds automatically
by applying some linguistic diagnostics.
Our approach is evaluated in a supervised learn-
ing scenario on a set of sentences with annotated
opinion holders and targets. We employ differ-
ent kinds of features, including features derived
from a semantic parser based on FrameNet. We
also compare our proposed model based on the
three opinion verb types against a new manually-
compiled lexicon in which the semantic roles of
opinion holders and targets for each individual
verb have been explicitly enumerated.
We also evaluate our approach in the context of
cross-domain opinion holder extraction. Thus we
demonstrate the importance of our approach in the
context of previous datasets and classifiers.
This is the first work that proposes to induce
both opinion holders and targets evoked by opin-
ion verbs with data-driven methods. Unlike previ-
ous work, we are able to categorize all verbs of a
pre-specified set of opinion verbs. Our approach
is a low-resource approach that is also applicable
to languages other than English. We demonstrate
this on German. A by-product of our study are
new resources including a verb lexicon specifying
</bodyText>
<page confidence="0.987147">
215
</page>
<note confidence="0.988065">
Proceedings of the 19th Conference on Computational Language Learning, pages 215–225,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.428086">
semantic roles for holders and targets.
</bodyText>
<sectionHeader confidence="0.972972" genericHeader="introduction">
2 Lexicon-based Opinion Role
Extraction
</sectionHeader>
<bodyText confidence="0.996913666666667">
Opinion holder and target extraction is a hard task
(Ruppenhofer et al., 2008). Conventional syntac-
tic or semantic levels of representation do not cap-
ture sufficient information that allows a reliable
prediction of opinion holders and targets. This is
illustrated by (1) and (2) which show that, even
with common semantic roles, i.e. agent and pa-
tient1, assigned to the entities, one may not be able
to discriminate between the opinion roles.
</bodyText>
<listItem confidence="0.908841666666667">
(1) Peteragent criticized Marypatient.
(criticize, Peter, holder) &amp; (criticize, Mary, target)
(2) Peteragent disappoints Marypatient.
</listItem>
<keyword confidence="0.262837">
(disappoint, Peter, target) &amp; (disappoint, Mary, holder)
</keyword>
<bodyText confidence="0.9992711">
We assume that it is lexical information that de-
cides what semantic role an opinion holder or
opinion target takes. As a consequence, we built a
gold-standard lexicon for verbs that encodes such
information. For example, it states that the target
of criticize is its patient, while for disappoint, the
target is its agent. This fine-grained lexicon also
accounts for the fact that a constituent can have
several roles given the same opinion verb. An ex-
treme case is:
</bodyText>
<listItem confidence="0.715872">
(3) [Peter]1 persuades [Mary]2 [to accept his invitation]3.
</listItem>
<bodyText confidence="0.86866">
The sentence conveys that:
</bodyText>
<listItem confidence="0.99992475">
• Peter wants Mary to do something. (view1)
• Mary is influenced by Peter. (view2)
• Peter has some attitude towards Mary accepting his invitation. (view3)
• Mary has some attitude towards accepting Peter’s invitation. (view4)
</listItem>
<bodyText confidence="0.67893">
This corresponds to the role assignments:
</bodyText>
<listItem confidence="0.8663036">
• view1: (persuade, [1], holder), (persuade, [2], target)
• view2: (persuade, [2], holder), (persuade, [1], target)
• view3: (persuade, [1], holder), (persuade, [3], target)
• view4: (persuade, [2], holder), (persuade, [3], target)
(in short: 2 opinion holders and 3 opinion targets).
Our lexicon also includes another dimension
neglected in many previous works. Many opinion
verbs predominantly express the sentiment of the
speaker of the utterance (or some nested source)
(4). This concept is also known as expressive sub-
jectivity (Wiebe et al., 2005) or speaker subjectiv-
ity (Maks and Vossen, 2012). In such opinions, the
opinion holder is not realized as a dependent of the
opinion verb.
(4) At my work, [they]1 are constantly gossiping.
</listItem>
<footnote confidence="0.526304666666667">
(gossip, speaker, holder) &amp; (gossip, [1], target)
1By agent and patient, we mean constituents labeled as
AO and Al in PropBank (Kingsbury and Palmer, 2002).
</footnote>
<bodyText confidence="0.999653470588235">
Our lexicon covers the 1175 verb lemmas con-
tained in the Subjectivity Lexicon (Wilson et al.,
2005). We annotated the semantic roles similar to
the format of PropBank (Kingsbury and Palmer,
2002). The basis of the annotation were online
dictionaries (e.g. Macmillan Dictionary) which
provide both a verb definition and example sen-
tences. We do not annotate implicature-related in-
formation about effects (Deng and Wiebe, 2014)
but inherent sentiment (the data release2 includes
more details regarding the annotation process and
our notion of holders and targets).
On a sample of 400 verbs, we measured an in-
terannotation agreement of Cohen’s n = 60.8 for
opinion holders, n = 62.3 for opinion targets and
n = 59.9 for speaker views. This agreement is
mostly substantial (Landis and Koch, 1977).
</bodyText>
<sectionHeader confidence="0.986362" genericHeader="method">
3 The Three Verb Categories
</sectionHeader>
<bodyText confidence="0.999983692307692">
Rather than induce the opinion roles for individ-
ual verbs, we group verbs that share similar opin-
ion role subcategorization. Thus, the main task for
induction is to decide which type an opinion verb
belongs to. Once the verb type has been estab-
lished, the typical semantic roles for opinion hold-
ers and targets can be derived from that type. The
verb categorization is motivated by the semantic
roles of the three common views (Table 1) that an
opinion holder can take. In our lexicon, all of the
opinion holders were observed with either of these
semantic roles. For facilitating induction, we as-
sume that those types are disjoint (see also §3.4).
</bodyText>
<subsectionHeader confidence="0.999874">
3.1 Verbs with Agent View (AG)
</subsectionHeader>
<bodyText confidence="0.9997112">
Verbs with an agent view, such as criticize, love
and believe, convey the sentiment of its agent.
Therefore, those verbs take the agent as opinion
holder and the patient as opinion target. Table 1
also exemplifies semantic role labels as a suitable
basis to align opinion holders and targets within a
particular verb type. For example, targets of AG-
verbs align to the patient, yet the patient can take
the form of various phrase types (i.e. NPs, PPs or
infinitive/complement phrases3).
</bodyText>
<footnote confidence="0.995722166666667">
2available at: www.coli.uni-saarland.de/
˜miwieg/conll_2015_op_roles_data.tgz
3Note that infinitive and complement clauses may rep-
resent a semantic role other than patient (e.g. the infinitive
clause in (3)). As these types of clauses are fairly unambigu-
ous, we marked them as targets even if they are no patients.
</footnote>
<page confidence="0.996543">
216
</page>
<table confidence="0.99974975">
Type Example Holder Target
AG [They]agent like [the idea]patient. agent pat.
[The guests]agent complained [about noise]patient.
[They]agent argue [that this plan is infeasible]patient.
PT [The noise]agent irritated [the guests]patient. pat. agent
[That gift]agent pleased [her]patient very much.
SP [They]agent cheated [in the exam]adjunct . -N/A- agent,
[He]agent besmirched [the King’s name]patient. (pat.)
</table>
<tableCaption confidence="0.999743">
Table 1: Verb types for opinion role extraction.
</tableCaption>
<subsectionHeader confidence="0.999891">
3.2 Verbs with Patient View (PT)
</subsectionHeader>
<bodyText confidence="0.999774">
Verbs with a patient view (irritate, upset and dis-
appoint) are opposite to AG-verbs in that those
verbs have the patient as opinion holder and the
agent as opinion target.
</bodyText>
<subsectionHeader confidence="0.99974">
3.3 Verbs with Speaker View (SP)
</subsectionHeader>
<bodyText confidence="0.999992230769231">
The third type we consider comprises all verbs
whose perspective is that of the speaker. That is,
these are verbs whose sentiment is primarily that
of the speaker of the utterance rather than persons
involved in the action to which is referred. Typical
examples are gossip, improve or cheat.
While the agent is usually the target of the senti-
ment of the speaker, it depends on the specific verb
whether its patient is also a target or not (in Table
1, only the patient of the second SP-verb, i.e. be-
smirch, is considered a target4). Since we aim at a
precise induction approach, we will always (only)
mark the agent of an induced SP-verb as a target.
</bodyText>
<subsectionHeader confidence="0.984052">
3.4 Relation to Fine-Grained Lexicon
</subsectionHeader>
<bodyText confidence="0.999883222222222">
Table 2 provides statistics as to how clear-cut the
three prototypical verb types are in the manually-
compiled fine-grained lexicon. These numbers
suggest that many verbs evoke several opinion
views (e.g. a verb with an AG-view may also
evoke a PT-view). While the fine-grained lexi-
con is fairly exhaustive in listing semantic roles
for opinion holders and targets, it may also oc-
casionally overgenerate. One major reason for
this is that we do not annotate on the sense-level
(word-sense disambiguation (Wiebe and Mihal-
cea, 2006) is still in its infancy) but on the lemma-
level. Accordingly, we attribute all views to all
senses, whereas actually certain views pertain only
to specific senses. However, we found that usually
one view is conveyed by most (if not all) senses of
a word. For example, the lexicon lists both an AG-
view and a PT-view for appease. This is correct
</bodyText>
<footnote confidence="0.986053">
4We consider the patient a target since the speaker has a
positive (non-defeasible) sentiment towards that entity.
</footnote>
<figureCaption confidence="0.84012">
Type Freq Type Freq
verbs with AG-view 868 verbs with PT-view 392
verbs with exclusive AG-view 371 verbs with exclusive PT-view 117
verbs with AG- and SP-view 352 verbs with PT- and AG-view 226
verbs with AG- and PT-view 226 verbs with PT- and SP-view 139
verbs with SP-view 537
verbs with exclusive SP-view 134
verbs with SP- and AG-view 352
verbs with SP- and PT-view 139
</figureCaption>
<tableCaption confidence="0.986099">
Table 2: Verb types in the fine-grained lexicon.
</tableCaption>
<table confidence="0.999457666666667">
Agent (AG) Patient (PT) Speaker (SP)
Freq Percent Freq Percent Freq Percent
450 38.3 188 16.0 537 45.7
</table>
<tableCaption confidence="0.999715">
Table 3: Verb types in the coarse-grained lexicon.
</tableCaption>
<bodyText confidence="0.999763703703704">
for (5) but wrong for (6). The AG-view is derived
from a definition to give your opponents what they
want. (6) does not convey an agent’s volitional ac-
tion. Here, the verb just conveys make someone
feel less angry. Similarly, the lexicon lists an SP-
view and an AG-view for degrade, which is right
for (7) but wrong for (8). The AG-view is derived
from a lexicon definition to treat someone in a way
that makes them stop respecting themselves. (8)
does not convey an agent’s volitional action. The
verb just conveys to make something worse. That
is, neither (6) nor (8) evoke an AG-view. We found
that these variations regularly occur. We adopt
the heuristic that verbs with an SP-view and AG-
or PT-view preserve the SP-view across their uses
(7)-(8). Verbs with both PT- and AG-view pre-
serve their PT-view (5)-(6). Following these ob-
servations, we converted our fine-grained lexicon
into a gold standard coarse-grained lexicon (only
3% of the verbs needed to be manually corrected
after the automatic conversion) in which a verb is
classified as AG, PT or SP according to its domi-
nant view. The final class distribution of this lexi-
con is shown in Table 3. In §5.2, we show through
an in-context evaluation that our coarse-grained
representation preserves most of the information
captured by the fine-grained representation.
</bodyText>
<listItem confidence="0.9990745">
(5) [Chamberlain]agent appeased [Hitler]patient.
(6) [The orange juice]agent appeased [him]patient for awhile.
(7) [Mary]agent degrades [Henrietta]patient.
(8) [This technique]agent degrades [the local water supply]patient.
</listItem>
<sectionHeader confidence="0.926958" genericHeader="method">
4 Induction of Verb Categories
</sectionHeader>
<bodyText confidence="0.9987825">
The task is to categorize each verb as a predom-
inant AG-, PT-, or SP-verb. Our approach com-
prises two steps. In the first step, seeds for the dif-
ferent verb types are extracted (§4.1). In the sec-
</bodyText>
<page confidence="0.868457">
217
</page>
<bodyText confidence="0.9851615">
AG argue, contend, speculate, fear, doubt, complain, con-
sider, praise, recommend, view, acknowledge, hope
PT interest, surprise, please, excite, disappoint, delight, im-
press, shock, trouble, embarrass, annoy, distress
SP murder, plot, incite, blaspheme, bewitch, bungle, de-
spoil, plagiarize, prevaricate, instigate, molest, conspire
</bodyText>
<tableCaption confidence="0.945717">
Table 4: The top 12 extracted verb seeds.
</tableCaption>
<bodyText confidence="0.999854310344827">
ond step, a similarity metric (§4.2) is employed in
order to propagate the verb type labels from the
seeds to the remaining opinion verbs (§4.3). The
North American News Text Corpus is used for seed
extraction and computation of verb similarities.
Wiegand and Klakow (2012) proposed methods
for extracting AG- and PT-verbs. We will re-use
these methods for generating seeds. A major con-
tribution of this paper is the introduction of the
third dimension, i.e. SP-verbs, in the context of
induction. We show that in combination with this
third dimension, one can categorize all opinion
verbs contained in a sentiment lexicon. Further-
more, given this three-way classification, we also
obtain better results on the detection of AG-verbs
and PT-verbs than by just detecting those verbs
in isolation without graph clustering (this will be
shown in Table 7 and discussed in §5.1).
A second major contribution of this work is that
we show that these methods are also equally im-
portant for opinion target extraction. So far, the
significance of AG- and PT-verbs has only been
demonstrated for opinion holder extraction.
In this work, we exclusively focus on the set of
1175 opinion verbs from the Subjectivity Lexicon.
However, this is owed solely to the effort required
to generate larger sets of evaluation data. In prin-
ciple, our induction approach is applicable to any
set of opinion verbs of arbitrary (e.g. larger) size.
</bodyText>
<subsectionHeader confidence="0.999309">
4.1 Pattern-based Seed Initialization
</subsectionHeader>
<bodyText confidence="0.999960666666666">
For AG-verbs, we rely on the findings of Wiegand
and Klakow (2012) who suggest that verbs predic-
tive for opinion holders can be induced with the
help of prototypical opinion holders. These com-
mon nouns, e.g. opponents (9) or critics (10), act
like opinion holders and, therefore, can be seen
as a proxy. Verbs co-occurring with prototypical
opinion holders do not represent the entire range
of opinion verbs but coincide with AG-verbs.
</bodyText>
<listItem confidence="0.997638">
(9) Opponents claim these arguments miss the point.
(10) Critics argued that the proposed limits were unconstitutional.
</listItem>
<bodyText confidence="0.999691888888889">
For PT-verbs, we make use of the adjective heuris-
tic proposed by Wiegand and Klakow (2012). The
authors make use of the observation that morpho-
logically related adjectives exist for PT-verbs, un-
like for AG- and SP-verbs. Therefore, in order
to extract PT-verbs, one needs to check whether
a verb in its past participle form, such as up-
set in (11), is identical to some predicate adjec-
tive (12).
</bodyText>
<listItem confidence="0.943362">
(11) He had upset_b me.
(12) I am upset,l� .
</listItem>
<bodyText confidence="0.9981476">
We are not aware of any previously published
approach effectively inducing SP-verbs. Noticing
that many of those verbs contain some form of re-
proach, we came up with the patterns accused of
XVBG and blamedforXVBG as in (13) and (14).
</bodyText>
<listItem confidence="0.9782785">
(13) He was accused offalsifying the documents.
(14) The UN was blamed for misinterpreting climate data.
</listItem>
<bodyText confidence="0.999444571428571">
Table 4 lists for each of the verb types the 12
seeds most frequently occurring with the respec-
tive patterns. We observed that the SP-verb seeds
are exclusively negative polar expressions. That
is why we also extracted seeds from an additional
pattern help to XVB producing prototypical posi-
tive SP-verbs, such as stabilize, allay or heal.
</bodyText>
<subsectionHeader confidence="0.9972925">
4.2 Similarity Metrics
4.2.1 Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999984111111111">
Recent research in machine learning has focused
on inducing vector representations of words. As
an example of a competitive word embedding
method, we induce vectors for our opinion verbs
with Word2Vec (Mikolov et al., 2013). Baroni et
al. (2014) showed that this method outperforms
count vector representations on a variety of tasks.
For the similarity between two verbs, we compute
the cosine-similarity between their vectors.
</bodyText>
<subsectionHeader confidence="0.949772">
4.2.2 WordNet::Similarity
</subsectionHeader>
<bodyText confidence="0.999728142857143">
We use WordNet::Similarity (Pedersen et al.,
2004) as an alternative source for similarity met-
rics. The metrics are based on WordNet’s graph
structure (Miller et al., 1990). Various relations
within WordNet have been shown to be effective
for polarity classification (Esuli and Sebastiani,
2006; Rao and Ravichandran, 2009).
</bodyText>
<subsectionHeader confidence="0.934507">
4.2.3 Coordination
</subsectionHeader>
<bodyText confidence="0.9997752">
Another method to measure similarity is ob-
tained by leveraging coordination. Coordination
is known to be a syntactic relation that also pre-
serves great semantic coherence (Ziering et al.,
2013), e.g. (15). It has been successfully applied
</bodyText>
<page confidence="0.991625">
218
</page>
<bodyText confidence="0.996669555555556">
not only to noun categorization (Riloff and Shep-
herd, 1997; Roark and Charniak, 1998) but also
to different tasks in sentiment analysis, includ-
ing polarity classification (Hatzivassiloglou and
McKeown, 1997), the induction of patient polarity
verbs (Goyal et al., 2010) and connotation learn-
ing (Kang et al., 2014). We use the dependency
relation from Stanford parser (Klein and Manning,
2003) to detect coordination (16).
</bodyText>
<listItem confidence="0.9195065">
(15) They criticize and hate him.
(16) conj(criticize,hate)
</listItem>
<bodyText confidence="0.8721495">
As a similarity function, we simply take the
absolute frequency of observing two words w1
</bodyText>
<construct confidence="0.5809245">
and w2 in a conjunction, i.e. sim(w1, w2) =
freq(conj(w1, w2)).
</construct>
<subsubsectionHeader confidence="0.862653">
4.2.4 Dependency-based Similarity
</subsubsectionHeader>
<bodyText confidence="0.9981574">
The metric proposed by Lin (1998) exploits the
rich set of dependency-relation labels in the con-
text of distributional similarity. Moreover, it has
been effectively used for the related task of ex-
tending frames of unknown predicates in semantic
parsing (Das and Smith, 2011).
The metric is based on dependency triples
(w, r, w′) where w and w′ are words and r is a
dependency relation (e.g. (argue-V, nsubj,
critics-N)). The metric is defined as:
</bodyText>
<equation confidence="0.7742588">
r-
(r,w)∈T (w1)∩T (w2)(I(w1 ,r,w)+I(w2,r,w))
sim(w1, w2) = r-(r,w)∈T (w1) I(w1,r,w)+r-(r,w)∈T (w2) I(w2,r,w)
where I(w, r, w′) = log kw,r,w′k×k∗,r,∗k
kw,r,∗k×k∗,r,w′k and T (w) is
</equation>
<bodyText confidence="0.914295">
defined as the set of pairs (r, w′) such that
</bodyText>
<equation confidence="0.577792">
log kw,r,w′k×k∗,r,∗k &gt; 0.
kw,r,∗k×k∗,r,w k
</equation>
<subsectionHeader confidence="0.935347">
4.3 Propagation Methods
</subsectionHeader>
<bodyText confidence="0.982889571428571">
We use the k nearest neighbour classifier (kNN)
(Cover and Hart, 1967) as a simple method for
propagating labels from seeds to other instances.
Alternatively, we consider verb categorization as a
clustering task on a graph G = (V, E, W) where
V is the set of nodes (i.e. our opinion verbs), E
is the set of edges connecting them with weights
</bodyText>
<listItem confidence="0.685817">
W : E → R+. W can be directly derived from
any of the similarity metrics (§4.2.1-§4.2.4). The
aim is that all nodes v E V are assigned a la-
bel l E {AG, PT, SP}. Initially, only the verb
seeds are labeled. We then use the Adsorption la-
bel propagation algorithm from junto (Talukdar et
al., 2008) in order propagate the labels from the
seeds to the remaining verbs.
</listItem>
<table confidence="0.999918909090909">
Acc Prec Rec F1
Baselines Majority Class 45.7 14.2 33.3 20.9
Only Seeds 8.9 87.0 9.8 17.6
Coordination kNN 45.2 61.5 47.3 53.4
graph 42.7 68.7 39.7 50.4
WordNet kNN 52.8 51.5 50.7 51.1
graph 51.1 51.9 51.5 51.5
Embedding kNN 59.3 58.4 61.0 59.7
graph 64.0 70.5 59.4 64.5
Dependency kNN 65.7 63.8 65.4 64.5
graph 70.3 72.0 68.0 70.6
</table>
<tableCaption confidence="0.999264">
Table 5: Eval. of similarity metrics and classifiers.
</tableCaption>
<sectionHeader confidence="0.999001" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998543">
5.1 Evaluation of the Induced Lexicon
</subsectionHeader>
<bodyText confidence="0.99992754054054">
Table 5 compares the performance of the differ-
ent similarity metrics when incorporated in either
kNN or graph clustering. The resulting catego-
rizations are compared against the gold standard
coarse-grained lexicon (§3.4). For kNN, we set
k = 3 for which we obtained best performance in
all our experiments.
As seeds, we took the top 40 AG-verbs, 30 PT-
verbs and 50 SP-verbs produced by the respective
initialization methods (§4.1). The seed propor-
tions should vaguely correspond to the actual class
distribution (Table 3). Large increases of the seed
sets do not improve the quality (as shown below).
10 of the 50 SP-verbs are extracted from the posi-
tive SP-patterns, while the remaining verbs are ex-
tracted from the negative SP-patterns (§4.1).
As baselines, we include a classifier only em-
ploying the seeds and a majority class classifier
always predicting an SP-verb. For word embed-
dings (§4.2.1) and WordNet::Similarity (§4.2.2),
we only report the performance of the best met-
ric/configuration, i.e. for embeddings, the con-
tinuous bag-of-words model with 500 dimensions
and for WordNet::Similarity, the Wu &amp; Palmer
measure (Wu and Palmer, 1994).
Table 5 shows that the baselines can be out-
performed by large margins. The performance
of the different similarity metrics varies. The
dependency-based metric performs notably better
than the other metrics. Together with word embed-
dings, it is the only metric for which graph cluster-
ing produces a notable improvement over kNN.
Table 6 illustrates the quality of the similar-
ity metrics for the present task. The table shows
that the dependency-based similarity metric pro-
vides the most suitable output. The poor qual-
ity of coordination may come as a surprise. That
</bodyText>
<page confidence="0.990102">
219
</page>
<table confidence="0.997645">
Coordin. appear, believe, refuse, vow, want, offend, shock, help, ex-
hilarate, challenge, support, distort
WordNet appal, scandalize, anger, rage, sicken, temper, hate, fear,
love, alarm, dread, tingle
Embedd. anger, dismay, disgust, protest, alarm, enrage, shock, regret,
concern, horrify, appal, sorrow
Depend. anger, infuriate, alarm, shock, stun, enrage, incense, dis-
may, upset, appal, offend, disappoint
</table>
<tableCaption confidence="0.96959">
Table 6: The 12 most similar verbs to outrage
(PT-verb) according to the different metrics (verbs
other than PT-verbs are underlined).
</tableCaption>
<table confidence="0.99917125">
AG-verbs PT-verbs SP-verbs
no graph graph no graph graph no graph graph
(Wiegand2012) (Wiegand2012)
55.45 69.12 38.59 67.66 52.16 72.03
</table>
<tableCaption confidence="0.986306">
Table 7: F-scores of entire output of pattern-based
</tableCaption>
<bodyText confidence="0.9939265">
extraction (§4.1) where no propagation is applied
(no graph) vs. best proposed induction method
from Table 5 (graph).
method suffers from data-sparsity. In our corpus,
the frequency of verbs co-occurring with outrage
in a conjunction is 5 or lower.5 The table also
shows that WordNet may not be appropriate for
our present verb categorization task. However, it
may be suitable for other subtasks in sentiment
analysis, particularly polarity classification. If we
consider the similar entries of outrage provided by
that metric, we find that polarity is largely pre-
served (10 out of 12 verbs are negative). This ob-
servation is consistent with Esuli and Sebastiani
(2006) and Rao and Ravichandran (2009).
In Table 5 we only used the top 40/30/50 verbs
from the initialization methods as seeds. We can
also compare the output of these methods (com-
bined with propagation, i.e. graph clustering) with
the entire verb lists produced by these pattern-
based initialization methods where no propagation
is applied. As far as AG- and PT-verbs are con-
cerned, the entire lists of these initialization meth-
ods correspond to the original approach of Wie-
gand and Klakow (2012). Table 7 shows the result.
The new graph-induction always outperforms the
original induction method by a large margin.
In Table 8, we compare our automatically gen-
</bodyText>
<footnote confidence="0.977356">
5We found that for frequently occurring opinion verbs,
this similarity metric produces more reasonable output.
</footnote>
<table confidence="0.771645">
Patternhatf Pattern Patterndoubte Goldhatf Gold Golddoubte
68.71 70.59 66.50 66.21 70.31 73.77
</table>
<tableCaption confidence="0.7503265">
Table 8: Comparison of automatic and gold seeds
(evaluation measure: macro-average F-score).
</tableCaption>
<table confidence="0.999589">
Coordin. WordNet Embedd. Depend.
Major. kNN graph kNN graph kNN graph kNN graph
English 20.9 53.4 50.4 51.1 51.5 59.7 64.5 64.5 70.6
German 22.9 43.8 48.9 53.2 59.9 54.3 60.9 58.3 63.1
</table>
<tableCaption confidence="0.820078">
Table 9: Comparison of English and German data
(evaluation measure: macro-average F-score).
</tableCaption>
<bodyText confidence="0.991369">
erated seeds using the patterns from §4.1 (Pat-
tern) with seeds extracted from our gold stan-
dard (Gold). We rank those verbs by fre-
quency. Size and verb type distribution are pre-
served. We also examine what impact doubling
the size of seeds (Gold|Patterndouble) and halv-
ing them (GoldlPatternhalf ) has on classification.
Dependency-based similarity and graph clustering
is used for all configurations. Only if we double
the amount of seeds are the gold seeds notably bet-
ter than the automatically generated seeds.
Since our induction approach just requires a
sentiment lexicon and aims at low-resource lan-
guages, we replicated the experiments for Ger-
man, as shown in Table 9. We use the PolArt-
sentiment lexicon (Klenner et al., 2009) (1416 en-
tries). (As a gold standard, we manually annotated
that lexicon according to our three verb types.)
As an unlabeled corpus, we chose the Huge Ger-
man Corpus6. As a parser, we used ParZu (Sen-
nrich et al., 2009). Instead of WordNet, we used
GermaNet (Hamp and Feldweg, 1997). The au-
tomatically generated seeds were manually trans-
lated from English to German. Table 9 shows
that as on English data, dependency-based similar-
ity combined with graph clustering performs best.
The fact that we can successfully replicate our
approach in another language supports the gen-
eral applicability of our proposed categorization of
verbs into three types for opinion role extraction.
</bodyText>
<subsectionHeader confidence="0.979301">
5.2 In-Context Evaluation
</subsectionHeader>
<bodyText confidence="0.9991465">
We now evaluate our induced knowledge in the
task of extracting opinion holders and targets from
actual text. For this in-context evaluation, we sam-
pled sentences from the North American News
Corpus in which our opinion verbs occurred. We
annotated all holders and targets of those verbs.
(A constituent may have several roles for the same
verb (§2).) The dataset contains about 1100 sen-
tences. We need to rely on this dataset since it
is the only corpus in which our opinion verbs are
</bodyText>
<footnote confidence="0.9786655">
6www.ims.uni-stuttgart.de/forschung/
ressourcen/korpora/hgc.html
</footnote>
<page confidence="0.98656">
220
</page>
<table confidence="0.905357916666667">
Features Description
cand lemma head lemma of candidate (phrase)
cand pos part-of-speech tag of head of candidate phrase
cand phrase phrase label of candidate
cand person is candidate a person
verb lemma verb lemmatized
verb pos part-of-speech tag of verb
word bag of words: all words within the sentence
pos part-of-speech sequence between cand. and verb
distance token distance between candidate and verb
const path from constituency parse tree from cand. to verb
subcat subcategorization frame of verb
srlpropbank/dep semantic role/dependency path between cand. and
brown verb (semantic roles based on PropBank)
Brown-clusters of cand word/verb word/word
srlframenet frame element name assigned to candidate and the
frame name (to which frame element belongs)
fine-grain lex is candidate holder/target/targetspeaker according
to the fine-grained lexicon
coarse-grain lex is candidate holder/target/targetspeaker according
to the coarse-grained lexicon
inducgraph is candidate holder/target/targetspeaker according
to the coarse-grained lexicon automatically induced
with graph clustering (and induced seeds (§4.1))
</table>
<tableCaption confidence="0.994693">
Table 10: Feature set for in-context classification.
</tableCaption>
<bodyText confidence="0.999581066666667">
widely represented and both holders and targets
are annotated.
We solve this task with supervised learning. As
a classifier, we employ Support Vector Machines
as implemented in SVMlight (Joachims, 1999).
The task is to extract three different entities: opin-
ion holders, opinion targets and opinion targets
evoked by speaker views. All those entities are al-
ways put into the relation to a specific opinion verb
in the sentence. The instance space thus consists
of tuples (verb, const), where verb is the mention
of an opinion verb and const is any possible (syn-
tactic) constituent in the respective sentence. The
dataset contains 753 holders, 745 targets and 499
targets of a speaker view. Since a constituent may
have several roles at the same time, we train three
binary classifiers for either of the entity types. On
a sample of 200 sentences, we measured an in-
terannotation agreement of Cohen’s r. = 0.69 for
holders, r. = 0.63 for targets and also r. = 0.63 for
targets of a speaker view.
Table 10 shows the features used in our super-
vised classifier. They have been previously found
effective (Choi et al., 2005; Jakob and Gurevych,
2010; Wiegand and Klakow, 2012; Yang and
Cardie, 2013). The standard features are the fea-
tures from cand word to brown. For semantic role
labeling of PropBank-structures, we used mate-
tools (Bj¨orkelund et al., 2009). For person detec-
tion, we employ named-entity tagging (Finkel et
</bodyText>
<table confidence="0.999763888888889">
Features Holder Target TargetSpeaker
standard 63.59 54.18 40.06
+srlframenet 65.44∗ 55.70∗ 42.14
+inducgraph 68.06∗◦ 59.61∗◦ 46.66∗◦
+srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦
+coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦†
+srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦†
+fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦†
+srlframenet+fine-grain lex 70.80∗◦† 63.72∗◦† 56.64∗◦†
</table>
<tableCaption confidence="0.924547666666667">
statistical significance testing (paired t-test, significance level p &lt; 0.05) ∗:
better than standard; ◦: better than +srlframenet; †: better than +inducgraph
Table 11: In-context evaluation (eval.: F-score).
</tableCaption>
<bodyText confidence="0.977653833333334">
al., 2005) and WordNet (Miller et al., 1990).
For semantic role labeling of FrameNet-
structures (srlframenet), we used Semafor (Das et
al., 2010) with the argument identification based
on dual decomposition (Das et al., 2012). We run
the configuration that also assigns frame structures
to unknown predicates (Das and Smith, 2011).
This is necessary as 45% of our opinion verbs are
not contained in FrameNet (v1.5). FrameNet has
been shown to enable a correct role assignment for
AG- and PT-verbs (Bethard et al., 2004; Kim and
Hovy, 2006). For instance, in (17) and (18), the
opinion holder is assigned to the same frame el-
ement EXPERIENCER. However, the PropBank
representation does not produce a correct align-
ment: In (17), the opinon holder is the agent of
the opinion verb, while in (18), the opinion holder
is the patient of the opinion verb.
</bodyText>
<figure confidence="0.5939166">
(17) PeterEXPERIENCER dislikes M
agent ypatient.
(dislike, Peter, holder)
EXPERIENCER
(18) Peteragent disappoints Marypatient .
</figure>
<figureCaption confidence="0.264863">
(disappoint, Mary, holder)
</figureCaption>
<bodyText confidence="0.999900722222222">
With the feature fine-grain lex, we want to val-
idate that our manually-compiled opinion role lex-
icon for verbs (§2), i.e. the lexicon that also allows
multiple opinion roles for the same semantic roles,
is effective for in-context evaluation. Coarse-
grain lex is derived from the fine-grained lexicon
(§3.4). With this feature, we measure how much
we lose by dropping the fine-grained representa-
tion. Inducgraph induces the verb types of the
coarse representation automatically by employing
the best induction method obtained in Table 5.
Table 11 compares the different features on 10-
fold crossvalidation. The table shows that the fea-
tures encoding opinion role information, including
our induction approach, are more effective than
srlframenet. Even though the fine-grained lexicon
produces the best results, we almost reach that per-
formance with the coarse-grained lexicon. This is
</bodyText>
<page confidence="0.994009">
221
</page>
<table confidence="0.882817833333333">
manual lexicons
Features inducgraph coarse-grain fine-grain
lexicon feature only (≈ unsuperv.) 52.38 55.81 60.68
lexicon with all other features 59.90∗ 61.71∗ 63.92◦
statistical significance testing (paired t-test, significance level p &lt; 0.05) ∗:
better than lexicon feature only; ◦: only significant on 2 out of 3 roles
</table>
<tableCaption confidence="0.641153666666667">
Table 12: Lexical resources and the impact
of other (not lexicon-based) features (evaluation
measure: macro-average F-score).
</tableCaption>
<table confidence="0.9999884">
Distribution of Verb Types
Corpus AG PT SP # sentences
MPQA (training+test) 77.5 7.7 14.8 15, 753
FICTION (test) 67.5 15.1 17.3 614
VERB (test) 34.8 14.0 52.7 1, 073
</table>
<tableCaption confidence="0.999698">
Table 13: Statistics on the different corpora used.
</tableCaption>
<bodyText confidence="0.999092290322581">
further evidence that our proposed three-way verb
categorization, which is also the basis of our in-
duction approach, is adequate.
Table 12 compares the performance of the dif-
ferent lexicons in isolation (this is comparable
with an unsupervised classifier, as each lexicon
feature has three values each predicting either of
the opinion roles) and in combination with the
standard (+srlf,,,,,,t) features. The table shows
that all lexicon features are strong features on their
own. The score of induction is lowest but this fea-
ture has been created without manual supervision.
Moreover, the improvement by adding the other
features is much larger for induction than for the
manually-built fine-grained lexicon. This means
that we can compensate some lexical knowledge
missing in induction by standard features.
Since we could substantially outperform the
features relying on FrameNet with our new lex-
ical resources, we looked closer at the predicted
frame structures. Beside obvious errors in auto-
matic frame assignment, we also found that there
are problems inherent in the frame design. Particu-
larly, the notion of SP-verbs (§3.3) is not properly
reflected. Many frames, such as SCRUTINY, typi-
cally devised for AG-verbs, such as investigate or
analyse, also contain SP-verbs like pry. This ob-
servation is in line with Ruppenhofer and Rehbein
(2012) who claim that extensions to FrameNet are
necessary to properly represent opinions evoked
by verbal predicates.
</bodyText>
<subsectionHeader confidence="0.9936155">
5.3 Comparison to Previous Cross-Domain
Opinion Holder Extraction
</subsectionHeader>
<bodyText confidence="0.990874">
We now compare our proposed induction ap-
proach with previous work on opinion holder ex-
</bodyText>
<table confidence="0.877236454545454">
in domain out of domain
Config MPQA FICTION VERB
MultiRel 72.54∗◦ 53.02 44.80
CK 62.98 52.91 43.88
CK + inducWiegand 2012 65.15 57.33∗ 50.83∗
CK + inducgraph 66.06∗ 65.03∗◦ 60.91∗◦
CK + coarse-grain lex 66.82∗ 64.13∗◦ 63.72∗◦†
CK + fine-grain lex 66.16∗ 64.98∗◦ 70.85∗◦†‡
statistical significance testing (permutation test, significance level p &lt; 0.05)
∗: better than CK; ◦: better than CK + inducWiegand 2012 ; †: better than
CK + inducgraph; ‡: better than CK + coarse-grain lex
</table>
<tableCaption confidence="0.9045485">
Table 14: Evaluation on opinion holder extraction
on various corpora (evaluation measure: F-score).
</tableCaption>
<bodyText confidence="0.998971891891892">
traction. We replicate several classifiers and com-
pare them to our new approach. (Because of the
limited space of this paper, we cannot also address
cross-domain opinion target extraction.) We con-
sider three different corpora as shown in Table 13.
MPQA (Wiebe et al., 2005) is the standard corpus
for fine-grained sentiment analysis. FICTION,
introduced in Wiegand and Klakow (2012), is a
collection of summaries of classic literary works.
VERB is the new corpus used in the previous
evaluation (§5.2). VERB and MPQA both orig-
inate from the news domain but VERB is sam-
pled in such a way that mentions of all opinion
verbs of the Subjectivity Lexicon are represented.
The other corpora consist of contiguous sentences.
They will have a bias towards only those opinion
verbs frequently occurring in that particular do-
main. This also results in different distributions
of verb types as shown in Table 13. For example,
SP-verbs are rare in MPQA. However, there ex-
ist plenty of them (Table 3). Other domains may
have much more frequent SP-verbs (just as FIC-
TION has more PT-verbs than MPQA). A robust
domain-independent classifier should therefore be
able to cope equally well with all three verb types.
MPQA is also the largest corpus. Following
Wiegand and Klakow (2012), this corpus is cho-
sen as a training set.7 Despite its size, however,
almost every second opinion verb from our set of
opinion verbs is not contained in that corpus.
In the evaluation, we only consider the opinion
holders of our opinion verbs. (Other opinion hold-
ers, both in the gold standard and the predictions
of the classifiers are ignored.) Recall that we take
the knowledge of what is an opinion verb as given.
Our graph-based induction can be arbitrarily ex-
tended by increasing the set of opinion verbs.
</bodyText>
<footnote confidence="0.9880155">
7The split-up of training and test set on the MPQA corpus
follows the specification of Johansson and Moschitti (2013).
</footnote>
<page confidence="0.996219">
222
</page>
<bodyText confidence="0.999985325">
For classifiers, we consider convolution ker-
nels CK from Wiegand and Klakow (2012) and
the sequence labeler from Johansson and Mos-
chitti (2013) MultiRel that incorporates relational
features taking into account interactions between
multiple opinion cues. It is currently the most so-
phisticated opinion holder extractor. CK can be
combined with additional knowledge. We com-
pare inducgraph with inducWiegand 2012, which
employs the word lists induced for AG- and PT-
verbs in the fashion of Wiegand and Klakow
(2012), i.e. without graph clustering. As an upper
bound for the induction methods, coarse grain lex
and fine grain lex are used.8 The combination
of CK with this additional knowledge follows the
best settings from Wiegand and Klakow (2012).9
Table 14 shows the results. MultiRel produces
the best performance on MPQA but suffers simi-
larly from a domain-mismatch as CK on FICTION
and VERB. MultiRel and CK cannot handle many
PT- and SP-verbs in those corpora, simply because
many of them do not occur in MPQA. On MPQA,
only the new induction approach and the lexicons
significantly improve CK. The knowledge of opin-
ion roles has a lower impact on MPQA. In that cor-
pus, most opinion verbs take their opinion holder
as an agent. Given the large size of MPQA, this
information can be easily learned from the train-
ing data. The situation is different for FICTION
and VERB where the knowledge from induction
largely improves classification. In these corpora,
opinion holders as agents are much less frequent
than on MPQA. The new induction proposed in
this paper also notably outperforms the induction
from Wiegand and Klakow (2012).
Although the fine-grained lexicon is among the
top performing systems, we only note large im-
provements on VERB. VERB has the highest pro-
portion of PT- and SP-verbs (Table 13). Knowl-
edge about role-assignment is most critical here.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9702598">
Most approaches for opinion role extraction em-
ploy supervised learning. The feature design is
8Wiegand and Klakow (2012) use a lexicon Lex which
just comprises the notion of AG and PT verbs, so our manual
lexicons are more accurate and harder to beat.
9For in-domain evaluation (i.e. MPQA) the trees (tree
structures are the input to CK) are augmented with verb cat-
egory information. For out-of-domain evaluation (i.e. FIC-
TION and VERB), we add to the predictions of CK the pre-
diction of a rule-based classifier using the opinion role assign-
ment according to the respective lexicon or induction method.
mainly inspired by semantic role labeling (Bethard
et al., 2004; Li et al., 2012). Some work also em-
ploys information from existing semantic role la-
belers based on FrameNet (Kim and Hovy, 2006)
or PropBank (Johansson and Moschitti, 2013;
Wiegand and Klakow, 2012). Although those re-
sources give extra information for opinion role ex-
traction in comparison to syntactic or other surface
features, we showed in this work that further task-
specific knowledge, i.e. either opinion verb types
or a manually-built opinion role lexicon, provide
even more accurate information.
There has been a substantial amount of research
on opinion target extraction. It focuses, however,
on the extraction of topic-specific opinion terms
(Jijkoun et al., 2010; Qiu et al., 2011) rather than
the variability of semantic roles for opinion hold-
ers and targets. Mitchell et al. (2013) present
a low-resource approach for target extraction but
their aim is to process Twitter messages without
using general syntax tools. In this work, we use
such tools. Our notion of low resources is different
in that we mean the absence of semantic resources
helpful for our task (e.g. FrameNet).
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99996075">
We presented an approach for opinion role in-
duction for verbal predicates. We assume that
those predicates can be divided into three differ-
ent verb types where each type is associated with
a characteristic mapping between semantic roles
and opinion holders and targets. In several ex-
periments, we demonstrated the relevance of those
three types. We showed that verbs can effectively
be categorized with graph clustering given a suit-
able similarity metric. The seeds are automatically
selected. Our proposed induction approach out-
performs both a previous induction approach and
features derived from semantic role labelers. We
also pointed out the importance of the knowledge
gained by induction in supervised cross-domain
classification.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.975344857142857">
The authors would like to thank Stephanie K¨oser for annotat-
ing parts of the resources presented in this paper. For proof-
reading the paper, the authors would also like to thank Ines
Rehbein, Asad Sayeed and Marc Schulder. We also thank
Ashutosh Modi for advising us on word embeddings. The au-
thors were partially supported by the German Research Foun-
dation (DFG) under grants RU 1873/2-1 and WI 4204/2-1.
</bodyText>
<page confidence="0.998286">
223
</page>
<sectionHeader confidence="0.99011" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999877411764705">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING/ACL.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of ACL.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Extract-
ing Opinion Propositions and Opinion Holders us-
ing Syntactic and Lexical Cues. In Computing At-
titude and Affect in Text: Theory and Applications.
Springer-Verlag.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual Semantic Role Labeling. In Pro-
ceedings of the CoNLL – Shared Task.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying Sources of Opinions
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of HLT/EMNLP.
Thomas Cover and Peter Hart. 1967. Nearest
neighbor pattern classification. Information Theory,
13(1):21–27.
Dipanjan Das and Noah A. Smith. 2011. Semi-
Supervised Frame-Semantic Parsing for Unknown
Predicates. In Proceedings of ACL.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In Proceedings of HLT/NAACL.
Dipanjan Das, Andr´e F.T. Martins, and Noah A. Smith.
2012. An Exact Dual Decomposition Algorithm for
Shallow Semantic Parsing with Constraints. In Pro-
ceedings of *SEM.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
Propagation via Implicature Constraints. In Pro-
ceedings of EACL.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings ofACL.
Amit Goyal, Ellen Riloff, and Hal Daume III. 2010.
Automatically Producing Plot Unit Representations
for Narrative Text. In Proceedings of EMNLP.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of EACL.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
Opinion Targets in a Single- and Cross-Domain Set-
ting with Conditional Random Fields. In Proceed-
ings of EMNLP.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating Focused Topic-
Specific Sentiment Lexicons. In Proceedings of
ACL.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Sch¨olkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 169–184. MIT
Press.
Richard Johansson and Alessandro Moschitti. 2013.
Relational Features in Fine-Grained Opinion Analy-
sis. Computational Linguistics, 39(3):473–509.
Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin
Choi. 2014. ConnotationWordNet: Learning Con-
notation over the Word+Sense Network. In Pro-
ceedings of ACL.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
Opinions, Opinion Holders, and Topics Expressed
in Online News Media Text. In Proceedings of ACL
Workshop on Sentiment and Subjectivity in Text.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of LREC.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings ofACL.
Manfred Klenner, Angela Fahrni, and Stefanos Pe-
trakis. 2009. PolArt: A Robust Tool for Sentiment
Analysis. In Proceedings of NoDaLiDa.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159–174.
Shoushan Li, Rongyang Wang, and Guodong Zhou.
2012. Opinion Target Extraction via Shallow Se-
mantic Parsing. In Proceedings of AAAI.
Dekang Lin. 1998. Automatic Retrieval and Clus-
tering of Similar Words. In Proceedings of
ACL/COLING.
Isa Maks and Piek Vossen. 2012. A lexicon model for
deep sentiment analysis and opinion mining applica-
tions. Decision Support Systems, 53:680–688.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at ICLR.
</reference>
<page confidence="0.980757">
224
</page>
<reference confidence="0.999927322580645">
George Miller, Richard Beckwith, Christine Fellbaum,
Derek Gross, and Katherine Miller. 1990. Intro-
duction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3:235–244.
Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open
Domain Targeted Sentiment. In Proceedings of
EMNLP.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity – Measuring
the Relatedness of Concepts. In Proceedings of
HLT/NAACL.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion Word Expansion and Target Extrac-
tion through Double Propagation. Computational
Linguistics, 37(1):9–27.
Delip Rao and Deepak Ravichandran. 2009. Semi-
Supervised Polarity Lexicon Induction. In Proceed-
ings of EACL.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons.
In Proceedings of EMNLP.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semi-automatic
semantic lexicon construction. In Proceedings of
COLING.
Josef Ruppenhofer and Ines Rehbein. 2012. Seman-
tic frames as an anchor representation for sentiment
analysis. In Proceedings of WASSA.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the Source and Targets of
Subjective Expressions. In Proceedings of LREC.
Rico Sennrich, Gerold Schneider, Martin Volk, and
Martin Warin. 2009. A New Hybrid Dependency
Parser for German. In Proceedings of GSCL.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-Supervised Acqui-
sition of Labeled Class Instances using Graph Ran-
dom Walks. In Proceedings of EMNLP.
Janyce Wiebe and Rada Mihalcea. 2006. Word Sense
and Subjectivity. In Proceedings of COLING/ACL.
Janyce Wiebe, Therea Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation, 39(2/3):164–210.
Michael Wiegand and Dietrich Klakow. 2012. Gener-
alization Methods for In-Domain and Cross-Domain
Opinion Holder Extraction. In Proceedings of
EACL.
Thesesa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of
HLT/EMNLP.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexcial selection. In Proceedings of ACL.
Bishan Yang and Claire Cardie. 2013. Joint Inference
for Fine-grained Opinion Extraction. In Proceed-
ings of ACL.
Patrick Ziering, Lonneke van der Plas, and Hinrich
Schuetze. 2013. Bootstrapping Semantic Lexicons
for Technical Domains. In Proceedings of IJCNLP.
</reference>
<page confidence="0.99882">
225
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.669808">
<title confidence="0.9963215">Opinion Holder and Target Extraction based on the Induction of Verbal Categories</title>
<author confidence="0.985291">Michael</author>
<affiliation confidence="0.853857">Spoken Language Saarland</affiliation>
<address confidence="0.98953">D-66123 Saarbr¨ucken, Germany</address>
<email confidence="0.999355">michael.wiegand@lsv.uni-saarland.de</email>
<abstract confidence="0.997538444444444">We present an approach for opinion role induction for verbal predicates. Our model rests on the assumption that opinion verbs can be divided into three different types where each type is associated with a characteristic mapping between semantic roles and opinion holders and targets. In several experiments, we demonstrate the relevance of those three categories for the task. We show that verbs can easily be categorized with semi-supervised graphbased clustering and some appropriate similarity metric. The seeds are obtained through linguistic diagnostics. We evaluate our approach against a new manuallycompiled opinion role lexicon and perform in-context classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="1391" citStr="Baker et al., 1998" startWordPosition="200" endWordPosition="203">cs. We evaluate our approach against a new manuallycompiled opinion role lexicon and perform in-context classification. 1 Introduction While there has been much research in sentiment analysis on subjectivity detection and polarity classification, there has been less work on the extraction of opinion roles, i.e. entities that express an opinion (opinion holders), and entities or propositions at which sentiment is directed (opinion targets). Previous research relies on large amounts of labeled training data or leverages general semantic resources which are expensive to construct, e.g. FrameNet (Baker et al., 1998). In this paper, we present an approach to induce opinion roles of verbal predicates. The input is a set of opinion verbs that can be found in a common sentiment lexicon. Our model rests on the assumption that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. Josef Ruppenhofer Dept. of Information Science and Language Technology Hildesheim University D-31141 Hildesheim, Germany ruppenho@uni-hildes</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17388" citStr="Baroni et al. (2014)" startWordPosition="2777" endWordPosition="2780">sts for each of the verb types the 12 seeds most frequently occurring with the respective patterns. We observed that the SP-verb seeds are exclusively negative polar expressions. That is why we also extracted seeds from an additional pattern help to XVB producing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtaine</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Hong Yu</author>
<author>Ashley Thornton</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Dan Jurafsky</author>
</authors>
<title>Extracting Opinion Propositions and Opinion Holders using Syntactic and Lexical Cues. In Computing Attitude and Affect in Text: Theory and Applications.</title>
<date>2004</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="30769" citStr="Bethard et al., 2004" startWordPosition="4867" endWordPosition="4870">better than +srlframenet; †: better than +inducgraph Table 11: In-context evaluation (eval.: F-score). al., 2005) and WordNet (Miller et al., 1990). For semantic role labeling of FrameNetstructures (srlframenet), we used Semafor (Das et al., 2010) with the argument identification based on dual decomposition (Das et al., 2012). We run the configuration that also assigns frame structures to unknown predicates (Das and Smith, 2011). This is necessary as 45% of our opinion verbs are not contained in FrameNet (v1.5). FrameNet has been shown to enable a correct role assignment for AG- and PT-verbs (Bethard et al., 2004; Kim and Hovy, 2006). For instance, in (17) and (18), the opinion holder is assigned to the same frame element EXPERIENCER. However, the PropBank representation does not produce a correct alignment: In (17), the opinon holder is the agent of the opinion verb, while in (18), the opinion holder is the patient of the opinion verb. (17) PeterEXPERIENCER dislikes M agent ypatient. (dislike, Peter, holder) EXPERIENCER (18) Peteragent disappoints Marypatient . (disappoint, Mary, holder) With the feature fine-grain lex, we want to validate that our manually-compiled opinion role lexicon for verbs (§2</context>
<context position="39453" citStr="Bethard et al., 2004" startWordPosition="6258" endWordPosition="6261">ploy supervised learning. The feature design is 8Wiegand and Klakow (2012) use a lexicon Lex which just comprises the notion of AG and PT verbs, so our manual lexicons are more accurate and harder to beat. 9For in-domain evaluation (i.e. MPQA) the trees (tree structures are the input to CK) are augmented with verb category information. For out-of-domain evaluation (i.e. FICTION and VERB), we add to the predictions of CK the prediction of a rule-based classifier using the opinion role assignment according to the respective lexicon or induction method. mainly inspired by semantic role labeling (Bethard et al., 2004; Li et al., 2012). Some work also employs information from existing semantic role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, howev</context>
</contexts>
<marker>Bethard, Yu, Thornton, Hatzivassiloglou, Jurafsky, 2004</marker>
<rawString>Steven Bethard, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky. 2004. Extracting Opinion Propositions and Opinion Holders using Syntactic and Lexical Cues. In Computing Attitude and Affect in Text: Theory and Applications. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual Semantic Role Labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the CoNLL – Shared Task.</booktitle>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual Semantic Role Labeling. In Proceedings of the CoNLL – Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="29363" citStr="Choi et al., 2005" startWordPosition="4667" endWordPosition="4670">the mention of an opinion verb and const is any possible (syntactic) constituent in the respective sentence. The dataset contains 753 holders, 745 targets and 499 targets of a speaker view. Since a constituent may have several roles at the same time, we train three binary classifiers for either of the entity types. On a sample of 200 sentences, we measured an interannotation agreement of Cohen’s r. = 0.69 for holders, r. = 0.63 for targets and also r. = 0.63 for targets of a speaker view. Table 10 shows the features used in our supervised classifier. They have been previously found effective (Choi et al., 2005; Jakob and Gurevych, 2010; Wiegand and Klakow, 2012; Yang and Cardie, 2013). The standard features are the features from cand word to brown. For semantic role labeling of PropBank-structures, we used matetools (Bj¨orkelund et al., 2009). For person detection, we employ named-entity tagging (Finkel et Features Holder Target TargetSpeaker standard 63.59 54.18 40.06 +srlframenet 65.44∗ 55.70∗ 42.14 +inducgraph 68.06∗◦ 59.61∗◦ 46.66∗◦ +srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦ +coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Cover</author>
<author>Peter Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<journal>Information Theory,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="19663" citStr="Cover and Hart, 1967" startWordPosition="3125" endWordPosition="3128">ask of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: r(r,w)∈T (w1)∩T (w2)(I(w1 ,r,w)+I(w2,r,w)) sim(w1, w2) = r-(r,w)∈T (w1) I(w1,r,w)+r-(r,w)∈T (w2) I(w2,r,w) where I(w, r, w′) = log kw,r,w′k×k∗,r,∗k kw,r,∗k×k∗,r,w′k and T (w) is defined as the set of pairs (r, w′) such that log kw,r,w′k×k∗,r,∗k &gt; 0. kw,r,∗k×k∗,r,w k 4.3 Propagation Methods We use the k nearest neighbour classifier (kNN) (Cover and Hart, 1967) as a simple method for propagating labels from seeds to other instances. Alternatively, we consider verb categorization as a clustering task on a graph G = (V, E, W) where V is the set of nodes (i.e. our opinion verbs), E is the set of edges connecting them with weights W : E → R+. W can be directly derived from any of the similarity metrics (§4.2.1-§4.2.4). The aim is that all nodes v E V are assigned a label l E {AG, PT, SP}. Initially, only the verb seeds are labeled. We then use the Adsorption label propagation algorithm from junto (Talukdar et al., 2008) in order propagate the labels fro</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>Thomas Cover and Peter Hart. 1967. Nearest neighbor pattern classification. Information Theory, 13(1):21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>SemiSupervised Frame-Semantic Parsing for Unknown Predicates.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19129" citStr="Das and Smith, 2011" startWordPosition="3038" endWordPosition="3041">e dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: r(r,w)∈T (w1)∩T (w2)(I(w1 ,r,w)+I(w2,r,w)) sim(w1, w2) = r-(r,w)∈T (w1) I(w1,r,w)+r-(r,w)∈T (w2) I(w2,r,w) where I(w, r, w′) = log kw,r,w′k×k∗,r,∗k kw,r,∗k×k∗,r,w′k and T (w) is defined as the set of pairs (r, w′) such that log kw,r,w′k×k∗,r,∗k &gt; 0. kw,r,∗k×k∗,r,w k 4.3 Propagation Methods We use the k nearest neighbour classifier (kNN) (Cover and Hart, 1967) as a simple method for propagating labels from seeds to other ins</context>
<context position="30581" citStr="Das and Smith, 2011" startWordPosition="4834" endWordPosition="4837">69.83∗◦† 62.89∗◦† 56.71∗◦† +srlframenet+fine-grain lex 70.80∗◦† 63.72∗◦† 56.64∗◦† statistical significance testing (paired t-test, significance level p &lt; 0.05) ∗: better than standard; ◦: better than +srlframenet; †: better than +inducgraph Table 11: In-context evaluation (eval.: F-score). al., 2005) and WordNet (Miller et al., 1990). For semantic role labeling of FrameNetstructures (srlframenet), we used Semafor (Das et al., 2010) with the argument identification based on dual decomposition (Das et al., 2012). We run the configuration that also assigns frame structures to unknown predicates (Das and Smith, 2011). This is necessary as 45% of our opinion verbs are not contained in FrameNet (v1.5). FrameNet has been shown to enable a correct role assignment for AG- and PT-verbs (Bethard et al., 2004; Kim and Hovy, 2006). For instance, in (17) and (18), the opinion holder is assigned to the same frame element EXPERIENCER. However, the PropBank representation does not produce a correct alignment: In (17), the opinon holder is the agent of the opinion verb, while in (18), the opinion holder is the patient of the opinion verb. (17) PeterEXPERIENCER dislikes M agent ypatient. (dislike, Peter, holder) EXPERIE</context>
</contexts>
<marker>Das, Smith, 2011</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2011. SemiSupervised Frame-Semantic Parsing for Unknown Predicates. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic FrameSemantic Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="30396" citStr="Das et al., 2010" startWordPosition="4806" endWordPosition="4809">6∗◦ 59.61∗◦ 46.66∗◦ +srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦ +coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦† +srlframenet+fine-grain lex 70.80∗◦† 63.72∗◦† 56.64∗◦† statistical significance testing (paired t-test, significance level p &lt; 0.05) ∗: better than standard; ◦: better than +srlframenet; †: better than +inducgraph Table 11: In-context evaluation (eval.: F-score). al., 2005) and WordNet (Miller et al., 1990). For semantic role labeling of FrameNetstructures (srlframenet), we used Semafor (Das et al., 2010) with the argument identification based on dual decomposition (Das et al., 2012). We run the configuration that also assigns frame structures to unknown predicates (Das and Smith, 2011). This is necessary as 45% of our opinion verbs are not contained in FrameNet (v1.5). FrameNet has been shown to enable a correct role assignment for AG- and PT-verbs (Bethard et al., 2004; Kim and Hovy, 2006). For instance, in (17) and (18), the opinion holder is assigned to the same frame element EXPERIENCER. However, the PropBank representation does not produce a correct alignment: In (17), the opinon holder </context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic FrameSemantic Parsing. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM.</booktitle>
<contexts>
<context position="30476" citStr="Das et al., 2012" startWordPosition="4818" endWordPosition="4821">n lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦† +srlframenet+fine-grain lex 70.80∗◦† 63.72∗◦† 56.64∗◦† statistical significance testing (paired t-test, significance level p &lt; 0.05) ∗: better than standard; ◦: better than +srlframenet; †: better than +inducgraph Table 11: In-context evaluation (eval.: F-score). al., 2005) and WordNet (Miller et al., 1990). For semantic role labeling of FrameNetstructures (srlframenet), we used Semafor (Das et al., 2010) with the argument identification based on dual decomposition (Das et al., 2012). We run the configuration that also assigns frame structures to unknown predicates (Das and Smith, 2011). This is necessary as 45% of our opinion verbs are not contained in FrameNet (v1.5). FrameNet has been shown to enable a correct role assignment for AG- and PT-verbs (Bethard et al., 2004; Kim and Hovy, 2006). For instance, in (17) and (18), the opinion holder is assigned to the same frame element EXPERIENCER. However, the PropBank representation does not produce a correct alignment: In (17), the opinon holder is the agent of the opinion verb, while in (18), the opinion holder is the patie</context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>Dipanjan Das, Andr´e F.T. Martins, and Noah A. Smith. 2012. An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints. In Proceedings of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Sentiment Propagation via Implicature Constraints.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="6983" citStr="Deng and Wiebe, 2014" startWordPosition="1073" endWordPosition="1076">ork, [they]1 are constantly gossiping. (gossip, speaker, holder) &amp; (gossip, [1], target) 1By agent and patient, we mean constituents labeled as AO and Al in PropBank (Kingsbury and Palmer, 2002). Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s n = 60.8 for opinion holders, n = 62.3 for opinion targets and n = 59.9 for speaker views. This agreement is mostly substantial (Landis and Koch, 1977). 3 The Three Verb Categories Rather than induce the opinion roles for individual verbs, we group verbs that share similar opinion role subcategorization. Thus, the main task for induction is to decide which type an opinion verb belo</context>
</contexts>
<marker>Deng, Wiebe, 2014</marker>
<rawString>Lingjia Deng and Janyce Wiebe. 2014. Sentiment Propagation via Implicature Constraints. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="17891" citStr="Esuli and Sebastiani, 2006" startWordPosition="2849" endWordPosition="2852">e word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al.</context>
<context position="23855" citStr="Esuli and Sebastiani (2006)" startWordPosition="3807" endWordPosition="3810">applied (no graph) vs. best proposed induction method from Table 5 (graph). method suffers from data-sparsity. In our corpus, the frequency of verbs co-occurring with outrage in a conjunction is 5 or lower.5 The table also shows that WordNet may not be appropriate for our present verb categorization task. However, it may be suitable for other subtasks in sentiment analysis, particularly polarity classification. If we consider the similar entries of outrage provided by that metric, we find that polarity is largely preserved (10 out of 12 verbs are negative). This observation is consistent with Esuli and Sebastiani (2006) and Rao and Ravichandran (2009). In Table 5 we only used the top 40/30/50 verbs from the initialization methods as seeds. We can also compare the output of these methods (combined with propagation, i.e. graph clustering) with the entire verb lists produced by these patternbased initialization methods where no propagation is applied. As far as AG- and PT-verbs are concerned, the entire lists of these initialization methods correspond to the original approach of Wiegand and Klakow (2012). Table 7 shows the result. The new graph-induction always outperforms the original induction method by a lar</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Ellen Riloff</author>
<author>Hal Daume</author>
</authors>
<title>Automatically Producing Plot Unit Representations for Narrative Text.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="18453" citStr="Goyal et al., 2010" startWordPosition="2933" endWordPosition="2936">e for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of exte</context>
</contexts>
<marker>Goyal, Riloff, Daume, 2010</marker>
<rawString>Amit Goyal, Ellen Riloff, and Hal Daume III. 2010. Automatically Producing Plot Unit Representations for Narrative Text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet -a Lexical-Semantic Net for German.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.</booktitle>
<contexts>
<context position="26099" citStr="Hamp and Feldweg, 1997" startWordPosition="4169" endWordPosition="4172">igurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6. As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our proposed categorization of verbs into three types for opinion role extraction. 5.2 In-Context Evaluation We now evaluate our induced knowledge in the task of extracting opinion holders and targets from actual text. For this in-context evaluation, we sampled sentences from the North </context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -a Lexical-Semantic Net for German. In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="18391" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="2923" endWordPosition="2926">et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreo</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the Semantic Orientation of Adjectives. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting Opinion Targets in a Single- and Cross-Domain Setting with Conditional Random Fields.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="29389" citStr="Jakob and Gurevych, 2010" startWordPosition="4671" endWordPosition="4674">pinion verb and const is any possible (syntactic) constituent in the respective sentence. The dataset contains 753 holders, 745 targets and 499 targets of a speaker view. Since a constituent may have several roles at the same time, we train three binary classifiers for either of the entity types. On a sample of 200 sentences, we measured an interannotation agreement of Cohen’s r. = 0.69 for holders, r. = 0.63 for targets and also r. = 0.63 for targets of a speaker view. Table 10 shows the features used in our supervised classifier. They have been previously found effective (Choi et al., 2005; Jakob and Gurevych, 2010; Wiegand and Klakow, 2012; Yang and Cardie, 2013). The standard features are the features from cand word to brown. For semantic role labeling of PropBank-structures, we used matetools (Bj¨orkelund et al., 2009). For person detection, we employ named-entity tagging (Finkel et Features Holder Target TargetSpeaker standard 63.59 54.18 40.06 +srlframenet 65.44∗ 55.70∗ 42.14 +inducgraph 68.06∗◦ 59.61∗◦ 46.66∗◦ +srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦ +coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦† +</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Extracting Opinion Targets in a Single- and Cross-Domain Setting with Conditional Random Fields. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
<author>Wouter Weerkamp</author>
</authors>
<title>Generating Focused TopicSpecific Sentiment Lexicons.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Jijkoun, de Rijke, Weerkamp, 2010</marker>
<rawString>Valentin Jijkoun, Maarten de Rijke, and Wouter Weerkamp. 2010. Generating Focused TopicSpecific Sentiment Lexicons. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="28449" citStr="Joachims, 1999" startWordPosition="4509" endWordPosition="4510">ngs) fine-grain lex is candidate holder/target/targetspeaker according to the fine-grained lexicon coarse-grain lex is candidate holder/target/targetspeaker according to the coarse-grained lexicon inducgraph is candidate holder/target/targetspeaker according to the coarse-grained lexicon automatically induced with graph clustering (and induced seeds (§4.1)) Table 10: Feature set for in-context classification. widely represented and both holders and targets are annotated. We solve this task with supervised learning. As a classifier, we employ Support Vector Machines as implemented in SVMlight (Joachims, 1999). The task is to extract three different entities: opinion holders, opinion targets and opinion targets evoked by speaker views. All those entities are always put into the relation to a specific opinion verb in the sentence. The instance space thus consists of tuples (verb, const), where verb is the mention of an opinion verb and const is any possible (syntactic) constituent in the respective sentence. The dataset contains 753 holders, 745 targets and 499 targets of a speaker view. Since a constituent may have several roles at the same time, we train three binary classifiers for either of the </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational Features in Fine-Grained Opinion Analysis.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="36900" citStr="Johansson and Moschitti (2013)" startWordPosition="5839" endWordPosition="5842">, this corpus is chosen as a training set.7 Despite its size, however, almost every second opinion verb from our set of opinion verbs is not contained in that corpus. In the evaluation, we only consider the opinion holders of our opinion verbs. (Other opinion holders, both in the gold standard and the predictions of the classifiers are ignored.) Recall that we take the knowledge of what is an opinion verb as given. Our graph-based induction can be arbitrarily extended by increasing the set of opinion verbs. 7The split-up of training and test set on the MPQA corpus follows the specification of Johansson and Moschitti (2013). 222 For classifiers, we consider convolution kernels CK from Wiegand and Klakow (2012) and the sequence labeler from Johansson and Moschitti (2013) MultiRel that incorporates relational features taking into account interactions between multiple opinion cues. It is currently the most sophisticated opinion holder extractor. CK can be combined with additional knowledge. We compare inducgraph with inducWiegand 2012, which employs the word lists induced for AG- and PTverbs in the fashion of Wiegand and Klakow (2012), i.e. without graph clustering. As an upper bound for the induction methods, coar</context>
<context position="39626" citStr="Johansson and Moschitti, 2013" startWordPosition="6287" endWordPosition="6290">ons are more accurate and harder to beat. 9For in-domain evaluation (i.e. MPQA) the trees (tree structures are the input to CK) are augmented with verb category information. For out-of-domain evaluation (i.e. FICTION and VERB), we add to the predictions of CK the prediction of a rule-based classifier using the opinion role assignment according to the respective lexicon or induction method. mainly inspired by semantic role labeling (Bethard et al., 2004; Li et al., 2012). Some work also employs information from existing semantic role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of semantic roles for opinion holders and targets.</context>
</contexts>
<marker>Johansson, Moschitti, 2013</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2013. Relational Features in Fine-Grained Opinion Analysis. Computational Linguistics, 39(3):473–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Seok Kang</author>
<author>Song Feng</author>
<author>Leman Akoglu</author>
<author>Yejin Choi</author>
</authors>
<title>ConnotationWordNet: Learning Connotation over the Word+Sense Network.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="18498" citStr="Kang et al., 2014" startWordPosition="2941" endWordPosition="2944">tiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semanti</context>
</contexts>
<marker>Kang, Feng, Akoglu, Choi, 2014</marker>
<rawString>Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin Choi. 2014. ConnotationWordNet: Learning Connotation over the Word+Sense Network. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="30790" citStr="Kim and Hovy, 2006" startWordPosition="4871" endWordPosition="4874">et; †: better than +inducgraph Table 11: In-context evaluation (eval.: F-score). al., 2005) and WordNet (Miller et al., 1990). For semantic role labeling of FrameNetstructures (srlframenet), we used Semafor (Das et al., 2010) with the argument identification based on dual decomposition (Das et al., 2012). We run the configuration that also assigns frame structures to unknown predicates (Das and Smith, 2011). This is necessary as 45% of our opinion verbs are not contained in FrameNet (v1.5). FrameNet has been shown to enable a correct role assignment for AG- and PT-verbs (Bethard et al., 2004; Kim and Hovy, 2006). For instance, in (17) and (18), the opinion holder is assigned to the same frame element EXPERIENCER. However, the PropBank representation does not produce a correct alignment: In (17), the opinon holder is the agent of the opinion verb, while in (18), the opinion holder is the patient of the opinion verb. (17) PeterEXPERIENCER dislikes M agent ypatient. (dislike, Peter, holder) EXPERIENCER (18) Peteragent disappoints Marypatient . (disappoint, Mary, holder) With the feature fine-grain lex, we want to validate that our manually-compiled opinion role lexicon for verbs (§2), i.e. the lexicon t</context>
<context position="39583" citStr="Kim and Hovy, 2006" startWordPosition="6281" endWordPosition="6284">and PT verbs, so our manual lexicons are more accurate and harder to beat. 9For in-domain evaluation (i.e. MPQA) the trees (tree structures are the input to CK) are augmented with verb category information. For out-of-domain evaluation (i.e. FICTION and VERB), we add to the predictions of CK the prediction of a rule-based classifier using the opinion role assignment according to the respective lexicon or induction method. mainly inspired by semantic role labeling (Bethard et al., 2004; Li et al., 2012). Some work also employs information from existing semantic role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of sema</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text. In Proceedings of ACL Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From TreeBank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="6556" citStr="Kingsbury and Palmer, 2002" startWordPosition="1007" endWordPosition="1010">nion targets). Our lexicon also includes another dimension neglected in many previous works. Many opinion verbs predominantly express the sentiment of the speaker of the utterance (or some nested source) (4). This concept is also known as expressive subjectivity (Wiebe et al., 2005) or speaker subjectivity (Maks and Vossen, 2012). In such opinions, the opinion holder is not realized as a dependent of the opinion verb. (4) At my work, [they]1 are constantly gossiping. (gossip, speaker, holder) &amp; (gossip, [1], target) 1By agent and patient, we mean constituents labeled as AO and Al in PropBank (Kingsbury and Palmer, 2002). Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measur</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From TreeBank to PropBank. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="18577" citStr="Klein and Manning, 2003" startWordPosition="2953" endWordPosition="2956">od to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Klenner</author>
<author>Angela Fahrni</author>
<author>Stefanos Petrakis</author>
</authors>
<title>PolArt: A Robust Tool for Sentiment Analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of NoDaLiDa.</booktitle>
<contexts>
<context position="25819" citStr="Klenner et al., 2009" startWordPosition="4119" endWordPosition="4122">those verbs by frequency. Size and verb type distribution are preserved. We also examine what impact doubling the size of seeds (Gold|Patterndouble) and halving them (GoldlPatternhalf ) has on classification. Dependency-based similarity and graph clustering is used for all configurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6. As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our pro</context>
</contexts>
<marker>Klenner, Fahrni, Petrakis, 2009</marker>
<rawString>Manfred Klenner, Angela Fahrni, and Stefanos Petrakis. 2009. PolArt: A Robust Tool for Sentiment Analysis. In Proceedings of NoDaLiDa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="7350" citStr="Landis and Koch, 1977" startWordPosition="1135" endWordPosition="1138">ngsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s n = 60.8 for opinion holders, n = 62.3 for opinion targets and n = 59.9 for speaker views. This agreement is mostly substantial (Landis and Koch, 1977). 3 The Three Verb Categories Rather than induce the opinion roles for individual verbs, we group verbs that share similar opinion role subcategorization. Thus, the main task for induction is to decide which type an opinion verb belongs to. Once the verb type has been established, the typical semantic roles for opinion holders and targets can be derived from that type. The verb categorization is motivated by the semantic roles of the three common views (Table 1) that an opinion holder can take. In our lexicon, all of the opinion holders were observed with either of these semantic roles. For fa</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Rongyang Wang</author>
<author>Guodong Zhou</author>
</authors>
<title>Opinion Target Extraction via Shallow Semantic Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="39471" citStr="Li et al., 2012" startWordPosition="6262" endWordPosition="6265">ng. The feature design is 8Wiegand and Klakow (2012) use a lexicon Lex which just comprises the notion of AG and PT verbs, so our manual lexicons are more accurate and harder to beat. 9For in-domain evaluation (i.e. MPQA) the trees (tree structures are the input to CK) are augmented with verb category information. For out-of-domain evaluation (i.e. FICTION and VERB), we add to the predictions of CK the prediction of a rule-based classifier using the opinion role assignment according to the respective lexicon or induction method. mainly inspired by semantic role labeling (Bethard et al., 2004; Li et al., 2012). Some work also employs information from existing semantic role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extract</context>
</contexts>
<marker>Li, Wang, Zhou, 2012</marker>
<rawString>Shoushan Li, Rongyang Wang, and Guodong Zhou. 2012. Opinion Target Extraction via Shallow Semantic Parsing. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/COLING.</booktitle>
<contexts>
<context position="18888" citStr="Lin (1998)" startWordPosition="3002" endWordPosition="3003"> to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: r(r,w)∈T (w1)∩T (w2)(I(w1 ,r,w)+I(w2,r,w)) sim(w1, w2) = r-(r,w)∈T (w1) I(w1,r,w)+r-(r,w)∈T (w2) I(w2,r,w) where I(w, r, w′) = log kw,r,w′k×k∗,r,∗k kw,r,∗k×k∗,r,w′k and T (w) is defined </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isa Maks</author>
<author>Piek Vossen</author>
</authors>
<title>A lexicon model for deep sentiment analysis and opinion mining applications. Decision Support Systems,</title>
<date>2012</date>
<pages>53--680</pages>
<contexts>
<context position="6260" citStr="Maks and Vossen, 2012" startWordPosition="958" endWordPosition="961">he role assignments: • view1: (persuade, [1], holder), (persuade, [2], target) • view2: (persuade, [2], holder), (persuade, [1], target) • view3: (persuade, [1], holder), (persuade, [3], target) • view4: (persuade, [2], holder), (persuade, [3], target) (in short: 2 opinion holders and 3 opinion targets). Our lexicon also includes another dimension neglected in many previous works. Many opinion verbs predominantly express the sentiment of the speaker of the utterance (or some nested source) (4). This concept is also known as expressive subjectivity (Wiebe et al., 2005) or speaker subjectivity (Maks and Vossen, 2012). In such opinions, the opinion holder is not realized as a dependent of the opinion verb. (4) At my work, [they]1 are constantly gossiping. (gossip, speaker, holder) &amp; (gossip, [1], target) 1By agent and patient, we mean constituents labeled as AO and Al in PropBank (Kingsbury and Palmer, 2002). Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a ver</context>
</contexts>
<marker>Maks, Vossen, 2012</marker>
<rawString>Isa Maks and Piek Vossen. 2012. A lexicon model for deep sentiment analysis and opinion mining applications. Decision Support Systems, 53:680–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="17366" citStr="Mikolov et al., 2013" startWordPosition="2773" endWordPosition="2776">limate data. Table 4 lists for each of the verb types the 12 seeds most frequently occurring with the respective patterns. We observed that the SP-verb seeds are exclusively negative polar expressions. That is why we also extracted seeds from an additional pattern help to XVB producing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christine Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--235</pages>
<contexts>
<context position="17769" citStr="Miller et al., 1990" startWordPosition="2832" endWordPosition="2835"> research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassilog</context>
<context position="30296" citStr="Miller et al., 1990" startWordPosition="4791" endWordPosition="4794">older Target TargetSpeaker standard 63.59 54.18 40.06 +srlframenet 65.44∗ 55.70∗ 42.14 +inducgraph 68.06∗◦ 59.61∗◦ 46.66∗◦ +srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦ +coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦† +srlframenet+fine-grain lex 70.80∗◦† 63.72∗◦† 56.64∗◦† statistical significance testing (paired t-test, significance level p &lt; 0.05) ∗: better than standard; ◦: better than +srlframenet; †: better than +inducgraph Table 11: In-context evaluation (eval.: F-score). al., 2005) and WordNet (Miller et al., 1990). For semantic role labeling of FrameNetstructures (srlframenet), we used Semafor (Das et al., 2010) with the argument identification based on dual decomposition (Das et al., 2012). We run the configuration that also assigns frame structures to unknown predicates (Das and Smith, 2011). This is necessary as 45% of our opinion verbs are not contained in FrameNet (v1.5). FrameNet has been shown to enable a correct role assignment for AG- and PT-verbs (Bethard et al., 2004; Kim and Hovy, 2006). For instance, in (17) and (18), the opinion holder is assigned to the same frame element EXPERIENCER. Ho</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George Miller, Richard Beckwith, Christine Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction to WordNet: An On-line Lexical Database. International Journal of Lexicography, 3:235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jacqueline Aguilar</author>
<author>Theresa Wilson</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Open Domain Targeted Sentiment.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Mitchell, Aguilar, Wilson, Van Durme, 2013</marker>
<rawString>Margaret Mitchell, Jacqueline Aguilar, Theresa Wilson, and Benjamin Van Durme. 2013. Open Domain Targeted Sentiment. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity – Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="17647" citStr="Pedersen et al., 2004" startWordPosition="2812" endWordPosition="2815">ducing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roa</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity – Measuring the Relatedness of Concepts. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion Word Expansion and Target Extraction through Double Propagation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="40147" citStr="Qiu et al., 2011" startWordPosition="6367" endWordPosition="6370"> role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of semantic roles for opinion holders and targets. Mitchell et al. (2013) present a low-resource approach for target extraction but their aim is to process Twitter messages without using general syntax tools. In this work, we use such tools. Our notion of low resources is different in that we mean the absence of semantic resources helpful for our task (e.g. FrameNet). 7 Conclusion We presented an approach for opinion role induction for verbal predicates. We assume that those predicates can be divided into three different verb types where each type is associated wit</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion Word Expansion and Target Extraction through Double Propagation. Computational Linguistics, 37(1):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>SemiSupervised Polarity Lexicon Induction.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="17920" citStr="Rao and Ravichandran, 2009" startWordPosition="2853" endWordPosition="2856">induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependenc</context>
<context position="23887" citStr="Rao and Ravichandran (2009)" startWordPosition="3812" endWordPosition="3815">osed induction method from Table 5 (graph). method suffers from data-sparsity. In our corpus, the frequency of verbs co-occurring with outrage in a conjunction is 5 or lower.5 The table also shows that WordNet may not be appropriate for our present verb categorization task. However, it may be suitable for other subtasks in sentiment analysis, particularly polarity classification. If we consider the similar entries of outrage provided by that metric, we find that polarity is largely preserved (10 out of 12 verbs are negative). This observation is consistent with Esuli and Sebastiani (2006) and Rao and Ravichandran (2009). In Table 5 we only used the top 40/30/50 verbs from the initialization methods as seeds. We can also compare the output of these methods (combined with propagation, i.e. graph clustering) with the entire verb lists produced by these patternbased initialization methods where no propagation is applied. As far as AG- and PT-verbs are concerned, the entire lists of these initialization methods correspond to the original approach of Wiegand and Klakow (2012). Table 7 shows the result. The new graph-induction always outperforms the original induction method by a large margin. In Table 8, we compar</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. SemiSupervised Polarity Lexicon Induction. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A CorpusBased Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="18242" citStr="Riloff and Shepherd, 1997" startWordPosition="2902" endWordPosition="2906">larity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-base</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A CorpusBased Approach for Building Semantic Lexicons. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Eugene Charniak</author>
</authors>
<title>Nounphrase co-occurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="18269" citStr="Roark and Charniak, 1998" startWordPosition="2907" endWordPosition="2910">04) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). (15) They criticize and hate him. (16) conj(criticize,hate) As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1, w2) = freq(conj(w1, w2)). 4.2.4 Dependency-based Similarity The metric pro</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>Brian Roark and Eugene Charniak. 1998. Nounphrase co-occurrence statistics for semi-automatic semantic lexicon construction. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Ines Rehbein</author>
</authors>
<title>Semantic frames as an anchor representation for sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of WASSA.</booktitle>
<contexts>
<context position="34157" citStr="Ruppenhofer and Rehbein (2012)" startWordPosition="5390" endWordPosition="5393">s that we can compensate some lexical knowledge missing in induction by standard features. Since we could substantially outperform the features relying on FrameNet with our new lexical resources, we looked closer at the predicted frame structures. Beside obvious errors in automatic frame assignment, we also found that there are problems inherent in the frame design. Particularly, the notion of SP-verbs (§3.3) is not properly reflected. Many frames, such as SCRUTINY, typically devised for AG-verbs, such as investigate or analyse, also contain SP-verbs like pry. This observation is in line with Ruppenhofer and Rehbein (2012) who claim that extensions to FrameNet are necessary to properly represent opinions evoked by verbal predicates. 5.3 Comparison to Previous Cross-Domain Opinion Holder Extraction We now compare our proposed induction approach with previous work on opinion holder exin domain out of domain Config MPQA FICTION VERB MultiRel 72.54∗◦ 53.02 44.80 CK 62.98 52.91 43.88 CK + inducWiegand 2012 65.15 57.33∗ 50.83∗ CK + inducgraph 66.06∗ 65.03∗◦ 60.91∗◦ CK + coarse-grain lex 66.82∗ 64.13∗◦ 63.72∗◦† CK + fine-grain lex 66.16∗ 64.98∗◦ 70.85∗◦†‡ statistical significance testing (permutation test, significanc</context>
</contexts>
<marker>Ruppenhofer, Rehbein, 2012</marker>
<rawString>Josef Ruppenhofer and Ines Rehbein. 2012. Semantic frames as an anchor representation for sentiment analysis. In Proceedings of WASSA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Finding the Source and Targets of Subjective Expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="4280" citStr="Ruppenhofer et al., 2008" startWordPosition="653" endWordPosition="656">us work, we are able to categorize all verbs of a pre-specified set of opinion verbs. Our approach is a low-resource approach that is also applicable to languages other than English. We demonstrate this on German. A by-product of our study are new resources including a verb lexicon specifying 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics semantic roles for holders and targets. 2 Lexicon-based Opinion Role Extraction Opinion holder and target extraction is a hard task (Ruppenhofer et al., 2008). Conventional syntactic or semantic levels of representation do not capture sufficient information that allows a reliable prediction of opinion holders and targets. This is illustrated by (1) and (2) which show that, even with common semantic roles, i.e. agent and patient1, assigned to the entities, one may not be able to discriminate between the opinion roles. (1) Peteragent criticized Marypatient. (criticize, Peter, holder) &amp; (criticize, Mary, target) (2) Peteragent disappoints Marypatient. (disappoint, Peter, target) &amp; (disappoint, Mary, holder) We assume that it is lexical information tha</context>
</contexts>
<marker>Ruppenhofer, Somasundaran, Wiebe, 2008</marker>
<rawString>Josef Ruppenhofer, Swapna Somasundaran, and Janyce Wiebe. 2008. Finding the Source and Targets of Subjective Expressions. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Gerold Schneider</author>
<author>Martin Volk</author>
<author>Martin Warin</author>
</authors>
<title>A New Hybrid Dependency Parser for German.</title>
<date>2009</date>
<booktitle>In Proceedings of GSCL.</booktitle>
<contexts>
<context position="26036" citStr="Sennrich et al., 2009" startWordPosition="4158" endWordPosition="4162">ncy-based similarity and graph clustering is used for all configurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6. As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our proposed categorization of verbs into three types for opinion role extraction. 5.2 In-Context Evaluation We now evaluate our induced knowledge in the task of extracting opinion holders and targets from actual text. For t</context>
</contexts>
<marker>Sennrich, Schneider, Volk, Warin, 2009</marker>
<rawString>Rico Sennrich, Gerold Schneider, Martin Volk, and Martin Warin. 2009. A New Hybrid Dependency Parser for German. In Proceedings of GSCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="20229" citStr="Talukdar et al., 2008" startWordPosition="3233" endWordPosition="3236">earest neighbour classifier (kNN) (Cover and Hart, 1967) as a simple method for propagating labels from seeds to other instances. Alternatively, we consider verb categorization as a clustering task on a graph G = (V, E, W) where V is the set of nodes (i.e. our opinion verbs), E is the set of edges connecting them with weights W : E → R+. W can be directly derived from any of the similarity metrics (§4.2.1-§4.2.4). The aim is that all nodes v E V are assigned a label l E {AG, PT, SP}. Initially, only the verb seeds are labeled. We then use the Adsorption label propagation algorithm from junto (Talukdar et al., 2008) in order propagate the labels from the seeds to the remaining verbs. Acc Prec Rec F1 Baselines Majority Class 45.7 14.2 33.3 20.9 Only Seeds 8.9 87.0 9.8 17.6 Coordination kNN 45.2 61.5 47.3 53.4 graph 42.7 68.7 39.7 50.4 WordNet kNN 52.8 51.5 50.7 51.1 graph 51.1 51.9 51.5 51.5 Embedding kNN 59.3 58.4 61.0 59.7 graph 64.0 70.5 59.4 64.5 Dependency kNN 65.7 63.8 65.4 64.5 graph 70.3 72.0 68.0 70.6 Table 5: Eval. of similarity metrics and classifiers. 5 Experiments 5.1 Evaluation of the Induced Lexicon Table 5 compares the performance of the different similarity metrics when incorporated in ei</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word Sense and Subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="10778" citStr="Wiebe and Mihalcea, 2006" startWordPosition="1693" endWordPosition="1697">s (only) mark the agent of an induced SP-verb as a target. 3.4 Relation to Fine-Grained Lexicon Table 2 provides statistics as to how clear-cut the three prototypical verb types are in the manuallycompiled fine-grained lexicon. These numbers suggest that many verbs evoke several opinion views (e.g. a verb with an AG-view may also evoke a PT-view). While the fine-grained lexicon is fairly exhaustive in listing semantic roles for opinion holders and targets, it may also occasionally overgenerate. One major reason for this is that we do not annotate on the sense-level (word-sense disambiguation (Wiebe and Mihalcea, 2006) is still in its infancy) but on the lemmalevel. Accordingly, we attribute all views to all senses, whereas actually certain views pertain only to specific senses. However, we found that usually one view is conveyed by most (if not all) senses of a word. For example, the lexicon lists both an AGview and a PT-view for appease. This is correct 4We consider the patient a target since the speaker has a positive (non-defeasible) sentiment towards that entity. Type Freq Type Freq verbs with AG-view 868 verbs with PT-view 392 verbs with exclusive AG-view 371 verbs with exclusive PT-view 117 verbs wit</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Mihalcea. 2006. Word Sense and Subjectivity. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Therea Wilson</author>
<author>Claire Cardie</author>
</authors>
<date>2005</date>
<booktitle>Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="6212" citStr="Wiebe et al., 2005" startWordPosition="950" endWordPosition="953">r’s invitation. (view4) This corresponds to the role assignments: • view1: (persuade, [1], holder), (persuade, [2], target) • view2: (persuade, [2], holder), (persuade, [1], target) • view3: (persuade, [1], holder), (persuade, [3], target) • view4: (persuade, [2], holder), (persuade, [3], target) (in short: 2 opinion holders and 3 opinion targets). Our lexicon also includes another dimension neglected in many previous works. Many opinion verbs predominantly express the sentiment of the speaker of the utterance (or some nested source) (4). This concept is also known as expressive subjectivity (Wiebe et al., 2005) or speaker subjectivity (Maks and Vossen, 2012). In such opinions, the opinion holder is not realized as a dependent of the opinion verb. (4) At my work, [they]1 are constantly gossiping. (gossip, speaker, holder) &amp; (gossip, [1], target) 1By agent and patient, we mean constituents labeled as AO and Al in PropBank (Kingsbury and Palmer, 2002). Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g</context>
<context position="35276" citStr="Wiebe et al., 2005" startWordPosition="5566" endWordPosition="5569">e-grain lex 66.16∗ 64.98∗◦ 70.85∗◦†‡ statistical significance testing (permutation test, significance level p &lt; 0.05) ∗: better than CK; ◦: better than CK + inducWiegand 2012 ; †: better than CK + inducgraph; ‡: better than CK + coarse-grain lex Table 14: Evaluation on opinion holder extraction on various corpora (evaluation measure: F-score). traction. We replicate several classifiers and compare them to our new approach. (Because of the limited space of this paper, we cannot also address cross-domain opinion target extraction.) We consider three different corpora as shown in Table 13. MPQA (Wiebe et al., 2005) is the standard corpus for fine-grained sentiment analysis. FICTION, introduced in Wiegand and Klakow (2012), is a collection of summaries of classic literary works. VERB is the new corpus used in the previous evaluation (§5.2). VERB and MPQA both originate from the news domain but VERB is sampled in such a way that mentions of all opinion verbs of the Subjectivity Lexicon are represented. The other corpora consist of contiguous sentences. They will have a bias towards only those opinion verbs frequently occurring in that particular domain. This also results in different distributions of verb</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Therea Wilson, and Claire Cardie. 2005. Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation, 39(2/3):164–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
<author>Dietrich Klakow</author>
</authors>
<title>Generalization Methods for In-Domain and Cross-Domain Opinion Holder Extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="14240" citStr="Wiegand and Klakow (2012)" startWordPosition="2261" endWordPosition="2264">ulate, fear, doubt, complain, consider, praise, recommend, view, acknowledge, hope PT interest, surprise, please, excite, disappoint, delight, impress, shock, trouble, embarrass, annoy, distress SP murder, plot, incite, blaspheme, bewitch, bungle, despoil, plagiarize, prevaricate, instigate, molest, conspire Table 4: The top 12 extracted verb seeds. ond step, a similarity metric (§4.2) is employed in order to propagate the verb type labels from the seeds to the remaining opinion verbs (§4.3). The North American News Text Corpus is used for seed extraction and computation of verb similarities. Wiegand and Klakow (2012) proposed methods for extracting AG- and PT-verbs. We will re-use these methods for generating seeds. A major contribution of this paper is the introduction of the third dimension, i.e. SP-verbs, in the context of induction. We show that in combination with this third dimension, one can categorize all opinion verbs contained in a sentiment lexicon. Furthermore, given this three-way classification, we also obtain better results on the detection of AG-verbs and PT-verbs than by just detecting those verbs in isolation without graph clustering (this will be shown in Table 7 and discussed in §5.1).</context>
<context position="15489" citStr="Wiegand and Klakow (2012)" startWordPosition="2464" endWordPosition="2467">tion of this work is that we show that these methods are also equally important for opinion target extraction. So far, the significance of AG- and PT-verbs has only been demonstrated for opinion holder extraction. In this work, we exclusively focus on the set of 1175 opinion verbs from the Subjectivity Lexicon. However, this is owed solely to the effort required to generate larger sets of evaluation data. In principle, our induction approach is applicable to any set of opinion verbs of arbitrary (e.g. larger) size. 4.1 Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen as a proxy. Verbs co-occurring with prototypical opinion holders do not represent the entire range of opinion verbs but coincide with AG-verbs. (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The authors make u</context>
<context position="24346" citStr="Wiegand and Klakow (2012)" startWordPosition="3888" endWordPosition="3892"> that polarity is largely preserved (10 out of 12 verbs are negative). This observation is consistent with Esuli and Sebastiani (2006) and Rao and Ravichandran (2009). In Table 5 we only used the top 40/30/50 verbs from the initialization methods as seeds. We can also compare the output of these methods (combined with propagation, i.e. graph clustering) with the entire verb lists produced by these patternbased initialization methods where no propagation is applied. As far as AG- and PT-verbs are concerned, the entire lists of these initialization methods correspond to the original approach of Wiegand and Klakow (2012). Table 7 shows the result. The new graph-induction always outperforms the original induction method by a large margin. In Table 8, we compare our automatically gen5We found that for frequently occurring opinion verbs, this similarity metric produces more reasonable output. Patternhatf Pattern Patterndoubte Goldhatf Gold Golddoubte 68.71 70.59 66.50 66.21 70.31 73.77 Table 8: Comparison of automatic and gold seeds (evaluation measure: macro-average F-score). Coordin. WordNet Embedd. Depend. Major. kNN graph kNN graph kNN graph kNN graph English 20.9 53.4 50.4 51.1 51.5 59.7 64.5 64.5 70.6 Germ</context>
<context position="29415" citStr="Wiegand and Klakow, 2012" startWordPosition="4675" endWordPosition="4678">ny possible (syntactic) constituent in the respective sentence. The dataset contains 753 holders, 745 targets and 499 targets of a speaker view. Since a constituent may have several roles at the same time, we train three binary classifiers for either of the entity types. On a sample of 200 sentences, we measured an interannotation agreement of Cohen’s r. = 0.69 for holders, r. = 0.63 for targets and also r. = 0.63 for targets of a speaker view. Table 10 shows the features used in our supervised classifier. They have been previously found effective (Choi et al., 2005; Jakob and Gurevych, 2010; Wiegand and Klakow, 2012; Yang and Cardie, 2013). The standard features are the features from cand word to brown. For semantic role labeling of PropBank-structures, we used matetools (Bj¨orkelund et al., 2009). For person detection, we employ named-entity tagging (Finkel et Features Holder Target TargetSpeaker standard 63.59 54.18 40.06 +srlframenet 65.44∗ 55.70∗ 42.14 +inducgraph 68.06∗◦ 59.61∗◦ 46.66∗◦ +srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦ +coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦† +srlframenet+fine-grain lex</context>
<context position="35385" citStr="Wiegand and Klakow (2012)" startWordPosition="5581" endWordPosition="5584">evel p &lt; 0.05) ∗: better than CK; ◦: better than CK + inducWiegand 2012 ; †: better than CK + inducgraph; ‡: better than CK + coarse-grain lex Table 14: Evaluation on opinion holder extraction on various corpora (evaluation measure: F-score). traction. We replicate several classifiers and compare them to our new approach. (Because of the limited space of this paper, we cannot also address cross-domain opinion target extraction.) We consider three different corpora as shown in Table 13. MPQA (Wiebe et al., 2005) is the standard corpus for fine-grained sentiment analysis. FICTION, introduced in Wiegand and Klakow (2012), is a collection of summaries of classic literary works. VERB is the new corpus used in the previous evaluation (§5.2). VERB and MPQA both originate from the news domain but VERB is sampled in such a way that mentions of all opinion verbs of the Subjectivity Lexicon are represented. The other corpora consist of contiguous sentences. They will have a bias towards only those opinion verbs frequently occurring in that particular domain. This also results in different distributions of verb types as shown in Table 13. For example, SP-verbs are rare in MPQA. However, there exist plenty of them (Tab</context>
<context position="36988" citStr="Wiegand and Klakow (2012)" startWordPosition="5853" endWordPosition="5856">inion verb from our set of opinion verbs is not contained in that corpus. In the evaluation, we only consider the opinion holders of our opinion verbs. (Other opinion holders, both in the gold standard and the predictions of the classifiers are ignored.) Recall that we take the knowledge of what is an opinion verb as given. Our graph-based induction can be arbitrarily extended by increasing the set of opinion verbs. 7The split-up of training and test set on the MPQA corpus follows the specification of Johansson and Moschitti (2013). 222 For classifiers, we consider convolution kernels CK from Wiegand and Klakow (2012) and the sequence labeler from Johansson and Moschitti (2013) MultiRel that incorporates relational features taking into account interactions between multiple opinion cues. It is currently the most sophisticated opinion holder extractor. CK can be combined with additional knowledge. We compare inducgraph with inducWiegand 2012, which employs the word lists induced for AG- and PTverbs in the fashion of Wiegand and Klakow (2012), i.e. without graph clustering. As an upper bound for the induction methods, coarse grain lex and fine grain lex are used.8 The combination of CK with this additional kn</context>
<context position="38539" citStr="Wiegand and Klakow (2012)" startWordPosition="6107" endWordPosition="6110">n MPQA. On MPQA, only the new induction approach and the lexicons significantly improve CK. The knowledge of opinion roles has a lower impact on MPQA. In that corpus, most opinion verbs take their opinion holder as an agent. Given the large size of MPQA, this information can be easily learned from the training data. The situation is different for FICTION and VERB where the knowledge from induction largely improves classification. In these corpora, opinion holders as agents are much less frequent than on MPQA. The new induction proposed in this paper also notably outperforms the induction from Wiegand and Klakow (2012). Although the fine-grained lexicon is among the top performing systems, we only note large improvements on VERB. VERB has the highest proportion of PT- and SP-verbs (Table 13). Knowledge about role-assignment is most critical here. 6 Related Work Most approaches for opinion role extraction employ supervised learning. The feature design is 8Wiegand and Klakow (2012) use a lexicon Lex which just comprises the notion of AG and PT verbs, so our manual lexicons are more accurate and harder to beat. 9For in-domain evaluation (i.e. MPQA) the trees (tree structures are the input to CK) are augmented </context>
</contexts>
<marker>Wiegand, Klakow, 2012</marker>
<rawString>Michael Wiegand and Dietrich Klakow. 2012. Generalization Methods for In-Domain and Cross-Domain Opinion Holder Extraction. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thesesa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phraselevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="6657" citStr="Wilson et al., 2005" startWordPosition="1024" endWordPosition="1027">rbs predominantly express the sentiment of the speaker of the utterance (or some nested source) (4). This concept is also known as expressive subjectivity (Wiebe et al., 2005) or speaker subjectivity (Maks and Vossen, 2012). In such opinions, the opinion holder is not realized as a dependent of the opinion verb. (4) At my work, [they]1 are constantly gossiping. (gossip, speaker, holder) &amp; (gossip, [1], target) 1By agent and patient, we mean constituents labeled as AO and Al in PropBank (Kingsbury and Palmer, 2002). Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s n = 60.8 for opinion holders, n = 62.3 for opinion targets</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Thesesa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phraselevel Sentiment Analysis. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexcial selection.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21884" citStr="Wu and Palmer, 1994" startWordPosition="3505" endWordPosition="3508"> increases of the seed sets do not improve the quality (as shown below). 10 of the 50 SP-verbs are extracted from the positive SP-patterns, while the remaining verbs are extracted from the negative SP-patterns (§4.1). As baselines, we include a classifier only employing the seeds and a majority class classifier always predicting an SP-verb. For word embeddings (§4.2.1) and WordNet::Similarity (§4.2.2), we only report the performance of the best metric/configuration, i.e. for embeddings, the continuous bag-of-words model with 500 dimensions and for WordNet::Similarity, the Wu &amp; Palmer measure (Wu and Palmer, 1994). Table 5 shows that the baselines can be outperformed by large margins. The performance of the different similarity metrics varies. The dependency-based metric performs notably better than the other metrics. Together with word embeddings, it is the only metric for which graph clustering produces a notable improvement over kNN. Table 6 illustrates the quality of the similarity metrics for the present task. The table shows that the dependency-based similarity metric provides the most suitable output. The poor quality of coordination may come as a surprise. That 219 Coordin. appear, believe, ref</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexcial selection. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint Inference for Fine-grained Opinion Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="29439" citStr="Yang and Cardie, 2013" startWordPosition="4679" endWordPosition="4682">nstituent in the respective sentence. The dataset contains 753 holders, 745 targets and 499 targets of a speaker view. Since a constituent may have several roles at the same time, we train three binary classifiers for either of the entity types. On a sample of 200 sentences, we measured an interannotation agreement of Cohen’s r. = 0.69 for holders, r. = 0.63 for targets and also r. = 0.63 for targets of a speaker view. Table 10 shows the features used in our supervised classifier. They have been previously found effective (Choi et al., 2005; Jakob and Gurevych, 2010; Wiegand and Klakow, 2012; Yang and Cardie, 2013). The standard features are the features from cand word to brown. For semantic role labeling of PropBank-structures, we used matetools (Bj¨orkelund et al., 2009). For person detection, we employ named-entity tagging (Finkel et Features Holder Target TargetSpeaker standard 63.59 54.18 40.06 +srlframenet 65.44∗ 55.70∗ 42.14 +inducgraph 68.06∗◦ 59.61∗◦ 46.66∗◦ +srlframenet+inducgraph 69.70∗◦ 60.47∗◦ 47.33∗◦ +coarse-grain lex 68.56∗◦ 59.89∗◦ 54.31∗◦† +srlframenet+coarse-grain lex 69.70∗◦ 60.68∗◦ 54.06∗◦† +fine-grain lex 69.83∗◦† 62.89∗◦† 56.71∗◦† +srlframenet+fine-grain lex 70.80∗◦† 63.72∗◦† 56.64</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint Inference for Fine-grained Opinion Extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ziering</author>
<author>Lonneke van der Plas</author>
<author>Hinrich Schuetze</author>
</authors>
<title>Bootstrapping Semantic Lexicons for Technical Domains.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<marker>Ziering, van der Plas, Schuetze, 2013</marker>
<rawString>Patrick Ziering, Lonneke van der Plas, and Hinrich Schuetze. 2013. Bootstrapping Semantic Lexicons for Technical Domains. In Proceedings of IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>