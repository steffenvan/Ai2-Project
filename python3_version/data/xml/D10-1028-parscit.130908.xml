<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004541">
<title confidence="0.994769">
Modeling Perspective using Adaptor Grammars
</title>
<author confidence="0.993212">
Eric A. Hardisty
</author>
<affiliation confidence="0.977666">
Department of Computer Science
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.890812">
College Park, MD
</address>
<email confidence="0.998624">
hardisty@cs.umd.edu
</email>
<author confidence="0.774662">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.725743333333333">
UMD iSchool
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.882034">
College Park, MD
</address>
<email confidence="0.998836">
jbg@umiacs.umd.edu
</email>
<author confidence="0.993552">
Philip Resnik
</author>
<affiliation confidence="0.975259333333333">
Department of Linguistics
and UMIACS
University of Maryland
</affiliation>
<address confidence="0.891001">
College Park, MD
</address>
<email confidence="0.999321">
resnik@umd.edu
</email>
<sectionHeader confidence="0.99392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999397176470588">
Strong indications of perspective can often
come from collocations of arbitrary length; for
example, someone writing get the government
out of my X is typically expressing a conserva-
tive rather than progressive viewpoint. How-
ever, going beyond unigram or bigram features
in perspective classification gives rise to prob-
lems of data sparsity. We address this prob-
lem using nonparametric Bayesian modeling,
specifically adaptor grammars (Johnson et al.,
2006). We demonstrate that an adaptive na¨ıve
Bayes model captures multiword lexical usages
associated with perspective, and establishes a
new state-of-the-art for perspective classifica-
tion results using the Bitter Lemons corpus, a
collection of essays about mid-east issues from
Israeli and Palestinian points of view.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999295">
Most work on the computational analysis of senti-
ment and perspective relies on lexical features. This
makes sense, since an author’s choice of words is
often used to express overt opinions (e.g. describing
healthcare reform as idiotic or wonderful) or to frame
a discussion in order to convey a perspective more
implicitly (e.g. using the term death tax instead of
estate tax). Moreover, it is easy and efficient to rep-
resent texts as collections of the words they contain,
in order to apply a well known arsenal of supervised
techniques (Laver et al., 2003; Mullen and Malouf,
2006; Yu et al., 2008).
At the same time, standard lexical features have
their limitations for this kind of analysis. Such fea-
tures are usually created by selecting some small
n-gram size in advance. Indeed, it is not uncommon
to see the feature space for sentiment analysis limited
to unigrams. However, important indicators of per-
spective can also be longer (get the government out
of my). Trying to capture these using standard ma-
chine learning approaches creates a problem, since
allowing n-grams as features for larger n gives rise
to problems of data sparsity.
In this paper, we employ nonparametric Bayesian
models (Orbanz and Teh, 2010) in order to address
this limitation. In contrast to parametric models, for
which a fixed number of parameters are specified in
advance, nonparametric models can “grow” to the
size best suited to the observed data. In text analysis,
models of this type have been employed primarily
for unsupervised discovery of latent structure — for
example, in topic modeling, when the true number of
topics is not known (Teh et al., 2006); in grammatical
inference, when the appropriate number of nontermi-
nal symbols is not known (Liang et al., 2007); and
in coreference resolution, when the number of enti-
ties in a given document is not specified in advance
(Haghighi and Klein, 2007). Here we use them for
supervised text classification.
Specifically, we use adaptor grammars (Johnson
et al., 2006), a formalism for nonparametric Bayesian
modeling that has recently proven useful in unsuper-
vised modeling of phonemes (Johnson, 2008), gram-
mar induction (Cohen et al., 2010), and named entity
structure learning (Johnson, 2010), to make super-
vised naive Bayes classification nonparametric in
order to improve perspective modeling. Intuitively,
naive Bayes associates each class or label with a
probability distribution over a fixed vocabulary. We
introduce adaptive naive Bayes (ANB), for which in
principle the vocabulary can grow as needed to in-
clude collocations of arbitrary length, as determined
</bodyText>
<page confidence="0.971064">
284
</page>
<note confidence="0.8173735">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999952142857143">
by the properties of the dataset. We show that using
adaptive naive Bayes improves on state of the art
classification using the Bitter Lemons corpus (Lin
et al., 2006), a document collection that has been
used by a variety of authors to evaluate perspective
classification.
In Section 2, we review adaptor grammars, show
how naive Bayes can be expressed within the for-
malism, and describe how — and how easily — an
adaptive naive Bayes model can be created. Section 3
validates the approach via experimentation on the Bit-
ter Lemons corpus. In Section 4, we summarize the
contributions of the paper and discuss directions for
future work.
</bodyText>
<sectionHeader confidence="0.675479" genericHeader="method">
2 Adapting Naive Bayes to be Less Naive
</sectionHeader>
<bodyText confidence="0.987932012820513">
In this work we apply the adaptor grammar formal-
ism introduced by Johnson, Griffiths, and Goldwa-
ter (Johnson et al., 2006). Adaptor grammars are a
generalization of probabilistic context free grammars
(PCFGs) that make it particularly easy to express non-
parametric Bayesian models of language simply and
readably using context free rules. Moreover, John-
son et al. provide an inference procedure based on
Markov Chain Monte Carlo techniques that makes
parameter estimation straightforward for all models
that can be expressed using adaptor grammars.1 Vari-
ational inference for adaptor grammars has also been
recently introduced (Cohen et al., 2010).
Briefly, adaptor grammars allow nonterminals to
be rewritten to entire subtrees. In contrast, a non-
terminal in a PCFG rewrites only to a collection
of grammar symbols; their subsequent productions
are independent of each other. For instance, a tradi-
tional PCFG might learn probabilities for the rewrite
rule PP H P NP. In contrast, an adaptor gram-
mar can learn (or “cache”) the production PP H
(P up)(NP(DET a)(N tree)). It does this by posit-
ing that the distribution over children for an adapted
non-terminal comes from a Pitman-Yor distribution.
A Pitman-Yor distribution (Pitman and Yor, 1997)
is a distribution over distributions. It has three pa-
rameters: the discount, a, such that 0 &lt; a &lt; 1,
the strength, b, a real number such that −a &lt; b,
1And, better still, they provide code that
implements the inference algorithm; see
http://www.cog.brown.edu/ mj/Software.htm.
and a probability distribution G0 known as the base
distribution. Adaptor grammars allow distributions
over subtrees to come from a Pitman-Yor distribu-
tion with the PCFG’s original distribution over trees
as the base distribution. The generative process for
obtaining draws from a distribution drawn from a
Pitman-Yor distribution can be described by the “Chi-
nese restaurant process” (CRP). We will use the CRP
to describe how to obtain a distribution over obser-
vations composed of sequences of n-grams, the key
to our model’s ability to capture perspective-bearing
n-grams.
Suppose that we have a base distribution Q that is
some distribution over all sequences of words (the
exact structure of such a distribution is unimportant;
such a distribution will be defined later in Table 1).
Suppose further we have a distribution 0 drawn from
PY (a, b, Q), and we wish to draw a series of obser-
vations w from 0. The CRP gives us a generative
process for doing those draws from 0, marginaliz-
ing out 0. Following the restaurant metaphor, we
imagine the ith customer in the series entering the
restaurant to take a seat at a table. The customer sits
by making a choice that determines the value of the
n-gram wi for that customer: she can either sit at an
existing table or start a new table of her own.2
If she sits at a new table j, that table is assigned
a draw yj from the base distribution, Q; note that,
since Q is a distribution over n-grams, yj is an n-
gram. The value of wi is therefore assigned to be yj,
and yj becomes the sequence of words assigned to
that new table. On the other hand, if she sits at an
existing table, then wi simply takes the sequence of
words already associated with that table (assigned as
above when it was first occupied).
The probability of joining an existing table j,
with cj patrons already seated at table j, is c;−a
c·+b ,
where c· is the number of patrons seated at all tables:
c· _ Ej, cj,. The probability of starting a new table
b+t*a h is b fbl
� were t s the number o tables resent&apos;
is c·+b presently
occupied.
Notice that 0 is a distribution over the same space
as Q, but it can drastically shift the mass of the dis-
tribution, compared with Q, as more and more pa-
</bodyText>
<footnote confidence="0.979233666666667">
2Note that we are abusing notation by allowing wi to cor-
respond to a word sequence of length &gt; 1 rather than a single
word.
</footnote>
<page confidence="0.998419">
285
</page>
<bodyText confidence="0.999844272727273">
trons are seated at tables. However, there is always
a chance of drawing from the base distribution, and
therefore every word sequence can also always be
drawn from 0.
In the next section we will write a naive Bayes-like
generative process using PCFGs. We will then use
the PCFG distribution as the base distribution for a
Pitman-Yor distribution, adapting the naive Bayes
process to give us a distribution over n-grams, thus
learning new language substructures that are useful
for modeling the differences in perspective.
</bodyText>
<subsectionHeader confidence="0.989428">
2.1 Classification Models as PCFGs
</subsectionHeader>
<bodyText confidence="0.999474153846154">
Naive Bayes is a venerable and popular mechanism
for text classification (Lewis, 1998). It posits that
there are K distinct categories of text — each with a
distinct distribution over words — and that every doc-
ument, represented as an exchangeable bag of words,
is drawn from one (and only one) of these distribu-
tions. Learning the per-category word distributions
and global prevalence of the classes is a problem of
posterior inference which can be approached using a
variety of inference techniques (Lowd and Domingos,
2005).
More formally, naive Bayes models can be ex-
pressed via the following generative process:3
</bodyText>
<listItem confidence="0.997889125">
1. Draw a global distribution over classes 0 ∼
Dir (α)
2. For each class i ∈ {1, ... , K}, draw a word
distribution Oi ∼ Dir (A)
3. For each document d ∈ {1, ... , M}:
(a) Draw a class assignment zd ∼ Mult (0)
(b) For each word position n ∈ {1, ... , Nd,
draw wd,n ∼ Mult (0zd)
</listItem>
<bodyText confidence="0.998999307692308">
A variant of the naive Bayes generative process can
be expressed using the adaptor grammar formalism
(Table 1). The left hand side of each rule represents
a nonterminal which can be expanded, and the right
hand side represents the rewrite rule. The rightmost
indices show replication; for instance, there are |V |
rules that allow WORDi to rewrite to each word in the
3Here α and A are hyperparameters used to specify priors
for the class distribution and classes’ word distributions, respec-
tively; α is a symmetric K-dimensional vector where each ele-
ment is ir. Nd is the length of document d. Resnik and Hardisty
(2010) provide a tutorial introduction to the naive Bayes genera-
tive process and underlying concepts.
</bodyText>
<equation confidence="0.972769833333333">
SENT 7→ DOCd d = 1, ... ,m
DOCd0.001 7→IDd WORDSi d = 1, ... , m;
i ∈ {1, K}
WORDSi 7→ WORDSi WORDi i ∈ {1, K}
WORDSi 7→ WORDi i ∈ {1, K}
WORDi 7→ v v ∈ V ; i ∈ {1, K}
</equation>
<tableCaption confidence="0.9792245">
Table 1: A naive Bayes-inspired model expressed as a
PCFG.
</tableCaption>
<bodyText confidence="0.999720555555556">
vocabulary. One can assume a symmetric Dirichlet
prior of Dir (1) over the production choices unless
otherwise specified — as with the DOCd production
rule above, where a sparse prior is used.
Notice that the distribution over expansions for
WORDi corresponds directly to Oi in Figure 1(a).
There are, however, some differences between the
model that we have described above and the standard
naive Bayes model depicted in Figure 1(a). In par-
ticular, there is no longer a single choice of class per
document; each sentence is assigned a class. If the
distribution over per-sentence labels is sparse (as it
is above for DOCd), this will closely approximate
naive Bayes, since it will be very unlikely for the
sentences in a document to have different labels. A
non-sparse prior leads to behavior more like models
that allow parts of a document to express sentiment
or perspective differently.
</bodyText>
<subsectionHeader confidence="0.998602">
2.2 Moving Beyond the Bag of Words
</subsectionHeader>
<bodyText confidence="0.999986555555556">
The naive Bayes generative distribution posits that
when writing a document, the author selects a distri-
bution of categories zd for the document from 0. The
author then generates words one at a time: each word
is selected independently from a flat multinomial
distribution 0zd over the vocabulary.
However, this is a very limited picture of how text
is related to underlying perspectives. Clearly words
are often connected with each other as collocations,
and, just as clearly, extending a flat vocabulary to
include bigram collocations does not suffice, since
sometimes relevant perspective-bearing phrases are
longer than two words. Consider phrases like health
care for all or government takeover of health care,
connected with progressive and conservative posi-
tions, respectively, during the national debate on
healthcare reform. Simply applying naive Bayes,
or any other model, to a bag of n-grams for high n is
</bodyText>
<page confidence="0.989572">
286
</page>
<figure confidence="0.999753157894737">
(a) Naive Bayes
(b) Adaptive Naive Bayes
α θ zd
λ
Wd,n
φi
Nd
K
M
α θ zd
a
b
Wd,n
φr
Nd
M
Ω
K
τ
</figure>
<figureCaption confidence="0.991338">
Figure 1: A plate diagram for naive Bayes and adaptive naive Bayes. Nodes represent random variables and parameters;
shaded nodes represent observations; lines represent probabilistic dependencies; and the rectangular plates denote
replication.
</figureCaption>
<bodyText confidence="0.99971345">
going to lead to unworkable levels of data sparsity;
a model should be flexible enough to support both
unigrams and longer phrases as needed.
Following Johnson (2010), however, we can use
adaptor grammars to extend naive Bayes flexibly to
include richer structure like collocations when they
improve the model, and not including them when
they do not. This can be accomplished by introduc-
ing adapted nonterminal rules: in a revised genera-
tive process, the author can draw from Pitman-Yor
distribution whose base distribution is over word se-
quences of arbitrary length.4 Thus in a setting where,
say, K = 2, and our two classes are PROGRESSIVE
and CONSERVATIVE, the sequence health care for all
might be generated as a single unit for the progressive
perspective, but in the conservative perspective the
same sequence might be generated as three separate
draws: health care, for, all. Such a model is pre-
sented in Figure 1(b). Note the following differences
between Figures 1(a) and 1(b):
</bodyText>
<listItem confidence="0.998473777777778">
• zd selects which Pitman-Yor distribution to draw
from for document d.
• Oi is the distribution over n-grams that comes
from the Pitman-Yor distribution.
• Wd n represents an n-gram draw from Oi
• a, b are the Pitman-Yor strength and discount
parameters.
• Q is the Pitman-Yor base distribution with T as
its uniform hyperparameter.
</listItem>
<bodyText confidence="0.942684054054054">
4As defined above, the base distribution is that of the PCFG
production rule WORDSi. Although it has non-zero probability
of producing any sequence of words, it is biased toward shorter
word sequences.
Returning to the CRP metaphor discussed when we
introduced the Pitman-Yor distribution, there are two
restaurants, one for the PROGRESSIVE distribution
and one for the CONSERVATIVE distribution. Health
care for all has its own table in the PROGRESSIVE
restaurant, and enough people are sitting at it to make
it popular. There is no such table in the CONSERVA-
TIVE restaurant, so in order to generate those words,
the phrase health care for all would need to come
from a new table; however, it is more easily explained
by three customers sitting at three existing, popular
tables: health care, for, and all.
We follow the convention of Johnson (2010) by
writing adapted nonterminals as underlined. The
grammar for adaptive naive Bayes is shown in Ta-
ble 2. The adapted COLLOCi rule means that every
time we need to generate that nonterminal, we are
actually drawing from a distribution drawn from a
Pitman-Yor distribution. The distribution over the
possible yields of the WORDSi rule serves as the
base distribution.
Given this generative process for documents, we
can now use statistical inference to uncover the pos-
terior distribution over the latent variables, thus dis-
covering the tables and seating assignments of our
metaphorical restaurants that each cater to a specific
perspective filled with tables populated by words and
n-grams.
The model presented in Table 2 is the most straight-
forward way of extending naive Bayes to collocations.
For completeness, we also consider the alternative
of using a shared base distribution rather than dis-
tinguishing the base distributions of the two classes.
</bodyText>
<page confidence="0.911017">
287
</page>
<equation confidence="0.920961333333333">
SENT &gt; DOCd d = 1, ... ,m
DOCd0.001 &gt;IDd SPANi d = 1, ... , m;
i E {1, K}
SPANi &gt; SPANi COLLOCi i E {1, K}
SPANi &gt; COLLOCi i E {1, K}
COLLOCi &gt; WORDSi i E {1, K}
WORDSi &gt; WORDSi WORDi i E {1, K}
WORDSi &gt; WORDi i E {1, K}
WORDi &gt; v v E V ; i E {1, K}
</equation>
<tableCaption confidence="0.9998338">
Table 2: An adaptive naive Bayes grammar. The
COLLOCi nonterminal’s distribution over yields is drawn
from a Pitman-Yor distribution rather than a Dirichlet over
production rules.
Table 3: An adaptive naive Bayes grammar with a com-
</tableCaption>
<figureCaption confidence="0.8052164">
mon base distribution for collocations. Note that, in con-
trast to Table 2, there are no subscripts on WORDS or
WORD.
Figure 2: An alternative adaptive naive Bayes with a com-
mon base distribution for both classes.
</figureCaption>
<figure confidence="0.999124">
α θ zd
a
b
Wd,n
φi
Nd
K
M
Ω
τ
Training Set
Corpus Filter
Vocabulary
Generator
Grammar
Generator
Test Set
Corpus Filter
AG Classifier
SENT &gt; DOCd d = 1, ... ,m
DOCd0.001 &gt; IDd SPANi d = 1, ... , m;
i E {1, K}
SPANi &gt; SPANi COLLOCi i E {1, K}
SPANi &gt; COLLOCi i E {1, K}
COLLOCi &gt; WORDS i E {1, K}
WORDS &gt; WORDS WORD
WORDS &gt; WORD
WORD &gt; v v E V
</figure>
<figureCaption confidence="0.99995">
Figure 3: Corpus preparation and experimental setup.
</figureCaption>
<bodyText confidence="0.9999548">
Briefly, using a shared base distribution posits that
the two classes use similar word distributions, but
generate collocations unique to each class, whereas
using separate base distributions assumes that the
distribution of words is unique to each class.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999873">
3.1 Corpus Description
</subsectionHeader>
<bodyText confidence="0.999968636363636">
We conducted our classification experiments on the
Bitter Lemons (BL) corpus, which is a collection of
297 essays averaging 700-800 words in length, on
various Middle East issues, written from both the
Israeli and Palestinian perspectives. The BL corpus
was compiled by Lin et al. (2006) and is derived from
a website that invites weekly discussions on a topic
and publishes essays from two sets of authors each
week.5 Two of the authors are guests, one from each
perspective, and two essays are from the site’s regular
contributors, also one from each perspective, for a
</bodyText>
<footnote confidence="0.936706">
5http://www.bitterlemons.org
</footnote>
<bodyText confidence="0.999862736842105">
total of four essays on each topic per week. We chose
this corpus to allow us to directly compare our results
with Greene and Resnik’s (2009) Observable Proxies
for Underlying Semantics (OPUS) features and Lin
et al.’s Latent Sentence Perspective Model (LSPM).
The classification goal for this corpus is to label each
document with the perspective of its author, either
Israeli or Palestinian.
Consistent with prior work, we prepared the corpus
by dividing it into two groups, one group containing
all of the essays written by the regular site contrib-
utors, which we call the Editor set, and one group
comprised of all the essays written by the guest con-
tributors, which we call the Guest set. Similar to the
above mentioned prior work, we perform classifica-
tion using one group as training data and the other as
test data and perform two folds of classification. The
overall experimental setup and corpus preparation
process is presented in Figure 3.
</bodyText>
<page confidence="0.993162">
288
</page>
<subsectionHeader confidence="0.992026">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999996885714285">
The vocabulary generator determines the vocabulary
used by a given experiment by converting the training
set to lower case, stemming with the Porter stemmer,
and filtering punctuation. We remove from the vocab-
ulary any words that appeared in only one document
regardless of frequency within that document, words
with frequencies lower than a threshold, and stop
words.6 The vocabulary is then passed to a grammar
generator and a corpus filter.
The grammar generator uses the vocabulary to gen-
erate the terminating rules of the grammar from the
ANB grammar presented in Tables 2 and 3. The cor-
pus filter takes in a set of documents and replaces all
words not in the vocabulary with “out of vocabulary”
markers. This process ensures that in all experiments
the vocabulary is composed entirely of words from
the training set. After the groups have been filtered,
the group used as the test set has its labels removed.
The test and training set are then sent, along with the
grammar, into the adaptor grammar inference engine.
Each experiment ran for 3000 iterations. For the
runs where adaptation was used we set the initial
Pitman-Yor a and b parameters to 0.01 and 10 respec-
tively, then slice sample (Johnson and Goldwater,
2009).
We use the resulting sentence parses for classifi-
cation. By design of the grammar, each sentence’s
words will belong to one and only one distribution.
We identify that distribution from each of the test
set sentence parses and use it as the sentence level
classification for that particular sentence. We then
use majority rule on the individual sentence classifi-
cations in a document to obtain the document classifi-
cation. (In most cases the sentence-level assignments
are overwhelmingly dominated by one class.)
</bodyText>
<subsectionHeader confidence="0.973686">
3.3 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999717714285714">
Table 4 gives the results and compares to prior
work. The support vector machine (SVM), NB-
B and LSPM results are taken directly from Lin
et al. (2006). NB-B indicates naive Bayes with
full Bayesian inference. LSPM is the Latent
Sentence Perspective Model, also from Lin et
al. (2006). OPUS results are taken from Greene
</bodyText>
<footnote confidence="0.964822">
6In these experiments, a frequency threshold of 4 was se-
lected prior to testing.
</footnote>
<table confidence="0.9999322">
Training Set Test Set Classifier Accuracy
Guests Editors SVM 88.22
Guests Editors NB-B 93.46
Guests Editors LSPM 94.93
Guests Editors OPUS 97.64
Guests Editors ANB* 99.32
Guests Editors ANB Com 99.93
Guests Editors ANB Sep 99.87
Editors Guests SVM 81.48
Editors Guests NB-B 85.85
Editors Guests LSPM 86.99
Editors Guests OPUS 85.86
Editors Guests ANB* 84.98
Editors Guests ANB Com 82.76
Editors Guests ANB Sep 88.28
</table>
<tableCaption confidence="0.9709195">
Table 4: Classification results. ANB* indicates the same
grammar as Adapted Naive Bayes, but with adaptation dis-
abled. Com and Sep refer to whether the base distribution
was common to both classes or separate.
</tableCaption>
<bodyText confidence="0.992641692307692">
and Resnik (2009). Briefly, OPUS features are gener-
ated from observable grammatical relations that come
from dependency parses of the corpus. Use of these
features provided the best classification accuracy for
this task prior to this work. ANB* refers to the gram-
mar from Table 2, but with adaptation disabled. The
reported accuracy values for ANB*, ANB with a
common base distribution (see Table 3), and ANB
with separate base distributions (see Table 2) are
the mean values from five separate sampling chains.
Bold face indicates statistical signficance (p &lt; 0.05)
by unpaired t-test between the reported value and
ANB*.
Consistent with all prior work on this corpus we
found that the classification accuracy for training on
editors and testing on guests was lower than the other
direction since the larger number of editors in the
guest set allows for greater generalization. The dif-
ference between ANB* and ANB with a common
base distribution is not statistically significant. Also
of note is that the classification accuracy improves
for testing on Guests when the ANB grammar is al-
lowed to adapt and a separate base distribution is used
for the two classes (88.28% versus 84.98% without
adaptation).
Table 5 presents some data on adapted rules
</bodyText>
<page confidence="0.985888">
289
</page>
<table confidence="0.998712666666667">
Class Group Unique Unique Percent of Group
Unigrams Cached n-grams Cached Vocabulary Cached
Israeli Editors 2,292 19,614 77.62
Palestinian Editors 2,180 17,314 86.54
Israeli Guests 2,262 19,398 79.91
Palestinian Guests 2,005 16,946 74.94
</table>
<tableCaption confidence="0.987851">
Table 5: Counts of cached unigrams and n-grams for the two classes compared to the vocabulary sizes.
</tableCaption>
<table confidence="0.999594444444445">
Israeli Palestinian
zionist dream american jew
zionist state achieve freedom
zionist movement palestinian freedom
american leadership support palestinian
american victory palestinian suffer
abandon violence palestinian territory
freedom (of the) press palestinian statehood
palestinian violence palestinian refugee
</table>
<tableCaption confidence="0.999441">
Table 6: Charged bigrams captured by the framework.
</tableCaption>
<bodyText confidence="0.99996754054054">
learned once inference is complete. The column
labeled unique unigrams cached indicates the num-
ber of unique unigrams that appear on the right hand
side of the adapted rules. Similarly, unique n-grams
cached indicates the number of unique n-grams that
appear on the right hand side of the adapted rules.
The rightmost column indicates the percentage of
terms from the group vocabulary that appear on the
right hand side of adapted rules as unigrams. Values
less than 100% indicate that the remaining vocabu-
lary terms are cached in n-grams. As the table shows,
a significant number of the rules learned during infer-
ence are n-grams of various sizes.
Inspection of the captured bigrams showed that
it captured sequences that a human might associate
with one perspective over the other. Table 6 lists just
a few of the more charged bigrams that were captured
in the adapted rules.
More specific caching information on the individ-
ual groups and classes is provided in Table 7. This
data clearly demonstrates that raw n-gram frequency
alone is not indicative of how many times an n-gram
is used as a cached rule. For example, consider the
bigram people go, which is used as a cached bigram
only three times, yet appears in the corpus 407 times.
Compare that with isra palestinian, which is cached
the same number of times but appears only 18 times
in the corpus. In other words, the sequence people go
is more easily explained by two sequential unigrams,
not a bigram. The ratio of cache use counts to raw
bigrams gives a measure of strength of collocation
between the terms of the n-gram. We conjecture that
the rareness of caching for n &gt; 2 is a function of the
small corpus size. Also of note is the improvement in
performance of ANB* over NB-B when training on
guests, which we suspect is due to our use of sampled
versus fixed hyperparameters.
</bodyText>
<sectionHeader confidence="0.997438" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99996412">
In this paper, we have applied adaptor grammars in
a supervised setting to model lexical properties of
text and improve document classification according
to perspective, by allowing nonparametric discovery
of collocations that aid in perspective classification.
The adaptive naive Bayes model improves on state
of the art supervised classification performance in
head-to-head comparisons with previous approaches.
Although there have been many investigations on
the efficacy of using multiword collocations in text
classification (Bekkerman and Allan, 2004), usually
such approaches depend on a preprocessing step such
as computing tf-idf or other measures of frequency
based on either word bigrams (Tan et al., 2002) or
character n-grams (Raskutti et al., 2001). In con-
trast, our approach combines phrase discovery with
the probabilistic model of the text. This allows for
the collocation selection and classification to be ex-
pressed in a single model, which can then be extended
later; it also is truly generative, as compared with fea-
ture induction and selection algorithms that either
under- or over-generate data.
There are a number of interesting directions in
which to take this research. As Johnson et al. (2006)
argue, and as we have confirmed here, the adaptor
</bodyText>
<page confidence="0.98562">
290
</page>
<table confidence="0.999667235294118">
Guest Editor
Israeli Palestinian Israeli Palestinian
palestinian OOV 11 299 palestinian isra 6 178 palestinian OOV 8 254 OOV israel 7 198
OOV palestinian 7 405 OOV palestinian 6 405 OOV palestinian 7 319 OOV palestinian 6 319
isra OOV 6 178 palestinian OOV 5 29 OOV israel 7 123 OOV work 5 254
israel OOV 6 94 one OOV 4 25 OOV us 6 115 OOV agreement 5 75
sharon OOV 4 74 side OOV 3 21 OOV part 5 56 palestinian reform 4 49
polit OOV 4 143 polit OOV 3 299 israel OOV 5 81 palestinian OOV 4 81
OOV us 4 29 peopl go 3 407 attempt OOV 5 91 OOV isra 4 15
OOV state 4 37 palestinian govern 3 94 time OOV 4 63 one OOV 4 27
israel palestinian 4 52 palestinian accept 3 220 remain OOV 4 85 isra palestinian 4 17
even OOV 4 43 OOV state 3 150 OOV time 4 70 isra OOV 4 63
arafat OOV 4 41 OOV israel 3 18 OOV area 4 49 howev OOV 4 149
appear OOV 4 53 OOV end 3 20 OOV arafat 4 28 want OOV 3 36
total OOV 3 150 OOV act 3 105 isra OOV 4 8 us OOV 3 35
palestinian would 3 65 isra palestinian 3 18 would OOV 3 28 recent OOV 3 220
palestinian isra 3 35 israel OOV 3 198 use OOV 3 198 palestinian isra 3 115
</table>
<tableCaption confidence="0.9799425">
Table 7: Most frequently used cached bigrams. The first colum in each section is the number of times that bigram was
used as a cached rule. The second column indicates the raw count of that bigram in the Guests or Editors group.
</tableCaption>
<bodyText confidence="0.999886071428571">
grammar formalism makes it quite easy to work with
latent variable models, in order to automatically dis-
cover structures in the data that have predictive value.
For example, it is easy to imagine a model where in
addition to a word distribution for each class, there
is also an additional shared “neutral” distribution:
for each sentence, the words in that sentence can ei-
ther come from the class-specific content distribution
or the shared neutral distribution. This turns out to
be the Latent Sentence Perspective Model of Lin et
al. (2006), which is straightforward to encode using
the adaptor grammar formalism simply by introduc-
ing two new nonterminals to represent the neutral
distribution:
</bodyText>
<equation confidence="0.9281069">
SENT 7→ DOCd d = 1, ... ,m
DOCd 7→ IDd WORDSi d = 1, ... ,m;
i ∈ {1, K}
DOCd 7→ IDd NEUTS d = 1, ... ,m;
WORDSi 7→ WORDSi WORDi i ∈ {1, K}
WORDSi 7→ WORDi i ∈ {1, K}
WORDi 7→ v v ∈ V ; i ∈ {1, K}
NEUT 7→ NEUTSi NEUTi
NEUT 7→ NEUT
NEUT 7→ v v ∈ V
</equation>
<bodyText confidence="0.99997844">
Running this grammar did not produce improvements
consistent with those reported by Lin et al. We plan to
investigate this further, and a natural follow-on would
be to experiment with adaptation for this variety of
latent structure, to produce an adapted LSPM-like
model analogous to adaptive naive Bayes.
Viewed in a larger context, computational classi-
fication of perspective is closely connected to social
scientists’ study of framing, which Entman (1993)
characterizes as follows: “To frame is to select some
aspects of a perceived reality and make them more
salient in a communicating text, in such a way as
to promote a particular problem definition, causal
interpretation, moral evaluation, and/or treatment rec-
ommendation for the item described.” Here and in
other work (e.g. (Laver et al., 2003; Mullen and Mal-
ouf, 2006; Yu et al., 2008; Monroe et al., 2008)),
it is clear that lexical evidence is one key to under-
standing how language is used to frame discussion
from one perspective or another; Resnik and Greene
(2009) have shown that syntactic choices can pro-
vide important evidence, as well. Another promising
direction for this work is the application of adaptor
grammar models as a way to capture both lexical and
grammatical aspects of framing in a unified model.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999934555555556">
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be con-
strued as representing the official views or policies
</bodyText>
<page confidence="0.990362">
291
</page>
<bodyText confidence="0.9999424">
of ARL, IARPA, the ODNI, or the U.S. Government.
The authors thank Mark Johnson and the anonymous
reviewers for their helpful comments and discussions.
We are particularly grateful to Mark Johnson for mak-
ing his adaptor grammar code available.
</bodyText>
<sectionHeader confidence="0.996468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886602040816">
R. Bekkerman and J. Allan. 2004. Using bigrams in text
categorization. Technical Report IR-408, Center of
Intelligent Information Retrieval, UMass Amherst.
S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Varia-
tional inference for adaptor grammars. In Conference
of the North American Chapter of the Association for
Computational Linguistics.
R.M. Entman. 1993. Framing: Toward Clarification of a
Fractured Paradigm. The Journal of Communication,
43(4):51–58.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 503–511.
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 848–855,
Prague, Czech Republic, June.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Conference of the North American Chapter of
the Association for Computational Linguistics, pages
317–325, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2006. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian mod-
els. In Proceedings of Advances in Neural Information
Processing Systems.
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proceedings of ACL-08: HLT, pages 398–
406, Columbus, Ohio, June.
Mark Johnson. 2010. PCFGs, topic models, adaptor gram-
mars and learning topical collocations and the structure
of proper names. In Proceedings of the Association for
Computational Linguistics, pages 1148–1157, Uppsala,
Sweden, July.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
pages 311–331.
David D. Lewis. 1998. Naive (bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
N´edellec and C´eline Rouveirol, editors, Proceedings
of ECML-98, 10th European Conference on Machine
Learning, number 1398, pages 4–15, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of Emperical Methods in
Natural Language Processing, pages 688–697.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? Identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Daniel Lowd and Pedro Domingos. 2005. Naive bayes
models for probability estimation. In ICML ’05: Pro-
ceedings of the 22nd international conference on Ma-
chine learning, pages 529–536, New York, NY, USA.
ACM.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn.
2008. Fightin’ Words: Lexical Feature Selection and
Evaluation for Identifying the Content of Political Con-
flict. Political Analysis, Vol. 16, Issue 4, pp. 372-403,
2008.
Tony Mullen and Robert Malouf. 2006. A preliminary in-
vestigation into sentiment analysis of informal political
discourse. In AAAI Symposium on Computational Ap-
proaches to Analysing Weblogs (AAAI-CAAW), pages
159–162.
P. Orbanz and Y. W. Teh. 2010. Bayesian nonparamet-
ric models. In Encyclopedia of Machine Learning.
Springer.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordinator.
Annals of Probability, 25(2):855–900.
Bhavani Raskutti, Herman L. Ferr´a, and Adam Kowal-
czyk. 2001. Second order features for maximising text
classification performance. In EMCL ’01: Proceedings
of the 12th European Conference on Machine Learning,
pages 419–430, London, UK. Springer-Verlag.
Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Re-
port UMIACS-TR-2010-04, University of Maryland.
http://www.lib.umd.edu/drum/handle/1903/10058.
Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee.
2002. The use of bigrams to enhance text categoriza-
tion. Inf. Process. Manage., 38(4):529–546.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association,
101(476):1566–1581.
B. Yu, S. Kaufmann, and D. Diermeier. 2008. Classify-
ing party affiliation from political speech. Journal of
Information Technology and Politics, 5(1):33–48.
</reference>
<page confidence="0.997488">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.203146">
<title confidence="0.999997">Modeling Perspective using Adaptor Grammars</title>
<author confidence="0.998493">A Eric</author>
<affiliation confidence="0.998602333333333">Department of Computer and University of</affiliation>
<address confidence="0.619082">College Park,</address>
<email confidence="0.999766">hardisty@cs.umd.edu</email>
<author confidence="0.978012">Jordan</author>
<affiliation confidence="0.921984666666667">UMD and University of</affiliation>
<address confidence="0.716653">College Park,</address>
<email confidence="0.999851">jbg@umiacs.umd.edu</email>
<author confidence="0.989386">Philip</author>
<affiliation confidence="0.998303">Department of and University of</affiliation>
<address confidence="0.641043">College Park,</address>
<email confidence="0.999865">resnik@umd.edu</email>
<abstract confidence="0.995584444444445">Strong indications of perspective can often come from collocations of arbitrary length; for someone writing the government of my X typically expressing a conservative rather than progressive viewpoint. However, going beyond unigram or bigram features in perspective classification gives rise to problems of data sparsity. We address this problem using nonparametric Bayesian modeling, specifically adaptor grammars (Johnson et al., We demonstrate that an na¨ıve captures multiword lexical usages associated with perspective, and establishes a new state-of-the-art for perspective classification results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>J Allan</author>
</authors>
<title>Using bigrams in text categorization.</title>
<date>2004</date>
<tech>Technical Report IR-408,</tech>
<institution>Center of Intelligent Information Retrieval, UMass Amherst.</institution>
<contexts>
<context position="26355" citStr="Bekkerman and Allan, 2004" startWordPosition="4378" endWordPosition="4381">ue to our use of sampled versus fixed hyperparameters. 4 Conclusions In this paper, we have applied adaptor grammars in a supervised setting to model lexical properties of text and improve document classification according to perspective, by allowing nonparametric discovery of collocations that aid in perspective classification. The adaptive naive Bayes model improves on state of the art supervised classification performance in head-to-head comparisons with previous approaches. Although there have been many investigations on the efficacy of using multiword collocations in text classification (Bekkerman and Allan, 2004), usually such approaches depend on a preprocessing step such as computing tf-idf or other measures of frequency based on either word bigrams (Tan et al., 2002) or character n-grams (Raskutti et al., 2001). In contrast, our approach combines phrase discovery with the probabilistic model of the text. This allows for the collocation selection and classification to be expressed in a single model, which can then be extended later; it also is truly generative, as compared with feature induction and selection algorithms that either under- or over-generate data. There are a number of interesting dire</context>
</contexts>
<marker>Bekkerman, Allan, 2004</marker>
<rawString>R. Bekkerman and J. Allan. 2004. Using bigrams in text categorization. Technical Report IR-408, Center of Intelligent Information Retrieval, UMass Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>D M Blei</author>
<author>N A Smith</author>
</authors>
<title>Variational inference for adaptor grammars.</title>
<date>2010</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3350" citStr="Cohen et al., 2010" startWordPosition="517" endWordPosition="520">, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised naive Bayes classification nonparametric in order to improve perspective modeling. Intuitively, naive Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive naive Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for</context>
<context position="5302" citStr="Cohen et al., 2010" startWordPosition="819" endWordPosition="822">aptor grammar formalism introduced by Johnson, Griffiths, and Goldwater (Johnson et al., 2006). Adaptor grammars are a generalization of probabilistic context free grammars (PCFGs) that make it particularly easy to express nonparametric Bayesian models of language simply and readably using context free rules. Moreover, Johnson et al. provide an inference procedure based on Markov Chain Monte Carlo techniques that makes parameter estimation straightforward for all models that can be expressed using adaptor grammars.1 Variational inference for adaptor grammars has also been recently introduced (Cohen et al., 2010). Briefly, adaptor grammars allow nonterminals to be rewritten to entire subtrees. In contrast, a nonterminal in a PCFG rewrites only to a collection of grammar symbols; their subsequent productions are independent of each other. For instance, a traditional PCFG might learn probabilities for the rewrite rule PP H P NP. In contrast, an adaptor grammar can learn (or “cache”) the production PP H (P up)(NP(DET a)(N tree)). It does this by positing that the distribution over children for an adapted non-terminal comes from a Pitman-Yor distribution. A Pitman-Yor distribution (Pitman and Yor, 1997) i</context>
</contexts>
<marker>Cohen, Blei, Smith, 2010</marker>
<rawString>S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Variational inference for adaptor grammars. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Entman</author>
</authors>
<title>Framing: Toward Clarification of a Fractured Paradigm.</title>
<date>1993</date>
<journal>The Journal of Communication,</journal>
<volume>43</volume>
<issue>4</issue>
<contexts>
<context position="29793" citStr="Entman (1993)" startWordPosition="5037" endWordPosition="5038"> = 1, ... ,m; WORDSi 7→ WORDSi WORDi i ∈ {1, K} WORDSi 7→ WORDi i ∈ {1, K} WORDi 7→ v v ∈ V ; i ∈ {1, K} NEUT 7→ NEUTSi NEUTi NEUT 7→ NEUT NEUT 7→ v v ∈ V Running this grammar did not produce improvements consistent with those reported by Lin et al. We plan to investigate this further, and a natural follow-on would be to experiment with adaptation for this variety of latent structure, to produce an adapted LSPM-like model analogous to adaptive naive Bayes. Viewed in a larger context, computational classification of perspective is closely connected to social scientists’ study of framing, which Entman (1993) characterizes as follows: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.” Here and in other work (e.g. (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008; Monroe et al., 2008)), it is clear that lexical evidence is one key to understanding how language is used to frame discussion from one perspective or another; Resnik and Greene (2009) have shown that syntactic c</context>
</contexts>
<marker>Entman, 1993</marker>
<rawString>R.M. Entman. 1993. Framing: Toward Clarification of a Fractured Paradigm. The Journal of Communication, 43(4):51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 503–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>848--855</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3061" citStr="Haghighi and Klein, 2007" startWordPosition="474" endWordPosition="477">st to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised naive Bayes classification nonparametric in order to improve perspective modeling. Intuitively, naive Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive naive Bayes (ANB), </context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848–855, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="20413" citStr="Johnson and Goldwater, 2009" startWordPosition="3427" endWordPosition="3430">er takes in a set of documents and replaces all words not in the vocabulary with “out of vocabulary” markers. This process ensures that in all experiments the vocabulary is composed entirely of words from the training set. After the groups have been filtered, the group used as the test set has its labels removed. The test and training set are then sent, along with the grammar, into the adaptor grammar inference engine. Each experiment ran for 3000 iterations. For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009). We use the resulting sentence parses for classification. By design of the grammar, each sentence’s words will belong to one and only one distribution. We identify that distribution from each of the test set sentence parses and use it as the sentence level classification for that particular sentence. We then use majority rule on the individual sentence classifications in a document to obtain the document classification. (In most cases the sentence-level assignments are overwhelmingly dominated by one class.) 3.3 Results and Analysis Table 4 gives the results and compares to prior work. The su</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2006</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="836" citStr="Johnson et al., 2006" startWordPosition="113" endWordPosition="116"> of Maryland College Park, MD jbg@umiacs.umd.edu Philip Resnik Department of Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu Abstract Strong indications of perspective can often come from collocations of arbitrary length; for example, someone writing get the government out of my X is typically expressing a conservative rather than progressive viewpoint. However, going beyond unigram or bigram features in perspective classification gives rise to problems of data sparsity. We address this problem using nonparametric Bayesian modeling, specifically adaptor grammars (Johnson et al., 2006). We demonstrate that an adaptive na¨ıve Bayes model captures multiword lexical usages associated with perspective, and establishes a new state-of-the-art for perspective classification results using the Bitter Lemons corpus, a collection of essays about mid-east issues from Israeli and Palestinian points of view. 1 Introduction Most work on the computational analysis of sentiment and perspective relies on lexical features. This makes sense, since an author’s choice of words is often used to express overt opinions (e.g. describing healthcare reform as idiotic or wonderful) or to frame a discus</context>
<context position="3176" citStr="Johnson et al., 2006" startWordPosition="491" endWordPosition="494">w” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised naive Bayes classification nonparametric in order to improve perspective modeling. Intuitively, naive Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive naive Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined</context>
<context position="4777" citStr="Johnson et al., 2006" startWordPosition="742" endWordPosition="745"> collection that has been used by a variety of authors to evaluate perspective classification. In Section 2, we review adaptor grammars, show how naive Bayes can be expressed within the formalism, and describe how — and how easily — an adaptive naive Bayes model can be created. Section 3 validates the approach via experimentation on the Bitter Lemons corpus. In Section 4, we summarize the contributions of the paper and discuss directions for future work. 2 Adapting Naive Bayes to be Less Naive In this work we apply the adaptor grammar formalism introduced by Johnson, Griffiths, and Goldwater (Johnson et al., 2006). Adaptor grammars are a generalization of probabilistic context free grammars (PCFGs) that make it particularly easy to express nonparametric Bayesian models of language simply and readably using context free rules. Moreover, Johnson et al. provide an inference procedure based on Markov Chain Monte Carlo techniques that makes parameter estimation straightforward for all models that can be expressed using adaptor grammars.1 Variational inference for adaptor grammars has also been recently introduced (Cohen et al., 2010). Briefly, adaptor grammars allow nonterminals to be rewritten to entire su</context>
<context position="27018" citStr="Johnson et al. (2006)" startWordPosition="4486" endWordPosition="4489">rocessing step such as computing tf-idf or other measures of frequency based on either word bigrams (Tan et al., 2002) or character n-grams (Raskutti et al., 2001). In contrast, our approach combines phrase discovery with the probabilistic model of the text. This allows for the collocation selection and classification to be expressed in a single model, which can then be extended later; it also is truly generative, as compared with feature induction and selection algorithms that either under- or over-generate data. There are a number of interesting directions in which to take this research. As Johnson et al. (2006) argue, and as we have confirmed here, the adaptor 290 Guest Editor Israeli Palestinian Israeli Palestinian palestinian OOV 11 299 palestinian isra 6 178 palestinian OOV 8 254 OOV israel 7 198 OOV palestinian 7 405 OOV palestinian 6 405 OOV palestinian 7 319 OOV palestinian 6 319 isra OOV 6 178 palestinian OOV 5 29 OOV israel 7 123 OOV work 5 254 israel OOV 6 94 one OOV 4 25 OOV us 6 115 OOV agreement 5 75 sharon OOV 4 74 side OOV 3 21 OOV part 5 56 palestinian reform 4 49 polit OOV 4 143 polit OOV 3 299 israel OOV 5 81 palestinian OOV 4 81 OOV us 4 29 peopl go 3 407 attempt OOV 5 91 OOV isra </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2006</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>398--406</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3310" citStr="Johnson, 2008" startWordPosition="512" endWordPosition="513">y of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised naive Bayes classification nonparametric in order to improve perspective modeling. Intuitively, naive Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive naive Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, MIT, Massachusetts, USA, 9</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proceedings of ACL-08: HLT, pages 398– 406, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>1148--1157</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3403" citStr="Johnson, 2010" startWordPosition="526" endWordPosition="527"> known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised naive Bayes classification nonparametric in order to improve perspective modeling. Intuitively, naive Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive naive Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics by the properties of the d</context>
<context position="13260" citStr="Johnson (2010)" startWordPosition="2206" endWordPosition="2207">n healthcare reform. Simply applying naive Bayes, or any other model, to a bag of n-grams for high n is 286 (a) Naive Bayes (b) Adaptive Naive Bayes α θ zd λ Wd,n φi Nd K M α θ zd a b Wd,n φr Nd M Ω K τ Figure 1: A plate diagram for naive Bayes and adaptive naive Bayes. Nodes represent random variables and parameters; shaded nodes represent observations; lines represent probabilistic dependencies; and the rectangular plates denote replication. going to lead to unworkable levels of data sparsity; a model should be flexible enough to support both unigrams and longer phrases as needed. Following Johnson (2010), however, we can use adaptor grammars to extend naive Bayes flexibly to include richer structure like collocations when they improve the model, and not including them when they do not. This can be accomplished by introducing adapted nonterminal rules: in a revised generative process, the author can draw from Pitman-Yor distribution whose base distribution is over word sequences of arbitrary length.4 Thus in a setting where, say, K = 2, and our two classes are PROGRESSIVE and CONSERVATIVE, the sequence health care for all might be generated as a single unit for the progressive perspective, but</context>
<context position="15266" citStr="Johnson (2010)" startWordPosition="2538" endWordPosition="2539">ussed when we introduced the Pitman-Yor distribution, there are two restaurants, one for the PROGRESSIVE distribution and one for the CONSERVATIVE distribution. Health care for all has its own table in the PROGRESSIVE restaurant, and enough people are sitting at it to make it popular. There is no such table in the CONSERVATIVE restaurant, so in order to generate those words, the phrase health care for all would need to come from a new table; however, it is more easily explained by three customers sitting at three existing, popular tables: health care, for, and all. We follow the convention of Johnson (2010) by writing adapted nonterminals as underlined. The grammar for adaptive naive Bayes is shown in Table 2. The adapted COLLOCi rule means that every time we need to generate that nonterminal, we are actually drawing from a distribution drawn from a Pitman-Yor distribution. The distribution over the possible yields of the WORDSi rule serves as the base distribution. Given this generative process for documents, we can now use statistical inference to uncover the posterior distribution over the latent variables, thus discovering the tables and seating assignments of our metaphorical restaurants th</context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In Proceedings of the Association for Computational Linguistics, pages 1148–1157, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>Kenneth Benoit</author>
<author>John Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data. American Political Science Review,</title>
<date>2003</date>
<pages>311--331</pages>
<contexts>
<context position="1724" citStr="Laver et al., 2003" startWordPosition="253" endWordPosition="256"> from Israeli and Palestinian points of view. 1 Introduction Most work on the computational analysis of sentiment and perspective relies on lexical features. This makes sense, since an author’s choice of words is often used to express overt opinions (e.g. describing healthcare reform as idiotic or wonderful) or to frame a discussion in order to convey a perspective more implicitly (e.g. using the term death tax instead of estate tax). Moreover, it is easy and efficient to represent texts as collections of the words they contain, in order to apply a well known arsenal of supervised techniques (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008). At the same time, standard lexical features have their limitations for this kind of analysis. Such features are usually created by selecting some small n-gram size in advance. Indeed, it is not uncommon to see the feature space for sentiment analysis limited to unigrams. However, important indicators of perspective can also be longer (get the government out of my). Trying to capture these using standard machine learning approaches creates a problem, since allowing n-grams as features for larger n gives rise to problems of data sparsity. In this pape</context>
<context position="30138" citStr="Laver et al., 2003" startWordPosition="5091" endWordPosition="5094">daptation for this variety of latent structure, to produce an adapted LSPM-like model analogous to adaptive naive Bayes. Viewed in a larger context, computational classification of perspective is closely connected to social scientists’ study of framing, which Entman (1993) characterizes as follows: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.” Here and in other work (e.g. (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008; Monroe et al., 2008)), it is clear that lexical evidence is one key to understanding how language is used to frame discussion from one perspective or another; Resnik and Greene (2009) have shown that syntactic choices can provide important evidence, as well. Another promising direction for this work is the application of adaptor grammar models as a way to capture both lexical and grammatical aspects of framing in a unified model. Acknowledgments This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF-</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>Michael Laver, Kenneth Benoit, and John Garry. 2003. Extracting policy positions from political texts using words as data. American Political Science Review, pages 311–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398,</booktitle>
<pages>4--15</pages>
<publisher>Springer Verlag,</publisher>
<location>Chemnitz, DE.</location>
<contexts>
<context position="9153" citStr="Lewis, 1998" startWordPosition="1493" endWordPosition="1494">ver, there is always a chance of drawing from the base distribution, and therefore every word sequence can also always be drawn from 0. In the next section we will write a naive Bayes-like generative process using PCFGs. We will then use the PCFG distribution as the base distribution for a Pitman-Yor distribution, adapting the naive Bayes process to give us a distribution over n-grams, thus learning new language substructures that are useful for modeling the differences in perspective. 2.1 Classification Models as PCFGs Naive Bayes is a venerable and popular mechanism for text classification (Lewis, 1998). It posits that there are K distinct categories of text — each with a distinct distribution over words — and that every document, represented as an exchangeable bag of words, is drawn from one (and only one) of these distributions. Learning the per-category word distributions and global prevalence of the classes is a problem of posterior inference which can be approached using a variety of inference techniques (Lowd and Domingos, 2005). More formally, naive Bayes models can be expressed via the following generative process:3 1. Draw a global distribution over classes 0 ∼ Dir (α) 2. For each c</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (bayes) at forty: The independence assumption in information retrieval. In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398, pages 4–15, Chemnitz, DE. Springer Verlag, Heidelberg, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of Emperical Methods in Natural Language Processing,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="2926" citStr="Liang et al., 2007" startWordPosition="451" endWordPosition="454">ity. In this paper, we employ nonparametric Bayesian models (Orbanz and Teh, 2010) in order to address this limitation. In contrast to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised naive Bayes classification nonparametric in order to improve perspective modeling. Intuitively, naive </context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of Emperical Methods in Natural Language Processing, pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? Identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="4144" citStr="Lin et al., 2006" startWordPosition="634" endWordPosition="637">ayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive naive Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics by the properties of the dataset. We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al., 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. In Section 2, we review adaptor grammars, show how naive Bayes can be expressed within the formalism, and describe how — and how easily — an adaptive naive Bayes model can be created. Section 3 validates the approach via experimentation on the Bitter Lemons corpus. In Section 4, we summarize the contributions of the paper and discuss directions for future work. 2 Adapting Naive Bayes to be Less Naive In this work we apply the adaptor grammar formalism introduced by Johnson, Griffiths, and</context>
<context position="17892" citStr="Lin et al. (2006)" startWordPosition="3008" endWordPosition="3011"> preparation and experimental setup. Briefly, using a shared base distribution posits that the two classes use similar word distributions, but generate collocations unique to each class, whereas using separate base distributions assumes that the distribution of words is unique to each class. 3 Experiments 3.1 Corpus Description We conducted our classification experiments on the Bitter Lemons (BL) corpus, which is a collection of 297 essays averaging 700-800 words in length, on various Middle East issues, written from both the Israeli and Palestinian perspectives. The BL corpus was compiled by Lin et al. (2006) and is derived from a website that invites weekly discussions on a topic and publishes essays from two sets of authors each week.5 Two of the authors are guests, one from each perspective, and two essays are from the site’s regular contributors, also one from each perspective, for a 5http://www.bitterlemons.org total of four essays on each topic per week. We chose this corpus to allow us to directly compare our results with Greene and Resnik’s (2009) Observable Proxies for Underlying Semantics (OPUS) features and Lin et al.’s Latent Sentence Perspective Model (LSPM). The classification goal f</context>
<context position="21103" citStr="Lin et al. (2006)" startWordPosition="3540" endWordPosition="3543">he grammar, each sentence’s words will belong to one and only one distribution. We identify that distribution from each of the test set sentence parses and use it as the sentence level classification for that particular sentence. We then use majority rule on the individual sentence classifications in a document to obtain the document classification. (In most cases the sentence-level assignments are overwhelmingly dominated by one class.) 3.3 Results and Analysis Table 4 gives the results and compares to prior work. The support vector machine (SVM), NBB and LSPM results are taken directly from Lin et al. (2006). NB-B indicates naive Bayes with full Bayesian inference. LSPM is the Latent Sentence Perspective Model, also from Lin et al. (2006). OPUS results are taken from Greene 6In these experiments, a frequency threshold of 4 was selected prior to testing. Training Set Test Set Classifier Accuracy Guests Editors SVM 88.22 Guests Editors NB-B 93.46 Guests Editors LSPM 94.93 Guests Editors OPUS 97.64 Guests Editors ANB* 99.32 Guests Editors ANB Com 99.93 Guests Editors ANB Sep 99.87 Editors Guests SVM 81.48 Editors Guests NB-B 85.85 Editors Guests LSPM 86.99 Editors Guests OPUS 85.86 Editors Guests AN</context>
<context position="28934" citStr="Lin et al. (2006)" startWordPosition="4874" endWordPosition="4877">umn indicates the raw count of that bigram in the Guests or Editors group. grammar formalism makes it quite easy to work with latent variable models, in order to automatically discover structures in the data that have predictive value. For example, it is easy to imagine a model where in addition to a word distribution for each class, there is also an additional shared “neutral” distribution: for each sentence, the words in that sentence can either come from the class-specific content distribution or the shared neutral distribution. This turns out to be the Latent Sentence Perspective Model of Lin et al. (2006), which is straightforward to encode using the adaptor grammar formalism simply by introducing two new nonterminals to represent the neutral distribution: SENT 7→ DOCd d = 1, ... ,m DOCd 7→ IDd WORDSi d = 1, ... ,m; i ∈ {1, K} DOCd 7→ IDd NEUTS d = 1, ... ,m; WORDSi 7→ WORDSi WORDi i ∈ {1, K} WORDSi 7→ WORDi i ∈ {1, K} WORDi 7→ v v ∈ V ; i ∈ {1, K} NEUT 7→ NEUTSi NEUTi NEUT 7→ NEUT NEUT 7→ v v ∈ V Running this grammar did not produce improvements consistent with those reported by Lin et al. We plan to investigate this further, and a natural follow-on would be to experiment with adaptation for </context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? Identifying perspectives at the document and sentence levels. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lowd</author>
<author>Pedro Domingos</author>
</authors>
<title>Naive bayes models for probability estimation.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>529--536</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9593" citStr="Lowd and Domingos, 2005" startWordPosition="1564" endWordPosition="1567">s that are useful for modeling the differences in perspective. 2.1 Classification Models as PCFGs Naive Bayes is a venerable and popular mechanism for text classification (Lewis, 1998). It posits that there are K distinct categories of text — each with a distinct distribution over words — and that every document, represented as an exchangeable bag of words, is drawn from one (and only one) of these distributions. Learning the per-category word distributions and global prevalence of the classes is a problem of posterior inference which can be approached using a variety of inference techniques (Lowd and Domingos, 2005). More formally, naive Bayes models can be expressed via the following generative process:3 1. Draw a global distribution over classes 0 ∼ Dir (α) 2. For each class i ∈ {1, ... , K}, draw a word distribution Oi ∼ Dir (A) 3. For each document d ∈ {1, ... , M}: (a) Draw a class assignment zd ∼ Mult (0) (b) For each word position n ∈ {1, ... , Nd, draw wd,n ∼ Mult (0zd) A variant of the naive Bayes generative process can be expressed using the adaptor grammar formalism (Table 1). The left hand side of each rule represents a nonterminal which can be expanded, and the right hand side represents the</context>
</contexts>
<marker>Lowd, Domingos, 2005</marker>
<rawString>Daniel Lowd and Pedro Domingos. 2005. Naive bayes models for probability estimation. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 529–536, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burt L Monroe</author>
<author>Michael P Colaresi</author>
<author>Kevin M Quinn</author>
</authors>
<title>Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict.</title>
<date>2008</date>
<journal>Political Analysis,</journal>
<volume>16</volume>
<pages>372--403</pages>
<contexts>
<context position="30202" citStr="Monroe et al., 2008" startWordPosition="5104" endWordPosition="5107">adapted LSPM-like model analogous to adaptive naive Bayes. Viewed in a larger context, computational classification of perspective is closely connected to social scientists’ study of framing, which Entman (1993) characterizes as follows: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.” Here and in other work (e.g. (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008; Monroe et al., 2008)), it is clear that lexical evidence is one key to understanding how language is used to frame discussion from one perspective or another; Resnik and Greene (2009) have shown that syntactic choices can provide important evidence, as well. Another promising direction for this work is the application of adaptor grammar models as a way to capture both lexical and grammatical aspects of framing in a unified model. Acknowledgments This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF-09-2-0072 and by the Office of the Director of National Intellig</context>
</contexts>
<marker>Monroe, Colaresi, Quinn, 2008</marker>
<rawString>Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn. 2008. Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict. Political Analysis, Vol. 16, Issue 4, pp. 372-403, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Robert Malouf</author>
</authors>
<title>A preliminary investigation into sentiment analysis of informal political discourse.</title>
<date>2006</date>
<booktitle>In AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW),</booktitle>
<pages>159--162</pages>
<contexts>
<context position="1749" citStr="Mullen and Malouf, 2006" startWordPosition="257" endWordPosition="260">lestinian points of view. 1 Introduction Most work on the computational analysis of sentiment and perspective relies on lexical features. This makes sense, since an author’s choice of words is often used to express overt opinions (e.g. describing healthcare reform as idiotic or wonderful) or to frame a discussion in order to convey a perspective more implicitly (e.g. using the term death tax instead of estate tax). Moreover, it is easy and efficient to represent texts as collections of the words they contain, in order to apply a well known arsenal of supervised techniques (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008). At the same time, standard lexical features have their limitations for this kind of analysis. Such features are usually created by selecting some small n-gram size in advance. Indeed, it is not uncommon to see the feature space for sentiment analysis limited to unigrams. However, important indicators of perspective can also be longer (get the government out of my). Trying to capture these using standard machine learning approaches creates a problem, since allowing n-grams as features for larger n gives rise to problems of data sparsity. In this paper, we employ nonparametri</context>
<context position="30163" citStr="Mullen and Malouf, 2006" startWordPosition="5095" endWordPosition="5099">ariety of latent structure, to produce an adapted LSPM-like model analogous to adaptive naive Bayes. Viewed in a larger context, computational classification of perspective is closely connected to social scientists’ study of framing, which Entman (1993) characterizes as follows: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.” Here and in other work (e.g. (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008; Monroe et al., 2008)), it is clear that lexical evidence is one key to understanding how language is used to frame discussion from one perspective or another; Resnik and Greene (2009) have shown that syntactic choices can provide important evidence, as well. Another promising direction for this work is the application of adaptor grammar models as a way to capture both lexical and grammatical aspects of framing in a unified model. Acknowledgments This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF-09-2-0072 and by the Offi</context>
</contexts>
<marker>Mullen, Malouf, 2006</marker>
<rawString>Tony Mullen and Robert Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse. In AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW), pages 159–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Orbanz</author>
<author>Y W Teh</author>
</authors>
<title>Bayesian nonparametric models.</title>
<date>2010</date>
<booktitle>In Encyclopedia of Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="2389" citStr="Orbanz and Teh, 2010" startWordPosition="362" endWordPosition="365"> At the same time, standard lexical features have their limitations for this kind of analysis. Such features are usually created by selecting some small n-gram size in advance. Indeed, it is not uncommon to see the feature space for sentiment analysis limited to unigrams. However, important indicators of perspective can also be longer (get the government out of my). Trying to capture these using standard machine learning approaches creates a problem, since allowing n-grams as features for larger n gives rise to problems of data sparsity. In this paper, we employ nonparametric Bayesian models (Orbanz and Teh, 2010) in order to address this limitation. In contrast to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in</context>
</contexts>
<marker>Orbanz, Teh, 2010</marker>
<rawString>P. Orbanz and Y. W. Teh. 2010. Bayesian nonparametric models. In Encyclopedia of Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Annals of Probability,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="5900" citStr="Pitman and Yor, 1997" startWordPosition="916" endWordPosition="919">ed (Cohen et al., 2010). Briefly, adaptor grammars allow nonterminals to be rewritten to entire subtrees. In contrast, a nonterminal in a PCFG rewrites only to a collection of grammar symbols; their subsequent productions are independent of each other. For instance, a traditional PCFG might learn probabilities for the rewrite rule PP H P NP. In contrast, an adaptor grammar can learn (or “cache”) the production PP H (P up)(NP(DET a)(N tree)). It does this by positing that the distribution over children for an adapted non-terminal comes from a Pitman-Yor distribution. A Pitman-Yor distribution (Pitman and Yor, 1997) is a distribution over distributions. It has three parameters: the discount, a, such that 0 &lt; a &lt; 1, the strength, b, a real number such that −a &lt; b, 1And, better still, they provide code that implements the inference algorithm; see http://www.cog.brown.edu/ mj/Software.htm. and a probability distribution G0 known as the base distribution. Adaptor grammars allow distributions over subtrees to come from a Pitman-Yor distribution with the PCFG’s original distribution over trees as the base distribution. The generative process for obtaining draws from a distribution drawn from a Pitman-Yor distr</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25(2):855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhavani Raskutti</author>
<author>Herman L Ferr´a</author>
<author>Adam Kowalczyk</author>
</authors>
<title>Second order features for maximising text classification performance.</title>
<date>2001</date>
<booktitle>In EMCL ’01: Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>419--430</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<marker>Raskutti, Ferr´a, Kowalczyk, 2001</marker>
<rawString>Bhavani Raskutti, Herman L. Ferr´a, and Adam Kowalczyk. 2001. Second order features for maximising text classification performance. In EMCL ’01: Proceedings of the 12th European Conference on Machine Learning, pages 419–430, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Eric Hardisty</author>
</authors>
<title>Gibbs sampling for the uninitiated.</title>
<date>2010</date>
<tech>Technical Report UMIACS-TR-2010-04,</tech>
<institution>University of Maryland.</institution>
<note>http://www.lib.umd.edu/drum/handle/1903/10058.</note>
<contexts>
<context position="10584" citStr="Resnik and Hardisty (2010)" startWordPosition="1748" endWordPosition="1751">0zd) A variant of the naive Bayes generative process can be expressed using the adaptor grammar formalism (Table 1). The left hand side of each rule represents a nonterminal which can be expanded, and the right hand side represents the rewrite rule. The rightmost indices show replication; for instance, there are |V | rules that allow WORDi to rewrite to each word in the 3Here α and A are hyperparameters used to specify priors for the class distribution and classes’ word distributions, respectively; α is a symmetric K-dimensional vector where each element is ir. Nd is the length of document d. Resnik and Hardisty (2010) provide a tutorial introduction to the naive Bayes generative process and underlying concepts. SENT 7→ DOCd d = 1, ... ,m DOCd0.001 7→IDd WORDSi d = 1, ... , m; i ∈ {1, K} WORDSi 7→ WORDSi WORDi i ∈ {1, K} WORDSi 7→ WORDi i ∈ {1, K} WORDi 7→ v v ∈ V ; i ∈ {1, K} Table 1: A naive Bayes-inspired model expressed as a PCFG. vocabulary. One can assume a symmetric Dirichlet prior of Dir (1) over the production choices unless otherwise specified — as with the DOCd production rule above, where a sparse prior is used. Notice that the distribution over expansions for WORDi corresponds directly to Oi in</context>
</contexts>
<marker>Resnik, Hardisty, 2010</marker>
<rawString>Philip Resnik and Eric Hardisty. 2010. Gibbs sampling for the uninitiated. Technical Report UMIACS-TR-2010-04, University of Maryland. http://www.lib.umd.edu/drum/handle/1903/10058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chade-Meng Tan</author>
<author>Yuan-Fang Wang</author>
<author>Chan-Do Lee</author>
</authors>
<title>The use of bigrams to enhance text categorization.</title>
<date>2002</date>
<journal>Inf. Process. Manage.,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="26515" citStr="Tan et al., 2002" startWordPosition="4404" endWordPosition="4407">of text and improve document classification according to perspective, by allowing nonparametric discovery of collocations that aid in perspective classification. The adaptive naive Bayes model improves on state of the art supervised classification performance in head-to-head comparisons with previous approaches. Although there have been many investigations on the efficacy of using multiword collocations in text classification (Bekkerman and Allan, 2004), usually such approaches depend on a preprocessing step such as computing tf-idf or other measures of frequency based on either word bigrams (Tan et al., 2002) or character n-grams (Raskutti et al., 2001). In contrast, our approach combines phrase discovery with the probabilistic model of the text. This allows for the collocation selection and classification to be expressed in a single model, which can then be extended later; it also is truly generative, as compared with feature induction and selection algorithms that either under- or over-generate data. There are a number of interesting directions in which to take this research. As Johnson et al. (2006) argue, and as we have confirmed here, the adaptor 290 Guest Editor Israeli Palestinian Israeli P</context>
</contexts>
<marker>Tan, Wang, Lee, 2002</marker>
<rawString>Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee. 2002. The use of bigrams to enhance text categorization. Inf. Process. Manage., 38(4):529–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="2814" citStr="Teh et al., 2006" startWordPosition="433" endWordPosition="436">oaches creates a problem, since allowing n-grams as features for larger n gives rise to problems of data sparsity. In this paper, we employ nonparametric Bayesian models (Orbanz and Teh, 2010) in order to address this limitation. In contrast to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make s</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Yu</author>
<author>S Kaufmann</author>
<author>D Diermeier</author>
</authors>
<title>Classifying party affiliation from political speech.</title>
<date>2008</date>
<journal>Journal of Information Technology and Politics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1767" citStr="Yu et al., 2008" startWordPosition="261" endWordPosition="264"> 1 Introduction Most work on the computational analysis of sentiment and perspective relies on lexical features. This makes sense, since an author’s choice of words is often used to express overt opinions (e.g. describing healthcare reform as idiotic or wonderful) or to frame a discussion in order to convey a perspective more implicitly (e.g. using the term death tax instead of estate tax). Moreover, it is easy and efficient to represent texts as collections of the words they contain, in order to apply a well known arsenal of supervised techniques (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008). At the same time, standard lexical features have their limitations for this kind of analysis. Such features are usually created by selecting some small n-gram size in advance. Indeed, it is not uncommon to see the feature space for sentiment analysis limited to unigrams. However, important indicators of perspective can also be longer (get the government out of my). Trying to capture these using standard machine learning approaches creates a problem, since allowing n-grams as features for larger n gives rise to problems of data sparsity. In this paper, we employ nonparametric Bayesian models </context>
<context position="30180" citStr="Yu et al., 2008" startWordPosition="5100" endWordPosition="5103">e, to produce an adapted LSPM-like model analogous to adaptive naive Bayes. Viewed in a larger context, computational classification of perspective is closely connected to social scientists’ study of framing, which Entman (1993) characterizes as follows: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.” Here and in other work (e.g. (Laver et al., 2003; Mullen and Malouf, 2006; Yu et al., 2008; Monroe et al., 2008)), it is clear that lexical evidence is one key to understanding how language is used to frame discussion from one perspective or another; Resnik and Greene (2009) have shown that syntactic choices can provide important evidence, as well. Another promising direction for this work is the application of adaptor grammar models as a way to capture both lexical and grammatical aspects of framing in a unified model. Acknowledgments This research was funded in part by the Army Research Laboratory through ARL Cooperative Agreement W911NF-09-2-0072 and by the Office of the Directo</context>
</contexts>
<marker>Yu, Kaufmann, Diermeier, 2008</marker>
<rawString>B. Yu, S. Kaufmann, and D. Diermeier. 2008. Classifying party affiliation from political speech. Journal of Information Technology and Politics, 5(1):33–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>