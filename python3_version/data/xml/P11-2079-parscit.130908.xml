<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008645">
<title confidence="0.94991">
Reordering Modeling using Weighted Alignment Matrices
</title>
<author confidence="0.834661">
Wang Ling, Tiago Luis, Jo˜ao Grac¸a, Luisa Coheur and Isabel Trancoso
</author>
<note confidence="0.644857">
L2F Spoken Systems Lab
INESC-ID Lisboa
</note>
<bodyText confidence="0.2814435">
{wang.ling,tiago.luis,joao.graca}@inesc-id.pt
{luisa.coheur,isabel.trancoso}@inesc-id.pt
</bodyText>
<sectionHeader confidence="0.987708" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998785166666667">
In most statistical machine translation sys-
tems, the phrase/rule extraction algorithm uses
alignments in the 1-best form, which might
contain spurious alignment points. The usage
of weighted alignment matrices that encode all
possible alignments has been shown to gener-
ate better phrase tables for phrase-based sys-
tems. We propose two algorithms to generate
the well known MSD reordering model using
weighted alignment matrices. Experiments on
the IWSLT 2010 evaluation datasets for two
language pairs with different alignment algo-
rithms show that our methods produce more
accurate reordering models, as can be shown
by an increase over the regular MSD models
of 0.4 BLEU points in the BTEC French to
English test set, and of 1.5 BLEU points in the
DIALOG Chinese to English test set.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999344583333333">
The translation quality of statistical phrase-based
systems (Koehn et al., 2003) is heavily dependent
on the quality of the translation and reordering mod-
els generated during the phrase extraction algo-
rithm (Ling et al., 2010). The basic phrase extrac-
tion algorithm uses word alignment information to
constraint the possible phrases that can be extracted.
It has been shown that better alignment quality gen-
erally leads to better results (Ganchev et al., 2008).
However the relationship between the word align-
ment quality and the results is not straightforward,
and it was shown in (Vilar et al., 2006) that better
alignments in terms of F-measure do not always lead
to better translation quality.
The fact that spurious word alignments might oc-
cur leads to the use of alternative representations for
word alignments that allow multiple alignment hy-
potheses, rather than the 1-best alignment (Venu-
gopal et al., 2009; Mi et al., 2008; Christopher
Dyer et al., 2008). While using n-best alignments
yields improvements over using the 1-best align-
ment, these methods are computationally expen-
sive. More recently, the method described in (Liu
et al., 2009) produces improvements over the meth-
ods above, while reducing the computational cost
by using weighted alignment matrices to represent
the alignment distribution over each parallel sen-
tence. However, their results were limited by the
fact that they had no method for extracting a reorder-
ing model from these matrices, and used a simple
distance-based model.
In this paper, we propose two methods for gener-
ating the MSD (Mono Swap Discontinuous) reorder-
ing model from the weighted alignment matrices.
First, we test a simple approach by using the 1-best
alignment to generate the reordering model, while
using the alignment matrix to produce the translation
model. This reordering model is a simple adaptation
of the MSD model to read from alignment matrices.
Secondly, we develop two algorithms to infer the re-
ordering model from the weighted alignment matrix
probabilities. The first one uses the alignment infor-
mation within phrase pairs, while the second uses
contextual information of the phrase pairs.
This paper is organized as follows: Section 2 de-
scribes the MSD model; Section 3 presents our two
algorithms; in Section 4 we report the results from
the experiments conducted using these algorithms,
</bodyText>
<page confidence="0.964265">
450
</page>
<note confidence="0.586099">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450–454,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9860425">
and comment on the results; we conclude in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.969886" genericHeader="method">
2 MSD models
</sectionHeader>
<bodyText confidence="0.999472571428572">
Moses (Koehn et al., 2007) allows many config-
urations for the reordering model to be used. In
this work, we will only refer to the default config-
uration (msd-bidirectional-fe), which uses the MSD
model, and calculates the reordering orientation for
the previous and the next word, for each phrase pair.
Other possible configurations are simpler than the
default one. For instance, the monotonicity model
only considers monotone and non-monotone orien-
tation types, whereas the MSD model also considers
the monotone orientation type, but distinguishes the
non-monotone orientation type between swap and
discontinuous. The approach presented in this work
can be adapted to the other configurations.
In the MSD model, during the phrase extraction,
given a source sentence S and a target sentence T,
the alignment set A, where aji is an alignment from i
to j, the phrase pair with words in positions between
i and j in S, Sji , and n and m in T, Tnm , can be
classified with one of three orientations with respect
to the previous word:
</bodyText>
<listItem confidence="0.950812">
• The orientation is monotonous if only the pre-
vious word in the source is aligned with the pre-
vious word in the target, or, more formally, if
an�1
i�1 E A n an�1
j+1 E� A.
• The orientation is swap, if only the next word
in the source is aligned with the previous word
in the target, or more formally, if an�1
j+1 E A n
an�1
i�1 E� A.
• The orientation is discontinuous if neither of
the above are true, which means, (an�1
</listItem>
<equation confidence="0.968913">
i�1 E
A n an�1
j+1 E A) V (an�1
i�1 E� A n an�1
j+1 E� A).
</equation>
<bodyText confidence="0.999903">
The orientations with respect to the next word are
given analogously. The reordering model is gener-
ated by grouping the phrase pairs that are equal, and
calculating the probabilities of the grouped phrase
pair being associated each orientation type and di-
rection, based on the orientations for each direction
that are extracted. Formally, the probability of the
phrase pair p having a monotonous orientation is
</bodyText>
<figureCaption confidence="0.99458875">
Figure 1: Enumeration of possible reordering cases with
respect to the previous word. Case a) is classified as
monotonous, case b) is classified as swap and cases c)
and d) are classified as discontinuous.
</figureCaption>
<equation confidence="0.915891666666667">
given by:
C(mono)
P(p, mono) = C(mono)+C(swap)+C(disc) (1)
</equation>
<bodyText confidence="0.9988675">
Where C(o) is the number of times a phrase is ex-
tracted with the orientation o in that group of phrase
pairs. Moses also provides many options for this
stage, such as types of smoothing. We use the de-
fault smoothing configuration which adds the fixed
value of 0.5 to all C(o).
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="method">
3 Weighted MSD Model
</sectionHeader>
<bodyText confidence="0.999971571428572">
When using a weighted alignment matrix, rather
than working with alignments points, we use the
probability of each word in the source aligning with
each word in the target. Thus, the regular MSD
model cannot be directly applied here.
One obvious solution to solve this problem is to
produce a 1-best alignment set along with the align-
ment matrix, and use the 1-best alignment to gen-
erate the reordering model, while using the align-
ment matrix to produce the translation model. How-
ever, this method would not be taking advantage of
the weighted alignment matrix. The following sub-
sections describe two algorithms that are proposed
to make use of the alignment probabilities.
</bodyText>
<subsectionHeader confidence="0.999697">
3.1 Score-based
</subsectionHeader>
<bodyText confidence="0.998053">
Each phrase pair that is extracted using the algorithm
described in (Liu et al., 2009) is given a score based
on its alignments. This score is higher if the align-
ment points in the phrase pair have high probabili-
ties, and if the alignment is consistent. Thus, if an
</bodyText>
<figure confidence="0.997157703703704">
prev
word(t)
prev
word(t)
target phrase
target phrase
next
word(s)
prev
word(s)
c)
d)
source phrase
source phrase
prev
word(t)
prev
word(t)
target phrase
target phrase
source phrase
source phrase
prev
word(s)
next
word(s)
a) b)
</figure>
<page confidence="0.995133">
451
</page>
<bodyText confidence="0.999978625">
extracted phrase pair has better quality, its orienta-
tion should have more weight than phrase pairs with
worse quality. We implement this by changing the
C(o) function in equation 1 from being the number
of the phrase pairs with the orientation o, to the sum
of the scores of those phrases. We also need to nor-
malize the scores for each group, due to the fixed
smoothing that is applied, since if the sum of the
scores is much lower (e.g. 0.1) than the smoothing
factor (0.5), the latter will overshadow the weight
of the phrase pairs. The normalization is done by
setting the phrase pair with the highest value of the
sum of all MSD probabilities to 1, and readjusting
other phrase pairs accordingly. Thus, a group of 3
phrase pairs that have the MSD probability sums of
0.1, 0.05 and 0.1, are all set to 1, 0.5 and 1.
</bodyText>
<subsectionHeader confidence="0.999326">
3.2 Context-based
</subsectionHeader>
<bodyText confidence="0.999943545454545">
We propose an alternative algorithm to calculate
the reordering orientations for each phrase pair.
Rather than classifying each phrase pair with either
monotonous (M), swap (S) or discontinuous (D),
we calculate the probability for each orientation, and
use these as weighted counts when creating the re-
ordering model. Thus, for the previous word, given
a weighted alignment matrix W, the phrase pair be-
tween the indexes i and j in S, Sji , and n and m in
T, Tnm , the probability values for each orientation
are given by:
</bodyText>
<listItem confidence="0.87171">
• Pc(M) = W n�1
</listItem>
<equation confidence="0.986548333333333">
i�1 × (1 − W n�1
j+1 )
• Pc(S) = W n�1
j+1 × (1 − W n�1
i�1 )
•
Pc(D) = W n�1
i�1 × Wn�1
j+1
+ (1 − Wn�1
i�1 ) × (1 − Wn�1
j+1 )
</equation>
<bodyText confidence="0.972528285714286">
These formulas derive from the adaptation of con-
ditions of each orientation presented in 2. In the
regular MSD model, the previous orientation for a
phrase pair is monotonous if the previous word in
the source phrase is aligned with the previous word
in the target phrase and not aligned with the next
word. Thus, the probability of a phrase pair to have a
monotonous orientation Pc(M) is given by the prob-
ability of the previous word in the source phrase
being aligned with the previous word in the target
phrase W n�1
i�1 , and the probability of the previous
word in the source to not be aligned with the next
word in the target (1 − W n�1
j+1 ). Also, the sum of
the probabilities of all orientations (Pc(M), Pc(S),
Pc(D)) for a given phrase pair can be trivially shown
to be 1. The probabilities for the next word are
given analogously. Following equation 1, the func-
tion C(o) is changed to be the sum of all Pc(o), from
the grouped phrase pairs.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987752">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9999944">
Our experiments were performed over two datasets,
the BTEC and the DIALOG parallel corpora from
the latest IWSLT evaluation 2010 (Paul et al., 2010).
BTEC is a multilingual speech corpus that contains
sentences related to tourism, such as the ones found
in phrasebooks. DIALOG is a collection of human-
mediated cross-lingual dialogs in travel situations.
The experiments performed with the BTEC cor-
pus used only the French-English subset, while the
ones perfomed with the DIALOG corpus used the
Chinese-English subset. The training corpora con-
tains about 19K sentences and 30K sentences, re-
spectively. The development corpus for the BTEC
task was the CSTAR03 test set composed by 506
sentences, and the test set was the IWSLT04 test set
composed by 500 sentences and 16 references. As
for the DIALOG task, the development set was the
IWSLT09 devset composed by 200 sentences, and
the test set was the CSTAR03 test set with 506 sen-
tences and 16 references.
</bodyText>
<subsectionHeader confidence="0.987419">
4.2 Setup
</subsectionHeader>
<bodyText confidence="0.999972384615385">
We use weighted alignment matrices based on Hid-
den Markov Models (HMMs), which are produced
by the the PostCAT toolkit1, based on the poste-
rior regularization framework (V. Grac¸a et al., 2010).
The extraction algorithm using weighted alignment
matrices employs the same method described in (Liu
et al., 2009), and the phrase pruning threshold was
set to 0.1. For the reordering model, we use the
distance-based reordering, and compare the results
with the MSD model using the 1-best alignment.
Then, we apply our two methods based on align-
ment matrices. Finally, we combine our two meth-
ods above by adapting the function C(o), to be the
</bodyText>
<footnote confidence="0.998144">
1http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html
</footnote>
<page confidence="0.998115">
452
</page>
<bodyText confidence="0.9999317">
sum of all P,(o), weighted by the scores of the re-
spective phrase pairs. The optimization of the trans-
lation model weights was done using MERT, and
each experiment was run 5 times, and the final score
is calculated as the average of the 5 runs, in order to
stabilize the results. Finally, the results were eval-
uated using BLEU-4, METEOR, TER and TERp.
The BLEU-4 and METEOR scores were computed
using 16 references. The TER and TERp were com-
puted using a single reference.
</bodyText>
<subsectionHeader confidence="0.999193">
4.3 Reordering model comparison
</subsectionHeader>
<bodyText confidence="0.999993305555555">
Tables 1 and 2 show the scores using the differ-
ent reordering models. Consistent improvements in
the BLEU scores may be observed when changing
from the MSD model to the models generated us-
ing alignment matrices. The results were consis-
tently better using our models in the DIALOG task,
since the English-Chinese language pair is more de-
pendent on the reordering model. This is evident
if we look at the difference in the scores between
the distance-based and the MSD models. Further-
more, in this task, we observe an improvement on all
scores from the MSD model to our weighted MSD
models, which suggests that the usage of alignment
matrices helps predict the reordering probabilities
more accurately.
We can also see that the context based reordering
model performs better than the score based model
in the BTEC task, which does not perform sig-
nificantly better than the regular MSD model in
this task. Furthermore, combining the score based
method with the context based method does not lead
to any improvements. We believe this is because the
alignment probabilities are much more accurate in
the English-French language pair, and phrase pair
scores remain consistent throughout the extraction,
making the score based approach and the regular
MSD model behave similarly. On the other hand,
in the DIALOG task, score based model has bet-
ter performance than the regular MSD model, and
the combination of both methods yields a significant
improvement over each method alone.
Table 3 shows a case where the context based
model is more accurate than the regular MSD model.
The alignment is obviously faulty, since the word
“two” is aligned with both “deux”, although it
should only be aligned with the first occurrence.
</bodyText>
<table confidence="0.999306">
BTEC BLEU METEOR TERp TER
Distance-based 61.84 65.38 27.60 22.40
MSD 62.02 65.93 27.40 22.80
score MSD 62.15 66.18 27.30 22.20
context MSD 62.42 66.29 27.00 22.00
combined MSD 62.42 66.14 27.10 22.20
</table>
<tableCaption confidence="0.954449">
Table 1: Results for the BTEC task.
</tableCaption>
<table confidence="0.9998935">
DIALOG BLEU METEOR TERp TER
Distance-based 36.29 45.15 49.00 41.20
MSD 39.56 46.85 47.20 39.60
score MSD 40.2 47.16 46.52 38.80
context MSD 40.14 47.14 45.88 39.00
combined MSD 41.03 47.69 46.20 38.20
</table>
<tableCaption confidence="0.996746">
Table 2: Results for the DIALOG task.
</tableCaption>
<bodyText confidence="0.999983611111111">
Furthermore, the word “twin” should be aligned
with “`a deux lit”, but it is aligned with “cham-
bres”. If we use the 1-best alignment to compute
the reordering type of the sentence pair “Je voudrais
r´eserver deux” / “I’d like to reserve two”, the re-
ordering type for the following orientation would
be monotonous, since the next word “chambres”
is falsely aligned with “twin”. However, it should
clearly be discontinuous, since the right alignment
for “twin” is “`a deux lit”. This problem is less seri-
ous when we use the weighted MSD model, since
the orientation probability mass would be divided
between monotonous and discontinuous since the
probability weighted matrix for the wrong alignment
is 0.5. On the BTEC task, some of the other scores
are lower than the MSD model, and we suspect that
this stems from the fact that our tuning process only
attempts to maximize the BLEU score.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.998742090909091">
In this paper we addressed the limitations of the
MSD reordering models extracted from the 1-best
alignments, and presented two algorithms to ex-
tract these models from weighted alignment matri-
ces. Experiments show that our models perform bet-
ter than the distance-based model and the regular
MSD model. The method based on scores showed a
good performance for the Chinese-English language
pair, but the performance for the English-French pair
was similar to the MSD model. On the other hand,
the method based on context improves the results on
</bodyText>
<page confidence="0.997568">
453
</page>
<table confidence="0.999524">
Alignment Je voudrais r´eserver deux chambres a` deux lits .
I 1
’d 0.7
like 0.7
to
reserve 1
two 1 0.5
twin 0.5 0.5
rooms 1
. 1
</table>
<tableCaption confidence="0.98262975">
Table 3: Weighted alignment matrix for a training sen-
tence pair from BTEC, with spurious alignment proba-
bilities. Alignment points with 0 probabilities are left
empty.
</tableCaption>
<bodyText confidence="0.998399">
both pairs. Finally, on the Chinese-English test, by
combining both methods we can achieve a BLEU
improvement of approximately 1.5%. The code used
in this work is currently integrated with the Geppetto
toolkit2 , and it will be made available in the next
version for public use.
</bodyText>
<sectionHeader confidence="0.998746" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999388">
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Tiago Lu´ıs is supported by
FCT grant SFRH/BD/62151/2009. The PhD the-
sis of Wang Ling is supported by FCT grant
SFRH/BD/51157/2010. The authors also wish to
thank the anonymous reviewers for many helpful
comments.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998902672413793">
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. Tech-
nical Report LAMP-TR-149, University of Maryland,
College Park, February.
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of ACL-08: HLT, pages 986–993, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
2http://code.google.com/p/geppetto/
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 48–54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Wang Ling, Tiago Lu´ıs, Joao Grac¸a, Lu´ısa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ’10:
International Workshop on Spoken Language Transla-
tion, pages 313–320, Paris, France.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2 - Volume 2, EMNLP ’09, pages 1017–1026,
Morristown, NJ, USA. Association for Computational
Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192–199, Columbus, Ohio, June. Association
for Computational Linguistics.
Michael Paul, Marcello Federico, and Sebastian St¨uker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ’10: International Workshop on Spo-
ken Language Translation, pages 3–27.
Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481–504.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Wider pipelines: N-best
alignments and parses in MT training.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
Aer: Do we need to ”improve” our alignments? In
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 205–212.
</reference>
<page confidence="0.99917">
454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.247212">
<title confidence="0.929261333333333">Reordering Modeling using Weighted Alignment Matrices Ling, Tiago Luis, Luisa Coheur and Isabel Spoken Systems</title>
<email confidence="0.318181">INESC-ID</email>
<abstract confidence="0.988415">In most statistical machine translation systems, the phrase/rule extraction algorithm uses alignments in the 1-best form, which might contain spurious alignment points. The usage of weighted alignment matrices that encode all possible alignments has been shown to generate better phrase tables for phrase-based systems. We propose two algorithms to generate the well known MSD reordering model using weighted alignment matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing Word Lattice Translation.</title>
<date>2008</date>
<tech>Technical Report LAMP-TR-149,</tech>
<institution>University of Maryland, College Park,</institution>
<contexts>
<context position="2025" citStr="Dyer et al., 2008" startWordPosition="304" endWordPosition="307">xtracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignment quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better alignments in terms of F-measure do not always lead to better translation quality. The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propose two methods for generating th</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Translation. Technical Report LAMP-TR-149, University of Maryland, College Park, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Better alignments = better translations?</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>986--993</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Ganchev, Grac¸a, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008. Better alignments = better translations? In Proceedings of ACL-08: HLT, pages 986–993, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1141" citStr="Koehn et al., 2003" startWordPosition="160" endWordPosition="163">to generate better phrase tables for phrase-based systems. We propose two algorithms to generate the well known MSD reordering model using weighted alignment matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The translation quality of statistical phrase-based systems (Koehn et al., 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al., 2010). The basic phrase extraction algorithm uses word alignment information to constraint the possible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignment quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better alignments in terms of F-measure do not always lead to better tra</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondrej Bojar</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3717" citStr="Koehn et al., 2007" startWordPosition="567" endWordPosition="570">ne uses the alignment information within phrase pairs, while the second uses contextual information of the phrase pairs. This paper is organized as follows: Section 2 describes the MSD model; Section 3 presents our two algorithms; in Section 4 we report the results from the experiments conducted using these algorithms, 450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450–454, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics and comment on the results; we conclude in Section 5. 2 MSD models Moses (Koehn et al., 2007) allows many configurations for the reordering model to be used. In this work, we will only refer to the default configuration (msd-bidirectional-fe), which uses the MSD model, and calculates the reordering orientation for the previous and the next word, for each phrase pair. Other possible configurations are simpler than the default one. For instance, the monotonicity model only considers monotone and non-monotone orientation types, whereas the MSD model also considers the monotone orientation type, but distinguishes the non-monotone orientation type between swap and discontinuous. The approa</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondrej Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
</authors>
<title>Tiago Lu´ıs, Joao Grac¸a, Lu´ısa Coheur, and Isabel Trancoso.</title>
<date>2010</date>
<booktitle>In IWSLT ’10: International Workshop on Spoken Language Translation,</booktitle>
<pages>313--320</pages>
<location>Paris, France.</location>
<marker>Ling, 2010</marker>
<rawString>Wang Ling, Tiago Lu´ıs, Joao Grac¸a, Lu´ısa Coheur, and Isabel Trancoso. 2010. Towards a general and extensible phrase-extraction algorithm. In IWSLT ’10: International Workshop on Spoken Language Translation, pages 313–320, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Tian Xia</author>
<author>Xinyan Xiao</author>
<author>Qun Liu</author>
</authors>
<title>Weighted alignment matrices for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>1017--1026</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2212" citStr="Liu et al., 2009" startWordPosition="332" endWordPosition="335">lts is not straightforward, and it was shown in (Vilar et al., 2006) that better alignments in terms of F-measure do not always lead to better translation quality. The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propose two methods for generating the MSD (Mono Swap Discontinuous) reordering model from the weighted alignment matrices. First, we test a simple approach by using the 1-best alignment to generate the reordering model, whi</context>
<context position="6955" citStr="Liu et al., 2009" startWordPosition="1132" endWordPosition="1135">n the target. Thus, the regular MSD model cannot be directly applied here. One obvious solution to solve this problem is to produce a 1-best alignment set along with the alignment matrix, and use the 1-best alignment to generate the reordering model, while using the alignment matrix to produce the translation model. However, this method would not be taking advantage of the weighted alignment matrix. The following subsections describe two algorithms that are proposed to make use of the alignment probabilities. 3.1 Score-based Each phrase pair that is extracted using the algorithm described in (Liu et al., 2009) is given a score based on its alignments. This score is higher if the alignment points in the phrase pair have high probabilities, and if the alignment is consistent. Thus, if an prev word(t) prev word(t) target phrase target phrase next word(s) prev word(s) c) d) source phrase source phrase prev word(t) prev word(t) target phrase target phrase source phrase source phrase prev word(s) next word(s) a) b) 451 extracted phrase pair has better quality, its orientation should have more weight than phrase pairs with worse quality. We implement this by changing the C(o) function in equation 1 from b</context>
<context position="11124" citStr="Liu et al., 2009" startWordPosition="1881" endWordPosition="1884"> test set composed by 506 sentences, and the test set was the IWSLT04 test set composed by 500 sentences and 16 references. As for the DIALOG task, the development set was the IWSLT09 devset composed by 200 sentences, and the test set was the CSTAR03 test set with 506 sentences and 16 references. 4.2 Setup We use weighted alignment matrices based on Hidden Markov Models (HMMs), which are produced by the the PostCAT toolkit1, based on the posterior regularization framework (V. Grac¸a et al., 2010). The extraction algorithm using weighted alignment matrices employs the same method described in (Liu et al., 2009), and the phrase pruning threshold was set to 0.1. For the reordering model, we use the distance-based reordering, and compare the results with the MSD model using the 1-best alignment. Then, we apply our two methods based on alignment matrices. Finally, we combine our two methods above by adapting the function C(o), to be the 1http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html 452 sum of all P,(o), weighted by the scores of the respective phrase pairs. The optimization of the translation model weights was done using MERT, and each experiment was run 5 times, and the final score is calculated as</context>
</contexts>
<marker>Liu, Xia, Xiao, Liu, 2009</marker>
<rawString>Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009. Weighted alignment matrices for statistical machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 1017–1026, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1993" citStr="Mi et al., 2008" startWordPosition="299" endWordPosition="302">ossible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignment quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better alignments in terms of F-measure do not always lead to better translation quality. The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propo</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08: HLT, pages 192–199, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Marcello Federico</author>
<author>Sebastian St¨uker</author>
</authors>
<title>Overview of the iwslt 2010 evaluation campaign.</title>
<date>2010</date>
<booktitle>In IWSLT ’10: International Workshop on Spoken Language Translation,</booktitle>
<pages>3--27</pages>
<marker>Paul, Federico, St¨uker, 2010</marker>
<rawString>Michael Paul, Marcello Federico, and Sebastian St¨uker. 2010. Overview of the iwslt 2010 evaluation campaign. In IWSLT ’10: International Workshop on Spoken Language Translation, pages 3–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao V Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Learning Tractable Word Alignment Models with Complex Constraints.</title>
<date>2010</date>
<journal>Comput. Linguist.,</journal>
<pages>36--481</pages>
<marker>Grac¸a, Ganchev, Taskar, 2010</marker>
<rawString>Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar. 2010. Learning Tractable Word Alignment Models with Complex Constraints. Comput. Linguist., 36:481–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Wider pipelines: N-best alignments and parses in MT training.</title>
<date>2009</date>
<contexts>
<context position="1976" citStr="Venugopal et al., 2009" startWordPosition="294" endWordPosition="298">tion to constraint the possible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignment quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better alignments in terms of F-measure do not always lead to better translation quality. The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In thi</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Wider pipelines: N-best alignments and parses in MT training.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Maja Popovic</author>
<author>Hermann Ney</author>
</authors>
<title>Aer: Do we need to ”improve” our alignments?</title>
<date>2006</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>205--212</pages>
<contexts>
<context position="1663" citStr="Vilar et al., 2006" startWordPosition="245" endWordPosition="248">. 1 Introduction The translation quality of statistical phrase-based systems (Koehn et al., 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al., 2010). The basic phrase extraction algorithm uses word alignment information to constraint the possible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignment quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better alignments in terms of F-measure do not always lead to better translation quality. The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, whil</context>
</contexts>
<marker>Vilar, Popovic, Ney, 2006</marker>
<rawString>David Vilar, Maja Popovic, and Hermann Ney. 2006. Aer: Do we need to ”improve” our alignments? In International Workshop on Spoken Language Translation (IWSLT), pages 205–212.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>