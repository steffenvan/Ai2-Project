<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.151785">
<title confidence="0.997663">
Incremental Speech Understanding in a Multi-Party Virtual Human
Dialogue System
</title>
<author confidence="0.99821">
David DeVault and David Traum
</author>
<affiliation confidence="0.997352">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.948312">
12015 Waterfront Drive, Playa Vista, CA 90094
</address>
<email confidence="0.999709">
{devault,traum}@ict.usc.edu
</email>
<sectionHeader confidence="0.996316" genericHeader="abstract">
1 Extended Abstract
</sectionHeader>
<bodyText confidence="0.99952628125">
This demonstration highlights some emerging ca-
pabilities for incremental speech understanding and
processing in virtual human dialogue systems. This
work is part of an ongoing effort that aims to en-
able realistic spoken dialogue with virtual humans in
multi-party negotiation scenarios (Pl¨uss et al., 2011;
Traum et al., 2008b). These scenarios are designed
to allow trainees to practice their negotiation skills
by engaging in face-to-face spoken negotiation with
one or more virtual humans.
An important component in achieving naturalistic
behavior in these negotiation scenarios, which ide-
ally should have the virtual humans demonstrating
fluid turn-taking, complex reasoning, and respond-
ing to factors like trust and emotions, is for the vir-
tual humans to begin to understand and in some
cases respond in real time to users’ speech, as the
users are speaking (DeVault et al., 2011b). These re-
sponses could range from relatively straightforward
turn management behaviors, like having a virtual hu-
man recognize when it is being addressed by a user
utterance, and possibly turn to look at the user who
has started speaking, to more complex responses
such as emotional reactions to the content of what
users are saying.
The current demonstration extends our previous
demonstration of incremental processing (Sagae et
al., 2010) in several important respects. First, it
includes additional indicators, as described in (De-
Vault et al., 2011a). Second, it is applied to a new
domain, an extension of that presented in (Pl¨uss et
al., 2011). Finally, it is integrated with the dialogue
</bodyText>
<page confidence="0.995106">
25
</page>
<figureCaption confidence="0.999771">
Figure 1: SASO negotiation in the saloon: Utah (left)
looking at Harmony (right).
</figureCaption>
<bodyText confidence="0.998268304347826">
models (Traum et al., 2008a), such that each par-
tial interpretation is given a full pragmatic interpre-
tation by each virtual character, which can be used
to generate real-time incremental non-verbal feed-
back (Wang et al., 2011).
Our demonstration is set in an implemented multi-
party negotiation domain (Pl¨uss et al., 2011) in
which two virtual humans, Utah and Harmony (pic-
tured in Figure 1), talk with two human negotiation
trainees, who play the roles of Ranger and Deputy.
The dialogue takes place inside a saloon in an Amer-
ican town in the Old West. In this negotiation sce-
nario, the goal of the two human role players is to
convince Utah and Harmony that Utah, who is cur-
rently employed as the local bartender, should take
on the job of town sheriff.
One of the research aims for this work is to
support natural dialogue interaction, an example of
which is the excerpt of human role play dialogue
shown in Figure 2. One of the key features of immer-
sive role plays is that people often react in multiple
ways to the utterances of others as they are speaking.
For example, in this excerpt, the beginning of the
</bodyText>
<note confidence="0.367348">
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 25–28,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9516585">
Ranger We can’t leave this place and have it overrun by outlaws.
Uh there’s no way that’s gonna happen so we’re gonna
make sure we’ve got a properly deputized and equipped
sheriff ready to maintain order in this area.
</bodyText>
<figure confidence="0.80937375">
00:03:56.660 - 00:04:08.830
Deputy Yeah and you know and and we’re willing to
00:04:06.370 - 00:04:09.850
Utah And I don’t have to leave the bar completely. I can still
uh be here part time and I can um we can hire someone to
do the like day to day work and I’ll do the I’ll supervise
them and I’ll teach them.
00:04:09.090 - 00:04:22.880
</figure>
<figureCaption confidence="0.9894555">
Figure 2: Dialogue excerpt from one of the role plays.
Timestamps indicate the start and end of each utterance.
</figureCaption>
<bodyText confidence="0.9999078">
Deputy’s utterance overlaps the end of the Ranger’s,
and then Utah interrupts the Deputy and takes the
floor a few seconds later.
Our prediction approach to incremental speech
understanding utilizes a corpus of in-domain spo-
ken utterances, including both paraphrases selected
and spoken by system developers, as well as spo-
ken utterances from user testing sessions (DeVault
et al., 2011b). An example of a corpus element is
shown in Figure 3. In previous negotiation domains,
we have found a fairly high word error rate in au-
tomatic speech recognition results for such sponta-
neous multi-party dialogue data; for example, our
average word error rate was 0.39 in the SASO-EN
negotiation domain (Traum et al., 2008b) with many
(15%) out of domain utterances. Our speech un-
derstanding framework is robust to these kinds of
problems (DeVault et al., 2011b), partly through
approximating the meaning of utterances. Utter-
ance meanings are represented using an attribute-
value matrix (AVM), where the attributes and val-
ues represent semantic information that is linked to
a domain-specific ontology and task model (Traum,
2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The
AVMs are linearized, using a path-value notation, as
seen in Figure 3. In our framework, we use this data
to train two data-driven models, one for incremen-
tal natural language understanding, and a second for
incremental confidence modeling.
The first step is to train a predictive incremental
understanding model. This model is based on maxi-
mum entropy classification, and treats entire individ-
ual frames as output classes, with input features ex-
tracted from partial ASR results, calculated in incre-
ments of 200 milliseconds (DeVault et al., 2011b).
</bodyText>
<listItem confidence="0.9843554">
• Utterance (speech): i’ve come here today to talk to you
about whether you’d like to become the sheriff of this town
• ASR (NLU input): have come here today to talk to you
about would the like to become the sheriff of this town
• Frame (NLU output):
</listItem>
<figure confidence="0.995439">
&lt;S&gt;.mood interrogative
&lt;S&gt;.sem.modal.desire want
&lt;S&gt;.sem.prop.agent utah
&lt;S&gt;.sem.prop.event providePublicServices
&lt;S&gt;.sem.prop.location town
&lt;S&gt;.sem.prop.theme sheriff-job
&lt;S&gt;.sem.prop.type event
&lt;S&gt;.sem.q-slot polarity
&lt;S&gt;.sem.speechact.type info-req
&lt;S&gt;.sem.type question
</figure>
<figureCaption confidence="0.999985">
Figure 3: Example of a corpus training example.
</figureCaption>
<bodyText confidence="0.9999445">
Each partial ASR result then serves as an incremen-
tal input to NLU, which is specially trained for par-
tial input as discussed in (Sagae et al., 2009). NLU
is predictive in the sense that, for each partial ASR
result, the NLU module produces as output the com-
plete frame that has been associated by a human an-
notator with the user’s complete utterance, even if
that utterance has not yet been fully processed by
the ASR. For a detailed analysis of the performance
of the predictive NLU, see (DeVault et al., 2011b).
The second step in our framework is to train a set
of incremental confidence models (DeVault et al.,
2011a), which allow the agents to assess in real time,
while a user is speaking, how well the understand-
ing process is proceeding. The incremental confi-
dence models build on the notion of NLU F-score,
which we use to quantify the quality of a predicted
NLU frame in relation to the hand-annotated correct
frame. The NLU F-score is the harmonic mean of
the precision and recall of the attribute-value pairs
(or frame elements) that compose the predicted and
correct frames for each partial ASR result. By using
precision and recall of frame elements, rather than
simply looking at frame accuracy, we take into ac-
count that certain frames are more similar than oth-
ers, and allow for cases when the correct frame is
not in the training set.
Each of our incremental confidence models
makes a binary prediction for each partial NLU re-
sult as an utterance proceeds. At each time t dur-
</bodyText>
<page confidence="0.997068">
26
</page>
<figureCaption confidence="0.999949">
Figure 4: Visualization of Incremental Speech Processing.
</figureCaption>
<bodyText confidence="0.999971317073171">
ing an utterance, we consider the current NLU F-
Score Ft as well as the final NLU F-Score Ffinal
that will be achieved at the conclusion of the ut-
terance. In (DeVault et al., 2009) and (DeVault
et al., 2011a), we explored the use of data-driven
decision tree classifiers to make predictions about
these values, for example whether Ft &gt; 21 (cur-
rent level of understanding is “high”), Ft &gt; Ffinal
(current level of understanding will not improve),
or Ffinal &gt; 21 (final level of understanding will be
“high”). In this demonstration, we focus on the
first and third of these incremental confidence met-
rics, which we summarize as “Now Understanding”
and “Will Understand”, respectively. In an evalua-
tion over all partial ASR results for 990 utterances
in this new scenario, we found the Now Under-
standing model to have precision/recall/F-Score of
.92/.75/.82, and the Will Understand model to have
precision/recall/F-Score of .93/.85/.89. These incre-
mental confidence models therefore provide poten-
tially useful real-time information to Utah and Har-
mony about whether they are currently understand-
ing a user utterance, and whether they will ever un-
derstand a user utterance.
The incremental ASR, NLU, and confidence
models are passed to the dialogue managers for each
of the agents, Harmony and Utah. These agents then
relate these inputs to their own models of dialogue
context, plans, and emotions, to calculate pragmatic
interpretations, including speech acts, reference res-
olution, participant status, and how they feel about
what is being discussed. A subset of this informa-
tion is passed to the non-verbal behavior generation
module to produce incremental non-verbal listening
behaviors (Wang et al., 2011).
In support of this demonstration, we have ex-
tended the implementation to include a real-time vi-
sualization of incremental speech processing results,
which will allow attendees to track the virtual hu-
mans’ understanding as an utterance progresses. An
example of this visualization is shown in Figure 4.
</bodyText>
<sectionHeader confidence="0.919494" genericHeader="categories and subject descriptors">
2 Demo script
</sectionHeader>
<bodyText confidence="0.999964875">
The demonstration begins with the demo operator
providing a brief overview of the system design, ne-
gotiation scenario, and incremental processing capa-
bilities. The virtual humans Utah and Harmony (see
Figure 1) are running and ready to begin a dialogue
with the user, who will play the role of the Ranger.
As the user speaks to Utah or Harmony, attendees
can observe the real time visualization of speech
</bodyText>
<page confidence="0.99519">
27
</page>
<bodyText confidence="0.999965864864865">
processing to observe changes in the incremental
processing results as the utterance progresses. Fur-
ther, the visualization interface enables the demo op-
erator to “rewind” an utterance and step through the
incremental processing results that arrived each 200
milliseconds, highlighting how specific partial ASR
results can change the virtual humans’ understand-
ing or confidence.
For example, Figure 4 shows the incremental
speech processing state at a moment 4.8 seconds into
a user’s 7.4 second long utterance, i’ve come here
today to talk to you about whether you’d like to be-
come the sheriff of this town. At this point in time,
the visualization shows (at top left) that the virtual
humans are confident that they are both Now Under-
standing and Will Understand this utterance. Next,
the graph (in white) shows the history of the agents’
expected NLU F-Score for this utterance (ranging
from 0 to 1). Beneath the graph, the partial ASR re-
sult (HAVE COME HERE TODAY TO TALK TO
YOU ABOUT...) is displayed (in white), along
with the currently predicted NLU frame (in blue).
For ease of comprehension, an English gloss (utah
do you want to be the sheriff?) for the NLU frame is
also shown (in blue) above the frame.
To the right, in pink, we show some of Utah and
Harmony’s agent state that is based on the current in-
cremental NLU results. The display shows that both
of the virtual humans believe that Utah is being ad-
dressed by this utterance, that utah has a positive at-
titude toward the content of the utterance while har-
mony does not, and that both have comprehension
and participation goals. Further, Harmony believes
she is a side participant at this moment. The demo
operator will explain and discuss this agent state in-
formation, including possible uses for this informa-
tion in response policies.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999821125">
We thank all the members of the ICT Virtual Hu-
mans team. The project or effort described here
has been sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not
necessarily reflect the position or the policy of the
United States Government, and no official endorse-
ment should be inferred.
</bodyText>
<sectionHeader confidence="0.989966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997975980392157">
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of SIGDIAL.
David DeVault, Kenji Sagae, and David Traum. 2011a.
Detecting the status of a predictive incremental speech
understanding model for real-time decision-making in
a spoken dialogue system. In Proceedings of Inter-
Speech.
David DeVault, Kenji Sagae, and David Traum. 2011b.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue &amp; Dis-
course, 2(1).
Arno Hartholt, Thomas Russ, David Traum, Eduard
Hovy, and Susan Robinson. 2008. A common ground
for virtual humans: Using an ontology in a natural
language oriented virtual human architecture. In Pro-
ceedings of LREC, Marrakech, Morocco, may.
Brian Pl¨uss, David DeVault, and David Traum. 2011.
Toward rapid development of multi-party virtual hu-
man negotiation scenarios. In Proceedings of SemDial
2011, the 15th Workshop on the Semantics and Prag-
matics of Dialogue.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language un-
derstanding of partial speech recognition results in dia-
logue systems. In Short Paper Proceedings of NAACL
HLT.
Kenji Sagae, David DeVault, and David R. Traum. 2010.
Interpretation of partial utterances in virtual human
dialogue systems. In Demonstration Proceedings of
NAACL-HLT.
D. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008a. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
David Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008b. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of IVA.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380–394, January.
Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011.
Towards more comprehensive listening behavior: Be-
yond the bobble head. In Hannes Vilhjlmsson, Stefan
Kopp, Stacy Marsella, and Kristinn Thrisson, editors,
Intelligent Virtual Agents, volume 6895 of Lecture
Notes in Computer Science, pages 216–227. Springer
Berlin / Heidelberg.
</reference>
<page confidence="0.999071">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003288">
<title confidence="0.973214">Incremental Speech Understanding in a Multi-Party Virtual Dialogue System</title>
<author confidence="0.888939">DeVault</author>
<affiliation confidence="0.999015">Institute for Creative University of Southern</affiliation>
<address confidence="0.997849">12015 Waterfront Drive, Playa Vista, CA 90094</address>
<abstract confidence="0.988974708333333">1 Extended Abstract This demonstration highlights some emerging capabilities for incremental speech understanding and processing in virtual human dialogue systems. This work is part of an ongoing effort that aims to enable realistic spoken dialogue with virtual humans in multi-party negotiation scenarios (Pl¨uss et al., 2011; Traum et al., 2008b). These scenarios are designed to allow trainees to practice their negotiation skills by engaging in face-to-face spoken negotiation with one or more virtual humans. An important component in achieving naturalistic behavior in these negotiation scenarios, which ideally should have the virtual humans demonstrating fluid turn-taking, complex reasoning, and responding to factors like trust and emotions, is for the virtual humans to begin to understand and in some cases respond in real time to users’ speech, as the users are speaking (DeVault et al., 2011b). These responses could range from relatively straightforward turn management behaviors, like having a virtual human recognize when it is being addressed by a user utterance, and possibly turn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (De- Vault et al., 2011a). Second, it is applied to a new domain, an extension of that presented in (Pl¨uss et al., 2011). Finally, it is integrated with the dialogue 25 Figure 1: SASO negotiation in the saloon: Utah (left) looking at Harmony (right). models (Traum et al., 2008a), such that each partial interpretation is given a full pragmatic interpretation by each virtual character, which can be used to generate real-time incremental non-verbal feedback (Wang et al., 2011). Our demonstration is set in an implemented multiparty negotiation domain (Pl¨uss et al., 2011) in which two virtual humans, Utah and Harmony (pictured in Figure 1), talk with two human negotiation trainees, who play the roles of Ranger and Deputy. The dialogue takes place inside a saloon in an American town in the Old West. In this negotiation scenario, the goal of the two human role players is to convince Utah and Harmony that Utah, who is currently employed as the local bartender, should take on the job of town sheriff. One of the research aims for this work is to support natural dialogue interaction, an example of which is the excerpt of human role play dialogue shown in Figure 2. One of the key features of immersive role plays is that people often react in multiple ways to the utterances of others as they are speaking. For example, in this excerpt, the beginning of the of the NAACL-HLT 2012: Demonstration pages Canada, June 3-8, 2012. Association for Computational Linguistics can’t leave this place and have it overrun by outlaws. Uh there’s no way that’s gonna happen so we’re gonna make sure we’ve got a properly deputized and equipped sheriff ready to maintain order in this area. 00:03:56.660 - 00:04:08.830 and you know and and we’re willing to 00:04:06.370 - 00:04:09.850 I don’t have to leave the bar completely. I can still uh be here part time and I can um we can hire someone to do the like day to day work and I’ll do the I’ll supervise them and I’ll teach them. 00:04:09.090 - 00:04:22.880 Figure 2: Dialogue excerpt from one of the role plays. Timestamps indicate the start and end of each utterance. Deputy’s utterance overlaps the end of the Ranger’s, and then Utah interrupts the Deputy and takes the floor a few seconds later. Our prediction approach to incremental speech understanding utilizes a corpus of in-domain spoken utterances, including both paraphrases selected and spoken by system developers, as well as spoken utterances from user testing sessions (DeVault et al., 2011b). An example of a corpus element is shown in Figure 3. In previous negotiation domains, we have found a fairly high word error rate in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), where the attributes and values represent semantic information that is linked to a domain-specific ontology and task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one for incremental natural language understanding, and a second for incremental confidence modeling. The first step is to train a predictive incremental understanding model. This model is based on maximum entropy classification, and treats entire individual frames as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b). Utterance (speech): come here today to talk to you about whether you’d like to become the sheriff of this town ASR (NLU input): come here today to talk to you about would the like to become the sheriff of this town • Frame (NLU output): &lt;S&gt;.mood interrogative &lt;S&gt;.sem.modal.desire want &lt;S&gt;.sem.prop.agent utah &lt;S&gt;.sem.prop.event providePublicServices &lt;S&gt;.sem.prop.location town &lt;S&gt;.sem.prop.theme sheriff-job &lt;S&gt;.sem.prop.type event &lt;S&gt;.sem.q-slot polarity &lt;S&gt;.sem.speechact.type info-req &lt;S&gt;.sem.type question Figure 3: Example of a corpus training example. Each partial ASR result then serves as an incremental input to NLU, which is specially trained for partial input as discussed in (Sagae et al., 2009). NLU is predictive in the sense that, for each partial ASR the NLU module produces as output the comthat has been associated by a human anwith the user’s even if that utterance has not yet been fully processed by the ASR. For a detailed analysis of the performance of the predictive NLU, see (DeVault et al., 2011b). The second step in our framework is to train a set of incremental confidence models (DeVault et al., 2011a), which allow the agents to assess in real time, while a user is speaking, how well the understanding process is proceeding. The incremental confidence models build on the notion of NLU F-score, which we use to quantify the quality of a predicted NLU frame in relation to the hand-annotated correct frame. The NLU F-score is the harmonic mean of the precision and recall of the attribute-value pairs that compose the predicted and correct frames for each partial ASR result. By using precision and recall of frame elements, rather than simply looking at frame accuracy, we take into account that certain frames are more similar than others, and allow for cases when the correct frame is not in the training set. Each of our incremental confidence models makes a binary prediction for each partial NLU reas an utterance proceeds. At each time dur- 26 Figure 4: Visualization of Incremental Speech Processing. ing an utterance, we consider the current NLU Fwell as the final NLU F-Score that will be achieved at the conclusion of the utterance. In (DeVault et al., 2009) and (DeVault et al., 2011a), we explored the use of data-driven decision tree classifiers to make predictions about values, for example whether (curlevel of understanding is “high”), (current level of understanding will not improve), (final level of understanding will be “high”). In this demonstration, we focus on the first and third of these incremental confidence metrics, which we summarize as “Now Understanding” and “Will Understand”, respectively. In an evaluation over all partial ASR results for 990 utterances in this new scenario, we found the Now Understanding model to have precision/recall/F-Score of .92/.75/.82, and the Will Understand model to have precision/recall/F-Score of .93/.85/.89. These incremental confidence models therefore provide potentially useful real-time information to Utah and Harmony about whether they are currently understanding a user utterance, and whether they will ever understand a user utterance. The incremental ASR, NLU, and confidence models are passed to the dialogue managers for each of the agents, Harmony and Utah. These agents then relate these inputs to their own models of dialogue context, plans, and emotions, to calculate pragmatic interpretations, including speech acts, reference resolution, participant status, and how they feel about what is being discussed. A subset of this information is passed to the non-verbal behavior generation module to produce incremental non-verbal listening behaviors (Wang et al., 2011). In support of this demonstration, we have extended the implementation to include a real-time visualization of incremental speech processing results, which will allow attendees to track the virtual humans’ understanding as an utterance progresses. An example of this visualization is shown in Figure 4. 2 Demo script The demonstration begins with the demo operator providing a brief overview of the system design, negotiation scenario, and incremental processing capabilities. The virtual humans Utah and Harmony (see Figure 1) are running and ready to begin a dialogue with the user, who will play the role of the Ranger. As the user speaks to Utah or Harmony, attendees can observe the real time visualization of speech 27 processing to observe changes in the incremental processing results as the utterance progresses. Further, the visualization interface enables the demo operator to “rewind” an utterance and step through the incremental processing results that arrived each 200 milliseconds, highlighting how specific partial ASR results can change the virtual humans’ understanding or confidence. For example, Figure 4 shows the incremental speech processing state at a moment 4.8 seconds into user’s 7.4 second long utterance, come here today to talk to you about whether you’d like to bethe sheriff of this At this point in time, the visualization shows (at top left) that the virtual humans are confident that they are both Now Understanding and Will Understand this utterance. Next, the graph (in white) shows the history of the agents’ expected NLU F-Score for this utterance (ranging from 0 to 1). Beneath the graph, the partial ASR re- COME HERE TODAY TO TALK TO is displayed (in white), along with the currently predicted NLU frame (in blue). ease of comprehension, an English gloss you want to be the for the NLU frame is also shown (in blue) above the frame. To the right, in pink, we show some of Utah and Harmony’s agent state that is based on the current incremental NLU results. The display shows that both of the virtual humans believe that Utah is being addressed by this utterance, that utah has a positive attitude toward the content of the utterance while harmony does not, and that both have comprehension and participation goals. Further, Harmony believes she is a side participant at this moment. The demo operator will explain and discuss this agent state information, including possible uses for this information in response policies. Acknowledgments We thank all the members of the ICT Virtual Humans team. The project or effort described here has been sponsored by the U.S. Army Research, Development, and Engineering Command (RDE- COM). Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred. References David DeVault, Kenji Sagae, and David Traum. 2009. Can I finish? Learning when to respond to incremental results in interactive dialogue. In Proof David DeVault, Kenji Sagae, and David Traum. 2011a. Detecting the status of a predictive incremental speech understanding model for real-time decision-making in spoken dialogue system. In of Inter-</abstract>
<note confidence="0.577207463414634">David DeVault, Kenji Sagae, and David Traum. 2011b. Incremental interpretation and prediction of utterance for interactive dialogue. &amp; Dis- 2(1). Arno Hartholt, Thomas Russ, David Traum, Eduard Hovy, and Susan Robinson. 2008. A common ground for virtual humans: Using an ontology in a natural oriented virtual human architecture. In Proof Marrakech, Morocco, may. Brian Pl¨uss, David DeVault, and David Traum. 2011. Toward rapid development of multi-party virtual hunegotiation scenarios. In of SemDial 2011, the 15th Workshop on the Semantics and Pragof Kenji Sagae, Gwen Christian, David DeVault, and David R. Traum. 2009. Towards natural language understanding of partial speech recognition results in diasystems. In Paper Proceedings of NAACL Kenji Sagae, David DeVault, and David R. Traum. 2010. Interpretation of partial utterances in virtual human systems. In Proceedings of D. Traum, W. Swartout, J. Gratch, and S. Marsella. 2008a. A virtual human dialogue model for non-team interaction. In L. Dybkjaer and W. Minker, editors, Trends in Discourse and Springer. David Traum, Stacy Marsella, Jonathan Gratch, Jina Lee, and Arno Hartholt. 2008b. Multi-party, multiissue, multi-strategy negotiation for multi-modal viragents. In of David Traum. 2003. Semantics and pragmatics of quesand answers for dialogue agents. In of the Workshop on Computational pages 380–394, January. Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011. Towards more comprehensive listening behavior: Beyond the bobble head. In Hannes Vilhjlmsson, Stefan Kopp, Stacy Marsella, and Kristinn Thrisson, editors, Virtual volume 6895 of in Computer pages 216–227. Springer Berlin / Heidelberg. 28</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kenji Sagae</author>
<author>David Traum</author>
</authors>
<title>Can I finish? Learning when to respond to incremental interpretation results in interactive dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGDIAL.</booktitle>
<contexts>
<context position="7906" citStr="DeVault et al., 2009" startWordPosition="1288" endWordPosition="1291">lt. By using precision and recall of frame elements, rather than simply looking at frame accuracy, we take into account that certain frames are more similar than others, and allow for cases when the correct frame is not in the training set. Each of our incremental confidence models makes a binary prediction for each partial NLU result as an utterance proceeds. At each time t dur26 Figure 4: Visualization of Incremental Speech Processing. ing an utterance, we consider the current NLU FScore Ft as well as the final NLU F-Score Ffinal that will be achieved at the conclusion of the utterance. In (DeVault et al., 2009) and (DeVault et al., 2011a), we explored the use of data-driven decision tree classifiers to make predictions about these values, for example whether Ft &gt; 21 (current level of understanding is “high”), Ft &gt; Ffinal (current level of understanding will not improve), or Ffinal &gt; 21 (final level of understanding will be “high”). In this demonstration, we focus on the first and third of these incremental confidence metrics, which we summarize as “Now Understanding” and “Will Understand”, respectively. In an evaluation over all partial ASR results for 990 utterances in this new scenario, we found t</context>
</contexts>
<marker>DeVault, Sagae, Traum, 2009</marker>
<rawString>David DeVault, Kenji Sagae, and David Traum. 2009. Can I finish? Learning when to respond to incremental interpretation results in interactive dialogue. In Proceedings of SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kenji Sagae</author>
<author>David Traum</author>
</authors>
<title>Detecting the status of a predictive incremental speech understanding model for real-time decision-making in a spoken dialogue system.</title>
<date>2011</date>
<booktitle>In Proceedings of InterSpeech.</booktitle>
<contexts>
<context position="1159" citStr="DeVault et al., 2011" startWordPosition="166" endWordPosition="169">egotiation scenarios (Pl¨uss et al., 2011; Traum et al., 2008b). These scenarios are designed to allow trainees to practice their negotiation skills by engaging in face-to-face spoken negotiation with one or more virtual humans. An important component in achieving naturalistic behavior in these negotiation scenarios, which ideally should have the virtual humans demonstrating fluid turn-taking, complex reasoning, and responding to factors like trust and emotions, is for the virtual humans to begin to understand and in some cases respond in real time to users’ speech, as the users are speaking (DeVault et al., 2011b). These responses could range from relatively straightforward turn management behaviors, like having a virtual human recognize when it is being addressed by a user utterance, and possibly turn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an </context>
<context position="4267" citStr="DeVault et al., 2011" startWordPosition="691" endWordPosition="694">meone to do the like day to day work and I’ll do the I’ll supervise them and I’ll teach them. 00:04:09.090 - 00:04:22.880 Figure 2: Dialogue excerpt from one of the role plays. Timestamps indicate the start and end of each utterance. Deputy’s utterance overlaps the end of the Ranger’s, and then Utah interrupts the Deputy and takes the floor a few seconds later. Our prediction approach to incremental speech understanding utilizes a corpus of in-domain spoken utterances, including both paraphrases selected and spoken by system developers, as well as spoken utterances from user testing sessions (DeVault et al., 2011b). An example of a corpus element is shown in Figure 3. In previous negotiation domains, we have found a fairly high word error rate in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), wher</context>
<context position="5595" citStr="DeVault et al., 2011" startWordPosition="904" endWordPosition="907">task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one for incremental natural language understanding, and a second for incremental confidence modeling. The first step is to train a predictive incremental understanding model. This model is based on maximum entropy classification, and treats entire individual frames as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b). • Utterance (speech): i’ve come here today to talk to you about whether you’d like to become the sheriff of this town • ASR (NLU input): have come here today to talk to you about would the like to become the sheriff of this town • Frame (NLU output): &lt;S&gt;.mood interrogative &lt;S&gt;.sem.modal.desire want &lt;S&gt;.sem.prop.agent utah &lt;S&gt;.sem.prop.event providePublicServices &lt;S&gt;.sem.prop.location town &lt;S&gt;.sem.prop.theme sheriff-job &lt;S&gt;.sem.prop.type event &lt;S&gt;.sem.q-slot polarity &lt;S&gt;.sem.speechact.type info-req &lt;S&gt;.sem.type question Figure 3: Example of a corpus training example. Each partial ASR result</context>
<context position="7932" citStr="DeVault et al., 2011" startWordPosition="1293" endWordPosition="1296">recall of frame elements, rather than simply looking at frame accuracy, we take into account that certain frames are more similar than others, and allow for cases when the correct frame is not in the training set. Each of our incremental confidence models makes a binary prediction for each partial NLU result as an utterance proceeds. At each time t dur26 Figure 4: Visualization of Incremental Speech Processing. ing an utterance, we consider the current NLU FScore Ft as well as the final NLU F-Score Ffinal that will be achieved at the conclusion of the utterance. In (DeVault et al., 2009) and (DeVault et al., 2011a), we explored the use of data-driven decision tree classifiers to make predictions about these values, for example whether Ft &gt; 21 (current level of understanding is “high”), Ft &gt; Ffinal (current level of understanding will not improve), or Ffinal &gt; 21 (final level of understanding will be “high”). In this demonstration, we focus on the first and third of these incremental confidence metrics, which we summarize as “Now Understanding” and “Will Understand”, respectively. In an evaluation over all partial ASR results for 990 utterances in this new scenario, we found the Now Understanding model</context>
</contexts>
<marker>DeVault, Sagae, Traum, 2011</marker>
<rawString>David DeVault, Kenji Sagae, and David Traum. 2011a. Detecting the status of a predictive incremental speech understanding model for real-time decision-making in a spoken dialogue system. In Proceedings of InterSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kenji Sagae</author>
<author>David Traum</author>
</authors>
<title>Incremental interpretation and prediction of utterance meaning for interactive dialogue.</title>
<date>2011</date>
<booktitle>Dialogue &amp; Discourse,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1159" citStr="DeVault et al., 2011" startWordPosition="166" endWordPosition="169">egotiation scenarios (Pl¨uss et al., 2011; Traum et al., 2008b). These scenarios are designed to allow trainees to practice their negotiation skills by engaging in face-to-face spoken negotiation with one or more virtual humans. An important component in achieving naturalistic behavior in these negotiation scenarios, which ideally should have the virtual humans demonstrating fluid turn-taking, complex reasoning, and responding to factors like trust and emotions, is for the virtual humans to begin to understand and in some cases respond in real time to users’ speech, as the users are speaking (DeVault et al., 2011b). These responses could range from relatively straightforward turn management behaviors, like having a virtual human recognize when it is being addressed by a user utterance, and possibly turn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an </context>
<context position="4267" citStr="DeVault et al., 2011" startWordPosition="691" endWordPosition="694">meone to do the like day to day work and I’ll do the I’ll supervise them and I’ll teach them. 00:04:09.090 - 00:04:22.880 Figure 2: Dialogue excerpt from one of the role plays. Timestamps indicate the start and end of each utterance. Deputy’s utterance overlaps the end of the Ranger’s, and then Utah interrupts the Deputy and takes the floor a few seconds later. Our prediction approach to incremental speech understanding utilizes a corpus of in-domain spoken utterances, including both paraphrases selected and spoken by system developers, as well as spoken utterances from user testing sessions (DeVault et al., 2011b). An example of a corpus element is shown in Figure 3. In previous negotiation domains, we have found a fairly high word error rate in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), wher</context>
<context position="5595" citStr="DeVault et al., 2011" startWordPosition="904" endWordPosition="907">task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one for incremental natural language understanding, and a second for incremental confidence modeling. The first step is to train a predictive incremental understanding model. This model is based on maximum entropy classification, and treats entire individual frames as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b). • Utterance (speech): i’ve come here today to talk to you about whether you’d like to become the sheriff of this town • ASR (NLU input): have come here today to talk to you about would the like to become the sheriff of this town • Frame (NLU output): &lt;S&gt;.mood interrogative &lt;S&gt;.sem.modal.desire want &lt;S&gt;.sem.prop.agent utah &lt;S&gt;.sem.prop.event providePublicServices &lt;S&gt;.sem.prop.location town &lt;S&gt;.sem.prop.theme sheriff-job &lt;S&gt;.sem.prop.type event &lt;S&gt;.sem.q-slot polarity &lt;S&gt;.sem.speechact.type info-req &lt;S&gt;.sem.type question Figure 3: Example of a corpus training example. Each partial ASR result</context>
<context position="7932" citStr="DeVault et al., 2011" startWordPosition="1293" endWordPosition="1296">recall of frame elements, rather than simply looking at frame accuracy, we take into account that certain frames are more similar than others, and allow for cases when the correct frame is not in the training set. Each of our incremental confidence models makes a binary prediction for each partial NLU result as an utterance proceeds. At each time t dur26 Figure 4: Visualization of Incremental Speech Processing. ing an utterance, we consider the current NLU FScore Ft as well as the final NLU F-Score Ffinal that will be achieved at the conclusion of the utterance. In (DeVault et al., 2009) and (DeVault et al., 2011a), we explored the use of data-driven decision tree classifiers to make predictions about these values, for example whether Ft &gt; 21 (current level of understanding is “high”), Ft &gt; Ffinal (current level of understanding will not improve), or Ffinal &gt; 21 (final level of understanding will be “high”). In this demonstration, we focus on the first and third of these incremental confidence metrics, which we summarize as “Now Understanding” and “Will Understand”, respectively. In an evaluation over all partial ASR results for 990 utterances in this new scenario, we found the Now Understanding model</context>
</contexts>
<marker>DeVault, Sagae, Traum, 2011</marker>
<rawString>David DeVault, Kenji Sagae, and David Traum. 2011b. Incremental interpretation and prediction of utterance meaning for interactive dialogue. Dialogue &amp; Discourse, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arno Hartholt</author>
<author>Thomas Russ</author>
<author>David Traum</author>
<author>Eduard Hovy</author>
<author>Susan Robinson</author>
</authors>
<title>A common ground for virtual humans: Using an ontology in a natural language oriented virtual human architecture.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="5021" citStr="Hartholt et al., 2008" startWordPosition="812" endWordPosition="815"> in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), where the attributes and values represent semantic information that is linked to a domain-specific ontology and task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one for incremental natural language understanding, and a second for incremental confidence modeling. The first step is to train a predictive incremental understanding model. This model is based on maximum entropy classification, and treats entire individual frames as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b). • Utterance (speech): </context>
</contexts>
<marker>Hartholt, Russ, Traum, Hovy, Robinson, 2008</marker>
<rawString>Arno Hartholt, Thomas Russ, David Traum, Eduard Hovy, and Susan Robinson. 2008. A common ground for virtual humans: Using an ontology in a natural language oriented virtual human architecture. In Proceedings of LREC, Marrakech, Morocco, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Pl¨uss</author>
<author>David DeVault</author>
<author>David Traum</author>
</authors>
<title>Toward rapid development of multi-party virtual human negotiation scenarios.</title>
<date>2011</date>
<booktitle>In Proceedings of SemDial 2011, the 15th Workshop on the Semantics and Pragmatics of Dialogue.</booktitle>
<marker>Pl¨uss, DeVault, Traum, 2011</marker>
<rawString>Brian Pl¨uss, David DeVault, and David Traum. 2011. Toward rapid development of multi-party virtual human negotiation scenarios. In Proceedings of SemDial 2011, the 15th Workshop on the Semantics and Pragmatics of Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Gwen Christian</author>
<author>David DeVault</author>
<author>David R Traum</author>
</authors>
<title>Towards natural language understanding of partial speech recognition results in dialogue systems.</title>
<date>2009</date>
<booktitle>In Short Paper Proceedings of NAACL HLT.</booktitle>
<contexts>
<context position="6321" citStr="Sagae et al., 2009" startWordPosition="1009" endWordPosition="1012">f of this town • ASR (NLU input): have come here today to talk to you about would the like to become the sheriff of this town • Frame (NLU output): &lt;S&gt;.mood interrogative &lt;S&gt;.sem.modal.desire want &lt;S&gt;.sem.prop.agent utah &lt;S&gt;.sem.prop.event providePublicServices &lt;S&gt;.sem.prop.location town &lt;S&gt;.sem.prop.theme sheriff-job &lt;S&gt;.sem.prop.type event &lt;S&gt;.sem.q-slot polarity &lt;S&gt;.sem.speechact.type info-req &lt;S&gt;.sem.type question Figure 3: Example of a corpus training example. Each partial ASR result then serves as an incremental input to NLU, which is specially trained for partial input as discussed in (Sagae et al., 2009). NLU is predictive in the sense that, for each partial ASR result, the NLU module produces as output the complete frame that has been associated by a human annotator with the user’s complete utterance, even if that utterance has not yet been fully processed by the ASR. For a detailed analysis of the performance of the predictive NLU, see (DeVault et al., 2011b). The second step in our framework is to train a set of incremental confidence models (DeVault et al., 2011a), which allow the agents to assess in real time, while a user is speaking, how well the understanding process is proceeding. Th</context>
</contexts>
<marker>Sagae, Christian, DeVault, Traum, 2009</marker>
<rawString>Kenji Sagae, Gwen Christian, David DeVault, and David R. Traum. 2009. Towards natural language understanding of partial speech recognition results in dialogue systems. In Short Paper Proceedings of NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>David DeVault</author>
<author>David R Traum</author>
</authors>
<title>Interpretation of partial utterances in virtual human dialogue systems.</title>
<date>2010</date>
<booktitle>In Demonstration Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="1602" citStr="Sagae et al., 2010" startWordPosition="235" endWordPosition="238">ke trust and emotions, is for the virtual humans to begin to understand and in some cases respond in real time to users’ speech, as the users are speaking (DeVault et al., 2011b). These responses could range from relatively straightforward turn management behaviors, like having a virtual human recognize when it is being addressed by a user utterance, and possibly turn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an extension of that presented in (Pl¨uss et al., 2011). Finally, it is integrated with the dialogue 25 Figure 1: SASO negotiation in the saloon: Utah (left) looking at Harmony (right). models (Traum et al., 2008a), such that each partial interpretation is given a full pragmatic interpretation by each virtual character, which can be used to generate real-time incremental non-verbal feedback (Wang et al., 2011). Our demonstration is set in an </context>
</contexts>
<marker>Sagae, DeVault, Traum, 2010</marker>
<rawString>Kenji Sagae, David DeVault, and David R. Traum. 2010. Interpretation of partial utterances in virtual human dialogue systems. In Demonstration Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
<author>W Swartout</author>
<author>J Gratch</author>
<author>S Marsella</author>
</authors>
<title>A virtual human dialogue model for non-team interaction.</title>
<date>2008</date>
<booktitle>Recent Trends in Discourse and Dialogue.</booktitle>
<editor>In L. Dybkjaer and W. Minker, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1968" citStr="Traum et al., 2008" startWordPosition="296" endWordPosition="299">urn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an extension of that presented in (Pl¨uss et al., 2011). Finally, it is integrated with the dialogue 25 Figure 1: SASO negotiation in the saloon: Utah (left) looking at Harmony (right). models (Traum et al., 2008a), such that each partial interpretation is given a full pragmatic interpretation by each virtual character, which can be used to generate real-time incremental non-verbal feedback (Wang et al., 2011). Our demonstration is set in an implemented multiparty negotiation domain (Pl¨uss et al., 2011) in which two virtual humans, Utah and Harmony (pictured in Figure 1), talk with two human negotiation trainees, who play the roles of Ranger and Deputy. The dialogue takes place inside a saloon in an American town in the Old West. In this negotiation scenario, the goal of the two human role players is</context>
<context position="4592" citStr="Traum et al., 2008" startWordPosition="746" endWordPosition="749">takes the floor a few seconds later. Our prediction approach to incremental speech understanding utilizes a corpus of in-domain spoken utterances, including both paraphrases selected and spoken by system developers, as well as spoken utterances from user testing sessions (DeVault et al., 2011b). An example of a corpus element is shown in Figure 3. In previous negotiation domains, we have found a fairly high word error rate in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), where the attributes and values represent semantic information that is linked to a domain-specific ontology and task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one </context>
</contexts>
<marker>Traum, Swartout, Gratch, Marsella, 2008</marker>
<rawString>D. Traum, W. Swartout, J. Gratch, and S. Marsella. 2008a. A virtual human dialogue model for non-team interaction. In L. Dybkjaer and W. Minker, editors, Recent Trends in Discourse and Dialogue. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Stacy Marsella</author>
<author>Jonathan Gratch</author>
<author>Jina Lee</author>
<author>Arno Hartholt</author>
</authors>
<title>Multi-party, multiissue, multi-strategy negotiation for multi-modal virtual agents.</title>
<date>2008</date>
<booktitle>In Proceedings of IVA.</booktitle>
<contexts>
<context position="1968" citStr="Traum et al., 2008" startWordPosition="296" endWordPosition="299">urn to look at the user who has started speaking, to more complex responses such as emotional reactions to the content of what users are saying. The current demonstration extends our previous demonstration of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an extension of that presented in (Pl¨uss et al., 2011). Finally, it is integrated with the dialogue 25 Figure 1: SASO negotiation in the saloon: Utah (left) looking at Harmony (right). models (Traum et al., 2008a), such that each partial interpretation is given a full pragmatic interpretation by each virtual character, which can be used to generate real-time incremental non-verbal feedback (Wang et al., 2011). Our demonstration is set in an implemented multiparty negotiation domain (Pl¨uss et al., 2011) in which two virtual humans, Utah and Harmony (pictured in Figure 1), talk with two human negotiation trainees, who play the roles of Ranger and Deputy. The dialogue takes place inside a saloon in an American town in the Old West. In this negotiation scenario, the goal of the two human role players is</context>
<context position="4592" citStr="Traum et al., 2008" startWordPosition="746" endWordPosition="749">takes the floor a few seconds later. Our prediction approach to incremental speech understanding utilizes a corpus of in-domain spoken utterances, including both paraphrases selected and spoken by system developers, as well as spoken utterances from user testing sessions (DeVault et al., 2011b). An example of a corpus element is shown in Figure 3. In previous negotiation domains, we have found a fairly high word error rate in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), where the attributes and values represent semantic information that is linked to a domain-specific ontology and task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one </context>
</contexts>
<marker>Traum, Marsella, Gratch, Lee, Hartholt, 2008</marker>
<rawString>David Traum, Stacy Marsella, Jonathan Gratch, Jina Lee, and Arno Hartholt. 2008b. Multi-party, multiissue, multi-strategy negotiation for multi-modal virtual agents. In Proceedings of IVA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
</authors>
<title>Semantics and pragmatics of questions and answers for dialogue agents.</title>
<date>2003</date>
<booktitle>In Proc. of the International Workshop on Computational Semantics,</booktitle>
<pages>380--394</pages>
<contexts>
<context position="4998" citStr="Traum, 2003" startWordPosition="810" endWordPosition="811">rd error rate in automatic speech recognition results for such spontaneous multi-party dialogue data; for example, our average word error rate was 0.39 in the SASO-EN negotiation domain (Traum et al., 2008b) with many (15%) out of domain utterances. Our speech understanding framework is robust to these kinds of problems (DeVault et al., 2011b), partly through approximating the meaning of utterances. Utterance meanings are represented using an attributevalue matrix (AVM), where the attributes and values represent semantic information that is linked to a domain-specific ontology and task model (Traum, 2003; Hartholt et al., 2008; Pl¨uss et al., 2011). The AVMs are linearized, using a path-value notation, as seen in Figure 3. In our framework, we use this data to train two data-driven models, one for incremental natural language understanding, and a second for incremental confidence modeling. The first step is to train a predictive incremental understanding model. This model is based on maximum entropy classification, and treats entire individual frames as output classes, with input features extracted from partial ASR results, calculated in increments of 200 milliseconds (DeVault et al., 2011b).</context>
</contexts>
<marker>Traum, 2003</marker>
<rawString>David Traum. 2003. Semantics and pragmatics of questions and answers for dialogue agents. In Proc. of the International Workshop on Computational Semantics, pages 380–394, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyang Wang</author>
<author>Jina Lee</author>
<author>Stacy Marsella</author>
</authors>
<title>Towards more comprehensive listening behavior: Beyond the bobble head.</title>
<date>2011</date>
<booktitle>Intelligent Virtual Agents,</booktitle>
<volume>6895</volume>
<pages>216--227</pages>
<editor>In Hannes Vilhjlmsson, Stefan Kopp, Stacy Marsella, and Kristinn Thrisson, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="2169" citStr="Wang et al., 2011" startWordPosition="328" endWordPosition="331">tion of incremental processing (Sagae et al., 2010) in several important respects. First, it includes additional indicators, as described in (DeVault et al., 2011a). Second, it is applied to a new domain, an extension of that presented in (Pl¨uss et al., 2011). Finally, it is integrated with the dialogue 25 Figure 1: SASO negotiation in the saloon: Utah (left) looking at Harmony (right). models (Traum et al., 2008a), such that each partial interpretation is given a full pragmatic interpretation by each virtual character, which can be used to generate real-time incremental non-verbal feedback (Wang et al., 2011). Our demonstration is set in an implemented multiparty negotiation domain (Pl¨uss et al., 2011) in which two virtual humans, Utah and Harmony (pictured in Figure 1), talk with two human negotiation trainees, who play the roles of Ranger and Deputy. The dialogue takes place inside a saloon in an American town in the Old West. In this negotiation scenario, the goal of the two human role players is to convince Utah and Harmony that Utah, who is currently employed as the local bartender, should take on the job of town sheriff. One of the research aims for this work is to support natural dialogue </context>
<context position="9434" citStr="Wang et al., 2011" startWordPosition="1525" endWordPosition="1528">anding a user utterance, and whether they will ever understand a user utterance. The incremental ASR, NLU, and confidence models are passed to the dialogue managers for each of the agents, Harmony and Utah. These agents then relate these inputs to their own models of dialogue context, plans, and emotions, to calculate pragmatic interpretations, including speech acts, reference resolution, participant status, and how they feel about what is being discussed. A subset of this information is passed to the non-verbal behavior generation module to produce incremental non-verbal listening behaviors (Wang et al., 2011). In support of this demonstration, we have extended the implementation to include a real-time visualization of incremental speech processing results, which will allow attendees to track the virtual humans’ understanding as an utterance progresses. An example of this visualization is shown in Figure 4. 2 Demo script The demonstration begins with the demo operator providing a brief overview of the system design, negotiation scenario, and incremental processing capabilities. The virtual humans Utah and Harmony (see Figure 1) are running and ready to begin a dialogue with the user, who will play </context>
</contexts>
<marker>Wang, Lee, Marsella, 2011</marker>
<rawString>Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011. Towards more comprehensive listening behavior: Beyond the bobble head. In Hannes Vilhjlmsson, Stefan Kopp, Stacy Marsella, and Kristinn Thrisson, editors, Intelligent Virtual Agents, volume 6895 of Lecture Notes in Computer Science, pages 216–227. Springer Berlin / Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>