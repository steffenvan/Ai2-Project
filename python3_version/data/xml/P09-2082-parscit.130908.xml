<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000482">
<title confidence="0.995737">
Learning foci for Question Answering over Topic Maps
</title>
<author confidence="0.96159">
Alexander Mikhailian†, Tiphaine Dalmas‡ and Rani Pinchuk††Space Application Services, Leuvensesteenweg 325, B-1932 Zaventem, Belgium
</author>
<email confidence="0.97602">
{alexander.mikhailian, rani.pinchuk}@spaceapplications.com
</email>
<note confidence="0.754553">
‡Aethys
</note>
<email confidence="0.991747">
tiphaine.dalmas@aethys.com
</email>
<sectionHeader confidence="0.99368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818833333333">
This paper introduces the concepts of ask-
ing point and expected answer type as vari-
ations of the question focus. They are of
particular importance for QA over semi-
structured data, as represented by Topic
Maps, OWL or custom XML formats.
We describe an approach to the identifica-
tion of the question focus from questions
asked to a Question Answering system
over Topic Maps by extracting the asking
point and falling back to the expected an-
swer type when necessary. We use known
machine learning techniques for expected
answer type extraction and we implement
a novel approach to the asking point ex-
traction. We also provide a mathematical
model to predict the performance of the
system.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992894117647">
Topic Maps is an ISO standard1 for knowledge
representation and information integration. It pro-
vides the ability to store complex meta-data to-
gether with the data itself.
This work addresses domain portable Question
Answering (QA) over Topic Maps. That is, a QA
system capable of retrieving answers to a question
asked against one particular topic map or topic
maps collection at a time. We concentrate on an
empirical approach to extract the question focus.
The extracted focus is then anchored to a topic
map construct. This way, we map the type of the
answer as provided in the question to the type of
the answer as available in the source data.
Our system runs over semi-structured data that
encodes ontological information. The classifica-
tion scheme we propose is based on one dynamic
</bodyText>
<footnote confidence="0.4816235">
1ISO/IEC 13250:2003,
http://www.isotopicmaps.org/sam/
</footnote>
<bodyText confidence="0.9986067">
and one static layer, contrasting with previous
work that uses static taxonomies (Li and Roth,
2002).
We use the term asking point or AP when the
type of the answer is explicit, e.g. the word
operas in the question What operas did Puccini
write?
We use the term expected answer type or EAT
when the type of the answer is implicit but can be
deduced from the question using formal methods.
The question Who composed Tosca? implies that
the answer is a person. That is, person is the ex-
pected answer type.
We consider that AP takes precedence over the
EAT. That is, if the AP (the explicit focus) has
been successfully identified in the question, it is
considered to be the type of the question, and the
EAT (the implicit focus) is left aside.
The claim that the exploitation of AP yields bet-
ter results in QA over Topic Maps has been tested
with 100 questions over the Italian Opera topic
map 2. AP, EAT and the answers of the ques-
tions were manually annotated. The answers to the
questions were annotated as topic map constructs
(i.e. as topics or as occurrences).
An evaluation for QA over Topic Maps has been
devised that has shown that choosing APs as foci
leads to a much better recall and precision. A de-
tailed description of this test is beyond the scope
of this paper.
</bodyText>
<sectionHeader confidence="0.94804" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.99933675">
We approach both AP and EAT extraction with
the same machine learning technology based on
the principle of maximum entropy (Ratnaparkhi,
1998)3.
</bodyText>
<footnote confidence="0.9990272">
2http://ontopia.net/omnigator/models/
topicmap_complete.jsp?tm=opera.ltm
3OpenNLP http://opennlp.sf.net was used for
tokenization, POS tagging and parsing. Maxent http://
maxent.sf.net was used as the maximum entropy engine
</footnote>
<page confidence="0.959182">
325
</page>
<note confidence="0.964312">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 325–328,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.3558875">
What are
Gold O
</figure>
<tableCaption confidence="0.999569">
Table 1: Gold standard AP annotation
</tableCaption>
<table confidence="0.998760333333333">
Class Word count %
AskingPoint 1842 9.3%
Other 17997 90.7%
</table>
<tableCaption confidence="0.998345">
Table 2: Distribution of AP classes (word level)
</tableCaption>
<bodyText confidence="0.9999908">
We annotated a corpus of 2100 questions. 1500
of those questions come from the Li &amp; Roth cor-
pus (Li and Roth, 2002), 500 questions were taken
from the TREC-10 questions and 100 questions
were asked over the Italian Opera topic map.
</bodyText>
<subsectionHeader confidence="0.982101">
2.1 AP extraction
</subsectionHeader>
<bodyText confidence="0.994541575757576">
We propose a model for extracting AP that is based
on word tagging. As opposed to EAT, AP is con-
structed on word level not on the question level.
Table 1 provides an annotated example of AP.
Our annotation guidelines limit the AP to the
noun phrase that is expected to be the type of the
answer. As such, it is different from the notion
of focus as a noun likely to be present in the an-
swer (Ferret et al., 2001) or as what the question
is all about (Moldovan et al., 1999). For instance,
a question such as Where is the Taj Mahal? does
not yield any AP. Although the main topic is the
Taj Mahal, the answer is not expected to be in a
parent-child relationship with the subject. Instead,
the sought after type is the EAT class LOCATION.
This distinction is important for QA over semi-
structured data where the data itself is likely to be
hierarchically organized.
Asking points were annotated in 1095 (52%)
questions out of 2100. The distribution of AP
classes in the annotated data is shown in the Ta-
ble 2.
A study of the inter-annotator agreement be-
tween two human annotators has been performed
on a set of 100 questions. The Cohen’s kappa
coefficient (Cohen, 1960) was at 0.781, which
is lower than the same measure for the inter-
annotator agreement on EAT. This is an expected
result, as the AP annotation is naturally perceived
as a more complex task. Nevertheless, this allows
to qualify the inter-annotator agreement as good.
For each word, a number of features were used
for EAT and AP extraction.
</bodyText>
<table confidence="0.998950285714286">
Class Count %
TIME 136 6.5%
NUMERIC 215 10.2%
DEFINITION 281 13.4%
LOCATION 329 15.7%
HUMAN 420 20.0%
OTHER 719 34.2%
</table>
<tableCaption confidence="0.776557">
Table 3: Distribution of EAT classes (question
level)
</tableCaption>
<bodyText confidence="0.999489545454546">
by the classifier, including strings and POS-tags
on a 4-word window. The WH-word and its com-
plement were also used as features, as well as the
parsed subject of the question and the first nominal
phrase.
A simple rule-based AP extraction has also been
implemented, for comparison. It operates by re-
trieving the WH-complement from the syntactic
parse of the question and stripping the initial arti-
cles and numerals, to match the annotation guide-
lines for AP.
</bodyText>
<subsectionHeader confidence="0.989982">
2.2 EAT extraction
</subsectionHeader>
<bodyText confidence="0.99997036">
EAT was supported by a taxonomy of 6 coarse
classes: HUMAN, NUMERIC, TIME, LOCA-
TION, DEFINITION and OTHER. This selection
is fairly close to the MUC typology of Named
Entities4 which has been the basis of numerous
feature-driven classifiers because of salient formal
indices that help identify the correct class.
We purposely limited the number of EAT
classes to 6 as AP extraction already provides
a fine-grained, dynamic classification from the
question to drive the subsequent search in the topic
map.
The distribution of EAT classes in the annotated
data is shown in the Table 3.
A study of the inter-annotator agreement be-
tween two human annotators has been performed
on a set of 200 questions. The resulting Cohen’s
kappa coefficient (Cohen, 1960) of 0.8858 allows
to qualify the inter-annotator agreement as very
good.
We followed Li &amp; Roth (Li and Roth, 2002)
to implement the features for the EAT classifier.
They included strings and POS-tags, as well as
syntactic parse information (WH-words and their
complements, auxiliaries, subjects). Four lists for
</bodyText>
<footnote confidence="0.961403">
4http://www.cs.nyu.edu/cs/faculty/
grishman/NEtask20.book_1.html
</footnote>
<figure confidence="0.938865857142857">
Italian
operas
?
O
AP
AP
O
</figure>
<page confidence="0.962673">
326
</page>
<table confidence="0.9988192">
Accuracy Value Std dev Std err
EAT 0.824 0.020 0.006
Lenient AP 0.963 0.020 0.004
Exact AP 0.888 0.052 0.009
Focus (AP+EAT) 0.827 0.020 0.006
</table>
<tableCaption confidence="0.812728">
Table 4: Accuracy of the classifiers (question
level)
</tableCaption>
<bodyText confidence="0.768992333333333">
words related to locations, people, quantities and
time were derived from WordNet and encoded as
semantic features.
</bodyText>
<sectionHeader confidence="0.991082" genericHeader="method">
3 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999910837837838">
The performance of the classifiers was evaluated
on our corpus of 2100 questions annotated for AP
and EAT. The corpus was split into 80% of training
and 20% test data, and data re-sampled 10 times in
order to account for variance.
Table 4 lists the figures for the accuracy of the
classifiers, that is, the ratio between the correct in-
stances and the overall number of instances. As
the AP classifier operates on words while the EAT
classifier operates on questions, we had to estimate
the accuracy of the AP classifier per question, to
allow for comparison. Two simple metrics are pos-
sible. A lenient metric assumes that the AP extrac-
tor performed correctly in the question if there is
an overlap between the system output and the an-
notation on the question level. An exact metric as-
sumes that the AP extractor performed correctly if
there is an exact match between the system output
and the annotation.
In the example What are Italian Operas? (Ta-
ble 1), assuming the system only tagged operas as
AP, lenient accuracy will be 1, exact accuracy will
be 0, precision for the AskingPoint class will be 1
and its recall will be 0.5.
Table 5 shows EAT results by class. Tables 6
and 7 show AP results by class for the machine
learning and the rule-based classifier.
As shown in Figure 1, when AP classification is
available it is used. During the evaluation, AP was
found in 49.4% of questions.
A mathematical model has been devised to pre-
dict the accuracy of the focus extractor on an an-
notated corpus.
It is expected that the focus accuracy, that is, the
accuracy of the focus extraction system, is depen-
dent on the performance of the AP and the EAT
classifiers. Given N the total number of questions,
</bodyText>
<table confidence="0.999799">
Class Precision Recall F-Score
DEFINITION 0.887 0.800 0.841
LOCATION 0.834 0.812 0.821
HUMAN 0.902 0.753 0.820
TIME 0.880 0.802 0.838
NUMERIC 0.943 0.782 0.854
OTHER 0.746 0.893 0.812
</table>
<tableCaption confidence="0.948933">
Table 5: EAT performance by class (question
level)
</tableCaption>
<table confidence="0.999892">
Class Precision Recall F-Score
AskingPoint 0.854 0.734 0.789
Other 0.973 0.987 0.980
</table>
<tableCaption confidence="0.981485">
Table 6: AP performance by class (word level)
</tableCaption>
<table confidence="0.999873333333333">
Class Precision Recall F-Score
AskingPoint 0.608 0.479 0.536
Other 0.948 0.968 0.958
</table>
<tableCaption confidence="0.92825">
Table 7: Rule-based AP performance by class
(word level)
</tableCaption>
<bodyText confidence="0.999598333333333">
we define the branching factor, that is, the percent-
age of questions for which AP is provided by the
system, as follows:
</bodyText>
<equation confidence="0.656378">
Y =(TPAP + FPAP)
N
</equation>
<bodyText confidence="0.5125488">
Figure 1 shows that the sum AP true posi-
tives and EAT correct classifications represents the
overall number of questions that were classified
correctly. This accuracy can be further developed
to present the dependencies as follows:
</bodyText>
<equation confidence="0.690851">
AFOCUS = PAPY + AEAT (1 − Y )
</equation>
<bodyText confidence="0.999853428571429">
That is, the overall accuracy is dependent on the
precision of the AskingPoint class of the AP clas-
sifier, the accuracy of EAT and the branching fac-
tor. The branching factor itself can be predicted
using the performance of the AP classifier and the
ratio between the number of questions annotated
with AP and the total number of questions.
</bodyText>
<equation confidence="0.324427333333333">
Y =(TPAP +FNAP
N )RAP
PAP
</equation>
<page confidence="0.992083">
327
</page>
<figureCaption confidence="0.998465">
Figure 1: Focus extraction flow diagram
</figureCaption>
<sectionHeader confidence="0.999533" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999736423076923">
(Atzeni et al., 2004; Paggio et al., 2004) describe
MOSES, a multilingual QA system delivering an-
swers from Topic Maps. MOSES extracts a focus
constraint (defined after (Rooth, 1992)) as part of
the question analysis, which is evaluated to an ac-
curacy of 76% for the 85 Danish questions and
70% for the 83 Italian questions. The focus is
an ontological type dependent from the topic map,
and its extraction is based on hand-crafted rules.
In our case, focus extraction – though defined with
topic map retrieval in mind – stays clear of on-
tological dependencies so that the same question
analysis module can be applied to any topic map.
In open domain QA, machine learning ap-
proaches have proved successful since Li &amp; Roth
(Li and Roth, 2006). Despite using similar fea-
tures, the F-Score (0.824) for our EAT classes is
slightly lower than reported by Li &amp; Roth (Li and
Roth, 2006) for coarse classes. We may speculate
that the difference is primarily due to our limited
training set size (1,680 questions versus 21,500
questions for Li &amp; Roth). On the other hand, we
are not aware of any work attempting to extract AP
on word level using machine learning in order to
provide dynamic classes to a question classifica-
tion module.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="method">
5 Future work and conclusion
</sectionHeader>
<bodyText confidence="0.999869684210526">
We presented a question classification system
based on our definition of focus geared towards
QA over semi-structured data where there is a
parent-child relationship between answers and
their types. The specificity of the focus degrades
gracefully in the approach described above. That
is, we attempt the extraction of the AP when possi-
ble and fall back on the EAT extraction otherwise.
We identify the focus dynamically, instead of
relying on a static taxonomy of question types,
and we do so using machine learning techniques
throughout the application stack.
A mathematical model has been devised to pre-
dict the performance of the focus extractor.
We are currently working on the exploitation of
the results provided by the focus extractor in the
subsequent modules of the QA over Topic Maps,
namely anchoring, navigation in the topic map,
graph algorithms and reasoning.
</bodyText>
<sectionHeader confidence="0.994762" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994636666666667">
This work has been partly funded by the Flemish
government (through IWT) as part of the ITEA2
project LINDO (ITEA2-06011).
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773870967742">
P. Atzeni, R. Basili, D. H. Hansen, P. Missier, P. Pag-
gio, M. T. Pazienza, and F. M. Zanzotto. 2004.
Ontology-Based Question Answering in a Federa-
tion of University Sites: The MOSES Case Study.
In NLDB, pages 413–420.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20, No.1:37–46.
O. Ferret, B. Grau, M. Hurault-Plantet, G. Illouz,
L. Monceaux, I. Robba, and A. Vilnat. 2001. Find-
ing an Answer Based on the Recognition of the
Question Focus. In 10th Text Retrieval Conference.
X. Li and D. Roth. 2002. Learning Question Classi-
fiers. In 19th International Conference on Compu-
tational Linguistics (COLING), pages 556–562.
X. Li and D. Roth. 2006. Learning Question Classi-
fiers: The Role of Semantic Information. Journal of
Natural Language Engineering, 12(3):229–250.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,
R. Goodrum, R. Girju, and V. Rus. 1999. LASSO:
A Tool for Surfing the Answer Net. In 8th Text Re-
trieval Conference.
P. Paggio, D. H. Hansen, R. Basili, M. T. Pazienza,
and F. M. Zanzotto. 2004. Ontology-based question
analysis in a multilingual environment: the MOSES
case study. In OntoLex (LREC).
A. Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Ph.D. the-
sis, University of Pennsylvania, Philadelphia, PA.
M. Rooth. 1992. A Theory of Focus Interpretation.
Natural Language Semantics, 1(1):75–116.
</reference>
<figure confidence="0.99829">
EAT
C +I
EAT
AP extraction
TN +FN
AP AP
EAT extraction
Focus
AP
TP +FP AP
</figure>
<page confidence="0.959306">
328
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577930">
<title confidence="0.779136333333333">Learning foci for Question Answering over Topic Maps Tiphaine Application Services, Leuvensesteenweg 325, B-1932 Zaventem, Belgium ‡Aethys</title>
<email confidence="0.997664">tiphaine.dalmas@aethys.com</email>
<abstract confidence="0.999062578947368">paper introduces the concepts of askpoint answer type variof the question They are of particular importance for QA over semistructured data, as represented by Topic Maps, OWL or custom XML formats. We describe an approach to the identificaof the question questions asked to a Question Answering system Topic Maps by extracting the falling back to the antype necessary. We use known learning techniques for type and we implement novel approach to the point extraction. We also provide a mathematical model to predict the performance of the system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Atzeni</author>
<author>R Basili</author>
<author>D H Hansen</author>
<author>P Missier</author>
<author>P Paggio</author>
<author>M T Pazienza</author>
<author>F M Zanzotto</author>
</authors>
<title>Ontology-Based Question Answering in a Federation of University Sites: The MOSES Case Study. In</title>
<date>2004</date>
<booktitle>NLDB,</booktitle>
<pages>413--420</pages>
<contexts>
<context position="10750" citStr="Atzeni et al., 2004" startWordPosition="1796" endWordPosition="1799"> the overall number of questions that were classified correctly. This accuracy can be further developed to present the dependencies as follows: AFOCUS = PAPY + AEAT (1 − Y ) That is, the overall accuracy is dependent on the precision of the AskingPoint class of the AP classifier, the accuracy of EAT and the branching factor. The branching factor itself can be predicted using the performance of the AP classifier and the ratio between the number of questions annotated with AP and the total number of questions. Y =(TPAP +FNAP N )RAP PAP 327 Figure 1: Focus extraction flow diagram 4 Related work (Atzeni et al., 2004; Paggio et al., 2004) describe MOSES, a multilingual QA system delivering answers from Topic Maps. MOSES extracts a focus constraint (defined after (Rooth, 1992)) as part of the question analysis, which is evaluated to an accuracy of 76% for the 85 Danish questions and 70% for the 83 Italian questions. The focus is an ontological type dependent from the topic map, and its extraction is based on hand-crafted rules. In our case, focus extraction – though defined with topic map retrieval in mind – stays clear of ontological dependencies so that the same question analysis module can be applied to</context>
</contexts>
<marker>Atzeni, Basili, Hansen, Missier, Paggio, Pazienza, Zanzotto, 2004</marker>
<rawString>P. Atzeni, R. Basili, D. H. Hansen, P. Missier, P. Paggio, M. T. Pazienza, and F. M. Zanzotto. 2004. Ontology-Based Question Answering in a Federation of University Sites: The MOSES Case Study. In NLDB, pages 413–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>20</volume>
<pages>1--37</pages>
<contexts>
<context position="5213" citStr="Cohen, 1960" startWordPosition="866" endWordPosition="867"> Although the main topic is the Taj Mahal, the answer is not expected to be in a parent-child relationship with the subject. Instead, the sought after type is the EAT class LOCATION. This distinction is important for QA over semistructured data where the data itself is likely to be hierarchically organized. Asking points were annotated in 1095 (52%) questions out of 2100. The distribution of AP classes in the annotated data is shown in the Table 2. A study of the inter-annotator agreement between two human annotators has been performed on a set of 100 questions. The Cohen’s kappa coefficient (Cohen, 1960) was at 0.781, which is lower than the same measure for the interannotator agreement on EAT. This is an expected result, as the AP annotation is naturally perceived as a more complex task. Nevertheless, this allows to qualify the inter-annotator agreement as good. For each word, a number of features were used for EAT and AP extraction. Class Count % TIME 136 6.5% NUMERIC 215 10.2% DEFINITION 281 13.4% LOCATION 329 15.7% HUMAN 420 20.0% OTHER 719 34.2% Table 3: Distribution of EAT classes (question level) by the classifier, including strings and POS-tags on a 4-word window. The WH-word and its </context>
<context position="6954" citStr="Cohen, 1960" startWordPosition="1156" endWordPosition="1157">the MUC typology of Named Entities4 which has been the basis of numerous feature-driven classifiers because of salient formal indices that help identify the correct class. We purposely limited the number of EAT classes to 6 as AP extraction already provides a fine-grained, dynamic classification from the question to drive the subsequent search in the topic map. The distribution of EAT classes in the annotated data is shown in the Table 3. A study of the inter-annotator agreement between two human annotators has been performed on a set of 200 questions. The resulting Cohen’s kappa coefficient (Cohen, 1960) of 0.8858 allows to qualify the inter-annotator agreement as very good. We followed Li &amp; Roth (Li and Roth, 2002) to implement the features for the EAT classifier. They included strings and POS-tags, as well as syntactic parse information (WH-words and their complements, auxiliaries, subjects). Four lists for 4http://www.cs.nyu.edu/cs/faculty/ grishman/NEtask20.book_1.html Italian operas ? O AP AP O 326 Accuracy Value Std dev Std err EAT 0.824 0.020 0.006 Lenient AP 0.963 0.020 0.004 Exact AP 0.888 0.052 0.009 Focus (AP+EAT) 0.827 0.020 0.006 Table 4: Accuracy of the classifiers (question lev</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, No.1:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>M Hurault-Plantet</author>
<author>G Illouz</author>
<author>L Monceaux</author>
<author>I Robba</author>
<author>A Vilnat</author>
</authors>
<title>Finding an Answer Based on the Recognition of the Question Focus.</title>
<date>2001</date>
<booktitle>In 10th Text Retrieval Conference.</booktitle>
<contexts>
<context position="4459" citStr="Ferret et al., 2001" startWordPosition="731" endWordPosition="734">500 of those questions come from the Li &amp; Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. 2.1 AP extraction We propose a model for extracting AP that is based on word tagging. As opposed to EAT, AP is constructed on word level not on the question level. Table 1 provides an annotated example of AP. Our annotation guidelines limit the AP to the noun phrase that is expected to be the type of the answer. As such, it is different from the notion of focus as a noun likely to be present in the answer (Ferret et al., 2001) or as what the question is all about (Moldovan et al., 1999). For instance, a question such as Where is the Taj Mahal? does not yield any AP. Although the main topic is the Taj Mahal, the answer is not expected to be in a parent-child relationship with the subject. Instead, the sought after type is the EAT class LOCATION. This distinction is important for QA over semistructured data where the data itself is likely to be hierarchically organized. Asking points were annotated in 1095 (52%) questions out of 2100. The distribution of AP classes in the annotated data is shown in the Table 2. A stu</context>
</contexts>
<marker>Ferret, Grau, Hurault-Plantet, Illouz, Monceaux, Robba, Vilnat, 2001</marker>
<rawString>O. Ferret, B. Grau, M. Hurault-Plantet, G. Illouz, L. Monceaux, I. Robba, and A. Vilnat. 2001. Finding an Answer Based on the Recognition of the Question Focus. In 10th Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers.</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>556--562</pages>
<contexts>
<context position="1935" citStr="Li and Roth, 2002" startWordPosition="292" endWordPosition="295"> one particular topic map or topic maps collection at a time. We concentrate on an empirical approach to extract the question focus. The extracted focus is then anchored to a topic map construct. This way, we map the type of the answer as provided in the question to the type of the answer as available in the source data. Our system runs over semi-structured data that encodes ontological information. The classification scheme we propose is based on one dynamic 1ISO/IEC 13250:2003, http://www.isotopicmaps.org/sam/ and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). We use the term asking point or AP when the type of the answer is explicit, e.g. the word operas in the question What operas did Puccini write? We use the term expected answer type or EAT when the type of the answer is implicit but can be deduced from the question using formal methods. The question Who composed Tosca? implies that the answer is a person. That is, person is the expected answer type. We consider that AP takes precedence over the EAT. That is, if the AP (the explicit focus) has been successfully identified in the question, it is considered to be the type of the question, and th</context>
<context position="3912" citStr="Li and Roth, 2002" startWordPosition="627" endWordPosition="630">net/omnigator/models/ topicmap_complete.jsp?tm=opera.ltm 3OpenNLP http://opennlp.sf.net was used for tokenization, POS tagging and parsing. Maxent http:// maxent.sf.net was used as the maximum entropy engine 325 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 325–328, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP What are Gold O Table 1: Gold standard AP annotation Class Word count % AskingPoint 1842 9.3% Other 17997 90.7% Table 2: Distribution of AP classes (word level) We annotated a corpus of 2100 questions. 1500 of those questions come from the Li &amp; Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. 2.1 AP extraction We propose a model for extracting AP that is based on word tagging. As opposed to EAT, AP is constructed on word level not on the question level. Table 1 provides an annotated example of AP. Our annotation guidelines limit the AP to the noun phrase that is expected to be the type of the answer. As such, it is different from the notion of focus as a noun likely to be present in the answer (Ferret et al., 2001) or as what the question is all about (Moldovan et al</context>
<context position="7068" citStr="Li and Roth, 2002" startWordPosition="1174" endWordPosition="1177">of salient formal indices that help identify the correct class. We purposely limited the number of EAT classes to 6 as AP extraction already provides a fine-grained, dynamic classification from the question to drive the subsequent search in the topic map. The distribution of EAT classes in the annotated data is shown in the Table 3. A study of the inter-annotator agreement between two human annotators has been performed on a set of 200 questions. The resulting Cohen’s kappa coefficient (Cohen, 1960) of 0.8858 allows to qualify the inter-annotator agreement as very good. We followed Li &amp; Roth (Li and Roth, 2002) to implement the features for the EAT classifier. They included strings and POS-tags, as well as syntactic parse information (WH-words and their complements, auxiliaries, subjects). Four lists for 4http://www.cs.nyu.edu/cs/faculty/ grishman/NEtask20.book_1.html Italian operas ? O AP AP O 326 Accuracy Value Std dev Std err EAT 0.824 0.020 0.006 Lenient AP 0.963 0.020 0.004 Exact AP 0.888 0.052 0.009 Focus (AP+EAT) 0.827 0.020 0.006 Table 4: Accuracy of the classifiers (question level) words related to locations, people, quantities and time were derived from WordNet and encoded as semantic feat</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning Question Classifiers. In 19th International Conference on Computational Linguistics (COLING), pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers: The Role of Semantic Information.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="11471" citStr="Li and Roth, 2006" startWordPosition="1921" endWordPosition="1924">ES extracts a focus constraint (defined after (Rooth, 1992)) as part of the question analysis, which is evaluated to an accuracy of 76% for the 85 Danish questions and 70% for the 83 Italian questions. The focus is an ontological type dependent from the topic map, and its extraction is based on hand-crafted rules. In our case, focus extraction – though defined with topic map retrieval in mind – stays clear of ontological dependencies so that the same question analysis module can be applied to any topic map. In open domain QA, machine learning approaches have proved successful since Li &amp; Roth (Li and Roth, 2006). Despite using similar features, the F-Score (0.824) for our EAT classes is slightly lower than reported by Li &amp; Roth (Li and Roth, 2006) for coarse classes. We may speculate that the difference is primarily due to our limited training set size (1,680 questions versus 21,500 questions for Li &amp; Roth). On the other hand, we are not aware of any work attempting to extract AP on word level using machine learning in order to provide dynamic classes to a question classification module. 5 Future work and conclusion We presented a question classification system based on our definition of focus geared</context>
</contexts>
<marker>Li, Roth, 2006</marker>
<rawString>X. Li and D. Roth. 2006. Learning Question Classifiers: The Role of Semantic Information. Journal of Natural Language Engineering, 12(3):229–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Goodrum</author>
<author>R Girju</author>
<author>V Rus</author>
</authors>
<title>LASSO: A Tool for Surfing the Answer Net.</title>
<date>1999</date>
<booktitle>In 8th Text Retrieval Conference.</booktitle>
<contexts>
<context position="4520" citStr="Moldovan et al., 1999" startWordPosition="743" endWordPosition="746">nd Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. 2.1 AP extraction We propose a model for extracting AP that is based on word tagging. As opposed to EAT, AP is constructed on word level not on the question level. Table 1 provides an annotated example of AP. Our annotation guidelines limit the AP to the noun phrase that is expected to be the type of the answer. As such, it is different from the notion of focus as a noun likely to be present in the answer (Ferret et al., 2001) or as what the question is all about (Moldovan et al., 1999). For instance, a question such as Where is the Taj Mahal? does not yield any AP. Although the main topic is the Taj Mahal, the answer is not expected to be in a parent-child relationship with the subject. Instead, the sought after type is the EAT class LOCATION. This distinction is important for QA over semistructured data where the data itself is likely to be hierarchically organized. Asking points were annotated in 1095 (52%) questions out of 2100. The distribution of AP classes in the annotated data is shown in the Table 2. A study of the inter-annotator agreement between two human annotat</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Girju, Rus, 1999</marker>
<rawString>D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R. Goodrum, R. Girju, and V. Rus. 1999. LASSO: A Tool for Surfing the Answer Net. In 8th Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Paggio</author>
<author>D H Hansen</author>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>F M Zanzotto</author>
</authors>
<title>Ontology-based question analysis in a multilingual environment: the MOSES case study.</title>
<date>2004</date>
<booktitle>In OntoLex (LREC).</booktitle>
<contexts>
<context position="10772" citStr="Paggio et al., 2004" startWordPosition="1800" endWordPosition="1803">f questions that were classified correctly. This accuracy can be further developed to present the dependencies as follows: AFOCUS = PAPY + AEAT (1 − Y ) That is, the overall accuracy is dependent on the precision of the AskingPoint class of the AP classifier, the accuracy of EAT and the branching factor. The branching factor itself can be predicted using the performance of the AP classifier and the ratio between the number of questions annotated with AP and the total number of questions. Y =(TPAP +FNAP N )RAP PAP 327 Figure 1: Focus extraction flow diagram 4 Related work (Atzeni et al., 2004; Paggio et al., 2004) describe MOSES, a multilingual QA system delivering answers from Topic Maps. MOSES extracts a focus constraint (defined after (Rooth, 1992)) as part of the question analysis, which is evaluated to an accuracy of 76% for the 85 Danish questions and 70% for the 83 Italian questions. The focus is an ontological type dependent from the topic map, and its extraction is based on hand-crafted rules. In our case, focus extraction – though defined with topic map retrieval in mind – stays clear of ontological dependencies so that the same question analysis module can be applied to any topic map. In ope</context>
</contexts>
<marker>Paggio, Hansen, Basili, Pazienza, Zanzotto, 2004</marker>
<rawString>P. Paggio, D. H. Hansen, R. Basili, M. T. Pazienza, and F. M. Zanzotto. 2004. Ontology-based question analysis in a multilingual environment: the MOSES case study. In OntoLex (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<journal>Natural Language Semantics,</journal>
<tech>Ph.D. thesis,</tech>
<volume>1</volume>
<issue>1</issue>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3275" citStr="Ratnaparkhi, 1998" startWordPosition="536" endWordPosition="537">s has been tested with 100 questions over the Italian Opera topic map 2. AP, EAT and the answers of the questions were manually annotated. The answers to the questions were annotated as topic map constructs (i.e. as topics or as occurrences). An evaluation for QA over Topic Maps has been devised that has shown that choosing APs as foci leads to a much better recall and precision. A detailed description of this test is beyond the scope of this paper. 2 System Architecture We approach both AP and EAT extraction with the same machine learning technology based on the principle of maximum entropy (Ratnaparkhi, 1998)3. 2http://ontopia.net/omnigator/models/ topicmap_complete.jsp?tm=opera.ltm 3OpenNLP http://opennlp.sf.net was used for tokenization, POS tagging and parsing. Maxent http:// maxent.sf.net was used as the maximum entropy engine 325 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 325–328, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP What are Gold O Table 1: Gold standard AP annotation Class Word count % AskingPoint 1842 9.3% Other 17997 90.7% Table 2: Distribution of AP classes (word level) We annotated a corpus of 2100 questions. 1500 of those questions come from the</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A. Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA. M. Rooth. 1992. A Theory of Focus Interpretation. Natural Language Semantics, 1(1):75–116.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>