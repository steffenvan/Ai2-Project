<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.980671">
Improved Part-of-Speech Tagging for Online Conversational Text
with Word Clusters
</title>
<author confidence="0.75557">
Olutobi Owoputi* Brendan O’Connor* Chris Dyer*
Kevin Gimpel† Nathan Schneider* Noah A. Smith*
</author>
<affiliation confidence="0.9585825">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
†Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
</affiliation>
<email confidence="0.834828">
Corresponding author: brenocon@cs.cmu.edu
</email>
<bodyText confidence="0.998679692307692">
Online conversational text, typified by microblogs,
chat, and text messages,1 is a challenge for natu-
ral language processing. Unlike the highly edited
genres that conventional NLP tools have been de-
veloped for, conversational text contains many non-
standard lexical items and syntactic patterns. These
are the result of unintentional errors, dialectal varia-
tion, conversational ellipsis, topic diversity, and cre-
ative use of language and orthography (Eisenstein,
2013). An example is shown in Fig. 1. As a re-
sult of this widespread variation, standard model-
ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. There
</bodyText>
<sectionHeader confidence="0.957088" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998566863636364">
We consider the problem of part-of-speech
tagging for informal, online conversational
text. We systematically evaluate the use of
large-scale unsupervised word clustering
and new lexical features to improve tagging
accuracy. With these features, our system
achieves state-of-the-art tagging results on
both Twitter and IRC POS tagging tasks;
Twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). Quali-
tative analysis of these word clusters yields
insights about NLP and linguistic phenomena
in this genre. Additionally, we contribute the
first POS annotation guidelines for such text
and release a new dataset of English language
tweets annotated using these guidelines.
Tagging software, annotation guidelines, and
large-scale word clusters are available at:
http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the “CMU
Twitter Part-of-Speech Tagger” and annotated
data.
</bodyText>
<sectionHeader confidence="0.97463" genericHeader="introduction">
1 Introduction
</sectionHeader>
<footnote confidence="0.845167">
1Also referred to as computer-mediated communication.
</footnote>
<figureCaption confidence="0.972525666666667">
Figure 1: Automatically tagged tweet showing nonstan-
dard orthography, capitalization, and abbreviation. Ignor-
ing the interjections and abbreviations, it glosses as He
</figureCaption>
<bodyText confidence="0.96401404">
asked for your last name so he can add you on Facebook.
The tagset is defined in Appendix A. Refer to Fig. 2 for
word clusters corresponding to some of these words.
is preliminary work on social media part-of-speech
(POS) tagging (Gimpel et al., 2011), named entity
recognition (Ritter et al., 2011; Liu et al., 2011), and
parsing (Foster et al., 2011), but accuracy rates are
still significantly lower than traditional well-edited
genres like newswire. Even web text parsing, which
is a comparatively easier genre than social media,
lags behind newspaper text (Petrov and McDonald,
2012), as does speech transcript parsing (McClosky
et al., 2010).
To tackle the challenge of novel words and con-
structions, we create a new Twitter part-of-speech
tagger—building on previous work by Gimpel et
al. (2011)—that includes new large-scale distribu-
tional features. This leads to state-of-the-art results
in POS tagging for both Twitter and Internet Relay
Chat (IRC) text. We also annotated a new dataset of
tweets with POS tags, improved the annotations in
the previous dataset from Gimpel et al., and devel-
oped annotation guidelines for manual POS tagging
of tweets. We release all of these resources to the
research community:
</bodyText>
<listItem confidence="0.9968436">
• an open-source part-of-speech tagger for online
conversational text (§2);
• unsupervised Twitter word clusters (§3);
• an improved emoticon detector for conversational
text (§4);
</listItem>
<figure confidence="0.9150825">
ikr smh he asked fir yo last
i G O V P D A
name so he can add u on
N P O V V O P
fb lololol
∧ i
</figure>
<page confidence="0.958648">
380
</page>
<note confidence="0.666376">
Proceedings of NAACL-HLT 2013, pages 380–390,
</note>
<address confidence="0.346894">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</address>
<listItem confidence="0.993988">
• POS annotation guidelines (§5.1); and
• a new dataset of 547 manually POS-annotated
tweets (§5).
</listItem>
<sectionHeader confidence="0.950641" genericHeader="method">
2 MEMM Tagger
</sectionHeader>
<bodyText confidence="0.99929125">
Our tagging model is a first-order maximum en-
tropy Markov model (MEMM), a discriminative se-
quence model for which training and decoding are
extremely efficient (Ratnaparkhi, 1996; McCallum
et al., 2000).2 The probability of a tag yt is condi-
tioned on the input sequence x and the tag to its left
yt−1, and is parameterized by a multiclass logistic
regression:
</bodyText>
<equation confidence="0.996859">
p(yt = k  |yt−1, x, t;3) a
� �
β(trans)
exp yt−1,k + Pj β(obs)
j,k fj(x, t)
</equation>
<bodyText confidence="0.987803325">
We use transition features for every pair of labels,
and extract base observation features from token t
and neighboring tokens, and conjoin them against
all K = 25 possible outputs in our coarse tagset
(Appendix A). Our feature sets will be discussed
below in detail.
Decoding. For experiments reported in this paper,
we use the O(|x|K2) Viterbi algorithm for predic-
tion; K is the number of tags. This exactly max-
imizes p(y  |x), but the MEMM also naturally al-
lows a faster O(|x|K) left-to-right greedy decoding:
fort = 1... |x|:
yt arg maxk p(yt = k  |�yt−1, x, t; 3)
which we find is 3 times faster and yields similar ac-
curacy as Viterbi (an insignificant accuracy decrease
of less than 0.1% absolute on the DAILY547 test set
discussed below). Speed is paramount for social me-
dia analysis applications—which often require the
processing of millions to billions of messages—so
we make greedy decoding the default in the released
software.
2Although when compared to CRFs, MEMMs theoretically
suffer from the “label bias” problem (Lafferty et al., 2001), our
system substantially outperforms the CRF-based taggers of pre-
vious work; and when comparing to Gimpel et al. system with
similar feature sets, we observed little difference in accuracy.
This is consistent with conventional wisdom that the quality
of lexical features is much more important than the paramet-
ric form of the sequence model, at least in our setting: part-of-
speech tagging with a small labeled training set.
This greedy tagger runs at 800 tweets/sec. (10,000
tokens/sec.) on a single CPU core, about 40 times
faster than Gimpel et al.’s system. The tokenizer by
itself (§4) runs at 3,500 tweets/sec.3
Training and regularization. During training,
the MEMM log-likelihood for a tagged tweet (x, y)
is the sum over the observed token tags yt, each con-
ditional on the tweet being tagged and the observed
previous tag (with a start symbol before the first to-
ken in x),
</bodyText>
<equation confidence="0.999651">
`(x, y,3) = P|x|
t=1 log p(yt  |yt−1, x, t;3).
</equation>
<bodyText confidence="0.989141">
We optimize the parameters 3 with OWL-QN, an
L1-capable variant of L-BFGS (Andrew and Gao,
2007; Liu and Nocedal, 1989) to minimize the regu-
larized objective
</bodyText>
<equation confidence="0.994794333333333">
P
1 hx,yi`(x, y,3) + R(3)
�
</equation>
<bodyText confidence="0.9980802">
where N is the number of tokens in the corpus and
the sum ranges over all tagged tweets (x, y) in the
training data. We use elastic net regularization (Zou
and Hastie, 2005), which is a linear combination of
L1 and L2 penalties; here j indexes over all features:
</bodyText>
<equation confidence="0.998241">
P P
R(3) = λ1 j |βj |+ 1 2λ2 j β2j
</equation>
<bodyText confidence="0.9988775">
Using even a very small L1 penalty eliminates many
irrelevant or noisy features.
</bodyText>
<sectionHeader confidence="0.981384" genericHeader="method">
3 Unsupervised Word Clusters
</sectionHeader>
<bodyText confidence="0.999646285714286">
Our POS tagger can make use of any number of pos-
sibly overlapping features. While we have only a
small amount of hand-labeled data for training, we
also have access to billions of tokens of unlabeled
conversational text from the web. Previous work has
shown that unlabeled text can be used to induce un-
supervised word clusters which can improve the per-
formance of many supervised NLP tasks (Koo et al.,
2008; Turian et al., 2010; Täckström et al., 2012, in-
ter alia). We use a similar approach here to improve
tagging performance for online conversational text.
We also make our induced clusters publicly avail-
able in the hope that they will be useful for other
NLP tasks in this genre.
</bodyText>
<footnote confidence="0.810038">
3Runtimes observed on an Intel Core i5 2.4 GHz laptop.
</footnote>
<figure confidence="0.5842385">
arg min
β
</figure>
<page confidence="0.984345">
381
</page>
<table confidence="0.97651825">
Binary path Top words (by frequency)
A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol
A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
A3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
A4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
A5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
B 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyou
C 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn agains
D 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandora
E1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qon
E2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
F 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
G1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd &gt;:) ;-p &gt;:d 8-) ;-d
G2 11101011001011 :) (: =) :)) :] :’) =] ^_^ :))) ^.^ [: ;))((: ^__^ (= ^-^ :))))
G3 1110101100111 :( :/ -_- -.- :-( :’( d: : |:s -__- =( =/ &gt;.&lt; -___- :-/ &lt;/3 :\ -____- ;( /: :(( &gt;_&lt; =[ :[ #fml
G4 111010110001 &lt;3 xoxo &lt;33 xo &lt;333 #love s2 &lt;URL-twitition.com&gt; #neversaynever &lt;3333
</table>
<figureCaption confidence="0.91314375">
Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, in
descending order. Boldfaced words appear in the example tweet (Figure 1). The binary strings are root-to-leaf paths
through the binary cluster tree. For example usage, see e.g. search.twitter.com, bing.com/social and
urbandictionary.com.
</figureCaption>
<subsectionHeader confidence="0.999843">
3.1 Clustering Method
</subsectionHeader>
<bodyText confidence="0.999818217391304">
We obtained hierarchical word clusters via Brown
clustering (Brown et al., 1992) on a large set of
unlabeled tweets.4 The algorithm partitions words
into a base set of 1,000 clusters, and induces a hi-
erarchy among those 1,000 clusters with a series of
greedy agglomerative merges that heuristically opti-
mize the likelihood of a hidden Markov model with a
one-class-per-lexical-type constraint. Not only does
Brown clustering produce effective features for dis-
criminative models, but its variants are better unsu-
pervised POS taggers than some models developed
nearly 20 years later; see comparisons in Blunsom
and Cohn (2011). The algorithm is attractive for our
purposes since it scales to large amounts of data.
When training on tweets drawn from a single
day, we observed time-specific biases (e.g., nu-
merical dates appearing in the same cluster as the
word tonight), so we assembled our unlabeled data
from a random sample of 100,000 tweets per day
from September 10, 2008 to August 14, 2012,
and filtered out non-English tweets (about 60% of
the sample) using langid.py (Lui and Baldwin,
2012).5 Each tweet was processed with our to-
</bodyText>
<footnote confidence="0.999744666666667">
4As implemented by Liang (2005), v. 1.3: https://
github.com/percyliang/brown-cluster
5https://github.com/saffsd/langid.py
</footnote>
<bodyText confidence="0.960621583333333">
kenizer and lowercased. We normalized all at-
mentions to (@MENTION) and URLs/email ad-
dresses to their domains (e.g. http://bit.ly/
dP8rR8 ==&gt;- (URL-bit.ly)). In an effort to reduce
spam, we removed duplicated tweet texts (this also
removes retweets) before word clustering. This
normalization and cleaning resulted in 56 million
unique tweets (847 million tokens). We set the
clustering software’s count threshold to only cluster
words appearing 40 or more times, yielding 216,856
word types, which took 42 hours to cluster on a sin-
gle CPU.
</bodyText>
<subsectionHeader confidence="0.99983">
3.2 Cluster Examples
</subsectionHeader>
<bodyText confidence="0.999801538461538">
Fig. 2 shows example clusters. Some of the chal-
lenging words in the example tweet (Fig. 1) are high-
lighted. The term lololol (an extension of lol for
“laughing out loud”) is grouped with a large number
of laughter acronyms (A1: “laughing my (fucking)
ass off,” “cracking the fuck up”). Since expressions
of laughter are so prevalent on Twitter, the algorithm
creates another laughter cluster (A1’s sibling A2),
that tends to have onomatopoeic, non-acronym vari-
ants (e.g., haha). The acronym ikr (“I know, right?”)
is grouped with expressive variations of “yes” and
“no” (A4). Note that A1–A4 are grouped in a fairly
specific subtree; and indeed, in this message ikr and
</bodyText>
<page confidence="0.991581">
382
</page>
<bodyText confidence="0.999538486486487">
lololol are both tagged as interjections.
smh (“shaking my head,” indicating disapproval)
seems related, though is always tagged in the an-
notated data as a miscellaneous abbreviation (G);
the difference between acronyms that are interjec-
tions versus other acronyms may be complicated.
Here, smh is in a related but distinct subtree from the
above expressions (A5); its usage in this example
is slightly different from its more common usage,
which it shares with the other words in its cluster:
message-ending expressions of commentary or emo-
tional reaction, sometimes as a metacomment on the
author’s message; e.g., Maybe you could get a guy
to date you if you actually respected yourself #smh
or There is really NO reason why other girls should
send my boyfriend a goodmorning text #justsaying.
We observe many variants of categories tradition-
ally considered closed-class, including pronouns (B:
u = “you”) and prepositions (C: fir = “for”).
There is also evidence of grammatical categories
specific to conversational genres of English; clusters
E1–E2 demonstrate variations of single-word con-
tractions for “going to” and “trying to,” some of
which have more complicated semantics.6
Finally, the HMM learns about orthographic vari-
ants, even though it treats all words as opaque sym-
bols; cluster F consists almost entirely of variants
of “so,” their frequencies monotonically decreasing
in the number of vowel repetitions—a phenomenon
called “expressive lengthening” or “affective length-
ening” (Brody and Diakopoulos, 2011; Schnoebe-
len, 2012). This suggests a future direction to jointly
model class sequence and orthographic informa-
tion (Clark, 2003; Smith and Eisner, 2005; Blunsom
and Cohn, 2011).
We have built an HTML viewer to browse these
and numerous other interesting examples.7
</bodyText>
<subsectionHeader confidence="0.999507">
3.3 Emoticons and Emoji
</subsectionHeader>
<bodyText confidence="0.737596875">
We use the term emoticon to mean a face or icon
constructed with traditional alphabetic or punctua-
6One coauthor, a native speaker of the Texan English dialect,
notes “finna” (short for “fixing to”, cluster E1) may be an im-
mediate future auxiliary, indicating an immediate future tense
that is present in many languages (though not in standard En-
glish). To illustrate: “She finna go” approximately means “She
will go,” but sooner, in the sense of “She is about to go.”
</bodyText>
<footnote confidence="0.8173705">
7http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
</footnote>
<bodyText confidence="0.99995003125">
tion symbols, and emoji to mean symbols rendered
in software as small pictures, in line with the text.
Since our tokenizer is careful to preserve emoti-
cons and other symbols (see §4), they are clustered
just like other words. Similar emoticons are clus-
tered together (G1–G4), including separate clusters
of happy [[ :) =) ^_^ ]], sad/disappointed [[ :/ :(
-_- &lt;/3 ]], love [[ ❑xoxo ❑.❑ ]] and winking [[
;) (^_-) ]] emoticons. The clusters are not per-
fectly aligned with our POS annotation guidelines;
for example, the “sad” emoticon cluster included
emotion-bearing terms that our guidelines define as
non-emoticons, such as #ugh, #tear, and #fml (“fuck
my life”), though these seem potentially useful for
sentiment analysis.
One difficult task is classifying different types
of symbols in tweets: our annotation guidelines
differentiate between emoticons, punctuation, and
garbage (apparently non-meaningful symbols or to-
kenization errors). Several Unicode character ranges
are reserved for emoji-style symbols (including the
three Unicode hearts in G4); however, depending
on the user’s software, characters in these ranges
might be rendered differently or not at all. We
have found instances where the clustering algo-
rithm groups proprietary iOS emoji symbols along
with normal emoticons; for example, the character
U+E056, which is interpreted on iOS as a smiling
face, is in the same G2 cluster as smiley face emoti-
cons. The symbol U+E12F, which represents a pic-
ture of a bag of money, is grouped with the words
cash and money.
</bodyText>
<subsectionHeader confidence="0.977644">
3.4 Cluster-Based Features
</subsectionHeader>
<bodyText confidence="0.999692571428571">
Since Brown clusters are hierarchical in a binary
tree, each word is associated with a tree path rep-
resented as a bitstring with length &lt; 16; we use pre-
fixes of the bitstring as features (for all prefix lengths
E 12, 4, 6,... ,16}). This allows sharing of statisti-
cal strength between similar clusters. Using prefix
features of hierarchical clusters in this way was sim-
ilarly found to be effective for named-entity recog-
nition (Turian et al., 2010) and Twitter POS tag-
ging (Ritter et al., 2011).
When checking to see if a word is associated with
a cluster, the tagger first normalizes the word using
the same techniques as described in §3.1, then cre-
ates a priority list of fuzzy match transformations
</bodyText>
<page confidence="0.995792">
383
</page>
<bodyText confidence="0.999997142857143">
of the word by removing repeated punctuation and
repeated characters. If the normalized word is not
in a cluster, the tagger considers the fuzzy matches.
Although only about 3% of the tokens in the devel-
opment set (§6) did not appear in a clustering, this
method resulted in a relative error decrease of 18%
among such word tokens.
</bodyText>
<subsectionHeader confidence="0.978905">
3.5 Other Lexical Features
</subsectionHeader>
<bodyText confidence="0.99993495">
Besides unsupervised word clusters, there are two
other sets of features that contain generalized lexi-
cal class information. We use the tag dictionary fea-
ture from Gimpel et al., which adds a feature for
a word’s most frequent part-of-speech tag.8 This
can be viewed as a feature-based domain adaptation
method, since it gives lexical type-level information
for standard English words, which the model learns
to map between PTB tags to the desired output tags.
Second, since the lack of consistent capitalization
conventions on Twitter makes it especially difficult
to recognize names—Gimpel et al. and Foster et
al. (2011) found relatively low accuracy on proper
nouns—we added a token-level name list feature,
which fires on (non-function) words from names
from several sources: Freebase lists of celebrities
and video games (Google, 2012), the Moby Words
list of US Locations,9 and lists of male, female, fam-
ily, and proper names from Mark Kantrowitz’s name
corpus.10
</bodyText>
<sectionHeader confidence="0.97821" genericHeader="method">
4 Tokenization and Emoticon Detection
</sectionHeader>
<bodyText confidence="0.994622785714286">
Word segmentation on Twitter is challenging due
to the lack of orthographic conventions; in partic-
ular, punctuation, emoticons, URLs, and other sym-
bols may have no whitespace separation from textual
8Frequencies came from the Wall Street Journal and Brown
corpus sections of the Penn Treebank. If a word has multiple
PTB tags, each tag is a feature with value for the frequency rank;
e.g. for three different tags in the PTB, this feature gives a value
of 1 for the most frequent tag, 2/3 for the second, etc. Coarse
versions of the PTB tags are used (Petrov et al., 2011). While
88% of words in the dictionary have only one tag, using rank
information seemed to give a small but consistent gain over only
using the most common tag, or using binary features conjoined
with rank as in Gimpel et al.
</bodyText>
<footnote confidence="0.99195325">
9http://icon.shef.ac.uk/Moby/mwords.html
10http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/areas/nlp/corpora/names/
0.html
</footnote>
<bodyText confidence="0.999771392857143">
words (e.g. no:-d,yes should parse as four tokens),
and internally may contain alphanumeric symbols
that could be mistaken for words: a naive split(/[^a-
zA-Z0-9]+/) tokenizer thinks the words “p” and “d”
are among the top 100 most common words on Twit-
ter, due to misanalysis of :p and :d. Traditional Penn
Treebank–style tokenizers are hardly better, often
breaking a string of punctuation characters into a
single token per character.
We rewrote twokenize (O’Connor et al.,
2010), a rule-based tokenizer, emoticon, and URL
detector, for use in the tagger. Emoticons are es-
pecially challenging, since they are open-class and
productive. We revise O’Connor et al.’s regular ex-
pression grammar that describes possible emoticons,
adding a grammar of horizontal emoticons (e.g. -_-),
known as “Eastern-style,”11 though we observe high
usage in English-speaking Twitter (Fig. 2, G2–G3).
We also add a number of other improvements to the
patterns. Because this system was used as prepro-
cessing for the word clustering experiment in §3, we
were able to infer the emoticon clusters in Fig. 2.
Furthermore, whether a token matches the emoticon
pattern is also used as a feature in the tagger (§2).
URL recognition is also difficult, since the http://
is often dropped, resulting in protocol-less URLs
like about.me. We add recognition patterns for these
by using a list of top-level and country domains.
</bodyText>
<sectionHeader confidence="0.997129" genericHeader="method">
5 Annotated Dataset
</sectionHeader>
<bodyText confidence="0.999937266666667">
Gimpel et al. (2011) provided a dataset of POS-
tagged tweets consisting almost entirely of tweets
sampled from one particular day (October 27,
2010). We were concerned about overfitting to time-
specific phenomena; for example, a substantial frac-
tion of the messages are about a basketball game
happening that day.
We created a new test set of 547 tweets for eval-
uation. The test set consists of one random English
tweet from every day between January 1, 2011 and
June 30, 2012. In order for a tweet to be considered
English, it had to contain at least one English word
other than a URL, emoticon, or at-mention. We no-
ticed biases in the outputs of langid.py, so we
instead selected these messages completely manu-
</bodyText>
<footnote confidence="0.9496675">
11http://en.wikipedia.org/wiki/List—of—
emoticons
</footnote>
<page confidence="0.996598">
384
</page>
<bodyText confidence="0.973175">
ally (going through a random sample of one day’s
messages until an English message was found).
</bodyText>
<subsectionHeader confidence="0.992906">
5.1 Annotation Methodology
</subsectionHeader>
<bodyText confidence="0.999981636363637">
Gimpel et al. provided a tagset for Twitter (shown in
Appendix A), which we used unmodified. The orig-
inal annotation guidelines were not published, but in
this work we recorded the rules governing tagging
decisions and made further revisions while annotat-
ing the new data.12 Some of our guidelines reiter-
ate or modify rules made by Penn Treebank annota-
tors, while others treat specific phenomena found on
Twitter (refer to the next section).
Our tweets were annotated by two annotators who
attempted to match the choices made in Gimpel et
al.’s dataset. The annotators also consulted the POS
annotations in the Penn Treebank (Marcus et al.,
1993) as an additional reference. Differences were
reconciled by a third annotator in discussion with all
annotators.13 During this process, an inconsistency
was found in Gimpel et al.’s data, which we cor-
rected (concerning the tagging of this/that, a change
to 100 labels, 0.4%). The new version of Gimpel et
al.’s data (called OCT27), as well as the newer mes-
sages (called DAILY547), are both included in our
data release.
</bodyText>
<subsectionHeader confidence="0.99678">
5.2 Compounds in Penn Treebank vs. Twitter
</subsectionHeader>
<bodyText confidence="0.99976125">
Ritter et al. (2011) annotated tweets using an aug-
mented version of the PTB tagset and presumably
followed the PTB annotation guidelines. We wrote
new guidelines because the PTB conventions are in-
appropriate for Twitter in several ways, as shown in
the design of Gimpel et al.’s tagset. Importantly,
“compound” tags (e.g., nominal+verbal and nomi-
nal+possessive) are used because tokenization is dif-
ficult or seemingly impossible for the nonstandard
word forms that are commonplace in conversational
text.
For example, the PTB tokenization splits contrac-
tions containing apostrophes: I’m ==&gt;. I/PRP ’m/VBP.
But conversational text often contains variants that
resist a single PTB tag (like im), or even chal-
lenge traditional English grammatical categories
</bodyText>
<footnote confidence="0.921028666666667">
12The annotation guidelines are available online at
http://www.ark.cs.cmu.edu/TweetNLP/
13Annotators are coauthors of this paper.
</footnote>
<bodyText confidence="0.999940813953489">
(like imma or umma, which both mean “I am go-
ing to”). One strategy would be to analyze these
forms into a PTB-style tokenization, as discussed in
Forsyth (2007), who proposes to analyze doncha as
do/VBP ncha/PRP, but notes it would be difficult.
We think this is impossible to handle in the rule-
based framework used by English tokenizers, given
the huge (and possibly growing) number of large
compounds like imma, gonna, w/that, etc. These
are not rare: the word clustering algorithm discov-
ers hundreds of such words as statistically coherent
classes (e.g. clusters E1 and E2 in Fig. 2); and the
word imma is the 962nd most common word in our
unlabeled corpus, more frequent than cat or near.
We do not attempt to do Twitter “normalization”
into traditional written English (Han and Baldwin,
2011), which we view as a lossy translation task. In
fact, many of Twitter’s unique linguistic phenomena
are due not only to its informal nature, but also a set
of authors that heavily skews towards younger ages
and minorities, with heavy usage of dialects that are
different than the standard American English most
often seen in NLP datasets (Eisenstein, 2013; Eisen-
stein et al., 2011). For example, we suspect that
imma may implicate tense and aspect markers from
African-American Vernacular English.14 Trying to
impose PTB-style tokenization on Twitter is linguis-
tically inappropriate: should the lexico-syntactic be-
havior of casual conversational chatter by young mi-
norities be straightjacketed into the stylistic conven-
tions of the 1980s Wall Street Journal? Instead, we
would like to directly analyze the syntax of online
conversational text on its own terms.
Thus, we choose to leave these word forms un-
tokenized and use compound tags, viewing com-
positional multiword analysis as challenging fu-
ture work.15 We believe that our strategy is suf-
ficient for many applications, such as chunking or
named entity recognition; many applications such
as sentiment analysis (Turney, 2002; Pang and Lee,
2008, §4.2.3), open information extraction (Carl-
son et al., 2010; Fader et al., 2011), and informa-
tion retrieval (Allan and Raghavan, 2002) use POS
</bodyText>
<footnote confidence="0.9598278">
14See “Tense and aspect” examples in http:
//en.wikipedia.org/wiki/African_American_
Vernacular_English
15For example, wtf has compositional behavior in “Wtf just
happened??”, but only debatably so in “Huh wtf”.
</footnote>
<page confidence="0.997207">
385
</page>
<figure confidence="0.994945705882353">
#Msg. #Tok. Tagset Dates
OCT27
DAILY547
NPSCHAT
1,827 26,594 App. A Oct 27-28, 2010
547 7,707 App. A Jan 2011–Jun 2012
10,578 44,997 PTB-like Oct–Nov 2006
(w/o sys. msg.)
7,935 37,081
RITTERTW
789 15,185 PTB-like unknown
Tagging Accuracy
75 80 85 90
●
●
●
● ● ● ●
</figure>
<tableCaption confidence="0.963066666666667">
Table 1: Annotated datasets: number of messages, to-
kens, tagset, and date range. More information in §5,
§6.3, and §6.2.
</tableCaption>
<bodyText confidence="0.998560090909091">
patterns that seem quite compatible with our ap-
proach. More complex downstream processing like
parsing is an interesting challenge, since contraction
parsing on traditional text is probably a benefit to
current parsers. We believe that any PTB-trained
tool requires substantial retraining and adaptation
for Twitter due to the huge genre and stylistic differ-
ences (Foster et al., 2011); thus tokenization conven-
tions are a relatively minor concern. Our simple-to-
annotate conventions make it easier to produce new
training data.
</bodyText>
<figure confidence="0.660780666666667">
u
s
o
</figure>
<sectionHeader confidence="0.897688" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999932666666667">
We are primarily concerned with performance on
our annotated datasets described in §5 (OCT27,
DAILY547), though for comparison to previous
work we also test on other corpora (RITTERTW in
§6.2, NPSCHAT in §6.3). The annotated datasets
are listed in Table 1.
</bodyText>
<subsectionHeader confidence="0.998519">
6.1 Main Experiments
</subsectionHeader>
<bodyText confidence="0.903906647058823">
We use OCT27 to refer to the entire dataset de-
scribed in Gimpel et al.; it is split into train-
ing, development, and test portions (OCT27TRAIN,
OCT27DEV, OCT27TEST). We use DAILY547 as
an additional test set. Neither OCT27TEST nor
DAILY547 were extensively evaluated against until
final ablation testing when writing this paper.
The total number of features is 3.7 million, all
of which are used under pure L2 regularization; but
only 60,000 are selected by elastic net regularization
with (A1, A2) = (0.25, 2), which achieves nearly the
same (but no better) accuracy as pure L2,16 and we
use it for all experiments. We observed that it was
16We conducted a grid search for the regularizer values on
part of DAILY547, and many regularizer values give the best or
nearly the best results. We suspect a different setup would have
yielded similar results.
</bodyText>
<figureCaption confidence="0.9489642">
Figure 3: OCT27 development set accuracy using only
clusters as features.
Table 3: DAILY547 accuracies (%) for tokens in and out
of a traditional dictionary, for models reported in rows 1
and 3 of Table 2.
</figureCaption>
<bodyText confidence="0.842817">
possible to get radically smaller models with only
</bodyText>
<subsectionHeader confidence="0.745407">
Numbe of Unlabeled Tweets
</subsectionHeader>
<bodyText confidence="0.928320857142857">
a slight degradation in performance: (4, 0.06) has
0.5% worse accuracy but uses only 1,632 features, a
small enough number to browse through manually.
First, we evaluate on the new test set, training on
all of OCT27. Due to DAILY547’s statistical repre-
sentativeness, we believe this gives the best view of
the tagger’s accuracy on English Twitter text. The
full tagger attains 93.2% accuracy (final row of Ta-
ble 2).
To facilitate comparisons with previous work, we
ran a series of experiments training only on OCT27’s
training and development sets, then report test re-
sults on both OCT27TEST and all of DAILY547,
shown in Table 2. Our tagger achieves substantially
higher accuracy than Gimpel et al. (2011).17
Feature ablation. A number of ablation tests in-
dicate the word clusters are a very strong source of
lexical knowledge. When dropping the tag dictio-
naries and name lists, the word clusters maintain
most of the accuracy (row 2). If we drop the clus-
ters and rely only on tag dictionaries and namelists,
accuracy decreases significantly (row 3). In fact,
we can remove all observation features except for
word clusters—no word features, orthographic fea-
17These numbers differ slightly from those reported by Gim-
pel et al., due to the corrections we made to the OCT27 data,
noted in Section 5.1. We retrained and evaluated their tagger
(version 0.2) on our corrected dataset.
</bodyText>
<figure confidence="0.997245294117647">
Model
No clusters
Total tokens
In dict. Out of dict.
● ●
93.4 85.0
92.0 (−1.4) 79.3 (−5.7)
●
4,808 1,394
e
O&apos; C
Fullerae
vera
0.6
0
1e+03 1e+05 1e+07
Number of Unlabeled Tweets
</figure>
<page confidence="0.992531">
386
</page>
<table confidence="0.998807777777778">
Feature set OCT27TEST DAILY547 NPSCHATTEST
All features 91.60 92.80 91.19
with clusters; without tagdicts, namelists 91.15 92.38 90.66
without clusters; with tagdicts, namelists 89.81 90.81 90.00
only clusters (and transitions) 89.50 90.54 89.55
without clusters, tagdicts, namelists 86.86 88.30 88.26
Gimpel et al. (2011) version 0.2 88.89 89.17
Inter-annotator agreement (Gimpel et al., 2011) 92.2
Model trained on all OCT27 93.2
</table>
<tableCaption confidence="0.994333">
Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are
roughly f0.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.
</tableCaption>
<figure confidence="0.894226875">
1
2
3
4
5
6
7
8
</figure>
<bodyText confidence="0.968524903225807">
tures, affix n-grams, capitalization, emoticon pat-
terns, etc.—and the accuracy is in fact still better
than the previous work (row 4).18
We also wanted to know whether to keep the tag
dictionary and name list features, but the splits re-
ported in Fig. 2 did not show statistically signifi-
cant differences; so to better discriminate between
ablations, we created a lopsided train/test split of
all data with a much larger test portion (26,974 to-
kens), having greater statistical power (tighter con-
fidence intervals of f 0.3%).19 The full system got
90.8% while the no–tag dictionary, no-namelists ab-
lation had 90.0%, a statistically significant differ-
ence. Therefore we retain these features.
Compared to the tagger in Gimpel et al., most of
our feature changes are in the new lexical features
described in §3.5.20 We do not reuse the other lex-
ical features from the previous work, including a
phonetic normalizer (Metaphone), a name list con-
sisting of words that are frequently capitalized, and
distributional features trained on a much smaller un-
labeled corpus; they are all worse than our new
lexical features described here. (We did include,
however, a variant of the tag dictionary feature that
uses phonetic normalization for lookup; it seemed to
yield a small improvement.)
18Furthermore, when evaluating the clusters as unsupervised
(hard) POS tags, we obtain a many-to-one accuracy of 89.2%
on DAILY547. Before computing this, we lowercased the text
to match the clusters and removed tokens tagged as URLs and
at-mentions.
19Reported confidence intervals in this paper are 95% bino-
mial normal approximation intervals for the proportion of cor-
rectly tagged tokens: f1.96.,/p(1 − p)/ntokm &lt; 1/\/n.
20Details on the exact feature set are available in a technical
report (Owoputi et al., 2012), also available on the website.
Non-traditional words. The word clusters are es-
pecially helpful with words that do not appear in tra-
ditional dictionaries. We constructed a dictionary
by lowercasing the union of the ispell ‘American’,
‘British’, and ‘English’ dictionaries, plus the stan-
dard Unix words file from Webster’s Second Inter-
national dictionary, totalling 260,985 word types.
After excluding tokens defined by the gold stan-
dard as punctuation, URLs, at-mentions, or emoti-
cons,21 22% of DAILY547’s tokens do not appear in
this dictionary. Without clusters, they are very dif-
ficult to classify (only 79.2% accuracy), but adding
clusters generates a 5.7 point improvement—much
larger than the effect on in-dictionary tokens (Ta-
ble 3).
Varying the amount of unlabeled data. A tagger
that only uses word clusters achieves an accuracy of
88.6% on the OCT27 development set.22 We created
several clusterings with different numbers of unla-
beled tweets, keeping the number of clusters con-
stant at 800. As shown in Fig. 3, there was initially
a logarithmic relationship between number of tweets
and accuracy, but accuracy (and lexical coverage)
levels out after 750,000 tweets. We use the largest
clustering (56 million tweets and 1,000 clusters) as
the default for the released tagger.
</bodyText>
<subsectionHeader confidence="0.998479">
6.2 Evaluation on RITTERTW
</subsectionHeader>
<footnote confidence="0.933949555555555">
Ritter et al. (2011) annotated a corpus of 787
tweets23 with a single annotator, using the PTB
21We retain hashtags since by our guidelines a #-prefixed to-
ken is ambiguous between a hashtag and a normal word, e.g. #1
or going #home.
22The only observation features are the word clusters of a
token and its immediate neighbors.
23https://github.com/aritter/twitter_nlp/
blob/master/data/annotated/pos.txt
</footnote>
<page confidence="0.998555">
387
388
</page>
<tableCaption confidence="0.91516675">
Table 4: Accuracy comparison on Ritter et al.’s Twitter
POS corpus (§6.2).
Table 5: Accuracy comparison on Forsyth’s NPSCHAT
IRC POS corpus (§6.3).
</tableCaption>
<bodyText confidence="0.999481777777778">
tagset plus several Twitter-specific tags, referred
to in Table 1 as RITTERTW. Linguistic concerns
notwithstanding (§5.2), for a controlled comparison,
we train and test our system on this data with the
same 4-fold cross-validation setup they used, attain-
ing 90.0% (f0.5%) accuracy. Ritter et al.’s CRF-
based tagger had 85.3% accuracy, and their best tag-
ger, trained on a concatenation of PTB, IRC, and
Twitter, achieved 88.3% (Table 4).
</bodyText>
<note confidence="0.5721">
6.3 IRC: Evaluation on NPSCHAT
</note>
<figureCaption confidence="0.559988714285714">
IRC is another medium of online conversational
text, with similar emoticons, misspellings, abbrevi-
ations and acronyms as Twitter data. We evaluate
our tagger on the NPS Chat Corpus (Forsyth and
Martell, 2007),24 a PTB-part-of-speech annotated
dataset of Internet Relay Chat (IRC) room messages
from 2006.
</figureCaption>
<bodyText confidence="0.993837148148148">
First, we compare to a tagger in the same setup as
experiments on this data in Forsyth (2007), training
on 90% of the data and testing on 10%; we average
results across 10-fold cross-validation.25 The full
tagger model achieved 93.4% (f0.3%) accuracy,
significantly improving over the best result they re-
port, 90.8% accuracy with a tagger trained on a mix
of several POS-annotated corpora.
We also perform the ablation experiments on this
corpus, with a slightly different experimental setup:
we first filter out system messages then split data
24Release 1.0: http://faculty.nps.edu/
cmartell/NPSChat.htm
25Forsyth actually used 30 different 90/10 random splits; we
prefer cross-validation because the same test data is never re-
peated, thus allowing straightforward confidence estimation of
accuracy from the number of tokens (via binomial sample vari-
ance, footnote 19). In all cases, the models are trained on the
same amount of data (90%).
into 5,067 training and 2,868 test messages. Results
show a similar pattern as the Twitter data (see final
column of Table 2). Thus the Twitter word clusters
are also useful for language in the medium of text
chat rooms; we suspect these clusters will be appli-
cable for deeper syntactic and semantic analysis in
other online conversational text mediums, such as
text messages and instant messages.
</bodyText>
<sectionHeader confidence="0.981632" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998053333333333">
We have constructed a state-of-the-art part-of-
speech tagger for the online conversational text
genres of Twitter and IRC, and have publicly re-
leased our new evaluation data, annotation guide-
lines, open-source tagger, and word clusters at
http://www.ark.cs.cmu.edu/TweetNLP.
</bodyText>
<sectionHeader confidence="0.776101" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<table confidence="0.448763666666667">
This research was supported in part by the National Sci-
ence Foundation (IIS-0915187 and IIS-1054319).
A Part-of-Speech Tagset
</table>
<tableCaption confidence="0.465378">
Table 6: POS tagset from Gimpel et al. (2011) used in this
paper, and described further in the released annotation
guidelines.
</tableCaption>
<figure confidence="0.998424106060606">
Accuracy
90.0 f 0.5
Tagger
This work
Ritter et al. (2011), basic CRF tagger
Ritter et al. (2011), trained on more data
85.3
88.3
Accuracy
93.4 f 0.3
Tagger
This work
Forsyth (2007)
90.8
common noun
pronoun (personal/WH; not possessive)
proper noun
nominal + possessive
proper noun + possessive
verb including copula, auxiliaries
nominal + verbal (e.g. i’m), verbal + nominal (let’s)
proper noun + verbal
adjective
adverb
interjection
determiner
pre- or postposition, or subordinating conjunction
coordinating conjunction
verb particle
existential there, predeterminers
X + verbal
hashtag (indicates topic/category for tweet)
at-mention (indicates a user as a recipient of a tweet)
discourse marker, indications of continuation across
multiple tweets
URL or email address
emoticon
numeral
punctuation
other abbreviations, foreign words, possessive endings,
symbols, garbage
N
O
^
S
Z
V
L
M
A
R
i
D
P
&amp;
T
X
Y
#
@
~
U
E
$
,
G
</figure>
<sectionHeader confidence="0.878796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895704761905">
J. Allan and H. Raghavan. 2002. Using part-of-speech
patterns to reduce query ambiguity. In Proc. of SIGIR.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech in-
duction. In Proc. of ACL.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proc. of EMNLP.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr-
uschka Jr, and T. M. Mitchell. 2010. Toward an archi-
tecture for never-ending language learning. In Proc. of
AAAI.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proc. of EACL.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
J. Eisenstein. 2013. What to do about bad language on
the internet. In Proc. of NAACL.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proc. of
EMNLP.
E. N. Forsyth and C. H. Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Proc. of ICSC.
E. N. Forsyth. 2007. Improving automated lexical and
discourse analysis of online chat dialog. Master’s the-
sis, Naval Postgraduate School.
J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,
J. Nivre, D. Hogan, and J. van Genabith. 2011. #hard-
toparse: POS tagging and parsing the Twitterverse. In
Proc. of AAAI-11 Workshop on Analysing Microtext.
K. Gimpel, N. Schneider, B. O’Connor, D. Das, D. Mills,
J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,
and N. A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proc. of ACL.
Google. 2012. Freebase data dumps. http://
download.freebase.com/datadumps/.
B. Han and T. Baldwin. 2011. Lexical normalisation of
short text messages: Makn sens a #twitter. In Proc. of
ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ofICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute of
Technology.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical programming, 45(1).
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recogniz-
ing named entities in tweets. In Proc. ofACL.
M. Lui and T. Baldwin. 2012. langid.py: An off-the-
shelf language identification tool. In Proc. of ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2).
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Proc. of
NAACL.
B. O’Connor, M. Krieger, and D. Ahn. 2010.
TweetMotif: exploratory search and topic summariza-
tion for Twitter. In Proc. of AAAI Conference on We-
blogs and Social Media.
O. Owoputi, B. O’Connor, C. Dyer, K. Gimpel, and
N. Schneider. 2012. Part-of-speech tagging for Twit-
ter: Word clusters and other advances. Technical Re-
port CMU-ML-12-107, Carnegie Mellon University.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimental
study. In Proc. of EMNLP.
T. Schnoebelen. 2012. Do you smile with your nose?
Stylistic variation in Twitter emoticons. University of
Pennsylvania Working Papers in Linguistics, 18(2):14.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
O. Täckström, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proc. of NAACL.
</reference>
<page confidence="0.988357">
389
</page>
<reference confidence="0.9992439">
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301–320.
</reference>
<page confidence="0.998228">
390
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.085218">
<title confidence="0.9739915">Improved Part-of-Speech Tagging for Online Conversational with Word Clusters</title>
<author confidence="0.959668">Brendan Chris Noah A</author>
<affiliation confidence="0.4392105">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, Technological Institute at Chicago, Chicago, IL 60637,</affiliation>
<abstract confidence="0.983038212121212">author: Online conversational text, typified by microblogs, and text is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at:</abstract>
<web confidence="0.850409">http://www.ark.cs.cmu.edu/TweetNLP</web>
<note confidence="0.761736666666667">This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>H Raghavan</author>
</authors>
<title>Using part-of-speech patterns to reduce query ambiguity.</title>
<date>2002</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="25340" citStr="Allan and Raghavan, 2002" startWordPosition="4075" endWordPosition="4078">tic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 385 #Msg. #Tok. Tagset Dates OCT27 DAILY547 NPSCHAT 1,827 26,594 App. A Oct 27-28, 2010 547 7,707 App. A Jan 2011–Jun 2012 10,578 44,997 PTB-like Oct–Nov 2006 (w/o sys. msg.) 7,935 37,081 RITTERTW 789 15,185 PTB-like unknown Tagging Accuracy 75 80 85 90 ● ● ● ● ● ● ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, </context>
</contexts>
<marker>Allan, Raghavan, 2002</marker>
<rawString>J. Allan and H. Raghavan. 2002. Using part-of-speech patterns to reduce query ambiguity. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>J Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="6443" citStr="Andrew and Gao, 2007" startWordPosition="1013" endWordPosition="1016">labeled training set. This greedy tagger runs at 800 tweets/sec. (10,000 tokens/sec.) on a single CPU core, about 40 times faster than Gimpel et al.’s system. The tokenizer by itself (§4) runs at 3,500 tweets/sec.3 Training and regularization. During training, the MEMM log-likelihood for a tagged tweet (x, y) is the sum over the observed token tags yt, each conditional on the tweet being tagged and the observed previous tag (with a start symbol before the first token in x), `(x, y,3) = P|x| t=1 log p(yt |yt−1, x, t;3). We optimize the parameters 3 with OWL-QN, an L1-capable variant of L-BFGS (Andrew and Gao, 2007; Liu and Nocedal, 1989) to minimize the regularized objective P 1 hx,yi`(x, y,3) + R(3) � where N is the number of tokens in the corpus and the sum ranges over all tagged tweets (x, y) in the training data. We use elastic net regularization (Zou and Hastie, 2005), which is a linear combination of L1 and L2 penalties; here j indexes over all features: P P R(3) = λ1 j |βj |+ 1 2λ2 j β2j Using even a very small L1 penalty eliminates many irrelevant or noisy features. 3 Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a smal</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>G. Andrew and J. Gao. 2007. Scalable training of L1-regularized log-linear models. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>A hierarchical PitmanYor process HMM for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10145" citStr="Blunsom and Cohn (2011)" startWordPosition="1659" endWordPosition="1662">e obtained hierarchical word clusters via Brown clustering (Brown et al., 1992) on a large set of unlabeled tweets.4 The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint. Not only does Brown clustering produce effective features for discriminative models, but its variants are better unsupervised POS taggers than some models developed nearly 20 years later; see comparisons in Blunsom and Cohn (2011). The algorithm is attractive for our purposes since it scales to large amounts of data. When training on tweets drawn from a single day, we observed time-specific biases (e.g., numerical dates appearing in the same cluster as the word tonight), so we assembled our unlabeled data from a random sample of 100,000 tweets per day from September 10, 2008 to August 14, 2012, and filtered out non-English tweets (about 60% of the sample) using langid.py (Lui and Baldwin, 2012).5 Each tweet was processed with our to4As implemented by Liang (2005), v. 1.3: https:// github.com/percyliang/brown-cluster 5h</context>
<context position="13714" citStr="Blunsom and Cohn, 2011" startWordPosition="2215" endWordPosition="2218">of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” 7http://www.ark.cs.</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>P. Blunsom and T. Cohn. 2011. A hierarchical PitmanYor process HMM for unsupervised part of speech induction. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>N Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="13537" citStr="Brody and Diakopoulos, 2011" startWordPosition="2188" endWordPosition="2191">: u = “you”) and prepositions (C: fir = “for”). There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many l</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>S. Brody and N. Diakopoulos. 2011. Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V de Souza</author>
<author>R L Mercer</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, de Souza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. de Souza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="25265" citStr="Carlson et al., 2010" startWordPosition="4062" endWordPosition="4066">tional chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 385 #Msg. #Tok. Tagset Dates OCT27 DAILY547 NPSCHAT 1,827 26,594 App. A Oct 27-28, 2010 547 7,707 App. A Jan 2011–Jun 2012 10,578 44,997 PTB-like Oct–Nov 2006 (w/o sys. msg.) 7,935 37,081 RITTERTW 789 15,185 PTB-like unknown Tagging Accuracy 75 80 85 90 ● ● ● ● ● ● ● Table 1: Annotated datasets: number </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr, and T. M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="13665" citStr="Clark, 2003" startWordPosition="2209" endWordPosition="2210">lusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the s</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="24387" citStr="Eisenstein et al., 2011" startWordPosition="3929" endWordPosition="3933">g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="835" citStr="Eisenstein, 2013" startWordPosition="111" endWordPosition="112">on University, Pittsburgh, PA 15213, USA †Toyota Technological Institute at Chicago, Chicago, IL 60637, USA Corresponding author: brenocon@cs.cmu.edu Online conversational text, typified by microblogs, chat, and text messages,1 is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (mo</context>
<context position="24361" citStr="Eisenstein, 2013" startWordPosition="3927" endWordPosition="3928">herent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositi</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>J. Eisenstein. 2013. What to do about bad language on the internet. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="25286" citStr="Fader et al., 2011" startWordPosition="4067" endWordPosition="4070">g minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 385 #Msg. #Tok. Tagset Dates OCT27 DAILY547 NPSCHAT 1,827 26,594 App. A Oct 27-28, 2010 547 7,707 App. A Jan 2011–Jun 2012 10,578 44,997 PTB-like Oct–Nov 2006 (w/o sys. msg.) 7,935 37,081 RITTERTW 789 15,185 PTB-like unknown Tagging Accuracy 75 80 85 90 ● ● ● ● ● ● ● Table 1: Annotated datasets: number of messages, tokens, </context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E N Forsyth</author>
<author>C H Martell</author>
</authors>
<title>Lexical and discourse analysis of online chat dialog.</title>
<date>2007</date>
<booktitle>In Proc. of ICSC.</booktitle>
<contexts>
<context position="34485" citStr="Forsyth and Martell, 2007" startWordPosition="5540" endWordPosition="5543">d to in Table 1 as RITTERTW. Linguistic concerns notwithstanding (§5.2), for a controlled comparison, we train and test our system on this data with the same 4-fold cross-validation setup they used, attaining 90.0% (f0.5%) accuracy. Ritter et al.’s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3% (Table 4). 6.3 IRC: Evaluation on NPSCHAT IRC is another medium of online conversational text, with similar emoticons, misspellings, abbreviations and acronyms as Twitter data. We evaluate our tagger on the NPS Chat Corpus (Forsyth and Martell, 2007),24 a PTB-part-of-speech annotated dataset of Internet Relay Chat (IRC) room messages from 2006. First, we compare to a tagger in the same setup as experiments on this data in Forsyth (2007), training on 90% of the data and testing on 10%; we average results across 10-fold cross-validation.25 The full tagger model achieved 93.4% (f0.3%) accuracy, significantly improving over the best result they report, 90.8% accuracy with a tagger trained on a mix of several POS-annotated corpora. We also perform the ablation experiments on this corpus, with a slightly different experimental setup: we first f</context>
</contexts>
<marker>Forsyth, Martell, 2007</marker>
<rawString>E. N. Forsyth and C. H. Martell. 2007. Lexical and discourse analysis of online chat dialog. In Proc. of ICSC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E N Forsyth</author>
</authors>
<title>Improving automated lexical and discourse analysis of online chat dialog. Master’s thesis,</title>
<date>2007</date>
<institution>Naval Postgraduate School.</institution>
<contexts>
<context position="23369" citStr="Forsyth (2007)" startWordPosition="3761" endWordPosition="3762">r the nonstandard word forms that are commonplace in conversational text. For example, the PTB tokenization splits contractions containing apostrophes: I’m ==&gt;. I/PRP ’m/VBP. But conversational text often contains variants that resist a single PTB tag (like im), or even challenge traditional English grammatical categories 12The annotation guidelines are available online at http://www.ark.cs.cmu.edu/TweetNLP/ 13Annotators are coauthors of this paper. (like imma or umma, which both mean “I am going to”). One strategy would be to analyze these forms into a PTB-style tokenization, as discussed in Forsyth (2007), who proposes to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult. We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc. These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional wr</context>
<context position="34675" citStr="Forsyth (2007)" startWordPosition="5573" endWordPosition="5574">aining 90.0% (f0.5%) accuracy. Ritter et al.’s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3% (Table 4). 6.3 IRC: Evaluation on NPSCHAT IRC is another medium of online conversational text, with similar emoticons, misspellings, abbreviations and acronyms as Twitter data. We evaluate our tagger on the NPS Chat Corpus (Forsyth and Martell, 2007),24 a PTB-part-of-speech annotated dataset of Internet Relay Chat (IRC) room messages from 2006. First, we compare to a tagger in the same setup as experiments on this data in Forsyth (2007), training on 90% of the data and testing on 10%; we average results across 10-fold cross-validation.25 The full tagger model achieved 93.4% (f0.3%) accuracy, significantly improving over the best result they report, 90.8% accuracy with a tagger trained on a mix of several POS-annotated corpora. We also perform the ablation experiments on this corpus, with a slightly different experimental setup: we first filter out system messages then split data 24Release 1.0: http://faculty.nps.edu/ cmartell/NPSChat.htm 25Forsyth actually used 30 different 90/10 random splits; we prefer cross-validation bec</context>
<context position="36659" citStr="Forsyth (2007)" startWordPosition="5878" endWordPosition="5879">er and IRC, and have publicly released our new evaluation data, annotation guidelines, open-source tagger, and word clusters at http://www.ark.cs.cmu.edu/TweetNLP. Acknowledgements This research was supported in part by the National Science Foundation (IIS-0915187 and IIS-1054319). A Part-of-Speech Tagset Table 6: POS tagset from Gimpel et al. (2011) used in this paper, and described further in the released annotation guidelines. Accuracy 90.0 f 0.5 Tagger This work Ritter et al. (2011), basic CRF tagger Ritter et al. (2011), trained on more data 85.3 88.3 Accuracy 93.4 f 0.3 Tagger This work Forsyth (2007) 90.8 common noun pronoun (personal/WH; not possessive) proper noun nominal + possessive proper noun + possessive verb including copula, auxiliaries nominal + verbal (e.g. i’m), verbal + nominal (let’s) proper noun + verbal adjective adverb interjection determiner pre- or postposition, or subordinating conjunction coordinating conjunction verb particle existential there, predeterminers X + verbal hashtag (indicates topic/category for tweet) at-mention (indicates a user as a recipient of a tweet) discourse marker, indications of continuation across multiple tweets URL or email address emoticon </context>
</contexts>
<marker>Forsyth, 2007</marker>
<rawString>E. N. Forsyth. 2007. Improving automated lexical and discourse analysis of online chat dialog. Master’s thesis, Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
<author>O Cetinoglu</author>
<author>J Wagner</author>
<author>J L Roux</author>
<author>S Hogan</author>
<author>J Nivre</author>
<author>D Hogan</author>
<author>J van Genabith</author>
</authors>
<title>hardtoparse: POS tagging and parsing the Twitterverse.</title>
<date>2011</date>
<booktitle>In Proc. of AAAI-11 Workshop on Analysing Microtext.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan, J. Nivre, D. Hogan, and J. van Genabith. 2011. #hardtoparse: POS tagging and parsing the Twitterverse. In Proc. of AAAI-11 Workshop on Analysing Microtext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N Schneider</author>
<author>B O’Connor</author>
<author>D Das</author>
<author>D Mills</author>
<author>J Eisenstein</author>
<author>M Heilman</author>
<author>D Yogatama</author>
<author>J Flanigan</author>
<author>N A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>K. Gimpel, N. Schneider, B. O’Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Freebase data dumps.</title>
<date>2012</date>
<note>http:// download.freebase.com/datadumps/.</note>
<contexts>
<context position="17819" citStr="Google, 2012" startWordPosition="2880" endWordPosition="2881">art-of-speech tag.8 This can be viewed as a feature-based domain adaptation method, since it gives lexical type-level information for standard English words, which the model learns to map between PTB tags to the desired output tags. Second, since the lack of consistent capitalization conventions on Twitter makes it especially difficult to recognize names—Gimpel et al. and Foster et al. (2011) found relatively low accuracy on proper nouns—we added a token-level name list feature, which fires on (non-function) words from names from several sources: Freebase lists of celebrities and video games (Google, 2012), the Moby Words list of US Locations,9 and lists of male, female, family, and proper names from Mark Kantrowitz’s name corpus.10 4 Tokenization and Emoticon Detection Word segmentation on Twitter is challenging due to the lack of orthographic conventions; in particular, punctuation, emoticons, URLs, and other symbols may have no whitespace separation from textual 8Frequencies came from the Wall Street Journal and Brown corpus sections of the Penn Treebank. If a word has multiple PTB tags, each tag is a feature with value for the frequency rank; e.g. for three different tags in the PTB, this f</context>
</contexts>
<marker>Google, 2012</marker>
<rawString>Google. 2012. Freebase data dumps. http:// download.freebase.com/datadumps/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Han</author>
<author>T Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="24006" citStr="Han and Baldwin, 2011" startWordPosition="3866" endWordPosition="3869">to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult. We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc. These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexic</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>B. Han and T. Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7349" citStr="Koo et al., 2008" startWordPosition="1180" endWordPosition="1183">ation of L1 and L2 penalties; here j indexes over all features: P P R(3) = λ1 j |βj |+ 1 2λ2 j β2j Using even a very small L1 penalty eliminates many irrelevant or noisy features. 3 Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3Runtimes observed on an Intel Core i5 2.4 GHz laptop. arg min β 381 Binary path Top words (by frequency) A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah A3 111010100100 </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML.</booktitle>
<contexts>
<context position="5419" citStr="Lafferty et al., 2001" startWordPosition="840" endWordPosition="843">but the MEMM also naturally allows a faster O(|x|K) left-to-right greedy decoding: fort = 1... |x|: yt arg maxk p(yt = k |�yt−1, x, t; 3) which we find is 3 times faster and yields similar accuracy as Viterbi (an insignificant accuracy decrease of less than 0.1% absolute on the DAILY547 test set discussed below). Speed is paramount for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. 2Although when compared to CRFs, MEMMs theoretically suffer from the “label bias” problem (Lafferty et al., 2001), our system substantially outperforms the CRF-based taggers of previous work; and when comparing to Gimpel et al. system with similar feature sets, we observed little difference in accuracy. This is consistent with conventional wisdom that the quality of lexical features is much more important than the parametric form of the sequence model, at least in our setting: part-ofspeech tagging with a small labeled training set. This greedy tagger runs at 800 tweets/sec. (10,000 tokens/sec.) on a single CPU core, about 40 times faster than Gimpel et al.’s system. The tokenizer by itself (§4) runs at </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="10688" citStr="Liang (2005)" startWordPosition="1753" endWordPosition="1754">ped nearly 20 years later; see comparisons in Blunsom and Cohn (2011). The algorithm is attractive for our purposes since it scales to large amounts of data. When training on tweets drawn from a single day, we observed time-specific biases (e.g., numerical dates appearing in the same cluster as the word tonight), so we assembled our unlabeled data from a random sample of 100,000 tweets per day from September 10, 2008 to August 14, 2012, and filtered out non-English tweets (about 60% of the sample) using langid.py (Lui and Baldwin, 2012).5 Each tweet was processed with our to4As implemented by Liang (2005), v. 1.3: https:// github.com/percyliang/brown-cluster 5https://github.com/saffsd/langid.py kenizer and lowercased. We normalized all atmentions to (@MENTION) and URLs/email addresses to their domains (e.g. http://bit.ly/ dP8rR8 ==&gt;- (URL-bit.ly)). In an effort to reduce spam, we removed duplicated tweet texts (this also removes retweets) before word clustering. This normalization and cleaning resulted in 56 million unique tweets (847 million tokens). We set the clustering software’s count threshold to only cluster words appearing 40 or more times, yielding 216,856 word types, which took 42 ho</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical programming,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="6467" citStr="Liu and Nocedal, 1989" startWordPosition="1017" endWordPosition="1020">This greedy tagger runs at 800 tweets/sec. (10,000 tokens/sec.) on a single CPU core, about 40 times faster than Gimpel et al.’s system. The tokenizer by itself (§4) runs at 3,500 tweets/sec.3 Training and regularization. During training, the MEMM log-likelihood for a tagged tweet (x, y) is the sum over the observed token tags yt, each conditional on the tweet being tagged and the observed previous tag (with a start symbol before the first token in x), `(x, y,3) = P|x| t=1 log p(yt |yt−1, x, t;3). We optimize the parameters 3 with OWL-QN, an L1-capable variant of L-BFGS (Andrew and Gao, 2007; Liu and Nocedal, 1989) to minimize the regularized objective P 1 hx,yi`(x, y,3) + R(3) � where N is the number of tokens in the corpus and the sum ranges over all tagged tweets (x, y) in the training data. We use elastic net regularization (Zou and Hastie, 2005), which is a linear combination of L1 and L2 penalties; here j indexes over all features: P P R(3) = λ1 j |βj |+ 1 2λ2 j β2j Using even a very small L1 penalty eliminates many irrelevant or noisy features. 3 Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>S Zhang</author>
<author>F Wei</author>
<author>M Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Proc. ofACL. M. Lui</booktitle>
<contexts>
<context position="2505" citStr="Liu et al., 2011" startWordPosition="353" endWordPosition="356"> of the “CMU Twitter Part-of-Speech Tagger” and annotated data. 1 Introduction 1Also referred to as computer-mediated communication. Figure 1: Automatically tagged tweet showing nonstandard orthography, capitalization, and abbreviation. Ignoring the interjections and abbreviations, it glosses as He asked for your last name so he can add you on Facebook. The tagset is defined in Appendix A. Refer to Fig. 2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for b</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recognizing named entities in tweets. In Proc. ofACL. M. Lui and T. Baldwin. 2012. langid.py: An off-theshelf language identification tool. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="21861" citStr="Marcus et al., 1993" startWordPosition="3527" endWordPosition="3530">agset for Twitter (shown in Appendix A), which we used unmodified. The original annotation guidelines were not published, but in this work we recorded the rules governing tagging decisions and made further revisions while annotating the new data.12 Some of our guidelines reiterate or modify rules made by Penn Treebank annotators, while others treat specific phenomena found on Twitter (refer to the next section). Our tweets were annotated by two annotators who attempted to match the choices made in Gimpel et al.’s dataset. The annotators also consulted the POS annotations in the Penn Treebank (Marcus et al., 1993) as an additional reference. Differences were reconciled by a third annotator in discussion with all annotators.13 During this process, an inconsistency was found in Gimpel et al.’s data, which we corrected (concerning the tagging of this/that, a change to 100 labels, 0.4%). The new version of Gimpel et al.’s data (called OCT27), as well as the newer messages (called DAILY547), are both included in our data release. 5.2 Compounds in Penn Treebank vs. Twitter Ritter et al. (2011) annotated tweets using an augmented version of the PTB tagset and presumably followed the PTB annotation guidelines.</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="4135" citStr="McCallum et al., 2000" startWordPosition="617" endWordPosition="620"> clusters (§3); • an improved emoticon detector for conversational text (§4); ikr smh he asked fir yo last i G O V P D A name so he can add u on N P O V V O P fb lololol ∧ i 380 Proceedings of NAACL-HLT 2013, pages 380–390, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics • POS annotation guidelines (§5.1); and • a new dataset of 547 manually POS-annotated tweets (§5). 2 MEMM Tagger Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1, and is parameterized by a multiclass logistic regression: p(yt = k |yt−1, x, t;3) a � � β(trans) exp yt−1,k + Pj β(obs) j,k fj(x, t) We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A). Our feature sets will be discussed below in detail. Decoding. For experiments reported in this paper, we use the O(|x|K2) Viterbi algorithm for predict</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="2835" citStr="McClosky et al., 2010" startWordPosition="402" endWordPosition="405">so he can add you on Facebook. The tagset is defined in Appendix A. Refer to Fig. 2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these resources to the research community: • an open-source part-of-</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2010. Automatic domain adaptation for parsing. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B O’Connor</author>
<author>M Krieger</author>
<author>D Ahn</author>
</authors>
<title>TweetMotif: exploratory search and topic summarization for Twitter.</title>
<date>2010</date>
<booktitle>In Proc. of AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>B. O’Connor, M. Krieger, and D. Ahn. 2010. TweetMotif: exploratory search and topic summarization for Twitter. In Proc. of AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Owoputi</author>
<author>B O’Connor</author>
<author>C Dyer</author>
<author>K Gimpel</author>
<author>N Schneider</author>
</authors>
<title>Part-of-speech tagging for Twitter: Word clusters and other advances.</title>
<date>2012</date>
<tech>Technical Report CMU-ML-12-107,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, 2012</marker>
<rawString>O. Owoputi, B. O’Connor, C. Dyer, K. Gimpel, and N. Schneider. 2012. Part-of-speech tagging for Twitter: Word clusters and other advances. Technical Report CMU-ML-12-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<publisher>Now Publishers.</publisher>
<contexts>
<context position="25205" citStr="Pang and Lee, 2008" startWordPosition="4054" endWordPosition="4057">e: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 385 #Msg. #Tok. Tagset Dates OCT27 DAILY547 NPSCHAT 1,827 26,594 App. A Oct 27-28, 2010 547 7,707 App. A Jan 2011–Jun 2012 10,578 44,997 PTB-like Oct–Nov 2006 (w/o sys. msg.) 7,935 37,081 RITTERTW 789 15,185 PTB-like unknown Tagging Accuracy 75</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Now Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>R McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</title>
<date>2012</date>
<contexts>
<context position="2776" citStr="Petrov and McDonald, 2012" startWordPosition="393" endWordPosition="396">s and abbreviations, it glosses as He asked for your last name so he can add you on Facebook. The tagset is defined in Appendix A. Refer to Fig. 2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these res</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>S. Petrov and R. McDonald. 2012. Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</title>
<date>2011</date>
<contexts>
<context position="18559" citStr="Petrov et al., 2011" startWordPosition="3005" endWordPosition="3008">corpus.10 4 Tokenization and Emoticon Detection Word segmentation on Twitter is challenging due to the lack of orthographic conventions; in particular, punctuation, emoticons, URLs, and other symbols may have no whitespace separation from textual 8Frequencies came from the Wall Street Journal and Brown corpus sections of the Penn Treebank. If a word has multiple PTB tags, each tag is a feature with value for the frequency rank; e.g. for three different tags in the PTB, this feature gives a value of 1 for the most frequent tag, 2/3 for the second, etc. Coarse versions of the PTB tags are used (Petrov et al., 2011). While 88% of words in the dictionary have only one tag, using rank information seemed to give a small but consistent gain over only using the most common tag, or using binary features conjoined with rank as in Gimpel et al. 9http://icon.shef.ac.uk/Moby/mwords.html 10http://www.cs.cmu.edu/afs/cs/project/ ai-repository/ai/areas/nlp/corpora/names/ 0.html words (e.g. no:-d,yes should parse as four tokens), and internally may contain alphanumeric symbols that could be mistaken for words: a naive split(/[^azA-Z0-9]+/) tokenizer thinks the words “p” and “d” are among the top 100 most common words o</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. 2011. A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="4111" citStr="Ratnaparkhi, 1996" startWordPosition="615" endWordPosition="616">rvised Twitter word clusters (§3); • an improved emoticon detector for conversational text (§4); ikr smh he asked fir yo last i G O V P D A name so he can add u on N P O V V O P fb lololol ∧ i 380 Proceedings of NAACL-HLT 2013, pages 380–390, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics • POS annotation guidelines (§5.1); and • a new dataset of 547 manually POS-annotated tweets (§5). 2 MEMM Tagger Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1, and is parameterized by a multiclass logistic regression: p(yt = k |yt−1, x, t;3) a � � β(trans) exp yt−1,k + Pj β(obs) j,k fj(x, t) We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A). Our feature sets will be discussed below in detail. Decoding. For experiments reported in this paper, we use the O(|x|K2) Viter</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2486" citStr="Ritter et al., 2011" startWordPosition="349" endWordPosition="352">describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data. 1 Introduction 1Also referred to as computer-mediated communication. Figure 1: Automatically tagged tweet showing nonstandard orthography, capitalization, and abbreviation. Ignoring the interjections and abbreviations, it glosses as He asked for your last name so he can add you on Facebook. The tagset is defined in Appendix A. Refer to Fig. 2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results i</context>
<context position="16407" citStr="Ritter et al., 2011" startWordPosition="2649" endWordPosition="2652">ol U+E12F, which represents a picture of a bag of money, is grouped with the words cash and money. 3.4 Cluster-Based Features Since Brown clusters are hierarchical in a binary tree, each word is associated with a tree path represented as a bitstring with length &lt; 16; we use prefixes of the bitstring as features (for all prefix lengths E 12, 4, 6,... ,16}). This allows sharing of statistical strength between similar clusters. Using prefix features of hierarchical clusters in this way was similarly found to be effective for named-entity recognition (Turian et al., 2010) and Twitter POS tagging (Ritter et al., 2011). When checking to see if a word is associated with a cluster, the tagger first normalizes the word using the same techniques as described in §3.1, then creates a priority list of fuzzy match transformations 383 of the word by removing repeated punctuation and repeated characters. If the normalized word is not in a cluster, the tagger considers the fuzzy matches. Although only about 3% of the tokens in the development set (§6) did not appear in a clustering, this method resulted in a relative error decrease of 18% among such word tokens. 3.5 Other Lexical Features Besides unsupervised word clu</context>
<context position="22344" citStr="Ritter et al. (2011)" startWordPosition="3607" endWordPosition="3610">the choices made in Gimpel et al.’s dataset. The annotators also consulted the POS annotations in the Penn Treebank (Marcus et al., 1993) as an additional reference. Differences were reconciled by a third annotator in discussion with all annotators.13 During this process, an inconsistency was found in Gimpel et al.’s data, which we corrected (concerning the tagging of this/that, a change to 100 labels, 0.4%). The new version of Gimpel et al.’s data (called OCT27), as well as the newer messages (called DAILY547), are both included in our data release. 5.2 Compounds in Penn Treebank vs. Twitter Ritter et al. (2011) annotated tweets using an augmented version of the PTB tagset and presumably followed the PTB annotation guidelines. We wrote new guidelines because the PTB conventions are inappropriate for Twitter in several ways, as shown in the design of Gimpel et al.’s tagset. Importantly, “compound” tags (e.g., nominal+verbal and nominal+possessive) are used because tokenization is difficult or seemingly impossible for the nonstandard word forms that are commonplace in conversational text. For example, the PTB tokenization splits contractions containing apostrophes: I’m ==&gt;. I/PRP ’m/VBP. But conversati</context>
<context position="33269" citStr="Ritter et al. (2011)" startWordPosition="5354" endWordPosition="5357">ictionary tokens (Table 3). Varying the amount of unlabeled data. A tagger that only uses word clusters achieves an accuracy of 88.6% on the OCT27 development set.22 We created several clusterings with different numbers of unlabeled tweets, keeping the number of clusters constant at 800. As shown in Fig. 3, there was initially a logarithmic relationship between number of tweets and accuracy, but accuracy (and lexical coverage) levels out after 750,000 tweets. We use the largest clustering (56 million tweets and 1,000 clusters) as the default for the released tagger. 6.2 Evaluation on RITTERTW Ritter et al. (2011) annotated a corpus of 787 tweets23 with a single annotator, using the PTB 21We retain hashtags since by our guidelines a #-prefixed token is ambiguous between a hashtag and a normal word, e.g. #1 or going #home. 22The only observation features are the word clusters of a token and its immediate neighbors. 23https://github.com/aritter/twitter_nlp/ blob/master/data/annotated/pos.txt 387 388 Table 4: Accuracy comparison on Ritter et al.’s Twitter POS corpus (§6.2). Table 5: Accuracy comparison on Forsyth’s NPSCHAT IRC POS corpus (§6.3). tagset plus several Twitter-specific tags, referred to in Ta</context>
<context position="36536" citStr="Ritter et al. (2011)" startWordPosition="5854" endWordPosition="5857">ages. 7 Conclusion We have constructed a state-of-the-art part-ofspeech tagger for the online conversational text genres of Twitter and IRC, and have publicly released our new evaluation data, annotation guidelines, open-source tagger, and word clusters at http://www.ark.cs.cmu.edu/TweetNLP. Acknowledgements This research was supported in part by the National Science Foundation (IIS-0915187 and IIS-1054319). A Part-of-Speech Tagset Table 6: POS tagset from Gimpel et al. (2011) used in this paper, and described further in the released annotation guidelines. Accuracy 90.0 f 0.5 Tagger This work Ritter et al. (2011), basic CRF tagger Ritter et al. (2011), trained on more data 85.3 88.3 Accuracy 93.4 f 0.3 Tagger This work Forsyth (2007) 90.8 common noun pronoun (personal/WH; not possessive) proper noun nominal + possessive proper noun + possessive verb including copula, auxiliaries nominal + verbal (e.g. i’m), verbal + nominal (let’s) proper noun + verbal adjective adverb interjection determiner pre- or postposition, or subordinating conjunction coordinating conjunction verb particle existential there, predeterminers X + verbal hashtag (indicates topic/category for tweet) at-mention (indicates a user as </context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schnoebelen</author>
</authors>
<title>Do you smile with your nose? Stylistic variation in Twitter emoticons. University of Pennsylvania Working Papers in Linguistics,</title>
<date>2012</date>
<contexts>
<context position="13557" citStr="Schnoebelen, 2012" startWordPosition="2192" endWordPosition="2194"> (C: fir = “for”). There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not</context>
</contexts>
<marker>Schnoebelen, 2012</marker>
<rawString>T. Schnoebelen. 2012. Do you smile with your nose? Stylistic variation in Twitter emoticons. University of Pennsylvania Working Papers in Linguistics, 18(2):14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13689" citStr="Smith and Eisner, 2005" startWordPosition="2211" endWordPosition="2214"> demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Täckström</author>
<author>R McDonald</author>
<author>J Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="7394" citStr="Täckström et al., 2012" startWordPosition="1188" endWordPosition="1191">dexes over all features: P P R(3) = λ1 j |βj |+ 1 2λ2 j β2j Using even a very small L1 penalty eliminates many irrelevant or noisy features. 3 Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3Runtimes observed on an Intel Core i5 2.4 GHz laptop. arg min β 381 Binary path Top words (by frequency) A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah A3 111010100100 yes yep yup nope yess yesss yessss ofcourse y</context>
</contexts>
<marker>Täckström, McDonald, Uszkoreit, 2012</marker>
<rawString>O. Täckström, R. McDonald, and J. Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7370" citStr="Turian et al., 2010" startWordPosition="1184" endWordPosition="1187"> penalties; here j indexes over all features: P P R(3) = λ1 j |βj |+ 1 2λ2 j β2j Using even a very small L1 penalty eliminates many irrelevant or noisy features. 3 Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3Runtimes observed on an Intel Core i5 2.4 GHz laptop. arg min β 381 Binary path Top words (by frequency) A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah A3 111010100100 yes yep yup nope yess</context>
<context position="16361" citStr="Turian et al., 2010" startWordPosition="2640" endWordPosition="2643"> G2 cluster as smiley face emoticons. The symbol U+E12F, which represents a picture of a bag of money, is grouped with the words cash and money. 3.4 Cluster-Based Features Since Brown clusters are hierarchical in a binary tree, each word is associated with a tree path represented as a bitstring with length &lt; 16; we use prefixes of the bitstring as features (for all prefix lengths E 12, 4, 6,... ,16}). This allows sharing of statistical strength between similar clusters. Using prefix features of hierarchical clusters in this way was similarly found to be effective for named-entity recognition (Turian et al., 2010) and Twitter POS tagging (Ritter et al., 2011). When checking to see if a word is associated with a cluster, the tagger first normalizes the word using the same techniques as described in §3.1, then creates a priority list of fuzzy match transformations 383 of the word by removing repeated punctuation and repeated characters. If the normalized word is not in a cluster, the tagger considers the fuzzy matches. Although only about 3% of the tokens in the development set (§6) did not appear in a clustering, this method resulted in a relative error decrease of 18% among such word tokens. 3.5 Other </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semisupervised learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25185" citStr="Turney, 2002" startWordPosition="4052" endWordPosition="4053">y inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 385 #Msg. #Tok. Tagset Dates OCT27 DAILY547 NPSCHAT 1,827 26,594 App. A Oct 27-28, 2010 547 7,707 App. A Jan 2011–Jun 2012 10,578 44,997 PTB-like Oct–Nov 2006 (w/o sys. msg.) 7,935 37,081 RITTERTW 789 15,185 PTB-like unknown</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zou</author>
<author>T Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="6707" citStr="Zou and Hastie, 2005" startWordPosition="1063" endWordPosition="1066">e MEMM log-likelihood for a tagged tweet (x, y) is the sum over the observed token tags yt, each conditional on the tweet being tagged and the observed previous tag (with a start symbol before the first token in x), `(x, y,3) = P|x| t=1 log p(yt |yt−1, x, t;3). We optimize the parameters 3 with OWL-QN, an L1-capable variant of L-BFGS (Andrew and Gao, 2007; Liu and Nocedal, 1989) to minimize the regularized objective P 1 hx,yi`(x, y,3) + R(3) � where N is the number of tokens in the corpus and the sum ranges over all tagged tweets (x, y) in the training data. We use elastic net regularization (Zou and Hastie, 2005), which is a linear combination of L1 and L2 penalties; here j indexes over all features: P P R(3) = λ1 j |βj |+ 1 2λ2 j β2j Using even a very small L1 penalty eliminates many irrelevant or noisy features. 3 Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of m</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>H. Zou and T. Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>