<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048353">
<title confidence="0.993969">
A Joint Rule Selection Model for Hierarchical Phrase-based Translation*
</title>
<author confidence="0.997896">
Lei Cui†, Dongdong Zhang‡, Mu Li‡, Ming Zhou‡, and Tiejun Zhao†
</author>
<affiliation confidence="0.9975055">
†School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.944002">
{cuilei,tjzhao}@mtlab.hit.edu.cn
</email>
<affiliation confidence="0.910872">
‡Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.998562">
{dozhang,muli,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.994793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843578947368">
In hierarchical phrase-based SMT sys-
tems, statistical models are integrated to
guide the hierarchical rule selection for
better translation performance. Previous
work mainly focused on the selection of
either the source side of a hierarchical rule
or the target side of a hierarchical rule
rather than considering both of them si-
multaneously. This paper presents a joint
model to predict the selection of hierar-
chical rules. The proposed model is esti-
mated based on four sub-models where the
rich context knowledge from both source
and target sides is leveraged. Our method
can be easily incorporated into the prac-
tical SMT systems with the log-linear
model framework. The experimental re-
sults show that our method can yield sig-
nificant improvements in performance.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997827491525424">
Hierarchical phrase-based model has strong ex-
pression capabilities of translation knowledge. It
can not only maintain the strength of phrase trans-
lation in traditional phrase-based models (Koehn
et al., 2003; Xiong et al., 2006), but also char-
acterize the complicated long distance reordering
similar to syntactic based statistical machine trans-
lation (SMT) models (Yamada and Knight, 2001;
Quirk et al., 2005; Galley et al., 2006; Liu et al.,
2006; Marcu et al., 2006; Mi et al., 2008; Shen et
al., 2008).
In hierarchical phrase-based SMT systems, due
to the flexibility of rule matching, a huge number
of hierarchical rules could be automatically learnt
from bilingual training corpus (Chiang, 2005).
SMT decoders are forced to face the challenge of
This work was finished while the first author visited Mi-
crosoft Research Asia as an intern.
proper rule selection for hypothesis generation, in-
cluding both source-side rule selection and target-
side rule selection where the source-side rule de-
termines what part of source words to be translated
and the target-side rule provides one of the candi-
date translations of the source-side rule. Improper
rule selections may result in poor translations.
There is some related work about the hierarchi-
cal rule selection. In the original work (Chiang,
2005), the target-side rule selection is analogous to
the model in traditional phrase-based SMT system
such as Pharaoh (Koehn et al., 2003). Extending
this work, (He et al., 2008; Liu et al., 2008) in-
tegrate rich context information of non-terminals
to predict the target-side rule selection. Different
from the above work where the probability dis-
tribution of source-side rule selection is uniform,
(Setiawan et al., 2009) proposes to select source-
side rules based on the captured function words
which often play an important role in word re-
ordering. There is also some work considering to
involve more rich contexts to guide the source-side
rule selection. (Marton and Resnik, 2008; Xiong
et al., 2009) explore the source syntactic informa-
tion to reward exact matching structure rules or
punish crossing structure rules.
All the previous work mainly focused on either
source-side rule selection task or target-side rule
selection task rather than both of them together.
The separation of these two tasks, however, weak-
ens the high interrelation between them. In this pa-
per, we propose to integrate both source-side and
target-side rule selection in a unified model. The
intuition is that the joint selection of source-side
and target-side rules is more reliable as it conducts
the search in a larger space than the single selec-
tion task does. It is expected that these two kinds
of selection can help and affect each other, which
may potentially lead to better hierarchical rule se-
lections with a relative global optimum instead of
a local optimum that might be reached in the pre-
</bodyText>
<page confidence="0.971146">
6
</page>
<subsubsectionHeader confidence="0.259397">
Proceedings of the ACL 2010 Conference Short Papers, pages 6–11,
</subsubsectionHeader>
<bodyText confidence="0.970827533333333">
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
vious methods. Our proposed joint probability
model is factored into four sub-models that can
be further classified into source-side and target-
side rule selection models or context-based and
context-free selection models. The context-based
models explore rich context features from both
source and target sides, including function words,
part-of-speech (POS) tags, syntactic structure in-
formation and so on. Our model can be easily in-
corporated as an independent feature into the prac-
tical hierarchical phrase-based systems with the
log-linear model framework. The experimental re-
sults indicate our method can improve the system
performance significantly.
</bodyText>
<sectionHeader confidence="0.956588" genericHeader="method">
2 Hierarchical Rule Selection Model
</sectionHeader>
<bodyText confidence="0.999125428571428">
Following (Chiang, 2005), (α, &apos;Y) is used to repre-
sent a synchronous context free grammar (SCFG)
rule extracted from the training corpus, where α
and &apos;Y are the source-side and target-side rule re-
spectively. Let C be the context of (α,&apos;Y). For-
mally, our joint probability model of hierarchical
rule selection is described as follows:
</bodyText>
<equation confidence="0.999704">
P(α,&apos;Y|C) = P(α|C)P(&apos;Y|α, C) (1)
</equation>
<bodyText confidence="0.999218375">
We decompose the joint probability model into
two sub-models based on the Bayes formulation,
where the first sub-model is source-side rule se-
lection model and the second one is the target-side
rule selection model.
For the source-side rule selection model, we fur-
ther compute it by the interpolation of two sub-
models:
</bodyText>
<equation confidence="0.998862">
BP3(α) + (1 − B)P3(α|C) (2)
</equation>
<bodyText confidence="0.998595857142857">
where P3(α) is the context-free source model
(CFSM) and P3(α|C) is the context-based source
model (CBSM), B is the interpolation weight that
can be optimized over the development data.
CFSM is the probability of source-side rule se-
lection that can be estimated based on maximum
likelihood estimation (MLE) method:
</bodyText>
<equation confidence="0.963231333333333">
Ey Count((α, &apos;Y))
P3(α) = (3)
Count(α)
</equation>
<bodyText confidence="0.999969833333333">
where the numerator is the total count of bilin-
gual rule pairs with the same source-side rule that
are extracted based on the extraction algorithm in
(Chiang, 2005), and the denominator is the total
amount of source-side rule patterns contained in
the monolingual source side of the training corpus.
CFSM is used to capture how likely the source-
side rule is linguistically motivated or has the cor-
responding target-side counterpart.
For CBSM, it can be naturally viewed as a clas-
sification problem where each distinct source-side
rule is a single class. However, considering the
huge number of classes may cause serious data
sparseness problem and thereby degrade the clas-
sification accuracy, we approximate CBSM by a
binary classification problem which can be solved
by the maximum entropy (ME) approach (Berger
et al., 1996) as follows:
</bodyText>
<equation confidence="0.968224666666667">
P3(α|C) P3(v|α, C)
exp[Ei Aihi(v, α, C)] (4)
EV/ exp[Ei Aihi(v�, α, C)]
</equation>
<bodyText confidence="0.999494416666666">
where v E 10, 11 is the indicator whether the
source-side rule is applied during decoding, v = 1
when the source-side rule is applied, otherwise
v = 0; hi is a feature function, Ai is the weight
of hi. CBSM estimates the probability of the
source-side rule being selected according to the
rich context information coming from the surface
strings and sub-phrases that will be reduced to
non-terminals during decoding.
Analogously, we decompose the target-side rule
selection model by the interpolation approach as
well:
</bodyText>
<equation confidence="0.997867">
�oPt(&apos;Y) + (1 − �o)Pt(&apos;Y|α, C) (5)
</equation>
<bodyText confidence="0.999465818181818">
where Pt(&apos;Y) is the context-free target model
(CFTM) and Pt(&apos;Y|α, C) is the context-based tar-
get model (CBTM), cp is the interpolation weight
that can be optimized over the development data.
In the similar way, we compute CFTM by the
MLE approach and estimate CBTM by the ME
approach. CFTM computes how likely the target-
side rule is linguistically motivated, while CBTM
predicts how likely the target-side rule is applied
according to the clues from the rich context infor-
mation.
</bodyText>
<sectionHeader confidence="0.845716" genericHeader="method">
3 Model Training of CBSM and CBTM
</sectionHeader>
<subsectionHeader confidence="0.998311">
3.1 The acquisition of training instances
</subsectionHeader>
<bodyText confidence="0.99959475">
CBSM and CBTM are trained by ME approach for
the binary classification, where a training instance
consists of a label and the context related to SCFG
rules. The context is divided into source context
</bodyText>
<page confidence="0.998765">
7
</page>
<figureCaption confidence="0.999944">
Figure 1: Example of training instances in CBSM and CBTM.
</figureCaption>
<bodyText confidence="0.998278277777778">
and target context. CBSM is trained only based
on the source context while CBTM is trained over
both the source and the target context. All the
training instances are automatically constructed
from the bilingual training corpus, which have la-
bels of either positive (i.e., v = 1) or negative (i.e.,
v = 0). This section explains how the training in-
stances are constructed for the training of CBSM
and CBTM.
Let s and t be the source sentence and target
sentence, W be the word alignment between them,
rs be a source-side rule that pattern-matches a
sub-phrase of s, rt be the target-side rule pattern-
matching a sub-phrase of t and being aligned to rs
based on W, and C(r) be the context features re-
lated to the rule r which will be explained in the
following section.
For the training of CBSM, if the SCFG rule
(rs, rt) can be extracted based on the rule extrac-
tion algorithm in (Chiang, 2005), (v = 1, C(rs))
is constructed as a positive instance, otherwise
(v = 0, C(rs)) is constructed as a negative in-
stance. For example in Figure 1(a), the context of
source-side rule ”X1 hezuo” that pattern-matches
the phrase ”youhao hezuo” produces a positive
instance, while the context of ”X1 youhao” that
pattern-matches the source phrase ”de youhao” or
”shuangfang de youhao” will produce a negative
instance as there are no corresponding plausible
target-side rules that can be extracted legally1.
For the training of CBTM, given rs, suppose
there is a SCFG rule set {(rs, rt )|1 G k G n}
extracted from multiple distinct sentence pairs in
the bilingual training corpus, among which we as-
sume (rs, rt) is extracted from the sentence pair
(s, t). Then, we construct (v = 1, C(rs), C(rt))
</bodyText>
<footnote confidence="0.989693333333333">
1Because the aligned target words are not contiguous and
”cooperation” is aligned to the word outside the source-side
rule.
</footnote>
<bodyText confidence="0.9969695">
as a positive instance, while the elements in {(v =
0, C(rs), C(rt ))|j =� i n 1 G j G n} are viewed
as negative instances since they fail to be applied
to the translation from s to t. For example in Fig-
ure 1(c), Rule (1) and Rule (2) are two different
SCFG rules extracted from Figure 1(a) and Figure
1(b) respectively, where their source-side rules are
the same. As Rule (1) cannot be applied to Fig-
ure 1(b) for the translation and Rule (2) cannot
be applied to Figure 1(a) for the translation either,
(v = 1, C(rs), C(rt )) and (v = 1, C(rs), C(rt))
are constructed as positive instances while (v =
0, C(rs), C(rt)) and (v = 0, C(rs), C(rt )) are
viewed as negative instances. It is noticed that
this instance construction method may lead to a
large quantity of negative instances and choke the
training procedure. In practice, to limit the size
of the training set, the negative instances con-
structed based on low-frequency target-side rules
are pruned.
</bodyText>
<subsectionHeader confidence="0.99858">
3.2 Context-based features for ME training
</subsectionHeader>
<bodyText confidence="0.999953764705882">
ME approach has the merit of easily combining
different features to predict the probability of each
class. We incorporate into the ME based model
the following informative context-based features
to train CBSM and CBTM. These features are
carefully designed to reduce the data sparseness
problem and some of them are inspired by pre-
vious work (He et al., 2008; Gimpel and Smith,
2008; Marton and Resnik, 2008; Chiang et al.,
2009; Setiawan et al., 2009; Shen et al., 2009;
Xiong et al., 2009):
1. Function word features, which indicate
whether the hierarchical source-side/target-
side rule strings and sub-phrases covered by
non-terminals contain function words that are
often important clues of predicting syntactic
structures.
</bodyText>
<page confidence="0.99019">
8
</page>
<listItem confidence="0.999053375">
2. POS features, which are POS tags of the
boundary source words covered by non-
terminals.
3. Syntactic features, which are the constituent
constraints of hierarchical source-side rules
exactly matching or crossing syntactic sub-
trees.
4. Rule format features, which are non-
terminal positions and orders in source-
side/target-side rules. This feature interacts
between source and target components since
it shows whether the translation ordering is
affected.
5. Length features, which are the length
of sub-phrases covered by source non-
terminals.
</listItem>
<sectionHeader confidence="0.9991" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998616">
4.1 Experiment setting
</subsectionHeader>
<bodyText confidence="0.99996844">
We implement a hierarchical phrase-based system
similar to the Hiero (Chiang, 2005) and evaluate
our method on the Chinese-to-English translation
task. Our bilingual training data comes from FBIS
corpus, which consists of around 160K sentence
pairs where the source data is parsed by the Berke-
ley parser (Petrov and Klein, 2007). The ME train-
ing toolkit, developed by (Zhang, 2006), is used to
train our CBSM and CBTM. The training size of
constructed positive instances for both CBSM and
CBTM is 4.68M, while the training size of con-
structed negative instances is 3.74M and 3.03M re-
spectively. Following (Setiawan et al., 2009), we
identify function words as the 128 most frequent
words in the corpus. The interpolation weights are
set to 0 = 0.75 and co = 0.70. The 5-gram lan-
guage model is trained over the English portion
of FBIS corpus plus Xinhua portion of the Giga-
word corpus. The development data is from NIST
2005 evaluation data and the test data is from
NIST 2006 and NIST 2008 evaluation data. The
evaluation metric is the case-insensitive BLEU4
(Papineni et al., 2002). Statistical significance in
BLEU score differences is tested by paired boot-
strap re-sampling (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.999478">
4.2 Comparison with related work
</subsectionHeader>
<bodyText confidence="0.999523941176471">
Our baseline is the implemented Hiero-like SMT
system where only the standard features are em-
ployed and the performance is state-of-the-art.
We compare our method with the baseline and
some typical approaches listed in Table 1 where
XP+ denotes the approach in (Marton and Resnik,
2008) and TOFW (topological ordering of func-
tion words) stands for the method in (Setiawan et
al., 2009). As (Xiong et al., 2009)’s work is based
on phrasal SMT system with bracketing transduc-
tion grammar rules (Wu, 1997) and (Shen et al.,
2009)’s work is based on the string-to-dependency
SMT model, we do not implement these two re-
lated work due to their different models from ours.
We also do not compare with (He et al., 2008)’s
work due to its less practicability of integrating
numerous sub-models.
</bodyText>
<table confidence="0.9968624">
Methods NIST 2006 NIST 2008
Baseline 0.3025 0.2200
XP+ 0.3061 0.2254
TOFW 0.3089 0.2253
Our method 0.3141 0.2318
</table>
<tableCaption confidence="0.999369">
Table 1: Comparison results, our method is signif-
</tableCaption>
<bodyText confidence="0.977004">
icantly better than the baseline, as well as the other
two approaches (p &lt; 0.01)
As shown in Table 1, all the methods outper-
form the baseline because they have extra mod-
els to guide the hierarchical rule selection in some
ways which might lead to better translation. Ap-
parently, our method also performs better than the
other two approaches, indicating that our method
is more effective in the hierarchical rule selection
as both source-side and target-side rules are se-
lected together.
</bodyText>
<subsectionHeader confidence="0.999944">
4.3 Effect of sub-models
</subsectionHeader>
<bodyText confidence="0.998105">
Due to the space limitation, we analyze the ef-
fect of sub-models upon the system performance,
rather than that of ME features, part of which have
been investigated in previous related work.
</bodyText>
<table confidence="0.9992036">
Settings NIST 2006 NIST 2008
Baseline 0.3025 0.2200
Baseline+CFSM 0.3092* 0.2266*
Baseline+CBSM 0.3077* 0.2247*
Baseline+CFTM 0.3076* 0.2286*
Baseline+CBTM 0.3060 0.2255*
Baseline+CFSM+CFTM 0.3109* 0.2289*
Baseline+CFSM+CBSM 0.3104* 0.2282*
Baseline+CFTM+CBTM 0.3099* 0.2299*
Baseline+all sub-models 0.3141* 0.2318*
</table>
<tableCaption confidence="0.9465785">
Table 2: Sub-model effect upon the performance,
*: significantly better than baseline (p &lt; 0.01)
</tableCaption>
<bodyText confidence="0.799991">
As shown in Table 2, when sub-models are inte-
</bodyText>
<page confidence="0.995872">
9
</page>
<bodyText confidence="0.9999848">
grated as independent features, the performance is
improved compared to the baseline, which shows
that each of the sub-models can improve the hier-
archical rule selection. It is noticeable that the per-
formance of the source-side rule selection model
is comparable with that of the target-side rule se-
lection model. Although CFSM and CFTM per-
form only slightly better than the others among
the individual sub-models, the best performance is
achieved when all the sub-models are integrated.
</bodyText>
<sectionHeader confidence="0.998708" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999948083333333">
Hierarchical rule selection is an important and
complicated task for hierarchical phrase-based
SMT system. We propose a joint probability
model for the hierarchical rule selection and the
experimental results prove the effectiveness of our
approach.
In the future work, we will explore more useful
features and test our method over the large scale
training corpus. A challenge might exist when
running the ME training toolkit over a big size
of training instances from the large scale training
data.
</bodyText>
<sectionHeader confidence="0.997238" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99837875">
We are especially grateful to the anonymous re-
viewers for their insightful comments. We also
thank Hendra Setiawan, Yuval Marton, Chi-Ho Li,
Shujie Liu and Nan Duan for helpful discussions.
</bodyText>
<sectionHeader confidence="0.997789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708685714286">
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1): pages 39-72.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
ACL, pages 263-270.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. HLT-NAACL, pages 218-226.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Proc.
ACL-Coling, pages 961-968.
Kevin Gimpel and Noah A. Smith. 2008. Rich Source-
Side Context for Statistical Machine Translation. In
Proc. the Third Workshop on Statistical Machine
Translation, pages 9-17.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving Statistical Machine Translation using Lexi-
calized Rule Selection. In Proc. Coling, pages 321-
328.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. EMNLP.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. HLT-
NAACL, pages 127-133.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum Entropy based Rule Selection
Model for Syntax-based Statistical Machine Trans-
lation. In Proc. EMNLP, pages 89-97.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-String Statistical Translation Rules.
In Proc. ACL, pages 704-711.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. ACL-Coling, pages 609-616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proc. EMNLP, pages 44-52.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-Based Trans-
lation. In Proc. ACL, pages 1003-1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proc. ACL, pages 192-199.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. ACL,
pages 311-318.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. HLT-NAACL,
pages 404-411.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proc. ACL, pages 271-279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proc. ACL, pages 577-585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical
Machine Translation. In Proc. EMNLP, pages 72-
80.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological Ordering of Function
Words in Hierarchical Phrase-based Translation. In
Proc. ACL, pages 324-332.
</reference>
<page confidence="0.9493">
10
</page>
<reference confidence="0.997238894736842">
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): pages 377-
403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proc. ACL-
Coling, pages 521-528.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A Syntax-Driven Bracketing Model for
Phrase-Based Translation. In Proc. ACL, pages
315-323.
Kenji Yamada and Kevin Knight. 2001. A Syntax-
based Statistical Translation Model. In Proc. ACL,
pages 523-530.
Le Zhang. 2006. Maximum entropy mod-
eling toolkit for python and c++. avail-
ableathttp://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
</reference>
<page confidence="0.999441">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.231714">
<title confidence="0.950517">Joint Rule Selection Model for Hierarchical Phrase-based</title>
<affiliation confidence="0.905219">of Computer Science and Technology Harbin Institute of Technology, Harbin, China</affiliation>
<email confidence="0.740313">cuilei@mtlab.hit.edu.cn</email>
<email confidence="0.740313">tjzhao@mtlab.hit.edu.cn</email>
<note confidence="0.378045">Research Asia, Beijing, China</note>
<abstract confidence="0.99847005">In hierarchical phrase-based SMT systems, statistical models are integrated to guide the hierarchical rule selection for better translation performance. Previous work mainly focused on the selection of either the source side of a hierarchical rule or the target side of a hierarchical rule rather than considering both of them simultaneously. This paper presents a joint model to predict the selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>39--72</pages>
<contexts>
<context position="6736" citStr="Berger et al., 1996" startWordPosition="1047" endWordPosition="1050">of source-side rule patterns contained in the monolingual source side of the training corpus. CFSM is used to capture how likely the sourceside rule is linguistically motivated or has the corresponding target-side counterpart. For CBSM, it can be naturally viewed as a classification problem where each distinct source-side rule is a single class. However, considering the huge number of classes may cause serious data sparseness problem and thereby degrade the classification accuracy, we approximate CBSM by a binary classification problem which can be solved by the maximum entropy (ME) approach (Berger et al., 1996) as follows: P3(α|C) P3(v|α, C) exp[Ei Aihi(v, α, C)] (4) EV/ exp[Ei Aihi(v�, α, C)] where v E 10, 11 is the indicator whether the source-side rule is applied during decoding, v = 1 when the source-side rule is applied, otherwise v = 0; hi is a feature function, Ai is the weight of hi. CBSM estimates the probability of the source-side rule being selected according to the rich context information coming from the surface strings and sub-phrases that will be reduced to non-terminals during decoding. Analogously, we decompose the target-side rule selection model by the interpolation approach as we</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1): pages 39-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation. In</title>
<date>2005</date>
<booktitle>Proc. ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1828" citStr="Chiang, 2005" startWordPosition="270" endWordPosition="271">dge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), t</context>
<context position="4868" citStr="Chiang, 2005" startWordPosition="745" endWordPosition="746">r classified into source-side and targetside rule selection models or context-based and context-free selection models. The context-based models explore rich context features from both source and target sides, including function words, part-of-speech (POS) tags, syntactic structure information and so on. Our model can be easily incorporated as an independent feature into the practical hierarchical phrase-based systems with the log-linear model framework. The experimental results indicate our method can improve the system performance significantly. 2 Hierarchical Rule Selection Model Following (Chiang, 2005), (α, &apos;Y) is used to represent a synchronous context free grammar (SCFG) rule extracted from the training corpus, where α and &apos;Y are the source-side and target-side rule respectively. Let C be the context of (α,&apos;Y). Formally, our joint probability model of hierarchical rule selection is described as follows: P(α,&apos;Y|C) = P(α|C)P(&apos;Y|α, C) (1) We decompose the joint probability model into two sub-models based on the Bayes formulation, where the first sub-model is source-side rule selection model and the second one is the target-side rule selection model. For the source-side rule selection model, </context>
<context position="9084" citStr="Chiang, 2005" startWordPosition="1456" endWordPosition="1457">or negative (i.e., v = 0). This section explains how the training instances are constructed for the training of CBSM and CBTM. Let s and t be the source sentence and target sentence, W be the word alignment between them, rs be a source-side rule that pattern-matches a sub-phrase of s, rt be the target-side rule patternmatching a sub-phrase of t and being aligned to rs based on W, and C(r) be the context features related to the rule r which will be explained in the following section. For the training of CBSM, if the SCFG rule (rs, rt) can be extracted based on the rule extraction algorithm in (Chiang, 2005), (v = 1, C(rs)) is constructed as a positive instance, otherwise (v = 0, C(rs)) is constructed as a negative instance. For example in Figure 1(a), the context of source-side rule ”X1 hezuo” that pattern-matches the phrase ”youhao hezuo” produces a positive instance, while the context of ”X1 youhao” that pattern-matches the source phrase ”de youhao” or ”shuangfang de youhao” will produce a negative instance as there are no corresponding plausible target-side rules that can be extracted legally1. For the training of CBTM, given rs, suppose there is a SCFG rule set {(rs, rt )|1 G k G n} extracte</context>
<context position="12392" citStr="Chiang, 2005" startWordPosition="2003" endWordPosition="2004"> source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This feature interacts between source and target components since it shows whether the translation ordering is affected. 5. Length features, which are the length of sub-phrases covered by source nonterminals. 4 Experiments 4.1 Experiment setting We implement a hierarchical phrase-based system similar to the Hiero (Chiang, 2005) and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser (Petrov and Klein, 2007). The ME training toolkit, developed by (Zhang, 2006), is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent w</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. ACL, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 New Features for Statistical Machine Translation. In</title>
<date>2009</date>
<booktitle>Proc. HLT-NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="11427" citStr="Chiang et al., 2009" startWordPosition="1860" endWordPosition="1863">cedure. In practice, to limit the size of the training set, the negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orde</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 New Features for Statistical Machine Translation. In Proc. HLT-NAACL, pages 218-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models. In</title>
<date>2006</date>
<booktitle>Proc. ACL-Coling,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1557" citStr="Galley et al., 2006" startWordPosition="224" endWordPosition="227">rporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words t</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. ACL-Coling, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Rich SourceSide Context for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="11381" citStr="Gimpel and Smith, 2008" startWordPosition="1852" endWordPosition="1855"> of negative instances and choke the training procedure. In practice, to limit the size of the training set, the negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format feat</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2008. Rich SourceSide Context for Statistical Machine Translation. In Proc. the Third Workshop on Statistical Machine Translation, pages 9-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving Statistical Machine Translation using Lexicalized Rule Selection.</title>
<date>2008</date>
<booktitle>In Proc. Coling,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="2598" citStr="He et al., 2008" startWordPosition="392" endWordPosition="395">selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structu</context>
<context position="11357" citStr="He et al., 2008" startWordPosition="1848" endWordPosition="1851"> a large quantity of negative instances and choke the training procedure. In practice, to limit the size of the training set, the negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtr</context>
<context position="14245" citStr="He et al., 2008" startWordPosition="2314" endWordPosition="2317">oyed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. Methods NIST 2006 NIST 2008 Baseline 0.3025 0.2200 XP+ 0.3061 0.2254 TOFW 0.3089 0.2253 Our method 0.3141 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p &lt; 0.01) As shown in Table 1, all the methods outperform the baseline because they have extra models to guide the hierarchical rule selection in some ways which might lead to better translation. Apparently, our method also performs better than the other two approaches, indicating that our m</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule Selection. In Proc. Coling, pages 321-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation. In</title>
<date>2004</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="13499" citStr="Koehn, 2004" startWordPosition="2189" endWordPosition="2190">3M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to 0 = 0.75 and co = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-depen</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. HLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1334" citStr="Koehn et al., 2003" startWordPosition="189" endWordPosition="192"> model to predict the selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited M</context>
<context position="2559" citStr="Koehn et al., 2003" startWordPosition="385" endWordPosition="388">ft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic infor</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. HLTNAACL, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation. In</title>
<date>2008</date>
<booktitle>Proc. EMNLP,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="2617" citStr="Liu et al., 2008" startWordPosition="396" endWordPosition="399">othesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish </context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 2008. Maximum Entropy based Rule Selection Model for Syntax-based Statistical Machine Translation. In Proc. EMNLP, pages 89-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>704--711</pages>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. In Proc. ACL, pages 704-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ACL-Coling,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1575" citStr="Liu et al., 2006" startWordPosition="228" endWordPosition="231">ctical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated an</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. In Proc. ACL-Coling, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. In</title>
<date>2006</date>
<booktitle>Proc. EMNLP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1595" citStr="Marcu et al., 2006" startWordPosition="232" endWordPosition="235"> with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side ru</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. In Proc. EMNLP, pages 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrased-Based Translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="3103" citStr="Marton and Resnik, 2008" startWordPosition="472" endWordPosition="475">del in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the </context>
<context position="11406" citStr="Marton and Resnik, 2008" startWordPosition="1856" endWordPosition="1859">nd choke the training procedure. In practice, to limit the size of the training set, the negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nontermin</context>
<context position="13820" citStr="Marton and Resnik, 2008" startWordPosition="2238" endWordPosition="2241">e development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. Methods NIST 2006 NIST 2008 Baseline 0.3025 0.2200 XP+ 0.3061 0.2254 TOFW 0.3089 0.2253 Our method 0</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In Proc. ACL, pages 1003-1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>ForestBased Translation. In</title>
<date>2008</date>
<booktitle>Proc. ACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1612" citStr="Mi et al., 2008" startWordPosition="236" endWordPosition="239"> model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one o</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. ForestBased Translation. In Proc. ACL, pages 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="13391" citStr="Papineni et al., 2002" startWordPosition="2172" endWordPosition="2175"> instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to 0 = 0.75 and co = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bra</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proc. ACL, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In</title>
<date>2007</date>
<booktitle>Proc. HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="12637" citStr="Petrov and Klein, 2007" startWordPosition="2040" endWordPosition="2043">ions and orders in sourceside/target-side rules. This feature interacts between source and target components since it shows whether the translation ordering is affected. 5. Length features, which are the length of sub-phrases covered by source nonterminals. 4 Experiments 4.1 Experiment setting We implement a hierarchical phrase-based system similar to the Hiero (Chiang, 2005) and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser (Petrov and Klein, 2007). The ME training toolkit, developed by (Zhang, 2006), is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to 0 = 0.75 and co = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 eval</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proc. HLT-NAACL, pages 404-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1536" citStr="Quirk et al., 2005" startWordPosition="220" endWordPosition="223">d can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what p</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proc. ACL, pages 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1632" citStr="Shen et al., 2008" startWordPosition="240" endWordPosition="243"> The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate tran</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proc. ACL, pages 577-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective Use of Linguistic and Contextual Information for Statistical Machine Translation. In</title>
<date>2009</date>
<booktitle>Proc. EMNLP,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="11469" citStr="Shen et al., 2009" startWordPosition="1868" endWordPosition="1871">e training set, the negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This f</context>
<context position="14060" citStr="Shen et al., 2009" startWordPosition="2280" endWordPosition="2283">s is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. Methods NIST 2006 NIST 2008 Baseline 0.3025 0.2200 XP+ 0.3061 0.2254 TOFW 0.3089 0.2253 Our method 0.3141 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p &lt; 0.01) As shown in Table 1, all the methods outperform the baseline because they have extra models to gu</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective Use of Linguistic and Contextual Information for Statistical Machine Translation. In Proc. EMNLP, pages 72-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min Yen Kan</author>
<author>Haizhou Li</author>
<author>Philip Resnik</author>
</authors>
<title>Topological Ordering of Function Words in Hierarchical Phrase-based Translation. In</title>
<date>2009</date>
<booktitle>Proc. ACL,</booktitle>
<pages>324--332</pages>
<contexts>
<context position="2843" citStr="Setiawan et al., 2009" startWordPosition="429" endWordPosition="432">didate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work (Chiang, 2005), the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens</context>
<context position="11450" citStr="Setiawan et al., 2009" startWordPosition="1864" endWordPosition="1867">to limit the size of the training set, the negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target</context>
<context position="12937" citStr="Setiawan et al., 2009" startWordPosition="2091" endWordPosition="2094">lement a hierarchical phrase-based system similar to the Hiero (Chiang, 2005) and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser (Petrov and Klein, 2007). The ME training toolkit, developed by (Zhang, 2006), is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following (Setiawan et al., 2009), we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to 0 = 0.75 and co = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our</context>
</contexts>
<marker>Setiawan, Kan, Li, Resnik, 2009</marker>
<rawString>Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip Resnik. 2009. Topological Ordering of Function Words in Hierarchical Phrase-based Translation. In Proc. ACL, pages 324-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="14036" citStr="Wu, 1997" startWordPosition="2277" endWordPosition="2278">core differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. Methods NIST 2006 NIST 2008 Baseline 0.3025 0.2200 XP+ 0.3061 0.2254 TOFW 0.3089 0.2253 Our method 0.3141 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two approaches (p &lt; 0.01) As shown in Table 1, all the methods outperform the baseline because they</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): pages 377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ACLColing,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="1355" citStr="Xiong et al., 2006" startWordPosition="193" endWordPosition="196">e selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asi</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proc. ACLColing, pages 521-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A Syntax-Driven Bracketing Model for Phrase-Based Translation. In</title>
<date>2009</date>
<booktitle>Proc. ACL,</booktitle>
<pages>315--323</pages>
<contexts>
<context position="3124" citStr="Xiong et al., 2009" startWordPosition="476" endWordPosition="479">-based SMT system such as Pharaoh (Koehn et al., 2003). Extending this work, (He et al., 2008; Liu et al., 2008) integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform, (Setiawan et al., 2009) proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection. (Marton and Resnik, 2008; Xiong et al., 2009) explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the search in a larger sp</context>
<context position="11490" citStr="Xiong et al., 2009" startWordPosition="1872" endWordPosition="1875"> negative instances constructed based on low-frequency target-side rules are pruned. 3.2 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work (He et al., 2008; Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009): 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 8 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This feature interacts betw</context>
<context position="13944" citStr="Xiong et al., 2009" startWordPosition="2260" endWordPosition="2263">ation metric is the case-insensitive BLEU4 (Papineni et al., 2002). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn, 2004). 4.2 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). As (Xiong et al., 2009)’s work is based on phrasal SMT system with bracketing transduction grammar rules (Wu, 1997) and (Shen et al., 2009)’s work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with (He et al., 2008)’s work due to its less practicability of integrating numerous sub-models. Methods NIST 2006 NIST 2008 Baseline 0.3025 0.2200 XP+ 0.3061 0.2254 TOFW 0.3089 0.2253 Our method 0.3141 0.2318 Table 1: Comparison results, our method is significantly better than the baseline, as well as the other two app</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2009</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based Translation. In Proc. ACL, pages 315-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Syntaxbased Statistical Translation Model.</title>
<date>2001</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="1516" citStr="Yamada and Knight, 2001" startWordPosition="216" endWordPosition="219">s is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance. 1 Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models (Koehn et al., 2003; Xiong et al., 2006), but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006; Mi et al., 2008; Shen et al., 2008). In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus (Chiang, 2005). SMT decoders are forced to face the challenge of This work was finished while the first author visited Microsoft Research Asia as an intern. proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side ru</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A Syntaxbased Statistical Translation Model. In Proc. ACL, pages 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for python and c++.</title>
<date>2006</date>
<note>availableathttp://homepages.inf.ed.ac.uk/ lzhang10/maxent_toolkit.html.</note>
<marker>Le Zhang, 2006</marker>
<rawString>Le Zhang. 2006. Maximum entropy modeling toolkit for python and c++. availableathttp://homepages.inf.ed.ac.uk/ lzhang10/maxent_toolkit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>