<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.983567">
Multiple Interpreters in a
Principle-Based Model of Sentence Processing
</title>
<note confidence="0.716147">
Matthew W. Crocker
</note>
<email confidence="0.986565">
e-mail: mwc@aipna.ed.ac.uk
</email>
<affiliation confidence="0.9977295">
Department of Artificial Intelligence Human Communication Research Centre
University of Edinburgh and University of Edinburgh
</affiliation>
<address confidence="0.930624">
80 South Bridge 2 Buccleuch Place
Edinburgh, Scotland, El11 1HN Edinburgh, Scotland, EH8 9LW
</address>
<sectionHeader confidence="0.988136" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999752538461539">
This paper describes a computational model of human
sentence processing based on the principles and pa-
rameters paradigm of current linguistic theory. The
syntactic processing model posits four modules, re-
covering phrase structure, long-distance dependencies,
coreference, and thematic structure. These four mod-
ules are implemented as meta-interpreters over their
relevant components of the grammar, permitting vari-
ation in the deductive strategies employed by each
module. These four interpreters are also `coroutined&apos;
via the freeze directive of constraint logic program-
ming to achieve incremental interpretation across the
modules.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999411142857143">
A central aim of computational psycholinguistics is the
development of models of human sentence processing
which account not only for empirical performance phe-
nomena, but which also provide some insight into the
nature of between parser and grammar relationship.
In concurrent research, we are developing a model of
sentence processing which has its roots in the princi-
ples and parameters paradigm of syntactic theory [1],
[2], which holds that a number of representations are
involved in determining a well-formed analysis of an
utterance. This, in conjunction with Fodor&apos;s Modu-
larity Hypothesis [6], has led us to postulate a model
which consists of four informationally encapsulated
modules for recovering (1) phrase structure, (2) chains
(3) coreference, and (4) thematic structure.
In this paper we will briefly review a model of sen-
tence processing which has been previously proposed
in [5] and [3]. We will illustrate how this model can
be naturally implemented within the logic program-
ming paradigm. In particular, we sketch a subset of
GB theory which defines principles in terms of their
representational units, or schemas. We then discuss
how the individual processors may be implemented as
specialised, informationally encapsulated interpreters,
and discuss how the &apos;freeze&apos; directive of constraint
logic programming can be used to effectively coroutine
the interpreters, to achieve incremental interpretation
and concurrency.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="method">
2 The Processing Model
</sectionHeader>
<bodyText confidence="0.997632333333333">
In the proposed model, we assume that the sentence
processor strives to optimise local comprehension and
integration of incoming material, into the current con-
text. That is, decisions about the current syntactic
analysis are made incrementally (for each input item)
on the basis of principles which are intended max-
imise the overall interpretation. We have dubbed this
the Principle of Incremental Comprehension (PIC),
stated roughly as follows:
</bodyText>
<listItem confidence="0.586345">
(1) Principle of Incremental Comprehension:
</listItem>
<bodyText confidence="0.9996476">
The sentence processor operates in such a
way as to maximise comprehension of the
sentence at each stage of processing.
The PIC demands that the language comprehen-
sion system (LCS), and any sub-system contained
within it (such as the syntactic and semantic pro-
cessors), apply maximally to any input, thereby con-
structing a maximal, partial interpretation for a given
partial input signal. This entails that each module
within the LCS apply concurrently.
The performance model is taken to be directly
compatible with the modular, language universal,
principle-based theory of current transformational
grammar [3]. We further suggest a highly modular or-
ganisation of the sentence processor wherein modules
are determined by the syntactic representations they
recover. This is motivated more generally by Fodor&apos;s
Modularity Hypothesis [6] which argues that the var-
ious perceptual/input systems consist of fast, dumb
informationally encapsulated modules. Specifically,
we posit four modules within the syntactic processor,
each affiliated with a &amp;quot;representational&amp;quot; or &amp;quot;informa-
tional&amp;quot; aspect of the grammar. These are outlined be-
low in conjunction with the grammatical subsystems
to which they are related&apos;:
</bodyText>
<footnote confidence="0.99149975">
1 This grouping of grammatical principles with representations
is both partial and provisional, and is intended only to give
the reader a feel for the &amp;quot;natural classes&amp;quot; exploited by the
model.
</footnote>
<table confidence="0.864232">
- 185 -
(2)
Modules &amp; Principles:
Phrase structure (PS): 7-theory, Move-a
Chains (ChS): Bounding, ECP
Theta structure (TS): 0-theory
Coreference (CiS): Binding, Control
</table>
<bodyText confidence="0.980755">
In Figure 1, we illustrate one possible instance of
the organisation within the Syntactic Processor. We
assume that the phrase structure module drives pro-
cessing based on lexical input, and that the thematic
structure co-ordinates the relevant aspects of each pro-
cessor for output to the semantic processor.
</bodyText>
<figureCaption confidence="0.998695">
Figure 1: Syntactic Processor Organisation
</figureCaption>
<bodyText confidence="0.9999328">
Just as the PIC applies to the main modules of the
LCS as discussed above, it also entails that all modules
within the syntactic processor act concurrently so as
to apply maximally for a partial input, as illustrated
by the operation shown in Figure 2. For the partial
input &amp;quot;What did John put ... &amp;quot;, we can recover the
partial phrase structure, including the trace in Infl2.
In addition, we can recover the chain linking the did to
its deep structure position in Infl (e-1), and also the
0-grid for the relation &apos;put&apos; including the saturated
agent role &apos;John&apos;. We might also go one step further
and postulate a trace as the direct object of put, which
could be the 0-position of What, but this action might
be incorrect if the sentence turned out to be What did
John put the book on?, for example.
</bodyText>
<sectionHeader confidence="0.976013" genericHeader="method">
3 Principles and Representations
</sectionHeader>
<bodyText confidence="0.912943">
Before proceeding to a discussion of the model&apos;s im-
plementation, we will first examine more closely the
representational paradigm which is employed, and dis-
cuss some aspects of the principles and parameters
theory we have adopted, restricting our attention pri-
marily to phrase structure and Chains. In general, a
2 We assume here a head movement analysis, where the head of
Intl moves to the head of Comp, to account for Subject-Aux
inversion.
</bodyText>
<figure confidence="0.539855">
&amp;quot;What did iohn put ...&amp;quot;
</figure>
<figureCaption confidence="0.998188">
Figure 2: Syntactic Processor Operation
</figureCaption>
<bodyText confidence="0.998710461538462">
particular representation can be broken down into two
fundamental components: 1) units of information, i.e.
the &apos;nodes&apos; or feature-bundles which are fundamental
to the representation, and 2) units of structure, the
minimal structural &apos;schema&apos; for relating &apos;nodes&apos; with
each other. With these two notions defined, the rep-
resentation can be viewed as some recursive instance
of its particular schema over a collection of nodes.
The representation of phrase structure (PS), as de-
termined principally by X-theory, encodes the local,
sister-hood relations and defines constituency. The
bar-level notation is used to distinguish the status of
satellites:
</bodyText>
<equation confidence="0.99790575">
X Specifier,
X- -4 Modifier, 7(
Y -4 Complements, X
X Lexeme
</equation>
<bodyText confidence="0.9997576">
The linear precedence of satellites with respect to
their sister X-nodes is taken to be parametrised for
each category and language. The final rule (d) above,
is simply the rule for lexical insertion. In addition to
the canonical structures defined by 7-theory, we re-
quire additional rules to permit Chomsky-adjunction
of phrases to maximal projections, via Move-a, and
the rules for inserting traces (or more generally, empty
categories) — a consequence of the Projection Princi-
ple — for moved heads and maximal projections.
Given the rules above, we can see that possible
phrase structures are limited to some combination of
binary (non-terminal) and unary (terminal) branches.
As discussed above, we can characterise the represen-
tational framework in terms of nodes and schemas:
</bodyText>
<figure confidence="0.96980887804878">
LexiclInput
Chain
Processor
Thematic
Processor
Thematic Output
Coindexation
Processor
;
Phrase Structure
Processor
a* OP me 0.
[
rel: put
grid: [agent: John ]
theme:
location:
.11.•
- - - J
s
1 C .,),Z..........
1
I did
... NP ,,,,,y,.....,...
V ..
- ,
I ..--&amp;quot;•-■
1 [ What ,.- e-1 V ] I ..
I [did,e-11 I 0 ... Put
i
John
Spec
What
- 186 -
Phrase Structure Schema
Node: N-Node: {Cat,LeveLID,Ftrs}
T-Node: {Cat,Phon,ID,Ftrs}
Schema: Branch: N -N ode/ [N-N ode,N-N ode]
Branch: N-Node/T-Node
Structure: Tree: N-Node/[TreeL,TreeRi
Tree: N-Node/T-Node
</figure>
<bodyText confidence="0.999971033333334">
We allow two types of nodes: 1) non-terminals (N-
Nodes), which are the nodes projected by X-theory,
consisting of category, bar level, a unique ID, and the
features projected from the head, and 2) terminals (T-
Nodes), which are either lexical items or empty cat-
egories, which lack bar level, but posses phonological
features (although these may be &apos;nil&apos; for some empty
categories). The schema, defines the unit of structure,
using the `/&apos; to represent immediate dominance, and
square brackets `[... ]&apos; to encode sister-hood and linear
precedence. Using this notation we define the two pos-
sible types of branches, binary and unary, where the
latter is applicable just in case the daughter is a termi-
nal node. The full PS representation (or Tree) is de-
fined by allowing non-terminal daughters to dominate
a recursive instance of the schema. It is interesting to
note that, for phrase structure at least, the relevant
principles of grammar can be stated purely as condi-
tions on branches, rather that trees. More generally,
we will assume the schema of a particular representa-
tion provides a formal characterisation of locality.
Just as phrase structure is defined in terms of
branches, we can define Chains as a sequence of links.
More specifically, each position contained by the chain
is a node, which represents its category and level (a
phrase or a head), the status of that position (either
A or 74) , its ID (or location), and relevant features
(such as L-marking, Case, and 8). If we adhere to the
representational paradigm used above, we can define
Chains in the following manner:
</bodyText>
<subsectionHeader confidence="0.571344">
Chain Schema
</subsectionHeader>
<construct confidence="0.8629936">
Node: C-Node: {Cat,Level,Pos,ID,Ftrs}
Schema: Link: &lt;C-Node i oo C-Node,&gt;
Structure: Chain: [C-Node I Chain] (where,
&lt;C-Node cx) head(Chain)&gt; )
Chain: [ ]
</construct>
<bodyText confidence="0.999736777777778">
If we let `co&apos; denote the linking of two C-Nodes,
then we can define a Chain to be an ordered fist of C-
Nodes, such that successive C-Nodes satisfy the link
relation. In the above definition we have used the
`1&apos; operator and list notation in the standard Prolog
sense. The &apos;head&apos; function returns the first C-Node in
a (sub) Chain (possibly [ ]), for purposes of satisfying
the link relation. Furthermore, &lt;C-Node oo [J&gt; is
a well-formed link denoting the tail, Deep-Structure
position, of a Chain. Indeed, if this is the only link in
the Chain we refer to it as a &apos;unit&apos; Chain, representing
an unmoved element.
We noted above that each representation&apos;s schema
provides a natural locality constraint. That is, we
should be able to state relevant principles and con-
straints locally, in terms of the schematic unit. This
clearly holds for Subjacency, a well-formedness condi-
tion which holds between two nodes of a link:
</bodyText>
<equation confidence="0.2982155">
(4) &lt;C-Nodei oo C-Nodei&gt; --P
sub jacent(C-Nodei ,C-Nodei )
</equation>
<bodyText confidence="0.995512444444444">
Other Chain conditions include the Case filter and
0-Criterion. The former stipulates that each NP
Chain receive Case at the &apos;highest&apos; A-position, while
the latter entails that each argument Chain receive ex-
actly one 0-role, assigned to the uniquely identifiable
&lt; C-Nodee oo [ ] &gt; link in a Chain. It is therefore
possible to enforce both of these conditions on locally
identifiable links of a Chain:
In an argument (NP) Chain,
</bodyText>
<listItem confidence="0.98078475">
i) &lt;C-Node x oo C-NodeA&gt;
case-mark(C-NodeA) or,
ii) C-NodeA = head(Chain)
case-mark(C-NodeA)
</listItem>
<bodyText confidence="0.9957027">
In an argument Chain,
&lt;C-Nodee oo H&gt; --4 theta-mark(C-Nodee)
In describing the representation of a Chain, we have
drawn upon Prolog&apos;s list notation. To carry this fur-
ther, we can consider the link operator `oo&apos; to be
equivalent to the `,&apos; separator in a list, such that for all
[...C-Nodei,C-Nodei ], C-Nodei oo C-Nodei holds.
In this way, we ensure that each node is well-formed
with respect to adjacent nodes (i.e. in accordance with
principles such as those identified in (4) &amp; (5) ).
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="method">
4 The Computational Model
</sectionHeader>
<bodyText confidence="0.9999902">
In the same manner that linguistic theory makes the
distinction between competence (the grammar) and
performance (the parser), logic programming distin-
guishes the declarative specification of a program from
its execution. A program specification consists of a set
of axioms from which solution(s) can be proved as de-
rived theorems. Within this paradigm, the nature of
computation is determined by the inferencing strat-
egy employed by the theorem prover. This aspect of
logic programming has often been exploited for pars-
ing; the so called Parsing as Deduction hypothesis.
In particular it has been shown that meta-interpreters
or program transformations can be used to affect the
manner in which a logic grammar is parsed [10] [11].
Recently, there has been an attempt to extend the
PAD hypothesis beyond its application to simple logic
grammars [14], [13] and [8]. In particular, Johnson
has developed a prototype parser for a fragment of
a GB grammar [9]. The system consists of a declara-
tive specification of the GB model, which incorporates
</bodyText>
<equation confidence="0.8042635">
(5)
- 187 -
</equation>
<bodyText confidence="0.999928222222222">
the various principles of grammar and multiple levels
of representation. Johnson then illustrates how the
fold/unfold transformation and goal freezing, when
applied to various components of the grammar, can be
used to render more or less efficient implementations.
Unsurprisingly, this deductive approach to parsing in-
herits a number of problems with automated deduc-
tion in general. Real (implemented) theorem provers
are, at least in the general case, incomplete. Indeed,
we can imagine that a true, deductive implementation
of GB would present a problem. Unlike traditional,
homogeneous phrase structure grammars, GB makes
use of abstract, modular principles, each of which may
be relevant to only a particular type or level of repre-
sentation. This modular, heterogeneous organisation
therefore makes the task of deriving some single, spe-
cialised interpreter with adequate coverage and effi-
ciency, a very difficult one.
</bodyText>
<subsectionHeader confidence="0.992981">
4.1 Deduction in a Modular System
</subsectionHeader>
<bodyText confidence="0.999967130434783">
In contrast with the single processor model employed
by Johnson, the system we propose consists of a num-
ber of processors over subsets of the grammar. Cen-
tral to the model is a declarative specification of the
principles of grammar, defined in terms of the rep-
resentations listed in (2), as described in §3. If we
take this specification of the grammar to be the &amp;quot;com-
petence component&amp;quot;, then the &amp;quot;performance compo-
nent&amp;quot; can be stated as a parse relation which maps
the input string to a well-formed &amp;quot;State&amp;quot;, where State
= PS,TS,ChS,CiS }, the 4-tuple constituting all as-
pects of syntactic analysis. The highest level of the
parser specifies how each module may communicate
with the others. Specifically, the PS processor acts
as input to the other processors which construct their
representations based on the PS representations and
their own &amp;quot;representation specific&amp;quot; knowledge. In a
weaker model, it may be possible for processors to in-
spect the current State (i.e. the other representations)
but crucially, no processor ever actually &amp;quot;constructs&amp;quot;
another processor&apos;s representation. The communica-
tion between modules is made explicit by the Prolog
specification shown below:
</bodyText>
<equation confidence="0.627771">
(6) parse(LexInput,State) : —
</equation>
<bodyText confidence="0.95452415">
State = {PS,TS,ChS,CiS},
ts_module(PS,TS),
chs_module(PS,ChS),
cis_module(PS ,CiS),
ps_module(LexInput,PS).
The parse relation defines the organisation of the
processors as shown in Figure 1. The Prolog speci-
fication above appears to suffer from the traditional
depth-first, left-to-right computation strategy used by
Prolog. That is, parse seems to imply the sequen-
tial execution of each processor. As Stabler has illus-
trated, however, it is possible to alter the computation
rule used [12], so as to permit incremental interpreta-
tion by each module: effectively coroutining the vari-
ous modules. Specifically, Prolog&apos;s freeze directive al-
lows processing of a predicate to be delayed temporar-
ily until a particular argument variable is (partially)
instantiated. In accord with the input dependencies
shown in (7), each module is frozen (or &apos;waits&apos;) on its
input:
</bodyText>
<table confidence="0.8847672">
Input dependencies
ts_module PS
chs _module PS
cis_module PS
ps_module LexInput
</table>
<bodyText confidence="0.9973171">
Us ng this feature we may effectively &amp;quot;coroutine&amp;quot;
the four modules, by freezing the PS processor on
Input, and freezing the remaining processors on PS.
The result is that each representation is constructed
incrementally, at each stage of processing. To illus-
trate this, consider once again the partial input string
&amp;quot;What did John put ... &amp;quot;. The result of the call
parse ( [what , did, joint, put I , Stet e) would yield
the following instantiation of State (with the repre-
sentations simplified for readability):
</bodyText>
<equation confidence="0.986063555555555">
(8)
State = PS = cp/[np/what,
cl/[c/did,
ip/[np/John
il/[i/trace-1,
vp/[v/put, _]]]],
TS = [rel:put,grid:[agent:john I _]],
ChS = [ [what, _], [did,trace-1] I,
CiS = - )
</equation>
<bodyText confidence="0.9999124">
The PS representation has been constructed as
much as possible, including the trace of the moved
head of Infl. The ChS represents a partial chain for
what and the entire chain for did, which moved from
its deep structure position to the head of CP, and TS
contains a partial 0-grid for the relation &apos;put&apos;, in which
the Agent role as been saturated.
This is reminiscent of Johnson&apos;s approach [9], but
differs crucially in a number of respects. Firstly, we
posit several processors which logically exploit the
gratrimar, and it is these processors which are corou-
tined, not the principles of grammar themselves. Each
interpreter is responsible for recovering only one, ho-
mogeneous representation, with respect to one input
representation. This makes reasoning about the com-
putational behaviour of individual processors much
easier. At each stage of processing, the individual pro-
cessors increment their representations if and only if,
for the current input, there is a &amp;quot;theorem&amp;quot; provable
from the grammar, which permits the new structure to
be added. This meta-level &amp;quot;parsing as deduction&amp;quot; ap-
proach permits more finely tuned control of the parser
as a whole, and allows us to specify distinct inferenc-
ing strategies for each interpreter, tailoring it to the
particular representation.
</bodyText>
<equation confidence="0.723543">
(7)
- 188 -
</equation>
<subsectionHeader confidence="0.723516">
4.2 The PS-Module Specification
</subsectionHeader>
<table confidence="0.972354857142857">
PS-View: Mother/[Left, Right]
Mother/Terminal
X-Bar Theory Move-alpha
XP_10&amp;quot; Specifier, X&apos; XP_0&amp;quot; Adjunct, XP
X&apos; —0&amp;quot; Modifier, X&apos; XP —0&amp;quot; trace
X&apos; -10&amp;quot; Complements, X X —0&amp;quot; trace
X -1&amp;quot;. Le_xeme
</table>
<figureCaption confidence="0.999342">
Figure 3: The Phrase Structure Module
</figureCaption>
<bodyText confidence="0.999963818181818">
We have illustrated in §3 that the various represen-
tations and grammatical principles may be defined in
terms of their respective schematic units. Given this,
the task of recovering representations (roughly pars-
ing) is simply a matter of proving well-formed rep-
resentations, as recursive instances of &apos;schematic ax-
ioms&apos;, i.e. those instantiations of a schema which are
considered well-formed by the grammar. The form
of the PS-Module can be depicted as in Figure 3.
The PS interpreter incorporates lexical input into the
phrase structure tree based on possible structures al-
lowed by the grammar. Possible structures are deter-
mined by the ps_view relation, which returns those
possible instantiations of the PS schema (as described
in §3) which are well-formed with respect to the rele-
vant principles of grammar. In general, ps_viee will
return any possible branch structure licensed by the
grammar, but is usually constrained by partial instan-
tiation of the query. In cases where, multiple analyses
are possible, the pa_view predicate may use some se-
lection rule to choose between them 3. The following
is a specification of the PS interpreter:
</bodyText>
<equation confidence="0.972020111111111">
(9) ps_module(X-XO,Node/PLD,R/RD}) : —
non_lexical(Node),
ps_view(Node/ [L,R]),
ps_module(X-X1,LAD),
ps_rnodule(X1-XO,R/RD).
ps_module(X-XO,Node/Daughters) : —
ps_ec_eval(X-XO,Node/Daughters).
ps_module(X-XO,Node/Daughters) : —
pslex_eval(X-XO,Node/Daughters).
</equation>
<bodyText confidence="0.9982765">
As we have discussed above, the ps.module is frozen
on lexical input represented here as as difference-list.
</bodyText>
<footnote confidence="0.890541333333333">
3 This is one way in which we might implement attachment
principles to account for human preferences, see Crocker [4]
for discussion.
</footnote>
<bodyText confidence="0.999905411764706">
The top-level of the PS interpreter is broken down
into three possible cases. The first handles non-lexical
nodes, i.e. those of category C or I, since phrase struc-
ture for these categories is not lexically determined,
and can be derived &apos;top-down&apos;, strictly from the gram-
mar. We can, for example, automatically hypothesize
a Subject specifier and VP complement for Ira The
second clause deals with the postulation of empty cat-
egories, while the third can be considered the &apos;base&apos;
case which simply attaches lexical material. Roughly,
ps_ec_eval attempts to derive a leaf which is a trace.
This action is then verified by the concurrent Chain
processor, which determines whether or not the trace
is licensed (see the following section). This imple-
ments an approximation of the filler-driven strategy
for identifying traces, a strategy widely accepted in
the psycholinguistic literature4.
</bodyText>
<subsectionHeader confidence="0.998488">
4.3 The Chain-Module Specification
</subsectionHeader>
<bodyText confidence="0.998748555555556">
Just as the phrase structure processor is delayed on
lexical input, the chain processor is frozen with re-
spect to phrase structure. The organisation of the
Chain Module is shown in Figure 4, and is virtually
identical to that of the PS Module (in Figure 3). How-
ever rather than recovering branches of phrase struc-
ture, it recovers links of chains, determining their well-
formedness with respect to the relevant grammatical
axioms.
</bodyText>
<figure confidence="0.957163857142857">
Subjacency:
C-Node) • C -Node 2) —10• subjacent(C-Nodel ,C-Node2)
Theta-Criterion:
C-Node , -&amp;quot;0&amp;quot; theta-marked(C-Node)
A-to-A-bar Constraint:
C-Node) , C -Node 21 not( not( a-pos(C-Nodel ) and
a-bar-post C-Node2) )
</figure>
<figureCaption confidence="0.999991">
Figure 4: The Chain Module
</figureCaption>
<bodyText confidence="0.999135833333333">
For this module, incremental processing is imple-
mented by &apos;freezing&apos; with respect to the input tree
representation. The following code illustrates how the
top-level of the Chain interpreter can traverse the PS
tree, such that it is coroutined with the recovery of
the PS representation:
</bodyText>
<footnote confidence="0.764218333333333">
4 The is roughly the A ctive Filler Strategy [7]. For discussion
on implementing such strategies within the present model see
[4].
</footnote>
<figure confidence="0.971300722222222">
PS-Tree _ow
Output
Chains
Output
Chain-View: [ C-Nodel , C-Node2]
[ C-Node , [I]
fzi• &apos;t■•r■
Interpreter for
Chain Structure
t A
ho, PS-Tree
Phrase Structure Output
Interpreter for
Lexical
Input
A
- 189 -
(10) chs_module(X/PLD,R/RDLCS) : —
</figure>
<bodyText confidence="0.936346588235294">
chain_int(X/[L,R],CS),
chs_module(L/LD,CS),
chs_module(R/RD,CS).
chs_module(X/Leaf,CS) : —
chain_int(X/Leaf,CS).
I will assume that chs_module is frozen such that
it will only execute if the daughter(s) of the current
sub-tree is instantiated. Given this, chs_module will
perform a top-down traversal of the PS tree, delaying
when the daughters are uninstantiated, thus corou-
tined with the PS-module. The chain_int predicate
then determines if any action is to be taken, for the
current branch, by the chain interpreter:
(11) chain_int(X/[Satellite,Right],CS) : —
visible( X/Patellite,RightbC-Node),
chain_member(C-Nodes,CS).
chainint( X/ [Left,Satellite] ,CS) : —
visible(X/[Left,Satellite),C-Node),
chain_member(C-Nodes,CS).
The chain_int predicate decides whether or not
the satellite of the current branch is relevant, or
&apos;visible&apos; to the Chain interpreter, and if so returns
an appropriate C-Node for that element. The two
visible entities are antecedents, i.e. arguments (if
we assume that all arguments form chains, possibly
of unit length) or elements in an A positions (such
as [Spec,CP] or a Chomsky-adjoined position) and
traces. If a trace or an antecedent is identified, then
it must be a member of a well-formed chain. The
chain_member predicate postulates new chains for lex-
ical antecedents, and attempts to append traces to ex-
isting chains. This operation must in turn satisfy the
chain_view relation, to ensure the added link obeys
the relevant grammatical constraints.
</bodyText>
<sectionHeader confidence="0.995693" genericHeader="conclusions">
5 Summary and Discussion
</sectionHeader>
<bodyText confidence="0.999994608695652">
In constructing a computational model of the pro-
posed theory of processing, we have employed the logic
programming paradigm which permits the transpar-
ent distinction between competence and performance.
At the highest level, we have a simple specification
of the models organisation, in addition we have em-
ployed a &apos;freeze&apos; control strategy which permits us to
coroutine the individual processors, permitting max-
imal incremental interpretation. The individual pro-
cessors consist of specialised interpreters which are in
turn designed to perform incrementally. The inter-
preters construct their representations, by incremen-
tally adding units of structure which are locally well-
formed with respect to the principles of the module.
The implementation is intended to allow some flex-
ibility in specifying the grammatical principles, and
varying the control strategies of various modules. It
is possible that some instantiations of the syntactic
theory will prove more or less compatible with the
processing model. It is hoped that such results may
point the way to a more unified theory of grammar
and processing or will at least shed greater light on
the nature of their relationship.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999818833333333">
I would like to thank Elisabet Engdahl and Robin
Cooper for their comments on various aspects of this
work. This research was conducted under the sup-
port of an ORS award, an Edinburgh University Stu-
dentship and the Human Communication Research
Centre.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999196489361702">
[1] N. Chomsky. Barriers. Linguistic Inquiry Mono-
graph Thirteen, The MIT Press, Cambridge, Mas-
sachusetts, 1986.
[2] N. Chomsky. Knowledge of Language: Its Nature,
Origin and Use. Convergence Series, Praeger,
New York, 1986.
M. Crocker. Incrementality and Modularity in
a Principle-Based Model of Sentence Process-
ing. Presented at The Workshop on GB-Parsing,
Geneva, Switzerland, 1990.
[4) M. Crocker. Multiple Meta-Interpreters in a Log-
ical Model of Sentence Processing. In Brown and
Koch, editors, Natural Language Understanding
and Logic Programming, III, Elsevier Science
Publishers (North-Holland), (to appear).
[51 M. Crocker. Principle-Based Sentence Process-
ing: A Cross-Linguistic Account. HCRC/RP 1,
Human Communication Research Centre, Uni-
versity of Edinburgh, U.K., March 1990.
[61 J. Fodor. Modularity of Mind. MIT Press, Cam-
bridge, Massachusetts, 1983.
[71 L. Frazier and C. Clifton. Successive Cyclicity in
the Grammar and Parser. Language and Cogni,
tive Processes, 4(2):93-126, 1989.
[91 M. Johnson. Program Transformation Tech-
niques for Deductive Parsing. In Brown and
Koch, editors, Natural Language Understanding
and Logic Programming, III, Elsevier Science
Publishers (North-Holland), (to appear).
[91 M. Johnson. Use of Knowledge of Lan-
guage. Journal of Psycholinguistic Research,
18(1), 1989.
[10] F. Pereira and D. Warren. Parsing as Deduction.
In Proceedings of Twenty-First Conference of the
ACL, Cambridge, Massachusetts, 1983.
[11] F. Pereira and S. Shieber. Prolog and Natural-
Language Analysis. CSLI Lecture Notes, Center
for the Study of Language and Information, Stan-
ford, California, 1987.
[12) E. Stabler. Avoid the pedestrian&apos;s paradox. un-
published ms., Dept. of Linguistics, UCLA, 1989.
[13] E. Stabler. Relaxation Techniques for Principle-
Based Parsing. Presented at The Workshop on
GB Parsing, Geneva, Switzerland, 1990.
[141 E. Stabler. The Logical Approach to Syntax. The
MIT Press, Cambridge, Massachusetts, (forth-
coming).
</reference>
<page confidence="0.9251315">
[31
190 -
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901997">
<title confidence="0.999736">Multiple Interpreters in a Principle-Based Model of Sentence Processing</title>
<author confidence="0.999944">Matthew W Crocker</author>
<email confidence="0.99984">e-mail:mwc@aipna.ed.ac.uk</email>
<affiliation confidence="0.9998795">Department of Artificial Intelligence Human Communication Research Centre University of Edinburgh and University of Edinburgh</affiliation>
<address confidence="0.9616375">80 South Bridge 2 Buccleuch Place Edinburgh, Scotland, El11 1HN Edinburgh, Scotland, EH8 9LW</address>
<abstract confidence="0.9977035">This paper describes a computational model of human processing based on the and parameters paradigm of current linguistic theory. The syntactic processing model posits four modules, recovering phrase structure, long-distance dependencies, coreference, and thematic structure. These four modules are implemented as meta-interpreters over their relevant components of the grammar, permitting variation in the deductive strategies employed by each module. These four interpreters are also `coroutined&apos; the of constraint logic programming to achieve incremental interpretation across the modules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Barriers</author>
</authors>
<title>Linguistic Inquiry Monograph Thirteen,</title>
<date>1986</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<marker>Barriers, 1986</marker>
<rawString>[1] N. Chomsky. Barriers. Linguistic Inquiry Monograph Thirteen, The MIT Press, Cambridge, Massachusetts, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Knowledge of Language: Its Nature, Origin and Use. Convergence Series, Praeger,</title>
<date>1986</date>
<location>New York,</location>
<marker>Chomsky, 1986</marker>
<rawString>[2] N. Chomsky. Knowledge of Language: Its Nature, Origin and Use. Convergence Series, Praeger, New York, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Crocker</author>
</authors>
<title>Incrementality and Modularity in a Principle-Based Model of Sentence Processing. Presented at The Workshop on GB-Parsing,</title>
<date>1990</date>
<location>Geneva, Switzerland,</location>
<marker>Crocker, 1990</marker>
<rawString>M. Crocker. Incrementality and Modularity in a Principle-Based Model of Sentence Processing. Presented at The Workshop on GB-Parsing, Geneva, Switzerland, 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Crocker</author>
</authors>
<title>Multiple Meta-Interpreters in a Logical Model of Sentence Processing.</title>
<booktitle>Natural Language Understanding and Logic Programming, III, Elsevier Science Publishers (North-Holland),</booktitle>
<editor>In Brown and Koch, editors,</editor>
<note>(to appear).</note>
<marker>Crocker, </marker>
<rawString>[4) M. Crocker. Multiple Meta-Interpreters in a Logical Model of Sentence Processing. In Brown and Koch, editors, Natural Language Understanding and Logic Programming, III, Elsevier Science Publishers (North-Holland), (to appear).</rawString>
</citation>
<citation valid="true">
<title>Principle-Based Sentence Processing: A Cross-Linguistic Account.</title>
<date>1990</date>
<journal>HCRC/RP</journal>
<volume>1</volume>
<institution>Human Communication Research Centre, University of Edinburgh,</institution>
<location>U.K.,</location>
<marker>1990</marker>
<rawString>[51 M. Crocker. Principle-Based Sentence Processing: A Cross-Linguistic Account. HCRC/RP 1, Human Communication Research Centre, University of Edinburgh, U.K., March 1990.</rawString>
</citation>
<citation valid="true">
<title>Modularity of Mind.</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<marker>1983</marker>
<rawString>[61 J. Fodor. Modularity of Mind. MIT Press, Cambridge, Massachusetts, 1983.</rawString>
</citation>
<citation valid="true">
<date>1989</date>
<booktitle>Successive Cyclicity in the Grammar and Parser. Language and Cogni, tive Processes,</booktitle>
<pages>4--2</pages>
<marker>1989</marker>
<rawString>[71 L. Frazier and C. Clifton. Successive Cyclicity in the Grammar and Parser. Language and Cogni, tive Processes, 4(2):93-126, 1989.</rawString>
</citation>
<citation valid="false">
<title>Program Transformation Techniques for Deductive Parsing.</title>
<booktitle>Natural Language Understanding and Logic Programming, III, Elsevier Science Publishers (North-Holland),</booktitle>
<editor>In Brown and Koch, editors,</editor>
<note>(to appear).</note>
<marker></marker>
<rawString>[91 M. Johnson. Program Transformation Techniques for Deductive Parsing. In Brown and Koch, editors, Natural Language Understanding and Logic Programming, III, Elsevier Science Publishers (North-Holland), (to appear).</rawString>
</citation>
<citation valid="true">
<title>Use of Knowledge of Language.</title>
<date>1989</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>18</volume>
<issue>1</issue>
<marker>1989</marker>
<rawString>[91 M. Johnson. Use of Knowledge of Language. Journal of Psycholinguistic Research, 18(1), 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>Parsing as Deduction.</title>
<date>1983</date>
<booktitle>In Proceedings of Twenty-First Conference of the ACL,</booktitle>
<location>Cambridge, Massachusetts,</location>
<marker>Pereira, Warren, 1983</marker>
<rawString>[10] F. Pereira and D. Warren. Parsing as Deduction. In Proceedings of Twenty-First Conference of the ACL, Cambridge, Massachusetts, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>S Shieber</author>
</authors>
<title>Prolog and NaturalLanguage Analysis.</title>
<date>1987</date>
<booktitle>CSLI Lecture Notes, Center for the Study of Language and Information,</booktitle>
<location>Stanford, California,</location>
<marker>Pereira, Shieber, 1987</marker>
<rawString>[11] F. Pereira and S. Shieber. Prolog and NaturalLanguage Analysis. CSLI Lecture Notes, Center for the Study of Language and Information, Stanford, California, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stabler</author>
</authors>
<title>Avoid the pedestrian&apos;s paradox. unpublished ms., Dept. of Linguistics,</title>
<date>1989</date>
<location>UCLA,</location>
<marker>Stabler, 1989</marker>
<rawString>[12) E. Stabler. Avoid the pedestrian&apos;s paradox. unpublished ms., Dept. of Linguistics, UCLA, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stabler</author>
</authors>
<title>Relaxation Techniques for PrincipleBased Parsing. Presented at</title>
<date>1990</date>
<booktitle>The Workshop on GB Parsing,</booktitle>
<location>Geneva, Switzerland,</location>
<marker>Stabler, 1990</marker>
<rawString>[13] E. Stabler. Relaxation Techniques for PrincipleBased Parsing. Presented at The Workshop on GB Parsing, Geneva, Switzerland, 1990.</rawString>
</citation>
<citation valid="false">
<title>The Logical Approach to Syntax.</title>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts, (forthcoming).</location>
<marker></marker>
<rawString>[141 E. Stabler. The Logical Approach to Syntax. The MIT Press, Cambridge, Massachusetts, (forthcoming).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>