<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006401">
<title confidence="0.9888295">
A Topic Similarity Model
for Hierarchical Phrase-based Translation
</title>
<author confidence="0.982018">
Xinyan Xiaot Deyi Xiongt Min Zhangt* Qun Liut Shouxun Lint
</author>
<affiliation confidence="0.985042333333333">
tKey Lab. of Intelligent Info. Processing tHuman Language Technology
Institute of Computing Technology Institute for Infocomm Research
Chinese Academy of Sciences
</affiliation>
<email confidence="0.974883">
{xiaoxinyan, liuqun, sxlin}@ict.ac.cn {dyxiong, mzhang∗}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999602555555556">
Previous work using topic model for statis-
tical machine translation (SMT) explore top-
ic information at the word level. Howev-
er, SMT has been advanced from word-based
paradigm to phrase/rule-based paradigm. We
therefore propose a topic similarity model to
exploit topic information at the synchronous
rule level for hierarchical phrase-based trans-
lation. We associate each synchronous rule
with a topic distribution, and select desirable
rules according to the similarity of their top-
ic distributions with given documents. We
show that our model significantly improves
the translation performance over the baseline
on NIST Chinese-to-English translation ex-
periments. Our model also achieves a better
performance and a faster speed than previous
approaches that work at the word level.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97972744">
Topic model (Hofmann, 1999; Blei et al., 2003) is
a popular technique for discovering the underlying
topic structure of documents. To exploit topic infor-
mation for statistical machine translation (SMT), re-
searchers have proposed various topic-specific lexi-
con translation models (Zhao and Xing, 2006; Zhao
and Xing, 2007; Tam et al., 2007) to improve trans-
lation quality.
Topic-specific lexicon translation models focus
on word-level translations. Such models first esti-
mate word translation probabilities conditioned on
topics, and then adapt lexical weights of phrases
∗Corresponding author
by these probabilities. However, the state-of-the-
art SMT systems translate sentences by using se-
quences of synchronous rules or phrases, instead of
translating word by word. Since a synchronous rule
is rarely factorized into individual words, we believe
that it is more reasonable to incorporate the topic
model directly at the rule level rather than the word
level.
Consequently, we propose a topic similari-
ty model for hierarchical phrase-based translation
(Chiang, 2007), where each synchronous rule is as-
sociated with a topic distribution. In particular,
</bodyText>
<listItem confidence="0.97371685">
• Given a document to be translated, we cal-
culate the topic similarity between a rule and
the document based on their topic distributions.
We augment the hierarchical phrase-based sys-
tem by integrating the proposed topic similarity
model as a new feature (Section 3.1).
• As we will discuss in Section 3.2, the similarity
between a generic rule and a given source docu-
ment computed by our topic similarity model is
often very low. We don’t want to penalize these
generic rules. Therefore we further propose a
topic sensitivity model which rewards generic
rules so as to complement the topic similarity
model.
• We estimate the topic distribution for a rule
based on both the source and target side topic
models (Section 4.1). In order to calculate sim-
ilarities between target-side topic distributions
of rules and source-side topic distributions of
given documents during decoding, we project
</listItem>
<page confidence="0.960802">
750
</page>
<note confidence="0.986582">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750–758,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.999333153846154">
0
0.6
0.4
0.2
0
1 5 10 15 20 25 30
(a) f&apos;� 4 R )7 =:�- opera-
tional capability
0.6
0.4
0.2
0
1 5 10 15 20 25 30
(b) !6&apos;-T X1 =:�- grands X1
0.6
0.4
0.2
1 5 10 15 20 2530
(c) �-T X1 � give X1
0.6
0.4
0.2
0
1 5 10 15 20 25 30
(d) X1 *fff 12-6-A X2 �
held talks X1 X2
</figure>
<figureCaption confidence="0.999563">
Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution,
</figureCaption>
<bodyText confidence="0.965535">
where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares
the same source Chinese string, but they have different topic distributions due to the different English translations.
the target-side topic distributions of rules into
the space of source-side topic model by one-to-
many projection (Section 4.2).
Experiments on Chinese-English translation tasks
(Section 6) show that, our method outperforms the
baseline hierarchial phrase-based system by +0.9
BLEU points. This result is also +0.5 points high-
er and 3 times faster than the previous topic-specific
lexicon translation method. We further show that
both the source-side and target-side topic distribu-
tions improve translation quality and their improve-
ments are complementary to each other.
</bodyText>
<sectionHeader confidence="0.974915" genericHeader="introduction">
2 Background: Topic Model
</sectionHeader>
<bodyText confidence="0.999965785714286">
A topic model is used for discovering the topics
that occur in a collection of documents. Both La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
and Probabilistic Latent Semantic Analysis (PLSA)
(Hofmann, 1999) are types of topic models. LDA
is the most common topic model currently in use,
therefore we exploit it for mining topics in this pa-
per. Here, we first give a brief description of LDA.
LDA views each document as a mixture pro-
portion of various topics, and generates each word
by multinomial distribution conditioned on a topic.
More specifically, as a generative process, LDA first
samples a document-topic distribution for each doc-
ument. Then, for each word in the document, it sam-
ples a topic index from the document-topic distribu-
tion and samples the word conditioned on the topic
index according the topic-word distribution.
Generally speaking, LDA contains two types of
parameters. The first one relates to the document-
topic distribution, which records the topic distribu-
tion of each document. The second one is used for
topic-word distribution, which represents each topic
as a distribution over words. Based on these param-
eters (and some hyper-parameters), LDA can infer a
topic assignment for each word in the documents. In
the following sections, we will use these parameters
and the topic assignments of words to estimate the
parameters in our method.
</bodyText>
<sectionHeader confidence="0.997783" genericHeader="method">
3 Topic Similarity Model
</sectionHeader>
<bodyText confidence="0.999169107142857">
Sentences should be translated in consistence with
their topics (Zhao and Xing, 2006; Zhao and Xing,
2007; Tam et al., 2007). In the hierarchical phrase
based system, a synchronous rule may be related to
some topics and unrelated to others. In terms of
probability, a rule often has an uneven probability
distribution over topics. The probability over a topic
is high if the rule is highly related to the topic, other-
wise the probability will be low. Therefore, we use
topic distribution to describe the relatedness of rules
to topics.
Figure 1 shows four synchronous rules (Chiang,
2007) with topic distributions, some of which con-
tain nonterminals. We can see that, although the
source part of rule (b) and (c) are identical, their top-
ic distributions are quite different. Rule (b) contains
a highest probability on the topic about “China-U.S.
relationship”, which means rule (b) is much more
related to this topic. In contrast, rule (c) contains
an even distribution over various topics. Thus, giv-
en a document about “China-U.S. relationship”, we
hope to encourage the system to apply rule (b) but
penalize the application of rule (c). We achieve this
by calculating similarity between the topic distribu-
tions of a rule and a document to be translated.
More formally, we associate each rule with a rule-
topic distribution P(zJr), where r is a rule, and z is
a topic. Suppose there are K topics, this distribution
</bodyText>
<page confidence="0.996644">
751
</page>
<bodyText confidence="0.9997569">
can be represented by a K-dimension vector. The
k-th component P(z = k|r) means the probability
of topic k given the rule r. The estimation of such
distribution will be described in Section 4.
Analogously, we represent the topic information
of a document d to be translated by a document-
topic distribution P(z|d), which is also a K-
dimension vector. The k-th dimension P(z = k|d)
means the probability of topic k given document d.
Different from rule-topic distribution, the document-
topic distribution can be directly inferred by an off-
the-shelf LDA tool.
Consequently, based on these two distribution-
s, we select a rule for a document to be translat-
ed according to their topic similarity (Section 3.1),
which measures the relatedness of the rule to the
document. In order to encourage the application
of generic rules which are often penalized by our
similarity model, we also propose a topic sensitivity
model (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999278">
3.1 Topic Similarity
</subsectionHeader>
<bodyText confidence="0.9999604">
By comparing the similarity of their topic distribu-
tions, we are able to decide whether a rule is suitable
for a given source document. The topic similarity
computes the distance of two topic distributions. We
calculate the topic similarity by Hellinger function:
</bodyText>
<equation confidence="0.979616666666667">
Similarity(P(z|d), P(z|r))
(\I \I )2
P(z = k|d) − P(z = k|r) (1)
</equation>
<bodyText confidence="0.999882571428571">
Hellinger function is used to calculate distribution
distance and is popular in topic model (Blei and Laf-
ferty, 2007).1 By topic similarity, we aim to encour-
age or penalize the application of a rule for a giv-
en document according to their topic distributions,
which then helps the SMT system make better trans-
lation decisions.
</bodyText>
<subsectionHeader confidence="0.999443">
3.2 Topic Sensitivity
</subsectionHeader>
<bodyText confidence="0.999903114285714">
Domain adaptation (Wu et al., 2008; Bertoldi and
Federico, 2009) often distinguishes general-domain
data from in-domain data. Similarly, we divide the
rules into topic-insensitive rules and topic-sensitive
&apos;We also try other distance functions, including Euclidean
distance, Kullback-Leibler divergence and cosine function.
They produce similar results in our preliminary experiments.
rules according to their topic distributions. Let’s
revisit Figure 1. We can easily find that the topic
distribution of rule (c) distribute evenly. This in-
dicates that it is insensitive to topics, and can be
applied in any topics. We call such a rule a topic-
insensitive rule. In contrast, the distributions of the
rest rules peak on a few topics. Such rules are called
topic-sensitive rules. Generally speaking, a topic-
insensitive rule has a fairly flat distribution, while a
topic-sensitive rule has a sharp distribution.
A document typically focuses on a few topics, and
has a sharp topic distribution. In contrast, the distri-
bution of topic-insensitive rule is fairly flat. Hence,
a topic-insensitive rule is always less similar to doc-
uments and is punished by the similarity function.
However, topic-insensitive rules may be more
preferable than topic-sensitive rules if neither of
them are similar to given documents. For a doc-
ument about the “military” topic, the rule (b) and
(c) in Figure 1 are both dissimilar to the document,
because rule (b) relates to the “China-U.S. relation-
ship” topic and rule (c) is topic-insensitive. Never-
theless, since rule (c) occurs more frequently across
various topics, it may be better to apply rule (c).
To address such issue of the topic similarity mod-
el, we further introduce a topic sensitivity model to
describe the topic sensitivity of a rule using entropy
as a metric:
</bodyText>
<equation confidence="0.99232825">
Sensitivity(P(z|r))
K
= E P(z = k|r) x log (P(z = k|r)) (2)
k=1
</equation>
<bodyText confidence="0.9999315">
According to the Eq. (2), a topic-insensitive rule has
a large entropy, while a topic-sensitive rule has a s-
maller entropy. By incorporating the topic sensitivi-
ty model with the topic similarity model, we enable
our SMT system to balance the selection of these t-
wo types of rules. Given rules with approximately
equal values of Eq. (1), we prefer topic-insensitive
rules.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="method">
4 Estimation
</sectionHeader>
<bodyText confidence="0.99988">
Unlike document-topic distribution that can be di-
rectly learned by LDA tools, we need to estimate the
rule-topic distribution according to our requirement.
In this paper, we try to exploit the topic information
</bodyText>
<equation confidence="0.882493">
=
K
E
k=1
</equation>
<page confidence="0.979782">
752
</page>
<bodyText confidence="0.999953285714286">
of both source and target language. To achieve this
goal, we use both source-side and target-side mono-
lingual topic models, and learn the correspondence
between the two topic models from word-aligned
bilingual corpus.
Specifically, we use two types of rule-topic dis-
tributions: one is source-side rule-topic distribution
and the other is target-side rule-topic distribution.
These two rule-topic distributions are estimated by
corresponding topic models in the same way (Sec-
tion 4.1). Notably, only source language documents
are available during decoding. In order to compute
the similarity between the target-side topic distribu-
tion of a rule and the source-side topic distribution
of a given document,we need to project the target-
side topic distribution of a synchronous rule into the
space of the source-side topic model (Section 4.2).
A more principle way is to learn a bilingual topic
model from bilingual corpus (Mimno et al., 2009).
However, we may face difficulty during decoding,
where only source language documents are avail-
able. It requires a marginalization to infer the mono-
lingual topic distribution using the bilingual topic
model. The high complexity of marginalization pro-
hibits such a summation in practice. Previous work
on bilingual topic model avoid this problem by some
monolingual assumptions. Zhao and Xing (2007)
assume that the topic model is generated in a mono-
lingual manner, while Tam et al., (2007) construct
their bilingual topic model by enforcing a one-to-
one correspondence between two monolingual topic
models. We also estimate our rule-topic distribution
by two monolingual topic models, but use a differ-
ent way to project target-side topics onto source-side
topics.
</bodyText>
<subsectionHeader confidence="0.994933">
4.1 Monolingual Topic Distribution Estimation
</subsectionHeader>
<bodyText confidence="0.999901954545455">
We estimate rule-topic distribution from word-
aligned bilingual training corpus with documen-
t boundaries explicitly given. The source and tar-
get side distributions are estimated in the same way.
For simplicity, we only describe the estimation of
source-side distribution in this section.
The process of rule-topic distribution estimation
is analogous to the traditional estimation of rule
translation probability (Chiang, 2007). In addition
to the word-aligned corpus, the input for estimation
also contains the source-side topic-document distri-
bution of every documents inferred by LDA tool.
We first extract synchronous rules from training
data in a traditional way. When a rule r is extracted
from a document d with topic distribution P(z|d),
we collect an instance (r, P(z|d), c), where c is the
fraction count of an instance as described in Chiang,
(2007). After extraction, we get a set of instances
Z = {(r, P(z|d), c)} with different document-topic
distributions for each rule. Using these instances,
we calculate the topic probability P(z = k|r) as
follows:
</bodyText>
<equation confidence="0.9991665">
�ICI c x P(z = k|d)
P(z = k|r) = (3)
EK
k′=1 EICI c x P(z = k′ |d)
</equation>
<bodyText confidence="0.998175">
By using both source-side and target-side
document-topic distribution, we obtain two rule-
topic distributions for each rule in total.
</bodyText>
<subsectionHeader confidence="0.983491">
4.2 Target-side Topic Distribution Projection
</subsectionHeader>
<bodyText confidence="0.999919111111111">
As described in the previous section, we also esti-
mate the target-side rule-topic distribution. How-
ever, only source document-topic distributions are
available during decoding. In order to calculate
the similarity between the target-side rule-topic dis-
tribution of a rule and the source-side document-
topic distribution of a source document, we need to
project target-side topics into the source-side topic
space. The projection contains two steps:
</bodyText>
<listItem confidence="0.938978333333333">
• In the first step, we learn the topic-to-topic cor-
respondence probability p(zf|ze) from target-
side topic ze to source-side topic zf.
• In the second step, we project the target-side
topic distribution of a rule into source-side top-
ic space using the correspondence probability.
</listItem>
<bodyText confidence="0.998808636363636">
In the first step, we estimate the correspondence
probability by the co-occurrence of the source-side
and the target-side topic assignment of the word-
aligned corpus. The topic assignments are output
by LDA tool. Thus, we denotes each sentence pair
by (zf, ze, a), where zf and ze are the topic as-
signments of source-side and target-side sentences
respectively, and a is a set of links {(i, j)}. A
link (i, j) means a source-side position i aligns to
a target-side position j. Thus, the co-occurrence of
a source-side topic with index kf and a target-side
</bodyText>
<page confidence="0.991315">
753
</page>
<table confidence="0.999648166666667">
e-topic f-topic 1 f-topic 2 f-topic 3
enterprises V&amp;quot;(agricultural) Ank(enterprise) A)W(develop)
rural V, (rural) **(market) Og(economic)
state Vfteasant) PAM(state) **(technology )
agricultural �M(reform) a](company) ;RPA(China)
market MOA(finance) $M(finance) R*(technique)
reform 4±(social) IRV(bank) !`*(industry)
production 9 (safety) &amp;*(investment) MiQ(structure)
peasants it (adjust) &apos;PA(manage) INNi(innovation)
owned OAA(policy) �M(reform) N&amp;(accelerate)
enterprise qC/\(income) Off(operation) �M(reform)
p(zf|ze) 0.38 0.28 0.16
</table>
<tableCaption confidence="0.976382">
Table 1: Example of topic-to-topic correspondence. The
last line shows the correspondence probability. Each col-
umn means atopic represented by its top-10 topical word-
s. The first column is a target-side topic, while the rest
three columns are source-side topics.
</tableCaption>
<equation confidence="0.8675905">
topic ke is calculated by:
J(zfi, kf) * J(ze;, ke) (4)
</equation>
<bodyText confidence="0.998811636363636">
where J(x, y) is the Kronecker function, which is 1
if x = y and 0 otherwise. We then compute the
probability of P(z = kf|z = ke) by normalizing
the co-occurrence count. Overall, after the first step,
we obtain an correspondence matrix MKe×Kf from
target-side topic to source-side topic, where the item
Mi,j represents the probability P(zf = i|ze = j).
In the second step, given the correspondence ma-
trix MKe×Kf, we project the target-side rule-topic
distribution P(ze|r) to the source-side topic space
by multiplication as follows:
</bodyText>
<equation confidence="0.998104">
T (P(ze|r)) = P(ze|r) ® MKe×Kf (5)
</equation>
<bodyText confidence="0.999955266666667">
In this way, we get a second distribution for a rule
in the source-side topic space, which we called pro-
jected target-side topic distribution T(P(ze|r)).
Obviously, our projection method allows one
target-side topic to align to multiple source-side top-
ics. This is different from the one-to-one correspon-
dence used by Tam et al., (2007). From the training
result of the correspondence matrix MKe×Kf, we
find that the topic correspondence between source
and target language is not necessarily one-to-one.
Typically, the probability P(z = kf|z = ke) of a
target-side topic mainly distributes on two or three
source-side topics. Table 1 shows an example of
a target-side topic with its three mainly aligned
source-side topics.
</bodyText>
<sectionHeader confidence="0.991684" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.998060428571429">
We incorporate our topic similarity model as a
new feature into a traditional hiero system (Chi-
ang, 2007) under discriminative framework (Och
and Ney, 2002). Considering there are a source-
side rule-topic distribution and a projected target-
side rule-topic distribution, we add four features in
total:
</bodyText>
<listItem confidence="0.9998615">
• Similarity (P(zf|d), P(zf|r))
• Similarity(P(zf|d),T(P(ze|r)))
• Sensitivity(P (zf|r))
• Sensitivity(T(P(ze|r))
</listItem>
<bodyText confidence="0.999979736842105">
To calculate the total score of a derivation on each
feature listed above during decoding, we sum up the
correspondent feature score of each applied rule.2
The source-side and projected target-side rule-
topic distribution are calculated before decoding.
During decoding, we first infer the topic distribution
P(zf|d) for a given document on source language.
When applying a rule, it is straightforward to calcu-
late these topic features. Obviously, the computa-
tional cost of these features is rather small.
In the topic-specific lexicon translation model,
given a source document, it first calculates the topic-
specific translation probability by normalizing the
entire lexicon translation table, and then adapts the
lexical weights of rules correspondingly. This makes
the decoding slower. Therefore, comparing with the
previous topic-specific lexicon translation method,
our method provides a more efficient way for incor-
porating topic model into SMT.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.995444166666667">
We try to answer the following questions by experi-
ments:
1. Is our topic similarity model able to improve
translation quality in terms of BLEU? Further-
more, are source-side and target-side rule-topic
distributions complementary to each other?
</bodyText>
<footnote confidence="0.574836333333333">
2Since glue rule and rules of unknown words are not extract-
ed from training data, here, we just ignore the calculation of the
four features for them.
</footnote>
<figure confidence="0.7674635">
E E
(zf,ze,a) (i,j)Ea
</figure>
<page confidence="0.965275">
754
</page>
<table confidence="0.999781857142857">
System MT06 MT08 Avg Speed
Baseline 30.20 21.93 26.07 12.6
TopicLex 30.65 22.29 26.47 3.3
SimSrc 30.41 22.69 26.55 11.5
SimTgt 30.51 22.39 26.45 11.7
SimSrc+SimTgt 30.73 22.69 26.71 11.2
Sim+Sen 30.95 22.92 26.94 10.2
</table>
<tableCaption confidence="0.6844194">
Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the
traditional hierarchical system (“Baseline”) and the topic-specific lexicon translation method (“TopicLex”). “SimSrc”
and “SimTgt” denote similarity by source-side and target-side rule-distribution respectively, while “Sim+Sen” acti-
vates the two similarity and two sensitivity features. “Avg” is the average BLEU score on the two test sets. Scores
marked in bold mean significantly (Koehn, 2004) better than Baseline (p &lt; 0.01).
</tableCaption>
<listItem confidence="0.997643333333333">
2. Is it helpful to introduce the topic sensitivi-
ty model to distinguish topic-insensitive and
topic-sensitive rules?
3. Is it necessary to project topics by one-to-many
correspondence instead of one-to-one corre-
spondence?
4. What is the effect of our method on various
types of rules, such as phrase rules and rules
with non-terminals?
</listItem>
<subsectionHeader confidence="0.981793">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999962789473684">
We present our experiments on the NIST Chinese-
English translation tasks. The bilingual training da-
ta contains 239K sentence pairs with 6.9M Chinese
words and 9.14M English words, which comes from
the FBIS portion of LDC data. There are 10,947
documents in the FBIS corpus. The monolingual da-
ta for training English language model includes the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the NIST
evaluation set of 2005 (MT05) as our development
set, and sets of MT06/MT08 as test sets. The num-
bers of documents in MT05, MT06, MT08 are 100,
79, and 109 respectively.
We obtained symmetric word alignments of train-
ing data by first running GIZA++ (Och and Ney,
2003) in both directions and then applying re-
finement rule “grow-diag-final-and” (Koehn et al.,
2003). The SCFG rules are extracted from this
word-aligned training data. A 4-gram language
model was trained on the monolingual data by the
SRILM toolkit (Stolcke, 2002). Case-insensitive
NIST BLEU (Papineni et al., 2002) was used to mea-
sure translation performance. We used minimum er-
ror rate training (Och, 2003) for optimizing the fea-
ture weights.
For the topic model, we used the open source L-
DA tool GibbsLDA++ for estimation and inference.3
GibssLDA++ is an implementation of LDA using
gibbs sampling for parameter estimation and infer-
ence. The source-side and target-side topic models
are estimated from the Chinese part and English part
of FBIS corpus respectively. We set the number of
topic K = 30 for both source-side and target-side,
and use the default setting of the tool for training and
inference.4 During decoding, we first infer the top-
ic distribution of given documents before translation
according to the topic model trained on Chinese part
of FBIS corpus.
</bodyText>
<subsectionHeader confidence="0.999887">
6.2 Effect of Topic Similarity Model
</subsectionHeader>
<bodyText confidence="0.9994906">
We compare our method with two baselines. In addi-
tion to the traditional hiero system, we also compare
with the topic-specific lexicon translation method in
Zhao and Xing (2007). The lexicon translation prob-
ability is adapted by:
</bodyText>
<equation confidence="0.995733">
p(f|e, DF) ∝ p(e|f, DF)P(f|DF) (6)
�= p(e|f, z = k)p(f|z = k)p(z = k|DF) (7)
k
</equation>
<bodyText confidence="0.9763425">
However, we simplify the estimation of p(e|f, z =
k) by directly using the word alignment corpus with
</bodyText>
<footnote confidence="0.99831375">
3http://gibbslda.sourceforge.net/
4We determine K by testing {15, 30, 50, 100, 200} in our
preliminary experiments. We find that K = 30 produces a s-
lightly better performance than other values.
</footnote>
<page confidence="0.990825">
755
</page>
<table confidence="0.999928">
Type Count Src% Tgt%
Phrase-rule 3.9M 83.4 84.4
Monotone-rule 19.2M 85.3 86.1
Reordering-rule 5.7M 85.9 86.8
All-rule 28.8M 85.1 86.0
</table>
<tableCaption confidence="0.9415154">
Table 3: Percentage of topic-sensitive rules of various
types of rule according to source-side (“Src”) and target-
side (“Tgt”) topic distributions. Phrase rules are fully
lexicalized, while monotone and reordering rules contain
nonterminals (Section 6.5).
</tableCaption>
<bodyText confidence="0.991726222222222">
topic assignment that is inferred by the GibbsL-
DA++. Despite the simplification of estimation, the
improvement of our implementation is comparable
with the improvement in Zhao et al.,(2007). Given a
new document, we need to adapt the lexical transla-
tion weights of the rules based on topic model. The
adapted lexicon translation model is added as a new
feature under the discriminative framework.
Table 2 shows the result of our method compar-
ing with the traditional system and the topic-lexicon
specific translation method described as above. By
using all the features (last line in the table), we im-
prove the translation performance over the baseline
system by 0.87 BLEU point on average. Our method
also outperforms the topic-lexicon specific transla-
tion method by 0.47 points. This verifies that topic
similarity model can improve the translation quality
significantly.
In order to gain insights into why our model is
helpful, we further investigate how many rules are
topic-sensitive. As described in Section 3.2, we use
entropy to measure the topic sensitivity. If the en-
tropy of a rule is smaller than a certain threshold,
then the rule is topic sensitive. Since documents of-
ten focus on some topics, we use the average entropy
of document-topic distribution of all training docu-
ments as the threshold. We compare both source-
side and target-side distribution shown in Table 3.
We find that more than 80 percents of the rules are
topic-sensitive, thus provides us a large space to im-
prove the translation by exploiting topics.
We also compare these methods in terms of the
decoding speed (words/second). The baseline trans-
lates 12.6 words per second, while the topic-specific
lexicon translation method only translates 3.3 word-
s in one second. The overhead of the topic-specific
</bodyText>
<table confidence="0.9992955">
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
One-to-One 30.27 22.12 26.20
One-to-Many 30.51 22.39 26.45
</table>
<tableCaption confidence="0.986666">
Table 4: Effects of one-to-one and one-to-many topic pro-
jection.
</tableCaption>
<bodyText confidence="0.999887388888889">
lexicon translation method mainly comes from the
adaptation of lexical weights. It takes 72.8% of
the time to do the adaptation, despite only lexical
weights of the used rules are adapted. In contrast,
our method has a speed of 10.2 words per second for
each sentence on average, which is three times faster
than the topic-specific lexicon translation method.
Meanwhile, we try to separate the effects of
source-side topic distribution from the target-side
topic distribution. From lines 4-6 of Table 2. We
clearly find that the two rule-topic distributions im-
prove the performance by 0.48 and 0.38 BLEU
points over the baseline respectively. It seems that
the source-side topic model is more helpful. Fur-
thermore, when combine these two distributions, the
improvement is increased to 0.64 points. This indi-
cates that the effects of source-side and target-side
distributions are complementary.
</bodyText>
<subsectionHeader confidence="0.999349">
6.3 Effect of Topic Sensitivity Model
</subsectionHeader>
<bodyText confidence="0.99998475">
As described in Section 3.2, because the similari-
ty features always punish topic-insensitive rules, we
introduce topic sensitivity features as a complemen-
t. In the last line of Table 2, we obtain a fur-
ther improvement of 0.23 points, when incorporat-
ing topic sensitivity features with topic similarity
features. This suggests that it is necessary to dis-
tinguish topic-insensitive and topic-sensitive rules.
</bodyText>
<subsectionHeader confidence="0.778165">
6.4 One-to-One Vs. One-to-Many Topic
Projection
</subsectionHeader>
<bodyText confidence="0.991252222222222">
In Section 4.2, we find that source-side topic and
target-side topics may not exactly match, hence we
use one-to-many topic correspondence. Yet anoth-
er method is to enforce one-to-one topic projection
(Tam et al., 2007). We achieve one-to-one projection
by aligning a target topic to the source topic with the
largest correspondence probability as calculated in
Section 4.2.
Table 4 compares the effects of these two method-
</bodyText>
<page confidence="0.991309">
756
</page>
<table confidence="0.999759">
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
Phrase-rule 30.53 22.29 26.41
Monotone-rule 30.72 22.62 26.67
Reordering-rule 30.31 22.40 26.36
All-rule 30.95 22.92 26.94
</table>
<tableCaption confidence="0.976998">
Table 5: Effect of our topic model on three types of rules.
Phrase rules are fully lexicalized, while monotone and
reordering rules contain nonterminals.
</tableCaption>
<bodyText confidence="0.998554666666667">
s. We find that the enforced one-to-one topic method
obtains a slight improvement over the baseline sys-
tem, while one-to-many projection achieves a larger
improvement. This confirms our observation of the
non-one-to-one mapping between source-side and
target-side topics.
</bodyText>
<subsectionHeader confidence="0.989017">
6.5 Effect on Various Types of Rules
</subsectionHeader>
<bodyText confidence="0.999990052631579">
To get a more detailed analysis of the result, we
further compare the effect of our method on differ-
ent types of rules. We divide the rules into three
types: phrase rules, which only contain terminal-
s and are the same as the phrase pairs in phrase-
based system; monotone rules, which contain non-
terminals and produce monotone translations; re-
ordering rules, which also contain non-terminals but
change the order of translations. We define the
monotone and reordering rules according to Chiang
et al., (2008).
Table 5 show the results. We can see that our
method achieves improvements on all the three type-
s of rules. Our topic similarity method on mono-
tone rule achieves the most improvement which is
0.6 BLEU points, while the improvement on reorder-
ing rules is the smallest among the three types. This
shows that topic information also helps the selec-
tions of rules with non-terminals.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999973757575757">
In addition to the topic-specific lexicon transla-
tion method mentioned in the previous sections,
researchers also explore topic model for machine
translation in other ways.
Foster and Kunh (2007) describe a mixture-model
approach for SMT adaptation. They first split a
training corpus into different domains. Then, they
train separate models on each domain. Finally, they
combine a specific domain translation model with a
general domain translation model depending on var-
ious text distances. One way to calculate the dis-
tance is using topic model.
Gong et al. (2010) introduce topic model for fil-
tering topic-mismatched phrase pairs. They first as-
sign a specific topic for the document to be translat-
ed. Similarly, each phrase pair is also assigned with
one specific topic. A phrase pair will be discarded if
its topic mismatches the document topic.
Researchers also introduce topic model for cross-
lingual language model adaptation (Tam et al., 2007;
Ruiz and Federico, 2011). They use bilingual topic
model to project latent topic distribution across lan-
guages. Based on the bilingual topic model, they ap-
ply the source-side topic weights into the target-side
topic model, and adapt the n-gram language model
of target side.
Our topic similarity model uses the document top-
ic information. From this point, our work is related
to context-dependent translation (Carpuat and Wu,
2007; He et al., 2008; Shen et al., 2009). Previous
work typically use neighboring words and sentence
level information, while our work extents the con-
text into the document level.
</bodyText>
<sectionHeader confidence="0.982384" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999810894736842">
We have presented a topic similarity model which
incorporates the rule-topic distributions on both the
source and target side into traditional hierarchical
phrase-based system. Our experimental results show
that our model achieves a better performance with
faster decoding speed than previous work on topic-
specific lexicon translation. This verifies the advan-
tage of exploiting topic model at the rule level over
the word level. Further improvement is achieved by
distinguishing topic-sensitive and topic-insensitive
rules using the topic sensitivity model.
In the future, we are interesting to find ways to
exploit topic model on bilingual data without docu-
ment boundaries, thus to enlarge the size of training
data. Furthermore, our training corpus mainly focus
on news, it is also interesting to apply our method on
corpus with more diverse topics. Finally, we hope to
apply our method to other translation models, espe-
cially syntax-based models.
</bodyText>
<page confidence="0.995803">
757
</page>
<sectionHeader confidence="0.986924" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999902">
The authors were supported by High-Technology
R&amp;D Program (863) Project No 2011AA01A207
and 2012BAH39B03. This work was done dur-
ing Xinyan Xiao’s internship at I2R. We would like
to thank Yun Huang, Zhengxian Gong, Wenliang
Chen, Jun lang, Xiangyu Duan, Jun Sun, Jinsong
Su and the anonymous reviewers for their insightful
comments.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883887323943">
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proc of WMT 2009.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. AAS, 1(1):17–35.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. JMLR, 3:993–1022.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. In Proceedings of the MT Sum-
mit XI.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128–
135, Prague, Czech Republic, June.
Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010.
Statistical machine translation based on lda. In Proc.
IUCS 2010, page 286 –290, Oct.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proc. EMNLP 2008.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of UAI 1999, pages 289–296.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proc. of EMNLP 2009.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Nick Ruiz and Marcello Federico. 2011. Topic adapta-
tion for lecture translation through bilingual latent se-
mantic models. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, July.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proc. ICSLP 2002.
Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical machine
translation. Machine Translation, 21(4):187–207.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proc. Coling 2008.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Proc.
ACL 2006.
Bin Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation. In
Proc. NIPS 2007.
</reference>
<page confidence="0.997123">
758
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.531691">
<title confidence="0.9987075">A Topic Similarity Model for Hierarchical Phrase-based Translation</title>
<author confidence="0.623932">Qun</author>
<affiliation confidence="0.962179333333333">Lab. of Intelligent Info. Processing Language Technology Institute of Computing Technology Institute for Infocomm Research Chinese Academy of Sciences</affiliation>
<email confidence="0.911565">liuqun,</email>
<abstract confidence="0.999492842105263">Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proc of WMT</booktitle>
<contexts>
<context position="9140" citStr="Bertoldi and Federico, 2009" startWordPosition="1464" endWordPosition="1467">iven source document. The topic similarity computes the distance of two topic distributions. We calculate the topic similarity by Hellinger function: Similarity(P(z|d), P(z|r)) (\I \I )2 P(z = k|d) − P(z = k|r) (1) Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions. 3.2 Topic Sensitivity Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data. Similarly, we divide the rules into topic-insensitive rules and topic-sensitive &apos;We also try other distance functions, including Euclidean distance, Kullback-Leibler divergence and cosine function. They produce similar results in our preliminary experiments. rules according to their topic distributions. Let’s revisit Figure 1. We can easily find that the topic distribution of rule (c) distribute evenly. This indicates that it is insensitive to topics, and can be applied in any topics. We call such a rule a topicinsensitive rule. In </context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proc of WMT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>A correlated topic model of science.</title>
<date>2007</date>
<journal>AAS,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="8844" citStr="Blei and Lafferty, 2007" startWordPosition="1414" endWordPosition="1418">er to encourage the application of generic rules which are often penalized by our similarity model, we also propose a topic sensitivity model (Section 3.2). 3.1 Topic Similarity By comparing the similarity of their topic distributions, we are able to decide whether a rule is suitable for a given source document. The topic similarity computes the distance of two topic distributions. We calculate the topic similarity by Hellinger function: Similarity(P(z|d), P(z|r)) (\I \I )2 P(z = k|d) − P(z = k|r) (1) Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions. 3.2 Topic Sensitivity Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data. Similarly, we divide the rules into topic-insensitive rules and topic-sensitive &apos;We also try other distance functions, including Euclidean distance, Kullback-Leibler divergence and cosine function. They produce similar results in our prelimin</context>
</contexts>
<marker>Blei, Lafferty, 2007</marker>
<rawString>David M. Blei and John D. Lafferty. 2007. A correlated topic model of science. AAS, 1(1):17–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="1218" citStr="Blei et al., 2003" startWordPosition="167" endWordPosition="170">a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level. 1 Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗Corresponding author by these probabilities. However, the state-of-theart </context>
<context position="4778" citStr="Blei et al., 2003" startWordPosition="744" endWordPosition="747">. Experiments on Chinese-English translation tasks (Section 6) show that, our method outperforms the baseline hierarchial phrase-based system by +0.9 BLEU points. This result is also +0.5 points higher and 3 times faster than the previous topic-specific lexicon translation method. We further show that both the source-side and target-side topic distributions improve translation quality and their improvements are complementary to each other. 2 Background: Topic Model A topic model is used for discovering the topics that occur in a collection of documents. Both Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) are types of topic models. LDA is the most common topic model currently in use, therefore we exploit it for mining topics in this paper. Here, we first give a brief description of LDA. LDA views each document as a mixture proportion of various topics, and generates each word by multinomial distribution conditioned on a topic. More specifically, as a generative process, LDA first samples a document-topic distribution for each document. Then, for each word in the document, it samples a topic index from the document-topic distribu</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Contextdependent phrasal translation lexicons for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="30456" citStr="Carpuat and Wu, 2007" startWordPosition="4803" endWordPosition="4806">cific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at t</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Contextdependent phrasal translation lexicons for statistical machine translation. In Proceedings of the MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="28678" citStr="Chiang et al., (2008)" startWordPosition="4516" endWordPosition="4519">ion of the non-one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 BLEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describe</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2238" citStr="Chiang, 2007" startWordPosition="318" endWordPosition="319">models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗Corresponding author by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level. Consequently, we propose a topic similarity model for hierarchical phrase-based translation (Chiang, 2007), where each synchronous rule is associated with a topic distribution. In particular, • Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions. We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as a new feature (Section 3.1). • As we will discuss in Section 3.2, the similarity between a generic rule and a given source document computed by our topic similarity model is often very low. We don’t want to penalize these generic rules. Therefore we further propose a topic</context>
<context position="6620" citStr="Chiang, 2007" startWordPosition="1046" endWordPosition="1047">3 Topic Similarity Model Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007). In the hierarchical phrase based system, a synchronous rule may be related to some topics and unrelated to others. In terms of probability, a rule often has an uneven probability distribution over topics. The probability over a topic is high if the rule is highly related to the topic, otherwise the probability will be low. Therefore, we use topic distribution to describe the relatedness of rules to topics. Figure 1 shows four synchronous rules (Chiang, 2007) with topic distributions, some of which contain nonterminals. We can see that, although the source part of rule (b) and (c) are identical, their topic distributions are quite different. Rule (b) contains a highest probability on the topic about “China-U.S. relationship”, which means rule (b) is much more related to this topic. In contrast, rule (c) contains an even distribution over various topics. Thus, given a document about “China-U.S. relationship”, we hope to encourage the system to apply rule (b) but penalize the application of rule (c). We achieve this by calculating similarity between</context>
<context position="13727" citStr="Chiang, 2007" startWordPosition="2181" endWordPosition="2182"> our rule-topic distribution by two monolingual topic models, but use a different way to project target-side topics onto source-side topics. 4.1 Monolingual Topic Distribution Estimation We estimate rule-topic distribution from wordaligned bilingual training corpus with document boundaries explicitly given. The source and target side distributions are estimated in the same way. For simplicity, we only describe the estimation of source-side distribution in this section. The process of rule-topic distribution estimation is analogous to the traditional estimation of rule translation probability (Chiang, 2007). In addition to the word-aligned corpus, the input for estimation also contains the source-side topic-document distribution of every documents inferred by LDA tool. We first extract synchronous rules from training data in a traditional way. When a rule r is extracted from a document d with topic distribution P(z|d), we collect an instance (r, P(z|d), c), where c is the fraction count of an instance as described in Chiang, (2007). After extraction, we get a set of instances Z = {(r, P(z|d), c)} with different document-topic distributions for each rule. Using these instances, we calculate the t</context>
<context position="18163" citStr="Chiang, 2007" startWordPosition="2869" endWordPosition="2871"> multiple source-side topics. This is different from the one-to-one correspondence used by Tam et al., (2007). From the training result of the correspondence matrix MKe×Kf, we find that the topic correspondence between source and target language is not necessarily one-to-one. Typically, the probability P(z = kf|z = ke) of a target-side topic mainly distributes on two or three source-side topics. Table 1 shows an example of a target-side topic with its three mainly aligned source-side topics. 5 Decoding We incorporate our topic similarity model as a new feature into a traditional hiero system (Chiang, 2007) under discriminative framework (Och and Ney, 2002). Considering there are a sourceside rule-topic distribution and a projected targetside rule-topic distribution, we add four features in total: • Similarity (P(zf|d), P(zf|r)) • Similarity(P(zf|d),T(P(ze|r))) • Sensitivity(P (zf|r)) • Sensitivity(T(P(ze|r)) To calculate the total score of a derivation on each feature listed above during decoding, we sum up the correspondent feature score of each applied rule.2 The source-side and projected target-side ruletopic distribution are calculated before decoding. During decoding, we first infer the to</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proc. of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic,</location>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proc. of the Second Workshop on Statistical Machine Translation, pages 128– 135, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Yu Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Statistical machine translation based on lda.</title>
<date>2010</date>
<booktitle>In Proc. IUCS 2010,</booktitle>
<pages>286--290</pages>
<contexts>
<context position="29641" citStr="Gong et al. (2010)" startWordPosition="4673" endWordPosition="4676">ctions of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describe a mixture-model approach for SMT adaptation. They first split a training corpus into different domains. Then, they train separate models on each domain. Finally, they combine a specific domain translation model with a general domain translation model depending on various text distances. One way to calculate the distance is using topic model. Gong et al. (2010) introduce topic model for filtering topic-mismatched phrase pairs. They first assign a specific topic for the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2010</marker>
<rawString>Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010. Statistical machine translation based on lda. In Proc. IUCS 2010, page 286 –290, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="30473" citStr="He et al., 2008" startWordPosition="4807" endWordPosition="4810">pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level ove</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proc. of UAI</booktitle>
<pages>289--296</pages>
<contexts>
<context position="1198" citStr="Hofmann, 1999" startWordPosition="165" endWordPosition="166">refore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level. 1 Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗Corresponding author by these probabilities. However, </context>
<context position="4844" citStr="Hofmann, 1999" startWordPosition="754" endWordPosition="755">at, our method outperforms the baseline hierarchial phrase-based system by +0.9 BLEU points. This result is also +0.5 points higher and 3 times faster than the previous topic-specific lexicon translation method. We further show that both the source-side and target-side topic distributions improve translation quality and their improvements are complementary to each other. 2 Background: Topic Model A topic model is used for discovering the topics that occur in a collection of documents. Both Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) are types of topic models. LDA is the most common topic model currently in use, therefore we exploit it for mining topics in this paper. Here, we first give a brief description of LDA. LDA views each document as a mixture proportion of various topics, and generates each word by multinomial distribution conditioned on a topic. More specifically, as a generative process, LDA first samples a document-topic distribution for each document. Then, for each word in the document, it samples a topic index from the document-topic distribution and samples the word conditioned on the topic index according</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proc. of UAI 1999, pages 289–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<contexts>
<context position="21759" citStr="Koehn et al., 2003" startWordPosition="3419" endWordPosition="3422">which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimate</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="20581" citStr="Koehn, 2004" startWordPosition="3227" endWordPosition="3228">5 11.5 SimTgt 30.51 22.39 26.45 11.7 SimSrc+SimTgt 30.73 22.69 26.71 11.2 Sim+Sen 30.95 22.92 26.94 10.2 Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (“Baseline”) and the topic-specific lexicon translation method (“TopicLex”). “SimSrc” and “SimTgt” denote similarity by source-side and target-side rule-distribution respectively, while “Sim+Sen” activates the two similarity and two sensitivity features. “Avg” is the average BLEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p &lt; 0.01). 2. Is it helpful to introduce the topic sensitivity model to distinguish topic-insensitive and topic-sensitive rules? 3. Is it necessary to project topics by one-to-many correspondence instead of one-to-one correspondence? 4. What is the effect of our method on various types of rules, such as phrase rules and rules with non-terminals? 6.1 Data We present our experiments on the NIST ChineseEnglish translation tasks. The bilingual training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="12490" citStr="Mimno et al., 2009" startWordPosition="1997" endWordPosition="2000">n and the other is target-side rule-topic distribution. These two rule-topic distributions are estimated by corresponding topic models in the same way (Section 4.1). Notably, only source language documents are available during decoding. In order to compute the similarity between the target-side topic distribution of a rule and the source-side topic distribution of a given document,we need to project the targetside topic distribution of a synchronous rule into the space of the source-side topic model (Section 4.2). A more principle way is to learn a bilingual topic model from bilingual corpus (Mimno et al., 2009). However, we may face difficulty during decoding, where only source language documents are available. It requires a marginalization to infer the monolingual topic distribution using the bilingual topic model. The high complexity of marginalization prohibits such a summation in practice. Previous work on bilingual topic model avoid this problem by some monolingual assumptions. Zhao and Xing (2007) assume that the topic model is generated in a monolingual manner, while Tam et al., (2007) construct their bilingual topic model by enforcing a one-toone correspondence between two monolingual topic </context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proc. of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="18214" citStr="Och and Ney, 2002" startWordPosition="2875" endWordPosition="2878">t from the one-to-one correspondence used by Tam et al., (2007). From the training result of the correspondence matrix MKe×Kf, we find that the topic correspondence between source and target language is not necessarily one-to-one. Typically, the probability P(z = kf|z = ke) of a target-side topic mainly distributes on two or three source-side topics. Table 1 shows an example of a target-side topic with its three mainly aligned source-side topics. 5 Decoding We incorporate our topic similarity model as a new feature into a traditional hiero system (Chiang, 2007) under discriminative framework (Och and Ney, 2002). Considering there are a sourceside rule-topic distribution and a projected targetside rule-topic distribution, we add four features in total: • Similarity (P(zf|d), P(zf|r)) • Similarity(P(zf|d),T(P(ze|r))) • Sensitivity(P (zf|r)) • Sensitivity(T(P(ze|r)) To calculate the total score of a derivation on each feature listed above during decoding, we sum up the correspondent feature score of each applied rule.2 The source-side and projected target-side ruletopic distribution are calculated before decoding. During decoding, we first infer the topic distribution P(zf|d) for a given document on so</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21663" citStr="Och and Ney, 2003" startWordPosition="3405" endWordPosition="3408">al training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling f</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="22069" citStr="Och, 2003" startWordPosition="3470" endWordPosition="3471">f MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During decoding, we first infer the topic distribution of given documents before translation accord</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="21976" citStr="Papineni et al., 2002" startWordPosition="3452" endWordPosition="3455">ins 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Topic adaptation for lecture translation through bilingual latent semantic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<contexts>
<context position="30050" citStr="Ruiz and Federico, 2011" startWordPosition="4739" endWordPosition="4742">main. Finally, they combine a specific domain translation model with a general domain translation model depending on various text distances. One way to calculate the distance is using topic model. Gong et al. (2010) introduce topic model for filtering topic-mismatched phrase pairs. They first assign a specific topic for the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and </context>
</contexts>
<marker>Ruiz, Federico, 2011</marker>
<rawString>Nick Ruiz and Marcello Federico. 2011. Topic adaptation for lecture translation through bilingual latent semantic models. In Proceedings of the Sixth Workshop on Statistical Machine Translation, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="30493" citStr="Shen et al., 2009" startWordPosition="4811" endWordPosition="4814">arded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level over the word level. Fu</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proc. EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP</booktitle>
<contexts>
<context position="21924" citStr="Stolcke, 2002" startWordPosition="3447" endWordPosition="3448"> portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default set</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian R Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual lsa-based adaptation for statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1511" citStr="Tam et al., 2007" startWordPosition="211" endWordPosition="214"> We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level. 1 Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗Corresponding author by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather </context>
<context position="6156" citStr="Tam et al., 2007" startWordPosition="967" endWordPosition="970">st one relates to the documenttopic distribution, which records the topic distribution of each document. The second one is used for topic-word distribution, which represents each topic as a distribution over words. Based on these parameters (and some hyper-parameters), LDA can infer a topic assignment for each word in the documents. In the following sections, we will use these parameters and the topic assignments of words to estimate the parameters in our method. 3 Topic Similarity Model Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007). In the hierarchical phrase based system, a synchronous rule may be related to some topics and unrelated to others. In terms of probability, a rule often has an uneven probability distribution over topics. The probability over a topic is high if the rule is highly related to the topic, otherwise the probability will be low. Therefore, we use topic distribution to describe the relatedness of rules to topics. Figure 1 shows four synchronous rules (Chiang, 2007) with topic distributions, some of which contain nonterminals. We can see that, although the source part of rule (b) and (c) are identic</context>
<context position="12981" citStr="Tam et al., (2007)" startWordPosition="2074" endWordPosition="2077"> topic model (Section 4.2). A more principle way is to learn a bilingual topic model from bilingual corpus (Mimno et al., 2009). However, we may face difficulty during decoding, where only source language documents are available. It requires a marginalization to infer the monolingual topic distribution using the bilingual topic model. The high complexity of marginalization prohibits such a summation in practice. Previous work on bilingual topic model avoid this problem by some monolingual assumptions. Zhao and Xing (2007) assume that the topic model is generated in a monolingual manner, while Tam et al., (2007) construct their bilingual topic model by enforcing a one-toone correspondence between two monolingual topic models. We also estimate our rule-topic distribution by two monolingual topic models, but use a different way to project target-side topics onto source-side topics. 4.1 Monolingual Topic Distribution Estimation We estimate rule-topic distribution from wordaligned bilingual training corpus with document boundaries explicitly given. The source and target side distributions are estimated in the same way. For simplicity, we only describe the estimation of source-side distribution in this se</context>
<context position="17659" citStr="Tam et al., (2007)" startWordPosition="2788" endWordPosition="2791">, where the item Mi,j represents the probability P(zf = i|ze = j). In the second step, given the correspondence matrix MKe×Kf, we project the target-side rule-topic distribution P(ze|r) to the source-side topic space by multiplication as follows: T (P(ze|r)) = P(ze|r) ® MKe×Kf (5) In this way, we get a second distribution for a rule in the source-side topic space, which we called projected target-side topic distribution T(P(ze|r)). Obviously, our projection method allows one target-side topic to align to multiple source-side topics. This is different from the one-to-one correspondence used by Tam et al., (2007). From the training result of the correspondence matrix MKe×Kf, we find that the topic correspondence between source and target language is not necessarily one-to-one. Typically, the probability P(z = kf|z = ke) of a target-side topic mainly distributes on two or three source-side topics. Table 1 shows an example of a target-side topic with its three mainly aligned source-side topics. 5 Decoding We incorporate our topic similarity model as a new feature into a traditional hiero system (Chiang, 2007) under discriminative framework (Och and Ney, 2002). Considering there are a sourceside rule-top</context>
<context position="27330" citStr="Tam et al., 2007" startWordPosition="4305" endWordPosition="4308">res always punish topic-insensitive rules, we introduce topic sensitivity features as a complement. In the last line of Table 2, we obtain a further improvement of 0.23 points, when incorporating topic sensitivity features with topic similarity features. This suggests that it is necessary to distinguish topic-insensitive and topic-sensitive rules. 6.4 One-to-One Vs. One-to-Many Topic Projection In Section 4.2, we find that source-side topic and target-side topics may not exactly match, hence we use one-to-many topic correspondence. Yet another method is to enforce one-to-one topic projection (Tam et al., 2007). We achieve one-to-one projection by aligning a target topic to the source topic with the largest correspondence probability as calculated in Section 4.2. Table 4 compares the effects of these two method756 System MT06 MT08 Avg Baseline 30.20 21.93 26.07 Phrase-rule 30.53 22.29 26.41 Monotone-rule 30.72 22.62 26.67 Reordering-rule 30.31 22.40 26.36 All-rule 30.95 22.92 26.94 Table 5: Effect of our topic model on three types of rules. Phrase rules are fully lexicalized, while monotone and reordering rules contain nonterminals. s. We find that the enforced one-to-one topic method obtains a slig</context>
<context position="30024" citStr="Tam et al., 2007" startWordPosition="4735" endWordPosition="4738"> models on each domain. Finally, they combine a specific domain translation model with a general domain translation model depending on various text distances. One way to calculate the distance is using topic model. Gong et al. (2010) introduce topic model for filtering topic-mismatched phrase pairs. They first assign a specific topic for the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the documen</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007. Bilingual lsa-based adaptation for statistical machine translation. Machine Translation, 21(4):187–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. Coling</booktitle>
<contexts>
<context position="9110" citStr="Wu et al., 2008" startWordPosition="1460" endWordPosition="1463"> suitable for a given source document. The topic similarity computes the distance of two topic distributions. We calculate the topic similarity by Hellinger function: Similarity(P(z|d), P(z|r)) (\I \I )2 P(z = k|d) − P(z = k|r) (1) Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions. 3.2 Topic Sensitivity Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data. Similarly, we divide the rules into topic-insensitive rules and topic-sensitive &apos;We also try other distance functions, including Euclidean distance, Kullback-Leibler divergence and cosine function. They produce similar results in our preliminary experiments. rules according to their topic distributions. Let’s revisit Figure 1. We can easily find that the topic distribution of rule (c) distribute evenly. This indicates that it is insensitive to topics, and can be applied in any topics. We call such a rul</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proc. Coling 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="1471" citStr="Zhao and Xing, 2006" startWordPosition="203" endWordPosition="206"> topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level. 1 Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗Corresponding author by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic </context>
<context position="6116" citStr="Zhao and Xing, 2006" startWordPosition="959" endWordPosition="962"> contains two types of parameters. The first one relates to the documenttopic distribution, which records the topic distribution of each document. The second one is used for topic-word distribution, which represents each topic as a distribution over words. Based on these parameters (and some hyper-parameters), LDA can infer a topic assignment for each word in the documents. In the following sections, we will use these parameters and the topic assignments of words to estimate the parameters in our method. 3 Topic Similarity Model Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007). In the hierarchical phrase based system, a synchronous rule may be related to some topics and unrelated to others. In terms of probability, a rule often has an uneven probability distribution over topics. The probability over a topic is high if the rule is highly related to the topic, otherwise the probability will be low. Therefore, we use topic distribution to describe the relatedness of rules to topics. Figure 1 shows four synchronous rules (Chiang, 2007) with topic distributions, some of which contain nonterminals. We can see that, although the sou</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In Proc. ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>HM-BiTAM: Bilingual topic exploration, word alignment, and translation.</title>
<date>2007</date>
<booktitle>In Proc. NIPS</booktitle>
<contexts>
<context position="1492" citStr="Zhao and Xing, 2007" startWordPosition="207" endWordPosition="210">with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level. 1 Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗Corresponding author by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the</context>
<context position="6137" citStr="Zhao and Xing, 2007" startWordPosition="963" endWordPosition="966">f parameters. The first one relates to the documenttopic distribution, which records the topic distribution of each document. The second one is used for topic-word distribution, which represents each topic as a distribution over words. Based on these parameters (and some hyper-parameters), LDA can infer a topic assignment for each word in the documents. In the following sections, we will use these parameters and the topic assignments of words to estimate the parameters in our method. 3 Topic Similarity Model Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007). In the hierarchical phrase based system, a synchronous rule may be related to some topics and unrelated to others. In terms of probability, a rule often has an uneven probability distribution over topics. The probability over a topic is high if the rule is highly related to the topic, otherwise the probability will be low. Therefore, we use topic distribution to describe the relatedness of rules to topics. Figure 1 shows four synchronous rules (Chiang, 2007) with topic distributions, some of which contain nonterminals. We can see that, although the source part of rule (b) </context>
<context position="12890" citStr="Zhao and Xing (2007)" startWordPosition="2057" endWordPosition="2060">ect the targetside topic distribution of a synchronous rule into the space of the source-side topic model (Section 4.2). A more principle way is to learn a bilingual topic model from bilingual corpus (Mimno et al., 2009). However, we may face difficulty during decoding, where only source language documents are available. It requires a marginalization to infer the monolingual topic distribution using the bilingual topic model. The high complexity of marginalization prohibits such a summation in practice. Previous work on bilingual topic model avoid this problem by some monolingual assumptions. Zhao and Xing (2007) assume that the topic model is generated in a monolingual manner, while Tam et al., (2007) construct their bilingual topic model by enforcing a one-toone correspondence between two monolingual topic models. We also estimate our rule-topic distribution by two monolingual topic models, but use a different way to project target-side topics onto source-side topics. 4.1 Monolingual Topic Distribution Estimation We estimate rule-topic distribution from wordaligned bilingual training corpus with document boundaries explicitly given. The source and target side distributions are estimated in the same </context>
<context position="22946" citStr="Zhao and Xing (2007)" startWordPosition="3612" endWordPosition="3615">et-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During decoding, we first infer the topic distribution of given documents before translation according to the topic model trained on Chinese part of FBIS corpus. 6.2 Effect of Topic Similarity Model We compare our method with two baselines. In addition to the traditional hiero system, we also compare with the topic-specific lexicon translation method in Zhao and Xing (2007). The lexicon translation probability is adapted by: p(f|e, DF) ∝ p(e|f, DF)P(f|DF) (6) �= p(e|f, z = k)p(f|z = k)p(z = k|DF) (7) k However, we simplify the estimation of p(e|f, z = k) by directly using the word alignment corpus with 3http://gibbslda.sourceforge.net/ 4We determine K by testing {15, 30, 50, 100, 200} in our preliminary experiments. We find that K = 30 produces a slightly better performance than other values. 755 Type Count Src% Tgt% Phrase-rule 3.9M 83.4 84.4 Monotone-rule 19.2M 85.3 86.1 Reordering-rule 5.7M 85.9 86.8 All-rule 28.8M 85.1 86.0 Table 3: Percentage of topic-sensi</context>
</contexts>
<marker>Zhao, Xing, 2007</marker>
<rawString>Bin Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual topic exploration, word alignment, and translation. In Proc. NIPS 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>