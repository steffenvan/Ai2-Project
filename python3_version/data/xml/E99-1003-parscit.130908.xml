<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.712156">
Proceedings of EACL &apos;99
</note>
<title confidence="0.989507">
TERM EXTRACTION + TERM CLUSTERING:
An Integrated Platform for Computer-Aided Terminology
</title>
<author confidence="0.743019">
Didier Bourigault
</author>
<address confidence="0.512341">
ERSS, UMR 5610 CNRS
Maison de la Recherche
5 allees Antonio Machado
31058 Toulouse cedex, .FRANCE
</address>
<email confidence="0.996181">
didier.bourigault@wanadoo.fr
</email>
<note confidence="0.5598475">
Christian Jacquemin
LIMSI-CNRS
</note>
<sectionHeader confidence="0.879516333333333" genericHeader="abstract">
BP 133
91403 ORSAY
FRANCE
</sectionHeader>
<email confidence="0.460733">
jacqueminOlimsi.fr
</email>
<sectionHeader confidence="0.98301" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.99997325">
A novel technique for automatic the-
saurus construction is proposed. It is
based on the complementary use of two
tools: (1) a Term Extraction tool that
acquires term candidates from tagged
corpora through a shallow grammar of
noun phrases, and (2) a Term Cluster-
ing tool that groups syntactic variants
(insertions). Experiments performed on
corpora in three technical domains yield
clusters of term candidates with preci-
sion rates between 93% and 98%.
</bodyText>
<sectionHeader confidence="0.96192" genericHeader="method">
1 Computational Terminology
</sectionHeader>
<bodyText confidence="0.999927394736842">
In the domain of corpus-based terminology two
types of tools are currently developed: tools
for automatic term extraction (Bourigault, 1993;
Justeson and Katz, 1995; Daille, 1996; Brun,
1998) and tools for automatic thesaurus construc-
tion (Grefenstette, 1994). These tools are ex-
pected to be complementary in the sense that
the links and clusters proposed in automatic the-
saurus construction can be exploited for structur-
ing the term candidates produced by the auto-
matic term extractors. In fact, complementarity
is difficult because term extractors provide mainly
multi-word terms, while tools for automatic the-
saurus construction yield clusters of single-word
terms.
On the one hand, term extractors focus on
multi-word terms for ontological motivations:
single-word terms are too polysemous and too
generic and it is therefore necessary to provide
the user with multi-word terms that represent
finer concepts in a domain. The counterpart of
this focus is that automatic term extractors yield
important volumes of data that require structur-
ing through a postprocessor. On the other hand,
tools for automatic thesaurus construction focus
on single-word terms for practical reasons. Since
they cluster terms through statistical measures
of context similarities, these tools exploit recur-
ring situations. Since single-word terms denote
broader concepts than multi-word terms, they ap-
pear more frequently in corpora and are therefore
more appropriate for statistical clustering.
The contribution of this paper is to propose
an integrated platform for computer-aided term
extraction and structuring that results from the
combination of LEXTER, a Term Extraction tool
(Bourigault et al., 1996), and FASTR1, a Term
Normalization tool (Jacquemin et al., 1997).
</bodyText>
<sectionHeader confidence="0.966367" genericHeader="method">
2 Components of the Platform for
Computer-Aided Terminology
</sectionHeader>
<bodyText confidence="0.9987588">
The platform for computer-aided terminology is
organized as a chain of four modules and the cor-
responding flowchart is given by Figure 1. The
modules are:
POS tagging First the corpus is processed by
Sylex, a Part-of-Speech tagger. Each word is
unambiguously tagged and receives a single
lemma.
Term Extraction LEXTER, the term extrac-
tion tool acquires term candidates from the
tagged corpus. In a first step, LEXTER ex-
ploits the part-of-speech categories for ex-
tracting maximal-length noun phrases. It re-
lies on makers of frontiers together with a
shallow grammar of noun phrases. In a sec-
ond step, LEXTER recursively decomposes
these maximal-length noun phrases into two
syntactic constituents (Head and Expansion).
Term Clustering The term clustering tool
groups the term candidates produced at the
</bodyText>
<footnote confidence="0.5415125">
FASTR can be downloaded from
wwv.limsi.fr/Individu/jacquemi/FASTR.
</footnote>
<page confidence="0.969951">
15
</page>
<figure confidence="0.995791375">
Proceedings of EACL &apos;99
Raw corpus
P-O-S Tagging
Sylex
Lemmatized and tagged corpus
Term Extraction
LEXTER
Network of term candidates
Term Clustering
FASTR
1
Clusters of term candidates
V
Data-base
Interface
Structured terminology
</figure>
<figureCaption confidence="0.973014">
Figure 1: Overview of the platform for computer-
aided terminology
</figureCaption>
<bodyText confidence="0.99991725">
preceding step through a self-indexing proce-
dure followed by a graph-based classification.
This task is basically performed by FASTR,
a term normalizer, that has been adapted to
the task at hand.
v==!&amp;quot;-la tion The last step of thesaurus construc-
tion is the validation of automatically ex-
tracted clusters of term candidates by a ter-
minologist and a domain expert. The vali-
dation is performed through a data-base in-
terface. The links are automatically updated
through the entire base and a structured the-
saurus is progressively constructed.
The following sections provide more details
about the components and evaluate the quality
of the terms thus extracted.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="method">
3 Term Extraction
</sectionHeader>
<subsectionHeader confidence="0.999282">
3.1 Term Extraction for the French
Language
</subsectionHeader>
<bodyText confidence="0.999669043478261">
Term extraction tools perform statistical or/and
syntactical analysis of text corpora in special-
ized technical or scientific domains. Term can-
didates correspond to sequences of words (most
of the time noun phrases) that are likely to be
terminological units. These candidates are ulti-
mately validated as entries by a terminologist in
charge of building a thesaurus. LEXTER, the
term extractor, is applied to the French language.
Since French is a Romance language, the syntac-
tic structure of terms and compounds is very sim-
ilar to the structure of non-compound and non-
terminological noun phrases. For instance, in
French, terms can contain prepositional phrases
with determiners such as: paroiNoun dePrep i&apos;Det
uretereNoun (ureteral wall). Because of this simi-
larity, the detection of terms and their variants in
French is more difficult than in the English lan-
guage.
The input of our term extraction tool is an un-
ambiguously tagged corpus. The extraction pro-
cess is composed of two main steps: Splitting and
Parsing.
</bodyText>
<subsectionHeader confidence="0.999599">
3.2 Splitting
</subsectionHeader>
<bodyText confidence="0.989986571428572">
The techniques of shallow parsing implemented
in the Splitting module detect morpho-syntactical
patterns that cannot be parts of terminological
noun phrases and that are therefore likely to in-
dicate noun phrases boundaries. Splitting tech-
niques are used in other shallow parsers such as
(Grefenstette, 1992). In the case of LEXTER, the
noun phrases which are isolated by splitting are
not intermediary data; they are not used by any
other automatic module in order to index or clas-
sify documents. The extracted noun phrases are
term candidates which are proposed to the user.
In such a situation, splitting must be performed
with high precision.
In order to process correctly some problem-
atic splittings, such as coordinations, attribu-
tive past participles and sequences preposition
+ determiner, the system acquires and uses
corpus-based selection restrictions of adjectives
and nouns (Bourigault et al., 1996).
For example, in order to disambiguate PP-
attachments, the system possesses a corpus-
based list of adjectives which accept a preposi-
tional argument built with the preposition a (at).
These selectional restrictions are acquired through
Corpus-Based Endogenous Learning (CBEL) as
follows: During a first pass, all the adjectives in a
predicative position followed by the preposition a
are collected. During a second pass, each time a
splitting rule has eliminated a sequence beginning
with the preposition a, the preceding adjective is
discarded from the list. Empirical analyses con-
firm the validity of this procedure. More complex
procedures of CBEL are implemented into LEX-
TER in order to acquire nouns sub-categorizing
the preposition a or the preposition sur (on), ad-
jectives sub-categorizing the preposition de (of),
past participles sub-categorizing the preposition
de (of), etc.
Ultimately, the Splitting module produces a set
of text sequences, mostly noun phrases, which we
Expert
</bodyText>
<page confidence="0.949555">
16
</page>
<bodyText confidence="0.923528333333333">
Proceedings of EACL &apos;99
refer to as Maximal-Length Noun Phrases (hence-
forth MLNP).
</bodyText>
<subsectionHeader confidence="0.99876">
3.3 Parsing
</subsectionHeader>
<bodyText confidence="0.9998871">
The Parsing module recursively decomposes the
maximal-length noun phrases into two syntac-
tic constituents: a constituent in head-position
(e.g. bronchial cell in the noun phrase cylindri-
cal bronchial cell, and cell in the noun phrase
bronchial cell), and a constituent in expansion po-
sition (e.g. cylindrical in the noun phrase cylin-
drical bronchial cell, and bronchial in the noun
phrase bronchial cell). The Parsing module ex-
ploits rules in order to extract two subgroups from
each MLNP, one in head-position and the other
one in expansion position. Most of MLNP se-
quences are ambiguous. Two (or more) binary
decompositions compete, corresponding to several
possibilities of prepositional phrase or adjective
attachment. The disambiguation is performed by
a corpus-based method which relies on endoge-
nous learning procedures (Bourigault, 1993; Rat-
naparkhi, 1998). An example of such a procedure
is given in Figure 2.
</bodyText>
<subsectionHeader confidence="0.975497">
3.4 Network of term candidates
</subsectionHeader>
<bodyText confidence="0.9999091">
The sub-groups generated by the Parsing module,
together with the maximal-length noun phrases
extracted by the Splitting module, are the term
candidates produced by the Term extraction tool.
This set of term candidates is represented as a
network: each multi-word term candidate is con-
nected to its head constituent and to its expansion
constituent by syntactic decomposition links. An
excerpt of a network of term candidates is given
in Figure 3. Vertical and horizontal links are syn-
tactic decomposition links produced by the Term
Extraction tool. The oblique link is a syntactic
variation link added by the Term Clustering tool.
The building of the network is especially im-
portant for the purpose of term acquisition. The
average number of multi-word term candidates is
8,000 for a 100,000 word corpus. The feedback
of several experiments in which our Term Extrac-
tion tool was used shows that the more structured
the set of term candidates is, the more efficiently
the validation task is performed. For example,
the structuring through syntactic decomposition
allows the system to underscore lists of terms that
share the same term either in head position or in
expansion position. Such paradigmatic series are
frequent in term banks, and initiating the valida-
tion task by analyzing such lists appears to be a
very efficient validation strategy.
This paper proposes a novel technique for en-
riching the network of term candidates through
</bodyText>
<figureCaption confidence="0.972478">
Figure 3: Excerpt of a network of term candidates.
</figureCaption>
<bodyText confidence="0.8946905">
the addition of syntactic variation links to syntac-
tic decomposition links.
</bodyText>
<sectionHeader confidence="0.998896" genericHeader="method">
4 Term Clustering
</sectionHeader>
<subsectionHeader confidence="0.999986">
4.1 Adapting a Normalization Tool
</subsectionHeader>
<bodyText confidence="0.999917705882353">
Term normalization is a procedure used in au-
tomatic indexing for conflating various term oc-
currences into unique canonical forms. More or
less linguistically-oriented techniques are used in
the literature for this task. Basic procedures
such as (Dillon and Gray, 1983) rely on function
word deletion, stemming, and alphabetical word
reordering. For example, the index library cat-
alogs is transformed into catalog /ibrar through
such simplification techniques.
In the platform presented in this paper, term
normalization is performed by FASTR, a shal-
low transformational parser which uses linguistic
knowledge -about the possible morpho-syntactic
transformations of canonical terms (Jacquemin et
al., 1997). Through this technique syntactically
and morphologically-related occurrences, such as
stabilisation de prix (price stabilization) and sta-
biliser leurs prix (stabilize their prices), are con-
flated.
Term variant extraction in FASTR differs from
preceding works such as (Evans et al., 1991) be-
cause it relies on a shallow syntactic analysis of
term variations instead of window-based measures
of term overlaps. In (Sparck Jones and Tait, 1984)
a knowledge-intensive technique is proposed for
extracting term variations. This approach has
however never been applied to large scale term ex-
traction because it is based on a full semantic anal-
ysis of sentences. Our approach is more realistic
because it does not involve large-scale knowledge-
intensive interpretation of texts that is known to
be unrealistic.
Our approach to the clustering of term can-
</bodyText>
<figure confidence="0.997581466666667">
cell
N3
bronchial cell
A2N3
Expansion link
bronchial
A2
cylindrical bronchial cell
A, A2 N3
cylindrical cell
A IN3 cylindrical
Expansion link A1
-o
as
a)
</figure>
<page confidence="0.977702">
17
</page>
<table confidence="0.976439333333333">
Proceedings of EACL &apos;99
Parsing rule Parse (2)
Noun&apos; Prep Noun2 Adj Head: Noun&apos; Prep Noun2
Parse (1) Head: Nouni
Head: Nonni Exp.: Noun2
Exp.: Noun2 Adj Exp.: Adj
Head: Noun2
Exp.: Adj
Disambiguation procedure:
</table>
<figure confidence="0.709834">
Look in the corpus for non ambiguous occurrences of the sub-groups:
(a) Noun2 Adj (b) Nouni Adj (c) Nonni Prep Noun2
Then choose:
</figure>
<figureCaption confidence="0.79964575">
if the sub-group (a) has been found, then choose Parse (1)
else if the sub-groups (b) or (c) have been found, then choose Parse (2)
else choose Parse (1)
Figure 2: An ambiguous parsing rule and associated disambiguation procedure
</figureCaption>
<bodyText confidence="0.999474928571429">
didates is to group the output of LEXTER, by
conflating term candidates with other term can-
didates instead of conflating corpus occurrences
with controlled terms. Our technique can be seen
as a kind of self-indexing in which term candidates
are indexed by themselves through FASTR, for
the purpose of conflating candidates that are vari-
ants of each other. Thus, the term candidate cel-
lule bronchique cylindrique (cylindrical bronchial
cell) is a variant of the other candidate cellule
cylindrique (cylindrical cell) because an adjecti-
val modifier is inserted in the first term. Through
the self-indexing procedure these two candidates
belong to the same cluster.
</bodyText>
<subsectionHeader confidence="0.99852">
4.2 Types of Syntactic Variation Rules
</subsectionHeader>
<bodyText confidence="0.9950885">
Because of this original framework, specific vari-
ations patterns were designed in order to capture
inter-term variations. In this study, we restrict
ourselves to syntactic variations and ignore mor-
phological modifications. The variations patterns
can be classified into the following two families:
Internal insertion of modifiers The insertion
of one or more modifiers inside a noun phrase
structure. For instance the following trans-
formation NAInsAj:
</bodyText>
<equation confidence="0.8999715">
Nouni Adj2
Noun i ((Adv? Adj)1-3 Adv?) Adj2
</equation>
<bodyText confidence="0.982727117647059">
describes the insertion of one to three adjec-
tival modifiers inside a Noun-Adjective struc-
ture in French. Through this transforma-
tion, the term candidate cellule bronchique
cylindrique (cylindrical bronchial cell) is rec-
ognized as a variant of the term candidate
cellule cylindrique (cylindrical cell). Other
internal modifications account for adverbial
and prepositional modifiers.
Preposition switch &amp; determiner insertion
In French, terms, compounds, and noun
phrases have comparable structures: gen-
erally a head noun followed by adjectival
or prepositional modifiers. Such terms may
vary through lexical changes without signif-
icant structural modifications. For example
NPNSynt:
</bodyText>
<equation confidence="0.4124245">
Nouni Prep2 Noufl3
--+ Nouni ((Prep Det?)?) Noun3
</equation>
<bodyText confidence="0.9991022">
accounts for preposition suppressions such
as fibre de collagenel fibre collagene (colla-
gen fiber), additions of determiners, and/or
preposition switches such as revetment de
surface&apos; revetement en surface (surface coat-
ing).
The complete rule set is shown in Table 1. Each
transformation given in the first column conflates
the term structure given in the second column and
the term structure given in the third column.
</bodyText>
<subsectionHeader confidence="0.999479">
4.3 Clustering
</subsectionHeader>
<bodyText confidence="0.999900142857143">
The output of FASTR is a set of links between
pairs of term candidates in which the target can-
didate is a variant of the source candidate. In
order to facilitate the validation of links by the ex-
pert, this output is converted into clusters of term
candidates. The syntactic variation links can be
considered as the edges of an undirected graph g
whose nodes are the term candidates. A node n1
representing a term t1 is connected to a node n2
representing t2 if and only if there is a transfor-
mation T such that Y(t1) = t2 or T(r2) = ti•
Each connected subgraph gi of g is considered as
a cluster of term candidates likely to correspond
to similar concepts. (A connected subgraph gi is
</bodyText>
<page confidence="0.999398">
18
</page>
<tableCaption confidence="0.9400785">
Proceedings of EACL &apos;99
Table 1: Syntactic variation rules exploited by the Term Clustering tool.
</tableCaption>
<table confidence="0.9988196875">
Ident. Base term Variant
NAInsAv
NAInsAj
NAInsN
Nouni Adj2 Nouni ((Adv? Adj)°&apos; Adv) Adj2
Noun&apos; Adj2 Nouni ((Adv? Adj)1-3 Adv?) Adj2
Noun&apos; Adj2 Noun&apos; ((Adv? Adj)?
(Prep? Det? (Adv? Adj)? Noun) (Adv? Adj)? Adv?) Adj2
ANInsAv Adji Noun2 (Adv) Adji Noun3
NPNSynt
NPNInsAj
NPNInsN
Noun&apos; Prep2 Noun3 Nonni ((Prep Det?)?) Noun3
Nouni Prep2 Noun3 Nonni ((Adv? Adj)°-3 Prep Det? (Adv? Adj)0-3 ) Noun3
Nouni Prep2 Noun3 Noun&apos; ((Adv? Adj)°-3 (Prep Det?)? (Adv? Adj)°-3 Noun
(Adv? Adj)°-3 (Prep Det?)? (Adv? Adj)0-3 ) Noun3
</table>
<figure confidence="0.5107884">
NPDNSynt Noun&apos; Prep2 Det4 Noun3 Nouni ((Prep Det?)?) Noun3
NPDNInsAj Nonni Prep2 Det4 Noun3 Noun&apos; ((Adv? Adj)°-3 Prep Det? (Adv? Adj)0-3 ) Noun3
NPDNInsN Noun&apos; Prep2 Det4 Noun3 Nonni ((Adv? Adj)°-3 (Prep Det?)? (Adv? Adj)°-3 Noun
(Adv? Adj)°-3 (Prep Det?)? (Adv? Adj)0-3 ) Noun3
nucleole souvent proiminent nucleole central proeminent
</figure>
<figureCaption confidence="0.999789">
Figure 4: A sample 4-term cluster.
</figureCaption>
<bodyText confidence="0.999824666666667">
such that for every pair of nodes (ni , n2) in gi,
there exists a path from ni to n2.)
For example, ti =nucleole proerninent (promi-
nent nucleolus), t2 =nucleole central proeminent
(prominent central nucleolus), t3 =nucleole sou-
vent proeminent (frequently prominent nucleo-
lus), and t4 =nucleole parfois proeminent (some-
times prominent nucleolus) are four term candi-
dates that build a star-shaped 4-word cluster il-
lustrated by Figure 4. Each edge is labelled with
the syntactic transformation T that maps one of
the nodes to the other.
</bodyText>
<sectionHeader confidence="0.997431" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9998342">
Experiments were made on three different corpora
described in Table 2. The first two lines of Table 2
report the size of the corpora and the number
of term candidates extracted by LEXTER from
these corpora. The third and fourth lines show
the number of links between term candidates ex-
tracted by FASTR and the number of connected
subgraphs corresponding to these links. Finally,
the last two lines report statistics on the size of the
clusters and the ratio of term candidates that be-
</bodyText>
<tableCaption confidence="0.999002">
Table 3: Frequencies of syntactic variations.
</tableCaption>
<table confidence="0.9997975">
[Menel.] [Brouss.] [DER]
NAInsAv 21% 30% 1%
NAInsAj 33% 25% 5%
NAInsN 23% 21% 13%
ANInsAv 3% 3% 0%
NPNSynt 2% 2% 18%
NPNInsAj 6% 11% 8%
NPNInsN 1% 2% 11%
NPDNSynt 1% 2% 22%
NPDNInsAj 8% 2% 11%
NPDNInsN 2% 2% 11%
Total l00% 100% 100%
</table>
<bodyText confidence="0.999643571428572">
long to one of the subgraphs produced by the clus-
tering algorithm. Although the variation rules im-
plemented in the Term Structuring tool are rather
restrictive (only syntactic insertion has been taken
into account), the number of links added to the
network of term candidates is noticeably high. An
average rate of 10% of multi-word term candidates
produced by LEXTER belong to one of the clus-
ters resulting from the recognition of term variants
by FA S T R.
Frequencies of syntactic variations are reported
in Table 3. A screen-shot showing the type of
validation that is proposed to the expert is given
by Figure 5.
</bodyText>
<sectionHeader confidence="0.991805" genericHeader="method">
6 Expert Evaluation
</sectionHeader>
<bodyText confidence="0.999713666666667">
Evaluation was performed by three experts, one in
each domain represented by each corpus. These
experts had already been involved in the con-
</bodyText>
<figure confidence="0.989194375">
NAInsAv NAInsAj
ti
nucleole proeminent
NAInsAv
14
•
nucleole patfois proiminent
t2
</figure>
<page confidence="0.986645">
19
</page>
<tableCaption confidence="0.924593">
Proceedings of EACL &apos;99
Table 2: The three corpora exploited in the experiments.
</tableCaption>
<table confidence="0.990235636363636">
[Broussais] [DER] [Menelas]
Domain anatomy pathology nuclear engineering coronarian diseases
Type of documents medical reports technical reports medical files
Number of words 40,000 230,000 110,000
Number of multi-word term 3,439 14,037 10,155
candidates
Number of variation links 240 785 634
Number of clusters 168 556 448
Maximal size of the clusters 10 13 13
Number of term candidates 438 (12.7%) 1,349 (9.6%) 1,173 (11.6%)
belonging to one cluster
</table>
<figure confidence="0.5130355">
&apos;.4k Microsoft Access
Mode Formuletire
</figure>
<figureCaption confidence="0.996381">
Figure 5: The expert interface for cluster validation
</figureCaption>
<figure confidence="0.8145035">
VTITitItICill nks
lrevetement focalement hyperplasique
vy
revetement pleural hyperplasique
evetement focalement hyperplasique
evetement hyperplasique
evetement pleural hyperplasique
evetement epithelial regulier focalement hyperplasique
evetement epithelium souvent hyperplasique
revetement epithelium souvent hyperplasique
revetement epithelial reguller focalement hyperplasique
element chscretement hyperplasique
</figure>
<page confidence="0.676095">
20
</page>
<bodyText confidence="0.990271964285715">
Proceedings of EACL &apos;99
struction of terminological products through the
analysis of the three corpora used in our ex-
periments: an ontology for a case-memory sys-
tem dedicated to the diagnosis support in pathol-
ogy ([Broussais]), a semantic dictionary for the
Menelas Natural Language Understanding sys-
tem ([Menelas]), and a structured thesaurus for a
computer-assisted technical writing tool ([DER]).
The precision rates are very satisfactory (from
93% to 98% corresponding to error rates of 7% and
2% given in the last line of Table 4), and show that
the proposed method must be considered as an
important progress in corpus-based terminology.
Only few links are judged as conceptually irrele-
vant by the experts. For example, image d&apos;embole
tumorale (image of a tumorous embolus) is not
considered as a correct variant of image tumorale
(image of a tumor) because the first occurrence
refers to an embolus while the second one refers
to a tumor.
The experts were required to assess the pro-
posed links and, in case of positive reply, they
were required to provide a judgment about the
actual conceptual relation between the connected
terms. Although they performed the validation in-
dependently, the three experts have proposed very
similar types of conceptual relations between term
candidates connected by syntactic variation links.
At a coarse-grained level, they proposed the same
three types of conceptual relations:
Synonymy Both connected terms are consid-
ered as equivalent by the expert: embole
tumorale (tumorous embolus)/ embole vascu-
laire tumorale (vascular tumorous embolus).
The preceding example corresponds to a fre-
quent situation of elliptic synonymy: the no-
tion of integrated metonymy (Kleiber, 1989).
In the medical domain, it is a common knowl-
edge that an embole tumorale is an embole
vasculaire tumorale, as everyone knows that
sunflower oil is a synonym of sunflower seed
oil.
Generic/specific relation One of the two
terms denotes a concept that is finer than
the other one: cellule epitheliale cylindrique
(cylindrical epithelial cell) is a specific type
of cellule cylindrique (cylindrical cell).
Attributive relation As in the preceding case,
there is a non-synonymous semantic relation
between the two terms. One of them denotes
a concept richer than the other one because it
carries an additional attributes: a noyau vo-
lumineux irregulier (large irregular nucleus)
is a noyau irregulier (irregular nucleus) that
is additionally volumineux (large).
</bodyText>
<sectionHeader confidence="0.996479" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999667705882353">
This study shows that the clustering of term can-
didates through term normalization is a powerful
technique for enriching the network of term can-
didates produced by a Term Extraction tool such
as LEXTER.
In our approach, term normalization is per-
formed through the conflation of specific term
variants. We have focused on syntactic vari-
ants that involve structural modifications (mainly
modifier insertions). As reported in (Jacquemin,
1999), morphological and semantic variations are
two other important families of term variations
which can also be extracted by FASTR. They will
be accounted for in order to enhance the number
of clustered term candidates. It is our purpose to
focus on these two types of variants in the near
future.
</bodyText>
<sectionHeader confidence="0.967893" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999872666666667">
The authors would like to thank the experts
for their comments and their evaluations of
our results: Pierre Zweigenbaum (AP/HP) on
[Menelas], Christel Le Bozec and Marie-Christine
Jaulent (AP/HP) on [Broussais], and Henry
Boccon-Gibod (DER-EDF) on [DER]. We are also
grateful to Henry Boccon-Gibod (DER-EDF) for
his support to this work. This work was partially
funded by Electricitie de France.
</bodyText>
<sectionHeader confidence="0.994646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.918408315789474">
Didier Bourigault, Isabelle Gonzalez-Mullier, and
Cecile Gros. 1996. Lexter, a natural language
processing tool for terminology extraction. In
Seventh EURALEX International Congress on
Lexicography (EURALEX96), Part II, pages
771-779.
Didier Bourigault. 1993. An endogeneous corpus-
based method for structural noun phrase disam-
biguation. In Proceedings, 6th Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL&apos;93), pages 81-86,
Utrecht.
Caroline Brun. 1998. Terminology finite-state
preprocessing for computational lfg. In Proceed-
ings, 36th Annual Meeting of the Association
for Computational Linguistics and 17th Inter-
national Conference on Computational Linguis-
tics (COLING-ACL &apos;98), pages 196-200, Mon-
treal.
</reference>
<page confidence="0.99924">
21
</page>
<tableCaption confidence="0.8887945">
Proceedings of EACL &apos;99
Table 4: Results of the validation.
</tableCaption>
<table confidence="0.888714625">
[Broussais] [Menelas] [DER]
Number of variation links proposed by the system 240 634 785
Number of variation links validated by the expert 240 227 344
Types of conceptual relation given by the expert 44 (18%) 14 (6%) 136 (40%)
synonymy
generic/specific 96 (40%) 147 (65%) 121 (35%)
attributive 96 (40%) 61 (27%) 62 (18%)
non relevant 4 (2%) 5 (2%) 25 (7%)
</table>
<reference confidence="0.99971535483871">
Beatrice Daille. 1996. Study and implementa-
tion of combined techniques for automatic ex-
traction of terminology. In Judith L. Klavans
and Philip Resnik, editors, The Balancing Act:
Combining Symbolic and Statistical Approaches
to Language, pages 49-66. MIT Press, Cam-
bridge, MA.
Martin Dillon and Ann S. Gray. 1983. FASIT :
A fully automatic syntactically based indexing
system. Journal of the American Society for
Information Science, 34(2):99-108.
David A. Evans, Kimberly Ginther-Webster,
Mary Hart, Robert G. Lefferts, and Ira A.
Monarch. 1991. Automatic indexing using se-
lective NLP and first-order thesauri. In Pro-
ceedings, Intelligent Multimedia Information
Retrieval Systems and Management (RIA0&apos;91),
pages 624-643, Barcelona.
Gregory Grefenstette. 1992. A knowledge-poor
technique for knowledge extraction from large
corpora. In Proceedings, 15th Annual Inter-
national ACM SIGIR Conference on Research
and Development in Information Retrieval (SI-
GIR&apos;92), Copenhagen.
Gregory Grefenstette. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer Aca-
demic Publisher, Boston, MA.
Christian Jacquemin, Judith L. Klavans, and Eve-
lyne Tzoukermann. 1997. Expansion of multi-
word terms for indexing and retrieval using
morphology and syntax. In Proceedings, 35th
Annual Meeting of the Association for Compu-
tational Linguistics and 8th Conference of the
European Chapter of the Association for Com-
putational Linguistics (ACL - EACL&apos;97), pages
24-31, Madrid.
Christian Jacquemin. 1999. Syntagmatic and
paradigmatic representations of term varia-
tion. In Proceedings, 37th Annual Meeting of
the Association for Computational Linguistics
(ACL &apos;99), University of Maryland.
John S. Justeson and Slava M. Katz. 1995. Tech-
nical terminology: some linguistic properties
and an algorithm for identification in text. Nat-
ural Language Engineering, 1(1):9-27.
George Kleiber. 1989. Paul est bronze versus la
peau de paul est bronzee. Contre une approche
referentielle analytique. In Harro Stammerjo-
hann, editor, Proceedings, Ve colloque interna-
tional de linguistique slavo-romane, pages 109-
134, Tubingen. Gunter Nair Verlag. Reprinted
in Nominales, A. Colin, Paris, 1995.
Adwait Ratnaparkhi. 1998. Statistical models
for unsupervised prepositional phrase attach-
ment. In Proceedings, 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on Compu-
tational Linguistics (COLING-ACL &apos;98), pages
1079-1085, Montreal.
Karen Sparck Jones and John I. Tait. 1984. Auto-
matic search term variant generation. Journal
of Documentation, 40(1):50-66.
</reference>
<page confidence="0.999027">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.626208">Proceedings of EACL &apos;99</note>
<title confidence="0.872639">TERM EXTRACTION + TERM CLUSTERING: An Integrated Platform for Computer-Aided Terminology Didier Bourigault ERSS, UMR 5610 CNRS</title>
<author confidence="0.943393">Maison de_la Recherche</author>
<address confidence="0.7935415">5 allees Antonio Machado 31058 Toulouse cedex, .FRANCE</address>
<email confidence="0.98571">didier.bourigault@wanadoo.fr</email>
<author confidence="0.999981">Christian Jacquemin</author>
<affiliation confidence="0.976315">LIMSI-CNRS</affiliation>
<address confidence="0.964553666666667">BP 133 91403 ORSAY FRANCE</address>
<email confidence="0.998782">jacqueminOlimsi.fr</email>
<abstract confidence="0.975499828947368">A novel technique for automatic thesaurus construction is proposed. It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions). Experiments performed on corpora in three technical domains yield clusters of term candidates with precision rates between 93% and 98%. 1 Computational Terminology In the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996; Brun, 1998) and tools for automatic thesaurus construction (Grefenstette, 1994). These tools are expected to be complementary in the sense that the links and clusters proposed in automatic thesaurus construction can be exploited for structuring the term candidates produced by the automatic term extractors. In fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic thesaurus construction yield clusters of single-word terms. On the one hand, term extractors focus on multi-word terms for ontological motivations: single-word terms are too polysemous and too generic and it is therefore necessary to provide the user with multi-word terms that represent finer concepts in a domain. The counterpart of this focus is that automatic term extractors yield important volumes of data that require structuring through a postprocessor. On the other hand, tools for automatic thesaurus construction focus on single-word terms for practical reasons. Since they cluster terms through statistical measures of context similarities, these tools exploit recurring situations. Since single-word terms denote broader concepts than multi-word terms, they appear more frequently in corpora and are therefore more appropriate for statistical clustering. The contribution of this paper is to propose an integrated platform for computer-aided term extraction and structuring that results from the of Term Extraction tool et al., 1996), and Term Normalization tool (Jacquemin et al., 1997). 2 Components of the Platform for Computer-Aided Terminology The platform for computer-aided terminology is organized as a chain of four modules and the corresponding flowchart is given by Figure 1. The modules are: tagging the corpus is processed by Part-of-Speech tagger. Each word is unambiguously tagged and receives a single lemma. Extraction term extraction tool acquires term candidates from the corpus. In a first step, exploits the part-of-speech categories for extracting maximal-length noun phrases. It reon makers of frontiers together with shallow grammar of noun phrases. In a secstep, decomposes these maximal-length noun phrases into two syntactic constituents (Head and Expansion). Clustering term clustering tool groups the term candidates produced at the be downloaded from wwv.limsi.fr/Individu/jacquemi/FASTR.</abstract>
<note confidence="0.897491">15 Proceedings of EACL &apos;99</note>
<title confidence="0.895850375">Raw corpus P-O-S Tagging Sylex Lemmatized and tagged corpus Term Extraction LEXTER Network of term candidates Term Clustering</title>
<abstract confidence="0.98511652">FASTR 1 Clusters of term candidates V Data-base Interface Structured terminology Figure 1: Overview of the platform for computeraided terminology preceding step through a self-indexing procedure followed by a graph-based classification. task is basically performed by a term normalizer, that has been adapted to the task at hand. v==!&amp;quot;-la tion The last step of thesaurus construction is the validation of automatically extracted clusters of term candidates by a terminologist and a domain expert. The validation is performed through a data-base interface. The links are automatically updated through the entire base and a structured thesaurus is progressively constructed. The following sections provide more details about the components and evaluate the quality of the terms thus extracted. 3 Term Extraction 3.1 Term Extraction for the French Language Term extraction tools perform statistical or/and syntactical analysis of text corpora in specialized technical or scientific domains. Term candidates correspond to sequences of words (most of the time noun phrases) that are likely to be terminological units. These candidates are ultimately validated as entries by a terminologist in of building a thesaurus. term extractor, is applied to the French language. French is a Romance language, the syntactic structure of terms and compounds is very similar to the structure of non-compound and nonterminological noun phrases. For instance, in French, terms can contain prepositional phrases determiners such as: paroiNoun i&apos;Det (ureteral wall). Because of this similarity, the detection of terms and their variants in French is more difficult than in the English language. The input of our term extraction tool is an unambiguously tagged corpus. The extraction process is composed of two main steps: Splitting and Parsing. 3.2 Splitting The techniques of shallow parsing implemented in the Splitting module detect morpho-syntactical patterns that cannot be parts of terminological noun phrases and that are therefore likely to indicate noun phrases boundaries. Splitting techniques are used in other shallow parsers such as 1992). In the case of noun phrases which are isolated by splitting are not intermediary data; they are not used by any other automatic module in order to index or classify documents. The extracted noun phrases are term candidates which are proposed to the user. In such a situation, splitting must be performed with high precision. In order to process correctly some problematic splittings, such as coordinations, attributive past participles and sequences preposition + determiner, the system acquires and uses corpus-based selection restrictions of adjectives and nouns (Bourigault et al., 1996). example, in order to disambiguate PPattachments, the system possesses a corpusbased list of adjectives which accept a preposiargument built with the preposition These selectional restrictions are acquired through Corpus-Based Endogenous Learning (CBEL) as follows: During a first pass, all the adjectives in a predicative position followed by the preposition a are collected. During a second pass, each time a splitting rule has eliminated a sequence beginning the preposition preceding adjective is discarded from the list. Empirical analyses confirm the validity of this procedure. More complex of CBEL are implemented into LEXorder to acquire nouns sub-categorizing preposition a or the preposition adsub-categorizing the preposition past participles sub-categorizing the preposition etc. Ultimately, the Splitting module produces a set of text sequences, mostly noun phrases, which we Expert 16 Proceedings of EACL &apos;99 refer to as Maximal-Length Noun Phrases (henceforth MLNP). The Parsing module recursively decomposes the maximal-length noun phrases into two syntactic constituents: a constituent in head-position cell the noun phrase cylindribronchial cell, the noun phrase cell), a constituent in expansion po- (e.g. the noun phrase cylinbronchial cell, the noun cell). Parsing module exploits rules in order to extract two subgroups from each MLNP, one in head-position and the other in expansion position. Most of seambiguous. Two (or more) binary decompositions compete, corresponding to several possibilities of prepositional phrase or adjective attachment. The disambiguation is performed by a corpus-based method which relies on endogenous learning procedures (Bourigault, 1993; Ratnaparkhi, 1998). An example of such a procedure is given in Figure 2. 3.4 Network of term candidates The sub-groups generated by the Parsing module, together with the maximal-length noun phrases extracted by the Splitting module, are the term candidates produced by the Term extraction tool. This set of term candidates is represented as a network: each multi-word term candidate is connected to its head constituent and to its expansion constituent by syntactic decomposition links. An excerpt of a network of term candidates is given Figure 3. Vertical and horizontal links are syntactic decomposition links produced by the Term Extraction tool. The oblique link is a syntactic variation link added by the Term Clustering tool. The building of the network is especially important for the purpose of term acquisition. The average number of multi-word term candidates is 8,000 for a 100,000 word corpus. The feedback of several experiments in which our Term Extraction tool was used shows that the more structured the set of term candidates is, the more efficiently the validation task is performed. For example, the structuring through syntactic decomposition allows the system to underscore lists of terms that share the same term either in head position or in expansion position. Such paradigmatic series are frequent in term banks, and initiating the validation task by analyzing such lists appears to be a very efficient validation strategy. This paper proposes a novel technique for enriching the network of term candidates through Figure 3: Excerpt of a network of term candidates. the addition of syntactic variation links to syntactic decomposition links. 4 Term Clustering 4.1 Adapting a Normalization Tool Term normalization is a procedure used in automatic indexing for conflating various term occurrences into unique canonical forms. More or less linguistically-oriented techniques are used in the literature for this task. Basic procedures such as (Dillon and Gray, 1983) rely on function word deletion, stemming, and alphabetical word For example, the index cattransformed into /ibrar such simplification techniques. In the platform presented in this paper, term is performed by shallow transformational parser which uses linguistic knowledge -about the possible morpho-syntactic transformations of canonical terms (Jacquemin et al., 1997). Through this technique syntactically and morphologically-related occurrences, such as de prix stabilization) and staleurs prix their prices), are conflated. variant extraction in from preceding works such as (Evans et al., 1991) because it relies on a shallow syntactic analysis of term variations instead of window-based measures of term overlaps. In (Sparck Jones and Tait, 1984) a knowledge-intensive technique is proposed for extracting term variations. This approach has however never been applied to large scale term extraction because it is based on a full semantic analysis of sentences. Our approach is more realistic because it does not involve large-scale knowledgeintensive interpretation of texts that is known to be unrealistic. approach to the clustering of term cancell bronchial cell Expansion link bronchial A2 cylindrical bronchial cell cylindrical cell cylindrical link -o as a</abstract>
<note confidence="0.842577">17 Proceedings of EACL &apos;99 Parsing rule Parse (2) Noun&apos; Prep Noun2 Adj Head: Noun&apos; Prep Noun2 Parse (1) Head: Nouni</note>
<address confidence="0.870747333333333">Exp.: Noun2 Exp.: Adj Exp.: Noun2 Adj Head: Noun2</address>
<email confidence="0.678581">Exp.:Adj</email>
<note confidence="0.938973">Disambiguation procedure: Look in the corpus for non ambiguous occurrences of the sub-groups: Adj (b) Nouni Adj (c) Prep Noun2 Then choose:</note>
<abstract confidence="0.982356706666667">if the sub-group (a) has been found, then choose Parse (1) else if the sub-groups (b) or (c) have been found, then choose Parse (2) else choose Parse (1) 2: ambiguous parsing rule and associated disambiguation procedure is to group the output of conflating term candidates with other term candidates instead of conflating corpus occurrences with controlled terms. Our technique can be seen as a kind of self-indexing in which term candidates indexed by themselves through the purpose of conflating candidates that are variof each other. Thus, the term candidate celbronchique cylindrique bronchial is a variant of the other candidate cell) because an adjectival modifier is inserted in the first term. Through the self-indexing procedure these two candidates belong to the same cluster. 4.2 Types of Syntactic Variation Rules Because of this original framework, specific variations patterns were designed in order to capture inter-term variations. In this study, we restrict ourselves to syntactic variations and ignore morphological modifications. The variations patterns can be classified into the following two families: insertion of modifiers insertion of one or more modifiers inside a noun phrase structure. For instance the following transformation NAInsAj: Adj2 iAdj2 describes the insertion of one to three adjectival modifiers inside a Noun-Adjective structure in French. Through this transformathe term candidate bronchique bronchial cell) is recognized as a variant of the term candidate cylindrique cell). Other internal modifications account for adverbial and prepositional modifiers. Preposition switch &amp; determiner insertion In French, terms, compounds, and noun phrases have comparable structures: generally a head noun followed by adjectival or prepositional modifiers. Such terms may vary through lexical changes without significant structural modifications. For example NPNSynt: Prep2 ((Prep Det?)?) preposition suppressions such de collagenel fibre collagene (collagen fiber), additions of determiners, and/or switches such as de revetement en surface coating). The complete rule set is shown in Table 1. Each transformation given in the first column conflates the term structure given in the second column and the term structure given in the third column. 4.3 Clustering output of a set of links between pairs of term candidates in which the target candidate is a variant of the source candidate. In order to facilitate the validation of links by the expert, this output is converted into clusters of term candidates. The syntactic variation links can be as the edges of an undirected graph whose nodes are the term candidates. A node n1 a term is connected to a node representing t2 if and only if there is a transfor- T such that = t2 or T(r2) = connected subgraph of considered as a cluster of term candidates likely to correspond similar concepts. (A connected subgraph is</abstract>
<note confidence="0.902113176470588">18 Proceedings of EACL &apos;99 Table 1: Syntactic variation rules exploited by the Term Clustering tool. Ident. Base term Variant NAInsAv NAInsAj NAInsN Adj2 Adv) Adj2 Adj2 Adj2 Noun) Adj2 ANInsAv Adji Noun2 (Adv) Adji Noun3 NPNSynt NPNInsAj NPNInsN Noun&apos; Prep2 Noun3 ((Prep Noun3 Prep2 Noun3 Prep Adj)0-3 ) Noun3 (Prep Noun (Prep Adj)0-3 ) Noun3 Prep2 Noun3 Noun&apos; Prep2 Det4 Noun3 Nouni ((Prep Noun3 Nonni Det4 Noun3 Prep Adj)0-3 ) Noun3 Noun&apos; Prep2 Det4 Noun3 (Prep Noun (Prep Adj)0-3 ) Noun3</note>
<abstract confidence="0.934193114942528">nucleole souvent proiminent nucleole central proeminent A sample 4-term cluster. that for every pair of nodes (ni , in gi, there exists a path from ni to n2.) example, =nucleole proerninent (prominucleolus), =nucleole central proeminent central nucleolus), t3 souproeminent prominent nucleoand =nucleole parfois proeminent (someprominent nucleolus) are four term candidates that build a star-shaped 4-word cluster illustrated by Figure 4. Each edge is labelled with the syntactic transformation T that maps one of the nodes to the other. 5 Experiments Experiments were made on three different corpora in Table The first two lines of Table 2 size of the corpora and the number term candidates extracted by these corpora. The third and fourth lines show the number of links between term candidates exby the number of connected subgraphs corresponding to these links. Finally, the last two lines report statistics on the size of the and the ratio of term candidates that be- Table 3: Frequencies of syntactic variations. [Menel.] [Brouss.] [DER] NAInsAv 21% 30% 1% NAInsAj 33% 25% 5% NAInsN 23% 21% 13% ANInsAv 3% 3% 0% NPNSynt 2% 2% 18% NPNInsAj 6% 11% 8% NPNInsN 1% 2% 11% NPDNSynt 1% 2% 22% NPDNInsAj 8% 2% 11% NPDNInsN 2% 2% 11% Total 100% 100% long to one of the subgraphs produced by the clustering algorithm. Although the variation rules implemented in the Term Structuring tool are rather restrictive (only syntactic insertion has been taken into account), the number of links added to the network of term candidates is noticeably high. An average rate of 10% of multi-word term candidates by to one of the clusters resulting from the recognition of term variants S T R. Frequencies of syntactic variations are reported in Table 3. A screen-shot showing the type of validation that is proposed to the expert is given by Figure 5. 6 Expert Evaluation Evaluation was performed by three experts, one in each domain represented by each corpus. These had already been involved in the con- NAInsAv NAInsAj ti nucleole proeminent NAInsAv • nucleole patfois proiminent t2 19 Proceedings of EACL &apos;99 Table 2: The three corpora exploited in the experiments. [Broussais] [DER] [Menelas] Domain anatomy pathology nuclear engineering coronarian diseases Type of documents medical reports technical reports medical files Number of words 40,000 230,000 110,000 Number of multi-word term candidates 3,439 14,037 10,155 Number of variation links 240 785 634 Number of clusters 168 556 448 Maximal size of the clusters 10 13 13 Number of term candidates 438 (12.7%) 1,349 (9.6%) 1,173 (11.6%) belonging to one cluster Mode Formuletire 5: The interface for cluster validation focalement hyperplasique vy revetement pleural hyperplasique evetement focalement hyperplasique evetement hyperplasique evetement pleural hyperplasique evetement epithelial regulier focalement hyperplasique evetement epithelium souvent hyperplasique revetement epithelium souvent hyperplasique revetement epithelial reguller focalement hyperplasique element chscretement hyperplasique 20 Proceedings of EACL &apos;99 struction of terminological products through the analysis of the three corpora used in our experiments: an ontology for a case-memory system dedicated to the diagnosis support in pathology ([Broussais]), a semantic dictionary for the Menelas Natural Language Understanding system ([Menelas]), and a structured thesaurus for a computer-assisted technical writing tool ([DER]). The precision rates are very satisfactory (from 93% to 98% corresponding to error rates of 7% and 2% given in the last line of Table 4), and show that the proposed method must be considered as an important progress in corpus-based terminology. Only few links are judged as conceptually irreleby the experts. For example, d&apos;embole of a tumorous embolus) is not as a correct variant of tumorale (image of a tumor) because the first occurrence refers to an embolus while the second one refers to a tumor. The experts were required to assess the proposed links and, in case of positive reply, they were required to provide a judgment about the actual conceptual relation between the connected terms. Although they performed the validation independently, the three experts have proposed very similar types of conceptual relations between term candidates connected by syntactic variation links. At a coarse-grained level, they proposed the same three types of conceptual relations: connected terms are considas equivalent by the expert: embolus)/ vascutumorale tumorous embolus). The preceding example corresponds to a frequent situation of elliptic synonymy: the noof metonymy 1989). In the medical domain, it is a common knowlthat an tumorale an tumorale, everyone knows that oil a synonym of seed oil. relation of the two terms denotes a concept that is finer than other one: epitheliale cylindrique (cylindrical epithelial cell) is a specific type cylindrique cell). relation in the preceding case, there is a non-synonymous semantic relation between the two terms. One of them denotes a concept richer than the other one because it an additional attributes: a voirregulier irregular nucleus) a irregulier nucleus) that additionally 7 Future Work This study shows that the clustering of term candidates through term normalization is a powerful technique for enriching the network of term candidates produced by a Term Extraction tool such as LEXTER. In our approach, term normalization is performed through the conflation of specific term variants. We have focused on syntactic variants that involve structural modifications (mainly modifier insertions). As reported in (Jacquemin, 1999), morphological and semantic variations are two other important families of term variations can also be extracted by will be accounted for in order to enhance the number of clustered term candidates. It is our purpose to focus on these two types of variants in the near future. Acknowledgement The authors would like to thank the experts for their comments and their evaluations of results: Pierre Zweigenbaum [Menelas], Christel Le Bozec and Marie-Christine Jaulent (AP/HP) on [Broussais], and Henry Boccon-Gibod (DER-EDF) on [DER]. We are also grateful to Henry Boccon-Gibod (DER-EDF) for his support to this work. This work was partially funded by Electricitie de France.</abstract>
<note confidence="0.9635164">References Didier Bourigault, Isabelle Gonzalez-Mullier, and Cecile Gros. 1996. Lexter, a natural language processing tool for terminology extraction. In Seventh EURALEX International Congress on (EURALEX96), Part II, 771-779. Didier Bourigault. 1993. An endogeneous corpusbased method for structural noun phrase disam- In 6th Conference of the European Chapter of the Association for Com- Linguistics (EACL&apos;93), Utrecht. Caroline Brun. 1998. Terminology finite-state for computational lfg. In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguis- (COLING-ACL &apos;98), Montreal. 21 Proceedings of EACL &apos;99 Table 4: Results of the validation. [Broussais] [Menelas] [DER] Number of variation links proposed by the system Number of variation links validated by the expert 240 634 785</note>
<phone confidence="0.822138">240 227 344</phone>
<abstract confidence="0.945241">Types of conceptual relation given by the expert synonymy 44 (18%) 136 (40%) generic/specific 96 (40%) 147 (65%) 121 (35%) attributive 96 (40%) 61 (27%) 62 (18%) non relevant 4 (2%) 5 (2%) 25 (7%) Beatrice Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In Judith L. Klavans</abstract>
<author confidence="0.757278">Philip Resnik</author>
<author confidence="0.757278">Balancing Act editors</author>
<affiliation confidence="0.841342">Combining Symbolic and Statistical Approaches</affiliation>
<address confidence="0.755611">Language, 49-66. MIT Press, Cambridge, MA.</address>
<degree confidence="0.502393">Martin Dillon and Ann S. Gray. 1983. FASIT : A fully automatic syntactically based indexing of the American Society for</degree>
<title confidence="0.879286">Science,</title>
<author confidence="0.8909315">David A Evans</author>
<author confidence="0.8909315">Kimberly Ginther-Webster</author>
<author confidence="0.8909315">Mary Hart</author>
<author confidence="0.8909315">Robert G Lefferts</author>
<author confidence="0.8909315">A Ira</author>
<note confidence="0.66513">Monarch. 1991. Automatic indexing using se- NLP and first-order thesauri. In Proceedings, Intelligent Multimedia Information Retrieval Systems and Management (RIA0&apos;91), pages 624-643, Barcelona. Gregory Grefenstette. 1992. A knowledge-poor technique for knowledge extraction from large In 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SI- Grefenstette. 1994. in Thesaurus Discovery. Academic Publisher, Boston, MA. Christian Jacquemin, Judith L. Klavans, and Evelyne Tzoukermann. 1997. Expansion of multiword terms for indexing and retrieval using and syntax. In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Com- Linguistics (ACL - EACL&apos;97), 24-31, Madrid. Christian Jacquemin. 1999. Syntagmatic and paradigmatic representations of term varia- In 37th Annual Meeting of the Association for Computational Linguistics &apos;99), of Maryland. John S. Justeson and Slava M. Katz. 1995. Tech-</note>
<abstract confidence="0.7101625">nical terminology: some linguistic properties an algorithm for identification in text. Nat- Language Engineering, Kleiber. 1989. est bronze de paul est bronzee. une approche referentielle analytique. In Harro Stammerjoeditor, Ve colloque internade linguistique slavo-romane, 109-</abstract>
<note confidence="0.844623076923077">134, Tubingen. Gunter Nair Verlag. Reprinted Colin, Paris, 1995. Adwait Ratnaparkhi. 1998. Statistical models for unsupervised prepositional phrase attach- In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Compu- Linguistics (COLING-ACL &apos;98), 1079-1085, Montreal. Karen Sparck Jones and John I. Tait. 1984. Autosearch term variant generation. Documentation, 22</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Didier Bourigault</author>
<author>Isabelle Gonzalez-Mullier</author>
<author>Cecile Gros</author>
</authors>
<title>Lexter, a natural language processing tool for terminology extraction.</title>
<date>1996</date>
<booktitle>In Seventh EURALEX International Congress on Lexicography (EURALEX96), Part II,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="2511" citStr="Bourigault et al., 1996" startWordPosition="364" endWordPosition="367">ssor. On the other hand, tools for automatic thesaurus construction focus on single-word terms for practical reasons. Since they cluster terms through statistical measures of context similarities, these tools exploit recurring situations. Since single-word terms denote broader concepts than multi-word terms, they appear more frequently in corpora and are therefore more appropriate for statistical clustering. The contribution of this paper is to propose an integrated platform for computer-aided term extraction and structuring that results from the combination of LEXTER, a Term Extraction tool (Bourigault et al., 1996), and FASTR1, a Term Normalization tool (Jacquemin et al., 1997). 2 Components of the Platform for Computer-Aided Terminology The platform for computer-aided terminology is organized as a chain of four modules and the corresponding flowchart is given by Figure 1. The modules are: POS tagging First the corpus is processed by Sylex, a Part-of-Speech tagger. Each word is unambiguously tagged and receives a single lemma. Term Extraction LEXTER, the term extraction tool acquires term candidates from the tagged corpus. In a first step, LEXTER exploits the part-of-speech categories for extracting max</context>
<context position="6468" citStr="Bourigault et al., 1996" startWordPosition="976" endWordPosition="979">(Grefenstette, 1992). In the case of LEXTER, the noun phrases which are isolated by splitting are not intermediary data; they are not used by any other automatic module in order to index or classify documents. The extracted noun phrases are term candidates which are proposed to the user. In such a situation, splitting must be performed with high precision. In order to process correctly some problematic splittings, such as coordinations, attributive past participles and sequences preposition + determiner, the system acquires and uses corpus-based selection restrictions of adjectives and nouns (Bourigault et al., 1996). For example, in order to disambiguate PPattachments, the system possesses a corpusbased list of adjectives which accept a prepositional argument built with the preposition a (at). These selectional restrictions are acquired through Corpus-Based Endogenous Learning (CBEL) as follows: During a first pass, all the adjectives in a predicative position followed by the preposition a are collected. During a second pass, each time a splitting rule has eliminated a sequence beginning with the preposition a, the preceding adjective is discarded from the list. Empirical analyses confirm the validity of</context>
</contexts>
<marker>Bourigault, Gonzalez-Mullier, Gros, 1996</marker>
<rawString>Didier Bourigault, Isabelle Gonzalez-Mullier, and Cecile Gros. 1996. Lexter, a natural language processing tool for terminology extraction. In Seventh EURALEX International Congress on Lexicography (EURALEX96), Part II, pages 771-779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Bourigault</author>
</authors>
<title>An endogeneous corpusbased method for structural noun phrase disambiguation.</title>
<date>1993</date>
<booktitle>In Proceedings, 6th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;93),</booktitle>
<pages>81--86</pages>
<location>Utrecht.</location>
<contexts>
<context position="961" citStr="Bourigault, 1993" startWordPosition="137" endWordPosition="138">novel technique for automatic thesaurus construction is proposed. It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions). Experiments performed on corpora in three technical domains yield clusters of term candidates with precision rates between 93% and 98%. 1 Computational Terminology In the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996; Brun, 1998) and tools for automatic thesaurus construction (Grefenstette, 1994). These tools are expected to be complementary in the sense that the links and clusters proposed in automatic thesaurus construction can be exploited for structuring the term candidates produced by the automatic term extractors. In fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic thesaurus construction yield clusters of single-word terms. On the one hand, term extractors focus on multi-word terms for ontolog</context>
<context position="8398" citStr="Bourigault, 1993" startWordPosition="1270" endWordPosition="1271">hrase bronchial cell), and a constituent in expansion position (e.g. cylindrical in the noun phrase cylindrical bronchial cell, and bronchial in the noun phrase bronchial cell). The Parsing module exploits rules in order to extract two subgroups from each MLNP, one in head-position and the other one in expansion position. Most of MLNP sequences are ambiguous. Two (or more) binary decompositions compete, corresponding to several possibilities of prepositional phrase or adjective attachment. The disambiguation is performed by a corpus-based method which relies on endogenous learning procedures (Bourigault, 1993; Ratnaparkhi, 1998). An example of such a procedure is given in Figure 2. 3.4 Network of term candidates The sub-groups generated by the Parsing module, together with the maximal-length noun phrases extracted by the Splitting module, are the term candidates produced by the Term extraction tool. This set of term candidates is represented as a network: each multi-word term candidate is connected to its head constituent and to its expansion constituent by syntactic decomposition links. An excerpt of a network of term candidates is given in Figure 3. Vertical and horizontal links are syntactic de</context>
</contexts>
<marker>Bourigault, 1993</marker>
<rawString>Didier Bourigault. 1993. An endogeneous corpusbased method for structural noun phrase disambiguation. In Proceedings, 6th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;93), pages 81-86, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
</authors>
<title>Terminology finite-state preprocessing for computational lfg.</title>
<date>1998</date>
<booktitle>In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL &apos;98),</booktitle>
<pages>196--200</pages>
<location>Montreal.</location>
<contexts>
<context position="1013" citStr="Brun, 1998" startWordPosition="145" endWordPosition="146">roposed. It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions). Experiments performed on corpora in three technical domains yield clusters of term candidates with precision rates between 93% and 98%. 1 Computational Terminology In the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996; Brun, 1998) and tools for automatic thesaurus construction (Grefenstette, 1994). These tools are expected to be complementary in the sense that the links and clusters proposed in automatic thesaurus construction can be exploited for structuring the term candidates produced by the automatic term extractors. In fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic thesaurus construction yield clusters of single-word terms. On the one hand, term extractors focus on multi-word terms for ontological motivations: single-word terms are too polysemo</context>
</contexts>
<marker>Brun, 1998</marker>
<rawString>Caroline Brun. 1998. Terminology finite-state preprocessing for computational lfg. In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL &apos;98), pages 196-200, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology.</title>
<date>1996</date>
<booktitle>The Balancing Act: Combining Symbolic and Statistical Approaches to Language,</booktitle>
<pages>49--66</pages>
<editor>In Judith L. Klavans and Philip Resnik, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1000" citStr="Daille, 1996" startWordPosition="143" endWordPosition="144">struction is proposed. It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions). Experiments performed on corpora in three technical domains yield clusters of term candidates with precision rates between 93% and 98%. 1 Computational Terminology In the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996; Brun, 1998) and tools for automatic thesaurus construction (Grefenstette, 1994). These tools are expected to be complementary in the sense that the links and clusters proposed in automatic thesaurus construction can be exploited for structuring the term candidates produced by the automatic term extractors. In fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic thesaurus construction yield clusters of single-word terms. On the one hand, term extractors focus on multi-word terms for ontological motivations: single-word terms are</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>Beatrice Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In Judith L. Klavans and Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Approaches to Language, pages 49-66. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Dillon</author>
<author>Ann S Gray</author>
</authors>
<title>FASIT : A fully automatic syntactically based indexing system.</title>
<date>1983</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>34--2</pages>
<contexts>
<context position="10391" citStr="Dillon and Gray, 1983" startWordPosition="1587" endWordPosition="1590">on task by analyzing such lists appears to be a very efficient validation strategy. This paper proposes a novel technique for enriching the network of term candidates through Figure 3: Excerpt of a network of term candidates. the addition of syntactic variation links to syntactic decomposition links. 4 Term Clustering 4.1 Adapting a Normalization Tool Term normalization is a procedure used in automatic indexing for conflating various term occurrences into unique canonical forms. More or less linguistically-oriented techniques are used in the literature for this task. Basic procedures such as (Dillon and Gray, 1983) rely on function word deletion, stemming, and alphabetical word reordering. For example, the index library catalogs is transformed into catalog /ibrar through such simplification techniques. In the platform presented in this paper, term normalization is performed by FASTR, a shallow transformational parser which uses linguistic knowledge -about the possible morpho-syntactic transformations of canonical terms (Jacquemin et al., 1997). Through this technique syntactically and morphologically-related occurrences, such as stabilisation de prix (price stabilization) and stabiliser leurs prix (stab</context>
</contexts>
<marker>Dillon, Gray, 1983</marker>
<rawString>Martin Dillon and Ann S. Gray. 1983. FASIT : A fully automatic syntactically based indexing system. Journal of the American Society for Information Science, 34(2):99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Evans</author>
<author>Kimberly Ginther-Webster</author>
<author>Mary Hart</author>
<author>Robert G Lefferts</author>
<author>Ira A Monarch</author>
</authors>
<title>Automatic indexing using selective NLP and first-order thesauri.</title>
<date>1991</date>
<booktitle>In Proceedings, Intelligent Multimedia Information Retrieval Systems and Management (RIA0&apos;91),</booktitle>
<pages>624--643</pages>
<location>Barcelona.</location>
<contexts>
<context position="11117" citStr="Evans et al., 1991" startWordPosition="1687" endWordPosition="1690"> catalogs is transformed into catalog /ibrar through such simplification techniques. In the platform presented in this paper, term normalization is performed by FASTR, a shallow transformational parser which uses linguistic knowledge -about the possible morpho-syntactic transformations of canonical terms (Jacquemin et al., 1997). Through this technique syntactically and morphologically-related occurrences, such as stabilisation de prix (price stabilization) and stabiliser leurs prix (stabilize their prices), are conflated. Term variant extraction in FASTR differs from preceding works such as (Evans et al., 1991) because it relies on a shallow syntactic analysis of term variations instead of window-based measures of term overlaps. In (Sparck Jones and Tait, 1984) a knowledge-intensive technique is proposed for extracting term variations. This approach has however never been applied to large scale term extraction because it is based on a full semantic analysis of sentences. Our approach is more realistic because it does not involve large-scale knowledgeintensive interpretation of texts that is known to be unrealistic. Our approach to the clustering of term cancell N3 bronchial cell A2N3 Expansion link </context>
</contexts>
<marker>Evans, Ginther-Webster, Hart, Lefferts, Monarch, 1991</marker>
<rawString>David A. Evans, Kimberly Ginther-Webster, Mary Hart, Robert G. Lefferts, and Ira A. Monarch. 1991. Automatic indexing using selective NLP and first-order thesauri. In Proceedings, Intelligent Multimedia Information Retrieval Systems and Management (RIA0&apos;91), pages 624-643, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>A knowledge-poor technique for knowledge extraction from large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings, 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;92),</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="5864" citStr="Grefenstette, 1992" startWordPosition="883" endWordPosition="884">un (ureteral wall). Because of this similarity, the detection of terms and their variants in French is more difficult than in the English language. The input of our term extraction tool is an unambiguously tagged corpus. The extraction process is composed of two main steps: Splitting and Parsing. 3.2 Splitting The techniques of shallow parsing implemented in the Splitting module detect morpho-syntactical patterns that cannot be parts of terminological noun phrases and that are therefore likely to indicate noun phrases boundaries. Splitting techniques are used in other shallow parsers such as (Grefenstette, 1992). In the case of LEXTER, the noun phrases which are isolated by splitting are not intermediary data; they are not used by any other automatic module in order to index or classify documents. The extracted noun phrases are term candidates which are proposed to the user. In such a situation, splitting must be performed with high precision. In order to process correctly some problematic splittings, such as coordinations, attributive past participles and sequences preposition + determiner, the system acquires and uses corpus-based selection restrictions of adjectives and nouns (Bourigault et al., 1</context>
</contexts>
<marker>Grefenstette, 1992</marker>
<rawString>Gregory Grefenstette. 1992. A knowledge-poor technique for knowledge extraction from large corpora. In Proceedings, 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;92), Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publisher,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="1081" citStr="Grefenstette, 1994" startWordPosition="154" endWordPosition="155">(1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions). Experiments performed on corpora in three technical domains yield clusters of term candidates with precision rates between 93% and 98%. 1 Computational Terminology In the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996; Brun, 1998) and tools for automatic thesaurus construction (Grefenstette, 1994). These tools are expected to be complementary in the sense that the links and clusters proposed in automatic thesaurus construction can be exploited for structuring the term candidates produced by the automatic term extractors. In fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic thesaurus construction yield clusters of single-word terms. On the one hand, term extractors focus on multi-word terms for ontological motivations: single-word terms are too polysemous and too generic and it is therefore necessary to provide the user</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publisher, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
<author>Judith L Klavans</author>
<author>Evelyne Tzoukermann</author>
</authors>
<title>Expansion of multiword terms for indexing and retrieval using morphology and syntax.</title>
<date>1997</date>
<booktitle>In Proceedings, 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL - EACL&apos;97),</booktitle>
<pages>24--31</pages>
<location>Madrid.</location>
<contexts>
<context position="2575" citStr="Jacquemin et al., 1997" startWordPosition="374" endWordPosition="377">on focus on single-word terms for practical reasons. Since they cluster terms through statistical measures of context similarities, these tools exploit recurring situations. Since single-word terms denote broader concepts than multi-word terms, they appear more frequently in corpora and are therefore more appropriate for statistical clustering. The contribution of this paper is to propose an integrated platform for computer-aided term extraction and structuring that results from the combination of LEXTER, a Term Extraction tool (Bourigault et al., 1996), and FASTR1, a Term Normalization tool (Jacquemin et al., 1997). 2 Components of the Platform for Computer-Aided Terminology The platform for computer-aided terminology is organized as a chain of four modules and the corresponding flowchart is given by Figure 1. The modules are: POS tagging First the corpus is processed by Sylex, a Part-of-Speech tagger. Each word is unambiguously tagged and receives a single lemma. Term Extraction LEXTER, the term extraction tool acquires term candidates from the tagged corpus. In a first step, LEXTER exploits the part-of-speech categories for extracting maximal-length noun phrases. It relies on makers of frontiers toget</context>
<context position="10828" citStr="Jacquemin et al., 1997" startWordPosition="1647" endWordPosition="1650">s term occurrences into unique canonical forms. More or less linguistically-oriented techniques are used in the literature for this task. Basic procedures such as (Dillon and Gray, 1983) rely on function word deletion, stemming, and alphabetical word reordering. For example, the index library catalogs is transformed into catalog /ibrar through such simplification techniques. In the platform presented in this paper, term normalization is performed by FASTR, a shallow transformational parser which uses linguistic knowledge -about the possible morpho-syntactic transformations of canonical terms (Jacquemin et al., 1997). Through this technique syntactically and morphologically-related occurrences, such as stabilisation de prix (price stabilization) and stabiliser leurs prix (stabilize their prices), are conflated. Term variant extraction in FASTR differs from preceding works such as (Evans et al., 1991) because it relies on a shallow syntactic analysis of term variations instead of window-based measures of term overlaps. In (Sparck Jones and Tait, 1984) a knowledge-intensive technique is proposed for extracting term variations. This approach has however never been applied to large scale term extraction becau</context>
</contexts>
<marker>Jacquemin, Klavans, Tzoukermann, 1997</marker>
<rawString>Christian Jacquemin, Judith L. Klavans, and Evelyne Tzoukermann. 1997. Expansion of multiword terms for indexing and retrieval using morphology and syntax. In Proceedings, 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL - EACL&apos;97), pages 24-31, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>Syntagmatic and paradigmatic representations of term variation.</title>
<date>1999</date>
<booktitle>In Proceedings, 37th Annual Meeting of the Association for Computational Linguistics (ACL &apos;99),</booktitle>
<institution>University of Maryland.</institution>
<contexts>
<context position="22582" citStr="Jacquemin, 1999" startWordPosition="3494" endWordPosition="3495">additional attributes: a noyau volumineux irregulier (large irregular nucleus) is a noyau irregulier (irregular nucleus) that is additionally volumineux (large). 7 Future Work This study shows that the clustering of term candidates through term normalization is a powerful technique for enriching the network of term candidates produced by a Term Extraction tool such as LEXTER. In our approach, term normalization is performed through the conflation of specific term variants. We have focused on syntactic variants that involve structural modifications (mainly modifier insertions). As reported in (Jacquemin, 1999), morphological and semantic variations are two other important families of term variations which can also be extracted by FASTR. They will be accounted for in order to enhance the number of clustered term candidates. It is our purpose to focus on these two types of variants in the near future. Acknowledgement The authors would like to thank the experts for their comments and their evaluations of our results: Pierre Zweigenbaum (AP/HP) on [Menelas], Christel Le Bozec and Marie-Christine Jaulent (AP/HP) on [Broussais], and Henry Boccon-Gibod (DER-EDF) on [DER]. We are also grateful to Henry Boc</context>
</contexts>
<marker>Jacquemin, 1999</marker>
<rawString>Christian Jacquemin. 1999. Syntagmatic and paradigmatic representations of term variation. In Proceedings, 37th Annual Meeting of the Association for Computational Linguistics (ACL &apos;99), University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Justeson</author>
<author>Slava M Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<pages>1--1</pages>
<contexts>
<context position="986" citStr="Justeson and Katz, 1995" startWordPosition="139" endWordPosition="142">r automatic thesaurus construction is proposed. It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions). Experiments performed on corpora in three technical domains yield clusters of term candidates with precision rates between 93% and 98%. 1 Computational Terminology In the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996; Brun, 1998) and tools for automatic thesaurus construction (Grefenstette, 1994). These tools are expected to be complementary in the sense that the links and clusters proposed in automatic thesaurus construction can be exploited for structuring the term candidates produced by the automatic term extractors. In fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic thesaurus construction yield clusters of single-word terms. On the one hand, term extractors focus on multi-word terms for ontological motivations: single-</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>John S. Justeson and Slava M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1(1):9-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kleiber</author>
</authors>
<title>Paul est bronze versus la peau de paul est bronzee. Contre une approche referentielle analytique.</title>
<date>1989</date>
<booktitle>In Harro Stammerjohann, editor, Proceedings, Ve colloque international de linguistique slavo-romane,</booktitle>
<pages>109--134</pages>
<editor>Tubingen. Gunter Nair Verlag. Reprinted in Nominales, A. Colin,</editor>
<location>Paris,</location>
<contexts>
<context position="21370" citStr="Kleiber, 1989" startWordPosition="3306" endWordPosition="3307"> relation between the connected terms. Although they performed the validation independently, the three experts have proposed very similar types of conceptual relations between term candidates connected by syntactic variation links. At a coarse-grained level, they proposed the same three types of conceptual relations: Synonymy Both connected terms are considered as equivalent by the expert: embole tumorale (tumorous embolus)/ embole vasculaire tumorale (vascular tumorous embolus). The preceding example corresponds to a frequent situation of elliptic synonymy: the notion of integrated metonymy (Kleiber, 1989). In the medical domain, it is a common knowledge that an embole tumorale is an embole vasculaire tumorale, as everyone knows that sunflower oil is a synonym of sunflower seed oil. Generic/specific relation One of the two terms denotes a concept that is finer than the other one: cellule epitheliale cylindrique (cylindrical epithelial cell) is a specific type of cellule cylindrique (cylindrical cell). Attributive relation As in the preceding case, there is a non-synonymous semantic relation between the two terms. One of them denotes a concept richer than the other one because it carries an addi</context>
</contexts>
<marker>Kleiber, 1989</marker>
<rawString>George Kleiber. 1989. Paul est bronze versus la peau de paul est bronzee. Contre une approche referentielle analytique. In Harro Stammerjohann, editor, Proceedings, Ve colloque international de linguistique slavo-romane, pages 109-134, Tubingen. Gunter Nair Verlag. Reprinted in Nominales, A. Colin, Paris, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Statistical models for unsupervised prepositional phrase attachment.</title>
<date>1998</date>
<booktitle>In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL &apos;98),</booktitle>
<pages>1079--1085</pages>
<location>Montreal.</location>
<contexts>
<context position="8418" citStr="Ratnaparkhi, 1998" startWordPosition="1272" endWordPosition="1274">ll), and a constituent in expansion position (e.g. cylindrical in the noun phrase cylindrical bronchial cell, and bronchial in the noun phrase bronchial cell). The Parsing module exploits rules in order to extract two subgroups from each MLNP, one in head-position and the other one in expansion position. Most of MLNP sequences are ambiguous. Two (or more) binary decompositions compete, corresponding to several possibilities of prepositional phrase or adjective attachment. The disambiguation is performed by a corpus-based method which relies on endogenous learning procedures (Bourigault, 1993; Ratnaparkhi, 1998). An example of such a procedure is given in Figure 2. 3.4 Network of term candidates The sub-groups generated by the Parsing module, together with the maximal-length noun phrases extracted by the Splitting module, are the term candidates produced by the Term extraction tool. This set of term candidates is represented as a network: each multi-word term candidate is connected to its head constituent and to its expansion constituent by syntactic decomposition links. An excerpt of a network of term candidates is given in Figure 3. Vertical and horizontal links are syntactic decomposition links pr</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Statistical models for unsupervised prepositional phrase attachment. In Proceedings, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL &apos;98), pages 1079-1085, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
<author>John I Tait</author>
</authors>
<title>Automatic search term variant generation.</title>
<date>1984</date>
<journal>Journal of Documentation,</journal>
<pages>40--1</pages>
<contexts>
<context position="11270" citStr="Jones and Tait, 1984" startWordPosition="1712" endWordPosition="1715">formed by FASTR, a shallow transformational parser which uses linguistic knowledge -about the possible morpho-syntactic transformations of canonical terms (Jacquemin et al., 1997). Through this technique syntactically and morphologically-related occurrences, such as stabilisation de prix (price stabilization) and stabiliser leurs prix (stabilize their prices), are conflated. Term variant extraction in FASTR differs from preceding works such as (Evans et al., 1991) because it relies on a shallow syntactic analysis of term variations instead of window-based measures of term overlaps. In (Sparck Jones and Tait, 1984) a knowledge-intensive technique is proposed for extracting term variations. This approach has however never been applied to large scale term extraction because it is based on a full semantic analysis of sentences. Our approach is more realistic because it does not involve large-scale knowledgeintensive interpretation of texts that is known to be unrealistic. Our approach to the clustering of term cancell N3 bronchial cell A2N3 Expansion link bronchial A2 cylindrical bronchial cell A, A2 N3 cylindrical cell A IN3 cylindrical Expansion link A1 -o as a) 17 Proceedings of EACL &apos;99 Parsing rule Pa</context>
</contexts>
<marker>Jones, Tait, 1984</marker>
<rawString>Karen Sparck Jones and John I. Tait. 1984. Automatic search term variant generation. Journal of Documentation, 40(1):50-66.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>