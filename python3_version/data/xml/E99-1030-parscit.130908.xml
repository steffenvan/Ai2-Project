<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019848">
<note confidence="0.71367">
Proceedings of EACL &apos;99
</note>
<title confidence="0.997733333333333">
The Development of Lexical Resources
for Information Extraction from Text
Combining WordNet and Dewey Decimal Classification*
</title>
<author confidence="0.967962">
Gabriela Cavaglia
</author>
<affiliation confidence="0.891595">
ITC-irst Centro per la Ricerca Scientifica e Tecnologica
</affiliation>
<address confidence="0.954145">
via Sommarive, 18
38050 Povo (TN), ITALY
</address>
<email confidence="0.999747">
e-mail: cavaglia@irst.itc.it
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999196">
Lexicon definition is one of the main bot-
tlenecks in the development of new am
plications in the field of Information Ex-
traction from text. Generic resources
(e.g., lexical databases) are promising for
reducing the cost of specific lexica defi-
nition, but they introduce lexical ambi-
guity. This paper proposes a methodol-
ogy for building application-specific lex-
ica by using WordNet. Lexical ambiguity
is kept under control by marking synsets
in WordNet with field labels taken from
the Dewey Decimal Classification.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882489361702">
One of the current issues in Information Extrac-
tion (IE) is efficient transportability, as the cost
of new applications is one of the factors limiting
the market. The lexicon definition process is cur-
rently one of the main bottlenecks in producing
applications. As a matter of fact the necessary lex-
icon for an average application is generally large
(hundreds to thousands of words) and most lexical
information is not transportable across domains.
The problem of lexicon transport is worsened by
the growing degree of lexicalization of IE systems:
nowadays several successful systems adopt lexical
rules at many levels.
The IE research mainstream focused essentially
on the definition of lexica starting from a corpus
sample (Riloff, 1993; Grishman, 1997) with the
implicit assumption that a corpus provided for an
application is representative of the whole applica-
&amp;quot;This work was carried on at ITC-IRST as part of
the author&apos;s dissertation for the degree in Philosophy
(University of Turin, supervisor: Carla Bazzanella).
The author wants to thank her supervisor at ITC-
IRST, Fabio Ciravegna, for his constant help. Alberto
Lavelli provided valuable comments to the paper.
tion requirement. Unfortunately one of the cur-
rent trends in IE is the progressive reduction of
the size of training corpora: e.g., from the 1,000
texts of the MUC-5 (MUC-5, 1993) to the 100
texts in MUC-6 (MUC-6, 1995). When the cor-
pus size is limited, the assumption of lexical rep-
resentativeness of the sample corpus may not hold
any longer, and the problem of producing a repre-
sentative lexicon starting from the corpus lexicon
arises (Grishman, 1995).
Generic resources are interesting as they con-
tain (among others) most of the terms necessary
for an IE application. Nevertheless up to now
the use of generic resources within IE system has
been limited for two main reasons. First the in-
formation associated to each term is often not de-
tailed enough for describing the relations neces-
sary for a IE lexicon; secondly the presence of a
large amount of lexical polysemy.
In this paper we propose a methodology for
semi-automatically developing the relevant part of
a lexicon .(foreground lexicon) for IE applications
by using both a small corpus and WordNet.
</bodyText>
<sectionHeader confidence="0.955889" genericHeader="method">
2 Developing IE Lexical Resources
</sectionHeader>
<bodyText confidence="0.853424">
Lexical information in IE can be divided into three
sources of information (Kilgarriff, 1997):
</bodyText>
<listItem confidence="0.9952236">
• an ontology, i.e. the templates to be filled;
• the foreground lexicon (FL), i.e. the terms
tightly bound to the ontology;
• the background lexicon (BL), i.e. the terms
not related or loosely related to the ontology.
</listItem>
<bodyText confidence="0.95105525">
In this paper we focus on FL only.
The FL has generally a limited size with re-
spect to the average dictionary of a language; its
dimension depends on each application needs, but
it is generally limited to some hundreds of words.
The level of quantitative and qualitative informa-
tion for each entry in the FL can be very high
and it is not transportable across domains and
</bodyText>
<page confidence="0.99662">
225
</page>
<bodyText confidence="0.989254642857143">
Proceedings of EACL &apos;99
applications, as it contains the mapping between
the entries and the ontology. Generic dictionaries
can contribute in identifying entries for the FL,
but generally do not provide useful information
for the mapping with the ontology. This map-
ping between words and ontology is generally to
be built by hand. Most of the time in transport-
ing the lexicon is spent in identifying and build-
ing FLs. Efficiently building FLs for applications
means building the right FL (or at least a reason-
able approximation of it) in a short time. The
right FL contains those words that are necessary
for the application and only those. The presence
of all the relevant terms should guarantee that the
information in the text is never lost; inserting just
the relevant terms allows to limit the development
effort, and should guarantee the system from noise
caused by spurious entries in the lexicon.
The BL could be seen as the complementary set
of the FL with respect to the generic language,
i.e. it contains all the words of the language that
do not belong to the FL. In general the quantity
of application specific information is small. Any
machine readable dictionary can be to some ex-
tent seen as a BL. The transport of BL to new
applications is not a problem, therefore it will not
be considered in this paper.
</bodyText>
<subsectionHeader confidence="0.999805">
2.1 Using Generic Lexical Resources
</subsectionHeader>
<bodyText confidence="0.999927">
We propose a development methodology for FLs
based on two steps:
</bodyText>
<listItem confidence="0.933318470588235">
• Bootstrapping: manual or semi-automatic
identification from the corpus of an initial lex-
icon (Core Lexicon), i.e. of the lexicon cover-
ing the corpus sample.
• Consolidation: extension of the Core Lexi-
con by using a generic dictionary in order to
completely cover the lexicon needed by the
application but not exhaustively represented
in the corpus sample.
We propose to use WordNet (Miller, 1990) as a
generic dictionary during the consolidation phase
because it can be profitably used for integrating
the Core Lexicon by adding for each term in a
semi-automatic way:
• its synonyms;
• hyponyms and (maybe) hypernyms;
• some coordinated terms.
</listItem>
<bodyText confidence="0.999491695652174">
As mentioned, there are two problems related
to the use of generic dictionaries with respect to
the IE needs.
First there is no clear way of extracting from
them the mapping between the FL and the ontol-
ogy; this is mainly due to a lack of information and
cannot in general be solved; generic lexica cannot
then be used during the bootstrapping phase to
generate the Core Lexicon.
Secondly experience showed that the lexical am-
biguity carried by generic dictionaries does not
allow their direct use in computational systems
(Basili and Pazienza, 1997; Morgan et al., 1995).
Even when they are used off-line, lexical ambigu-
ity can introduce so much noise (and then over-
head) in the lexical development process that their
use can be inconvenient from the point of view of
efficiency and effectiveness.
The next section explains how it is possible
to cope with lexical ambiguity in WordNet by
combining its information with another source of
information: the Dewey Decimal Classification
(DDC) (Dewey, 1989).
</bodyText>
<sectionHeader confidence="0.97034" genericHeader="method">
3 Reducing the lexical ambiguity
in WordNet
</sectionHeader>
<bodyText confidence="0.977167371428572">
The main problem with the use of WordNet is lex-
ical polysemy 1. Lexical polysemy is present when
a word is associated to many senses (synsets). In
general it is not easy to discriminate between dif-
ferent synsets. It is then necessary to find a way
for helping the lexicon developer in selecting the
correct synset for a word.
In order to cope with lexical polysemy, we pro-
pose to integrate WordNet synsets with an addi-
tional information: a set of field labels. Field la-
bels are indicators, generally used in dictionaries,
which provide information about the use of the
word in a semantic field. Semantic fields are sets
of words tied together by &amp;quot;similarity&amp;quot; covering the
most part of the lexical area of a specific domain.
Marking synsets with field labels has a clear ad-
vantage: in general, given a polysemous word in
WordNet and a particular field label, in most of
the cases the word is disambiguated. For example
Security is polysemous as it belongs to 9 different
synsets; only the second one is related to the eco-
nomic domain. If we mark this synset with the
field label Economy, it is possible to disambiguate
the term Security when analyzing texts in an eco-
nomic context. Note that WordNet being a hier-
archy, marking a synset with a field label means
also marking all its sub-hierarchy with such field
label. In the Security example, if we mark the sec-
ond synset with the field label Economy we also
associate the same field label to the synonym Cer-
tificate, to the 13 direct hyponyms and to the 27
lActually the problem is related to both polysemy
and omonymy. As WordNet does not distinguish be-
tween them, we will use the term polysemy for refer-
ring to both.
</bodyText>
<page confidence="0.996568">
226
</page>
<figureCaption confidence="0.996654">
Figure 1: An extract of the Dewey hierarchy relevant for the financial field
</figureCaption>
<figure confidence="0.998516810344828">
Proceedings of EACL &apos;99
333
Lands.
332.64 332.65
Exchange of sec. International exc.
3322
Spec.bank. instit.
332.3
Loan instiddion
332.8
Bite ciscount
332.9
Counterfeiting
330
Economics
320
Polk science
332
Finandal e.
380
Comm., trans.
370
Education
300 800 900
Liter. c rhet Geo. e histosy
Social sciences
337
Late:turn:mil e.
000
Generalities
310
Gm. statistics
390
2ustorn, folk.
200
Religion
100
Philos. e pays.
338
Production
331
Labors.
339
Macroeconomics
332.66
L bank
332.63
Form of L
332.67
L by field oft
332.62
Brokerage firms
332.1
Bank
332.6
Investiment
332.7
Credit
</figure>
<bodyText confidence="0.9966646">
indirect ones; moreover we can also inspect its co-
ordinated terms and assign the same label to 9 of
the 33 coordinate terms (and then to their direct
and indirect hyponyms). Marking is equivalent to
assigning WordNet synsets to sets each of them
referring to a particular semantic field. Marking
the structure allows us to solve the problem of
choosing which synsets are relevant for the do-
main. Associating a domain (e.g., finance) to one
or more field labels should allow us to determine
in principle the synsets relevant for the domain.
It is possible to greatly reduce the ambiguity im-
plied by the use of WordNet by finding the correct
set of field labels that cover all the WordNet hier-
archy in an uniform way. Therefore we can reduce
the overhead in building the FL using WordNet.
Our assumption is that using semantic fields
taken from the DDC2 , all the possible domains
can then be covered. This is because the first ten
classes of the DDC (an extract is shown in fig-
ure 1) exhaust the traditional academic disciplines
and so they also cover the generic knowledge of the
world. The integration consists in marking parts
of WordNet&apos;s hierarchy, i.e. some synsets, with
semantic labels taken from the DDC.
</bodyText>
<sectionHeader confidence="0.9789665" genericHeader="method">
4 The development cycle using
WN-FDDC
</sectionHeader>
<bodyText confidence="0.9991415">
The consolidation phase mentioned in section 2.1
can be integrated with the use of the WN+DDC
</bodyText>
<footnote confidence="0.9032896">
&apos;The Dewey Decimal Classification is the most
widely used library classification system in the world;
at the broadest level, it classifies concepts into ten
main classes, which cover the entire world of knowl-
edge.
</footnote>
<bodyText confidence="0.9995255">
as generic resource (see figure 2). Before starting
the development, the set of field labels relevant for
the application must be identified. Then the Core
Lexicon is identified in the usual way.
</bodyText>
<listItem confidence="0.909934875">
Using WN+DDC it is possible for each term in
the Core Lexicon to:
• identify the synsets the term belongs to; am-
biguities are reduced by applying the inter-
section of the field labels chosen for the cur-
rent application and those associated to the
possible synsets.
• integrate the Core Lexicon by adding, for
</listItem>
<bodyText confidence="0.949334307692308">
each term: synonyms in the synsets, hy-
ponyms and (maybe) hypernyms and some
coordinated terms.
The proposed methodology is corpus centered
(starting from the corpus analysis to build the
Core Lexicon) and can always be profitably ap-
plied. It also provides a criterion for building lex-
ical resources for specific domains. It can be ap-
plied in a semiautomatic way. It has the advan-
tage of using the information contained in Word-
Net for expanding the FL beyond the corpus lim-
itations, keeping under control the ambiguity im-
plied by the use of a generic resource.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999959714285714">
Up to now experiments have been carried on in
the financial domain, and in particular in the do-
main of bonds issued by banks. Experiments are
continuing. The construction of WN+DDC is a
long process that has to be done in general. Up
to now we have just started inserting in WordNet
the field labels that are interesting for the domain
</bodyText>
<page confidence="0.983232">
227
</page>
<figure confidence="0.9403045">
Proceedings of EACL &apos;99
WordNet+DDC
</figure>
<figureCaption confidence="0.999485">
Figure 2: Outline of the final Consolidation phase.
</figureCaption>
<bodyText confidence="0.99081125">
add hiponyms
hypernyms
under analysis. If the final experiments will con-
firm the usefulness of the approach, we will extend
the integration to the rest of the WordNet hierar-
chy. The final evaluation will include a compari-
son of the lexicon produced by using WN-I-DDC
with a normally developed lexicon in the domain
of bond-issue (Ciravegna et al., 1999). The eval-
uation will consider both quality and quantity of
terms and development time of the whole lexicon.
One of the issues that we are currently investi-
gating is that of choosing the correct set of field
labels from DDC: DDC is very detailed and it is
not worth integrating it completely with Word-
Net. It is necessary to individuate the correct set
of labels by pruning the DDC hierarchy at some
level. We are currently investigating the effective-
ness of just selecting the first three levels of the
hierarchy.
</bodyText>
<sectionHeader confidence="0.981395" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.9173564">
Roberto Basili and Maria Teresa Pazienza. 1997.
Lexical acquisition for information extraction.
In M. T. Pazienza, editor, Information Extrac-
tion: A multidisciplinary approach to an emerg-
ing information technology. Springer Verlag.
</bodyText>
<reference confidence="0.999446791666666">
Fabio Ciravegna, Alberto Lavelli, Nadia
Mana Luca Gilardoni, Silvia Mazza, Mas-
simo Ferraro, Johannes Matiasek, William
Black, Fabio Rinaldi, and David Mowatt.
1999. Facile: Classifying texts integrating
pattern matching and information extraction.
In Proceedings of the Sixteenth International
Joint Conference on Artificial Intelligence
(IJCA199). Stockholm, Sweden.
Melvil Dewey. 1989. Dewey Decimal Classifi-
cation and Relative Index. Edition 20. Forest
Press, Albany.
Ralph Grishman. 1995. The NYU system for
MUC-6 or where&apos;s syntax? In Sixth mes-
sage understanding conference MUC-6. Morgan
Kaufmann Publishers.
Ralph Grishman. 1997. Information extraction:
Techniques and challenges. In M. T. Pazienza,
editor, Information Extraction: a multidisci-
plinary approach to an emerging technology.
Springer Verlag.
Adam Kilgarriff. 1997. Foreground and back-
ground lexicons and word sense disambiguation
for information extraction. In International
Workshop on Lexically Driven Information Ex-
traction, Frascati, Italy.
G.A. Miller. 1990. Wordnet: an on-line lexical
database. International Journal of Lexicogra-
phy, 4(3).
Richard Morgan, Roberto Garigliano, Paul
Callaghan, Sanjay Poria, Mark Smith, Ag-
nieszka Urbanowicz, Russel Collingham,
Marco Costantino, Chris Cooper, and the
LOLITA Group. 1995. University of Durham:
Description of the LOLITA system as used
for MUC-6. In Sixth message understand-
ing conference MUC-6. Morgan Kaufmann
Publishers.
MUC-5. 1993. Fifth Message Understanding Con-
ference (MUC5). Morgan Kaufmann Publish-
ers, August.
MUC-6. 1995. Sixth Message Understanding
Conference (MUC-6). Morgan Kaufmann Pub-
lishers.
Ellen Riloff. 1993. Automatically constructing
a dictionary for information extraction tasks.
In Proceedings of the Eleventh National Confer-
ence on Artificial Intelligence, pages 811-816.
</reference>
<page confidence="0.997663">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520209">
<note confidence="0.806043">Proceedings of EACL &apos;99</note>
<title confidence="0.998072">The Development of Lexical Resources for Information Extraction from Text Combining WordNet and Dewey Decimal Classification*</title>
<author confidence="0.998719">Gabriela Cavaglia</author>
<affiliation confidence="0.984746">ITC-irst Centro per la Ricerca Scientifica e Tecnologica</affiliation>
<address confidence="0.9976185">via Sommarive, 18 38050 Povo (TN), ITALY</address>
<email confidence="0.999601">e-mail:cavaglia@irst.itc.it</email>
<abstract confidence="0.975619642857143">Lexicon definition is one of the main bottlenecks in the development of new am plications in the field of Information Extraction from text. Generic resources (e.g., lexical databases) are promising for reducing the cost of specific lexica definition, but they introduce lexical ambiguity. This paper proposes a methodology for building application-specific lexica by using WordNet. Lexical ambiguity is kept under control by marking synsets in WordNet with field labels taken from the Dewey Decimal Classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabio Ciravegna</author>
<author>Alberto Lavelli</author>
<author>Nadia Mana Luca Gilardoni</author>
<author>Silvia Mazza</author>
<author>Massimo Ferraro</author>
<author>Johannes Matiasek</author>
<author>William Black</author>
<author>Fabio Rinaldi</author>
<author>David Mowatt</author>
</authors>
<title>Facile: Classifying texts integrating pattern matching and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence (IJCA199).</booktitle>
<location>Stockholm,</location>
<contexts>
<context position="12661" citStr="Ciravegna et al., 1999" startWordPosition="2119" endWordPosition="2122">construction of WN+DDC is a long process that has to be done in general. Up to now we have just started inserting in WordNet the field labels that are interesting for the domain 227 Proceedings of EACL &apos;99 WordNet+DDC Figure 2: Outline of the final Consolidation phase. add hiponyms hypernyms under analysis. If the final experiments will confirm the usefulness of the approach, we will extend the integration to the rest of the WordNet hierarchy. The final evaluation will include a comparison of the lexicon produced by using WN-I-DDC with a normally developed lexicon in the domain of bond-issue (Ciravegna et al., 1999). The evaluation will consider both quality and quantity of terms and development time of the whole lexicon. One of the issues that we are currently investigating is that of choosing the correct set of field labels from DDC: DDC is very detailed and it is not worth integrating it completely with WordNet. It is necessary to individuate the correct set of labels by pruning the DDC hierarchy at some level. We are currently investigating the effectiveness of just selecting the first three levels of the hierarchy. References Roberto Basili and Maria Teresa Pazienza. 1997. Lexical acquisition for in</context>
</contexts>
<marker>Ciravegna, Lavelli, Gilardoni, Mazza, Ferraro, Matiasek, Black, Rinaldi, Mowatt, 1999</marker>
<rawString>Fabio Ciravegna, Alberto Lavelli, Nadia Mana Luca Gilardoni, Silvia Mazza, Massimo Ferraro, Johannes Matiasek, William Black, Fabio Rinaldi, and David Mowatt. 1999. Facile: Classifying texts integrating pattern matching and information extraction. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence (IJCA199). Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melvil Dewey</author>
</authors>
<title>Dewey Decimal Classification and Relative Index. Edition 20.</title>
<date>1989</date>
<publisher>Forest Press,</publisher>
<location>Albany.</location>
<contexts>
<context position="6866" citStr="Dewey, 1989" startWordPosition="1124" endWordPosition="1125">perience showed that the lexical ambiguity carried by generic dictionaries does not allow their direct use in computational systems (Basili and Pazienza, 1997; Morgan et al., 1995). Even when they are used off-line, lexical ambiguity can introduce so much noise (and then overhead) in the lexical development process that their use can be inconvenient from the point of view of efficiency and effectiveness. The next section explains how it is possible to cope with lexical ambiguity in WordNet by combining its information with another source of information: the Dewey Decimal Classification (DDC) (Dewey, 1989). 3 Reducing the lexical ambiguity in WordNet The main problem with the use of WordNet is lexical polysemy 1. Lexical polysemy is present when a word is associated to many senses (synsets). In general it is not easy to discriminate between different synsets. It is then necessary to find a way for helping the lexicon developer in selecting the correct synset for a word. In order to cope with lexical polysemy, we propose to integrate WordNet synsets with an additional information: a set of field labels. Field labels are indicators, generally used in dictionaries, which provide information about </context>
</contexts>
<marker>Dewey, 1989</marker>
<rawString>Melvil Dewey. 1989. Dewey Decimal Classification and Relative Index. Edition 20. Forest Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>The NYU system for MUC-6 or where&apos;s syntax?</title>
<date>1995</date>
<booktitle>In Sixth message understanding conference MUC-6.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="2462" citStr="Grishman, 1995" startWordPosition="382" endWordPosition="383">zanella). The author wants to thank her supervisor at ITCIRST, Fabio Ciravegna, for his constant help. Alberto Lavelli provided valuable comments to the paper. tion requirement. Unfortunately one of the current trends in IE is the progressive reduction of the size of training corpora: e.g., from the 1,000 texts of the MUC-5 (MUC-5, 1993) to the 100 texts in MUC-6 (MUC-6, 1995). When the corpus size is limited, the assumption of lexical representativeness of the sample corpus may not hold any longer, and the problem of producing a representative lexicon starting from the corpus lexicon arises (Grishman, 1995). Generic resources are interesting as they contain (among others) most of the terms necessary for an IE application. Nevertheless up to now the use of generic resources within IE system has been limited for two main reasons. First the information associated to each term is often not detailed enough for describing the relations necessary for a IE lexicon; secondly the presence of a large amount of lexical polysemy. In this paper we propose a methodology for semi-automatically developing the relevant part of a lexicon .(foreground lexicon) for IE applications by using both a small corpus and Wo</context>
</contexts>
<marker>Grishman, 1995</marker>
<rawString>Ralph Grishman. 1995. The NYU system for MUC-6 or where&apos;s syntax? In Sixth message understanding conference MUC-6. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Information extraction: Techniques and challenges.</title>
<date>1997</date>
<editor>In M. T. Pazienza, editor,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="1590" citStr="Grishman, 1997" startWordPosition="239" endWordPosition="240"> limiting the market. The lexicon definition process is currently one of the main bottlenecks in producing applications. As a matter of fact the necessary lexicon for an average application is generally large (hundreds to thousands of words) and most lexical information is not transportable across domains. The problem of lexicon transport is worsened by the growing degree of lexicalization of IE systems: nowadays several successful systems adopt lexical rules at many levels. The IE research mainstream focused essentially on the definition of lexica starting from a corpus sample (Riloff, 1993; Grishman, 1997) with the implicit assumption that a corpus provided for an application is representative of the whole applica&amp;quot;This work was carried on at ITC-IRST as part of the author&apos;s dissertation for the degree in Philosophy (University of Turin, supervisor: Carla Bazzanella). The author wants to thank her supervisor at ITCIRST, Fabio Ciravegna, for his constant help. Alberto Lavelli provided valuable comments to the paper. tion requirement. Unfortunately one of the current trends in IE is the progressive reduction of the size of training corpora: e.g., from the 1,000 texts of the MUC-5 (MUC-5, 1993) to </context>
</contexts>
<marker>Grishman, 1997</marker>
<rawString>Ralph Grishman. 1997. Information extraction: Techniques and challenges. In M. T. Pazienza, editor, Information Extraction: a multidisciplinary approach to an emerging technology. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Foreground and background lexicons and word sense disambiguation for information extraction.</title>
<date>1997</date>
<booktitle>In International Workshop on Lexically Driven Information Extraction,</booktitle>
<location>Frascati, Italy.</location>
<contexts>
<context position="3196" citStr="Kilgarriff, 1997" startWordPosition="502" endWordPosition="503"> Nevertheless up to now the use of generic resources within IE system has been limited for two main reasons. First the information associated to each term is often not detailed enough for describing the relations necessary for a IE lexicon; secondly the presence of a large amount of lexical polysemy. In this paper we propose a methodology for semi-automatically developing the relevant part of a lexicon .(foreground lexicon) for IE applications by using both a small corpus and WordNet. 2 Developing IE Lexical Resources Lexical information in IE can be divided into three sources of information (Kilgarriff, 1997): • an ontology, i.e. the templates to be filled; • the foreground lexicon (FL), i.e. the terms tightly bound to the ontology; • the background lexicon (BL), i.e. the terms not related or loosely related to the ontology. In this paper we focus on FL only. The FL has generally a limited size with respect to the average dictionary of a language; its dimension depends on each application needs, but it is generally limited to some hundreds of words. The level of quantitative and qualitative information for each entry in the FL can be very high and it is not transportable across domains and 225 Pro</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Adam Kilgarriff. 1997. Foreground and background lexicons and word sense disambiguation for information extraction. In International Workshop on Lexically Driven Information Extraction, Frascati, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: an on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="5615" citStr="Miller, 1990" startWordPosition="918" endWordPosition="919"> BL. The transport of BL to new applications is not a problem, therefore it will not be considered in this paper. 2.1 Using Generic Lexical Resources We propose a development methodology for FLs based on two steps: • Bootstrapping: manual or semi-automatic identification from the corpus of an initial lexicon (Core Lexicon), i.e. of the lexicon covering the corpus sample. • Consolidation: extension of the Core Lexicon by using a generic dictionary in order to completely cover the lexicon needed by the application but not exhaustively represented in the corpus sample. We propose to use WordNet (Miller, 1990) as a generic dictionary during the consolidation phase because it can be profitably used for integrating the Core Lexicon by adding for each term in a semi-automatic way: • its synonyms; • hyponyms and (maybe) hypernyms; • some coordinated terms. As mentioned, there are two problems related to the use of generic dictionaries with respect to the IE needs. First there is no clear way of extracting from them the mapping between the FL and the ontology; this is mainly due to a lack of information and cannot in general be solved; generic lexica cannot then be used during the bootstrapping phase to</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G.A. Miller. 1990. Wordnet: an on-line lexical database. International Journal of Lexicography, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Morgan</author>
<author>Roberto Garigliano</author>
<author>Paul Callaghan</author>
<author>Sanjay Poria</author>
<author>Mark Smith</author>
</authors>
<title>Agnieszka Urbanowicz, Russel Collingham, Marco Costantino, Chris Cooper, and the LOLITA Group.</title>
<date>1995</date>
<booktitle>In Sixth message understanding conference MUC-6.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<institution>University of Durham: Description of</institution>
<contexts>
<context position="6434" citStr="Morgan et al., 1995" startWordPosition="1053" endWordPosition="1056">and (maybe) hypernyms; • some coordinated terms. As mentioned, there are two problems related to the use of generic dictionaries with respect to the IE needs. First there is no clear way of extracting from them the mapping between the FL and the ontology; this is mainly due to a lack of information and cannot in general be solved; generic lexica cannot then be used during the bootstrapping phase to generate the Core Lexicon. Secondly experience showed that the lexical ambiguity carried by generic dictionaries does not allow their direct use in computational systems (Basili and Pazienza, 1997; Morgan et al., 1995). Even when they are used off-line, lexical ambiguity can introduce so much noise (and then overhead) in the lexical development process that their use can be inconvenient from the point of view of efficiency and effectiveness. The next section explains how it is possible to cope with lexical ambiguity in WordNet by combining its information with another source of information: the Dewey Decimal Classification (DDC) (Dewey, 1989). 3 Reducing the lexical ambiguity in WordNet The main problem with the use of WordNet is lexical polysemy 1. Lexical polysemy is present when a word is associated to m</context>
</contexts>
<marker>Morgan, Garigliano, Callaghan, Poria, Smith, 1995</marker>
<rawString>Richard Morgan, Roberto Garigliano, Paul Callaghan, Sanjay Poria, Mark Smith, Agnieszka Urbanowicz, Russel Collingham, Marco Costantino, Chris Cooper, and the LOLITA Group. 1995. University of Durham: Description of the LOLITA system as used for MUC-6. In Sixth message understanding conference MUC-6. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-5</author>
</authors>
<date>1993</date>
<booktitle>Fifth Message Understanding Conference (MUC5).</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<marker>MUC-5, 1993</marker>
<rawString>MUC-5. 1993. Fifth Message Understanding Conference (MUC5). Morgan Kaufmann Publishers, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-6</author>
</authors>
<date>1995</date>
<booktitle>Sixth Message Understanding Conference (MUC-6).</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<marker>MUC-6, 1995</marker>
<rawString>MUC-6. 1995. Sixth Message Understanding Conference (MUC-6). Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence,</booktitle>
<pages>811--816</pages>
<contexts>
<context position="1573" citStr="Riloff, 1993" startWordPosition="237" endWordPosition="238">of the factors limiting the market. The lexicon definition process is currently one of the main bottlenecks in producing applications. As a matter of fact the necessary lexicon for an average application is generally large (hundreds to thousands of words) and most lexical information is not transportable across domains. The problem of lexicon transport is worsened by the growing degree of lexicalization of IE systems: nowadays several successful systems adopt lexical rules at many levels. The IE research mainstream focused essentially on the definition of lexica starting from a corpus sample (Riloff, 1993; Grishman, 1997) with the implicit assumption that a corpus provided for an application is representative of the whole applica&amp;quot;This work was carried on at ITC-IRST as part of the author&apos;s dissertation for the degree in Philosophy (University of Turin, supervisor: Carla Bazzanella). The author wants to thank her supervisor at ITCIRST, Fabio Ciravegna, for his constant help. Alberto Lavelli provided valuable comments to the paper. tion requirement. Unfortunately one of the current trends in IE is the progressive reduction of the size of training corpora: e.g., from the 1,000 texts of the MUC-5 </context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>