<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.947884">
Redundancy Detection in ESL Writings
</title>
<author confidence="0.98796">
Huichao Xue and Rebecca Hwa
</author>
<affiliation confidence="0.9989035">
Department of Computer Science,
University of Pittsburgh,
</affiliation>
<address confidence="0.49512">
210 S Bouquet St, Pittsburgh, PA 15260, USA
</address>
<email confidence="0.999301">
{hux10,hwa}@cs.pitt.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997625">
This paper investigates redundancy detec-
tion in ESL writings. We propose a mea-
sure that assigns high scores to words and
phrases that are likely to be redundant
within a given sentence. The measure is
composed of two components: one cap-
tures fluency with a language model; the
other captures meaning preservation based
on analyzing alignments between words
and their translations. Experiments show
that the proposed measure is five times
more accurate than the random baseline.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990591959183674">
Writing concisely is challenging. It is especially
the case when writing in a foreign language that
one is still learning. As a non-native speaker, it
is more difficult to judge whether a word or a
phrase is redundant. This study focuses on auto-
matically detecting redundancies in English as a
Second Language learners’ writings.
Redundancies occur when the writer includes
some extraneous word or phrase that do not add
to the meaning of the sentence but possibly make
the sentence more awkward to read. Upon re-
moval of the unnecessary words or phrases, the
sentence should improve in its fluency while main-
taining the original meaning. In the NUCLE
corpus (Dahlmeier and Ng, 2011), an annotated
learner corpus comprised of essays written by pri-
marily Singaporean students, 13.71% errors are
tagged as “local redundancy errors”, making re-
dundancy error the second most frequent prob-
lem.1
Although redundancies occur frequently, it has
not been studied as widely as other ESL errors. A
1The most frequent error type is Wrong collocation/idiom
preposition, which comprises 15.69% of the total errors.
major challenge is that, unlike mistakes that vio-
late the grammaticality of a sentence, redundan-
cies do not necessarily “break” the sentence. De-
termining which word or phrase is redundant is
more of a stylistic question; it is more subjective,
and sometimes difficult even for a native speaker.
To the best of our knowledge, this paper reports
a first study on redundancy detection. In particu-
lar, we focus on the task of defining a redundancy
measure that estimates the likelihood that a given
word or phrase within a sentence might be extrane-
ous. We propose a measure that takes into account
each word’s contribution to fluency and meaning.
The fluency component computes the language
model score of the sentence after the deletion of a
word or a phrase. The meaning preservation com-
ponent makes use of the sentence’s translation into
another language as pivot, then it applies a statis-
tical machine translation (SMT) alignment model
to infer the contribution of each word/phrase to the
meaning of the sentence. As a first experiment, we
evaluate our measures on their abilities in picking
the most redundant phrase of a given length. We
show that our measure is five times more accurate
than a random baseline.
</bodyText>
<sectionHeader confidence="0.991982" genericHeader="method">
2 Redundancies in ESL Writings
</sectionHeader>
<bodyText confidence="0.949850428571429">
According to The Elements of Style (Strunk,
1918): concise writing requires that “every word
tell.” In that sense, words that “do not tell”
are redundant. Determining whether a certain
word/phrase is redundant is a stylistic question,
which is difficult to quantify. As a result, most an-
notation resources do not explicitly identify redun-
dancies. One exception is the NUCLE corpus. Be-
low are some examples from the NUCLE corpus,
where the bold-faced words/phrases are marked as
redundant.
Ext: First of all, there should be a careful con-
sideration about what are the things that gov-
ernments should pay for.
</bodyText>
<page confidence="0.985465">
683
</page>
<note confidence="0.828732875">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 683–691,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
Exa: GM wishes to reposition itself as an inno-
vative company to the public.
Ex3: These findings are often unpredictable and
uncertain.
Ex4:... the cost incurred is not only just large
sum of money ...
</note>
<bodyText confidence="0.997124404255319">
These words/phrases are considered redundant be-
cause they are unnecessary (e.g. Ex1, Ext) or
repetitive (e.g. Ex3, Ex4).
However, in NUCLE’s annotation scheme,
some words that were marked redundant are re-
ally words that carry undesirable meanings. For
example:
Ex5:... through which they can insert a special
. . .
Exe:... the analysis and therefore selection of
a single solution for adaptation...
Note that unlike redundancies, these undesirable
words/phrases change the sentences’ meanings.
Despite the difference in definitions, our exper-
imental work uses the NUCLE corpus because
it provides many real world examples of redun-
dancy.
While redundancy detection has not yet been
widely studied, it is related to several areas of ac-
tive research, such as grammatical error correction
(GEC), sentence simplification and sentence com-
pression.
Work in GEC attempts to build automatic sys-
tems to detect/correct grammatical errors (Lea-
cock et al., 2010; Liu et al., 2010; Tetreault et al.,
2010; Dahlmeier and Ng, 2011; Rozovskaya and
Roth, 2010). Both redundancy detection and GEC
aim to improve students’ writings. However, be-
cause redundancies do not necessarily break gram-
maticality, they have received little attention in
GEC.
Sentence compression and sentence simplifica-
tion also consider deleting words from input sen-
tences. However, these tasks have different goals.
Automated sentence simplification (Coster and
Kauchak, 2011) systems aim at reducing the gram-
matical complexity of an input sentence. To il-
lustrate the difference, consider the phrase “crit-
ical reception.” A sentence simplification sys-
tem might rewrite it into “reviews”; but a sys-
tem that removes redundancy should leave it un-
changed because neither “critical” nor “reception”
is extraneous. Moreover, consider the redundant
phrase “had once before” in Ex4. A simplification
system does not need to change it because these
words do not add complexity to the sentence.
... and the cost incurred is not only just large sum of money ...
</bodyText>
<figureCaption confidence="0.664751">
Figure 1: Among the three circled words, “just”
</figureCaption>
<bodyText confidence="0.995208170731707">
is more redundant because deleting it hurts nether
fluency nor meaning.
Sentence compression systems (Jing, 2000;
Knight and Marcu, 2000; McDonald, 2006;
Clarke and Lapata, 2007) aim at shortening a
sentence while retaining the most important in-
formation and keeping it grammatically correct.
This goal distinguishes these systems from ours
in two major aspects. First, sentence compres-
sion systems assume that the original sentence is
well-written; therefore retaining words specific to
the sentence (e.g. “uncertain” in Ex3) can be a
good strategy (Clarke and Lapata, 2007). In the
ESL context, however, even specific words could
still be redundant. For example, although “un-
certain” is specific to Ex3, it is redundant, be-
cause its meaning is already implied by “unpre-
dictable”. Second, sentence compression systems
try to shorten a sentence as much as possible, but
an ESL redundancy detector should leave as much
of the input sentences unchanged, if possible.
One challenge involved in redundancy detection
is that it often involves open class words (Ex3), as
well as multi-word expressions (Ex1, Ex4). Cur-
rent GEC systems dealing with such error types
are mostly MT based. MT systems tend to ei-
ther require large training corpora (Brockett et al.,
2006; Liu et al., 2010), or provide whole sentence
rewritings (Madnani et al., 2012). Hermet and
D´esilets (2009) attempted to extract single prepo-
sition corrections from whole sentence rewritings.
Our work incorporates alignments information to
handle complex changes on both word and phrase
levels.
In our approximation, we consider MT out-
put as an approximation of word/phrase mean-
ings. Using words in other languages to repre-
sent meanings has been explored in Carpuat and
Wu (2007), where the focus is the aligned words’
identities. Our work instead focuses more on how
many words each word is aligned to.
</bodyText>
<sectionHeader confidence="0.998183" genericHeader="method">
3 A Probabilistic Model of Redundancy
</sectionHeader>
<bodyText confidence="0.999832">
We consider a word or a phrase to be redundant
if deleting it results in a fluent English sentence
that conveys the same meaning as before. For
example, “not” and “the” are not considered re-
</bodyText>
<page confidence="0.996469">
684
</page>
<figure confidence="0.56967075">
(b) Multiple English words
aligned to the same meaning
unit. These words are con-
sidered redundant.
</figure>
<figureCaption confidence="0.900651">
Figure 2: Configurations our system consider as
</figureCaption>
<bodyText confidence="0.967727081632653">
redundant. In each figure, the shaded squares are
the words considered to be more redundant than
other words in the same figure.
dundant in Figure 1. This is because discarding
“not” would flip the sentence’s meaning; discard-
ing “the” would lose a necessary determiner be-
fore a noun. In contrast, discarding “just” would
hurt neither fluency nor meaning. It is thus con-
sidered to be more redundant.
Therefore, our computational model needs to
consider words’ contributions to both fluency and
meaning. Figure 2 illustrates words’ contribution
to meaning. In those two examples, each sub-
graph visualizes a sentence: English words corre-
spond to squares in the top row, while their mean-
ings correspond to circles in the bottom row. The
knowledge of which word represents what mean-
ing helps in evaluating its contribution. In partic-
ular, if a word does not connote any significant
meaning, deleting it would not affect the overall
sentence; if several words express the same mean-
ing, then deleting some of them might not affect
the overall sentence either. Also, deleting a more
semantically meaningful word (or phrase) is more
likely to cause a loss of meaning of the overall sen-
tence (e.g. uncertain v.s. the).
Our model computes a single probabilistic value
for both fluency judgment and meaning preserva-
tion – the log-likelihood that after deleting a cer-
tain word or phrase of a sentence, the new sen-
tence is still fluent and conveys the same meaning
as before. This value reflects our definition of re-
dundancy – the higher this probability, the more
redundant the given word/phrase is.
More formally, suppose an English sentence e
contains le words: e = e1e2 ... el.; after some
sub-string es,t = es ... et(1 ≤ s ≤ t ≤ le) is
deleted from e, we obtain a shorter sentence, de-
noted as es,t. We wish to compute the quantity
�
R(s, t; e), the chance that the sub-string es,t is re-
dundant in sentence e. We propose a probabilistic
model to formalize this notion.
Let M be a random variable over some meaning
representation; Pr(M|e) is the likelihood that M
carries the meaning of e. If the sub-string es,t is
redundant, then the new sentence es,t
� should still
express the same meaning; Pr(es,t
</bodyText>
<equation confidence="0.741746">
� |M) computes
</equation>
<bodyText confidence="0.9862665">
the likelihood that the after-deletion sentence can
be generated from meaning M.
</bodyText>
<equation confidence="0.997100647058823">
R(s, t; e)
�=log Pr(m|e) Pr(es,t
� |m)
M=m
Pr(m|e) Pr(es,t
� ) Pr(m|es,t
� )
Pr(m)
= log Pr(es,t
� ) + log
M=m
� Pr(m|es,t
Pr(m)
� ) Pr(m|e)
= LM(es,t
� ) + AGR(M|es,t
� , e) (1)
</equation>
<bodyText confidence="0.9974335">
The first term LM(es,t
� ) is the after-deletion sen-
tence’s log-likelihood, which reflects its fluency.
We calculate the first term with a trigram language
model (LM).
The second term AGR(M|es,t
� , e) can be inter-
preted as the chance that e and es,t
� carry the same
meaning, discounted by “chance agreement”. This
term captures meaning preservation.
The two terms above are complementary to each
other. Intuitively, LM prefers keeping common
words in es,t
� (e.g. the, to) while AGR prefers
keeping words specific to e (e.g. disease, hyper-
tension).
To make the calculation of the second term
practical, we make two simplifying assumptions.
Assumption 1 A sentence’s meaning can be rep-
resented by its translations in another language; its
words’ contributions to the meaning of the sen-
tence can be represented by the mapping between
the words in the original sentence and its transla-
tions (Figure 3).
Note that the choice of translation language may
impact the interpretation of words’ contributions.
We will discuss about this issue in our experiments
(Section 5).
Assumption 2 Instead of considering all possi-
ble translations f for e, our computation will make
use of the most likely translation, f*.
</bodyText>
<figure confidence="0.967911375">
(a) Unaligned English
words are considered
redundant.
�=log
M=m
685
a new idea is created through results from rigorous process of research .
创建 一个 新 的 想法 , 通过 严谨 的 研究 过程 中 的 结果 。
</figure>
<figureCaption confidence="0.995979">
Figure 3: Illustration of Assumption 1 and Approximation 1. An English sentence’s meaning is presented
</figureCaption>
<bodyText confidence="0.96184175">
as a Chinese translation. Meanwhile, each (English) word’s contribution to the sentence meaning is
realized as a word alignment. For Approximation 1, note that sentence alignments normally won’t be
affected before/after deleting words (e.g. “results from”) from the source sentence.
With the two approximations:
</bodyText>
<equation confidence="0.996059">
− ) Pr(f∗|e)
AGR(M|es,t
− , e) ≈ log Pr(f∗|es,t
Pr(f∗)
= log Pr(f∗|es,t
− ) + C1(e)
</equation>
<bodyText confidence="0.95607548">
(We use Ci(e) to denote constant numbers within
sentence e throughout the paper.)
We now rely on a statistical machine translation
model to approximate the translation probability
log Pr(f∗|es,t
− ).
One naive way of calculating this probabil-
ity measure is to consult the MT system. This
method, however, is too computationally expen-
sive for one single input sentence. For a sentence
of length n, calculating the redundancy measure
for all chunks in it would require issuing O(n2)
translation queries. We propose an approximation
that instead calculates the difference of translation
probability caused by discarding es,t, based on an
analysis on the alignment structure between e and
f∗. We show the measure boils down to sum-
ming the expected number of aligned words for
each ei(s ≤ i ≤ t), and possibly weighting these
numbers by ei’s unigram probability. This method
requires one translation query, and O(n2) queries
into a language model, which is much more suit-
able for practical applications. Our method also
sheds light on the role of alignment structures in
the redundancy detection context.
</bodyText>
<subsectionHeader confidence="0.998733">
3.1 Alignments Approximation
</subsectionHeader>
<bodyText confidence="0.962266041666667">
One key insight in our approximation is that the
alignment structure a between es,t
− and f∗ would
be largely similar with the alignment structure be-
tween e and f∗. We illustrate this notion in Fig-
ure 3. Note that after deleting two words “results
from” from the source sentence in Figure 3, the
alignment structure remains unchanged elsewhere.
Also, “R ”, the word once connected with “re-
sults”, can now be seen as connected to blanks.
We hence approximate log Pr(f∗|es,t
− ) by
reusing the alignment structure between e
and f∗. To make the alignment structures
compatible, we start with redefining es,t
− as
e1, e2, ... , es−1, ❑, ... , ❑, et+1, ... , ele, where
the deleted words are left blank.
Let Pr(a|f, e) be the posterior distribution of
alignment structure between sentence pair (f, e).
Approximation 1 We formalize the similarity
between the alignment structures by assuming the
KL-divergence between their alignment distribu-
tions to be small.
</bodyText>
<equation confidence="0.9901178">
DKL(a|f∗, e; a|f∗, es,t
− ) ≈ 0
This allows using Pr(a|f∗, e) to help approximate
log Pr(f∗|es,t
− ):
log Pr(f∗|es,t
− )
�
+
a
</equation>
<bodyText confidence="0.95154025">
We then use an SMT model to calculate
log Pr(f∗|es,t
− , a), the translation probability under
a given alignment structure.
</bodyText>
<subsectionHeader confidence="0.991011">
3.2 The Translation Model
</subsectionHeader>
<bodyText confidence="0.798814333333333">
Approximation 2 We will use IBM
Model 1 (Brown et al., 1993) to calculate
log Pr(f∗|es,t
− , a)
IBM Model 1 is one of the earliest statisti-
cal translation models. It helps us to compute
</bodyText>
<equation confidence="0.921721047619048">
Pr(a|f∗, e)Pr(f∗, a|es,t
− )
Pr(a|f∗, e)
− )
=a
�=log
a
� Pr(f∗, a|es,t
Pr(a|f∗, e) log Pr(a|f∗, e)
DKL(a|f*,e;a|f*,es,t
− )≈0
�≈ Pr(a|f∗, e) log Pr(f∗|es,t
a − , a) + C2(e)
� Y �
�
Pr(a|f∗, e) log Pr(f∗|es,t
− )/Pr(f∗, a|es,t
− )
Pr(a|f∗, e)
686
log Pr(f∗|es,t
</equation>
<bodyText confidence="0.8968265">
− , a) by making explicit how each
word contributes to words it aligns with. In partic-
ular, to compute the probability that f is a transla-
tion of e, Pr(f|e), IBM Model 1 defined a gener-
ative alignment model where every word fi in f is
aligned with exactly one word eai in e, so that fi
and eai are word level translations of each other.
log Pry f�∗|°) = −Cc, where Cc is a constant
number. A larger Cc value indicates a higher
importance of ej during the translation.
</bodyText>
<equation confidence="0.750271666666667">
�
DIFF(es,t
− , e) = −Cc
s≤j≤t 1≤i≤lf∗
=−Cc � A(j) (2)
� Ai,j
s≤j≤t
�
a
�=
a
Pr(a|f∗, e) log Pr(f∗|es,t −, a)
log Pr(fi∗|es,t
−ai)
1≤i≤lf∗
</equation>
<bodyText confidence="0.991752">
Here A(j) is the expected number of align-
ments to ej. This metric demonstrates the in-
tuition that words aligned to more words in
the translation are less redundant.
</bodyText>
<figure confidence="0.943830833333334">
�
Pr(a|f∗, e)
�= � Pr(fi ∗|es,t
a Pr(a|f∗, e) −ai)
1≤i≤lf∗ log Pr(fi ∗|eai) + C3(e)
Note that
</figure>
<bodyText confidence="0.724834333333333">
2. We note that rare words are often more im-
portant, and therefore harder to be generated.
We assume Pr(fi∗|❑) = Pr(ej|❑) Pr(fi∗|ej).
</bodyText>
<equation confidence="0.97474152173913">
DIFF(es,t
− , e)
log Pr(fi ∗|es,t � 0 , for ai V Is ... t}
−ai) = log Pr r(f∗1°)) , otherwise
Pr(fi∗|eai)
�= � Pr(ej|�) Pr(fi ∗|ej)
s≤j≤t 1≤i≤lf∗ Ai,j log
Pr(fi∗|ej)
Pr(fi ∗|�)
Iai=j log
Pr(fi∗|ej)
Pr(fi ∗|es,t
−ai)
Pr(fi∗|eai)
� � �
=Pr(a|f∗, e)
a 1≤i≤lf∗ s≤j≤t
�= � Pr(ai = j|f∗, e) Pr(fi∗|❑)
s≤j≤t 1≤i≤lf∗ v log Pr(fi∗|ej)
Ai,j
� �� �
DIFF(es,t
− ,e)
</equation>
<bodyText confidence="0.989301">
Here Ai,j = Pr(ai = j  |f∗, e), which is the
probability of the i-th word in the translation being
aligned to the j-th word in the original sentence.
</bodyText>
<subsectionHeader confidence="0.931884">
3.3 Per-word Contribution
</subsectionHeader>
<bodyText confidence="0.920419">
Through deductions,
</bodyText>
<equation confidence="0.99994175">
R(s, t; e) = LM(es,t
− ) + DIFF(es,t
− , e)
+C1(e) + C2(e) + C3(e)
</equation>
<bodyText confidence="0.991936666666667">
the redundancy measure boils down to how we
define Pr(fi∗|❑j), which is: when we discard ej,
how do we generate the word it aligns fi∗ with in
its translation. This value reflects ej’s contribution
in generating fi∗.
We approximate Pr(fi∗|❑j) in two ways.
</bodyText>
<listItem confidence="0.928711">
1. Suppose that all words in the translation
</listItem>
<bodyText confidence="0.621013">
are of equal importance. We assume
</bodyText>
<equation confidence="0.9053425">
�= A(j) log Pr(ej|EI) (3)
s≤j≤t
</equation>
<bodyText confidence="0.986911636363636">
This gives us counts on how likely each
word is aligned with Chinese words ac-
cording to Pr(a|f∗, e), where each word is
weighted by its importance log Pr(ej|❑). We
use ej’s unigram probability to approximate
log Pr(ej|❑).
When estimating the alignment probabilities
Ai,j, we smooth the alignment result from Google
translation using Dirichlet-smoothing, where we
set α = 0.1 empirically based on experiments in
the development dataset.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999946785714286">
A fully automated redundancy detector has to de-
cide (1) whether a given sentence contains any re-
dundancy errors; (2) how many words constitute
the redundant part; and (3) which exact words are
redundant. In this paper, we focus on the third part
while assuming the first two are given. Thus, our
experimental task is: given a sentence known to
contain a redundant phrase of a particular length,
can that redundant phrase be accurately identified?
For most sentences in our study, this results in
choosing one from around 20 words/phrases.
While the task has a somewhat limited scope, it
allows us to see how we could formally measure
the difference between redundant words/phrases
</bodyText>
<equation confidence="0.672121">
� Pr(a|f∗, e) � log
a 1≤i≤lf∗
</equation>
<page confidence="0.989314">
687
</page>
<bodyText confidence="0.999967857142857">
and non-redundant ones. For each measure, we
observe whether it has assigned the highest score
to the redundant part of the sentence. We compare
the proposed redundancy model described in Sec-
tion 3 against a set of baselines and other potential
redundancy measures (to be described shortly).
To better understand different measures’ perfor-
mance on function words vs. content words, we
also calculate the percentage of redundant func-
tion/content words that are detected successfully –
accuracy in both categories. In our experiments,
we consider prepositions and determiners as func-
tion words; and we consider other words/phrases
as content words/phrases.
</bodyText>
<subsectionHeader confidence="0.95691">
4.1 Redundancy Measures
</subsectionHeader>
<bodyText confidence="0.999911125">
To gain insight into redundancy error detection’s
difficulty, we first consider a random baseline.
random The random baseline assigns a random
score to each word/phrase. The resulting system
will pick one word/phrase of the given length at
random.
We consider relying on large scale language
models to decide redundancy.
trigram We use a trigram language model to
capture fluency, by calculating the log-likelihood
of the whole sentence after discarding the given
word/phrase. A higher probability indicates a
higher fluency.
round-trip Inspired by Madnani et al. (2012;
Hermet and D´esilets (2009), an MT system may
eliminate grammar errors with the help of large
scale language models. In this method, we analyze
which parts are considered redundant by an MT
system by comparing the original sentence with
its round-trip translation. We use Google trans-
late to first translate one sentence into a pivot lan-
guage, and then back to English. We measure one
phrase’s redundancy by the number of words that
disappeared after the round-trip. We determine if
one word disappeared in two ways:
extract word match: one word is considered
disappeared if the same word does not occur
in the round-trip.
aligned word: we use the Berkeley
aligner (DeNero and Klein, 2007) to
align original sentences with their round-trip
translations. Unaligned words are considered
to have disappeared.
We consider measures for words/phrases’ con-
tributions to sentence meaning.
sig-score This measure accounts for whether
one word wi is capturing the gist of a sen-
tence (Clarke and Lapata, 2007)2. It was shown to
help decide whether one part should be discarded
during sentence compression.
</bodyText>
<equation confidence="0.982846">
I(wi) = − l · fi log Fa
N Fi
</equation>
<bodyText confidence="0.999488481481482">
fi and Fi are the frequencies of wi in the current
document and a large corpus respectively; Fa is
the number of all word occurrences in the corpus;
l is the number of clause constituents above wi;
N is the deepest level of clause embeddings. This
measure assigns low scores to document specific
words occurring at deep syntax levels.
align # We use the number of alignments that a
word/phrase has in the translation to measure its
redundancy, as deducted in Equation 2.
contrib We compute the word/phrase’s contri-
bution to meaning, according to Equation 3.
We consider the combinations of measures.
trigram + Ccalign # We use a linear combina-
tion between language model and align # (Equa-
tion 2). We tune Cc on development data.
trigram+contrib This measure (as we proposed
in Section 3) is the sum of the trigram lan-
guage model component and the contrib compo-
nent which represents the phrase’s contribution to
meaning.
trigram+α round-trip/sig-score We combine
language model with round-trip and sig-score
linearly (McDonald, 2006; Clarke and Lapata,
2007). To obtain baselines that are as strong as
possible, we tune the weight α on evaluation data
for best accuracy.
</bodyText>
<subsectionHeader confidence="0.997158">
4.2 Pivot Languages
</subsectionHeader>
<bodyText confidence="0.998404">
Our proposed model uses machine translation out-
puts from different pivot languages. To see which
language helps measuring redundancy, we com-
pare 52 pivot languages available at Google trans-
late3 for meaning representation4.
</bodyText>
<footnote confidence="0.975104">
2We extend this measure, which was only defined for con-
tent words in Clarke and Lapata (2007), to include all English
words.
3http://translate.google.com
4These languages include Albanian (sq), Arabic (ar),
Azerbaijani (az), Irish (ga), Estonian (et), Basque (eu),
</footnote>
<page confidence="0.987834">
688
</page>
<table confidence="0.999557333333333">
length count percentage
1 356 67.55%
2 80 15.18%
3 40 7.59%
4 18 3.42%
other 33 6.26%
</table>
<tableCaption confidence="0.916954">
Table 1: Length distribution of redundant chunks’
lengths in the evaluation data.
</tableCaption>
<subsectionHeader confidence="0.999044">
4.3 Data and Tools
</subsectionHeader>
<bodyText confidence="0.999895791666667">
We extract instances from the NUCLE cor-
pus (Dahlmeier and Ng, 2011), an error annotated
corpus mainly written by Singaporean students,
to conduct this study. The corpus is composed
of 1,414 student essays on various topics. An-
notations in NUCLE include error locations, er-
ror types, and suggested corrections. Redundancy
errors are marked by annotators as Rloc. In this
study, we only consider the cases where the sug-
gested correction is to delete the redundant part
(97.09% among all Rloc errors).
To construct our evaluation dataset, we
pick sentences with exactly one redundant
word/phrase. This is the most common case
(81.18%) among sentences containing redundant
words/phrases. We use 10% of the essays (336
sentences) for development purposes, and an-
other 200 essays as the evaluation corpus (527
sentences). A distribution of redundant chunks’
lengths in evaluation corpus is shown in Table 1.
We train a trigram language model using the
SRILM toolkit (Stolcke, 2002) on the Agence
France-Presse (afp) portion of the English Giga-
words corpus.
</bodyText>
<sectionHeader confidence="0.998891" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.569785052631579">
The experiment aims to address the following
questions: (1) Does a sentence’s translation serve
as a reasonable approximation for its meaning? (2)
Byelorussian (be), Bulgarian (bg), Icelandic (is), Polish (pl),
Persian (fa), Boolean (language ((Afrikaans) (af), Danish
(da), German (de), Russian (ru), French (fr), Tagalog (tl),
Finnish (fi), Khmer (km), Georgian (ka), Gujarati (gu),
Haitian (Creole (ht), Korean (ko), Dutch (nl), Galician (gl),
Catalan (ca), Czech (cs), Kannada (kn), Croatian (hr), Latin
(la), Latvian (lv), Lao (lo), Lithuanian (lt), Romanian (ro),
Maltese (mt), Malay (ms), Macedonian (mk), Bengali (bn),
Norwegian (no), Portuguese (pt), Japanese (ja), Swedish (sv),
Serbian (sr), Esperanto (eo), Slovak (sk), Slovenian (sl),
Swahili (sw), Telugu (te), Tamil (ta), Thai (th), Turkish (tr),
Welsh (cy), Urdu (ur), Ukrainian (uk), Hebrew (iw), Greek
(el), Spanish (es), Hungarian (hu), Armenian (hy), Italian (it),
Yiddish (yi), Hindi (hi), Indonesian (id), English (en), Viet-
namese (vi), Simplified Chinese (zh-CN), Traditional Chi-
nese (zh-TW).
</bodyText>
<table confidence="0.999862941176471">
Metrics overall function content
words words
random 4.44% 4.62% 4.36%
trigram 8.06% 3.95% 9.73%
sig-score 10.71% 22.16% 6.07%
round-trip (aligned word) 10.69% 12.72% 9.87%
round-trip (exact word 5.75% 4.27% 6.35%
match)
trigram + α round-trip 14.80% 11.84% 16.00%
(aligned word)
trigram + α round-trip 9.49% 4.61% 11.47%
(exact word match)
trigram + α sig-score 11.01% 22.68% 6.28%
align # 5.04% 3.36% 5.72%
trigram + C.×align # 9.58% 4.61% 11.60%
contrib 8.59% 20.23% 3.87%
trigram + contrib 21.63% 38.16% 14.93%
</table>
<tableCaption confidence="0.997566">
Table 2: Redundancy part identification accuracies
</tableCaption>
<bodyText confidence="0.988366181818182">
for different redundancy metrics on NUCLE cor-
pus, using French as the pivot language.
If so, does the choice of the pivot language matter?
(3) How do the potentially conflicting goals of pre-
serving fluency versus preserving meaning impact
the definition of a redundancy measure?
Our experimental results are presented in Fig-
ure 4 and Table 2. In Figure 4 we compare using
different pivot languages in our proposed model;
in Table 2 we compare using different redundancy
metrics for the same pivot language – French.
</bodyText>
<figureCaption confidence="0.550313">
Figure 4: Using different pivot languages for re-
dundancy measurement.
</figureCaption>
<bodyText confidence="0.9999191">
First, compared to other measures, our proposed
model best captures redundancy. In particular, our
model picks the correct redundant chunk 21.63%
of the time, which is five times higher than the ran-
dom baseline. This suggests that using translation
to approximate sentence meanings is a plausible
option. Note that one partial reason for the low
figures is the limitation of data resources. During
error analysis, we found linkers/connectors (e.g.
moreover, however) and modal auxiliaries (e.g.
</bodyText>
<page confidence="0.997326">
689
</page>
<bodyText confidence="0.999981986111111">
can, had) are often marked redundant when they
actually carry undesirable meanings (Ex6, Ex5).
These cases comprise a 16% portion among our
model’s failures. Despite this limitation, the evalu-
ation still suggests that current approaches are not
ready for a full redundancy detection pipeline.
Second, we find that the choice of pivot lan-
guage does make a difference. Experimental result
suggests that the system tends to achieve higher
redundancy detection accuracy when using trans-
lations of a language more similar to English. In
particular, when using European languages (e.g.
German (de), French (fr), Hungarian (hu) etc.) as
pivot, the system performs much better than using
Asian languages (e.g. Chinese (zh-CN), Japanese
(ja), Thai (th) etc.). One reason for this phe-
nomenon is that the default Google translation out-
put in Asian languages (as well as the alignment
between English and these languages) are orga-
nized into characters, while characters are not the
minimum meaning component. For example, in
Chinese, “WW is the translation of “explana-
tion”, but the two characters “ ” and “W mean
“to solve” and “to release” respectively. In the
alignment output, this will cause certain words be-
ing associated with more or less alignments than
others. In this case, the number of alignments no
longer directly reflect how many meaning units
a certain word helps to express. To confirm this
phenomenon, we tried improving the system using
Simplified Chinese as the pivot language by merg-
ing characters together. In particular, we applied
Chinese tokenization (Chang et al., 2008), and
then merged alignments accordingly. This raised
the system’s accuracy from 17.74% to 20.11%.
Third, to better understand the salient features
of a successful redundancy measure, we experi-
mented with using different components in isola-
tion. We find that the language model component
is better at detecting redundant content words,
while the alignment analysis component is better
at detecting redundant function words. The lan-
guage model detects the function word redundan-
cies with a worse accuracy than the random base-
line; the alignment analysis component also has a
worse accuracy than the random baseline on con-
tent words. However, the English language model
and the alignment analysis result can build on top
of each other when we analyze the redundancies.
We also found that alignments help us to better
account for each word’s contribution to the “mean-
ing” of the sentence. A linear combination of a
language model score and our proposed measure
based on analysis of alignments best captures re-
dundancy. However, as our experimental results
suggest, it is necessary both to use alignments in
translation outputs, and to use them in a good
way. Alignments help isolating fluency from the
meaning component – making them easy to inte-
grate. As our experiments demonstrated, although
methods comparing Google round-trip transla-
tion’s output with the original sentence could lead
to a 10.69% prediction accuracy, it is harder to
combine it with the English language model. This
is partly because of the non-orthogonality of these
two measures – the English language model has
already been used in the round-trip translation re-
sult. Also, an information theoretical interpreta-
tion of alignments is essential for the model’s suc-
cess. For example, a more naive way of using
alignment results, align #, which counts the num-
ber of alignments, leads to a much lower accuracy.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999961777777778">
Despite the prevalence of redundant phrases in
ESL writings, there has not been much work in the
automatic detection of these problems. We con-
duct a first study on developing a computational
model of redundancies. We propose to account for
words/phrases redundancies by comparing an ESL
sentence with outputs from off-the-shelf machine
translation systems. We propose a redundancy
measure based on this comparison. We show that
by interpreting the translation outputs with IBM
Models, redundancies can be measured by a lin-
ear combination of a language model score and
the words’ contribution to the sentence’s mean-
ing. This measure accounts for both the fluency
and completeness of a sentence after removing one
chunk. The proposed measure outperforms the di-
rect round-trip translation and a random baseline
by a large margin.
</bodyText>
<sectionHeader confidence="0.997796" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9619905">
This work is supported by U.S. National Science
Foundation Grant IIS-0745914. We thank the
anonymous reviewers for their suggestions; we
also thank Joel Tetreault, Janyce Wiebe, Wencan
Luo, Fan Zhang, Lingjia Deng, Jiahe Qian, Nitin
Madnani and Yafei Wei for helpful discussions.
</bodyText>
<page confidence="0.996732">
690
</page>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896333333333">
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
smt techniques. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, ACL-44, pages 249–
256, Sydney, Australia. Association for Computa-
tional Linguistics.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263–311.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224–232. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1–11.
Will Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1–9, Portland, Oregon, June. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ’11, pages 915–923, Portland, Oregon, USA.
Association for Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17–24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Matthieu Hermet and Alain D´esilets. 2009. Using first
and second language models to correct preposition
errors in second language authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 64–72.
Association for Computational Linguistics.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310–315. Association for Computa-
tional Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence and Twelfth
Conference on Innovative Applications of Artificial
Intelligence, pages 703–710. AAAI Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated grammatical error detection for
language learners. Synthesis lectures on human lan-
guage technologies, 3(1):1–134.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 1068–1076, Cam-
bridge, Massachusetts. Association for Computa-
tional Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182–
190, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
volume 6, pages 297–304. Association for Compu-
tational Linguistics.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 961–970, Cambridge, Massachusetts. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the interna-
tional conference on spoken language processing,
volume 2, pages 901–904.
William Strunk. 1918. The elements of style / by
William Strunk, Jr.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selec-
tion and error detection. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ’10,
pages 353–358, Uppsala, Sweden. Association for
Computational Linguistics.
</reference>
<page confidence="0.998261">
691
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957536">
<title confidence="0.998598">Redundancy Detection in ESL Writings</title>
<author confidence="0.97439">Xue</author>
<affiliation confidence="0.9996315">Department of Computer University of</affiliation>
<address confidence="0.993573">210 S Bouquet St, Pittsburgh, PA 15260,</address>
<abstract confidence="0.999271846153846">paper investigates detec- ESL writings. We propose a measure that assigns high scores to words and phrases that are likely to be redundant within a given sentence. The measure is composed of two components: one capa language model; the captures preservation on analyzing alignments between words and their translations. Experiments show that the proposed measure is five times more accurate than the random baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal smt techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>249--256</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="7373" citStr="Brockett et al., 2006" startWordPosition="1164" endWordPosition="1167"> be redundant. For example, although “uncertain” is specific to Ex3, it is redundant, because its meaning is already implied by “unpredictable”. Second, sentence compression systems try to shorten a sentence as much as possible, but an ESL redundancy detector should leave as much of the input sentences unchanged, if possible. One challenge involved in redundancy detection is that it often involves open class words (Ex3), as well as multi-word expressions (Ex1, Ex4). Current GEC systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligne</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL errors using phrasal smt techniques. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 249– 256, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="15129" citStr="Brown et al., 1993" startWordPosition="2479" endWordPosition="2482">ted words are left blank. Let Pr(a|f, e) be the posterior distribution of alignment structure between sentence pair (f, e). Approximation 1 We formalize the similarity between the alignment structures by assuming the KL-divergence between their alignment distributions to be small. DKL(a|f∗, e; a|f∗, es,t − ) ≈ 0 This allows using Pr(a|f∗, e) to help approximate log Pr(f∗|es,t − ): log Pr(f∗|es,t − ) � + a We then use an SMT model to calculate log Pr(f∗|es,t − , a), the translation probability under a given alignment structure. 3.2 The Translation Model Approximation 2 We will use IBM Model 1 (Brown et al., 1993) to calculate log Pr(f∗|es,t − , a) IBM Model 1 is one of the earliest statistical translation models. It helps us to compute Pr(a|f∗, e)Pr(f∗, a|es,t − ) Pr(a|f∗, e) − ) =a �=log a � Pr(f∗, a|es,t Pr(a|f∗, e) log Pr(a|f∗, e) DKL(a|f*,e;a|f*,es,t − )≈0 �≈ Pr(a|f∗, e) log Pr(f∗|es,t a − , a) + C2(e) � Y � � Pr(a|f∗, e) log Pr(f∗|es,t − )/Pr(f∗, a|es,t − ) Pr(a|f∗, e) 686 log Pr(f∗|es,t − , a) by making explicit how each word contributes to words it aligns with. In particular, to compute the probability that f is a translation of e, Pr(f|e), IBM Model 1 defined a generative alignment model where</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<contexts>
<context position="7854" citStr="Carpuat and Wu (2007)" startWordPosition="1239" endWordPosition="1242">C systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligned to. 3 A Probabilistic Model of Redundancy We consider a word or a phrase to be redundant if deleting it results in a fluent English sentence that conveys the same meaning as before. For example, “not” and “the” are not considered re684 (b) Multiple English words aligned to the same meaning unit. These words are considered redundant. Figure 2: Configurations our system consider as redundant. In each figure, the shaded squares are the words considered to be more redundant than</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28282" citStr="Chang et al., 2008" startWordPosition="4629" endWordPosition="4632"> the minimum meaning component. For example, in Chinese, “WW is the translation of “explanation”, but the two characters “ ” and “W mean “to solve” and “to release” respectively. In the alignment output, this will cause certain words being associated with more or less alignments than others. In this case, the number of alignments no longer directly reflect how many meaning units a certain word helps to express. To confirm this phenomenon, we tried improving the system using Simplified Chinese as the pivot language by merging characters together. In particular, we applied Chinese tokenization (Chang et al., 2008), and then merged alignments accordingly. This raised the system’s accuracy from 17.74% to 20.11%. Third, to better understand the salient features of a successful redundancy measure, we experimented with using different components in isolation. We find that the language model component is better at detecting redundant content words, while the alignment analysis component is better at detecting redundant function words. The language model detects the function word redundancies with a worse accuracy than the random baseline; the alignment analysis component also has a worse accuracy than the ra</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="6293" citStr="Clarke and Lapata, 2007" startWordPosition="992" endWordPosition="995">m might rewrite it into “reviews”; but a system that removes redundancy should leave it unchanged because neither “critical” nor “reception” is extraneous. Moreover, consider the redundant phrase “had once before” in Ex4. A simplification system does not need to change it because these words do not add complexity to the sentence. ... and the cost incurred is not only just large sum of money ... Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3, it is redundant, because its meaning is already implied by “unpredictabl</context>
<context position="20921" citStr="Clarke and Lapata, 2007" startWordPosition="3472" endWordPosition="3475">easure one phrase’s redundancy by the number of words that disappeared after the round-trip. We determine if one word disappeared in two ways: extract word match: one word is considered disappeared if the same word does not occur in the round-trip. aligned word: we use the Berkeley aligner (DeNero and Klein, 2007) to align original sentences with their round-trip translations. Unaligned words are considered to have disappeared. We consider measures for words/phrases’ contributions to sentence meaning. sig-score This measure accounts for whether one word wi is capturing the gist of a sentence (Clarke and Lapata, 2007)2. It was shown to help decide whether one part should be discarded during sentence compression. I(wi) = − l · fi log Fa N Fi fi and Fi are the frequencies of wi in the current document and a large corpus respectively; Fa is the number of all word occurrences in the corpus; l is the number of clause constituents above wi; N is the deepest level of clause embeddings. This measure assigns low scores to document specific words occurring at deep syntax levels. align # We use the number of alignments that a word/phrase has in the translation to measure its redundancy, as deducted in Equation 2. con</context>
<context position="22554" citStr="Clarke and Lapata (2007)" startWordPosition="3745" endWordPosition="3748">he phrase’s contribution to meaning. trigram+α round-trip/sig-score We combine language model with round-trip and sig-score linearly (McDonald, 2006; Clarke and Lapata, 2007). To obtain baselines that are as strong as possible, we tune the weight α on evaluation data for best accuracy. 4.2 Pivot Languages Our proposed model uses machine translation outputs from different pivot languages. To see which language helps measuring redundancy, we compare 52 pivot languages available at Google translate3 for meaning representation4. 2We extend this measure, which was only defined for content words in Clarke and Lapata (2007), to include all English words. 3http://translate.google.com 4These languages include Albanian (sq), Arabic (ar), Azerbaijani (az), Irish (ga), Estonian (et), Basque (eu), 688 length count percentage 1 356 67.55% 2 80 15.18% 3 40 7.59% 4 18 3.42% other 33 6.26% Table 1: Length distribution of redundant chunks’ lengths in the evaluation data. 4.3 Data and Tools We extract instances from the NUCLE corpus (Dahlmeier and Ng, 2011), an error annotated corpus mainly written by Singaporean students, to conduct this study. The corpus is composed of 1,414 student essays on various topics. Annotations i</context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Coster</author>
<author>David Kauchak</author>
</authors>
<title>Learning to simplify sentences using wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="5492" citStr="Coster and Kauchak, 2011" startWordPosition="863" endWordPosition="866">ification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence simplification system might rewrite it into “reviews”; but a system that removes redundancy should leave it unchanged because neither “critical” nor “reception” is extraneous. Moreover, consider the redundant phrase “had once before” in Ex4. A simplification system does not need to change it because these words do not add complexity to the sentence. ... and the cost incurred is not only just large sum of money ... Figure 1: Among the three</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>Will Coster and David Kauchak. 2011. Learning to simplify sentences using wikipedia. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 1–9, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>915--923</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1375" citStr="Dahlmeier and Ng, 2011" startWordPosition="215" endWordPosition="218"> when writing in a foreign language that one is still learning. As a non-native speaker, it is more difficult to judge whether a word or a phrase is redundant. This study focuses on automatically detecting redundancies in English as a Second Language learners’ writings. Redundancies occur when the writer includes some extraneous word or phrase that do not add to the meaning of the sentence but possibly make the sentence more awkward to read. Upon removal of the unnecessary words or phrases, the sentence should improve in its fluency while maintaining the original meaning. In the NUCLE corpus (Dahlmeier and Ng, 2011), an annotated learner corpus comprised of essays written by primarily Singaporean students, 13.71% errors are tagged as “local redundancy errors”, making redundancy error the second most frequent problem.1 Although redundancies occur frequently, it has not been studied as widely as other ESL errors. A 1The most frequent error type is Wrong collocation/idiom preposition, which comprises 15.69% of the total errors. major challenge is that, unlike mistakes that violate the grammaticality of a sentence, redundancies do not necessarily “break” the sentence. Determining which word or phrase is redu</context>
<context position="5075" citStr="Dahlmeier and Ng, 2011" startWordPosition="805" endWordPosition="808">ion... Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence simplification system migh</context>
<context position="22984" citStr="Dahlmeier and Ng, 2011" startWordPosition="3813" endWordPosition="3816">dundancy, we compare 52 pivot languages available at Google translate3 for meaning representation4. 2We extend this measure, which was only defined for content words in Clarke and Lapata (2007), to include all English words. 3http://translate.google.com 4These languages include Albanian (sq), Arabic (ar), Azerbaijani (az), Irish (ga), Estonian (et), Basque (eu), 688 length count percentage 1 356 67.55% 2 80 15.18% 3 40 7.59% 4 18 3.42% other 33 6.26% Table 1: Length distribution of redundant chunks’ lengths in the evaluation data. 4.3 Data and Tools We extract instances from the NUCLE corpus (Dahlmeier and Ng, 2011), an error annotated corpus mainly written by Singaporean students, to conduct this study. The corpus is composed of 1,414 student essays on various topics. Annotations in NUCLE include error locations, error types, and suggested corrections. Redundancy errors are marked by annotators as Rloc. In this study, we only consider the cases where the suggested correction is to delete the redundant part (97.09% among all Rloc errors). To construct our evaluation dataset, we pick sentences with exactly one redundant word/phrase. This is the most common case (81.18%) among sentences containing redundan</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammatical error correction with alternating structure optimization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 915–923, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="20612" citStr="DeNero and Klein, 2007" startWordPosition="3426" endWordPosition="3429">rors with the help of large scale language models. In this method, we analyze which parts are considered redundant by an MT system by comparing the original sentence with its round-trip translation. We use Google translate to first translate one sentence into a pivot language, and then back to English. We measure one phrase’s redundancy by the number of words that disappeared after the round-trip. We determine if one word disappeared in two ways: extract word match: one word is considered disappeared if the same word does not occur in the round-trip. aligned word: we use the Berkeley aligner (DeNero and Klein, 2007) to align original sentences with their round-trip translations. Unaligned words are considered to have disappeared. We consider measures for words/phrases’ contributions to sentence meaning. sig-score This measure accounts for whether one word wi is capturing the gist of a sentence (Clarke and Lapata, 2007)2. It was shown to help decide whether one part should be discarded during sentence compression. I(wi) = − l · fi log Fa N Fi fi and Fi are the frequencies of wi in the current document and a large corpus respectively; Fa is the number of all word occurrences in the corpus; l is the number </context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 17–24, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Hermet</author>
<author>Alain D´esilets</author>
</authors>
<title>Using first and second language models to correct preposition errors in second language authoring.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>64--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hermet, D´esilets, 2009</marker>
<rawString>Matthieu Hermet and Alain D´esilets. 2009. Using first and second language models to correct preposition errors in second language authoring. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 64–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>310--315</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6227" citStr="Jing, 2000" startWordPosition="984" endWordPosition="985">critical reception.” A sentence simplification system might rewrite it into “reviews”; but a system that removes redundancy should leave it unchanged because neither “critical” nor “reception” is extraneous. Moreover, consider the redundant phrase “had once before” in Ex4. A simplification system does not need to change it because these words do not add complexity to the sentence. ... and the cost incurred is not only just large sum of money ... Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3, it is </context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of the sixth conference on Applied natural language processing, pages 310–315. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>703--710</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="6251" citStr="Knight and Marcu, 2000" startWordPosition="986" endWordPosition="989">eption.” A sentence simplification system might rewrite it into “reviews”; but a system that removes redundancy should leave it unchanged because neither “critical” nor “reception” is extraneous. Moreover, consider the redundant phrase “had once before” in Ex4. A simplification system does not need to change it because these words do not add complexity to the sentence. ... and the cost incurred is not only just large sum of money ... Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3, it is redundant, because its m</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 703–710. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>M Gamon</author>
<author>J Tetreault</author>
</authors>
<title>Automated grammatical error detection for language learners. Synthesis lectures on human language technologies,</title>
<date>2010</date>
<pages>3--1</pages>
<contexts>
<context position="5009" citStr="Leacock et al., 2010" startWordPosition="792" endWordPosition="796">nalysis and therefore selection of a single solution for adaptation... Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the </context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault. 2010. Automated grammatical error detection for language learners. Synthesis lectures on human language technologies, 3(1):1–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Bo Han</author>
<author>Kuan Li</author>
<author>Stephan Hyeonjun Stiller</author>
<author>Ming Zhou</author>
</authors>
<title>SRL-based verb selection for ESL.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1068--1076</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="5027" citStr="Liu et al., 2010" startWordPosition="797" endWordPosition="800">selection of a single solution for adaptation... Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical r</context>
<context position="7392" citStr="Liu et al., 2010" startWordPosition="1168" endWordPosition="1171">ple, although “uncertain” is specific to Ex3, it is redundant, because its meaning is already implied by “unpredictable”. Second, sentence compression systems try to shorten a sentence as much as possible, but an ESL redundancy detector should leave as much of the input sentences unchanged, if possible. One challenge involved in redundancy detection is that it often involves open class words (Ex3), as well as multi-word expressions (Ex1, Ex4). Current GEC systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligned to. 3 A Probabili</context>
</contexts>
<marker>Liu, Han, Li, Stiller, Zhou, 2010</marker>
<rawString>Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun Stiller, and Ming Zhou. 2010. SRL-based verb selection for ESL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1068–1076, Cambridge, Massachusetts. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7453" citStr="Madnani et al., 2012" startWordPosition="1177" endWordPosition="1180">dant, because its meaning is already implied by “unpredictable”. Second, sentence compression systems try to shorten a sentence as much as possible, but an ESL redundancy detector should leave as much of the input sentences unchanged, if possible. One challenge involved in redundancy detection is that it often involves open class words (Ex3), as well as multi-word expressions (Ex1, Ex4). Current GEC systems dealing with such error types are mostly MT based. MT systems tend to either require large training corpora (Brockett et al., 2006; Liu et al., 2010), or provide whole sentence rewritings (Madnani et al., 2012). Hermet and D´esilets (2009) attempted to extract single preposition corrections from whole sentence rewritings. Our work incorporates alignments information to handle complex changes on both word and phrase levels. In our approximation, we consider MT output as an approximation of word/phrase meanings. Using words in other languages to represent meanings has been explored in Carpuat and Wu (2007), where the focus is the aligned words’ identities. Our work instead focuses more on how many words each word is aligned to. 3 A Probabilistic Model of Redundancy We consider a word or a phrase to be</context>
<context position="19921" citStr="Madnani et al. (2012" startWordPosition="3311" endWordPosition="3314"> as content words/phrases. 4.1 Redundancy Measures To gain insight into redundancy error detection’s difficulty, we first consider a random baseline. random The random baseline assigns a random score to each word/phrase. The resulting system will pick one word/phrase of the given length at random. We consider relying on large scale language models to decide redundancy. trigram We use a trigram language model to capture fluency, by calculating the log-likelihood of the whole sentence after discarding the given word/phrase. A higher probability indicates a higher fluency. round-trip Inspired by Madnani et al. (2012; Hermet and D´esilets (2009), an MT system may eliminate grammar errors with the help of large scale language models. In this method, we analyze which parts are considered redundant by an MT system by comparing the original sentence with its round-trip translation. We use Google translate to first translate one sentence into a pivot language, and then back to English. We measure one phrase’s redundancy by the number of words that disappeared after the round-trip. We determine if one word disappeared in two ways: extract word match: one word is considered disappeared if the same word does not </context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182– 190, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<volume>6</volume>
<pages>297--304</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6267" citStr="McDonald, 2006" startWordPosition="990" endWordPosition="991">lification system might rewrite it into “reviews”; but a system that removes redundancy should leave it unchanged because neither “critical” nor “reception” is extraneous. Moreover, consider the redundant phrase “had once before” in Ex4. A simplification system does not need to change it because these words do not add complexity to the sentence. ... and the cost incurred is not only just large sum of money ... Figure 1: Among the three circled words, “just” is more redundant because deleting it hurts nether fluency nor meaning. Sentence compression systems (Jing, 2000; Knight and Marcu, 2000; McDonald, 2006; Clarke and Lapata, 2007) aim at shortening a sentence while retaining the most important information and keeping it grammatically correct. This goal distinguishes these systems from ours in two major aspects. First, sentence compression systems assume that the original sentence is well-written; therefore retaining words specific to the sentence (e.g. “uncertain” in Ex3) can be a good strategy (Clarke and Lapata, 2007). In the ESL context, however, even specific words could still be redundant. For example, although “uncertain” is specific to Ex3, it is redundant, because its meaning is alread</context>
<context position="22078" citStr="McDonald, 2006" startWordPosition="3670" endWordPosition="3671">to measure its redundancy, as deducted in Equation 2. contrib We compute the word/phrase’s contribution to meaning, according to Equation 3. We consider the combinations of measures. trigram + Ccalign # We use a linear combination between language model and align # (Equation 2). We tune Cc on development data. trigram+contrib This measure (as we proposed in Section 3) is the sum of the trigram language model component and the contrib component which represents the phrase’s contribution to meaning. trigram+α round-trip/sig-score We combine language model with round-trip and sig-score linearly (McDonald, 2006; Clarke and Lapata, 2007). To obtain baselines that are as strong as possible, we tune the weight α on evaluation data for best accuracy. 4.2 Pivot Languages Our proposed model uses machine translation outputs from different pivot languages. To see which language helps measuring redundancy, we compare 52 pivot languages available at Google translate3 for meaning representation4. 2We extend this measure, which was only defined for content words in Clarke and Lapata (2007), to include all English words. 3http://translate.google.com 4These languages include Albanian (sq), Arabic (ar), Azerbaijan</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, volume 6, pages 297–304. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Generating confusion sets for context-sensitive error correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>961--970</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="5103" citStr="Rozovskaya and Roth, 2010" startWordPosition="809" endWordPosition="812">redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence simplification system might rewrite it into “reviews”;</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2010. Generating confusion sets for context-sensitive error correction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 961–970, Cambridge, Massachusetts. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="23892" citStr="Stolcke, 2002" startWordPosition="3957" endWordPosition="3958"> In this study, we only consider the cases where the suggested correction is to delete the redundant part (97.09% among all Rloc errors). To construct our evaluation dataset, we pick sentences with exactly one redundant word/phrase. This is the most common case (81.18%) among sentences containing redundant words/phrases. We use 10% of the essays (336 sentences) for development purposes, and another 200 essays as the evaluation corpus (527 sentences). A distribution of redundant chunks’ lengths in evaluation corpus is shown in Table 1. We train a trigram language model using the SRILM toolkit (Stolcke, 2002) on the Agence France-Presse (afp) portion of the English Gigawords corpus. 5 Experiments The experiment aims to address the following questions: (1) Does a sentence’s translation serve as a reasonable approximation for its meaning? (2) Byelorussian (be), Bulgarian (bg), Icelandic (is), Polish (pl), Persian (fa), Boolean (language ((Afrikaans) (af), Danish (da), German (de), Russian (ru), French (fr), Tagalog (tl), Finnish (fi), Khmer (km), Georgian (ka), Gujarati (gu), Haitian (Creole (ht), Korean (ko), Dutch (nl), Galician (gl), Catalan (ca), Czech (cs), Kannada (kn), Croatian (hr), Latin (l</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Strunk</author>
</authors>
<title>The elements of style / by</title>
<date>1918</date>
<location>William Strunk, Jr.</location>
<contexts>
<context position="3087" citStr="Strunk, 1918" startWordPosition="496" endWordPosition="497">guage model score of the sentence after the deletion of a word or a phrase. The meaning preservation component makes use of the sentence’s translation into another language as pivot, then it applies a statistical machine translation (SMT) alignment model to infer the contribution of each word/phrase to the meaning of the sentence. As a first experiment, we evaluate our measures on their abilities in picking the most redundant phrase of a given length. We show that our measure is five times more accurate than a random baseline. 2 Redundancies in ESL Writings According to The Elements of Style (Strunk, 1918): concise writing requires that “every word tell.” In that sense, words that “do not tell” are redundant. Determining whether a certain word/phrase is redundant is a stylistic question, which is difficult to quantify. As a result, most annotation resources do not explicitly identify redundancies. One exception is the NUCLE corpus. Below are some examples from the NUCLE corpus, where the bold-faced words/phrases are marked as redundant. Ext: First of all, there should be a careful consideration about what are the things that governments should pay for. 683 Proceedings of the 14th Conference of </context>
</contexts>
<marker>Strunk, 1918</marker>
<rawString>William Strunk. 1918. The elements of style / by William Strunk, Jr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Jennifer Foster</author>
<author>Martin Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>353--358</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="5051" citStr="Tetreault et al., 2010" startWordPosition="801" endWordPosition="804">gle solution for adaptation... Note that unlike redundancies, these undesirable words/phrases change the sentences’ meanings. Despite the difference in definitions, our experimental work uses the NUCLE corpus because it provides many real world examples of redundancy. While redundancy detection has not yet been widely studied, it is related to several areas of active research, such as grammatical error correction (GEC), sentence simplification and sentence compression. Work in GEC attempts to build automatic systems to detect/correct grammatical errors (Leacock et al., 2010; Liu et al., 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2010). Both redundancy detection and GEC aim to improve students’ writings. However, because redundancies do not necessarily break grammaticality, they have received little attention in GEC. Sentence compression and sentence simplification also consider deleting words from input sentences. However, these tasks have different goals. Automated sentence simplification (Coster and Kauchak, 2011) systems aim at reducing the grammatical complexity of an input sentence. To illustrate the difference, consider the phrase “critical reception.” A sentence si</context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>Joel Tetreault, Jennifer Foster, and Martin Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 353–358, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>