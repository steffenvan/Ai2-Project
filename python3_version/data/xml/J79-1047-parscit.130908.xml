<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.670137">
American Journal ot Computational Linguistics Microfiche 47
</note>
<sectionHeader confidence="0.925302" genericHeader="method">
A SURVEY OF
SYNTACTIC ANALYSIS PROCEDURES
FOR NATURAL LANGUAGE
RALPH GRISHMAN
</sectionHeader>
<subsectionHeader confidence="0.278351">
Computer Science Department
</subsectionHeader>
<note confidence="0.6900162">
Courant-Institute of Mathematic41 Sciences
New York University
251 Mercer Street, New York. 10012
This survey was prepared under contract No. N00014-67A-0467-0032 with the
Office of Naval Research, and was originally issued as Report No. NSO-8 of
</note>
<title confidence="0.6607982">
the Courant Institute of Mathematical Sciences, New York University.
Copyright 0 1976
Association for Computational Linguistics
A SURVEY OF SYNTACTIC ANALYSIS PROCEDURES
FOR NATURAL LANGUAGE
</title>
<author confidence="0.823836">
RALPH GRISHMAN
</author>
<affiliation confidence="0.932513">
Computer Science Department
Courant Institute of Mathematical Sciences
New York University
</affiliation>
<bodyText confidence="0.758923833333334">
This survey was prepared under contract
No. N00014-67A-0467-0032 with the Offipe
Naval Research, and was originally issued os
Report No. NS0-8 of the Courant Institute of
Mathematical Sciences, New York University.
SUMMARY
This report includes a brief discussion of the
role of automatic syntactic analysis, a survey
of parsing procedures used in the analysis of
natural language, and a discussion of the
approaches taken to a number of difficult lin-
guistic problems, such as conjunction and gra-
ded acceptability. It also contains precise
specifications in the programming language SETL
of a number of parsing algorithms, including
several context-free parsers, a unrestricted
rewriting rule parser, and a transformational
parser.
</bodyText>
<page confidence="0.4333635">
2
3
</page>
<tableCaption confidence="0.862671">
Table of Contents
</tableCaption>
<figure confidence="0.293428">
Page
1. INTRODUCTION 4
</figure>
<address confidence="0.8171235">
1.1 The Role of Syntactic Analysis
1.2 Computational and Theoretical Linguistics L)
2. GENERAL SURVEY OF PARSING PROCEDURES 11
2.1 Early Systems: Context-Free and Context-Sensitive
Parsers 11
2.2 Transformational Analyzers: First Systems 14
2.3 Transformational Analyzers: Subsequent DevelopmetS
2.4 Other Syntactic Analysis Procedures ,1
2.5 Parsing with Probability and Graded Acceptability.
2.6 Conjunction and Adjunction
</address>
<sectionHeader confidence="0.442118" genericHeader="method">
3. ALGORITHM SPECIFICATIONS 71
</sectionHeader>
<subsectionHeader confidence="0.981465">
3.1 Parsing Algorithms for Context-Free Grammars 31
3.2 A Parser for Unrestricted Rewriting Rule Grammars • 54
3.3 Parsing Procedures for Transformational Grammars ▪ 60
</subsectionHeader>
<note confidence="0.235022">
APPENDIX. A Very Short Introduction to SETL 84
BIBLIOGRAPHY 92
</note>
<sectionHeader confidence="0.951199" genericHeader="method">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.996525642857143">
The Computer Science Department of New York University,
under contract to the Office of Naval Research, has prepared
a series of reports examining selected areas of artificial
intelligence. We hope in these eritical surveys to place in
perspective the main lines of past research and thereby perhaps
to suggest fruitful directions for future work. As part of
these surveys we have prepared precise specificatiOns, in the
programming language SETL, of some of the basic algorithms in
each area. These specifications are intended to provide clear
points of reference for structuring our review of past work.
This first report is concerned with natural language proces-
sing systems, systems which are able to accept instructions &apos;or
data in natural language. In iery general terms, these systems
process the text through several stages:
</bodyText>
<listItem confidence="0.9162631875">
(1) syntactic: analyzes the structure of each sentence (or
other text unit) and rearranges the elements of the sentence
to simplify its structures; this stage should recognize
paraphrases due to alternatiNre arrangements of words in a
sentence
(2) semantic; restructures the sentences into the form used
for internal processing, such as inference or data retrieval;
depending on the application, the output may be a command in
an information retrieval language, a structure based on some
set of semantic &amp;quot;primitives&amp;quot;, or a tabular structure suitable
as a data base; this stage should recognize some of the
paraphrases due to alternative choices of words.
(3) pragmatic: interprets the text based on particular context
(problem situation or data base); this stage should recognize
sentences which are equivalent in effect.(such as &amp;quot;Throw that
switch.&amp;quot; and &amp;quot;Turn on the light.&amp;quot;).
</listItem>
<bodyText confidence="0.997207458333333">
The reader will note that these stages are very- vaguely charac-
terized. Current language processing systems differ very greatly
in their structure and not even these general divisions can
be identified in all systems.
Ine pragmatic stage is the most heterogeneous and the common
threads which do appear are based more on genera/ problem-solving
methods than on specifically linguistic techniques. Since the
semantic stage maps into the notation required by the pragmatics,
it is correspondingly varied. There is, however, a fair amount
of current research on the selection of semantic primitives or
semantic classes; some of this research is reviewed in the
proceedings of a recent Courant Institute symposium on Directions
in Artificial Intelligence (Courant Computer Science Report No. 7).
The syntactic stage is by far the best established aftd_most
clearly defined. There is a general (although far from total)
agreement on the most -,basic underlying principles, and there are
a number of widely-used procedures. For this stage, therefore,
it seems possible to present a survey of current research in
some organized fashion. In the report which follows, we have
endeavored to show tire relation between the various syntactic
analyzers in terms of their historical development, linguistic
assumptions, and alalysis procedures. For a broader survey of
automated language processing, readers are referred to [Walker
1973].
</bodyText>
<subsectionHeader confidence="0.997793">
1.1 The Role of Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.997063826086957">
The systems we shall be describing are all motivated by
particular applications requiring natural language input, rather
than by purely linguistic considerations. Consequently, the
p,arsing of a text (determining its structure) Will be viewed as
an essential step preliminary to processing the information in
the text, rather than as an end in itself.
There are a wide variety of applications involving natural
language input, such as machine translation, information retrieval,
question answering, command systems, and data collection. It may
therefore seem at first that there would be little text processing
which would/be generally useful
structural description (e.g. a
There are, however, a numbet of
sentence structure, and thereby
cation-specific processing. For
sentences (enclosed in brackets
omitted or &amp;quot;zeroed&amp;quot;:
beyond the determination of a
parse tree) for each sentence.
operations wi-OLch can regularize
simplify _the subsequen appli-
example, some material in
in the examples below) can be
</bodyText>
<page confidence="0.906535">
6
</page>
<bodyText confidence="0.824073">
John ate cake and Mary [ate] cookies.
</bodyText>
<listItem confidence="0.993386333333333">
• . five or more [than five] radishes
He talks faster than John [talks].
• . . the man [whom] I met . .
</listItem>
<bodyText confidence="0.981343945454546">
Sentence structure can be regularized by restoring such zeroed
information. Other transformations can relate sentences with
normal word order (I crushed those grapes. That I like wine
is evident.) to passive (Those grapes were crushed by me.) and
cleft (It is evident that I like wine.) constructions, and can
relate nominal (the barbarians&apos; destruction of Rome) and verbal
(the barbarians destroyed Rome) constructions. Such transforma-
tions will permit further (e.g., semantic) processing to concern
itself with a much smaller numper of structures. In addition,
if the structures are appropriately chosen, operator-operand
relations should be clearly evident in the output of the
syntactic stage.
Some lexical processes, such as nominalization and lexical
decomposition, are considered syntactic by some and semantic by
others. Whether a clear division between the syntactic and
semantic stages is possible at all has been a major point of
controversy in linguistics -- between interpretive and genera-
tive semanticists -- over the past decade. We may therefore
expect that, while some transformations will clearly be the
province of the syntactic stage and others the province
of the semantic stage, there will be a considerable fuzzy
area-in between. This, however, should not disqualify
automatic Syntactic analysis as an area of separate research;
there is hardly a field of science or engineering which is clearly
delineated from its neighbors.
The last few years have seen most work in language processing
devoted to the development of integrated systems, combining
syntactic, semantic, pragmatic, and generative components. This
was a healthy and predictable reaction to the earlier research,
which had largely approached syntactic processing in isolation
from these other areas. It produced some systems whose modest
successes dispelled the skepticism that natural language proces-
sors would ever be able to do anything. These systems indicated
how syntactic, semantic, and pragmatic information must interact
to select the correct sentence analysis.
It is now generally understood that syntactic processing by
itself is inadequate to select the intended analysis of a sentence.
We should not conclude from this, however, that it is impossible
to study the processes of syntax analysis separately from the
other components. Rather, it means that syntax analysis must
be studied with an understanding of its role in a larger system
and the information it should be able to call upon from other
components (i.e., the processing which the subsequent components
must do to select among the analyses produced by the syntactic
component).
While recognizing the importance of total systems in insuring
that none of the problems has fallen in the gaps between stages
and been forgotten, it still seems that more specialized research
projects are essential if the field-of natural language proces-
sing is to mature. The development of another total system will
not advance the field unless it endeavors to perform some
particular processing task better than its predecessors; the
problems are too vast for each research project to usefully
attack the problems involved in all the phases of processing at
once.
</bodyText>
<equation confidence="0.350433">
* * *
</equation>
<bodyText confidence="0.63122775">
Some researchers have asserted recently that hatural language
processing can be done without syntax analysis. It seems to us
that such claims are exaggerated, but they do arise out of some
observations that are not without validity:
</bodyText>
<listItem confidence="0.793317">
(1) For the relatively simple, sentences whose semantics is
</listItem>
<bodyText confidence="0.975661">
within the scope of current artificial intelligence systems,
sophisticated syntactic processing is unnecessary.
This Was certainly true of some early questiOn-answering systeTqs,
whose syntax was limited to a few fixed imperative structures,
into which adjective and prepositional phraSe modifiers could be
inserted. It is questionable whether this is true of the most
syntactically sophisticated of today&apos;s systems (such as Petrick&apos;s)
In any case, it is hard to imagine how sentences of the complexity
typical in technical writing could be understood without utilizing
syntactic (as well as semantic) restrictions to select the
</bodyText>
<listItem confidence="0.899980166666667">
correct analysis.
(2) Syntactic analysis may appear in guises other than the
traditional parsing procedures; it can be interwoven with
other components of the system and can be embedded into the
analysis programs themselves. This will often increase the
parsing speed considerably.
</listItem>
<bodyText confidence="0.992713272727273">
The &amp;quot;grammar in program&amp;quot; approach which characterized many of
the early machine translation efforts is still employed in some
of today&apos;s systems. Its primary justification seems to be
parsing efficiency, but this should be a secondary consideration
for research purposes at present, singe most current systems are
able to parse (or, as often, reject as unanalyzable) a sentence
in under a minute. More important as research goals should be
the ability to manage grammatical complexity and the ability to
communicate successful methods to others. In both these regards,
a syntactic analyzer using aunified, semiformal set of rules
is bound to be more effective.
</bodyText>
<listItem confidence="0.9698">
(3) Syntax analysis can be driven by semantic analysis (instead
of being a separate, earlier stage), and, in particular,
can be done by looking for semantic patterns in the sentence.
</listItem>
<bodyText confidence="0.868665846153846">
Syntax analysis is done separately because there are rules of
sentence formation and transformation which can be stated in
terms of the relatively broad syntactic categories (tensed verb,
count noun, etc.). If the semantic classes are subcategori-
zations of the syntactic ones, then clearly the transformations
could be stated in terms of sequences of semantic classes.
For those transformations which are properly syntactic, however,
we would find that several transformations at the semantic stage
would be required in place of one at the syntactic stage;
certain useful generalizations would be lost.
The strongest argument of those advocating a semantics-driven
syntax is the ability of people to interpret sentences from
semantic clues in the face of syntactic errors or missing infor-
mation (&amp;quot;I want to xx to the movies tonight.&amp;quot;). This argument
works both ways, however -- people can also use syntactic rules
when semantics is lackinq; for example, to understand the function
of a word in a sentence without knowing its meaning (&amp;quot;Isn&apos;t that
man wearing a very frimple coat?&amp;quot;). Ultimately, we want an
analyzer which can work from partial information of either kind,
and research in that direction is to be welcomed (some work on
parsing in the face of uncertainty has been done by speech-under-
standing groups). At the same time, since successful processing
of &amp;quot;perfect&amp;quot; sentences is presumably a prerequisite for processing
imperfect sentences, it seems reasonable to continue devoting
substantial effort to the considarable problems which remain in
analyzing perfect sentences.
</bodyText>
<subsectionHeader confidence="0.938659">
1.2 Computationaa and Theoretical Linguistics
</subsectionHeader>
<bodyText confidence="0.888971">
Theoretical linguists and the sort of computational
</bodyText>
<footnote confidence="0.9089374">
linguists we have been considering espouse quite different
research objectives. A primary interest of transformational
linguists is explaining grammatical competence -- how people
come to accept some sentences as grammatical and reject others
as ungrammatical. In partiCular, they are concerned with
</footnote>
<page confidence="0.7234">
10
</page>
<bodyText confidence="0.987061242424243">
language universals -- principles of grammar which apply to all
natural languages.
Computational linguists, in contrast, are usually delighted
if they can manage to handle one language (two, if they&apos;re
translating). Their primary concern lies in transforming
sentences -- often assumed to be grammatical -- into a form
acceptable to some particular application system. They are
concerned with the efficiency of such processing, whereas
theoretical linguists generally don&apos;t worry about the recogni
tion problem at all.
Nonetheless, the two specialties should have many common
areas of interest. Questions of grammaticality are important,
because exuerience has shown that a grammatical constraint
whichlin one case determines if A sentence is or is not
acceptable will in other cases be needed to choose between
correct and incorrect analyses of a sentence. The relations
between sets of sentences, which are a prime focus of transfor-
mational grammar, particularly in the Harrisian framework, are
crucial to the success of syntactic analysis procedures, since
they enable a large variety of sentences to be reduced to a
relatively small number of structures.
More generally, both specialties seek to understand a
particular mode&apos;of communication. Traditional linguists are
interested in a mode which has evolved as an efficient means
of communicating ideas between people; ultimately, we may hope
that they will understand not only the principles of language
structure, but also some of the reasons why language has
developed in this way. Computational linguists, in studying
how language can be used for man-machine communication, are
really asking much the same questions. They want to develop
a mode of coituaunication for which people are naturally suited
and they want to understand the principles for designing
languages which are efficient for commanicating ideas.
</bodyText>
<page confidence="0.937056">
11
</page>
<sectionHeader confidence="0.645417" genericHeader="method">
2. GENERAL SURVPI OF PARSING PROCEDURES
</sectionHeader>
<bodyText confidence="0.987424166666667">
We can impose several rough groupings on the set of parsers
in order to structure the following survey. To begin with, we
may try to separate those systems developed with some reference
to transformational theory from the nontransformational systems.
This turns out also to be an approximate historical di.vision,
since most systems written since 1965 have made some connection
with transformational theory, even though their methods of
analysis my 1,)e only di&apos;Stantly related to transformational
mechanisms.
The transformational systems may in turn be divided into
those parsers which have been systematically derived from a
specific transformational generative grammar and those which
have &amp;quot;sacrificed&amp;quot; this direct connection with a generative
grammar in order to obtain a more direct and efficient algorithm
for recovering base structures. This division appears to be
in part a result of our inadequate theoretical understanding of
transformational grammars, and may be reduced by some recent
theoretical work on transformational grammar.
</bodyText>
<subsectionHeader confidence="0.983124">
2.1 Early_Syst9ms: Contexts-Free and Context-Sensitive Parsers
</subsectionHeader>
<bodyText confidence="0.995708333333333">
The pretransformational systems, developed mostly between
1959 and 1965, were, with a few exceptions, parsers for context-
free languages, although cloaked in a number of different guises.
These systems were based on immediate constituent analysis,
dependency theory, linguistic string theory, or sometimes no
theory at all.
The largest and probably the most important of these early
projects was the Harvard Predictive Analyzer [Kuno 19621. A
predictive analyzer is a top-down parser for context-free
grammars written in Greibach normal form; this formulation of
the grammar was adopted from earlier work by Ida Rhodes for
her Russian-English translation project. The size of the
</bodyText>
<page confidence="0.342831">
12
</page>
<bodyText confidence="0.930873117647059">
grammar was staggering: a 1963 report [Kuno 19631 quotes
figures of 133 14ord classes sand about 2100 productions. Even
with a grammar of this size, the system did not incorporate
simple agreement restrictions of English synta2 Since the
program was designed to produce parses for sentences which
were presumed to be grammatical (and not to differentiate
between grammatical and nongrammaticaI septences), it was
at first hoped that it could operate without these restric-
tions. It was soon discovered, however, that these restric-
tions were required to eliminate invalid analyses of grammatical
sentencds. Because the direct inclusion of, say, subject-verb
number agreement would cause a large increase in an already
very large grammar, the Harvard group chose instead to include
a special mechanism in the parsing program to perform a
rudimentary check on number agreement. Thus the Harvard Predic-
tive Analyzer, though probably the most successful of the
context-free analyzers, clearly indicated the inadequacy of
a context-free formulation of natural language grammar.
The Harvard Predictive Analyzer parsing algorithm progressed
through several stages. The first version of the predictive
analyzer produced only one analysis of a sentence. The next
version introdtced an automatic backup mechanism in order to
produce all analyses of a sentence. This i5 an exponential time
algorithm, hence very slow or long sentences; a 1962 report
gives typical times as 1 minute for an 18 word sentence and
12 minutes for a 35 word sentence. An improvement of more than
an order of magnitude was obtained in the final version of the
program by using a bit matrix for a path-elimination technique
[Kuno 1965]. When an attempt was made to match a nonterminal
symbol to the sentence beginning at a particular word and no
match was found/ the corresponding bit was turned on; if the
same symbol came up again later in the parsing at the same
point in the sentence, the program would not have to try to
match it again.
</bodyText>
<page confidence="0.513862">
13
</page>
<bodyText confidence="0.99005346">
Another important early parser was the immediate constituent
analyzer used at RAND. This system used a grammar in Chomsky
normal form,and a parsing algorithm designed by John Cocke,
Which produced all analyses bottom-up in a single left-to-
right scan of the sentence [Hays 1967]. This was a fast
algorithm but- because all parses were developed simultaneously,
it needed a lot of space for long sentences; the Rand system
appears therefore to have been limited to sentences of about
30 words.
A differeht bottom-up analysis procedure was used ip the
first linguistic string analysis program developed at the
University of Pennsylvania [Harris 1965]. This procedure,
called a cycling cancelling, automaton, makes a series of left-
to-right passes through tie sentence; in each pass one type of
reduction was &apos;performed. The string parser recOgnized two
classes of stri&apos;ngs: first order, ,not containing verb-object,
and second order, containing verb-object; the reduction-of the
sentences was correspondingly done in two stages. In addition
to these reductions, which corresponded to context-free rules
the parsipg program also included some syntactic restrictions
which were checked when seconcil order strings were reduced.
A system incorporating this dycling automaton scheme was later
used by Bross at ROsewll Park for the analysis of medical
reports [Bross 1968, Shapiro 1971].
As far as we know, on,;.y one major parsing system has been
developed using a context-sensitive phrase structure grammar.
This was DEACON, Direct English Access and Control, which was
designed as a natural laqguage interface to a comrand,contrcol,
and information retrieval system for the Army and was developed
at General Electric [Craig 1966]. DEACON was one of the first
systems to provide flexible, systematic interaction between
the parser and the semantic component. Associated with each
production in the grammar was a semantic rule. These rules
operated on a ring-structured data base and had the functions
14
of locating, adding, and changing information in the data base.
The parsing was done bottom-up, developing all analyses of the
sentence in parallel. As each reduction was performed, the
associated semantic rule was invoked. In the dase of a query,
the sequence of rules associated with the correct analysis was
supposed to locate the desired answer in the data base. In
some cases a rule could not be applied to the data base (e.g.,
a particular relation between two items did not exist); the
rule then returned a failure signal to the parser, indicating
that the analysis was semantically anomalous, and this analysis
was aborted.
Woods&apos; has noted [Woods 197(.] that the parser used in the
DEACON project may produce redundant parses, and has given
a parsing algorithm for context-sensitive languages which
remedies this deficiency.
</bodyText>
<subsectionHeader confidence="0.999235">
2.2 Transformational Analyzers: First Systems
</subsectionHeader>
<bodyText confidence="0.947569647058824">
When the theory of transformational grammar was elaborated
in the early 1960&apos;s there was considerable interest in finding
a corresponding recognition procedure. Because the grammar is
stated in a generative form, however, this is no simple matter.
A (Chomsky) tree transformational grammar consists of a set
of context-sensitive phrase structure rules, which generate a
set of base trees, and a set of transformations, which act on
the base trees to produce the surface trees. A (Harris) string
transformational grammar consists of a finite set of sequences
of word categories, called kernel sentences, and a set of
transformations which combine and modify these kernel sentences
to make the other sentences of the language. There are at
least three basic problems in reversing the generative process:
(I) for a tree transformational grammar, assigning to a given
sentence a set of parse trees which includes all the surface
trees which would be assigned by the transformational grammar
15
</bodyText>
<listItem confidence="0.97270125">
(2) given a tree not in the base, determining which sequences
of transformations might have applied to generate this tree
(3) having decided on a transformation whose result may be
the present tree, undoing this transformation
</listItem>
<bodyText confidence="0.999792103448276">
If we attack each of these problems in the most straightforward
manner, we are likely to try many false paths which will not
lead to an analysis. For the first problem, we, could use a
context-free grammar which will give all the surface trees
assigned by the transformational grammar, and probably lots more
The superabundance of &apos;false&amp;quot; surface trees is aggravated by the
fact that most English words have more than one word category
(play more than one syntactic role), although normally only one
is used in any given sentence. For the second and third problems, we
can construct a set of reverse transformations; however, since
we are probably unable to determine uniquely in advance the
transformations which produced a given tree, we will have to
try many sequences of reverse transformations which will not
yield a base tree.
Because of these problems, the earliest recognition procedure,
suggested by Matthews, was based on the idea of synthesizing trees
to match a given sentence. Although some checks were to have
been made against the sentence during the generation procedure,
it was still an inherently very inefficient procedure and was
never implemented. Twa major systems were developed in the
mid-60&apos;s, however, which did have limited success: the system
of Zwicky et al. at MITRE and that of Petrick.
The transformational generative grammar from which the MITRE
group worked had a base coMponent with about 275 rules and a set
of 54 transformations [Zwicky 1965]. For the recognition proce-
dure they developed manually a context-free &amp;quot;covering&amp;quot; grammar
with about 550 productions to produce the surface trees and a
set of 134 reverse transformational rules. Their recognition
procedure had four phases:
</bodyText>
<footnote confidence="0.5541275">
(1) analysis of the sentence using the context-free covering
grammar (with a bottom-up parser)
</footnote>
<note confidence="0.261142">
16
</note>
<bodyText confidence="0.866802588235294">
(2) application of the reverse transrormational rules
&apos;0) for each candidate base tree produced by steps (1) and (2),
a check whether it can in fact be generated by the base
component
(4) for each base tree and sequence of transformations
which passes the test in step (3), the (forward) trans-
formations are applied to verify that the original
sentence can in fact be generated
(The final check in step (4) is required bocause the covering
grammar may lead, to spurious matches of a transformation to the
sentence in the reVerse transformational process and because
the reverse transformations may not incorporatd all the
constraints included in the forward transformations.) The
covering grammar produced a large number of spurious surface
analyses which the parser must process. The 1965 report for
example, cites a 12 word sentence which produced 48 parses
with .&apos;the covering grammar; each must be followed through steps
(2) and (3) before most can be eliminated. The system was
therefore very slow; 36 minutes were required to analyze one
11 word sentence.
Two measures were taken by the MITRE group to speed up the
program: &amp;quot;super-trees&amp;quot; and rejection rules [Walker 1966].
&amp;quot;Super-trees&amp;quot; was the MITRE term for a nodal span representation
in which several parse trees were represented in a single
structure. They intended to apply the reverse transformations
to these super-trees, thus processing several possible surface
trees simultaneously; it is not clear if they succeeded in
implementing this idea. Rejection rules were tests which were
applied to the tree during the reverse transformational process
(step (2) above), in order to eliminate some trees as early
as possible in the parsing. The rejection rules incorporated
some constraints which previously were only in the forward
transformational component, and so eliminated some trees in
step (2) which before had survived to step (4). The rejection
</bodyText>
<figure confidence="0.756878333333333">
17
rules had a significant effect on parsing times -A- the 11 word
1
</figure>
<bodyText confidence="0.982307983870968">
sentence which took 36 minutes before now took only 6 -2-
The system developed by Petrick [Petrick 1965, 1966;
Keyser 1967] is similar in outline: applying a series of
reverse transformation, checking if the resultin/ tree can
be generated by the base component, and the: verifying the
analysis by applying the forward transfoLmations to the base
tree. There are, however, several dilfferences from the MITRE
system, motivated by the desire to have a parser which could
be produced automatically from the generative formulation of
the grammar. Petrick devised a procedure to generate, from
the base component and transformations, an enlarged context-
free grammar sufficient to analyze the surface sentence struc-
tures. He also automatically converted a set of forward trans-
formations me.,..sting certain conditions into pseudo-inverse
(reverse) transformations. His parsing procedure also aiffered
from the MITRE algorithm in the way in which the reverse
transformations are applied. In the MITRE program reverse trans-
formations operated on a sentence tree, just like foward
transformations in a Chomsky grammar. Petrick, on the other hand,
did not construct a surface tree in the analysis phase; when a
particular reverse transformation came up for consideration, he
built just enough structure above the sentence (using the enlarged
context-free grammar) to determine if the transformation was
applicable. If it was, the transformation was applied and the
structure above the sentence then torn down again; what was
passed from one reverse transformation to the next was only
the string of word categories. In the verifying phase, of course,
Petrick had to follow the rules of Chomsky grammar and apply
the forward transformations to a sentence tree.
The price for generality was paid in efficiency. Petrick&apos;s
problems were more severe than MITRE&apos;s for two reasons. First,
the rAbsence of a sentence tree during the application of the
reverse transformational rules meant that many sequences of
18
revrse transformations were tried which did not correspond to
any sequence of tree transformations and hence would eventually
be rejected. Second, if several reverse transformations Could
apply at some point in the analysis, the procedure could not
tell in advance which would lead to a valid deep structure.
Consequently, each one had to be tried and the resulting struc-
ture followed to a deep structure of a &amp;quot;dead end&amp;quot; (where no
more transformations apply). This produces a growth in the number
of analysis paths which is exponential in the number of reverse
transformations applied. This explOsion can be avoided only if the
reverse transformations include tests of the current analysis tree
to determine which transformations applied to generate this tree.
Such tests were included in the manually prepared reverse trans-
formations of the MITRE group, but it would have been far too
complicated for Petrick to produce such tests automatically when
inverting the transformations.
Pfetrick&apos;s system has been significantly revised over the
past decade [Petrick 1973, Plath 1974a]. In the current system
the covering grammar and reverse transformations are both
prepared manually. The transformational decomposition process
works on a tree (as did MITRE&apos;S), and considerably flexibility
has been provided in stating the transformations and the condi-
tions of applicability. The transformations and conditions
may be stated either in the traditional form (used by linguists)
or in terms of elementary operations combined in LISP procedures.
The resulting system is fast enouqh to be used in an information
retrieval system with a grammar of moderate size; most requests
are processed in less than one minute.
</bodyText>
<subsectionHeader confidence="0.940421">
2.3 Transformational Analyzers: Subsequent Developments
</subsectionHeader>
<bodyText confidence="0.982097315">
One result of the early transformational systems was a
recognition of the importance of finding an efficient parsing
procedure if transformational analysis was ever to be a useful
19
technique. As the systems indicated, there are two main
obstacles to an efficient procedure. First, there is the problem
of refining the surface analysis, so that each sentence produces
fewer trees for which transformational decomposition must be
attempted. This has generally been approached by using a more
powerful mechanism than a context-,free parser for the surface
analysis. Second, there is the problem of determining the base
structure (or kernel sentences) from the surface structure in a
relatively direct fashion. This has generally been done by
associating particular rules for building the deep structure
with rules of the surface structure analysis. The approach
here has generally been ad hoc, developing a reverse mapping
without plicit reterence to a corresponding set of forward
transformations.
Several groups which have played a significant role in the
development of current parsing systems have been tied together
by their common use of recursive transition networks. Although
their use of these transition networks is not central to their
basic contribution, it is frequently referred to and so deserves
a few words of explanation. A transition network is a set of
nodes (including one initial and at least one terminal node)
and a set of directed arcs between the nodes, labeled with
symbols from the language; it is a standard representation for
regular languages. A recursive transition network is a set of
transition networks in which the arcs of one network may also
be labeled with the names of other networks; it is a form of
representation of context-free languages. In contrast to the
usual context-free phrase structure grammars, this is equivalent
to allowing regular expressions in place of finite sequences of
elements in productions. This does not increase the weak
generative capactity of the grammars, but allows nonrecursive
formulations for otherwise recursive constructions.
The first system using such a network was developed by
Thorne, Bratley, and Dewar at Edinburgh [Thorne 1968, Dewar 1969].
They started with a regular base grammar, i.e., a transition
20
network. The importance of using a regular base lies in their
claim that gome transformations are equivalent in effect to
changing the base to a recursive transition network. Transfor-
mations which could not be handled in this fasion, such as
conjunction, were incorporated inw the parsing program. Parsing
a sentence with this surface grammar should filPen also give some
indication of the associated base and transfo.rmational structure.
Their published papers ao not describe, however, thee process by
which the surface grammar is constructed and so it is not clear
just how the transformation and base structure is extracted
from their parse.
The recursive transition network was deVeloped into an
augmented recursive transition network grammar in the system of
Bobrow and Fraser Mohrow hn augmented network is one
in which an arbitrary predicate, written in same general purpose
language (in this case, LISP).may be associated with each arc in
the network. A transition in the network is not allowed if the
predicate associated with the arc fails. These predicates
perform two functions in the grammar. First, they are used
to incorporate restrictions in the language which would be
difficult or impossible to state within the oentext-free
mechanisms of the recursive network, e.g., agreement restrictions.
Second, they are used to construct the deep structure tree as the
sentence is being parsed.
The aUgmented transition network was further developed by
Woods at Bolt Beranek and Newman. In order to regularize the
predicates, he introduced a standard set of operations for
building and testing the deep structure [Woods 197010]. He
considerably enlarged the scope of the grammar and added a
semantic component fbr translating the deep structure into
information retrieval commands. With these additions, the
system served as a moderately successful natural language input
interface to a retrieval system for data about moon rocks [Woods
1972, 19731. The augmented transition network, and in parti-
cular the formalism developed by Woods, has proven to be an
Zi
effective instrument for constructing natural language front-ends
which is relatively simple to Lmplement and use; it is probably
the most widely used procedure today.
Like several of the systems described above, Proto-RELADES,
developed 1 IBM Cambridge [Culicover 1969], tried to obtain an
efficient transformational decomposition algorithm by linking
the rules for building the deep structure to the productions
of the surface grammar. Their surface grammar was also augmented
by restrictions (in PL/I this time). However, their system
differed from those mentioned earlier in several important
respects: First, the surface grammar allowed context-sensitive
as well as context-free rules. Second, the rules which built
the deep structure during the parse were in the form of reverse
transformations acting on an (incomplete) sentence tree (in
contrast to the rules used by Woods, for example, which first
put- wo-r-ds- -into registers labeled &amp;quot;subject&amp;quot;, &amp;quot;verb&amp;quot;, and &amp;quot;object&amp;quot;
and later build a tree out of them). Proto-RELADES was tested
as a restricted English language preprocessor for a library card
catalog retrieval system [Loveman 1971].
One drawback of these procedures was the relatively ad hoc
methods, from a linguistic point of view, used to construct the
surface grammars and to tie them in to the appropriate reverse
transformations. A more principled approach to transformational
decomposition was proposed by Joshi and Hiz [Joshi 1962, Hiz 1967]-
In contrast to the systems described above, their procedure
was based on Harris&apos; string transformational grammar.
One advantage of the Harrisian theory over that of Chomsky is
the theoretical basis it provides for the segmentation of the
sentence into &amp;quot;linguistic strings&amp;quot; (Chomsky&apos;s theory, in
contrast, makes no general assertions about the surface structure
of sentences.) The procedure of Joshi and Hiz was predicated on
the claim that, from an analysis of the sentence into linguistic
strings, one could directly determine the transformations which
acted to produce the sentence, without having to try many sequences
22
af reverse transformations. Their proposed system therefore
consisted of a procedure for linguistic string analysis (a
context-free parsing problem at the level of simplification
of their original proposal) and a set of rules which constructed
from each string a corresponding kernel-like sentence.
Their original proposal was a simplified scheme which
accounted for only a limited set of transformations. It has
been followed by agood deal of theoretical work on adjunct
grammars and trace conditions [Joshi 1973] which has laid a
formal basis for their procedures. These studies indicate Kow
it may be possible, starting from a transformational grammar
not specifically oriented towards recognition, to determine the
features of a sentence which indicate that a particular transfor-
mation applied in generating it, and hence to produce an effi-
cient analysis procedure.
Another group which has used linguistic string analysis is
the Linguistic String Project at New York University, led by
Sager [Sager 1967, 1973; Grishman 1973a, 1973b]. Their system,
which has gone through several versions since 1965, is based
on a context-free grammar augmented with restrictions. Because
they were conce ned with processing scientific text, rather than
commands or queries, they were led to develop a grammar of
particularly broad coverage. The present grammar has about 250
context-free rules and about 200 restrictions; although nOt as
swift as some of the smaller systems, the parser is able to
analyze most sentences in less than one minute. Because of the
large size of their grammar, this grouptas been particularly
concerned with, techniques for organizing and specifying the
grammar which will facilitate further development. In particular,
the most recent implementation of their system has added a special
language designed for the economical and perspicuous statement
of the restrictions [Sager 19751.
One of the earlier versions of this „system, with a much more
restricted grammar, was used as the front end for an information
23
retrieval system developed by Cautin at the University of
Pennsylvania (Cautin, 1969).
The Linguistic String Project system has recently been extended
to include a transformational decomposition phase; this phase
follows the linguistiC string analysis [Hobbs 1975]. As in
the case of the Joshi-Hiz parser, the strings identified in
the sentence generally indicate which reverse transformations
must be applied. The transformations are written in an exten-
sion of the language which was used for writing the restrictions.
The ...systems of Woods, Petrick, and Sager exhibit a range of
approaches to the problem of transiormational decomposition.
Their parsing procedures are similar in lany respects: they have
a context-free grammar as the framework for their surface analysis,
and they use procedures both to express grafimatical constraints
and to effect the reverse transformations. Petrick&apos;s system
differs from the others in two primary respects: the restrictions
on the context-free grammar are imposed by filtering transfoLma-
tions which act early in the transformational phase to reject
ill-formed trees, rather than by procedures operating during
the surface analysis. This wolld seem to be disadvantageous
from the point of view of efficiency, since erroneous parses
which might be aborted at the beginning of the surface analysis
must be followed through the entire surface analysis and part
of the transformational decomposition. Second, the transforma-
tions are not associated with particular productions of the
surface grammar, but rather with particular patterns in the
tree (&amp;quot;structural descriptions&amp;quot;), so pattern matching opera-
tions are required to determine which transformations to apply.
These differences reflect Petrick&apos;s desire to remain as close
as is practical to the formalism of transformational linguistics.
The primary distinction of the Woods system is that the deep
structure tree is built during the surface analysis. Conse-
quently, his &amp;quot;transformat±onal&amp;quot; procedures consist of tree
building rather than tree transforming operations. The tradeoffs
between this approach and the two-stage analyzers of Petrick
24
and Sager are difficult to weigh at this time. They are part
of the more general problem of parallel vs. serial processing;
e.g. should semantic analysis be done concurrently with
y*ntactic analysis. Parallel processing is preferred if the
added time required by the deeper analysis is outweighed by the
fraction of incorrect analyses which can be eliminated early in
the parsing process. In the case of s-mantic analysis, it
clearly depends on the relative complexity of the syntactic and
semantic components. In the case of transformational analysis,
it depends on the fraction of grammatical and selectional
constraints which can be expressed at the surface level (if
most of these can only be realized through transformational
analysis, concurrelDt transformational analysis is probably more
efficient). This may depend in turn on the type of surface
analysis; for example, the relationships exhibited by linguistic
string analysis are suitable for expressing many of these
constraints, so there is less motivation in the Linguistic String
Project system for concurrent transformational decomposition.
</bodyText>
<subsectionHeader confidence="0.981984">
2.4. Other Syntactic Analysis Procedures
</subsectionHeader>
<bodyText confidence="0.99918">
The system developed by Winograd at M.I.T. [Winograd 1971)
for accepting English commands and questions about a &amp;quot;block
world&amp;quot; also uses a context-free grammar augmented by re8tric-
tions. Winograd&apos;s context-free grammar was encoded as a set
of procedures instead of a data structure to be interpreted,
but this is not a material difference. His grammar is based
on Halliday&apos;s &amp;quot;systemic grammar&amp;quot; to the extent that it
extracts from a sentence the set of features described by
Halliday; however, Halliday&apos;s grammar (at least in its present
stage of development) is essentially descriptive rather than
generative) so most of the detailed grammatical structure had
to be supplied by Winograd. His parser does not construct a
deep structure; rather, it builds semantic structures directly
</bodyText>
<equation confidence="0.9487875">
r°1 Er-
r..4)
</equation>
<bodyText confidence="0.98549025">
during parsing. The primary distinctive feature of his systeM
is the integration of the syntactic component with semantics
and pragmatics (the manipulation of objects in the block world);
hi $ parser is thus able to use not only syntactic constraints
but also semantic and pragmatic information in selecting a
proper sentence analysis. With regard to the serial vs.
parallel distinction drawn in the previous section, his system
would be characterized as highly patallel.
A number of natural language systems have used grammars
composed of unrestricted phrase-structure rewriting rules.
Since unrestricted rewriting rules, like transformational
grammars, can be used to define any recursively enumerable
language, they may be sufficient for analyzing both surface and
deep structure. As with transformational grammars, it will in
practice be necessary to impose some constraint (such as ordering)
on the rules, so that the language defined is recursive; other-
wise a parser will never be able to determine whether some
sentences are grammatical or not.
One parser for unrestricted rewriting rules was described
by Kay [Kay 19671]. This parser included a number of mechanisms
for restricting the application of rules, such as rule ordering,
specifying part of the structure dominated by one element-of
the rule, or requirifig the equality of the structures dominated
by two elements. These mechanisms do not increase the genera-
tive power of the grammars, but are designed to make grammars
easier to write. Kay described how his parser coOld be used
to effect someiteverse transformations.
Kay&apos;s parser was incorporated into a system called REL
(Rapidly Extensible Langiaage) developed by Thompson, Dostert,
et al. at the California Institute of Technology [Thompson 1969,
Dostert 1971]. Kay&apos;s original parser was augmented by allowing
a set of binary features to be associated with each node,
including feature tests as part of the rewrite rules, and
permitting more general restrictions where the features were
inadequate. The REL system was designed to support a number
26
of gratmars, each interfaced to its own data base. One of
the is REL English, which analyzes a subset of English into a
set of subject-verb-object-time modifier deep structures;
this grammarhas 239 rules. In support of the use of general
rewrite rules with features, they note that only 29 of the
239 rules required constraints which could not be conveniently
stated in terms of feature tests. This is also a factor in
efficiency, since binary feature tests can be performed very
quickly.
Another system which uses unrestricted rewriting rules with
optional conditions on the elements is the &amp;quot;Q&amp;quot; system developed
by Colmerauer [Colmerauer 1970J. this system is presently being
used in a machine translation pr9ject at the University of
Montreal [Kittredge 19731.
Colmerauer and de Chastellier (de Chastellier 1969] have
also investigated the possibility of using Wijngaarden grammars
(as were developed for specifying ALGOL 68) fortransformational
decomposition and machine translation. Like unrestricted
rewriting rules, W-grammars can define every recursively enumer-
able language, and so can perform the functions of the surface
and reverse transformational components. They show how portions
of transformational grammars of English and French may be
rewritten as W-grammars, with the pseudo-rules in the W-grammar
taking the place of the transformations.
</bodyText>
<subsectionHeader confidence="0.999369">
2.5 Parsing with Propability and Graded Acceptability
</subsectionHeader>
<bodyText confidence="0.97734198">
In all the systems ditscribed above, a sharp line was drawn
between correct and incobcect parses: a terminal node either
did or did not match the next word in the sentence; an analysis
of a phrase was either acceptable or unacceptable. There are
circumstances under which we would want to relax these require-
ments. For one thing, in analyzing connected speech, the
segmentation and identification of words can never be done with
complete certainty. At best, one can say that a certain sound
has some probability of being one phoneme and some other
probability of being another phoneme; some expected phonemes
may be lost entirely in the sound received. Consequently,
one will associate some number with each terminal node, indi-
cating the probability or quality of match; nonterminal nodes
will be assigned some value based on the values of the terminal
nodes 1)eneath. Another circumstance arises in natural language
systems which are sophisticated enough to realize that syntactic
and semantic restrictions are rarely all-or-nothing affairs,
and that some restrictions are stronger than others, For example,
the nominative-accusative distinction has become quite weak
for relative pronouns (?The man who I met yestelay.) but remains
strong for personal pronouns (*The man whom me met yesterday.).
As a result, a parser which wants to get the best analysis even
if every analysis violates some constraint must associate a
measure of grammaticality or acceptability with the analyses
of portions of the Sentence, and ultimately with the analyses
of the entire sentence.
In principle, one could generate every sentence analysis with
a nonzero acceptability or probability of match, and then select
the best analysis obtained. Hobbs [1974] has described a modi-
fication to the bottom-up nodal spans parsing algorithm which
uses this approach. Wilks [1975] uses an essentially similar
technique in his language analyzer based on &amp;quot;preference
semantics&amp;quot;
A more efficient approach, called &amp;quot;best-first&amp;quot; parsing,
has been developed by Paxton and Robinson of the Stanford Research
Institute as part of a speech understanding system [Paxton 19731.
Their procedure involves a modification of the standard top-down
serial parsing algorithm for context-free grammars. The
standard algorithm generates one possible parse tree until it
gets stuck (generates a terminal node which does not match the
next sentence word); it then &apos;backs up&amp;quot; to try another alternative.
28
The best-first procedure instead tries all alternatives in
parallel. A measure is associated with each alternative path,
indicating the likelihood that this analysis matches the
sentence processed so far and that it can be extended to a
complete sentence analysis. At each moment, the path with the
highest likelihood is extended; if its measure falls below that
of mime otner path, the parser shifts its attention to that
other path.
</bodyText>
<subsectionHeader confidence="0.996257">
2.6 ConiuOction and Adjunction
</subsectionHeader>
<bodyText confidence="0.998147448275862">
There are certain pervasive natural language cOnstructiens
which do not fit naturally into the standard syntax analysis
procedures, such as augmented context-free parsers. Two of
these are coordinate conjunctions and adjuncts. Special
measUres have been developed to handle these constructions;
these measures deserve brief mention here.
The allowed patterns of occurrence of conjoinings in a
sentence are quite regular. Loosely speaking, a sequence of
elements in the sentence tree may be followed by a conjunction
and by some or all of the elements immediately preceding the
conjunction. For example, allowed patterns of conjoining
include subject-verb-object-and-subject-verb-object (I drank
milk and Mary ate cake.), subject-verb-object-and-verb-object
(I drank milk and ate cake.), and subject-verb-object-and-object
(I drank milk and seltzer.). There are certain exceptions, known
as gapping phenomena, in which one of the elements following the
conjunction may be omitted; for example, subject-verb-object-and-
subject-object (I drank milk and Mary seltzer.).
The trouble with coordinate conjunctions is that they can
occur almost anywhere in the structure of a sentence. Thus,
Although it would be possible to extend a context-free surface
grammar to allow for all&apos;possible conjoinings, such an extension
would Increase the size of the grammar by perhaps an order of
magnitude. The alternative scheme which has therefore been
developed involves the automatic generation of productions
which allow for conjunction as required during the parsing
process. When a conjunction is encountered in the sentence,
the normal parsing procedure is interrupted and a special
conjunction node is inserted in the parse tree. The alter-
native values of this node provide for the various conjoined
element sequences allowed at this point.
An interrupt mechanism of this sort including provision for
gapping, is part of the Linguistic String Project parser [Sager
1967]. A similar mechanism is included in Woods&apos; augmented
transition network parser [Woods 1973] and a number of other
systems.
This solves the problem of correcting the context-free
grammar for conjunctions, but the context-free grammar is
generally only a small part of the total system. The task
remains of modifying the routines which enforce grammatical
constraints and the transformations to account for conjunctions.
Since practically every routine which examines a parse tree
is somehow affected by conjunction, this can be a large job,
but fortunately the changes are very regular for most routines.
The Linguistic String Project grammar; by performing all
operations on the parse tree through a small number of low-
level routines, was able to localize the changes to these
routines and a small number of restrictions (such as number
agreement) which are specially affected by conjunction [Raze
1974].
Certain classes of adjuncts or modifiers give rise to a
different kind of problem: a high degree of syntactic ambiguity.
For instance, in the sentence, &amp;quot;I fixed the-pipe under the sink
in the bathroom with a wrench.&amp;quot; there is no syntactic basis
for deciding whether the pipe had a wrench the sink had a
wrench, the bathroom had a wrench, or the tixing was done with
a wrench. If semantic and pragmatic restrictions are invoked
during the syntactic analysis, the parser will have to generate
</bodyText>
<subsectionHeader confidence="0.214801">
30
</subsectionHeader>
<bodyText confidence="0.968879533333333">
several analyses, all but one of which will (hopefully) be
rejected by the restrictions; this is moderately inefficient.
If syntactic analysis precedes semantic processing; the
ambiguities of the various adjuncts will be multiplied
producing dozens of analyses for a sentence of moderate size;
this is hopelessly inefficient.
A more efficient solution has the parser identify the
adjuncts and list fol. each adjunct the words it could be
modifying, without generating a complew separate an-alysis
for each possibility. The ambiguities associated with the
adjuncts are thus factored out. The semantic and pragmatic
components may then choose for each adjunct its most likely
or acceptable host (modified word). This may be done either
during the syntactic analysis [Woods 1973, Simmons 1975] or
after the syntax phase is complete [Borgida 1975, Hobbs 1975].
</bodyText>
<page confidence="0.495632">
:31
</page>
<sectionHeader confidence="0.93718" genericHeader="method">
3. ALGORITHM SPECIFICATIONS
</sectionHeader>
<bodyText confidence="0.999941583333333">
We present below precise specifications for some of the
parsing algorithms which have been discussed. These algorithms
are presented in SETL, a programming language which is based on
concepts from set theory and has been developed at New York
University by a group led by Jack Schwartz. The large variety
of data types, operators, and control structures in SETL makes
it possible to specify the algorithms in a relatively compact
and natural fashion. An implementation is available which
includes most of the features of the specification language,
so that algorithms can be tested in essentially the form in
which they are published. A description of the subset of SETL
which has been used in this report is given in the appendix.
</bodyText>
<subsectionHeader confidence="0.996841">
3.1 Parsing Algorithms for Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.979084773584906">
Context-free grammars played a major role i the early stages
of automatic natural language analysis. Although they have now
generally been superceded by more complex and powerful grammars,
many of these grammars are based on or have as one of their
components a context-free grammar. The selection of an efficient
context-free parser therefore remains an important consideration
in natural language analysis.
Jecause so many different context-free parsers have been
proposed, a comprehensive survey would be impractitable. We shall
rather present a taxonomy according to which most context-free
parsers can be classified, and illustrate this classification
with five of the possible basic algorithms. At the end we shall
mention which Of these are being used in current natural language
systems.
The first division we shall make is according to the amount
of memory space required by the parser. Type 0 parsers store
only the parse tree currently being built. The other parsers
gradually accumulate data from which all parses of a sentence
can be extracted; types 1, 2 and 3 store this data in decreas-
ingly compact representations. The four types are:
(0) Develops a single parse tree at a time; at any instant the
store holds a set of nodes corresponding to the nodes of an
incomplete potential parse tree
(1) The store holds a set of nodes, each of which represents the
fact that some substring of the sentence, from word f to word
k, can be analyzed as some symbol N.
(2) The store holds a set of nodes, each of which represents an
analysis of some substring of the sentence, from word f to
word k, as some symbol N (if there are several different
analyses of words f to 9, as some symbol N, there will be several
nodes corresponding to a single node in a type 1 parser).
(3) The store holds a Set of nodes, each of which corresponds to
an analysis of some substring of the sentence; from word f to
word k, as some symbol N appearing as part of some incomplete
potential parse tree (if symbol N, spanning words f to k,
appears in several of the incomplete potential parse trees,
there will be several nodes corresponding to each node in a
type 2 parser).
Type (0) parsers require only an amount of storage proportional
to the length of the input sentence. The storage requirements of
type (1) parsers grow as the cube of the length, while the require-
ments for types (2) and (3) grow exponentially.
A second division can be made between top-down and bottom-up
parsers. A third criterion for classification is whether
alternative parses of a sentence are all produced together
(parallel parser) or are generated sequentially (serial parser)
this division does not apply to type (0) parsers.
Finer divisions can be made of some of these categories.
For example, among bottom-up parsers we can distinguish those
which perform a reduction only when all required elements have
been found from those which make a tentative reduction when the
first element of a production is found (so-called &amp;quot;left-corner
parsers&amp;quot;). Parallel parsers can be classified according to the
</bodyText>
<page confidence="0.70101">
32
33
</page>
<bodyText confidence="0.862898833333333">
orderinc, strategy they use in building nodes: by leftmost or
rightmost word subsumed (i.e., spanned) by the node or by level.
In addition, we shall not consider a number of optimization
strategies, such as selectivity matrices and shaper and general-
ized shaper tests for top-down parsers.
We shall n&apos;ow describe algorithms in five of the categories:
</bodyText>
<table confidence="0.9470908">
A Type 0 Top-down serial
Type 2 Bottom-up parallel
Type 1 Bottom-up parallel
Type2 Top-down serial
rype 1 Top-down serial
</table>
<bodyText confidence="0.968623866666667">
We have not included any type 3 parsers because, despite their
profligate use of storage, they do not operate much faster than
type 0 parsers. The only reported use of such a parser of
which we are aware is the &amp;quot;Error-Correcting Parse Algorithm&amp;quot; of
Irons (Comm. ACM 6, 669 (1963)). A top-down left-to-right
parallel strategy was employed so that the parser could make a
suitable modification to the sentence when it &amp;quot;got stuck&amp;quot; because
of an error in the input.
SATL Drocedures are given for these five parsers. The input
data structures are the same in all cases: The sentence, passed
through parameter SENTENCE, is a tuple. The elements of the
tuple, the words of the sentence, are to be matched by terminal
symbols from the grammar. The context-free grammar, passed
through parameter GRAMMAR, is a set each of whose elements
corresponds to a production. The production
</bodyText>
<equation confidence="0.940519">
a0 al a2 ... an
is transformed into the (n+1)-tuple
&lt;a01a11a21...,an&gt; .
</equation>
<bodyText confidence="0.955779">
The root symbol of the grammar is passed to the parser in
parameter ROOT.
</bodyText>
<page confidence="0.62509">
34
</page>
<table confidence="0.409084">
Algorithm A, Type 0 TDp-down Serial
This procedure builds the parse trees for the input sentence
</table>
<tableCaption confidence="0.476929571428571">
sequentially in a two-dimensiongl array TREE. The first
subscript of TREE specifies the number of the node, the second
selects z.4 component of the node as follows:
TREE(n,&apos;NAMET) name of node n
TREE(n,&apos;PARENT&apos;) number of &apos;parent node of node n
(= 0 for root node)
TRLE(n,&apos;DAUGHTERS&apos;) tuple of numbers of daughter nodes
</tableCaption>
<table confidence="0.372874333333333">
of node n
TREE(n,&apos;CURRENT OPTION&apos;) tuple of current production used
to expand this node
</table>
<tableCaption confidence="0.617815">
TREE(n,&apos;ALTERNATIVE OPTIONS,&apos;) set of tuples representing
productions not yot tried for
this node
TREE(n,&apos;FW1) number of first sentence word
subsumed by node n
TREE(n,&apos;LW+11) (number of last seltence word subsumed
by node n) + 1
</tableCaption>
<bodyText confidence="0.966833391304348">
As each analysis of the sentence is completed, it is added
to the set PARSES. When parsing is finished, this svt of trees
is returned as the value of the function PARSE.
The variable NODES holds a count of the number of nodes in the
parse tree; this is also the number of the node most roacently added
to the tree. WORD holds the number of the next word in .he sentence
to be matched.
The heart of the parser is the recursive procedure EXPAND. EXPAND
is passed one argument, the number of a node in the parse tree. rf
EXPAND has not been called for this node before, it will try to
expand the node, i4e., build a parse tree below the
;735
node which matches part of the remainder of the sentence.
If EXPAND has already been called once for this node -- so that
a tree already exists below this node -- EXPAND tries to find an
alternate tree below the node which will match up with part of
the remainder of the sentence.
If EXPAND is successful -- an (alternate) tree below the node
was found -- it returns the value true; if it is unsuccessful,
it returns false. In the case where the node corresponds to
a terminal symbol, EXPAND will return true on the first call
only if the symbol matches the next word in the sentence; it
will always return false on the second call.
</bodyText>
<equation confidence="0.775170952380952">
definef PARSE (GRAMMAR, ROOTISENTENCE) ;
local PARSES, TREE, NODES, WORD;
TREE = nk;
PARSES
WORD = 1;
NODES = 1;
/* set up root node (node 1) */
TREE(1, &apos;NAME&apos;) = ROOT;
TREE(1, &apos;FW&apos;) = WORD;
/* loop until all parse trees have been formed.*/
(while EXPAND(1))
,/* if tree spans entire sentence, add to set */
if WORD eq ((SENTENCE)+1) then
PARSES = PARSES U (TREE);
end if WORD;
end while EXPAND;
return PARSES;
end PARSE;
36
definef EXPAND(X);
local I, OPT;
</equation>
<bodyText confidence="0.748401285714286">
if GRAMMARITREE(X,&apos;NAME&apos;)1 eq nk then
/* terminal symbol */
if TREE(X, &apos;ALTERNATE OPTIONS&apos;) eq Q then
/* first call -- test for match with sentence */
TREE(X, &apos;ALTERNATE OPTIONS&apos;) =
if WORD le #SENTENCE then
if SENTENCE(WORD) eq TREE(X, &apos;NAME&apos;) then
</bodyText>
<equation confidence="0.9901351">
WORD = WORD + 1;
TREE(X, &apos;LW+1&apos;) - WORD;
return true;
end if SENTENCE;
end if WORD;
else /* second call */
WORD = WORD-I;
end if TREE;
return false;
end if GRAMMAR;
</equation>
<bodyText confidence="0.8723094">
/* nonterminal symbol */
if TREE(X, &apos;ALTERNATE OPTIONS&apos;) eq then
/* first call, retrieve options from grammar */
TREE(X, &apos;ALTERNATE OPTIONS&apos;) = GRAMMAR(TREE(X, &apos;NAME&apos;)};
TREE(X, &apos;DAUGHTERS&apos;) = nult;
</bodyText>
<equation confidence="0.9935678">
OPT= 2;
else /* second or subsequent call */
OPT = TREE(X, &apos;CURRENT OPTION&apos;);
I = #OPT;
end if TREE;
</equation>
<bodyText confidence="0.9458945">
/* select next option to try */
GETOPT: if OPT eq 2 then
OPT from TREE(X, &apos;ALTERNATE OPTIONS&apos;)-
TREE(X, &apos;CURRENT OPTION&apos;) OPT;
</bodyText>
<equation confidence="0.989620076923077">
I = 1;
end if OPT;
/* epand node */
Vhile I ge 1)
/* work on tth element of current option */
/* if corresponding node not in parse tree, add it */
kf TREE(X, &apos;DAUGHTERS&apos;) (I) eq 2 then
NODES = NODES + 1;
TREE(NODES,&apos;NAME&apos;) =.0PT(I);
TREE(NODES,&apos;FW&apos;) = WORD;
TREE(NODES,&apos;PARENT&apos;) = X;
TREE(X, &apos;DAUGHTERS&apos;)(I) = NODES;
end if TREE;
</equation>
<bodyText confidence="0.9523852">
/* try for an(other) expansion of this node */
if EXPAND(TREE(X, &apos;DAUGHTERS&apos;) (I)) then
/* expansion found... if this is last element, return
successfully, else advance to next element */
if I eq #OPT then
</bodyText>
<equation confidence="0.840507333333333">
TREE(X,&apos;LW+1&apos;) = WORD;
return true;
else
I = I + 1;
end if I;
else
/* no expansion found ... erase this node and examine
previous element */
TREE (NODES) = 2;
NODES = NODES-1;
TREE(X, &apos;DAUGHTERS&apos;)(I) = 2;
I = 1-1;
end if EXPAND;
end while I;
a
</equation>
<bodyText confidence="0.94619124137931">
/* all expansions for this option have been generated;
if more options, loop, else return false, */
OPT= 2;
if TREE(X,IALTERNATE OPTIONS&apos;) ne nR, then go to GETOPT;;
return false;
end EXPAND;
One way of viewing this procedure is to consider each node
as a separate process. Each process creates and invokes the
processes corresponding to its daughter nodes. In SETL,
the algorithm cannot be represented directly in this way, since
there are no medlanisms for creating and suspending processes.
Instead, the data which would correspond to the local variables
of the process are stored as components of each node in the
parse tree. In languages which provide for the suspension of
processes, such as SIMULA, the algorithm can be represented even
more succinctly (see, for example, a version of this algorithm
in &amp;quot;Hierarchical Program Structures&amp;quot; by 0.-J. Dahl and
C. A. R. Hoare, in Structured Pro9rammin9 by 0.-J. Dahl et al.,
page 201).
Algorithm B. Type 2 Bottom-up Parallel
This algorithm is sometimes called the &amp;quot;Immediate Corastituent
Analysis&amp;quot; (ICA) algorithm, because it was used quite early in
parsing natural language with ICA grammars. It-constructs all
nodes in a single left-to-right pass over the sentence. As each
word is scanned, the parser builds all nodes which subsume a
portion of the sentence ending at that word. The nodes (&amp;quot;spans&amp;quot;)
are accumualted in a two-dimensional array SPAN, whose first
subscript specifies the number of the span and whose second
subscript selects a component of the span, as follows:
</bodyText>
<figure confidence="0.6669926">
38
SPAN (n,
SPAN (n,
SPAN (11,
SPAN
&apos;NAME&apos;) = name of span n
&apos;FW&apos;) = number of first sentence word subsumed by span n
&apos;Lli+11)= (number of last sentence word subsumed
by span n) + 1
&apos;DAUGHTERS&apos;) = tuple of numbers of daughter spans
</figure>
<bodyText confidence="0.730361555555556">
of span n.
At the end of the routine is some code to convert SPAN,
a graph structure with each span potentially a part of many
parses, into a set of parse trees. This code has two parts:
a loop to find all root nodes created in the immediate
constituent analysis, and a recursive routine EXPAND which makes
copies of all descendants of the root node and puts them in TREE.
Each node in the tree has the following components:
TREE(m, &apos;NAME&apos;) = name of node n
TREE(n, &apos;FW&apos;) = number of first sentence word subsumed
by node n
TREE(n„ ILW+11) = (number of last sentence word subsumed
by node n) +1
TREE(n, &apos;DAUGHTERS&apos;) = tuple of numbers of daughter nodes
of node n
TREE(n, &apos;PAPF.NT&apos;)= number of parent node of node n.
The set of parse trees is accumulated in PARSES and finally
returned as the value of the function PARSE.
</bodyText>
<note confidence="0.399903">
40
</note>
<table confidence="0.880094741935484">
definef PARSE(GRAMMAR,ROOT,SENTENCE); DEFELIST, REM, SPAN,
local TODO, WORD, CURRENT, DEP&apos;, DEFNANE,
SPANS, TREE, NODES, PARSES, MS, I;
/* initialization */
SPAN - n1;
SPANS = 0;
TODO = nl;
/* iterate over WORD=last word subsumed
by spans being constructed */
(1 &lt;= VWORD &lt;= #SENTENCE)
/* add span whose name is sentence word */
ADDSPAN(SENTENCE(WORD), WORD, WORD+1, nult);
/* TODO contains the numbers of spans which were
just created and for which we have not yet
checked whether they can be used as the last
daughter span in blailding some more spans */
(while TODO ne nl)
/* select a span from TODO */
CURRENT from TODO&apos;;
/* loop over all productions whose last element
= name of current span */
(VDEF E GRAMMARIDEF(#DEF) eq SPAN(CURRENT,&apos;NAME1))
/* separate left and right sides of production */
DEFNAME = hd DEF;
DEFELIST = ti DEF;
/* if elements preceding last element of production
can be matched by spans, add a new span whose
name = left-hand side of production for each match*/
(VREM E MATCH(DEFELIST(1: (#nFELIST)-1),
SPAN(CURRENT,&apos;FIV)))
ADDSPAN(DEFNAME, hd REM, SPAN(CURRENT,&apos;LW+11),&apos;
</table>
<footnote confidence="0.62434">
(ti REM) .+ &lt;CURRENT&gt;);
end VREM; end VDEF;
end while TODO;
end 1 ,= VWORD;
</footnote>
<page confidence="0.849644">
41
</page>
<bodyText confidence="0.698087">
/* ektract trees from set of spans
</bodyText>
<equation confidence="0.880194769230769">
PARSES = n1;
(1 &lt;= VI &lt;= SPANSI(SPAN(I;&apos;NAM4&apos;) &apos;eq ROOT) and
(SPAN(I,&apos;FW1) eq 1) and
(SPAN(I,&apos;LW+1&apos;) eq ((#SENTENCE)+1)))
NODES = 1; ) 1) ;
TREE = n1;
TREE{l} = SPAN{I};
TREE (1, &apos;DAUGHTERS&apos;) = EXPAND(TREE(1,1DAUGHTERS&apos;
end 1 &lt;= VI;
return PARSES;
end PARSE;
define MATCH(ELIST, ENDWDP1);
local I, NTUP;
</equation>
<bodyText confidence="0.900069125">
/* MATCH finds all n-tuples of spans whose names
match the elements of the n-tuple ELIST and which
span a portion of the sentence whose last word +1
= ENDWDP1; returns a set, each element of which
is an (n+1)-tuple, whose tail is one of the n-tupleL,
of spans and whose head is the number of the first
word spanned by the n-tuple of spans
if ELIST eq nult
</bodyText>
<figure confidence="0.27881595">
then return {&lt;sENDWDP1&gt;);
else return 1U: 1 &lt;= I &lt;= NODES,&apos;
(if(SPAN(I,&apos;NAME&apos;) eq ELIST(?ELIST))
and (SPAN(Ir&apos;LW+1&apos;) eq ENDWDP1)
then {NTUP .4- &lt;I&gt;, NTUP e
MATCH(ELIST(1:(tELIST)71),SPAN(I,&apos;FW)))
else n1);;
end MATCH;
42
define ADDSPAN(NAME, FW e LWP1, DAUGHTERS)
/* ADDSPAN builds a span whose components
are passed in the four parameters */
SPANS SPANS+1;
SPAN(SPANS,&apos;NAME&apos;) = NAME;
SPAN(SPANS,&apos;Fie) = FW;
SPAN(SPANS,ILW+1&apos;) = LWP1;
SPAN(SPANS,&apos;DAUGHTERS&apos;) = DAUGHTERS;
SPANS in TODO;
return;
ADOSPAN,
definef EXPAND(DAW,PAR) ;
/* creates a node for each span in DAW and each
descendant thereof, and returns a tuple with
the numbers of the nodes (in MEE) corresponding
to the spans in DAW */
local S, D, N;
if DAN eq II then return Q; ;
= nult,
(VS E DAN)
NOWS = NODES 4-, 1;
= NODES;
TREE(N) = SPAN{S};
TBEE(N,IPARENTI) = PAR;
TREE(N,IDALIGHTERS&apos;) = EXPAND(TREE(N,&apos;DAUGHTERS&apos;),N);
D= D &lt;N&gt;;
end VS;
return D;
end EXPAND;
43
Algorithm C Type 1 Bottom-up Parallel
</figure>
<figureCaption confidence="0.873776545454545">
Algorithm C is the basic &amp;quot;nodal spans&amp;quot; parsing algorithm
[Cocke 1970]. The sequencing logic is identical to that for
Algorithm B. The Qnly difference in the tree representation
im that all spans in Algorithm B with common values in the NAME,
FW, and LW+1 components are joined into a single span in
Algorithm C. The DAUGHTERS component now becomes a set, each of
whose elements corresponds to the value of the DAUGHTERS
component of one of the spans in Algorithm B (this set is
called the &amp;quot;division list&amp;quot; in the nodal spans algorithm):
SPAN(n,IDAUGHTERS&apos;) = a set each of whose elements is a tuple
of numbers of daughter nodes of span n
</figureCaption>
<bodyText confidence="0.869984">
In order to effect this change in the tree, it is necessary only
to modify the procedure ADDSPAN to check whether a span with the
specified value of NAME, FW, and LW+1 alraay exists:
define ADDSPAN(NAME, FW, LWP1, DAUGHTERS);
</bodyText>
<equation confidence="0.967176">
local S;
if 1 &lt;= 3S &lt;= SPANS&apos; I (SRA-4(S,&apos;NANE&apos;) eq NAME) and
(SPAM(S,&apos;FW1) eq FW) and
(SPAN(SI&apos;LWA-1&apos;) erl LWP1)
then DAUGHTERS in SPAN(S,&apos;DAUGHTERS&apos;);
else SPANS = SPANS+1;
SPAN(SPANS,&apos;NAAE1) = NAME;
SPAN(SPANSITEIV) = FK:
44
SP1N(SPANS,&apos;LW+1&apos;) = LWP1;
</equation>
<table confidence="0.463721">
SPAN (SPANS, &apos;DAUGHTEAS &apos; ) = {DAUGHTERS} ;
SPANS in TODO;
end if;
return;
end ADDNODE;
</table>
<bodyText confidence="0.997974931034483">
The procedure for converting the spans into a set of trees is
snow more complicated than for Algorithm B; see, for example,
OWens [1975], Sec. 7.
Algorithm D. Type 2 Top-down Serial
We now seek to combine the advantages of algorithm A with
those of algorithms B and C. Algorithms B and C would construct
any given tree over a portion of the sentence only once, whereas
algorithm A might construct some trees many times during the course
of a parse. On the other hand, B and C would construct many trees
which A would never try to build. More precisely, B and C would
build trees while processing word n+1 which could not enter into
any parse for any sentence whose first n words were those processed
so far.
To combine these algorithms, we shall return to the basic
framework provided by algorithm A. To this we add a mechanism
for recording &amp;quot;well formed substrings.&amp;quot; The first time the parser
tries to analyze a portion of the sentence beginning at word f as
an instance of symbol N, this mechanism records any and all trees
constructed below node N. The next time the parser tries symbol N
at word f, the saving mechanism retrieves this information so
that the trees below N need not actually be rebuilt.
The previously-completed trees are stored in the two-dimensional
array WFS, whose structure is identical to that of SPAN in
algorithm- B,:
WFS(n,LNAMEI) = name of well-formed substring n
WFS(nrIFW1) = first word of well-formed substring n
WFS(nfILW+1&apos;) = (last word of well-formed substring n) + 1
WFS(n,&apos;DAUGHTERS&apos;) = tuple of numbers of daughter substrings
of substring n
</bodyText>
<page confidence="0.575568">
45
</page>
<bodyText confidence="0.978866777777778">
WFSS -,holds the numbe&apos;r of substrings in WFS. When the parsing
operation is complete, WFS will contain a subset of the elements
which were in TREE at the end of algorithm B.
The tree used by the top-down parser must be augmented to allow
for the possibility that the parser is not building a tree below
a given node but rather consulting the table of well-formed
substrings for that node. In that case the node will have, instead
of a tuple of daughters and a set of alternative options, the
number of the well-formed substring currently being used in the
tree and the set of alternative well-formed substrings. The
structure of a node is thus:
TREE-kn,&apos;NANE&apos;) = name of node n
TREE(n,&apos;PARENT&apos;) = number of parent node of node n
(= 0 for root node)
TREE(n,&apos;DAUGHTERS&apos;) = tuple of numbers of daughter nodes of node n
(= nult if node is matched by well-formed substring)
TREE(n,&apos;WFS&apos;) =number of well-formed substring matched to node n
(= Q if not matched to a substring)
TREE(n,&apos;CURRENT OPTION&apos;) = tuple of current production used to
expand node n
(= Q if node is matched by a well-formed substring)
TREE(n,&apos;ALTERNATE OPTIONS&apos;) =
set of tUples representing productions not
yet tried for node n
TREE(nl&apos;ALTERNATE WFS&apos;) =
set of numbers of well-form6d substrings not
yet tried for node n
</bodyText>
<equation confidence="0.5183975">
TREE(n,&apos;FW1) = number of first word subsumed by node n
TREE(n,1LW+1&apos;) = (number of last word subsumed by node n) + I
</equation>
<bodyText confidence="0.930145">
Finally, we require a table which indicates, for each symbol N
and sentence word f, whether all the well-formed substrings for N
starting at f have been recorded in WFS. For this the parser uses
the two-dimensional array EXPANDED: EXPANDED(N,f) = true if all
substrings have been recorded, Q if not.
The text of procedure D is given below; comments are included
only for those statements added to procedure A.
46
</bodyText>
<equation confidence="0.845451789473684">
definef PARSE(GRAMMAR, ROOT, SENTENCE);
local PARSES, TREE, NODES, WORD, WFS, WFSS, EXPANDED;
TREE = rap;
PARSES = nt;
WFS = nk;
WFSS = 0;
EXPANDED = nX;
WORD = 1;
NODES = 1;
TIrlE(1,&apos;NANE&apos;) = ROOT;
TREE(1,IFW) = WORD;
(while EXPAND(1))
if WORD eq (IISENTENCE 4. 1) THEN
PARSES = PARSES U (TREE);
end if WORD;
end while;
return &lt;PARSES,WFS&gt;;
end PARSE;
definef EXPAND(X);
</equation>
<bodyText confidence="0.6643028">
local I, S, LAST, OPT;
if EXPANDED(TREE(X,&apos;NAME&apos;), TREE(X,&apos;FW&apos;)) eq true then
/* the expansions for this symbol have been computed before */
/* if this is a new node, get its WFS entries */
if TREE(XWALTERNATE WFS&apos;) eq Q then
</bodyText>
<equation confidence="0.59591625">
TREE(X,&apos;ALTERNATE WFS&apos;) = IS, 1 &lt; S &lt; WFSS I
(WFS(S,&apos;NAME&apos;) eq TREE(X,&apos;NAME&apos;)) and
(WFS(S,&apos;FW) eq TREE(X,&apos;FW))1
end if TREE;
</equation>
<bodyText confidence="0.318665">
if TREE(X,&apos;ALTERNATE WFS&apos;) eq nR, then
/* all WFSs tried for this node */
WORD = TREE(X,&apos;FW);
return false;
</bodyText>
<page confidence="0.5434695">
else
47
</page>
<table confidence="0.639139142857143">
* select next WFS for node */
TREE(x,&apos;WFSI) from TREE(X,&apos;ALTERNATE WFS&apos;)
WORD = WFS(TREE(WWFS&apos;), &apos;LW+1&apos;);
TREE(X,&apos;LW+1&apos;) WORD;
return true;
end if TREE;
end if EXPANDED;
</table>
<construct confidence="0.9547654">
if GRAMMARfTREE(X,&apos;NAME1)) eq nk then
if TREE(X0&apos;ALTERNATE OPTIONS&apos;) eq then
TREE(X,&apos;ALTERNATE OPTIONS&apos;)
if WORD le #SENTENCE then
if SENTENCE(WORD) eq TREE(X,&apos;NAME&apos;) then
</construct>
<equation confidence="0.9453818">
WORD = WORD+1;
TREE(X,&apos;LW+1&apos;) = WORD;
* add WFS recording match to terminal symbol */
ADDWFS(TREE{X});
TREE(X,&apos;WFS&apos;) = WFSS;
</equation>
<reference confidence="0.9076708">
return true;
end if SENTENCE;
end if WORD;
else WORD = WORD — 1;
end if TREE;
/* matcn to terminal symbol, if successful, has been recorded*/
EXPANDED (TREE (X, &apos;NAME ) ,WORD) = true;
return false;
end if GRAMMAR;
if TREE(Xl&apos;ALTERNATE OPTIONS&apos;) eq Q then
</reference>
<equation confidence="0.76336875">
TREE (x, OPTIONS&apos;) = GRAMMAR{TREE(X, &apos;NAME&apos;) };
TREE.(X,IDAUGHTERS&apos;) = nult;
OPT= Q;
else OPT = TREE(X,&apos;CURRENt OPTION&apos;);
I = #OPT;
end if TREE;
48
GETOPT: if OPT eq Q then
OPT from TREE(&apos;XWALTERNATE OPTIONS&apos;);
TREE(X,&apos;CURRENT OPTION&apos;)= OPT;
1=1;
end if OPT;
(while I ge 1)
if TREE(X,&apos;DAUGHTERS&apos;)(I) eq Q then
NODES = NODES + 1;
TREE(NODESI&apos;NAMEI) = OPT(I);
TREE(NODES,IFW) = WORD;
TREE(NODEWPARENTI) = X;
TPEE(X,&apos;DAUGHTERS&apos;)(I) = NODES;
end if TREE;
</equation>
<bodyText confidence="0.32062625">
If EXPAND(TREE(X,&apos;DAUGHTERS&apos;)(I)) then
if I eq #OPT then
TREE(X,&apos;LW+1&apos;) WORD;
/* record substring matched by node X */
</bodyText>
<equation confidence="0.8999904">
ADDWPS(TREE(X1);
TREE(X,&apos;WFS&apos;) = WFSS;
return true;
se
I = I+1
end if I;
else TREE(NODES) = Q;
NODES = NODES-1;
TREE(X,&apos;DAUGHTERS&apos;)(I)
I = I-1;
</equation>
<reference confidence="0.749108875">
end if EXPAND;
end while I;
OPT = Q;
if TREE(X,&apos;ALTERNATE OPTIONS&apos;) ne nR, then go to GETOPT;;
/* all expansions tried */
EXPANDED(TREE(X,&apos;NAME&apos;),WORD) = true;
return false;
end EXPAND;
</reference>
<page confidence="0.628151">
49
</page>
<equation confidence="0.933252615384615">
define ADDWFS(NODEX)
/* add an entry to WFS */
local I;
WFSS = WFSS+1;
WFS(WFSS,&apos;NAME&apos;) = NODEX(&apos;NAME&apos;);
WFS(WFSWFW) NODEX(&apos;FW1);
WFS(WFSS,114W+11) = NODEX(&apos;LW+1&apos;);
if NODEX(&apos;DAUGHTERS&apos;) ne Q then
WFS(WFSS,&apos;DAUGHTERSI) [4.: I E NODEX(&apos;DAUGHTERS&apos;)]
&lt;TREE(I,&apos;WFS&apos;)&gt;;
end if NODEX;
return;
end ADDWFS;
</equation>
<bodyText confidence="0.990649066666667">
Note that this parser returns an ordered pair consisting of
the set of trees and the set of well-formed substrings,
since the trees alone do not contain complete information
about the sentence analysis.
Algorithm E Type 1 Top-Down Serial
To complete our set of algorithms, we shall apply to
Algorithm D the same change we made to convert Algorithm B
to Algorithm C. That is, where in Algorithm D we may have
had Sever..1 aell formed substrings with the same values of
NAME, FN, LW+1, we shall combine these into a single substring
in Algorithm E; The component DAUGHTERS &apos;becomes a set, each
oZ whose elements is a tuple corresponding to the value of
DAUGHTERS of one of the substrings in Algorithm D. Just as
we only had to change ADDNODE in Algorithm B, we only have
to change ADDWFS in Algorithm D.
</bodyText>
<reference confidence="0.488987333333333">
define ADDWFP(NODEX);
local W, I, DAUGHTERS;
/* comp4te DAUGHTERS for substring */
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.056215">
<note confidence="0.837599">Journal ot Computational Linguistics 47</note>
<title confidence="0.981176333333333">A SURVEY OF PROCEDURES LANGUAGE</title>
<author confidence="0.997979">RALPH GRISHMAN</author>
<affiliation confidence="0.996420333333333">Computer Science Department Courant-Institute of Mathematic41 Sciences New York University</affiliation>
<address confidence="0.992035">251 Mercer Street, New York. 10012</address>
<note confidence="0.9938105">This survey was prepared under contract No. N00014-67A-0467-0032 with the of Naval and was originally issued as Report No. NSO-8 of Courant Institute of Mathematical Sciences, York Copyright 0 1976</note>
<title confidence="0.96702">Association for Computational Linguistics A SURVEY OF SYNTACTIC ANALYSIS PROCEDURES FOR NATURAL LANGUAGE</title>
<author confidence="0.998639">RALPH GRISHMAN</author>
<affiliation confidence="0.988347333333333">Computer Science Department Courant Institute of Mathematical Sciences New York University</affiliation>
<abstract confidence="0.833773">This survey was prepared under contract No. N00014-67A-0467-0032 with the Offipe and was originally issued os Report No. NS0-8 of the Courant Institute of Mathematical Sciences, New York University. SUMMARY This report includes a brief discussion of the role of automatic syntactic analysis, a survey of parsing procedures used in the analysis of and a discussion of the taken to a of difficult linproblems, such as conjunction and graded acceptability. It also contains specifications in the programming language SETL of a number of parsing algorithms, including parsers, a unrestricted rule parser, parser.</abstract>
<note confidence="0.492210666666667">2 3 Contents</note>
<page confidence="0.663693"></page>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>return true</author>
</authors>
<title>end if SENTENCE; end if WORD; else WORD = WORD — 1; end if TREE; /* matcn to terminal symbol, if successful, has been recorded*/ EXPANDED (TREE (X, &apos;NAME ) ,WORD) = true; return false; end if GRAMMAR; if TREE(Xl&apos;ALTERNATE OPTIONS&apos;) eq Q then end if EXPAND; end while I; OPT = Q; if TREE(X,&apos;ALTERNATE OPTIONS&apos;) ne nR, then go to GETOPT;; /* all expansions tried */ EXPANDED(TREE(X,&apos;NAME&apos;),WORD) = true; return false; end EXPAND; define ADDWFP(NODEX); local W, I, DAUGHTERS; /* comp4te DAUGHTERS for substring */</title>
<marker>true, </marker>
<rawString>return true; end if SENTENCE; end if WORD; else WORD = WORD — 1; end if TREE; /* matcn to terminal symbol, if successful, has been recorded*/ EXPANDED (TREE (X, &apos;NAME ) ,WORD) = true; return false; end if GRAMMAR; if TREE(Xl&apos;ALTERNATE OPTIONS&apos;) eq Q then end if EXPAND; end while I; OPT = Q; if TREE(X,&apos;ALTERNATE OPTIONS&apos;) ne nR, then go to GETOPT;; /* all expansions tried */ EXPANDED(TREE(X,&apos;NAME&apos;),WORD) = true; return false; end EXPAND; define ADDWFP(NODEX); local W, I, DAUGHTERS; /* comp4te DAUGHTERS for substring */</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>