<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.78535">
Event-Driven Headline Generation
</title>
<author confidence="0.944052">
Rui Sunt, Yue Zhangt, Meishan Zhangt and Donghong Jit
</author>
<affiliation confidence="0.975508">
t Computer School, Wuhan University, China
t Singapore University of Technology and Design
</affiliation>
<email confidence="0.8961795">
{ruisun, dhji}@whu.edu.cn
{yue zhang, meishan zhang}@sutd.edu.sg
</email>
<sectionHeader confidence="0.993071" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896375">
We propose an event-driven model for
headline generation. Given an input
document, the system identifies a key
event chain by extracting a set of structural
events that describe them. Then a novel
multi-sentence compression algorithm
is used to fuse the extracted events,
generating a headline for the document.
Our model can be viewed as a novel
combination of extractive and abstractive
headline generation, combining the
advantages of both methods using event
structures. Standard evaluation shows that
our model achieves the best performance
compared with previous state-of-the-art
systems.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999873761904762">
Headline generation (HG) is a text summarization
task, which aims to describe an article (or a set of
related paragraphs) using a single short sentence.
The task is useful in a number of practical
scenarios, such as compressing text for mobile
device users (Corston-Oliver, 2001), generating
table of contents (Erbs et al., 2013), and email
summarization (Wan and McKeown, 2004). This
task is challenging in not only informativeness
and readability, which are challenges to common
summarization tasks, but also the length reduction,
which is unique for headline generation.
Previous headline generation models fall into
two main categories, namely extractive HG
and abstractive HG (Woodsend et al., 2010;
Alfonseca et al., 2013). Both consist of
two steps: candidate extraction and headline
generation. Extractive models choose a set of
salient sentences in candidate extraction, and
then exploit sentence compression techniques to
achieve headline generation (Dorr et al., 2003;
</bodyText>
<figureCaption confidence="0.772296333333333">
Figure 1: System framework.
Zajic et al., 2005). Abstractive models choose a
set of informative phrases for candidate extraction,
and then exploit sentence synthesis techniques for
headline generation (Soricut and Marcu, 2007;
Woodsend et al., 2010; Xu et al., 2010).
</figureCaption>
<bodyText confidence="0.999839444444445">
Extractive HG and abstractive HG have
their respective advantages and disadvantages.
Extractive models can generate more readable
headlines, because the final title is derived by
tailoring human-written sentences. However,
extractive models give less informative titles
(Alfonseca et al., 2013), because sentences
are very sparse, making high-recall candidate
extraction difficult. In contrast, abstractive models
use phrases as the basic processing units, which
are much less sparse. However, it is more difficult
for abstractive HG to ensure the grammaticality
of the generated titles, given that sentence
synthesis is still very inaccurate based on a set
of phrases with little grammatical information
(Zhang, 2013).
In this paper, we propose an event-driven model
for headline generation, which alleviates the
</bodyText>
<figure confidence="0.996632375">
Candidate #1 ... Candidate #i ... Candidate #K
Phrases Events Sentences
Multi-Sentence Compression
Candidate Ranking
Headline
Texts
Candidate Extraction
Headline Generation
</figure>
<page confidence="0.990312">
462
</page>
<note confidence="0.978228333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 462–472,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999864911764706">
disadvantages of both extractive and abstractive
HG. The framework of the proposed model is
shown in Figure 1. In particular, we use
events as the basic processing units for candidate
extraction. We use structured tuples to represent
the subject, predicate and object of an event. This
form of event representation is widely used in
open information extraction (Fader et al., 2011;
Qiu and Zhang, 2014). Intuitively, events can
be regarded as a trade-off between sentences
and phrases. Events are meaningful structures,
containing necessary grammatical information,
and yet are much less sparse than sentences.
We use salience measures of both sentences and
phrases for event extraction, and thus our model
can be regarded as a combination of extractive and
abstractive HG.
During the headline generation step, A graph-
based multi-sentence compression (MSC) model
is proposed to generate a final title, given multiple
events. First a directed acyclic word graph is
constructed based on the extracted events, and
then a beam-search algorithm is used to find the
best title based on path scoring.
We conduct experiments on standard datasets
for headline generation. The results show
that headline generation can benefit not only
from exploiting events as the basic processing
units, but also from the proposed graph-based
MSC model. Both our candidate extraction
and headline generation methods outperform
competitive baseline methods, and our model
achieves the best results compared with previous
state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.990557" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99907075">
Previous extractive and abstractive models take
two main steps, namely candidate extraction and
headline generation. Here, we introduce these two
types of models according to the two steps.
</bodyText>
<subsectionHeader confidence="0.937426">
2.1 Extractive Headline Generation
</subsectionHeader>
<bodyText confidence="0.999824208333333">
Candidate Extraction. Extractive models exploit
sentences as the basic processing units in this step.
Sentences are ranked by their salience according
to specific strategies (Dorr et al., 2003; Erkan and
Radev, 2004; Zajic et al., 2005). One of the state-
of-the-art approaches is the work of Erkan and
Radev (2004), which exploits centroid, position
and length features to compute sentence salience.
We re-implemented this method as our baseline
sentence ranking method. In this paper, we use
SentRank to denote this method.
Headline Generation. Given a set of sentences,
extractive models exploit sentence compression
techniques to generate a final title. Most previous
work exploits single-sentence compression (SSC)
techniques. Dorr et al. (2003) proposed the Hedge
Trimmer algorithm to compress a sentence by
making use of handcrafted linguistically-based
rules. Alfonseca et al. (2013) introduce a
multi-sentence compression (MSC) model into
headline generation, using it as a baseline in their
work. They indicated that the most important
information is distributed across several sentences
in the text.
</bodyText>
<subsectionHeader confidence="0.99807">
2.2 Abstractive Headline Generation
</subsectionHeader>
<bodyText confidence="0.9871249375">
Candidate Extraction. Different from extractive
models, abstractive models exploit phrases as the
basic processing units. A set of salient phrases
are selected according to specific principles during
candidate extraction (Schwartz, 01; Soricut and
Marcu, 2007; Xu et al., 2010; Woodsend et
al., 2010). Xu et al. (2010) propose to rank
phrases using background knowledge extracted
from Wikipedia. Woodsend et al. (2010) use
supervised models to learn the salience score of
each phrase. Here, we use the work of Soricut
and Marcu (2007) , namely PhraseRank, as
our baseline phrase ranking method, which is an
unsupervised model without external resources.
The method exploits unsupervised topic discovery
to find a set of salient phrases.
Headline Generation. In the headline generation
step, abstractive models exploit sentence synthesis
technologies to accomplish headline generation.
Zajic et al. (2005) exploit unsupervised topic
discovery to find key phrases, and use the
Hedge Trimmer algorithm to compress candidate
sentences. One or more key phrases are added
into the compressed fragment according to the
length of the headline. Soricut and Marcu
(2007) employ WIDL-expressions to generate
headlines. Xu et al. (2010) employ keyword
clustering based on several bag-of-words models
to construct a headline. Woodsend et al.
(2010) use quasi-synchronous grammar (QG) to
optimize phrase selection and surface realization
preferences jointly.
</bodyText>
<page confidence="0.99981">
463
</page>
<sectionHeader confidence="0.998587" genericHeader="method">
3 Our Model
</sectionHeader>
<bodyText confidence="0.99978925">
Similar to extractive and abstractive models, the
proposed event-driven model consists of two
steps, namely candidate extraction and headline
generation.
</bodyText>
<subsectionHeader confidence="0.999754">
3.1 Candidate Extraction
</subsectionHeader>
<bodyText confidence="0.9973733">
We exploit events as the basic units for candidate
extraction. Here an event is a tuple (S, P, O),
where S is the subject, P is the predicate and O is
the object. For example, for the sentence “Ukraine
Delays Announcement of New Government”, the
event is (Ukraine, Delays, Announcement). This
type of event structures has been used in open
information extraction (Fader et al., 2011), and has
a range of NLP applications (Ding et al., 2014; Ng
et al., 2014).
A sentence is a well-formed structure with
complete syntactic information, but can contain
redundant information for text summarization,
which makes sentences very sparse. Phrases can
be used to avoid the sparsity problem, but with
little syntactic information between phrases, fluent
headline generation is difficult. Events can be
regarded as a trade-off between sentences and
phrases. They are meaningful structures without
redundant components, less sparse than sentences
and containing more syntactic information than
phrases.
In our system, candidate event extraction is
performed on a bipartite graph, where the two
types of nodes are lexical chains (Section 3.1.2)
and events (Section 3.1.1), respectively. Mutual
Reinforcement Principle (Zha, 2002) is applied
to jointly learn chain and event salience on the
bipartite graph for a given input. We obtain the
top-k candidate events by their salience measures.
</bodyText>
<subsectionHeader confidence="0.992685">
3.1.1 Extracting Events
</subsectionHeader>
<bodyText confidence="0.9978684">
We apply an open-domain event extraction
approach. Different from traditional event
extraction, for which types and arguments are pre-
defined, open event extraction does not have a
closed set of entities and relations (Fader et al.,
2011). We follow Hu’s work (Hu et al., 2013) to
extract events.
Given a text, we first use the Stanford
dependency parser1 to obtain the Stanford typed
dependency structures of the sentences (Marneffe
</bodyText>
<footnote confidence="0.62809275">
and Manning, 2008). Then we focus on
1http://nlp.stanford.edu/software/lex-parser.shtml
DT NNPS MD VB DT NNP NNP POS NNS
the Keenans could demand the Aryan Nations ’ assets
</footnote>
<figureCaption confidence="0.868191666666667">
Figure 2: Dependency tree for the sentence
“the Keenans could demand the Aryan Nations’
assets”.
</figureCaption>
<bodyText confidence="0.993152166666667">
two relations, nsubj and dobj, for extracting
event arguments. Event arguments that have
the same predicate are merged into one event,
represented by tuple (Subject, Predicate, Object).
For example, given the sentence, “the Keenans
could demand the Aryan Nations’ assets”, Figure
2 present its partial parsing tree. Based
on the parsing results, two event arguments
are obtained: nsubj(demand, Keenans) and
dobj(demand, assets). The two event arguments
are merged into one event: (Keenans, demand,
assets).
</bodyText>
<subsectionHeader confidence="0.995874">
3.1.2 Extracting Lexical Chains
</subsectionHeader>
<bodyText confidence="0.9995825">
Lexical chains are used to link semantically-
related words and phrases (Morris and Hirst, 1991;
Barzilay and Elhadad, 1997). A lexical chain is
analogous to a semantic synset. Compared with
words, lexical chains are less sparse for event
ranking.
</bodyText>
<listItem confidence="0.842138777777778">
Given a text, we follow Boudin and Morin
(2013) to construct lexical chains based on the
following principles:
1. All words that are identical after stemming
are treated as one word;
2. All NPs with the same head word fall into one
lexical chain;2
3. A pronoun is added to the corresponding
lexical chain if it refers to a word in the chain
(The coreference resolution is performed
using the Stanford Coreference Resolution
system);3
4. Lexical chains are merged if their main words
are in the same synset of WordNet.4
2NPs are extracted according to the dependency relations
nn and amod. As shown in Figure 2, we can extract the noun
phrase Aryan Nations according to the dependency relation
nn(Nations, Aryan).
</listItem>
<footnote confidence="0.988883">
3http://nlp.stanford.edu/software/dcoref.shtml
4http://wordnet.princeton.edu/
</footnote>
<figure confidence="0.75298625">
det aux nn
nsubj
dobj
poss
</figure>
<page confidence="0.99791">
464
</page>
<bodyText confidence="0.999974125">
At initialization, each word in the document is a
lexical chain. We repeatedly merge existing chains
by the four principles above until convergence.
In particular, we focus on content words only,
including verbs, nouns and adjective words. After
the merging, each lexical chain represents a word
cluster, and the first occuring word in it can be
used as the main word of chain.
</bodyText>
<subsectionHeader confidence="0.982288">
3.1.3 Learning Salient Events
</subsectionHeader>
<bodyText confidence="0.999700727272727">
Intuitively, one word should be more important if
it occurs in more important events. Similarly, one
event should be more important if it includes more
important words. Inspired by this, we construct a
bipartite graph between lexical chains and events,
shown in Figure 3, and then exploit MRP to jointly
learn the salience of lexical chains and events.
MRP has been demonstrated effective for jointly
learning the vertex weights of a bipartite graph
(Zhang et al., 2008; Ventura et al., 2013).
Given a text, we construct bipartite graph
between the lexical chains and events, with an
edge being constructed between a lexical chain
and an event if the event contains a word in the
lexical chain. Suppose that there are n events
{e1, · · · , en} and m lexical chains: {l1, · · · , lm}
in the bipartite graph Gbi. Their scores are
represented by sal(e) = {sal(e1),···,sal(en)}
and sal(l) = {sal(l1), · · · , sal(lm)}, respectively.
We compute the final sal(e) and sal(l) iteratively
by MRP. At each step, sal(ei) and sal(lj) are
computed as follows:
</bodyText>
<equation confidence="0.954898857142857">
sal(ei) a 1:m rij x sal(lj)
j=1
n
sal(lj) a rij x sal(ei) (1)
i=1
rij = E (lR,ei)∈Gbi w(lj) · w(ei)
A
</equation>
<bodyText confidence="0.999990666666667">
where rij E R denotes the cohesion between
lexicon chain li and event ej, A is a normalization
factor, sal(·) denotes the salience, and the initial
values of sal(e) and sal(t) can be assigned
randomly.
The remaining problem is how to define the
salience score of a given lexicon chain li and a
given event ej. In this work, we use the guidance
of abstractive and extractive models to compute
</bodyText>
<subsubsectionHeader confidence="0.787296">
Lexical Chains Events
</subsubsectionHeader>
<figureCaption confidence="0.986952">
Figure 3: Bipartite graph where two vertex sets
denote lexical chains and events, respectively.
</figureCaption>
<bodyText confidence="0.793278">
sal(lj) and sal(ei), respectively, as shown below:
</bodyText>
<equation confidence="0.998002">
w(lj) = 1: salabs(w)
w∈lR
1: w(ei) =
s∈Sen(ei)
</equation>
<bodyText confidence="0.999985333333333">
where salabs(·) denotes the word salience score
of an abstractive model, salext(·) denotes the
sentence salience score of an extractive model,
and Sen(ei) denotes the sentence set where ei
is extracted from. We exploit our baseline
sentence ranking method, SentRank, to obtain
the sentence salience score, and use our baseline
phrase ranking method, PhraseRank, to obtain
the phrase salience score.
</bodyText>
<subsectionHeader confidence="0.999242">
3.2 Headline Generation
</subsectionHeader>
<bodyText confidence="0.999843636363636">
We use a graph-based multi-sentence compression
(MSC) model to generate the final title for the
proposed event-driven model. The model is
inspired by Filippova (2010). First, a weighted
directed acyclic word graph is built, with a start
node and an end node in the graph. A headline
can be obtained by any path from the start node
to the end node. We measure each candidate path
by a scoring function. Based on the measurement,
we exploit a beam-search algorithm to find the
optimum path.
</bodyText>
<subsubsectionHeader confidence="0.501556">
3.2.1 Word-Graph Construction
</subsubsectionHeader>
<bodyText confidence="0.999988875">
Given a set of candidate events CE, we extract
all the sentences that contain the events. In
particular, we add two artificial words, (S) and
(E), to the start position and end position of
all sentences, respectively. Following Filippova
(2010), we extract all words in the sentences as
graph vertexes, and then construct edges based
on these words. Filippova (2010) adds edges
</bodyText>
<equation confidence="0.625754">
(2)
salext(s)
</equation>
<page confidence="0.990985">
465
</page>
<figureCaption confidence="0.9628355">
Figure 4: Word graph generated from candidates
and a possible compression path.
</figureCaption>
<bodyText confidence="0.999657764705883">
for all the word pairs that are adjacent in one
sentence. The title generated using this strategy
can mistakenly contain common word bigrams(
i.e. adjacent words) in different sentences. To
address this, we change the strategy slightly, by
adding edges for all word pairs of one sentence in
the original order. In another words, if word wj
occurs after wi in one sentence, then we add an
edge wi → wj for the graph. Figure 4 gives an
example of the word graph. The search space of
the graph is larger compared with that of Filippova
(2010) because of more added edges.
Different from Filippova (2010), salience
information is introduced into the calculation of
the weights of vertexes. One word that occurs
in more salient candidate should have higher
weight. Given a graph G = (V, £), where V =
</bodyText>
<equation confidence="0.8727216">
{V1, · · · , Vn} denotes the word nodes and £ =
{Eij E {0, 1}, i, j E [1, n]} denotes the edges.
The vertex weight is computed as follows:
�w(Vi) = sal(e) exp{−dist(Vi.w, e)} (3)
eECE
</equation>
<bodyText confidence="0.999706">
where sal(e) is the salience score of an event
from the candidate extraction step, Vi.w denotes
the word of vertex Vi, and dist(w, e) denotes the
distance from the word w to the event e, which
are defined by the minimum distance from w
to all the related words of e in a sentence by
the dependency path5 between them. Intuitively,
equation 3 demonstrates that a vertex is salient
when its corresponding word is close to salient
</bodyText>
<footnote confidence="0.973668">
5The distance is +∞ when e and w are not in one
sentence.
</footnote>
<bodyText confidence="0.998420625">
events. It is worth noting that the formula
can adapt to extractive and abstractive models
as well, by replacing events with sentences and
phrases. We use them for the SentRank and
PhraseRank baseline systems in Section 4.3,
respectively.
The equation to compute the edge weight is
adopted from Filippova (2010):
</bodyText>
<equation confidence="0.999930333333333">
�w&apos;(Eij) = rdist(Vi.w, Vj.w)
w(Eij) = w(Vi)w(Vj) · w&apos;(Eij)
s w(Vi) + w(Vj) (4)
</equation>
<bodyText confidence="0.998677">
where w&apos;(Eij) refers to the sum of
rdist(Vi.w,Vj.w) over all sentences, and rdist(·)
denotes the reciprocal distance of two words in a
sentence by the dependency path. By the formula,
an edge is salient when the corresponding vertex
weights are large or the corresponding words are
close.
</bodyText>
<subsectionHeader confidence="0.791613">
3.2.2 Scoring Method
</subsectionHeader>
<bodyText confidence="0.999898125">
The key to our MSC model is the path scoring
function. We measure a candidate path based
on two aspects. Besides the sum edge score of
the path, we exploit a trigram language model to
compute a fluency score of the path. Language
models have been commonly used to generate
more readable titles.
The overall score of a path is compute by:
</bodyText>
<equation confidence="0.998283142857143">
score(p) = edge(p) + A x flu(p)
E
EijEp ln{w(Eij)}
edge(p) =
n
flu(p) = Ei ln{p(wi|wi−2wi−1)}
n
</equation>
<bodyText confidence="0.9993705">
where p is a candidate path and the corresponding
word sequence of p is w1 · · · wn. A trigram
language model is trained using SRILM6 on
English Gigaword (LDC2011T07).
</bodyText>
<subsectionHeader confidence="0.769831">
3.2.3 Beam Search
</subsectionHeader>
<bodyText confidence="0.965450142857143">
Beam search has been widely used aiming to
find the sub optimum result (Collins and Roark,
2004; Zhang and Clark, 2011), when exact
inference is extremely difficult. Assuming our
word graph has a vertex size of n, the worst
computation complexity is O(n4) when using a
trigram language model, which is time consuming.
</bodyText>
<footnote confidence="0.628106">
6http://www.speech.sri.com/projects/srilm/
</footnote>
<figure confidence="0.999877136363637">
(S)
King
Norodom
...
Hun
...
rejected
...
on
Sun
opposition
...
for
party
...
talks
...
groups
...
...
(E)
(5)
</figure>
<page confidence="0.846696">
466
</page>
<figure confidence="0.950581947368421">
Input: G +— (V, £), LM, B
Output: best
candidates +— { {(S)} }
loop do
beam +— { }
for each candidate in candidates
if candidate endwith (E)
ADDTOBEAM(beam, candidate)
continue
for each Vi in V
candidate +— ADDVERTEX(candidate, Vi)
COMPUTESCORE(candidate, LM)
ADDTOBEAM(beam, candidate)
end for
end for
candidates +— TOP-K(beam, B)
if candidates all endwith (E) : break
end loop
best +— BEST(candidates)
</figure>
<figureCaption confidence="0.999972">
Figure 5: The beam-search algorithm.
</figureCaption>
<bodyText confidence="0.9980072">
Using beam search, assuming the beam size is B,
the time complexity decreases to O(Bn2).
Pseudo-code of our beam search algorithm is
shown in Figure 5. During search, we use
candidates to save a fixed size (B) of partial
results. For each iteration, we generate a set of
new candidates by adding one vertex from the
graph, computing their scores, and maintaining
the top B candidates for the next iteration. If
one candidate reaches the end of the graph, we
do not expand it, directly adding it into the new
candidate set according to its current score. If
all the candidates reach the end, the searching
algorithm terminates and the result path is the
candidate from candidates with the highest score.
</bodyText>
<sectionHeader confidence="0.998579" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.957339">
4.1 Settings
</subsectionHeader>
<bodyText confidence="0.999955">
We use the standard HG test dataset to evaluate
our model, which consists of 500 articles from
DUC–04 task 17, where each article is provided
with four reference headlines. In particular, we
use the first 100 articles from DUC–07 as our
development set. There are averaged 40 events per
article in the two datasets. All the pre-processing
steps, including POS tagging, lemma analysis,
dependency parsing and anaphora resolution, are
</bodyText>
<footnote confidence="0.719283">
7http://duc.nist.gov/duc2004/tasks.html
</footnote>
<bodyText confidence="0.999811285714286">
conducted using the Stanford NLP tools (Marneffe
and Manning, 2008). The MRP iteration number
is set to 10.
We use ROUGE (Lin, 2004) to automatically
measure the model performance, which has been
widely used in summarization tasks (Wang et al.,
2013; Ng et al., 2014). We focus on Rouge1
and Rouge2 scores, following Xu et al. (2010).
In addition, we conduct human evaluations, using
the same method as Woodsend et al. (2010).
Four participants are asked to rate the generated
headlines by three criteria: informativeness (how
much important information in the article does
the headline describe?), fluency (is it fluent to
read?) and coherence (does it capture the topic of
article?). Each headline is given a subjective score
from 0 to 5, with 0 being the worst and 5 being
the best. The first 50 documents from the test set
and their corresponding headlines are selected for
human rating. We conduct significant tests using
t-test.
</bodyText>
<subsectionHeader confidence="0.994523">
4.2 Development Results
</subsectionHeader>
<bodyText confidence="0.999321916666667">
There are three important parameters in the
proposed event-driven model, including the beam
size B, the fluency weight A and the number
of candidate events N. We find the optimum
parameters on development dataset in this section.
For efficiency, the three parameters are optimized
separately. The best performance is achieved with
B = 8, A = 0.4 and N = 10. We report the model
results on the development dataset to study the
influences of the three parameters, respectively,
with the other two parameters being set with their
best value.
</bodyText>
<subsectionHeader confidence="0.972489">
4.2.1 Influence of Beam Size
</subsectionHeader>
<bodyText confidence="0.999960833333333">
We perform experiments with different beam
widths. Figure 6 shows the results of the proposed
model with beam sizes of 1, 2, 4, 8, 16, 32,
64. As can be seen, our model can achieve the
best performances when the beam size is set to 8.
Larger beam sizes do not bring better results.
</bodyText>
<subsectionHeader confidence="0.779334">
4.2.2 Influence of Fluency Weight
</subsectionHeader>
<bodyText confidence="0.999933">
The fluency score is used for generating readable
titles, while the edge score is used for generating
informative titles. The balance between them is
important. By default, we set one to the weight
of edge score, and find the best weight A for the
fluency score. We set A ranging from 0 to 1 with
and interval of 0.1, to investigate the influence of
</bodyText>
<page confidence="0.998357">
467
</page>
<figure confidence="0.913107">
Beam Size
</figure>
<figureCaption confidence="0.9996555">
Figure 6: Results with different beam sizes.
Figure 7: Results using different fluency weights.
</figureCaption>
<bodyText confidence="0.92413">
this parameter8. Figure 7 shows the results. The
best result is obtained when A = 0.4.
</bodyText>
<subsectionHeader confidence="0.92345">
4.2.3 Influence of Candidate Event Count
</subsectionHeader>
<bodyText confidence="0.999965272727273">
Ideally, all the sentences of an original text should
be considered in multi-sentence compression. But
an excess of sentences would bring more noise.
We suppose that the number of candidate events
N is important as well. To study its influence, we
report the model results with different N, from 1
to 15 with an interval of 1. As shown in Figure
8, the performance increases significantly from 1
to 10, and no more gains when N &gt; 10. The
performance decreases drastically when M ranges
from 12 to 15.
</bodyText>
<subsectionHeader confidence="0.998214">
4.3 Final Results
</subsectionHeader>
<bodyText confidence="0.999985">
Table 1 shows the final results on the test
dataset. The performances of the proposed event-
driven model are shown by EventRank. In
addition, we use our graph-based MSC model to
</bodyText>
<footnote confidence="0.8608895">
8Preliminary results show that λ is better below one.
9The mark ∗ denotes the results are inaccurate, which are
guessed from the figures in the published paper.
Number of Candidate Events
</footnote>
<figureCaption confidence="0.976889">
Figure 8: Results using different numbers of
</figureCaption>
<table confidence="0.999020214285714">
candidate events.
Method Model Type Rouge1 Rouge2
Our SalMSC
SentRank Extractive 0.3511 0.1375
PhraseRank Abstractive 0.3706 0.1415
EventRank Event-driven 0.42471 0.14841
Using MSC
SentRank Extractive 0.2773 0.0980
PhraseRank Abstractive 0.3652 0.1299
EventRank Event-driven 0.38221 0.13801
Other work
SentRank+SSC Extractive 0.2752 0.0855
Topiary Abstractive 0.2835 0.0872
Woodsend Abstractive 0.26* 0.06*9
</table>
<tableCaption confidence="0.999376">
Table 1: Performance comparison for automatic
</tableCaption>
<bodyText confidence="0.979887357142857">
evaluation. The mark ‡ denotes that the result is
significantly better with a p-value below 0.01.
generate titles for SentRank and PhraseRank,
respectively, as mentioned in Section 3.2.1. By
comparison with the two models, we can examine
the effectiveness of the event-driven model. As
shown in Table 1, the event-driven model achieves
the best scores on both Rouge1 and Rouge2,
demonstrating events are more effective than
sentences and phrases.
Further, we compare our proposed MSC
method with the MSC proposed by Filippova
(2010), to study the effectiveness of our
novel MSC. We use MSC10 and SalMSC11 to
</bodyText>
<footnote confidence="0.98984725">
10The MSC source code, published by Boudin and Morin
(2013), is available at https://github.com/boudinfl/takahe.
11Our source code is available at https://github.com/
dram218/WordGraphCompression.
</footnote>
<figure confidence="0.998147672727273">
0.42
0.4
0.38
0.36
0.34
0.32
0.3
10 20 30 40 50 60
Rouge1
Rouge2
2 4 6 8 10 12 14
Rouge1
Rouge2
0.42
0.4
0.38
0.36
0.34
0.32
0.16
0.15
0.14
0.13
0.12
Rouge1
0.165
0.16
0.155
0.15
Rouge2
0.145
0.14
0.135
0.42
0.41
0.4
Rouge1
0.39
0.38
0.37
0.36
0.35 0.13
0 0.2 0.4 0.6 0.8 1
Fluency Weight
Rouge1
Rouge2
0.16 Rouge1
0.15
0.14
0.13
Rouge2
0.12
0.11
0.1
Rouge2
</figure>
<page confidence="0.997697">
468
</page>
<table confidence="0.99953725">
Method Info. Infu. Cohe.
SentRank 4.13 2.85 2.54
PhraseRank 4.21 3.25 2.62
EventRank 4.35$ 3.41$ 3.22$
</table>
<tableCaption confidence="0.995423">
Table 2: Results from the manual evaluation. The
</tableCaption>
<bodyText confidence="0.999746321428571">
mark ‡ denotes the result is significantly better
with a p-value below 0.01.
SentRank, PhraseRank and EventRank to
denote their MSC method and our proposed MSC,
respectively, applying them, respectively. As
shown in Table 1, better performance is achieved
by our MSC, demonstrating the effectiveness of
our proposed MSC. Similarly, the event-driven
model can achieve the best results.
We report results of previous state-of-the-art
systems as well. SentRank+SSC denotes the
result of Erkan and Radev (2004), which uses
our SentRank and SSC to obtain the final title.
Topiary denotes the result of Zajic et al. (2005),
which is an early abstractive model. Woodsend
denotes the result of Woodsend et al. (2010),
which is an abstractive model using a quasi-
synchronous grammar to generate a title. As
shown in Table 1, MSC is significantly better than
SSC, and our event-driven model achieves the
best performance, compared with state-of-the-art
systems.
Following Alfonseca et al. (2013), we conduct
human evaluation also. The results are shown in
Table 2, by three aspects: informativeness, fluency
and coherence. The overall tendency is similar to
the results, and the event-driven model achieves
the best results.
</bodyText>
<subsectionHeader confidence="0.998261">
4.4 Example Outputs
</subsectionHeader>
<bodyText confidence="0.999616769230769">
We show several representative examples of the
proposed event-driven model, in comparison with
the extractive and abstractive models. The
examples are shown in Table 3.
In the first example, the results of both
SentRank and PhraseRank contain the
redundant phrase “catastrophe Tuesday”. The
output of PhraseRank is less fluent compared
with that of SentRank. The preposition “for”
is not recovered by the headline generation
system PhraseRank. In contrast, the output of
EventRank is better, capturing the major event
in the reference title.
</bodyText>
<table confidence="0.999806">
Method Generated Headlines
Reference Honduras, other Caribbean countries brace
SentRank for the wrath of Hurricane Mitch
PhraseRank Honduras braced for potential catastrophe
EventRank Tuesday as Hurricane Mitch roared through
northwest Caribbean
Honduras braced catastrophe Tuesday
Hurricane Mitch roared northwest
Caribbean
Honduras braced for Hurricane Mitch
roared through northwest Caribbean
Reference At Ibero-American summit Castro protests
SentRank arrest of Pinochet in London
PhraseRank Castro disagreed with the arrest Augusto
EventRank Pinochet calling international meddling
Cuban President Fidel Castro disagreed
arrest London Chilean dictator Augusto
Pinochet
Fidel Castro disagreed with arrest in
London of Chilean dictator Augusto
Pinochet
Reference Cambodian leader Hun Sen rejects
SentRank opposition demands for talks in Beijing
PhraseRank Hun Sen accusing opposition parties of
EventRank internationalize the political crisis
opposition parties demands talks
internationalize political crisis
Cambodian leader Hun Sen rejected
opposition parties demands for talks
</table>
<tableCaption confidence="0.9702215">
Table 3: Comparison of headlines generated by the
different methods.
</tableCaption>
<bodyText confidence="0.999905541666667">
In the second example, the outputs of three
systems all lose the phrase “Ibero-American
summit”. SentRank gives different additional
information compared with PhraseRank and
EventRank. Overall, the three outputs can be
regarded as comparable. PhraseRank also has a
fluency problem by ignoring some function words.
In the third example, SentRank does not
capture the information on “demands for talks”.
PhraseRank discards the preposition word
“for”. The output of EventRank is better, being
both more fluent and more informative.
From the three examples, we can see that
SentRank tends to generate more readable
titles, but may lose some important information.
PhraseRank tends to generate a title with
more important words, but the fluency is
relatively weak even with MSC. EventRank
combines the advantages of both SentRank
and PhraseRank, generating titles that contain
more important events with complete structures.
The observation verifies our hypothesis in the
introduction — that extractive models have
the problem of low information coverage, and
</bodyText>
<page confidence="0.999206">
469
</page>
<bodyText confidence="0.99973625">
abstractive models have the problem of poor
grammaticality. The event-driven mothod can
alleviate both issues since event offer a trade-off
between sentence and phrase.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999501534482759">
Our event-driven model is different from
traditional extractive (Dorr et al., 2003; Erkan
and Radev, 2004; Alfonseca et al., 2013) and
abstractive models (Zajic et al., 2005; Soricut
and Marcu, 2007; Woodsend et al., 2010; Xu
et al., 2010) in that events are used as the basic
processing units instead of sentences and phrases.
As mentioned above, events are a trade-off
between sentences and phrases, avoiding sparsity
and structureless problems. In particular, our
event-driven model can interact with sentences
and phrases, thus is a light combination for two
traditional models.
The event-driven model is mainly inspired
by Alfonseca et al. (2013), who exploit events
for multi-document headline generation. They
leverage titles of sub-documents for supervised
training. In contrast, we generate a title for a
single document using an unsupervised model.
We use novel approaches for event ranking and
title generation.
In recent years, sentence compression (Galanis
and Androutsopoulos, 2010; Yoshikawa and Iida,
2012; Wang et al., 2013; Li et al., 2014;
Thadani, 2014) has received much attention.
Some methods can be directly applied for multi-
document summarization (Wang et al., 2013; Li
et al., 2014). To our knowledge, few studies
have been explored on applying them in headline
generation.
Multi-sentence compression based on word
graph was first proposed by Filippova (2010).
Some subsequent work was presented recently.
Boudin and Morin (2013) propose that the key
phrase is helpful to sentence generation. The
key phrases are extracted according to syntactic
pattern and introduced to identify shortest path
in their work. Mehdad et al. (2013; Mehdad
et al. (2014) introduce the MSC based on word
graph into meeting summarization. Tzouridis et
al. (2014) cast multi-sentence compression as a
structured predication problem. They use a large-
margin approach to adapt parameterised edge
weights to the data in order to acquire the shortest
path. In their work, the sentences introduced to
a word graph are treated equally, and the edges in
the graph are constructed according to the adjacent
order in original sentence.
Our MSC model is also inspired by Filippova
(2010). Our approach is more aggressive
than their approach, generating compressions
with arbitrary length by using a different edge
construction strategy. In addition, our search
algorithm is also different from theirs. Our
graph-based MSC model is also similar in
spirit to sentence fusion, which has been used
for multi-document summarization (Barzilay and
McKeown, 2005; Elsner and Santhanam, 2011).
</bodyText>
<sectionHeader confidence="0.997002" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999815">
We proposed an event-driven model headline
generation, introducing a graph-based MSC model
to generate the final title, based on a set of
events. Our event-driven model can incorporate
sentence and phrase salience, which has been used
in extractive and abstractive HG models. The
proposed graph-based MSC model is not limited
to our event-driven model. It can be applied
on extractive and abstractive models as well.
Experimental results on DUC–04 demonstrate
that event-driven model can achieve better results
than extractive and abstractive models, and the
proposed graph-based MSC model can bring
improved performances compared with previous
MSC techniques. Our final event-driven model
obtains the best result on this dataset.
For future work, we plan to explore two
directions. Firstly, we plan to introduce event
relations to learning event salience. In addition,
we plan to investigate other methods about multi-
sentence compression and sentence fusion, such as
supervised methods.
</bodyText>
<sectionHeader confidence="0.996985" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.97337175">
We thank all reviewers for their detailed
comments. This work is supported by the
State Key Program of National Natural Science
Foundation of China (Grant No.61133012), the
National Natural Science Foundation of China
(Grant No.61373108, 61373056), the National
Philosophy Social Science Major Bidding Project
of China (Grant No.11&amp;ZD189), and the Key
Program of Natural Science Foundation of
Hubei, China (Grant No.2012FFA088). The
corresponding authors of this paper are Meishan
Zhang and Donghong Ji.
</bodyText>
<page confidence="0.997099">
470
</page>
<sectionHeader confidence="0.982754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803778846154">
Enrique Alfonseca, Daniele Pighin and Guillermo
Garrido. 2013. HEADY: News headline abstraction
through event pattern clustering. In Proceedings of
ACL 2013,pages 1243–1253.
Regina Barzilay and Michael Elhadad. 1997.
Using Lexical Chains for Text Summarization.
In Proceedings of the Intelligent Scalable Text
Summarization Workshop(ISTS’97), Madrid.
Regina Barzilay and Kathleen R. McKeown.
2005. Sentence fusion for multidocument news
summarization. Computational Linguistics, 31(3),
pages 297–328.
Florian Boudin and Emmanuel Morin. 2013.
Keyphrase Extraction for N-best Reranking in
Multi-Sentence Compression. In Proccedings of the
NAACL HLT 2013 conference, page 298–305.
James Clarke and Mirella Lapata. 2010. Discourse
Constraints for Document Compression.
Computational Linguistics, 36(3), pages 411–
441.
Michael Collins and Brian Roark. 2004. Incremental
Parsing with the Perceptron Algorithm. In
Proceedings of ACL 2004, pages 111-118.
Corston-Oliver, Simon. 2001. Text compaction for
display on very small screens. In Proceedings of
the NAACL Workshop on Automatic Summarization,
Pittsburg, PA, 3 June 2001, pages 89–98.
Xiao Ding, Yue Zhang, Ting Liu, Junwen Duan.
2014. Using Structured Events to Predict Stock
Price Movement : An Empirical Investigation. In
Proceedings of EMNLP 2014, pages 1415–1425.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In proceedings of the
HLT–NAACL 03 on Text summarization workshop,
volume 5, pages 1–8.
Micha Elsner and Deepak Santhanam. 2011. Learning
to fuse disparate sentences. In Proceedings of ACL
2011, pages 54–63.
Nicolai Erbs, Iryna Gurevych and Torsten Zesch.
2013. Hierarchy Identification for Automatically
Generating Table-of-Contents. In Proceedings of
Recent Advances in Natural Language Processing,
Hissar, Bulgaria, pages 252–260.
Gunes Erkan and Dragomir R Radev. 2004. LexRank :
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research 22, 2004, pages 457–479.
Fader A, Soderland S, Etzioni O. 2011. Identifying
relations for open information extraction. In
Proceedings of EMNLP 2011, pages 1535–1545.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In
Proceedings of Coling 2010, pages 322–330.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings ofNAACL 2010, pages
885–893.
Barbara J. Grosz and Scott Weinstein and Aravind K.
Joshi. 1995. Centering: A framework for modeling
the local coherence of discourse. Computational
Linguistics, volume 21, pages 203–225.
Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina,
Reid Swanson and Marilyn A.Walker. 2013.
Unsupervised Induction of Contingent Event Pairs
from Film Scenes. In Proceedings of EMNLP 2013,
pages 369–379.
Chen Li,Yang Liu, Fei Liu, Lin Zhao, Fuliang Weng.
2014. Improving Multi-documents Summarization
by Sentence Compression based on Expanded
Constituent Parse Trees. In Proceedings of EMNLP
2014, pages 691–701.
Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Text
Summarization Branckes Out: Proceedings of the
ACL–04 Workshop, pages 74–81.
Andre F.T. Martins and Noah A. Smith. 2009.
Summarization with a joint model for sentence
extraction and compression. In Proceedings of
the Workshop on Integer Linear Programming for
Natural Language Processing, pages 1–9.
Yashar Mehdad, Giuseppe Carenini, Frank W.Tompa
and Raymond T.Ng. 2013. Abstractive Meeting
Summarization with Entailment and Fusion. In
Proceedings of the 14th European Workshop on
Natural Language Generation, pages 136–146.
Yashar Mehdad, Giuseppe Carenini and Raymond
T.Ng. 2014. Abstractive Summarization of
Spoken and Written Conversations Based on Phrasal
Queries. In Proceedings of ACL 2014, pages 1220–
1230.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of
the structure of text. Computational Linguistics,
17(1), pages 21–48.
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. The stanford typed dependencies
representation. In COLING 2008 Workshop
on Cross-framework and Cross-domain Parser
Evaluation.
Jun-Ping Ng, Yan Chen, Min-Yen Kan, Zhoujun Li.
2014. Exploiting Timelines to Enhance Multi-
document Summarization. Proceedings of ACL
2014, pages 923–933.
</reference>
<page confidence="0.981827">
471
</page>
<reference confidence="0.99975996875">
Likun Qiu and Yue Zhang. 2014. ZORE: A Syntax-
based System for Chinese Open Relation Extraction.
Proceedings of EMNLP 2014, pages 1870–1880.
Robert G. Sargent. 1988. Polynomial Time Joint
Structural Inference for Sentence Compression.
Management Science, 34(10), pages 1231–1251.
Schwartz R. 1988. Unsupervised topic discovery. In
Proceedings of workshop on language modeling and
information retrieval, pages 72–77.
R. Soricut, and D. Marcu. 2007. Abstractive headline
generation using WIDL-expressions. Information
Processing and Management, 43(6), pages 1536–
1548.
Kapil Thadani. 2014. Approximation Strategies
for Multi-Structure Sentence Compression.
Proceedings of ACL 2014, pages 1241–1251.
Emmanouil Tzouridis, Jamal Abdul Nasir and Ulf
Brefeld. 2014. Learning to Summarise Related
Sentences. Proceedings of COLING 2014,Dublin,
Ireland, August 23-29 2014. pages 1636–1647.
Carles Ventura, Xavier Giro-i-Nieto, Veronica
Vilaplana, Daniel Giribet, and Eusebio Carasusan.
2013. Automatic keyframe selection based on
Mutual Reinforcement Algorithm. In Proceedings
of 11th international workshop on content-based
multimedia indexing(CBMI), pages 29–34.
Stephen Wan and Kathleen McKeown. 2004.
Generating overview summaries of ongoing email
thread discussions. In Proceedings of COLING
2004, Geneva, Switzerland, 2004, pages 1384–1394.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu
Florian, Claire Cardie. 2013. A sentence
compression based framework to query-focused
mutli-document summarization. In Proceedings of
ACL 2013, Sofia, Bulgaria, August 4-9 2013, pages
1384–1394.
Kristian Woodsend, Yansong Feng and Mirella Lapata.
2010. Title generation with quasi-synchronous
grammar. In Proceedings of EMNLP 2010, pages
513–523.
Songhua Xu, Shaohui Yang and Francis C.M. Lau.
2010. Keyword extraction and headline generation
using novel work features. In Proceedings of AAAI
2010, pages 1461–1466.
Katsumasa Yoshikawa and Ryu Iida. 2012. Sentence
Compression with Semantic Role Constraints. In
Proceedings of ACL 2012, pages 349–353.
David Zajic, Bonnie Dorr and Richard Schwartz. 2005.
Headline generation for written and broadcast news.
lamp-tr-120, cs-tr-4698.
Hongyuan Zha. 2002. Generic summarization
and keyphrase extraction using mutual reinforement
principle and sentence clustering. In Proceedings of
SIGIR 2002, pages 113–120.
Qi Zhang, Xipeng Qiu, Xuanjing Huang, Wu Lide.
2008. Learning semantic lexicons using graph
mutual reinforcement based bootstrapping. Acta
Automatica Sinica, 34(10), pages 1257–1261.
Yue Zhang, Stephen Clark. 2011. Syntactic Processing
Using the Generalized Perceptron and Beam Search.
Computational Linguistics, 37(1), pages 105–150.
Yue Zhang. 2013. Partial-Tree Linearization:
Generalized Word Ordering for Text Synthesis. In
Proceedings of IJCAI 2013, pages 2232–2238.
</reference>
<page confidence="0.998491">
472
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.504843">
<title confidence="0.997872">Event-Driven Headline Generation</title>
<author confidence="0.906907">Yue Meishan</author>
<author confidence="0.906907">Donghong</author>
<affiliation confidence="0.9966435">tComputer School, Wuhan University, tSingapore University of Technology and</affiliation>
<email confidence="0.58264">zhang,meishan</email>
<abstract confidence="0.997701764705882">We propose an event-driven model for headline generation. Given an input document, the system identifies a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Daniele Pighin</author>
<author>Guillermo Garrido</author>
</authors>
<title>HEADY: News headline abstraction through event pattern clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,pages</booktitle>
<pages>1243--1253</pages>
<contexts>
<context position="1590" citStr="Alfonseca et al., 2013" startWordPosition="228" endWordPosition="231">ated paragraphs) using a single short sentence. The task is useful in a number of practical scenarios, such as compressing text for mobile device users (Corston-Oliver, 2001), generating table of contents (Erbs et al., 2013), and email summarization (Wan and McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvanta</context>
<context position="6011" citStr="Alfonseca et al. (2013)" startWordPosition="869" endWordPosition="872">pproaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline sentence ranking method. In this paper, we use SentRank to denote this method. Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to compress a sentence by making use of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Abstractive Headline Generation Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases u</context>
<context position="26225" citStr="Alfonseca et al. (2013)" startWordPosition="4159" endWordPosition="4162">hieve the best results. We report results of previous state-of-the-art systems as well. SentRank+SSC denotes the result of Erkan and Radev (2004), which uses our SentRank and SSC to obtain the final title. Topiary denotes the result of Zajic et al. (2005), which is an early abstractive model. Woodsend denotes the result of Woodsend et al. (2010), which is an abstractive model using a quasisynchronous grammar to generate a title. As shown in Table 1, MSC is significantly better than SSC, and our event-driven model achieves the best performance, compared with state-of-the-art systems. Following Alfonseca et al. (2013), we conduct human evaluation also. The results are shown in Table 2, by three aspects: informativeness, fluency and coherence. The overall tendency is similar to the results, and the event-driven model achieves the best results. 4.4 Example Outputs We show several representative examples of the proposed event-driven model, in comparison with the extractive and abstractive models. The examples are shown in Table 3. In the first example, the results of both SentRank and PhraseRank contain the redundant phrase “catastrophe Tuesday”. The output of PhraseRank is less fluent compared with that of S</context>
<context position="29544" citStr="Alfonseca et al., 2013" startWordPosition="4632" endWordPosition="4635">ely weak even with MSC. EventRank combines the advantages of both SentRank and PhraseRank, generating titles that contain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titl</context>
</contexts>
<marker>Alfonseca, Pighin, Garrido, 2013</marker>
<rawString>Enrique Alfonseca, Daniele Pighin and Guillermo Garrido. 2013. HEADY: News headline abstraction through event pattern clustering. In Proceedings of ACL 2013,pages 1243–1253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Intelligent Scalable Text Summarization Workshop(ISTS’97),</booktitle>
<location>Madrid.</location>
<contexts>
<context position="10672" citStr="Barzilay and Elhadad, 1997" startWordPosition="1567" endWordPosition="1570"> extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3. A pronoun is added to the corresponding lexical chain if it refers to a word in the chain (The coreference resolution is performed using the Stanford Coreference Resolution system);3 4. Lexical chains are merged if their main</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the Intelligent Scalable Text Summarization Workshop(ISTS’97), Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>297--328</pages>
<contexts>
<context position="31961" citStr="Barzilay and McKeown, 2005" startWordPosition="5003" endWordPosition="5006">rder to acquire the shortest path. In their work, the sentences introduced to a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has been used in extractive and abstractive HG models. The proposed graph-based MSC model is not limited to our event-driven model. It can be applied on extractive and abstractive models as well. Experimental results on DUC–04 demonstrate that event-driven model can achieve better results than extractive and abstracti</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3), pages 297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Boudin</author>
<author>Emmanuel Morin</author>
</authors>
<title>Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression.</title>
<date>2013</date>
<booktitle>In Proccedings of the NAACL HLT 2013 conference,</booktitle>
<pages>298--305</pages>
<contexts>
<context position="10843" citStr="Boudin and Morin (2013)" startWordPosition="1596" endWordPosition="1599">sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3. A pronoun is added to the corresponding lexical chain if it refers to a word in the chain (The coreference resolution is performed using the Stanford Coreference Resolution system);3 4. Lexical chains are merged if their main words are in the same synset of WordNet.4 2NPs are extracted according to the dependency relations nn and amod. As shown in Figure 2, we can extract the noun phrase Aryan</context>
<context position="24589" citStr="Boudin and Morin (2013)" startWordPosition="3895" endWordPosition="3898">lt is significantly better with a p-value below 0.01. generate titles for SentRank and PhraseRank, respectively, as mentioned in Section 3.2.1. By comparison with the two models, we can examine the effectiveness of the event-driven model. As shown in Table 1, the event-driven model achieves the best scores on both Rouge1 and Rouge2, demonstrating events are more effective than sentences and phrases. Further, we compare our proposed MSC method with the MSC proposed by Filippova (2010), to study the effectiveness of our novel MSC. We use MSC10 and SalMSC11 to 10The MSC source code, published by Boudin and Morin (2013), is available at https://github.com/boudinfl/takahe. 11Our source code is available at https://github.com/ dram218/WordGraphCompression. 0.42 0.4 0.38 0.36 0.34 0.32 0.3 10 20 30 40 50 60 Rouge1 Rouge2 2 4 6 8 10 12 14 Rouge1 Rouge2 0.42 0.4 0.38 0.36 0.34 0.32 0.16 0.15 0.14 0.13 0.12 Rouge1 0.165 0.16 0.155 0.15 Rouge2 0.145 0.14 0.135 0.42 0.41 0.4 Rouge1 0.39 0.38 0.37 0.36 0.35 0.13 0 0.2 0.4 0.6 0.8 1 Fluency Weight Rouge1 Rouge2 0.16 Rouge1 0.15 0.14 0.13 Rouge2 0.12 0.11 0.1 Rouge2 468 Method Info. Infu. Cohe. SentRank 4.13 2.85 2.54 PhraseRank 4.21 3.25 2.62 EventRank 4.35$ 3.41$ 3.2</context>
<context position="30869" citStr="Boudin and Morin (2013)" startWordPosition="4833" endWordPosition="4836">ing an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to a word graph are treated equally, and the edges in the g</context>
</contexts>
<marker>Boudin, Morin, 2013</marker>
<rawString>Florian Boudin and Emmanuel Morin. 2013. Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression. In Proccedings of the NAACL HLT 2013 conference, page 298–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Discourse Constraints for Document Compression.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<pages>411--441</pages>
<marker>Clarke, Lapata, 2010</marker>
<rawString>James Clarke and Mirella Lapata. 2010. Discourse Constraints for Document Compression. Computational Linguistics, 36(3), pages 411– 441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental Parsing with the Perceptron Algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>111--118</pages>
<contexts>
<context position="18084" citStr="Collins and Roark, 2004" startWordPosition="2832" endWordPosition="2835">two aspects. Besides the sum edge score of the path, we exploit a trigram language model to compute a fluency score of the path. Language models have been commonly used to generate more readable titles. The overall score of a path is compute by: score(p) = edge(p) + A x flu(p) E EijEp ln{w(Eij)} edge(p) = n flu(p) = Ei ln{p(wi|wi−2wi−1)} n where p is a candidate path and the corresponding word sequence of p is w1 · · · wn. A trigram language model is trained using SRILM6 on English Gigaword (LDC2011T07). 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4) when using a trigram language model, which is time consuming. 6http://www.speech.sri.com/projects/srilm/ (S) King Norodom ... Hun ... rejected ... on Sun opposition ... for party ... talks ... groups ... ... (E) (5) 466 Input: G +— (V, £), LM, B Output: best candidates +— { {(S)} } loop do beam +— { } for each candidate in candidates if candidate endwith (E) ADDTOBEAM(beam, candidate) continue for each Vi in V candidate +— ADDVERTEX(ca</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental Parsing with the Perceptron Algorithm. In Proceedings of ACL 2004, pages 111-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
</authors>
<title>Text compaction for display on very small screens.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on Automatic Summarization,</booktitle>
<pages>89--98</pages>
<location>Pittsburg, PA, 3</location>
<contexts>
<context position="1141" citStr="Corston-Oliver, 2001" startWordPosition="164" endWordPosition="165">ts, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. 1 Introduction Headline generation (HG) is a text summarization task, which aims to describe an article (or a set of related paragraphs) using a single short sentence. The task is useful in a number of practical scenarios, such as compressing text for mobile device users (Corston-Oliver, 2001), generating table of contents (Erbs et al., 2013), and email summarization (Wan and McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction,</context>
</contexts>
<marker>Corston-Oliver, 2001</marker>
<rawString>Corston-Oliver, Simon. 2001. Text compaction for display on very small screens. In Proceedings of the NAACL Workshop on Automatic Summarization, Pittsburg, PA, 3 June 2001, pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ding</author>
<author>Yue Zhang</author>
<author>Ting Liu</author>
<author>Junwen Duan</author>
</authors>
<title>Using Structured Events to Predict Stock Price Movement : An Empirical Investigation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP 2014,</booktitle>
<pages>1415--1425</pages>
<contexts>
<context position="8346" citStr="Ding et al., 2014" startWordPosition="1220" endWordPosition="1223">imilar to extractive and abstractive models, the proposed event-driven model consists of two steps, namely candidate extraction and headline generation. 3.1 Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is per</context>
</contexts>
<marker>Ding, Zhang, Liu, Duan, 2014</marker>
<rawString>Xiao Ding, Yue Zhang, Ting Liu, Junwen Duan. 2014. Using Structured Events to Predict Stock Price Movement : An Empirical Investigation. In Proceedings of EMNLP 2014, pages 1415–1425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In proceedings of the HLT–NAACL 03 on Text summarization workshop,</booktitle>
<volume>5</volume>
<pages>1--8</pages>
<contexts>
<context position="1840" citStr="Dorr et al., 2003" startWordPosition="263" endWordPosition="266">nd McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse,</context>
<context position="5314" citStr="Dorr et al., 2003" startWordPosition="766" endWordPosition="769">our candidate extraction and headline generation methods outperform competitive baseline methods, and our model achieves the best results compared with previous state-of-the-art systems. 2 Background Previous extractive and abstractive models take two main steps, namely candidate extraction and headline generation. Here, we introduce these two types of models according to the two steps. 2.1 Extractive Headline Generation Candidate Extraction. Extractive models exploit sentences as the basic processing units in this step. Sentences are ranked by their salience according to specific strategies (Dorr et al., 2003; Erkan and Radev, 2004; Zajic et al., 2005). One of the stateof-the-art approaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline sentence ranking method. In this paper, we use SentRank to denote this method. Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to com</context>
<context position="29496" citStr="Dorr et al., 2003" startWordPosition="4624" endWordPosition="4627">mportant words, but the fluency is relatively weak even with MSC. EventRank combines the advantages of both SentRank and PhraseRank, generating titles that contain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In proceedings of the HLT–NAACL 03 on Text summarization workshop, volume 5, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Deepak Santhanam</author>
</authors>
<title>Learning to fuse disparate sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>54--63</pages>
<contexts>
<context position="31990" citStr="Elsner and Santhanam, 2011" startWordPosition="5007" endWordPosition="5010"> path. In their work, the sentences introduced to a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has been used in extractive and abstractive HG models. The proposed graph-based MSC model is not limited to our event-driven model. It can be applied on extractive and abstractive models as well. Experimental results on DUC–04 demonstrate that event-driven model can achieve better results than extractive and abstractive models, and the proposed g</context>
</contexts>
<marker>Elsner, Santhanam, 2011</marker>
<rawString>Micha Elsner and Deepak Santhanam. 2011. Learning to fuse disparate sentences. In Proceedings of ACL 2011, pages 54–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolai Erbs</author>
</authors>
<title>Iryna Gurevych and Torsten Zesch.</title>
<date>2013</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>252--260</pages>
<location>Hissar, Bulgaria,</location>
<marker>Erbs, 2013</marker>
<rawString>Nicolai Erbs, Iryna Gurevych and Torsten Zesch. 2013. Hierarchy Identification for Automatically Generating Table-of-Contents. In Proceedings of Recent Advances in Natural Language Processing, Hissar, Bulgaria, pages 252–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexRank : Graph-based Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>22</volume>
<pages>457--479</pages>
<contexts>
<context position="5337" citStr="Erkan and Radev, 2004" startWordPosition="770" endWordPosition="773">ction and headline generation methods outperform competitive baseline methods, and our model achieves the best results compared with previous state-of-the-art systems. 2 Background Previous extractive and abstractive models take two main steps, namely candidate extraction and headline generation. Here, we introduce these two types of models according to the two steps. 2.1 Extractive Headline Generation Candidate Extraction. Extractive models exploit sentences as the basic processing units in this step. Sentences are ranked by their salience according to specific strategies (Dorr et al., 2003; Erkan and Radev, 2004; Zajic et al., 2005). One of the stateof-the-art approaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline sentence ranking method. In this paper, we use SentRank to denote this method. Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to compress a sentence by mak</context>
<context position="25747" citStr="Erkan and Radev (2004)" startWordPosition="4082" endWordPosition="4085">13 2.85 2.54 PhraseRank 4.21 3.25 2.62 EventRank 4.35$ 3.41$ 3.22$ Table 2: Results from the manual evaluation. The mark ‡ denotes the result is significantly better with a p-value below 0.01. SentRank, PhraseRank and EventRank to denote their MSC method and our proposed MSC, respectively, applying them, respectively. As shown in Table 1, better performance is achieved by our MSC, demonstrating the effectiveness of our proposed MSC. Similarly, the event-driven model can achieve the best results. We report results of previous state-of-the-art systems as well. SentRank+SSC denotes the result of Erkan and Radev (2004), which uses our SentRank and SSC to obtain the final title. Topiary denotes the result of Zajic et al. (2005), which is an early abstractive model. Woodsend denotes the result of Woodsend et al. (2010), which is an abstractive model using a quasisynchronous grammar to generate a title. As shown in Table 1, MSC is significantly better than SSC, and our event-driven model achieves the best performance, compared with state-of-the-art systems. Following Alfonseca et al. (2013), we conduct human evaluation also. The results are shown in Table 2, by three aspects: informativeness, fluency and coher</context>
<context position="29519" citStr="Erkan and Radev, 2004" startWordPosition="4628" endWordPosition="4631"> the fluency is relatively weak even with MSC. EventRank combines the advantages of both SentRank and PhraseRank, generating titles that contain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline gener</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R Radev. 2004. LexRank : Graph-based Lexical Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research 22, 2004, pages 457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="3742" citStr="Fader et al., 2011" startWordPosition="535" endWordPosition="538"> Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 462–472, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed base</context>
<context position="8290" citStr="Fader et al., 2011" startWordPosition="1209" endWordPosition="1212">surface realization preferences jointly. 463 3 Our Model Similar to extractive and abstractive models, the proposed event-driven model consists of two steps, namely candidate extraction and headline generation. 3.1 Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than p</context>
<context position="9545" citStr="Fader et al., 2011" startWordPosition="1397" endWordPosition="1400">nt extraction is performed on a bipartite graph, where the two types of nodes are lexical chains (Section 3.1.2) and events (Section 3.1.1), respectively. Mutual Reinforcement Principle (Zha, 2002) is applied to jointly learn chain and event salience on the bipartite graph for a given input. We obtain the top-k candidate events by their salience measures. 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities and relations (Fader et al., 2011). We follow Hu’s work (Hu et al., 2013) to extract events. Given a text, we first use the Stanford dependency parser1 to obtain the Stanford typed dependency structures of the sentences (Marneffe and Manning, 2008). Then we focus on 1http://nlp.stanford.edu/software/lex-parser.shtml DT NNPS MD VB DT NNP NNP POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. two relations, nsubj and dobj, for extracting event arguments. Event arguments that have the same predicate are merged into one event,</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Fader A, Soderland S, Etzioni O. 2011. Identifying relations for open information extraction. In Proceedings of EMNLP 2011, pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-sentence compression: Finding shortest paths in word graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>322--330</pages>
<contexts>
<context position="14372" citStr="Filippova (2010)" startWordPosition="2181" endWordPosition="2182"> w∈lR 1: w(ei) = s∈Sen(ei) where salabs(·) denotes the word salience score of an abstractive model, salext(·) denotes the sentence salience score of an extractive model, and Sen(ei) denotes the sentence set where ei is extracted from. We exploit our baseline sentence ranking method, SentRank, to obtain the sentence salience score, and use our baseline phrase ranking method, PhraseRank, to obtain the phrase salience score. 3.2 Headline Generation We use a graph-based multi-sentence compression (MSC) model to generate the final title for the proposed event-driven model. The model is inspired by Filippova (2010). First, a weighted directed acyclic word graph is built, with a start node and an end node in the graph. A headline can be obtained by any path from the start node to the end node. We measure each candidate path by a scoring function. Based on the measurement, we exploit a beam-search algorithm to find the optimum path. 3.2.1 Word-Graph Construction Given a set of candidate events CE, we extract all the sentences that contain the events. In particular, we add two artificial words, (S) and (E), to the start position and end position of all sentences, respectively. Following Filippova (2010), w</context>
<context position="15740" citStr="Filippova (2010)" startWordPosition="2417" endWordPosition="2418">ure 4: Word graph generated from candidates and a possible compression path. for all the word pairs that are adjacent in one sentence. The title generated using this strategy can mistakenly contain common word bigrams( i.e. adjacent words) in different sentences. To address this, we change the strategy slightly, by adding edges for all word pairs of one sentence in the original order. In another words, if word wj occurs after wi in one sentence, then we add an edge wi → wj for the graph. Figure 4 gives an example of the word graph. The search space of the graph is larger compared with that of Filippova (2010) because of more added edges. Different from Filippova (2010), salience information is introduced into the calculation of the weights of vertexes. One word that occurs in more salient candidate should have higher weight. Given a graph G = (V, £), where V = {V1, · · · , Vn} denotes the word nodes and £ = {Eij E {0, 1}, i, j E [1, n]} denotes the edges. The vertex weight is computed as follows: �w(Vi) = sal(e) exp{−dist(Vi.w, e)} (3) eECE where sal(e) is the salience score of an event from the candidate extraction step, Vi.w denotes the word of vertex Vi, and dist(w, e) denotes the distance from</context>
<context position="16978" citStr="Filippova (2010)" startWordPosition="2639" endWordPosition="2640">nt e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 5The distance is +∞ when e and w are not in one sentence. events. It is worth noting that the formula can adapt to extractive and abstractive models as well, by replacing events with sentences and phrases. We use them for the SentRank and PhraseRank baseline systems in Section 4.3, respectively. The equation to compute the edge weight is adopted from Filippova (2010): �w&apos;(Eij) = rdist(Vi.w, Vj.w) w(Eij) = w(Vi)w(Vj) · w&apos;(Eij) s w(Vi) + w(Vj) (4) where w&apos;(Eij) refers to the sum of rdist(Vi.w,Vj.w) over all sentences, and rdist(·) denotes the reciprocal distance of two words in a sentence by the dependency path. By the formula, an edge is salient when the corresponding vertex weights are large or the corresponding words are close. 3.2.2 Scoring Method The key to our MSC model is the path scoring function. We measure a candidate path based on two aspects. Besides the sum edge score of the path, we exploit a trigram language model to compute a fluency score o</context>
<context position="24454" citStr="Filippova (2010)" startWordPosition="3873" endWordPosition="3874">872 Woodsend Abstractive 0.26* 0.06*9 Table 1: Performance comparison for automatic evaluation. The mark ‡ denotes that the result is significantly better with a p-value below 0.01. generate titles for SentRank and PhraseRank, respectively, as mentioned in Section 3.2.1. By comparison with the two models, we can examine the effectiveness of the event-driven model. As shown in Table 1, the event-driven model achieves the best scores on both Rouge1 and Rouge2, demonstrating events are more effective than sentences and phrases. Further, we compare our proposed MSC method with the MSC proposed by Filippova (2010), to study the effectiveness of our novel MSC. We use MSC10 and SalMSC11 to 10The MSC source code, published by Boudin and Morin (2013), is available at https://github.com/boudinfl/takahe. 11Our source code is available at https://github.com/ dram218/WordGraphCompression. 0.42 0.4 0.38 0.36 0.34 0.32 0.3 10 20 30 40 50 60 Rouge1 Rouge2 2 4 6 8 10 12 14 Rouge1 Rouge2 0.42 0.4 0.38 0.36 0.34 0.32 0.16 0.15 0.14 0.13 0.12 Rouge1 0.165 0.16 0.155 0.15 Rouge2 0.145 0.14 0.135 0.42 0.41 0.4 Rouge1 0.39 0.38 0.37 0.36 0.35 0.13 0 0.2 0.4 0.6 0.8 1 Fluency Weight Rouge1 Rouge2 0.16 Rouge1 0.15 0.14 0.</context>
<context position="30799" citStr="Filippova (2010)" startWordPosition="4825" endWordPosition="4826">ning. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences </context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-sentence compression: Finding shortest paths in word graphs. In Proceedings of Coling 2010, pages 322–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>An extractive supervised two-stage method for sentence compression.</title>
<date>2010</date>
<booktitle>In Proceedings ofNAACL 2010,</booktitle>
<pages>885--893</pages>
<contexts>
<context position="30409" citStr="Galanis and Androutsopoulos, 2010" startWordPosition="4761" endWordPosition="4764">rade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced </context>
</contexts>
<marker>Galanis, Androutsopoulos, 2010</marker>
<rawString>Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence compression. In Proceedings ofNAACL 2010, pages 885–893.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<pages>203--225</pages>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J. Grosz and Scott Weinstein and Aravind K. Joshi. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, volume 21, pages 203–225.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhichao Hu</author>
</authors>
<title>Elahe Rahimtoroghi,</title>
<location>Larissa Munishkina,</location>
<marker>Hu, </marker>
<rawString>Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reid Swanson</author>
<author>Marilyn A Walker</author>
</authors>
<title>Unsupervised Induction of Contingent Event Pairs from Film Scenes.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP 2013,</booktitle>
<pages>369--379</pages>
<marker>Swanson, Walker, 2013</marker>
<rawString>Reid Swanson and Marilyn A.Walker. 2013. Unsupervised Induction of Contingent Event Pairs from Film Scenes. In Proceedings of EMNLP 2013, pages 369–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Yang Liu</author>
<author>Fei Liu</author>
<author>Lin Zhao</author>
<author>Fuliang Weng</author>
</authors>
<title>Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP 2014,</booktitle>
<pages>691--701</pages>
<contexts>
<context position="30471" citStr="Li et al., 2014" startWordPosition="4773" endWordPosition="4776">lems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; </context>
</contexts>
<marker>Li, Liu, Liu, Zhao, Weng, 2014</marker>
<rawString>Chen Li,Yang Liu, Fei Liu, Lin Zhao, Fuliang Weng. 2014. Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees. In Proceedings of EMNLP 2014, pages 691–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branckes Out: Proceedings of the ACL–04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="20240" citStr="Lin, 2004" startWordPosition="3182" endWordPosition="3183">ings We use the standard HG test dataset to evaluate our model, which consists of 500 articles from DUC–04 task 17, where each article is provided with four reference headlines. In particular, we use the first 100 articles from DUC–07 as our development set. There are averaged 40 events per article in the two datasets. All the pre-processing steps, including POS tagging, lemma analysis, dependency parsing and anaphora resolution, are 7http://duc.nist.gov/duc2004/tasks.html conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branckes Out: Proceedings of the ACL–04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>1--9</pages>
<marker>Martins, Smith, 2009</marker>
<rawString>Andre F.T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>Frank W Tompa</author>
<author>Raymond T Ng</author>
</authors>
<title>Abstractive Meeting Summarization with Entailment and Fusion.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Workshop on Natural Language Generation,</booktitle>
<pages>136--146</pages>
<contexts>
<context position="31069" citStr="Mehdad et al. (2013" startWordPosition="4865" endWordPosition="4868">2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressi</context>
</contexts>
<marker>Mehdad, Carenini, Tompa, Ng, 2013</marker>
<rawString>Yashar Mehdad, Giuseppe Carenini, Frank W.Tompa and Raymond T.Ng. 2013. Abstractive Meeting Summarization with Entailment and Fusion. In Proceedings of the 14th European Workshop on Natural Language Generation, pages 136–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<pages>1220--1230</pages>
<contexts>
<context position="31091" citStr="Mehdad et al. (2014)" startWordPosition="4869" endWordPosition="4872">; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary len</context>
</contexts>
<marker>Mehdad, Carenini, Ng, 2014</marker>
<rawString>Yashar Mehdad, Giuseppe Carenini and Raymond T.Ng. 2014. Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries. In Proceedings of ACL 2014, pages 1220– 1230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<pages>21--48</pages>
<contexts>
<context position="10643" citStr="Morris and Hirst, 1991" startWordPosition="1563" endWordPosition="1566">ons, nsubj and dobj, for extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3. A pronoun is added to the corresponding lexical chain if it refers to a word in the chain (The coreference resolution is performed using the Stanford Coreference Resolution system);3 4. Lexical ch</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1), pages 21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ping Ng</author>
<author>Yan Chen</author>
<author>Min-Yen Kan</author>
<author>Zhoujun Li</author>
</authors>
<title>Exploiting Timelines to Enhance Multidocument Summarization.</title>
<date>2014</date>
<booktitle>Proceedings of ACL 2014,</booktitle>
<pages>923--933</pages>
<contexts>
<context position="8364" citStr="Ng et al., 2014" startWordPosition="1224" endWordPosition="1227">e and abstractive models, the proposed event-driven model consists of two steps, namely candidate extraction and headline generation. 3.1 Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is performed on a bipart</context>
<context position="20375" citStr="Ng et al., 2014" startWordPosition="3202" endWordPosition="3205">ticle is provided with four reference headlines. In particular, we use the first 100 articles from DUC–07 as our development set. There are averaged 40 events per article in the two datasets. All the pre-processing steps, including POS tagging, lemma analysis, dependency parsing and anaphora resolution, are 7http://duc.nist.gov/duc2004/tasks.html conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from 0 to 5, with 0 being the worst and 5 being the best. The first 50 documents from the test set and their corresponding headlines are se</context>
</contexts>
<marker>Ng, Chen, Kan, Li, 2014</marker>
<rawString>Jun-Ping Ng, Yan Chen, Min-Yen Kan, Zhoujun Li. 2014. Exploiting Timelines to Enhance Multidocument Summarization. Proceedings of ACL 2014, pages 923–933.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Likun Qiu</author>
<author>Yue Zhang</author>
</authors>
<title>ZORE: A Syntaxbased System for Chinese Open Relation Extraction.</title>
<date>2014</date>
<booktitle>Proceedings of EMNLP 2014,</booktitle>
<pages>1870--1880</pages>
<contexts>
<context position="3764" citStr="Qiu and Zhang, 2014" startWordPosition="539" endWordPosition="542">he Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 462–472, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed based on the extracted eve</context>
</contexts>
<marker>Qiu, Zhang, 2014</marker>
<rawString>Likun Qiu and Yue Zhang. 2014. ZORE: A Syntaxbased System for Chinese Open Relation Extraction. Proceedings of EMNLP 2014, pages 1870–1880.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert G Sargent</author>
</authors>
<title>Polynomial Time Joint Structural Inference for Sentence Compression.</title>
<date>1988</date>
<journal>Management Science,</journal>
<volume>34</volume>
<issue>10</issue>
<pages>1231--1251</pages>
<marker>Sargent, 1988</marker>
<rawString>Robert G. Sargent. 1988. Polynomial Time Joint Structural Inference for Sentence Compression. Management Science, 34(10), pages 1231–1251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
</authors>
<title>Unsupervised topic discovery.</title>
<date>1988</date>
<booktitle>In Proceedings of workshop on language modeling and information retrieval,</booktitle>
<pages>72--77</pages>
<marker>Schwartz, 1988</marker>
<rawString>Schwartz R. 1988. Unsupervised topic discovery. In Proceedings of workshop on language modeling and information retrieval, pages 72–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Abstractive headline generation using WIDL-expressions.</title>
<date>2007</date>
<journal>Information Processing and Management,</journal>
<volume>43</volume>
<issue>6</issue>
<pages>1536--1548</pages>
<contexts>
<context position="2067" citStr="Soricut and Marcu, 2007" startWordPosition="295" endWordPosition="298"> headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammatic</context>
<context position="6526" citStr="Soricut and Marcu, 2007" startWordPosition="940" endWordPosition="943">ithm to compress a sentence by making use of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Abstractive Headline Generation Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extracted from Wikipedia. Woodsend et al. (2010) use supervised models to learn the salience score of each phrase. Here, we use the work of Soricut and Marcu (2007) , namely PhraseRank, as our baseline phrase ranking method, which is an unsupervised model without external resources. The method exploits unsupervised topic discovery to find a set of salient phrases. Headline Generation. In the headline generation step, abstractive models exploit sentence synthesis technologies to accomp</context>
<context position="29612" citStr="Soricut and Marcu, 2007" startWordPosition="4643" endWordPosition="4646">ntRank and PhraseRank, generating titles that contain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generat</context>
</contexts>
<marker>Soricut, Marcu, 2007</marker>
<rawString>R. Soricut, and D. Marcu. 2007. Abstractive headline generation using WIDL-expressions. Information Processing and Management, 43(6), pages 1536– 1548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
</authors>
<title>Approximation Strategies for Multi-Structure Sentence Compression.</title>
<date>2014</date>
<booktitle>Proceedings of ACL 2014,</booktitle>
<pages>1241--1251</pages>
<contexts>
<context position="30487" citStr="Thadani, 2014" startWordPosition="4777" endWordPosition="4778">ar, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2</context>
</contexts>
<marker>Thadani, 2014</marker>
<rawString>Kapil Thadani. 2014. Approximation Strategies for Multi-Structure Sentence Compression. Proceedings of ACL 2014, pages 1241–1251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanouil Tzouridis</author>
<author>Jamal Abdul Nasir</author>
<author>Ulf Brefeld</author>
</authors>
<title>Learning to Summarise Related Sentences.</title>
<date>2014</date>
<booktitle>Proceedings of COLING 2014,Dublin,</booktitle>
<pages>1636--1647</pages>
<location>Ireland,</location>
<contexts>
<context position="31181" citStr="Tzouridis et al. (2014)" startWordPosition="4883" endWordPosition="4886">multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is </context>
</contexts>
<marker>Tzouridis, Nasir, Brefeld, 2014</marker>
<rawString>Emmanouil Tzouridis, Jamal Abdul Nasir and Ulf Brefeld. 2014. Learning to Summarise Related Sentences. Proceedings of COLING 2014,Dublin, Ireland, August 23-29 2014. pages 1636–1647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carles Ventura</author>
<author>Xavier Giro-i-Nieto</author>
<author>Veronica Vilaplana</author>
<author>Daniel Giribet</author>
<author>Eusebio Carasusan</author>
</authors>
<title>Automatic keyframe selection based on Mutual Reinforcement Algorithm.</title>
<date>2013</date>
<booktitle>In Proceedings of 11th international workshop on content-based multimedia indexing(CBMI),</booktitle>
<pages>29--34</pages>
<contexts>
<context position="12518" citStr="Ventura et al., 2013" startWordPosition="1865" endWordPosition="1868">epresents a word cluster, and the first occuring word in it can be used as the main word of chain. 3.1.3 Learning Salient Events Intuitively, one word should be more important if it occurs in more important events. Similarly, one event should be more important if it includes more important words. Inspired by this, we construct a bipartite graph between lexical chains and events, shown in Figure 3, and then exploit MRP to jointly learn the salience of lexical chains and events. MRP has been demonstrated effective for jointly learning the vertex weights of a bipartite graph (Zhang et al., 2008; Ventura et al., 2013). Given a text, we construct bipartite graph between the lexical chains and events, with an edge being constructed between a lexical chain and an event if the event contains a word in the lexical chain. Suppose that there are n events {e1, · · · , en} and m lexical chains: {l1, · · · , lm} in the bipartite graph Gbi. Their scores are represented by sal(e) = {sal(e1),···,sal(en)} and sal(l) = {sal(l1), · · · , sal(lm)}, respectively. We compute the final sal(e) and sal(l) iteratively by MRP. At each step, sal(ei) and sal(lj) are computed as follows: sal(ei) a 1:m rij x sal(lj) j=1 n sal(lj) a r</context>
</contexts>
<marker>Ventura, Giro-i-Nieto, Vilaplana, Giribet, Carasusan, 2013</marker>
<rawString>Carles Ventura, Xavier Giro-i-Nieto, Veronica Vilaplana, Daniel Giribet, and Eusebio Carasusan. 2013. Automatic keyframe selection based on Mutual Reinforcement Algorithm. In Proceedings of 11th international workshop on content-based multimedia indexing(CBMI), pages 29–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Generating overview summaries of ongoing email thread discussions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>1384--1394</pages>
<location>Geneva,</location>
<contexts>
<context position="1240" citStr="Wan and McKeown, 2004" startWordPosition="177" endWordPosition="180">active and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. 1 Introduction Headline generation (HG) is a text summarization task, which aims to describe an article (or a set of related paragraphs) using a single short sentence. The task is useful in a number of practical scenarios, such as compressing text for mobile device users (Corston-Oliver, 2001), generating table of contents (Erbs et al., 2013), and email summarization (Wan and McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003</context>
</contexts>
<marker>Wan, McKeown, 2004</marker>
<rawString>Stephen Wan and Kathleen McKeown. 2004. Generating overview summaries of ongoing email thread discussions. In Proceedings of COLING 2004, Geneva, Switzerland, 2004, pages 1384–1394.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lu Wang</author>
</authors>
<location>Hema Raghavan, Vittorio Castelli, Radu</location>
<marker>Wang, </marker>
<rawString>Lu Wang, Hema Raghavan, Vittorio Castelli, Radu</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie Florian</author>
</authors>
<title>A sentence compression based framework to query-focused mutli-document summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<pages>1384--1394</pages>
<location>Sofia, Bulgaria,</location>
<marker>Florian, 2013</marker>
<rawString>Florian, Claire Cardie. 2013. A sentence compression based framework to query-focused mutli-document summarization. In Proceedings of ACL 2013, Sofia, Bulgaria, August 4-9 2013, pages 1384–1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Title generation with quasi-synchronous grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>513--523</pages>
<contexts>
<context position="1565" citStr="Woodsend et al., 2010" startWordPosition="224" endWordPosition="227">rticle (or a set of related paragraphs) using a single short sentence. The task is useful in a number of practical scenarios, such as compressing text for mobile device users (Corston-Oliver, 2001), generating table of contents (Erbs et al., 2013), and email summarization (Wan and McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective </context>
<context position="6567" citStr="Woodsend et al., 2010" startWordPosition="948" endWordPosition="951">of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Abstractive Headline Generation Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extracted from Wikipedia. Woodsend et al. (2010) use supervised models to learn the salience score of each phrase. Here, we use the work of Soricut and Marcu (2007) , namely PhraseRank, as our baseline phrase ranking method, which is an unsupervised model without external resources. The method exploits unsupervised topic discovery to find a set of salient phrases. Headline Generation. In the headline generation step, abstractive models exploit sentence synthesis technologies to accomplish headline generation. Zajic et al. (2</context>
<context position="20533" citStr="Woodsend et al. (2010)" startWordPosition="3229" endWordPosition="3232">events per article in the two datasets. All the pre-processing steps, including POS tagging, lemma analysis, dependency parsing and anaphora resolution, are 7http://duc.nist.gov/duc2004/tasks.html conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from 0 to 5, with 0 being the worst and 5 being the best. The first 50 documents from the test set and their corresponding headlines are selected for human rating. We conduct significant tests using t-test. 4.2 Development Results There are three important parameters in the proposed event-driven </context>
<context position="25949" citStr="Woodsend et al. (2010)" startWordPosition="4117" endWordPosition="4120"> PhraseRank and EventRank to denote their MSC method and our proposed MSC, respectively, applying them, respectively. As shown in Table 1, better performance is achieved by our MSC, demonstrating the effectiveness of our proposed MSC. Similarly, the event-driven model can achieve the best results. We report results of previous state-of-the-art systems as well. SentRank+SSC denotes the result of Erkan and Radev (2004), which uses our SentRank and SSC to obtain the final title. Topiary denotes the result of Zajic et al. (2005), which is an early abstractive model. Woodsend denotes the result of Woodsend et al. (2010), which is an abstractive model using a quasisynchronous grammar to generate a title. As shown in Table 1, MSC is significantly better than SSC, and our event-driven model achieves the best performance, compared with state-of-the-art systems. Following Alfonseca et al. (2013), we conduct human evaluation also. The results are shown in Table 2, by three aspects: informativeness, fluency and coherence. The overall tendency is similar to the results, and the event-driven model achieves the best results. 4.4 Example Outputs We show several representative examples of the proposed event-driven model</context>
<context position="29635" citStr="Woodsend et al., 2010" startWordPosition="4647" endWordPosition="4650">nerating titles that contain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single </context>
</contexts>
<marker>Woodsend, Feng, Lapata, 2010</marker>
<rawString>Kristian Woodsend, Yansong Feng and Mirella Lapata. 2010. Title generation with quasi-synchronous grammar. In Proceedings of EMNLP 2010, pages 513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songhua Xu</author>
<author>Shaohui Yang</author>
<author>Francis C M Lau</author>
</authors>
<title>Keyword extraction and headline generation using novel work features.</title>
<date>2010</date>
<booktitle>In Proceedings of AAAI</booktitle>
<pages>1461--1466</pages>
<contexts>
<context position="2108" citStr="Xu et al., 2010" startWordPosition="303" endWordPosition="306">ategories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated titles, given that</context>
<context position="6543" citStr="Xu et al., 2010" startWordPosition="944" endWordPosition="947">ce by making use of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Abstractive Headline Generation Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extracted from Wikipedia. Woodsend et al. (2010) use supervised models to learn the salience score of each phrase. Here, we use the work of Soricut and Marcu (2007) , namely PhraseRank, as our baseline phrase ranking method, which is an unsupervised model without external resources. The method exploits unsupervised topic discovery to find a set of salient phrases. Headline Generation. In the headline generation step, abstractive models exploit sentence synthesis technologies to accomplish headline gen</context>
<context position="20441" citStr="Xu et al. (2010)" startWordPosition="3214" endWordPosition="3217"> use the first 100 articles from DUC–07 as our development set. There are averaged 40 events per article in the two datasets. All the pre-processing steps, including POS tagging, lemma analysis, dependency parsing and anaphora resolution, are 7http://duc.nist.gov/duc2004/tasks.html conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from 0 to 5, with 0 being the worst and 5 being the best. The first 50 documents from the test set and their corresponding headlines are selected for human rating. We conduct significant tests using t-test</context>
<context position="29653" citStr="Xu et al., 2010" startWordPosition="4651" endWordPosition="4654">ntain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an </context>
</contexts>
<marker>Xu, Yang, Lau, 2010</marker>
<rawString>Songhua Xu, Shaohui Yang and Francis C.M. Lau. 2010. Keyword extraction and headline generation using novel work features. In Proceedings of AAAI 2010, pages 1461–1466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Ryu Iida</author>
</authors>
<title>Sentence Compression with Semantic Role Constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>349--353</pages>
<contexts>
<context position="30435" citStr="Yoshikawa and Iida, 2012" startWordPosition="4765" endWordPosition="4768">ses, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path </context>
</contexts>
<marker>Yoshikawa, Iida, 2012</marker>
<rawString>Katsumasa Yoshikawa and Ryu Iida. 2012. Sentence Compression with Semantic Role Constraints. In Proceedings of ACL 2012, pages 349–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Headline generation for written and broadcast news.</title>
<date>2005</date>
<pages>120--4698</pages>
<contexts>
<context position="1889" citStr="Zajic et al., 2005" startWordPosition="271" endWordPosition="274">not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficul</context>
<context position="5358" citStr="Zajic et al., 2005" startWordPosition="774" endWordPosition="777">ration methods outperform competitive baseline methods, and our model achieves the best results compared with previous state-of-the-art systems. 2 Background Previous extractive and abstractive models take two main steps, namely candidate extraction and headline generation. Here, we introduce these two types of models according to the two steps. 2.1 Extractive Headline Generation Candidate Extraction. Extractive models exploit sentences as the basic processing units in this step. Sentences are ranked by their salience according to specific strategies (Dorr et al., 2003; Erkan and Radev, 2004; Zajic et al., 2005). One of the stateof-the-art approaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline sentence ranking method. In this paper, we use SentRank to denote this method. Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to compress a sentence by making use of handcrafte</context>
<context position="7171" citStr="Zajic et al. (2005)" startWordPosition="1038" endWordPosition="1041">d et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extracted from Wikipedia. Woodsend et al. (2010) use supervised models to learn the salience score of each phrase. Here, we use the work of Soricut and Marcu (2007) , namely PhraseRank, as our baseline phrase ranking method, which is an unsupervised model without external resources. The method exploits unsupervised topic discovery to find a set of salient phrases. Headline Generation. In the headline generation step, abstractive models exploit sentence synthesis technologies to accomplish headline generation. Zajic et al. (2005) exploit unsupervised topic discovery to find key phrases, and use the Hedge Trimmer algorithm to compress candidate sentences. One or more key phrases are added into the compressed fragment according to the length of the headline. Soricut and Marcu (2007) employ WIDL-expressions to generate headlines. Xu et al. (2010) employ keyword clustering based on several bag-of-words models to construct a headline. Woodsend et al. (2010) use quasi-synchronous grammar (QG) to optimize phrase selection and surface realization preferences jointly. 463 3 Our Model Similar to extractive and abstractive model</context>
<context position="25857" citStr="Zajic et al. (2005)" startWordPosition="4102" endWordPosition="4105">he mark ‡ denotes the result is significantly better with a p-value below 0.01. SentRank, PhraseRank and EventRank to denote their MSC method and our proposed MSC, respectively, applying them, respectively. As shown in Table 1, better performance is achieved by our MSC, demonstrating the effectiveness of our proposed MSC. Similarly, the event-driven model can achieve the best results. We report results of previous state-of-the-art systems as well. SentRank+SSC denotes the result of Erkan and Radev (2004), which uses our SentRank and SSC to obtain the final title. Topiary denotes the result of Zajic et al. (2005), which is an early abstractive model. Woodsend denotes the result of Woodsend et al. (2010), which is an abstractive model using a quasisynchronous grammar to generate a title. As shown in Table 1, MSC is significantly better than SSC, and our event-driven model achieves the best performance, compared with state-of-the-art systems. Following Alfonseca et al. (2013), we conduct human evaluation also. The results are shown in Table 2, by three aspects: informativeness, fluency and coherence. The overall tendency is similar to the results, and the event-driven model achieves the best results. 4.</context>
<context position="29587" citStr="Zajic et al., 2005" startWordPosition="4639" endWordPosition="4642">dvantages of both SentRank and PhraseRank, generating titles that contain more important events with complete structures. The observation verifies our hypothesis in the introduction — that extractive models have the problem of low information coverage, and 469 abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training</context>
</contexts>
<marker>Zajic, Dorr, Schwartz, 2005</marker>
<rawString>David Zajic, Bonnie Dorr and Richard Schwartz. 2005. Headline generation for written and broadcast news. lamp-tr-120, cs-tr-4698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>113--120</pages>
<contexts>
<context position="9123" citStr="Zha, 2002" startWordPosition="1332" endWordPosition="1333">s sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is performed on a bipartite graph, where the two types of nodes are lexical chains (Section 3.1.2) and events (Section 3.1.1), respectively. Mutual Reinforcement Principle (Zha, 2002) is applied to jointly learn chain and event salience on the bipartite graph for a given input. We obtain the top-k candidate events by their salience measures. 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities and relations (Fader et al., 2011). We follow Hu’s work (Hu et al., 2013) to extract events. Given a text, we first use the Stanford dependency parser1 to obtain the Stanford typed dependency structures of the se</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforement principle and sentence clustering. In Proceedings of SIGIR 2002, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
</authors>
<title>Xipeng Qiu, Xuanjing Huang, Wu Lide.</title>
<date>2008</date>
<journal>Acta Automatica Sinica,</journal>
<volume>34</volume>
<issue>10</issue>
<pages>1257--1261</pages>
<marker>Zhang, 2008</marker>
<rawString>Qi Zhang, Xipeng Qiu, Xuanjing Huang, Wu Lide. 2008. Learning semantic lexicons using graph mutual reinforcement based bootstrapping. Acta Automatica Sinica, 34(10), pages 1257–1261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic Processing Using the Generalized Perceptron and Beam Search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>105--150</pages>
<contexts>
<context position="18108" citStr="Zhang and Clark, 2011" startWordPosition="2836" endWordPosition="2839">sum edge score of the path, we exploit a trigram language model to compute a fluency score of the path. Language models have been commonly used to generate more readable titles. The overall score of a path is compute by: score(p) = edge(p) + A x flu(p) E EijEp ln{w(Eij)} edge(p) = n flu(p) = Ei ln{p(wi|wi−2wi−1)} n where p is a candidate path and the corresponding word sequence of p is w1 · · · wn. A trigram language model is trained using SRILM6 on English Gigaword (LDC2011T07). 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4) when using a trigram language model, which is time consuming. 6http://www.speech.sri.com/projects/srilm/ (S) King Norodom ... Hun ... rejected ... on Sun opposition ... for party ... talks ... groups ... ... (E) (5) 466 Input: G +— (V, £), LM, B Output: best candidates +— { {(S)} } loop do beam +— { } for each candidate in candidates if candidate endwith (E) ADDTOBEAM(beam, candidate) continue for each Vi in V candidate +— ADDVERTEX(candidate, Vi) COMPUTESCOR</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang, Stephen Clark. 2011. Syntactic Processing Using the Generalized Perceptron and Beam Search. Computational Linguistics, 37(1), pages 105–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
</authors>
<title>Partial-Tree Linearization: Generalized Word Ordering for Text Synthesis.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCAI 2013,</booktitle>
<pages>2232--2238</pages>
<contexts>
<context position="2828" citStr="Zhang, 2013" startWordPosition="406" endWordPosition="407"> generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated titles, given that sentence synthesis is still very inaccurate based on a set of phrases with little grammatical information (Zhang, 2013). In this paper, we propose an event-driven model for headline generation, which alleviates the Candidate #1 ... Candidate #i ... Candidate #K Phrases Events Sentences Multi-Sentence Compression Candidate Ranking Headline Texts Candidate Extraction Headline Generation 462 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 462–472, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics disadvantages of both extractive and abstractive HG. The framework</context>
</contexts>
<marker>Zhang, 2013</marker>
<rawString>Yue Zhang. 2013. Partial-Tree Linearization: Generalized Word Ordering for Text Synthesis. In Proceedings of IJCAI 2013, pages 2232–2238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>