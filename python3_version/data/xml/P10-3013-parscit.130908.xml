<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000232">
<title confidence="0.989953">
Automatic Selectional Preference Acquisition for Latin verbs
</title>
<author confidence="0.996321">
Barbara McGillivray
</author>
<affiliation confidence="0.7574785">
University of Pisa
Italy
</affiliation>
<email confidence="0.974621">
b.mcgillivray@ling.unipi.it
</email>
<sectionHeader confidence="0.993038" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999832222222222">
We present a system that automatically
induces Selectional Preferences (SPs) for
Latin verbs from two treebanks by using
Latin WordNet. Our method overcomes
some of the problems connected with data
sparseness and the small size of the input
corpora. We also suggest a way to evalu-
ate the acquired SPs on unseen events ex-
tracted from other Latin corpora.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903782608696">
Automatic acquisition of semantic information
from corpora is a challenge for research on low-
resourced languages, especially when semanti-
cally annotated corpora are not available. Latin is
definitely a high-resourced language for what con-
cerns the number of available texts and traditional
lexical resources such as dictionaries. Neverthe-
less, it is a low-resourced language from a compu-
tational point of view (McGillivray et al., 2009).
As far as NLP tools for Latin are concerned,
parsing experiments with machine learning tech-
niques are ongoing (Bamman and Crane, 2008;
Passarotti and Ruffolo, forthcoming), although
more work is still needed in this direction, espe-
cially given the small size of the training data.
As a matter of fact, only three syntactically an-
notated Latin corpora are available (and still in
progress): the Latin Dependency Treebank (LDT,
53,000 tokens) for classical Latin (Bamman and
Crane, 2006), the Index Thomisticus Treebank (IT-
TB, 54,000 tokens) for Thomas Aquinas’s works
(Passarotti, 2007), and the PROIEL treebank (ap-
proximately 100,000 tokens) for the Bible (Haug
and Jøndal, 2008). In addition, a Latin version
of WordNet – Latin WordNet (LWN; Minozzi,
(2009) – is being compiled, consisting of around
10,000 lemmas inserted in the multilingual struc-
ture of MultiWordNet (Bentivogli et al., 2004).
The number and the size of these resources are
small when compared with the corpora and the
lexicons for modern languages, e. g. English.
Concerning semantic processing, no seman-
tically annotated Latin corpus is available yet;
building such a corpus manually would take con-
siderable time and energy. Hence, research in
computational semantics for Latin would benefit
from exploiting the existing resources and tools
through automatic lexical acquisition methods.
In this paper we deal with automatic acquisition
of verbal selectional preferences (SPs) for Latin,
i. e. the semantic preferences of verbs on their ar-
guments: e. g. we expect the object position of the
verb edo ‘eat’ to be mostly filled by nouns from the
food domain. For this task, we propose a method
inspired by Alishahi (2008) and outlined in an ear-
lier version on the IT-TB in McGillivray (2009).
SPs are defined as probability distributions over
semantic features extracted as sets of LWN nodes.
The input data are two subcategorization lexicons
automatically extracted from the LDT and the IT-
TB (McGillivray and Passarotti, 2009).
Our main contribution is to create a new tool for
semantic processing of Latin by adapting compu-
tational techniques developed for extant languages
to the special case of Latin. A successful adapta-
tion is contingent on overcoming corpus size dif-
ferences. The way our model combines the syntac-
tic information contained in the treebanks with the
lexical semantic knowledge from LWN allows us
to overcome some of the difficulties related to the
small size of the input corpora. This is the main
difference from corpora for modern languages, to-
gether with the absence of semantic annotation.
Moreover, we face the problem of evaluating our
system’s ability to generalize over unseen cases by
using text occurrences, as access to human linguis-
tic judgements is denied for Latin.
In the rest of the paper we will briefly summa-
rize previous work on SP acquisition and motivate
</bodyText>
<page confidence="0.989196">
73
</page>
<note confidence="0.602571">
Proceedings of the ACL 2010 Student Research Workshop, pages 73–78,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.993438224137931">
our approach (section 2); we will then describe our
system (section 3), report on first results and evalu-
ation (section 4), and finally conclude by suggest-
ing future directions of research (section 5).
2 Background and motivation
The state-of-the-art systems for automatic acqui-
sition of verbal SPs collect argument headwords
from a corpus (for example, apple, meat, salad as
objects of eat) and then generalize the observed
behaviour over unseen cases, either in the form of
words (how likely is it to find sausage in the object
position of eat?) or word classes (how likely is it
to find VEGETABLE, FOOD, etc?).
WN-based approaches translate the generaliza-
tion problem into estimating preference probabil-
ities over a noun hierarchy and solve it by means
of different statistical tools that use the input data
as a training set: cf. inter al. Resnik (1993), Li
and Abe (1998), Clark and Weir (1999). Agirre
and Martinez (2001) acquire SPs for verb classes
instead of single verb lemmas by using a semanti-
cally annotated corpus and WN.
Distributional methods aim at automatically in-
ducing semantic classes from distributional data in
corpora by means of various similarity measures
and unsupervised clustering algorithms: cf. e. g.
Rooth et al. (1999) and Erk (2007). Bamman and
Crane (2008) is the only distributional approach
dealing with Latin. They use an automatically
parsed corpus of 3.5 million words, then calculate
SPs with the log-likelihood test, and obtain an as-
sociation score for each (verb, noun) pair.
The main difference between these previous
systems and our case is the size of the input cor-
pus. In fact, our dataset consists of subcatego-
rization frames extracted from two relatively small
treebanks, amounting to a little over 100,000 word
tokens overall. This results in a large number of
low-frequency (verb, noun) associations, which
may not reflect the actual distributions of Latin
verbs. This state improves if we group the obser-
vations into clusters. Such a method, proposed by
Alishahi (2008), proved effective in our case.
The originality of this approach is an incre-
mental clustering algorithm for verb occurrences
called frames which are identified by specific syn-
tactic and semantic features, such as the number
of verbal arguments, the syntactic pattern, and the
semantic properties of each argument, i. e. the
WN hypernyms of the argument’s fillers. Based
on a probabilistic measure of similarity between
the frames’ features, the clustering produces larger
sets called constructions. The constructions for a
verb contribute to the next step, which acquires
the verb’s SPs as semantic profiles, i. e. probabil-
ity distributions over the semantic properties. The
model exploits the structure of WN so that predic-
tions over unseen cases are possible.
</bodyText>
<sectionHeader confidence="0.996203" genericHeader="method">
3 The model
</sectionHeader>
<bodyText confidence="0.999951090909091">
The input data are two corpus-driven subcate-
gorization lexicons which record the subcatego-
rization frames of each verbal token occurring
in the corpora: these frames contain morpho-
syntactic information on the verb’s arguments, as
well as their lexical fillers. For example, ‘eo
+ A (in)Obj[acc]{exsilium}’ represents an active
occurrence of the verb eo ‘go’ with a prepositional
phrase introduced by the preposition in ‘to, into’
and composed by an accusative noun phrase filled
by the lemma exsilium ‘exile’, as in the sentence1
</bodyText>
<equation confidence="0.83519">
(1) eat in exsilium
</equation>
<bodyText confidence="0.978119416666667">
go:SBJV.PRS.3SG to exile:ACC.N.SG
‘he goes into exile’.
We illustrate how we adapted Alishahi’s defini-
tions of frame features and formulae to our case.
Alishahi uses a semantically annotated English
corpus, so she defines the verb’s semantic prim-
itives, the arguments’ participant roles and their
semantic categories; since we do not have such an-
notation, we used the WN semantic information.
The syntactic feature of a frame (ft1) is the
set of syntactic slots of its verb’s subcategoriza-
tion pattern, extracted from the lexicons. In the
above example, ‘A (in)Obj[acc]’. In addition, the
first type of semantic features of a frame (ft2)
collects the semantic properties of the verb’s ar-
guments as the set of LWN synonyms and hy-
pernyms of their fillers. In the previous exam-
ple this is {exsilium ‘exile’, proscriptio ‘proscrip-
tion’, rejection, actio, actus ‘act’}.2 The second
type of semantic features of a frame (ft3) col-
lects the semantic properties of the verb in the
form of the verb’s synsets. In the above example,
these are all synsets of eo ‘go’, among which ‘{eo,
gradior, grassor, ingredior, procedo, prodeo,
</bodyText>
<footnote confidence="0.9156886">
1Cicero, In Catilinam, II, 7.
2We listed the LWN node of the lemma exsilium, followed
by its hypernyms; each node – apart from rejection, which
is English and is not filled by a Latin lemma in LWN – is
translated by the corresponding node in the English WN.
</footnote>
<page confidence="0.998445">
74
</page>
<bodyText confidence="0.997994333333333">
vado}’ (‘{progress, come on, come along, ad-
vance, get on, get along, shape up}’ in the En-
glish WN).
</bodyText>
<subsectionHeader confidence="0.999965">
3.1 Clustering of frames
</subsectionHeader>
<bodyText confidence="0.99979475">
The constructions are incrementally built as new
frames are included in them; a new frame F is as-
signed to a construction K if F probabilistically
shares some features with the frames in K so that
</bodyText>
<equation confidence="0.992319">
K = arg max P(k|F) = arg maxP(k)P(F|k),
k k
</equation>
<bodyText confidence="0.9990552">
where k ranges over the set of all constructions,
including the baseline k0 = {F}. The prior
probability P(k) is calculated from the number of
frames contained in k divided by the total number
of frames. Assuming that the frame features are
independent, the posterior probability P(F|k) is
the product of three probabilities, each one corre-
sponding to the probability that a feature displays
in k the same value it displays in F: Pi(fti(F)|k)
for i = 1, 2,3:
</bodyText>
<equation confidence="0.999097">
P(F|k) = Y Pi(fti(F)|k)
i=1,2,3
</equation>
<bodyText confidence="0.9999974">
We estimated the probability of a match be-
tween the value of ft1 in k and the value of ft1
in F as the sum of the syntactic scores between
F and each frame h contained in k, divided the
number nk of frames in k:
</bodyText>
<equation confidence="0.9153715">
P(ft1(F) |k) = Ph∈k synt score(h, F)
nk
</equation>
<bodyText confidence="0.97358">
where the syntactic score synt score(h, F) =
|SCS(h)∩SCS(F)|
|SCS(F ) |calculates the number of syntac-
tic slots shared by h and F over the number of
slots in F. P(ft1(F)|k) is 1 when all the frames
in k contain all the syntactic slots of F.
For each argument position a, we estimated the
probability P(ft2(F)|k) as the sum of the seman-
tic scores between F and each h in k:
</bodyText>
<equation confidence="0.926754">
P(ft2(F)  |k) = Ph∈k sem score(h, F)
nk
</equation>
<bodyText confidence="0.882191666666667">
where the semantic score sem score(h, F) =
|S(h)∩S(F)|
|S(F ) |counts the overlap between the seman-
tic properties S(h) of h (i.e. the LWN hyper-
nyms of the fillers in h) and the semantic prop-
erties S(F) of F (for argument a), over |S(F)|.
</bodyText>
<equation confidence="0.8687265">
P(ft3(F)|k) = Ph∈k syns score(h, F)
nk
</equation>
<bodyText confidence="0.9134185">
where the synset score syns score(h, F) =
|Synsets(verb(h))∩Synsets(verb(F)) |calculates the
|Synsets(verb(F))|
overlap between the synsets for the verb in h and
the synsets for the verb in F over the number of
synsets for the verb in F.3
We introduced the syntactic and synset scores in
order to account for a frequent phenomenon in our
data: the partial matches between the values of the
features in F and in k.
</bodyText>
<subsectionHeader confidence="0.999549">
3.2 Selectional preferences
</subsectionHeader>
<bodyText confidence="0.999962">
The clustering algorithm defines the set of con-
structions in which the generalization step over
unseen cases is performed. SPs are defined as
semantic profiles, that is, probability distributions
over the semantic properties, i. e. LWN nodes. For
example, we get the probability of the node actio
‘act’ in the position ‘A (in)Obj[acc]’ for eo ‘go’.
If s is a semantic property and a an argument
position for a verb v, the semantic profile Pa(s|v)
is the sum of Pa(s, k|v) over all constructions k
containing v or a WN-synonym of v, i. e. a verb
contained in one or more synsets for v. Pa(s, k|v)
</bodyText>
<equation confidence="0.98167275">
is approximated as P(k,v)Pa(s|k,v)
P(v) , where P(k, v)
is estimated as nk · f req(k,v)
Pk&apos; nk&apos;·freq(k&apos;,v)
</equation>
<bodyText confidence="0.996996">
To estimate Pa(s|k, v) we consider each frame
h in k and account for: a) the similarity between v
and the verb in h; b) the similarity between s and
the fillers of h. This is achieved by calculating a
similarity score between h, v, a and s, defined as:
</bodyText>
<equation confidence="0.9774575">
Pf |s ∩ S(f)|
syns score(v,V (h)) · Nfil(h, a) (1)
</equation>
<bodyText confidence="0.999797714285714">
where V (h) in (1) contains the verbs of h,
Nfil(h, a) counts the a-fillers in h, f ranges in the
set of a-fillers in h, S(f) contains the semantic
properties for f and |s∩S(f) |is 1 when s appears
in S(f) and 0 otherwise.
Pa(s|k, v) is thus obtained by normalizing the
sum of these similarity scores over all frames in
k, divided by the total number of frames in k con-
taining v or its synonyms.
The similarity scores weight the contributions
of the synonyms of v, whose fillers play a role in
the generalization step. This is our innovation with
respect to Alishahi (2008)’s system. It was intro-
duced because of the sparseness of our data, where
</bodyText>
<footnote confidence="0.973693333333333">
3The algorithm uses smoothed versions of all the previous
formulae by adding a very small constant so that the proba-
bilities are never 0.
</footnote>
<page confidence="0.963077">
75
</page>
<equation confidence="0.907220466666667">
k h
1 induco+P Sb[acc]{forma}
introduco + P Sb{PR}
introduco + P Sb{forma}
addo+P Sb{praesidium}
2 induco+A Obj[acc]{forma}
immitto+A Obj[acc]{PR},Obj[dat]{antrum}
introduco + A Obj[acc]{NP}
3 introduco + A (in)Obj[acc]{finis},Obj[acc]{copia},Sb{NP}
induco+A (in)Obj[acc]{effectus},Obj[acc]{forma}
4 introduco + A Obj[acc]{forma}
induco+A Obj[acc]{perfectio},Sb[nom]{PR}
induco+A Obj[acc]{forma}n
5 immitto+A Obj[acc]{PR},Obj[dat]{antrum}
introduco + A Obj[acc]{NP}
</equation>
<tableCaption confidence="0.974611">
Table 1: Constructions (k) for the frames (h) con-
</tableCaption>
<bodyText confidence="0.618400666666667">
taining the verb introduco ‘bring in’.
many verbs are hapaxes, which makes the gener-
alization from their fillers difficult.
</bodyText>
<sectionHeader confidence="0.998614" genericHeader="evaluation">
4 Results and evaluation
</sectionHeader>
<bodyText confidence="0.999820147058823">
The clustering algorithm was run on 15509 frames
and it generated 7105 constructions. Table 1 dis-
plays the 5 constructions assigned to the 9 frames
where the verb introduco ‘bring in, introduce’ oc-
curs. Note the semantic similarity between addo
‘add to, bring to’, immitto ‘send against, insert’,
induco ‘bring forward, introduce’ and introduco,
and the similarity between the syntactic patterns
and the argument fillers within the same construc-
tion. For example, finis ‘end, borders’ and ef-
fectus ‘result’ share the semantic properties AT-
TRIBUTE, COGNITIO ‘cognition’, CONSCIENTIA
‘conscience’, EVENTUM ‘event’, among others.
The vast majority of constructions contain less
than 4 frames. This contrasts with the more gen-
eral constructions found by Alishahi (2008) and
can be explained by several factors. First, the cov-
erage of LWN is quite low with respect to the
fillers in our dataset. In fact, 782 fillers out of
2408 could not be assigned to any LWN synset;
for these lemmas the semantic scores with all the
other nouns are 0, causing probabilities lower than
the baseline; this results in assigning the frame to
the singleton construction consisting of the frame
itself. The same happens for fillers consisting of
verbal lemmas, participles, pronouns and named
entities, which amount to a third of the total num-
ber. Furthermore, the data are not tagged by sense
and the system deals with noun ambiguity by list-
ing together all synsets of a word n (and their hy-
pernyms) to form the semantic properties for n:
consequently, each sense contributes to the seman-
tic description of n in relation to the number of
hypernyms it carries, rather than to its observed
</bodyText>
<table confidence="0.9109025">
semantic property probability
actio ‘act’ 0.0089
actus ‘act’ 0.0089
pars ‘part’ 0.0089
object 0.0088
physical object 0.0088
instrumentality 0.0088
instrumentation 0.0088
location 0.0088
populus ‘people’ 0.0088
plaga ‘region’ 0.0088
regio ‘region’ 0.0088
arvum ‘area’ 0.0088
orbis ‘area’ 0.0088
external body part ‘ 0.0088
nympha ‘nymph’, ‘water’ 0.0088
latex ‘water’ 0.0088
lympha ‘water’ 0.0088
intercapedo ‘gap, break’ 0.0088
orificium ‘opening’ 0.0088
</table>
<tableCaption confidence="0.8697565">
Table 2: Top 20 semantic properties in the seman-
tic profile for ascendo ‘ascend’ + A (de)Obj[abl].
</tableCaption>
<bodyText confidence="0.999758914285714">
frequency. Finally, a common problem in SP ac-
quisition systems is the noise in the data, including
tagging and metaphorical usages. This problem
is even greater in our case, where the small size
of the data underestimates the variance and there-
fore overestimates the contribution of noisy obser-
vations. Metaphorical and abstract usages are es-
pecially frequent in the data from the IT-TB, due
to the philosophical domain of the texts.
As to the SP acquisition, we ran the system
on all constructions generated by the clustering.
We excluded the pronouns occurring as argument
fillers, and manually tagged the named entities.
For each verb lemma and slot we obtained a proba-
bility distribution over the 6608 LWN noun nodes.
Table 2 displays the 20 semantic properties
with the highest SP probabilities as ablative argu-
ments of ascendo ‘ascend’ introduced by de ‘down
from’, ‘out of’. This semantic profile was cre-
ated from the following fillers for the verbs con-
tained in the constructions for ascendo and its
synonyms: abyssus ‘abyss’, fumus ‘smoke’, lacus
‘lake’, machina ‘machine’, manus ‘hand’, negoti-
atio ‘business’, mare ‘sea’, os ‘mouth’, templum
‘temple’, terra ‘land’. These nouns are well repre-
sented by the semantic properties related to water
and physical places. Note also the high rank of
general properties like actio ‘act’, which are asso-
ciated to a large number of fillers and thus gener-
ally get a high probability.
Regarding evaluation, we are interested in test-
ing two properties of our model: calibration
and discrimination. Calibration is related to the
model’s ability to distinguish between high and
low probabilities. We verify that our model is
</bodyText>
<page confidence="0.962452">
76
</page>
<bodyText confidence="0.999515333333333">
adequately calibrated, since its SP distribution is
always very skewed (cf. figure 1). Therefore,
the model is able to assign a high probability to
a small set of nouns (preferred nouns) and a low
probability to a large set of nouns (the rest), thus
performing better than the baseline model, defined
as the model that assigns the uniform distribution
over all nouns (4724 LWN leaf nodes). Moreover,
our model’s entropy is always lower than the base-
line: 12.2 vs. the 6.9-11.3 range; by the maximum
entropy principle, this confirms that the system
uses some information for estimating the proba-
bilities: LWN structure, co-occurrence frequency,
syntactic patterns. However, we have no guaran-
tee that the model uses this information sensibly.
For this, we test the system’s discrimination po-
tential, i. e. its ability to correctly estimate the SP
probability of each single LWN node.
</bodyText>
<table confidence="0.9945615625">
noun SP probability
pars ‘part’ 0.0029
locus ‘place’ 0.0026
forma ‘form’ 0.0023
ratio ‘account’‘reason’, ‘opinion’ 0.0023
respectus ‘consideration’ 0.0022
caput ‘head’, ‘origin’ 0.0022
anima ‘soul’ 0.0021
animus ‘soul’, ‘spirit’ 0.0020
figura ‘form’, ‘figure’ 0.0020
spiritus ‘spirit’ 0.0020
causa cause’ ‘ 0.0020
corpus ‘body’ 0.0019
sententia ‘judgement’ 0.0019
finitio ‘limit’, ‘definition’ 0.0019
species ‘sight’, ‘appearance’ 0.0019
</table>
<tableCaption confidence="0.9887625">
Table 3: 15 nouns with the highest probabilities as
accusative objects of dico ‘say’.
</tableCaption>
<figureCaption confidence="0.930491">
Figure 1: Decreasing SP probabilities of the LWN
</figureCaption>
<bodyText confidence="0.990972787234043">
leaf nodes for the objects of dico ‘say’.
Table 3 displays the 15 nouns with the highest
probabilities as direct objects for dico ‘say’. From
table 3 – and the rest of the distribution, repre-
sented in figure 1 – we see that the model assigns
a high probability to most seen fillers for dico in
the corpus: anima ‘soul’, corpus ‘body’, locus
‘place’, pars ‘part’, etc.
For what concerns evaluating the SP probabil-
ity assigned to nouns unseen in the training set,
Alishahi (2008) follows the approach suggested
by Resnik (1993), using human plausibility judge-
ments on verb-noun pairs. Given the absence of
native speakers of Latin, we used random occur-
rences in corpora, considered as positive examples
of plausible argument fillers; on the other hand, we
cannot extract non-plausible fillers from a corpus
unless we use a frequency-based criterion. How-
ever, we can measure how well our system predicts
the probability of these unseen events.
As a preliminary evaluation experiment, we
randomly selected from our corpora a list of 19
high-frequency verbs (freq.&gt;51) and 7 medium-
frequency verbs (11&lt;freq.&lt;50), for each of which
we chose an interesting argument slot. Then we
randomly extracted one filler for each such pair
from two collections of Latin texts (Perseus Dig-
ital Library and Corpus Thomisticum), provided
that it was not in the training set. The semantic
score in equation 1 on page 3 is then calculated
between the set of semantic properties of n and
that for f, to obtain the probability of finding the
random filler n as an argument for a verb v.
For each of the 26 (verb, slot) pairs, we looked
at three measures of central tendency: mean, me-
dian and the value of the third quantile, which
were compared with the probability assigned by
the model to the random filler. If this probabil-
ity was higher than the measure, the outcome was
considered a success. The successes were 22 for
the mean, 25 for the median and 19 for the third
quartile.4 For all three measures a binomial test
found the success rate to be statistically significant
at the 5% level. For example, table 3 and figure
1 show that the filler for dico+A Obj[acc] in the
evaluation set – sententia ‘judgement’ – is ranked
13th within the verb’s semantic profile.
</bodyText>
<sectionHeader confidence="0.97417" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9439057">
We proposed a method for automatically acquiring
probabilistic SP for Latin verbs from a small cor-
pus using the WN hierarchy; we suggested some
4The dataset consists of all LWN leaf nodes n, for which
we calculated Pa(n|v). By definition, if we divide the dataset
in four equal-sized parts (quartiles), 25% of the leaf nodes
have a probability higher than the value at the third quartile.
Therefore, in 19 cases out of 26 the random fillers are placed
in the high-probability quarter of the plot, which is a good
result, since this is where the preferred arguments gather.
</bodyText>
<page confidence="0.99511">
77
</page>
<bodyText confidence="0.999965666666667">
new strategies for tackling the data sparseness in
the crucial generalization step over unseen cases.
Our work also contributes to the state of the art in
semantic processing of Latin by integrating syn-
tactic information from annotated corpora with the
lexical resource LWN. This demonstrates the use-
fulness of the method for small corpora and the
relevance of computational approaches for histor-
ical linguistics.
In order to measure the impact of the frame
clusters for the SP acquisition, we plan to run the
system for SP acquisition without performing the
clustering step, thus defining all constructions as
singleton sets containing one frame each. Finally,
an extensive evaluation will require a more com-
prehensive set, composed of a higher number of
unseen argument fillers; from the frequencies of
these nouns, it will be possible to directly compare
plausible arguments (high frequency) and implau-
sible ones (low frequency). For this, a larger auto-
matically parsed corpus will be necessary.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998675">
We wish to thank Afra Alishahi, Stefano Minozzi
and three anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999731610389611">
E. Agirre and D. Martinez. 2001. Learning class-to-
class selectional preferences. In Proceedings of the
ACL/EACL 2001 Workshop on Computational Nat-
ural Language Learning (CoNLL-2001), pages 1–8.
A. Alishahi. 2008. A probabilistic model of early ar-
gument structure acquisition. Ph.D. thesis, Depart-
ment of Computer Science, University of Toronto.
D. Bamman and G. Crane. 2006. The design and use
of a Latin dependency treebank. In Proceedings of
the Fifth International Workshop on Treebanks and
Linguistic Theories, pages 67–78. ´UFAL MFF UK.
D. Bamman and G. Crane. 2008. Building a dynamic
lexicon from a digital library. In Proceedings of the
8th ACM/IEEE-CS Joint Conference on Digital Li-
braries, pages 11–20.
L. Bentivogli, P. Forner, and and Pianta E. Magnini,
B. 2004. Revising wordnet domains hierarchy: Se-
mantics, coverage, and balancing. In Proceedings of
COLING Workshop on Multilingual Linguistic Re-
sources, pages 101–108.
S. Clark and D. Weir. 1999. An iterative approach
to estimating frequencies over a semantic hierarchy.
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora. University of Maryland,
pages 258–265.
K. Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 216–223.
D. T. T. Haug and M. L. Jøndal. 2008. Creating a par-
allel treebank of the old Indo-European Bible trans-
lations. In Proceedings of Language Technologies
for Cultural Heritage Workshop, pages 27–34.
H. Li and N. Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217–244.
B. McGillivray and M. Passarotti. 2009. The devel-
opment of the Index Thomisticus Treebank Valency
Lexicon. In Proceedings of the Workshop on Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education,
pages 33–40.
B. McGillivray, M. Passarotti, and P. Ruffolo. 2009.
The Index Thomisticus treebank project: Annota-
tion, parsing and valency lexicon. TAL, 50(2):103–
127.
B. McGillivray. 2009. Selectional Preferences from
a Latin treebank. In Przepi´orkowski A. Passarotti,
M., S. Raynaud, and F. van Eynde, editors, Proceed-
ings of the Eigth International Workshop on Tree-
banks and Linguistic Theories (TLT8), pages 131–
136. EDUCatt.
S. Minozzi. 2009. The Latin Wordnet project.
In P. Anreiter and M. Kienpointner, editors, Pro-
ceedings of the 15th International Colloquium on
Latin Linguistics (ICLL), Innsbrucker Beitraege zur
Sprachwissenschaft.
M. Passarotti and P. Ruffolo. forthcoming. Parsing
the Index Thomisticus Treebank. some preliminary
results. In P. Anreiter and M. Kienpointner, edi-
tors, Proceedings of the 15th International Collo-
quium on Latin Linguistics, Innsbrucker Beitr¨age
zur Sprachwissenschaft.
M. Passarotti. 2007. Verso il Lessico Tomistico Bi-
culturale. La treebank dell’Index Thomisticus. In
R. Petrilli and D. Femia, editors, Atti del XIII Con-
gresso Nazionale della Societ`a di Filosofia del Lin-
guaggio, pages 187–205.
P. Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D.
thesis, University of Pennsylvania.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and
F. Beil. 1999. Inducing a semantically annotated
lexicon via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics, pages 104–111.
</reference>
<page confidence="0.998823">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726472">
<title confidence="0.999028">Automatic Selectional Preference Acquisition for Latin verbs</title>
<author confidence="0.999958">Barbara McGillivray</author>
<affiliation confidence="0.999965">University of Pisa</affiliation>
<address confidence="0.990864">Italy</address>
<email confidence="0.998982">b.mcgillivray@ling.unipi.it</email>
<abstract confidence="0.9732492">We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from two treebanks by using Latin WordNet. Our method overcomes some of the problems connected with data sparseness and the small size of the input corpora. We also suggest a way to evaluate the acquired SPs on unseen events extracted from other Latin corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
</authors>
<title>Learning class-toclass selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural Language Learning (CoNLL-2001),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4896" citStr="Agirre and Martinez (2001)" startWordPosition="771" endWordPosition="774">llect argument headwords from a corpus (for example, apple, meat, salad as objects of eat) and then generalize the observed behaviour over unseen cases, either in the form of words (how likely is it to find sausage in the object position of eat?) or word classes (how likely is it to find VEGETABLE, FOOD, etc?). WN-based approaches translate the generalization problem into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test, and obtain an association score for each (verb, noun) pair. The main</context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>E. Agirre and D. Martinez. 2001. Learning class-toclass selectional preferences. In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural Language Learning (CoNLL-2001), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alishahi</author>
</authors>
<title>A probabilistic model of early argument structure acquisition.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="2641" citStr="Alishahi (2008)" startWordPosition="407" endWordPosition="408">ically annotated Latin corpus is available yet; building such a corpus manually would take considerable time and energy. Hence, research in computational semantics for Latin would benefit from exploiting the existing resources and tools through automatic lexical acquisition methods. In this paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of verbs on their arguments: e. g. we expect the object position of the verb edo ‘eat’ to be mostly filled by nouns from the food domain. For this task, we propose a method inspired by Alishahi (2008) and outlined in an earlier version on the IT-TB in McGillivray (2009). SPs are defined as probability distributions over semantic features extracted as sets of LWN nodes. The input data are two subcategorization lexicons automatically extracted from the LDT and the ITTB (McGillivray and Passarotti, 2009). Our main contribution is to create a new tool for semantic processing of Latin by adapting computational techniques developed for extant languages to the special case of Latin. A successful adaptation is contingent on overcoming corpus size differences. The way our model combines the syntact</context>
<context position="5990" citStr="Alishahi (2008)" startWordPosition="947" endWordPosition="948">ds, then calculate SPs with the log-likelihood test, and obtain an association score for each (verb, noun) pair. The main difference between these previous systems and our case is the size of the input corpus. In fact, our dataset consists of subcategorization frames extracted from two relatively small treebanks, amounting to a little over 100,000 word tokens overall. This results in a large number of low-frequency (verb, noun) associations, which may not reflect the actual distributions of Latin verbs. This state improves if we group the observations into clusters. Such a method, proposed by Alishahi (2008), proved effective in our case. The originality of this approach is an incremental clustering algorithm for verb occurrences called frames which are identified by specific syntactic and semantic features, such as the number of verbal arguments, the syntactic pattern, and the semantic properties of each argument, i. e. the WN hypernyms of the argument’s fillers. Based on a probabilistic measure of similarity between the frames’ features, the clustering produces larger sets called constructions. The constructions for a verb contribute to the next step, which acquires the verb’s SPs as semantic p</context>
<context position="12515" citStr="Alishahi (2008)" startWordPosition="2089" endWordPosition="2090">(f)| syns score(v,V (h)) · Nfil(h, a) (1) where V (h) in (1) contains the verbs of h, Nfil(h, a) counts the a-fillers in h, f ranges in the set of a-fillers in h, S(f) contains the semantic properties for f and |s∩S(f) |is 1 when s appears in S(f) and 0 otherwise. Pa(s|k, v) is thus obtained by normalizing the sum of these similarity scores over all frames in k, divided by the total number of frames in k containing v or its synonyms. The similarity scores weight the contributions of the synonyms of v, whose fillers play a role in the generalization step. This is our innovation with respect to Alishahi (2008)’s system. It was introduced because of the sparseness of our data, where 3The algorithm uses smoothed versions of all the previous formulae by adding a very small constant so that the probabilities are never 0. 75 k h 1 induco+P Sb[acc]{forma} introduco + P Sb{PR} introduco + P Sb{forma} addo+P Sb{praesidium} 2 induco+A Obj[acc]{forma} immitto+A Obj[acc]{PR},Obj[dat]{antrum} introduco + A Obj[acc]{NP} 3 introduco + A (in)Obj[acc]{finis},Obj[acc]{copia},Sb{NP} induco+A (in)Obj[acc]{effectus},Obj[acc]{forma} 4 introduco + A Obj[acc]{forma} induco+A Obj[acc]{perfectio},Sb[nom]{PR} induco+A Obj[a</context>
<context position="14159" citStr="Alishahi (2008)" startWordPosition="2321" endWordPosition="2322">ere the verb introduco ‘bring in, introduce’ occurs. Note the semantic similarity between addo ‘add to, bring to’, immitto ‘send against, insert’, induco ‘bring forward, introduce’ and introduco, and the similarity between the syntactic patterns and the argument fillers within the same construction. For example, finis ‘end, borders’ and effectus ‘result’ share the semantic properties ATTRIBUTE, COGNITIO ‘cognition’, CONSCIENTIA ‘conscience’, EVENTUM ‘event’, among others. The vast majority of constructions contain less than 4 frames. This contrasts with the more general constructions found by Alishahi (2008) and can be explained by several factors. First, the coverage of LWN is quite low with respect to the fillers in our dataset. In fact, 782 fillers out of 2408 could not be assigned to any LWN synset; for these lemmas the semantic scores with all the other nouns are 0, causing probabilities lower than the baseline; this results in assigning the frame to the singleton construction consisting of the frame itself. The same happens for fillers consisting of verbal lemmas, participles, pronouns and named entities, which amount to a third of the total number. Furthermore, the data are not tagged by s</context>
<context position="19216" citStr="Alishahi (2008)" startWordPosition="3132" endWordPosition="3133">19 Table 3: 15 nouns with the highest probabilities as accusative objects of dico ‘say’. Figure 1: Decreasing SP probabilities of the LWN leaf nodes for the objects of dico ‘say’. Table 3 displays the 15 nouns with the highest probabilities as direct objects for dico ‘say’. From table 3 – and the rest of the distribution, represented in figure 1 – we see that the model assigns a high probability to most seen fillers for dico in the corpus: anima ‘soul’, corpus ‘body’, locus ‘place’, pars ‘part’, etc. For what concerns evaluating the SP probability assigned to nouns unseen in the training set, Alishahi (2008) follows the approach suggested by Resnik (1993), using human plausibility judgements on verb-noun pairs. Given the absence of native speakers of Latin, we used random occurrences in corpora, considered as positive examples of plausible argument fillers; on the other hand, we cannot extract non-plausible fillers from a corpus unless we use a frequency-based criterion. However, we can measure how well our system predicts the probability of these unseen events. As a preliminary evaluation experiment, we randomly selected from our corpora a list of 19 high-frequency verbs (freq.&gt;51) and 7 mediumf</context>
</contexts>
<marker>Alishahi, 2008</marker>
<rawString>A. Alishahi. 2008. A probabilistic model of early argument structure acquisition. Ph.D. thesis, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bamman</author>
<author>G Crane</author>
</authors>
<title>The design and use of a Latin dependency treebank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>67--78</pages>
<publisher>UFAL MFF UK.</publisher>
<contexts>
<context position="1435" citStr="Bamman and Crane, 2006" startWordPosition="214" endWordPosition="217">sources such as dictionaries. Nevertheless, it is a low-resourced language from a computational point of view (McGillivray et al., 2009). As far as NLP tools for Latin are concerned, parsing experiments with machine learning techniques are ongoing (Bamman and Crane, 2008; Passarotti and Ruffolo, forthcoming), although more work is still needed in this direction, especially given the small size of the training data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PROIEL treebank (approximately 100,000 tokens) for the Bible (Haug and Jøndal, 2008). In addition, a Latin version of WordNet – Latin WordNet (LWN; Minozzi, (2009) – is being compiled, consisting of around 10,000 lemmas inserted in the multilingual structure of MultiWordNet (Bentivogli et al., 2004). The number and the size of these resources are small when compared with the corpora and the lexicons for modern languages, e. g. English. Concerning semantic processing, no semantically an</context>
</contexts>
<marker>Bamman, Crane, 2006</marker>
<rawString>D. Bamman and G. Crane. 2006. The design and use of a Latin dependency treebank. In Proceedings of the Fifth International Workshop on Treebanks and Linguistic Theories, pages 67–78. ´UFAL MFF UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bamman</author>
<author>G Crane</author>
</authors>
<title>Building a dynamic lexicon from a digital library.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th ACM/IEEE-CS Joint Conference on Digital Libraries,</booktitle>
<pages>11--20</pages>
<contexts>
<context position="1083" citStr="Bamman and Crane, 2008" startWordPosition="159" endWordPosition="162">extracted from other Latin corpora. 1 Introduction Automatic acquisition of semantic information from corpora is a challenge for research on lowresourced languages, especially when semantically annotated corpora are not available. Latin is definitely a high-resourced language for what concerns the number of available texts and traditional lexical resources such as dictionaries. Nevertheless, it is a low-resourced language from a computational point of view (McGillivray et al., 2009). As far as NLP tools for Latin are concerned, parsing experiments with machine learning techniques are ongoing (Bamman and Crane, 2008; Passarotti and Ruffolo, forthcoming), although more work is still needed in this direction, especially given the small size of the training data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PROIEL treebank (approximately 100,000 tokens) for the Bible (Haug and Jøndal, 2008). In addition, a Latin version of WordNet – Latin Word</context>
<context position="5260" citStr="Bamman and Crane (2008)" startWordPosition="828" endWordPosition="831">em into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test, and obtain an association score for each (verb, noun) pair. The main difference between these previous systems and our case is the size of the input corpus. In fact, our dataset consists of subcategorization frames extracted from two relatively small treebanks, amounting to a little over 100,000 word tokens overall. This results in a large number of low-frequency (verb, noun) associations, which may not reflect the actual distri</context>
</contexts>
<marker>Bamman, Crane, 2008</marker>
<rawString>D. Bamman and G. Crane. 2008. Building a dynamic lexicon from a digital library. In Proceedings of the 8th ACM/IEEE-CS Joint Conference on Digital Libraries, pages 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>P Forner</author>
<author>Pianta E Magnini</author>
<author>B</author>
</authors>
<title>Revising wordnet domains hierarchy: Semantics, coverage, and balancing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING Workshop on Multilingual Linguistic Resources,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="1845" citStr="Bentivogli et al., 2004" startWordPosition="277" endWordPosition="280">ing data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PROIEL treebank (approximately 100,000 tokens) for the Bible (Haug and Jøndal, 2008). In addition, a Latin version of WordNet – Latin WordNet (LWN; Minozzi, (2009) – is being compiled, consisting of around 10,000 lemmas inserted in the multilingual structure of MultiWordNet (Bentivogli et al., 2004). The number and the size of these resources are small when compared with the corpora and the lexicons for modern languages, e. g. English. Concerning semantic processing, no semantically annotated Latin corpus is available yet; building such a corpus manually would take considerable time and energy. Hence, research in computational semantics for Latin would benefit from exploiting the existing resources and tools through automatic lexical acquisition methods. In this paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of v</context>
</contexts>
<marker>Bentivogli, Forner, Magnini, B, 2004</marker>
<rawString>L. Bentivogli, P. Forner, and and Pianta E. Magnini, B. 2004. Revising wordnet domains hierarchy: Semantics, coverage, and balancing. In Proceedings of COLING Workshop on Multilingual Linguistic Resources, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>D Weir</author>
</authors>
<title>An iterative approach to estimating frequencies over a semantic hierarchy.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very</booktitle>
<pages>258--265</pages>
<institution>Large Corpora. University of Maryland,</institution>
<contexts>
<context position="4868" citStr="Clark and Weir (1999)" startWordPosition="767" endWordPosition="770">sition of verbal SPs collect argument headwords from a corpus (for example, apple, meat, salad as objects of eat) and then generalize the observed behaviour over unseen cases, either in the form of words (how likely is it to find sausage in the object position of eat?) or word classes (how likely is it to find VEGETABLE, FOOD, etc?). WN-based approaches translate the generalization problem into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test, and obtain an association score for each</context>
</contexts>
<marker>Clark, Weir, 1999</marker>
<rawString>S. Clark and D. Weir. 1999. An iterative approach to estimating frequencies over a semantic hierarchy. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. University of Maryland, pages 258–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="5235" citStr="Erk (2007)" startWordPosition="826" endWordPosition="827">zation problem into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test, and obtain an association score for each (verb, noun) pair. The main difference between these previous systems and our case is the size of the input corpus. In fact, our dataset consists of subcategorization frames extracted from two relatively small treebanks, amounting to a little over 100,000 word tokens overall. This results in a large number of low-frequency (verb, noun) associations, which may not </context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>K. Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T T Haug</author>
<author>M L Jøndal</author>
</authors>
<title>Creating a parallel treebank of the old Indo-European Bible translations.</title>
<date>2008</date>
<booktitle>In Proceedings of Language Technologies for Cultural Heritage Workshop,</booktitle>
<pages>27--34</pages>
<contexts>
<context position="1629" citStr="Haug and Jøndal, 2008" startWordPosition="243" endWordPosition="246">riments with machine learning techniques are ongoing (Bamman and Crane, 2008; Passarotti and Ruffolo, forthcoming), although more work is still needed in this direction, especially given the small size of the training data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PROIEL treebank (approximately 100,000 tokens) for the Bible (Haug and Jøndal, 2008). In addition, a Latin version of WordNet – Latin WordNet (LWN; Minozzi, (2009) – is being compiled, consisting of around 10,000 lemmas inserted in the multilingual structure of MultiWordNet (Bentivogli et al., 2004). The number and the size of these resources are small when compared with the corpora and the lexicons for modern languages, e. g. English. Concerning semantic processing, no semantically annotated Latin corpus is available yet; building such a corpus manually would take considerable time and energy. Hence, research in computational semantics for Latin would benefit from exploiting</context>
</contexts>
<marker>Haug, Jøndal, 2008</marker>
<rawString>D. T. T. Haug and M. L. Jøndal. 2008. Creating a parallel treebank of the old Indo-European Bible translations. In Proceedings of Language Technologies for Cultural Heritage Workshop, pages 27–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="4845" citStr="Li and Abe (1998)" startWordPosition="763" endWordPosition="766">for automatic acquisition of verbal SPs collect argument headwords from a corpus (for example, apple, meat, salad as objects of eat) and then generalize the observed behaviour over unseen cases, either in the form of words (how likely is it to find sausage in the object position of eat?) or word classes (how likely is it to find VEGETABLE, FOOD, etc?). WN-based approaches translate the generalization problem into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test, and obtain an ass</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>H. Li and N. Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B McGillivray</author>
<author>M Passarotti</author>
</authors>
<title>The development of the Index Thomisticus Treebank Valency Lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2947" citStr="McGillivray and Passarotti, 2009" startWordPosition="453" endWordPosition="456">his paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of verbs on their arguments: e. g. we expect the object position of the verb edo ‘eat’ to be mostly filled by nouns from the food domain. For this task, we propose a method inspired by Alishahi (2008) and outlined in an earlier version on the IT-TB in McGillivray (2009). SPs are defined as probability distributions over semantic features extracted as sets of LWN nodes. The input data are two subcategorization lexicons automatically extracted from the LDT and the ITTB (McGillivray and Passarotti, 2009). Our main contribution is to create a new tool for semantic processing of Latin by adapting computational techniques developed for extant languages to the special case of Latin. A successful adaptation is contingent on overcoming corpus size differences. The way our model combines the syntactic information contained in the treebanks with the lexical semantic knowledge from LWN allows us to overcome some of the difficulties related to the small size of the input corpora. This is the main difference from corpora for modern languages, together with the absence of semantic annotation. Moreover, w</context>
</contexts>
<marker>McGillivray, Passarotti, 2009</marker>
<rawString>B. McGillivray and M. Passarotti. 2009. The development of the Index Thomisticus Treebank Valency Lexicon. In Proceedings of the Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B McGillivray</author>
<author>M Passarotti</author>
<author>P Ruffolo</author>
</authors>
<title>The Index Thomisticus treebank project: Annotation, parsing and valency lexicon.</title>
<date>2009</date>
<journal>TAL,</journal>
<volume>50</volume>
<issue>2</issue>
<pages>127</pages>
<contexts>
<context position="948" citStr="McGillivray et al., 2009" startWordPosition="137" endWordPosition="140">nnected with data sparseness and the small size of the input corpora. We also suggest a way to evaluate the acquired SPs on unseen events extracted from other Latin corpora. 1 Introduction Automatic acquisition of semantic information from corpora is a challenge for research on lowresourced languages, especially when semantically annotated corpora are not available. Latin is definitely a high-resourced language for what concerns the number of available texts and traditional lexical resources such as dictionaries. Nevertheless, it is a low-resourced language from a computational point of view (McGillivray et al., 2009). As far as NLP tools for Latin are concerned, parsing experiments with machine learning techniques are ongoing (Bamman and Crane, 2008; Passarotti and Ruffolo, forthcoming), although more work is still needed in this direction, especially given the small size of the training data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PRO</context>
</contexts>
<marker>McGillivray, Passarotti, Ruffolo, 2009</marker>
<rawString>B. McGillivray, M. Passarotti, and P. Ruffolo. 2009. The Index Thomisticus treebank project: Annotation, parsing and valency lexicon. TAL, 50(2):103– 127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B McGillivray</author>
</authors>
<title>Selectional Preferences from a Latin treebank. In</title>
<date>2009</date>
<booktitle>Proceedings of the Eigth International Workshop on Treebanks and Linguistic Theories (TLT8),</booktitle>
<pages>131--136</pages>
<editor>Przepi´orkowski A. Passarotti, M., S. Raynaud, and F. van Eynde, editors,</editor>
<contexts>
<context position="2711" citStr="McGillivray (2009)" startWordPosition="420" endWordPosition="421">pus manually would take considerable time and energy. Hence, research in computational semantics for Latin would benefit from exploiting the existing resources and tools through automatic lexical acquisition methods. In this paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of verbs on their arguments: e. g. we expect the object position of the verb edo ‘eat’ to be mostly filled by nouns from the food domain. For this task, we propose a method inspired by Alishahi (2008) and outlined in an earlier version on the IT-TB in McGillivray (2009). SPs are defined as probability distributions over semantic features extracted as sets of LWN nodes. The input data are two subcategorization lexicons automatically extracted from the LDT and the ITTB (McGillivray and Passarotti, 2009). Our main contribution is to create a new tool for semantic processing of Latin by adapting computational techniques developed for extant languages to the special case of Latin. A successful adaptation is contingent on overcoming corpus size differences. The way our model combines the syntactic information contained in the treebanks with the lexical semantic kn</context>
</contexts>
<marker>McGillivray, 2009</marker>
<rawString>B. McGillivray. 2009. Selectional Preferences from a Latin treebank. In Przepi´orkowski A. Passarotti, M., S. Raynaud, and F. van Eynde, editors, Proceedings of the Eigth International Workshop on Treebanks and Linguistic Theories (TLT8), pages 131– 136. EDUCatt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Minozzi</author>
</authors>
<title>The Latin Wordnet project.</title>
<date>2009</date>
<booktitle>Proceedings of the 15th International Colloquium on Latin Linguistics (ICLL), Innsbrucker Beitraege zur Sprachwissenschaft.</booktitle>
<editor>In P. Anreiter and M. Kienpointner, editors,</editor>
<contexts>
<context position="1708" citStr="Minozzi, (2009)" startWordPosition="258" endWordPosition="259">ti and Ruffolo, forthcoming), although more work is still needed in this direction, especially given the small size of the training data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PROIEL treebank (approximately 100,000 tokens) for the Bible (Haug and Jøndal, 2008). In addition, a Latin version of WordNet – Latin WordNet (LWN; Minozzi, (2009) – is being compiled, consisting of around 10,000 lemmas inserted in the multilingual structure of MultiWordNet (Bentivogli et al., 2004). The number and the size of these resources are small when compared with the corpora and the lexicons for modern languages, e. g. English. Concerning semantic processing, no semantically annotated Latin corpus is available yet; building such a corpus manually would take considerable time and energy. Hence, research in computational semantics for Latin would benefit from exploiting the existing resources and tools through automatic lexical acquisition methods</context>
</contexts>
<marker>Minozzi, 2009</marker>
<rawString>S. Minozzi. 2009. The Latin Wordnet project. In P. Anreiter and M. Kienpointner, editors, Proceedings of the 15th International Colloquium on Latin Linguistics (ICLL), Innsbrucker Beitraege zur Sprachwissenschaft.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Parsing the Index Thomisticus Treebank. some preliminary results.</title>
<booktitle>Proceedings of the 15th International Colloquium on Latin Linguistics, Innsbrucker Beitr¨age zur Sprachwissenschaft.</booktitle>
<editor>In P. Anreiter and M. Kienpointner, editors,</editor>
<marker>forthcoming, </marker>
<rawString>M. Passarotti and P. Ruffolo. forthcoming. Parsing the Index Thomisticus Treebank. some preliminary results. In P. Anreiter and M. Kienpointner, editors, Proceedings of the 15th International Colloquium on Latin Linguistics, Innsbrucker Beitr¨age zur Sprachwissenschaft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Passarotti</author>
</authors>
<title>Verso il Lessico Tomistico Biculturale. La treebank dell’Index Thomisticus. In</title>
<date>2007</date>
<booktitle>Atti del XIII Congresso Nazionale della Societ`a di Filosofia del Linguaggio,</booktitle>
<pages>187--205</pages>
<editor>R. Petrilli and D. Femia, editors,</editor>
<contexts>
<context position="1535" citStr="Passarotti, 2007" startWordPosition="230" endWordPosition="231">iew (McGillivray et al., 2009). As far as NLP tools for Latin are concerned, parsing experiments with machine learning techniques are ongoing (Bamman and Crane, 2008; Passarotti and Ruffolo, forthcoming), although more work is still needed in this direction, especially given the small size of the training data. As a matter of fact, only three syntactically annotated Latin corpora are available (and still in progress): the Latin Dependency Treebank (LDT, 53,000 tokens) for classical Latin (Bamman and Crane, 2006), the Index Thomisticus Treebank (ITTB, 54,000 tokens) for Thomas Aquinas’s works (Passarotti, 2007), and the PROIEL treebank (approximately 100,000 tokens) for the Bible (Haug and Jøndal, 2008). In addition, a Latin version of WordNet – Latin WordNet (LWN; Minozzi, (2009) – is being compiled, consisting of around 10,000 lemmas inserted in the multilingual structure of MultiWordNet (Bentivogli et al., 2004). The number and the size of these resources are small when compared with the corpora and the lexicons for modern languages, e. g. English. Concerning semantic processing, no semantically annotated Latin corpus is available yet; building such a corpus manually would take considerable time </context>
</contexts>
<marker>Passarotti, 2007</marker>
<rawString>M. Passarotti. 2007. Verso il Lessico Tomistico Biculturale. La treebank dell’Index Thomisticus. In R. Petrilli and D. Femia, editors, Atti del XIII Congresso Nazionale della Societ`a di Filosofia del Linguaggio, pages 187–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and Information: A ClassBased Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4826" citStr="Resnik (1993)" startWordPosition="761" endWordPosition="762">he-art systems for automatic acquisition of verbal SPs collect argument headwords from a corpus (for example, apple, meat, salad as objects of eat) and then generalize the observed behaviour over unseen cases, either in the form of words (how likely is it to find sausage in the object position of eat?) or word classes (how likely is it to find VEGETABLE, FOOD, etc?). WN-based approaches translate the generalization problem into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test</context>
<context position="19264" citStr="Resnik (1993)" startWordPosition="3139" endWordPosition="3140">s as accusative objects of dico ‘say’. Figure 1: Decreasing SP probabilities of the LWN leaf nodes for the objects of dico ‘say’. Table 3 displays the 15 nouns with the highest probabilities as direct objects for dico ‘say’. From table 3 – and the rest of the distribution, represented in figure 1 – we see that the model assigns a high probability to most seen fillers for dico in the corpus: anima ‘soul’, corpus ‘body’, locus ‘place’, pars ‘part’, etc. For what concerns evaluating the SP probability assigned to nouns unseen in the training set, Alishahi (2008) follows the approach suggested by Resnik (1993), using human plausibility judgements on verb-noun pairs. Given the absence of native speakers of Latin, we used random occurrences in corpora, considered as positive examples of plausible argument fillers; on the other hand, we cannot extract non-plausible fillers from a corpus unless we use a frequency-based criterion. However, we can measure how well our system predicts the probability of these unseen events. As a preliminary evaluation experiment, we randomly selected from our corpora a list of 19 high-frequency verbs (freq.&gt;51) and 7 mediumfrequency verbs (11&lt;freq.&lt;50), for each of which </context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Selection and Information: A ClassBased Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Prescher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="5220" citStr="Rooth et al. (1999)" startWordPosition="821" endWordPosition="824">s translate the generalization problem into estimating preference probabilities over a noun hierarchy and solve it by means of different statistical tools that use the input data as a training set: cf. inter al. Resnik (1993), Li and Abe (1998), Clark and Weir (1999). Agirre and Martinez (2001) acquire SPs for verb classes instead of single verb lemmas by using a semantically annotated corpus and WN. Distributional methods aim at automatically inducing semantic classes from distributional data in corpora by means of various similarity measures and unsupervised clustering algorithms: cf. e. g. Rooth et al. (1999) and Erk (2007). Bamman and Crane (2008) is the only distributional approach dealing with Latin. They use an automatically parsed corpus of 3.5 million words, then calculate SPs with the log-likelihood test, and obtain an association score for each (verb, noun) pair. The main difference between these previous systems and our case is the size of the input corpus. In fact, our dataset consists of subcategorization frames extracted from two relatively small treebanks, amounting to a little over 100,000 word tokens overall. This results in a large number of low-frequency (verb, noun) associations,</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 104–111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>