<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008954">
<title confidence="0.998542">
It Makes Sense: A Wide-Coverage Word Sense Disambiguation System
for Free Text
</title>
<author confidence="0.999157">
Zhi Zhong and Hwee Tou Ng
</author>
<affiliation confidence="0.9998165">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9705585">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998149">
{zhongzhi, nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99962535">
Word sense disambiguation (WSD)
systems based on supervised learning
achieved the best performance in SensE-
val and SemEval workshops. However,
there are few publicly available open
source WSD systems. This limits the use
of WSD in other applications, especially
for researchers whose research interests
are not in WSD.
In this paper, we present IMS, a supervised
English all-words WSD system. The flex-
ible framework of IMS allows users to in-
tegrate different preprocessing tools, ad-
ditional features, and different classifiers.
By default, we use linear support vector
machines as the classifier with multiple
knowledge-based features. In our imple-
mentation, IMS achieves state-of-the-art
results on several SensEval and SemEval
tasks.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934517241379">
Word sense disambiguation (WSD) refers to the
task of identifying the correct sense of an ambigu-
ous word in a given context. As a fundamental
task in natural language processing (NLP), WSD
can benefit applications such as machine transla-
tion (Chan et al., 2007a; Carpuat and Wu, 2007)
and information retrieval (Stokoe et al., 2003).
In previous SensEval workshops, the supervised
learning approach has proven to be the most suc-
cessful WSD approach (Palmer et al., 2001; Sny-
der and Palmer, 2004; Pradhan et al., 2007). In
the most recent SemEval-2007 English all-words
tasks, most of the top systems were based on su-
pervised learning methods. These systems used
a set of knowledge sources drawn from sense-
annotated data, and achieved significant improve-
ments over the baselines.
However, developing such a system requires
much effort. As a result, very few open source
WSD systems are publicly available – the only
other publicly available WSD system that we are
aware of is SenseLearner (Mihalcea and Csomai,
2005). Therefore, for applications which employ
WSD as a component, researchers can only make
use of some baselines or unsupervised methods.
An open source supervised WSD system will pro-
mote the use of WSD in other applications.
In this paper, we present an English all-words
WSD system, IMS (It Makes Sense), built using a
supervised learning approach. IMS is a Java im-
plementation, which provides an extensible and
flexible platform for researchers interested in us-
ing a WSD component. Users can choose differ-
ent tools to perform preprocessing, such as trying
out various features in the feature extraction step,
and applying different machine learning methods
or toolkits in the classification step. Following
Lee and Ng (2002), we adopt support vector ma-
chines (SVM) as the classifier and integrate mul-
tiple knowledge sources including parts-of-speech
(POS), surrounding words, and local collocations
as features. We also provide classification mod-
els trained with examples collected from parallel
texts, SEMCOR (Miller et al., 1994), and the DSO
corpus (Ng and Lee, 1996).
A previous implementation of the IMS sys-
tem, NUS-PT (Chan et al., 2007b), participated in
SemEval-2007 English all-words tasks and ranked
first and second in the coarse-grained and fine-
grained task, respectively. Our current IMS im-
plementation achieves competitive accuracies on
several SensEval/SemEval English lexical-sample
and all-words tasks.
The remainder of this paper is organized as
follows. Section 2 gives the system description,
which introduces the system framework and the
details of the implementation. In Section 3, we
present the evaluation results of IMS on SensE-
</bodyText>
<page confidence="0.982826">
78
</page>
<note confidence="0.5942925">
Proceedings of the ACL 2010 System Demonstrations, pages 78–83,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.963328">
val/SemEval English tasks. Finally, we conclude
in Section 4.
</bodyText>
<sectionHeader confidence="0.957307" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999830333333333">
In this section, we first outline the IMS system,
and introduce the default preprocessing tools, the
feature types, and the machine learning method
used in our implementation. Then we briefly ex-
plain the collection of training data for content
words.
</bodyText>
<subsectionHeader confidence="0.997417">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.999813421052632">
Figure 1 shows the system architecture of IMS.
The system accepts any input text. For each con-
tent word w (noun, verb, adjective, or adverb) in
the input text, IMS disambiguates the sense of w
and outputs a list of the senses of w, where each
sense si is assigned a probability according to the
likelihood of si appearing in that context. The
sense inventory used is based on WordNet (Miller,
1990) version 1.7.1.
IMS consists of three independent modules:
preprocessing, feature and instance extraction, and
classification. Knowledge sources are generated
from input texts in the preprocessing step. With
these knowledge sources, instances together with
their features are extracted in the instance and fea-
ture extraction step. Then we train one classifica-
tion model for each word type. The model will be
used to classify test instances of the corresponding
word type.
</bodyText>
<subsectionHeader confidence="0.876335">
2.1.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.996954555555555">
Preprocessing is the step to convert input texts into
formatted information. Users can integrate differ-
ent tools in this step. These tools are applied on the
input texts to extract knowledge sources such as
sentence boundaries, part-of-speech tags, etc. The
extracted knowledge sources are stored for use in
the later steps.
In IMS, preprocessing is carried out in four
steps:
</bodyText>
<listItem confidence="0.997780571428571">
• Detect the sentence boundaries in a raw input
text with a sentence splitter.
• Tokenize the split sentences with a tokenizer.
• Assign POS tags to all tokens with a POS tag-
ger.
• Find the lemma form of each token with a
lemmatizer.
</listItem>
<bodyText confidence="0.999498285714286">
By default, the sentence splitter and POS tag-
ger in the OpenNLP toolkit1 are used for sen-
tence splitting and POS tagging. A Java version of
Penn TreeBank tokenizer2 is applied in tokeniza-
tion. JWNL3, a Java API for accessing the Word-
Net (Miller, 1990) thesaurus, is used to find the
lemma form of each token.
</bodyText>
<subsectionHeader confidence="0.711296">
2.1.2 Feature and Instance Extraction
</subsectionHeader>
<bodyText confidence="0.999879">
After gathering the formatted information in the
preprocessing step, we use an instance extractor
together with a list of feature extractors to extract
the instances and their associated features.
Previous research has found that combining
multiple knowledge sources achieves high WSD
accuracy (Ng and Lee, 1996; Lee and Ng, 2002;
Decadt et al., 2004). In IMS, we follow Lee and
Ng (2002) and combine three knowledge sources
for all content word types4:
</bodyText>
<listItem confidence="0.935447478260869">
• POS Tags of Surrounding Words We use
the POS tags of three words to the left and
three words to the right of the target ambigu-
ous word, and the target word itself. The
POS tag feature cannot cross sentence bound-
ary, which means all the associated surround-
ing words should be in the same sentence as
the target word. If a word crosses sentence
boundary, the corresponding POS tag value
will be assigned as null.
For example, suppose we want to disam-
biguate the word interest in a POS-tagged
sentence “My/PRP$ brother/NN has/VBZ
always/RB taken/VBN a/DT keen/JJ inter-
est/NN in/IN my/PRP$ work/NN ./.”. The 7
POS tag features for this instance are &lt;VBN,
DT, JJ, NN, IN, PRP$, NN&gt;.
• Surrounding Words Surrounding words fea-
tures include all the individual words in the
surrounding context of an ambiguous word
w. The surrounding words can be in the cur-
rent sentence or immediately adjacent sen-
tences.
</listItem>
<bodyText confidence="0.998929333333333">
However, we remove the words that are in
a list of stop words. Words that contain
no alphabetic characters, such as punctuation
</bodyText>
<footnote confidence="0.9982302">
1http://opennlp.sourceforge.net/
2http://www.cis.upenn.edu/—treebank/
tokenizer.sed
3http://jwordnet.sourceforge.net/
4Syntactic relations are omitted for efficiency reason.
</footnote>
<page confidence="0.997877">
79
</page>
<figureCaption confidence="0.999894">
Figure 1: IMS system architecture
</figureCaption>
<bodyText confidence="0.999262142857143">
symbols and numbers, are also discarded.
The remaining words are converted to their
lemma forms in lower case. Each lemma is
considered as one feature. The feature value
is set to be 1 if the corresponding lemma oc-
curs in the surrounding context of w, 0 other-
wise.
For example, suppose there is a set of sur-
rounding words features {account, economy,
rate, take} in the training data set of the word
interest. For a test instance of interest in
the sentence “My brother has always taken a
keen interest in my work .”, the surrounding
word feature vector will be &lt;0, 0, 0, 1&gt;.
</bodyText>
<listItem confidence="0.990831">
• Local Collocations We use 11 local collo-
</listItem>
<bodyText confidence="0.935103952380952">
cations features including: C−2,−2, C−1,−1,
C1,1, C2,2, C−2,−1, C−1,1, C1,2, C−3,−1,
C−2,1, C−1,2, and C1,3, where Cj,� refers to
an ordered sequence of words in the same
sentence of w. Offsets i and j denote the
starting and ending positions of the sequence
relative to w, where a negative (positive) off-
set refers to a word to the left (right) of w.
For example, suppose in the training data set,
the word interest has a set of local colloca-
tions {“account .”, “of all”, “in my”, “to
be”} for C1,2. For a test instance of inter-
est in the sentence “My brother has always
taken a keen interest in my work .”, the value
of feature C1,2 will be “in my”.
As shown in Figure 1, we implement one fea-
ture extractor for each feature type. The IMS soft-
ware package is organized in such a way that users
can easily specify their own feature set by im-
plementing more feature extractors to exploit new
features.
</bodyText>
<sectionHeader confidence="0.452647" genericHeader="method">
2.1.3 Classification
</sectionHeader>
<bodyText confidence="0.999976407407408">
In IMS, the classifier trains a model for each word
type which has training data during the training
process. The instances collected in the previous
step are converted to the format expected by the
machine learning toolkit in use. Thus, the classifi-
cation step is separate from the feature extraction
step. We use LIBLINEAR5 (Fan et al., 2008) as
the default classifier of IMS, with a linear kernel
and all the parameters set to their default values.
Accordingly, we implement an interface to convert
the instances into the LIBLINEAR feature vector
format.
The utilization of other machine learning soft-
ware can be achieved by implementing the corre-
sponding module interfaces to them. For instance,
IMS provides module interfaces to the WEKA ma-
chine learning toolkit (Witten and Frank, 2005),
LIBSVM6, and MaxEnt7.
The trained classification models will be ap-
plied to the test instances of the corresponding
word types in the testing process. If a test instance
word type is not seen during training, we will out-
put its predefined default sense, i.e., the WordNet
first sense, as the answer. Furthermore, if a word
type has neither training data nor predefined de-
fault sense, we will output “U”, which stands for
the missing sense, as the answer.
</bodyText>
<footnote confidence="0.9961856">
5http://www.bwaldvogel.de/
liblinear-java/
6http://www.csie.ntu.edu.tw/-cjlin/
libsvm/
7http://maxent.sourceforge.net/
</footnote>
<page confidence="0.99297">
80
</page>
<subsectionHeader confidence="0.9555205">
2.2 The Training Data Set for All-Words
Tasks
</subsectionHeader>
<bodyText confidence="0.9998878">
Once we have a supervised WSD system, for the
users who only need WSD as a component in
their applications, it is also important to provide
them the classification models. The performance
of a supervised WSD system greatly depends on
the size of the sense-annotated training data used.
To overcome the lack of sense-annotated train-
ing examples, besides the training instances from
the widely used sense-annotated corpus SEMCOR
(Miller et al., 1994) and DSO corpus (Ng and Lee,
1996), we also follow the approach described in
Chan and Ng (2005) to extract more training ex-
amples from parallel texts.
The process of extracting training examples
from parallel texts is as follows:
</bodyText>
<listItem confidence="0.998673260869565">
• Collect a set of sentence-aligned parallel
texts. In our case, we use six English-Chinese
parallel corpora: Hong Kong Hansards, Hong
Kong News, Hong Kong Laws, Sinorama,
Xinhua News, and the English translation of
Chinese Treebank. They are all available
from the Linguistic Data Consortium (LDC).
• Perform tokenization on the English texts
with the Penn TreeBank tokenizer.
• Perform Chinese word segmentation on the
Chinese texts with the Chinese word segmen-
tation method proposed by Low et al. (2005).
• Perform word alignment on the parallel texts
using the GIZA++ software (Och and Ney,
2000).
• Assign Chinese translations to each sense of
an English word w.
• Pick the occurrences of w which are aligned
to its chosen Chinese translations in the word
alignment output of GIZA++.
• Identify the senses of the selected occur-
rences of w by referring to their aligned Chi-
nese translations.
</listItem>
<bodyText confidence="0.999397125">
Finally, the English side of these selected occur-
rences together with their assigned senses are used
as training data.
We only extract training examples from paral-
lel texts for the top 60% most frequently occur-
ring polysemous content words in Brown Corpus
(BC), which includes 730 nouns, 190 verbs, and
326 adjectives. For each of the top 60% nouns and
adjectives, we gather a maximum of 1,000 training
examples from parallel texts. For each of the top
60% verbs, we extract not more than 500 examples
from parallel texts, as well as up to 500 examples
from the DSO corpus. We also make use of the
sense-annotated examples from SEMCOR as part
of our training data for all nouns, verbs, adjectives,
and 28 most frequently occurring adverbs in BC.
</bodyText>
<table confidence="0.7879875">
POS noun verb adj adv
# oftypes 11,445 4,705 5,129 28
</table>
<tableCaption confidence="0.987708">
Table 1: Statistics of the word types which have
training data for WordNet 1.7.1 sense inventory
</tableCaption>
<bodyText confidence="0.9792025">
The frequencies of word types which we have
training instances for WordNet sense inventory
version 1.7.1 are listed in Table 1. We generated
classification models with the IMS system for over
21,000 word types which we have training data.
On average, each word type has 38 training in-
stances. The total size of the models is about 200
megabytes.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999855666666667">
In our experiments, we evaluate our IMS system
on SensEval and SemEval tasks, the benchmark
data sets for WSD. The evaluation on both lexical-
sample and all-words tasks measures the accuracy
of our IMS system as well as the quality of the
training data we have collected.
</bodyText>
<subsectionHeader confidence="0.894604">
3.1 English Lexical-Sample Tasks
</subsectionHeader>
<table confidence="0.9989032">
SensEval-2 SensEval-3
IMS 65.3% 72.6%
Rank 1 System 64.2% 72.9%
Rank 2 System 63.8% 72.6%
MFS 47.6% 55.2%
</table>
<tableCaption confidence="0.942273">
Table 2: WSD accuracies on SensEval lexical-
sample tasks
</tableCaption>
<bodyText confidence="0.996564375">
In SensEval English lexical-sample tasks, both
the training and test data sets are provided. A com-
mon baseline for lexical-sample task is to select
the most frequent sense (MFS) in the training data
as the answer.
We evaluate IMS on the SensEval-2 and
SensEval-3 English lexical-sample tasks. Table 2
compares the performance of our system to the top
</bodyText>
<page confidence="0.996758">
81
</page>
<bodyText confidence="0.999804428571428">
two systems that participated in the above tasks
(Yarowsky et al., 2001; Mihalcea and Moldovan,
2001; Mihalcea et al., 2004). Evaluation results
show that IMS achieves significantly better accu-
racies than the MFS baseline. Comparing to the
top participating systems, IMS achieves compara-
ble results.
</bodyText>
<subsectionHeader confidence="0.99925">
3.2 English All-Words Tasks
</subsectionHeader>
<bodyText confidence="0.999896586206897">
In SensEval and SemEval English all-words tasks,
no training data are provided. Therefore, the MFS
baseline is no longer suitable for all-words tasks.
Because the order of senses in WordNet is based
on the frequency of senses in SEMCOR, the Word-
Net first sense (WNs1) baseline always assigns the
first sense in WordNet as the answer. We will use
it as the baseline in all-words tasks.
Using the training data collected with the
method described in Section 2.2, we apply our sys-
tem on the SensEval-2, SensEval-3, and SemEval-
2007 English all-words tasks. Similarly, we also
compare the performance of our system to the top
two systems that participated in the above tasks
(Palmer et al., 2001; Snyder and Palmer, 2004;
Pradhan et al., 2007). The evaluation results are
shown in Table 3. IMS easily beats the WNs1
baseline. It ranks first in SensEval-3 English fine-
grained all-words task and SemEval-2007 English
coarse-grained all-words task, and is also compet-
itive in the remaining tasks. It is worth noting
that because of the small test data set in SemEval-
2007 English fine-grained all-words task, the dif-
ferences between IMS and the best participating
systems are not statistically significant.
Overall, IMS achieves good WSD accuracies on
both all-words and lexical-sample tasks. The per-
formance of IMS shows that it is a state-of-the-art
WSD system.
</bodyText>
<sectionHeader confidence="0.995771" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999964535714286">
This paper presents IMS, an English all-words
WSD system. The goal of IMS is to provide a
flexible platform for supervised WSD, as well as
an all-words WSD component with good perfor-
mance for other applications.
The framework of IMS allows us to integrate
different preprocessing tools to generate knowl-
edge sources. Users can implement various fea-
ture types and different machine learning methods
or toolkits according to their requirements. By
default, the IMS system implements three kinds
of feature types and uses a linear kernel SVM as
the classifier. Our evaluation on English lexical-
sample tasks proves the strength of our system.
With this system, we also provide a large num-
ber of classification models trained with the sense-
annotated training examples from SEMCOR, DSO
corpus, and 6 parallel corpora, for all content
words. Evaluation on English all-words tasks
shows that IMS with these models achieves state-
of-the-art WSD accuracies compared to the top
participating systems.
As a Java-based system, IMS is platform
independent. The source code of IMS and
the classification models can be found on the
homepage: http://nlp.comp.nus.edu.
sg/software and are available for research,
non-commercial use.
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9981394">
This research is done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.
</bodyText>
<sectionHeader confidence="0.997961" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995825111111111">
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72, Prague,
Czech Republic.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
Proceedings of the 20th National Conference on Ar-
tificial Intelligence (AAAI), pages 1037–1042, Pitts-
burgh, Pennsylvania, USA.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007a. Word sense disambiguation improves sta-
tistical machine translation. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 33–40, Prague,
Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007b.
NUS-PT: Exploiting parallel texts for word sense
disambiguation in the English all-words tasks. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages
253–256, Prague, Czech Republic.
Bart Decadt, Veronique Hoste, and Walter Daelemans.
2004. GAMBL, genetic algorithm optimization of
memory-based WSD. In Proceedings of the Third
</reference>
<page confidence="0.997753">
82
</page>
<table confidence="0.999475666666667">
SensEval-2 SensEval-3 SemEval -2007
Fine-grained Fine-grained Fine-grained Coarse-grained
IMS 68.2% 67.6% 58.3% 82.6%
Rank 1 System 69.0% 65.2% 59.1% 82.5%
Rank 2 System 63.6% 64.6% 58.7% 81.6%
WNs1 61.9% 62.4% 51.4% 78.9%
</table>
<tableCaption confidence="0.998009">
Table 3: WSD accuracies on SensEval/SemEval all-words tasks
</tableCaption>
<reference confidence="0.996556204545454">
International Workshop on Evaluating Word Sense
Disambiguation Systems (SensEval-3), pages 108–
112, Barcelona, Spain.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Pro-
ceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 41–48, Philadelphia, Pennsylvania, USA.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161–164, Jeju Island, Korea.
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL) Interactive Poster and Demonstration
Sessions, pages 53–56, Ann Arbor, Michigan, USA.
Rada Mihalcea and Dan Moldovan. 2001. Pattern
learning and active feature selection for word sense
disambiguation. In Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems (SensEval-2), pages 127–130,
Toulouse, France.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The SensEval-3 English lexical sam-
ple task. In Proceedings of the Third International
Workshop on Evaluating Word Sense Disambigua-
tion Systems (SensEval-3), pages 25–28, Barcelona,
Spain.
George Miller, Martin Chodorow, Shari Landes, Clau-
dia Leacock, and Robert Thomas. 1994. Using a
semantic concordance for sense identification. In
Proceedings ofARPA Human Language Technology
Workshop, pages 240–243, Morristown, New Jersey,
USA.
George Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography,
3(4):235–312.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 40–47,
Santa Cruz, California, USA.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 440–447, Hong
Kong.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. En-
glish tasks: All-words and verb lexical sample. In
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems
(SensEval-2), pages 21–24, Toulouse, France.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87–92,
Prague, Czech Republic.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the Third
International Workshop on Evaluating Word Sense
Disambiguation Systems (SensEval-3), pages 41–
43, Barcelona, Spain.
Christopher Stokoe, Michael P. Oakes, and John Tait.
2003. Word sense disambiguation in information
retrieval revisited. In Proceedings of the Twenty-
Sixth Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval (SIGIR), pages 159–166, Toronto, Canada.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
David Yarowsky, Radu Florian, Siviu Cucerzan, and
Charles Schafer. 2001. The Johns Hopkins
SensEval-2 system description. In Proceedings of
the Second International Workshop on Evaluating
Word Sense Disambiguation Systems (SensEval-2),
pages 163–166, Toulouse, France.
</reference>
<page confidence="0.999311">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.320077">
<title confidence="0.874186">It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text</title>
<author confidence="0.993515">Zhong Tou Ng</author>
<affiliation confidence="0.9998795">Department of Computer Science National University of Singapore</affiliation>
<address confidence="0.906641">13 Computing Drive</address>
<date confidence="0.52005">Singapore 117417</date>
<abstract confidence="0.995793523809524">Word sense disambiguation (WSD) systems based on supervised learning achieved the best performance in SensEval and SemEval workshops. However, there are few publicly available open source WSD systems. This limits the use of WSD in other applications, especially for researchers whose research interests are not in WSD. In this paper, we present IMS, a supervised English all-words WSD system. The flexible framework of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1282" citStr="Carpuat and Wu, 2007" startWordPosition="189" endWordPosition="192">k of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few open source WSD systems are</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Scaling up word sense disambiguation via parallel texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1037--1042</pages>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="11210" citStr="Chan and Ng (2005)" startWordPosition="1805" endWordPosition="1808">//maxent.sourceforge.net/ 80 2.2 The Training Data Set for All-Words Tasks Once we have a supervised WSD system, for the users who only need WSD as a component in their applications, it is also important to provide them the classification models. The performance of a supervised WSD system greatly depends on the size of the sense-annotated training data used. To overcome the lack of sense-annotated training examples, besides the training instances from the widely used sense-annotated corpus SEMCOR (Miller et al., 1994) and DSO corpus (Ng and Lee, 1996), we also follow the approach described in Chan and Ng (2005) to extract more training examples from parallel texts. The process of extracting training examples from parallel texts is as follows: • Collect a set of sentence-aligned parallel texts. In our case, we use six English-Chinese parallel corpora: Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and the English translation of Chinese Treebank. They are all available from the Linguistic Data Consortium (LDC). • Perform tokenization on the English texts with the Penn TreeBank tokenizer. • Perform Chinese word segmentation on the Chinese texts with the Chinese word segmenta</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI), pages 1037–1042, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>33--40</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1258" citStr="Chan et al., 2007" startWordPosition="185" endWordPosition="188">he flexible framework of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few ope</context>
<context position="3156" citStr="Chan et al., 2007" startWordPosition="487" endWordPosition="490"> preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, SEMCOR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensE78 Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden, 13 Ju</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007a. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 33–40, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>Zhi Zhong</author>
</authors>
<title>NUS-PT: Exploiting parallel texts for word sense disambiguation in the English all-words tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>253--256</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1258" citStr="Chan et al., 2007" startWordPosition="185" endWordPosition="188">he flexible framework of IMS allows users to integrate different preprocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few ope</context>
<context position="3156" citStr="Chan et al., 2007" startWordPosition="487" endWordPosition="490"> preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, SEMCOR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensE78 Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden, 13 Ju</context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007b. NUS-PT: Exploiting parallel texts for word sense disambiguation in the English all-words tasks. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 253–256, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Decadt</author>
<author>Veronique Hoste</author>
<author>Walter Daelemans</author>
</authors>
<title>GAMBL, genetic algorithm optimization of memory-based WSD.</title>
<date>2004</date>
<booktitle>In Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3),</booktitle>
<pages>108--112</pages>
<location>Barcelona,</location>
<contexts>
<context position="6371" citStr="Decadt et al., 2004" startWordPosition="999" endWordPosition="1002">sed for sentence splitting and POS tagging. A Java version of Penn TreeBank tokenizer2 is applied in tokenization. JWNL3, a Java API for accessing the WordNet (Miller, 1990) thesaurus, is used to find the lemma form of each token. 2.1.2 Feature and Instance Extraction After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features. Previous research has found that combining multiple knowledge sources achieves high WSD accuracy (Ng and Lee, 1996; Lee and Ng, 2002; Decadt et al., 2004). In IMS, we follow Lee and Ng (2002) and combine three knowledge sources for all content word types4: • POS Tags of Surrounding Words We use the POS tags of three words to the left and three words to the right of the target ambiguous word, and the target word itself. The POS tag feature cannot cross sentence boundary, which means all the associated surrounding words should be in the same sentence as the target word. If a word crosses sentence boundary, the corresponding POS tag value will be assigned as null. For example, suppose we want to disambiguate the word interest in a POS-tagged sente</context>
</contexts>
<marker>Decadt, Hoste, Daelemans, 2004</marker>
<rawString>Bart Decadt, Veronique Hoste, and Walter Daelemans. 2004. GAMBL, genetic algorithm optimization of memory-based WSD. In Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3), pages 108– 112, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9595" citStr="Fan et al., 2008" startWordPosition="1552" endWordPosition="1555">”. As shown in Figure 1, we implement one feature extractor for each feature type. The IMS software package is organized in such a way that users can easily specify their own feature set by implementing more feature extractors to exploit new features. 2.1.3 Classification In IMS, the classifier trains a model for each word type which has training data during the training process. The instances collected in the previous step are converted to the format expected by the machine learning toolkit in use. Thus, the classification step is separate from the feature extraction step. We use LIBLINEAR5 (Fan et al., 2008) as the default classifier of IMS, with a linear kernel and all the parameters set to their default values. Accordingly, we implement an interface to convert the instances into the LIBLINEAR feature vector format. The utilization of other machine learning software can be achieved by implementing the corresponding module interfaces to them. For instance, IMS provides module interfaces to the WEKA machine learning toolkit (Witten and Frank, 2005), LIBSVM6, and MaxEnt7. The trained classification models will be applied to the test instances of the corresponding word types in the testing process. </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>41--48</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="2738" citStr="Lee and Ng (2002)" startWordPosition="422" endWordPosition="425">es or unsupervised methods. An open source supervised WSD system will promote the use of WSD in other applications. In this paper, we present an English all-words WSD system, IMS (It Makes Sense), built using a supervised learning approach. IMS is a Java implementation, which provides an extensible and flexible platform for researchers interested in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, SEMCOR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves</context>
<context position="6349" citStr="Lee and Ng, 2002" startWordPosition="995" endWordPosition="998">NLP toolkit1 are used for sentence splitting and POS tagging. A Java version of Penn TreeBank tokenizer2 is applied in tokenization. JWNL3, a Java API for accessing the WordNet (Miller, 1990) thesaurus, is used to find the lemma form of each token. 2.1.2 Feature and Instance Extraction After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features. Previous research has found that combining multiple knowledge sources achieves high WSD accuracy (Ng and Lee, 1996; Lee and Ng, 2002; Decadt et al., 2004). In IMS, we follow Lee and Ng (2002) and combine three knowledge sources for all content word types4: • POS Tags of Surrounding Words We use the POS tags of three words to the left and three words to the right of the target ambiguous word, and the target word itself. The POS tag feature cannot cross sentence boundary, which means all the associated surrounding words should be in the same sentence as the target word. If a word crosses sentence boundary, the corresponding POS tag value will be assigned as null. For example, suppose we want to disambiguate the word interest</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 41–48, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>Jeju Island,</location>
<contexts>
<context position="11851" citStr="Low et al. (2005)" startWordPosition="1906" endWordPosition="1909">g examples from parallel texts. The process of extracting training examples from parallel texts is as follows: • Collect a set of sentence-aligned parallel texts. In our case, we use six English-Chinese parallel corpora: Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and the English translation of Chinese Treebank. They are all available from the Linguistic Data Consortium (LDC). • Perform tokenization on the English texts with the Penn TreeBank tokenizer. • Perform Chinese word segmentation on the Chinese texts with the Chinese word segmentation method proposed by Low et al. (2005). • Perform word alignment on the parallel texts using the GIZA++ software (Och and Ney, 2000). • Assign Chinese translations to each sense of an English word w. • Pick the occurrences of w which are aligned to its chosen Chinese translations in the word alignment output of GIZA++. • Identify the senses of the selected occurrences of w by referring to their aligned Chinese translations. Finally, the English side of these selected occurrences together with their assigned senses are used as training data. We only extract training examples from parallel texts for the top 60% most frequently occur</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>SenseLearner: Word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL) Interactive Poster and Demonstration Sessions,</booktitle>
<pages>53--56</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="2013" citStr="Mihalcea and Csomai, 2005" startWordPosition="307" endWordPosition="310"> approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available – the only other publicly available WSD system that we are aware of is SenseLearner (Mihalcea and Csomai, 2005). Therefore, for applications which employ WSD as a component, researchers can only make use of some baselines or unsupervised methods. An open source supervised WSD system will promote the use of WSD in other applications. In this paper, we present an English all-words WSD system, IMS (It Makes Sense), built using a supervised learning approach. IMS is a Java implementation, which provides an extensible and flexible platform for researchers interested in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extract</context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2005. SenseLearner: Word sense disambiguation for all words in unrestricted text. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL) Interactive Poster and Demonstration Sessions, pages 53–56, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Pattern learning and active feature selection for word sense disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2),</booktitle>
<pages>127--130</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="14417" citStr="Mihalcea and Moldovan, 2001" startWordPosition="2344" endWordPosition="2347">mple Tasks SensEval-2 SensEval-3 IMS 65.3% 72.6% Rank 1 System 64.2% 72.9% Rank 2 System 63.8% 72.6% MFS 47.6% 55.2% Table 2: WSD accuracies on SensEval lexicalsample tasks In SensEval English lexical-sample tasks, both the training and test data sets are provided. A common baseline for lexical-sample task is to select the most frequent sense (MFS) in the training data as the answer. We evaluate IMS on the SensEval-2 and SensEval-3 English lexical-sample tasks. Table 2 compares the performance of our system to the top 81 two systems that participated in the above tasks (Yarowsky et al., 2001; Mihalcea and Moldovan, 2001; Mihalcea et al., 2004). Evaluation results show that IMS achieves significantly better accuracies than the MFS baseline. Comparing to the top participating systems, IMS achieves comparable results. 3.2 English All-Words Tasks In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 2001. Pattern learning and active feature selection for word sense disambiguation. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2), pages 127–130, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The SensEval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>In Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3),</booktitle>
<pages>25--28</pages>
<location>Barcelona,</location>
<contexts>
<context position="14441" citStr="Mihalcea et al., 2004" startWordPosition="2348" endWordPosition="2351">l-3 IMS 65.3% 72.6% Rank 1 System 64.2% 72.9% Rank 2 System 63.8% 72.6% MFS 47.6% 55.2% Table 2: WSD accuracies on SensEval lexicalsample tasks In SensEval English lexical-sample tasks, both the training and test data sets are provided. A common baseline for lexical-sample task is to select the most frequent sense (MFS) in the training data as the answer. We evaluate IMS on the SensEval-2 and SensEval-3 English lexical-sample tasks. Table 2 compares the performance of our system to the top 81 two systems that participated in the above tasks (Yarowsky et al., 2001; Mihalcea and Moldovan, 2001; Mihalcea et al., 2004). Evaluation results show that IMS achieves significantly better accuracies than the MFS baseline. Comparing to the top participating systems, IMS achieves comparable results. 3.2 English All-Words Tasks In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks. Using the t</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The SensEval-3 English lexical sample task. In Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3), pages 25–28, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Martin Chodorow</author>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Robert Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proceedings ofARPA Human Language Technology Workshop,</booktitle>
<pages>240--243</pages>
<location>Morristown, New Jersey, USA.</location>
<contexts>
<context position="3045" citStr="Miller et al., 1994" startWordPosition="466" endWordPosition="469">flexible platform for researchers interested in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, SEMCOR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation r</context>
<context position="11115" citStr="Miller et al., 1994" startWordPosition="1787" endWordPosition="1790">er. 5http://www.bwaldvogel.de/ liblinear-java/ 6http://www.csie.ntu.edu.tw/-cjlin/ libsvm/ 7http://maxent.sourceforge.net/ 80 2.2 The Training Data Set for All-Words Tasks Once we have a supervised WSD system, for the users who only need WSD as a component in their applications, it is also important to provide them the classification models. The performance of a supervised WSD system greatly depends on the size of the sense-annotated training data used. To overcome the lack of sense-annotated training examples, besides the training instances from the widely used sense-annotated corpus SEMCOR (Miller et al., 1994) and DSO corpus (Ng and Lee, 1996), we also follow the approach described in Chan and Ng (2005) to extract more training examples from parallel texts. The process of extracting training examples from parallel texts is as follows: • Collect a set of sentence-aligned parallel texts. In our case, we use six English-Chinese parallel corpora: Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and the English translation of Chinese Treebank. They are all available from the Linguistic Data Consortium (LDC). • Perform tokenization on the English texts with the Penn TreeBank tok</context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>George Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert Thomas. 1994. Using a semantic concordance for sense identification. In Proceedings ofARPA Human Language Technology Workshop, pages 240–243, Morristown, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="4570" citStr="Miller, 1990" startWordPosition="709" endWordPosition="710">uce the default preprocessing tools, the feature types, and the machine learning method used in our implementation. Then we briefly explain the collection of training data for content words. 2.1 System Architecture Figure 1 shows the system architecture of IMS. The system accepts any input text. For each content word w (noun, verb, adjective, or adverb) in the input text, IMS disambiguates the sense of w and outputs a list of the senses of w, where each sense si is assigned a probability according to the likelihood of si appearing in that context. The sense inventory used is based on WordNet (Miller, 1990) version 1.7.1. IMS consists of three independent modules: preprocessing, feature and instance extraction, and classification. Knowledge sources are generated from input texts in the preprocessing step. With these knowledge sources, instances together with their features are extracted in the instance and feature extraction step. Then we train one classification model for each word type. The model will be used to classify test instances of the corresponding word type. 2.1.1 Preprocessing Preprocessing is the step to convert input texts into formatted information. Users can integrate different t</context>
<context position="5924" citStr="Miller, 1990" startWordPosition="931" endWordPosition="932">etc. The extracted knowledge sources are stored for use in the later steps. In IMS, preprocessing is carried out in four steps: • Detect the sentence boundaries in a raw input text with a sentence splitter. • Tokenize the split sentences with a tokenizer. • Assign POS tags to all tokens with a POS tagger. • Find the lemma form of each token with a lemmatizer. By default, the sentence splitter and POS tagger in the OpenNLP toolkit1 are used for sentence splitting and POS tagging. A Java version of Penn TreeBank tokenizer2 is applied in tokenization. JWNL3, a Java API for accessing the WordNet (Miller, 1990) thesaurus, is used to find the lemma form of each token. 2.1.2 Feature and Instance Extraction After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features. Previous research has found that combining multiple knowledge sources achieves high WSD accuracy (Ng and Lee, 1996; Lee and Ng, 2002; Decadt et al., 2004). In IMS, we follow Lee and Ng (2002) and combine three knowledge sources for all content word types4: • POS Tags of Surrounding Words We use the POS tag</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>40--47</pages>
<location>Santa Cruz, California, USA.</location>
<contexts>
<context position="3084" citStr="Ng and Lee, 1996" startWordPosition="474" endWordPosition="477">ed in using a WSD component. Users can choose different tools to perform preprocessing, such as trying out various features in the feature extraction step, and applying different machine learning methods or toolkits in the classification step. Following Lee and Ng (2002), we adopt support vector machines (SVM) as the classifier and integrate multiple knowledge sources including parts-of-speech (POS), surrounding words, and local collocations as features. We also provide classification models trained with examples collected from parallel texts, SEMCOR (Miller et al., 1994), and the DSO corpus (Ng and Lee, 1996). A previous implementation of the IMS system, NUS-PT (Chan et al., 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and finegrained task, respectively. Our current IMS implementation achieves competitive accuracies on several SensEval/SemEval English lexical-sample and all-words tasks. The remainder of this paper is organized as follows. Section 2 gives the system description, which introduces the system framework and the details of the implementation. In Section 3, we present the evaluation results of IMS on SensE78 Proceedings of</context>
<context position="6331" citStr="Ng and Lee, 1996" startWordPosition="991" endWordPosition="994">tagger in the OpenNLP toolkit1 are used for sentence splitting and POS tagging. A Java version of Penn TreeBank tokenizer2 is applied in tokenization. JWNL3, a Java API for accessing the WordNet (Miller, 1990) thesaurus, is used to find the lemma form of each token. 2.1.2 Feature and Instance Extraction After gathering the formatted information in the preprocessing step, we use an instance extractor together with a list of feature extractors to extract the instances and their associated features. Previous research has found that combining multiple knowledge sources achieves high WSD accuracy (Ng and Lee, 1996; Lee and Ng, 2002; Decadt et al., 2004). In IMS, we follow Lee and Ng (2002) and combine three knowledge sources for all content word types4: • POS Tags of Surrounding Words We use the POS tags of three words to the left and three words to the right of the target ambiguous word, and the target word itself. The POS tag feature cannot cross sentence boundary, which means all the associated surrounding words should be in the same sentence as the target word. If a word crosses sentence boundary, the corresponding POS tag value will be assigned as null. For example, suppose we want to disambiguate</context>
<context position="11149" citStr="Ng and Lee, 1996" startWordPosition="1794" endWordPosition="1797">ear-java/ 6http://www.csie.ntu.edu.tw/-cjlin/ libsvm/ 7http://maxent.sourceforge.net/ 80 2.2 The Training Data Set for All-Words Tasks Once we have a supervised WSD system, for the users who only need WSD as a component in their applications, it is also important to provide them the classification models. The performance of a supervised WSD system greatly depends on the size of the sense-annotated training data used. To overcome the lack of sense-annotated training examples, besides the training instances from the widely used sense-annotated corpus SEMCOR (Miller et al., 1994) and DSO corpus (Ng and Lee, 1996), we also follow the approach described in Chan and Ng (2005) to extract more training examples from parallel texts. The process of extracting training examples from parallel texts is as follows: • Collect a set of sentence-aligned parallel texts. In our case, we use six English-Chinese parallel corpora: Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and the English translation of Chinese Treebank. They are all available from the Linguistic Data Consortium (LDC). • Perform tokenization on the English texts with the Penn TreeBank tokenizer. • Perform Chinese word seg</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), pages 40–47, Santa Cruz, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>440--447</pages>
<location>Hong Kong.</location>
<contexts>
<context position="11945" citStr="Och and Ney, 2000" startWordPosition="1922" endWordPosition="1925">ts is as follows: • Collect a set of sentence-aligned parallel texts. In our case, we use six English-Chinese parallel corpora: Hong Kong Hansards, Hong Kong News, Hong Kong Laws, Sinorama, Xinhua News, and the English translation of Chinese Treebank. They are all available from the Linguistic Data Consortium (LDC). • Perform tokenization on the English texts with the Penn TreeBank tokenizer. • Perform Chinese word segmentation on the Chinese texts with the Chinese word segmentation method proposed by Low et al. (2005). • Perform word alignment on the parallel texts using the GIZA++ software (Och and Ney, 2000). • Assign Chinese translations to each sense of an English word w. • Pick the occurrences of w which are aligned to its chosen Chinese translations in the word alignment output of GIZA++. • Identify the senses of the selected occurrences of w by referring to their aligned Chinese translations. Finally, the English side of these selected occurrences together with their assigned senses are used as training data. We only extract training examples from parallel texts for the top 60% most frequently occurring polysemous content words in Brown Corpus (BC), which includes 730 nouns, 190 verbs, and 3</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 440–447, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All-words and verb lexical sample.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2),</booktitle>
<pages>21--24</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1467" citStr="Palmer et al., 2001" startWordPosition="218" endWordPosition="221">multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available – the only other publicly available WSD system that we are aware of is SenseLearner (Mihalcea and Csomai, 2005). Therefore, for applications which employ WSD as a co</context>
<context position="15335" citStr="Palmer et al., 2001" startWordPosition="2494" endWordPosition="2497">rovided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks. Using the training data collected with the method described in Section 2.2, we apply our system on the SensEval-2, SensEval-3, and SemEval2007 English all-words tasks. Similarly, we also compare the performance of our system to the top two systems that participated in the above tasks (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). The evaluation results are shown in Table 3. IMS easily beats the WNs1 baseline. It ranks first in SensEval-3 English finegrained all-words task and SemEval-2007 English coarse-grained all-words task, and is also competitive in the remaining tasks. It is worth noting that because of the small test data set in SemEval2007 English fine-grained all-words task, the differences between IMS and the best participating systems are not statistically significant. Overall, IMS achieves good WSD accuracies on both all-words and lexical-sample tasks. The pe</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All-words and verb lexical sample. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2), pages 21–24, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval-2007 task-17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1515" citStr="Pradhan et al., 2007" startWordPosition="227" endWordPosition="230">ementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available – the only other publicly available WSD system that we are aware of is SenseLearner (Mihalcea and Csomai, 2005). Therefore, for applications which employ WSD as a component, researchers can only make use of some b</context>
<context position="15383" citStr="Pradhan et al., 2007" startWordPosition="2502" endWordPosition="2505">ger suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks. Using the training data collected with the method described in Section 2.2, we apply our system on the SensEval-2, SensEval-3, and SemEval2007 English all-words tasks. Similarly, we also compare the performance of our system to the top two systems that participated in the above tasks (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). The evaluation results are shown in Table 3. IMS easily beats the WNs1 baseline. It ranks first in SensEval-3 English finegrained all-words task and SemEval-2007 English coarse-grained all-words task, and is also competitive in the remaining tasks. It is worth noting that because of the small test data set in SemEval2007 English fine-grained all-words task, the differences between IMS and the best participating systems are not statistically significant. Overall, IMS achieves good WSD accuracies on both all-words and lexical-sample tasks. The performance of IMS shows that it is a state-of-the</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. SemEval-2007 task-17: English lexical sample, SRL and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3),</booktitle>
<pages>41--43</pages>
<location>Barcelona,</location>
<contexts>
<context position="1492" citStr="Snyder and Palmer, 2004" startWordPosition="222" endWordPosition="226">sed features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available – the only other publicly available WSD system that we are aware of is SenseLearner (Mihalcea and Csomai, 2005). Therefore, for applications which employ WSD as a component, researchers can </context>
<context position="15360" citStr="Snyder and Palmer, 2004" startWordPosition="2498" endWordPosition="2501">he MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will use it as the baseline in all-words tasks. Using the training data collected with the method described in Section 2.2, we apply our system on the SensEval-2, SensEval-3, and SemEval2007 English all-words tasks. Similarly, we also compare the performance of our system to the top two systems that participated in the above tasks (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). The evaluation results are shown in Table 3. IMS easily beats the WNs1 baseline. It ranks first in SensEval-3 English finegrained all-words task and SemEval-2007 English coarse-grained all-words task, and is also competitive in the remaining tasks. It is worth noting that because of the small test data set in SemEval2007 English fine-grained all-words task, the differences between IMS and the best participating systems are not statistically significant. Overall, IMS achieves good WSD accuracies on both all-words and lexical-sample tasks. The performance of IMS shows th</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English all-words task. In Proceedings of the Third International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-3), pages 41– 43, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Stokoe</author>
<author>Michael P Oakes</author>
<author>John Tait</author>
</authors>
<title>Word sense disambiguation in information retrieval revisited.</title>
<date>2003</date>
<booktitle>In Proceedings of the TwentySixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>159--166</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="1330" citStr="Stokoe et al., 2003" startWordPosition="196" endWordPosition="199">rocessing tools, additional features, and different classifiers. By default, we use linear support vector machines as the classifier with multiple knowledge-based features. In our implementation, IMS achieves state-of-the-art results on several SensEval and SemEval tasks. 1 Introduction Word sense disambiguation (WSD) refers to the task of identifying the correct sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007a; Carpuat and Wu, 2007) and information retrieval (Stokoe et al., 2003). In previous SensEval workshops, the supervised learning approach has proven to be the most successful WSD approach (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In the most recent SemEval-2007 English all-words tasks, most of the top systems were based on supervised learning methods. These systems used a set of knowledge sources drawn from senseannotated data, and achieved significant improvements over the baselines. However, developing such a system requires much effort. As a result, very few open source WSD systems are publicly available – the only other publicly av</context>
</contexts>
<marker>Stokoe, Oakes, Tait, 2003</marker>
<rawString>Christopher Stokoe, Michael P. Oakes, and John Tait. 2003. Word sense disambiguation in information retrieval revisited. In Proceedings of the TwentySixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 159–166, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>edition.</title>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="10043" citStr="Witten and Frank, 2005" startWordPosition="1623" endWordPosition="1626">to the format expected by the machine learning toolkit in use. Thus, the classification step is separate from the feature extraction step. We use LIBLINEAR5 (Fan et al., 2008) as the default classifier of IMS, with a linear kernel and all the parameters set to their default values. Accordingly, we implement an interface to convert the instances into the LIBLINEAR feature vector format. The utilization of other machine learning software can be achieved by implementing the corresponding module interfaces to them. For instance, IMS provides module interfaces to the WEKA machine learning toolkit (Witten and Frank, 2005), LIBSVM6, and MaxEnt7. The trained classification models will be applied to the test instances of the corresponding word types in the testing process. If a test instance word type is not seen during training, we will output its predefined default sense, i.e., the WordNet first sense, as the answer. Furthermore, if a word type has neither training data nor predefined default sense, we will output “U”, which stands for the missing sense, as the answer. 5http://www.bwaldvogel.de/ liblinear-java/ 6http://www.csie.ntu.edu.tw/-cjlin/ libsvm/ 7http://maxent.sourceforge.net/ 80 2.2 The Training Data </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
<author>Siviu Cucerzan</author>
<author>Charles Schafer</author>
</authors>
<title>The Johns Hopkins SensEval-2 system description.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2),</booktitle>
<pages>163--166</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="14388" citStr="Yarowsky et al., 2001" startWordPosition="2340" endWordPosition="2343"> 3.1 English Lexical-Sample Tasks SensEval-2 SensEval-3 IMS 65.3% 72.6% Rank 1 System 64.2% 72.9% Rank 2 System 63.8% 72.6% MFS 47.6% 55.2% Table 2: WSD accuracies on SensEval lexicalsample tasks In SensEval English lexical-sample tasks, both the training and test data sets are provided. A common baseline for lexical-sample task is to select the most frequent sense (MFS) in the training data as the answer. We evaluate IMS on the SensEval-2 and SensEval-3 English lexical-sample tasks. Table 2 compares the performance of our system to the top 81 two systems that participated in the above tasks (Yarowsky et al., 2001; Mihalcea and Moldovan, 2001; Mihalcea et al., 2004). Evaluation results show that IMS achieves significantly better accuracies than the MFS baseline. Comparing to the top participating systems, IMS achieves comparable results. 3.2 English All-Words Tasks In SensEval and SemEval English all-words tasks, no training data are provided. Therefore, the MFS baseline is no longer suitable for all-words tasks. Because the order of senses in WordNet is based on the frequency of senses in SEMCOR, the WordNet first sense (WNs1) baseline always assigns the first sense in WordNet as the answer. We will u</context>
</contexts>
<marker>Yarowsky, Florian, Cucerzan, Schafer, 2001</marker>
<rawString>David Yarowsky, Radu Florian, Siviu Cucerzan, and Charles Schafer. 2001. The Johns Hopkins SensEval-2 system description. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SensEval-2), pages 163–166, Toulouse, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>