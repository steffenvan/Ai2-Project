<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001551">
<title confidence="0.982289">
Large-Coverage Root Lexicon Extraction for Hindi
</title>
<author confidence="0.973349">
Cohan Sujay Carlos Monojit Choudhury Sandipan Dandapat
</author>
<affiliation confidence="0.972001">
Microsoft Research India
</affiliation>
<email confidence="0.994178">
monojitc@microsoft.com
</email>
<sectionHeader confidence="0.993786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852692307692">
This paper describes a method using mor-
phological rules and heuristics, for the au-
tomatic extraction of large-coverage lexi-
cons of stems and root word-forms from
a raw text corpus. We cast the problem
of high-coverage lexicon extraction as one
of stemming followed by root word-form
selection. We examine the use of POS
tagging to improve precision and recall of
stemming and thereby the coverage of the
lexicon. We present accuracy, precision
and recall scores for the system on a Hindi
corpus.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964222222223">
Large-coverage morphological lexicons are an es-
sential component of morphological analysers.
Morphological analysers find application in lan-
guage processing systems for tasks like tagging,
parsing and machine translation. While raw text
is an abundant and easily accessible linguistic re-
source, high-coverage morphological lexicons are
scarce or unavailable in Hindi as in many other
languages (Cl´ement et al., 2004). Thus, the devel-
opment of better algorithms for the extraction of
morphological lexicons from raw text corpora is a
task of considerable importance.
A root word-form lexicon is an intermediate
stage in the creation of a morphological lexicon.
In this paper, we consider the problem of extract-
ing a large-coverage root word-form lexicon for
the Hindi language, a highly inflectional and mod-
erately agglutinative Indo-European language spo-
ken widely in South Asia.
Since a POS tagger, another basic tool, was
available along with POS tagged data to train it,
and since the error patterns indicated that POS tag-
ging could greatly improve the accuracy of the lex-
icon, we used the POS tagger in our experiments
on lexicon extraction.
Previous work in morphological lexicon extrac-
tion from a raw corpus often does not achieve very
high precision and recall (de Lima, 1998; Oliver
and Tadi´c, 2004). In some previous work the pro-
cess of lexicon extraction involves incremental or
post-construction manual validation of the entire
lexicon (Cl´ement et al., 2004; Sagot, 2005; Fors-
berg et al., 2006; Sagot et al., 2006; Sagot, 2007).
Our method attempts to improve on and extend
the previous work by increasing the precision and
recall of the system to such a point that manual
validation might even be rendered unnecessary.
Yet another difference, to our knowledge, is that
in our method we cast the problem of lexicon ex-
traction as two subproblems: that of stemming and
following it, that of root word-form selection.
The input resources for our system are as fol-
lows: a) raw text corpus, b) morphological rules,
c) POS tagger and d) word-segmentation labelled
data. We output a stem lexicon and a root word-
form lexicon.
We take as input a raw text corpus and a set
of morphological rules. We first run a stemming
algorithm that uses the morphological rules and
some heuristics to obtain a stem dictionary. We
then create a root dictionary from the stem dictio-
nary.
The last two input resources are optional but
when a POS tagger is utilized, the F-score (har-
monic mean of precision and recall) of the root
lexicon can be as high as 94.6%.
In the rest of the paper, we provide a brief
overview of the morphological features of the
Hindi language, followed by a description of our
method including the specification of rules, the
corpora and the heuristics for stemming and root
word-form selection. We then evaluate the system
with and without the POS tagger.
</bodyText>
<note confidence="0.9230405">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 121–129,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.995508">
121
</page>
<sectionHeader confidence="0.782945" genericHeader="introduction">
2 Hindi Orthography and Morphology
</sectionHeader>
<bodyText confidence="0.9999715">
There are some features peculiar to Hindi orthog-
raphy and to the character encoding system that
we use. These need to be compensated for in the
system. It was also found that Hindi’s inflectional
morphology has certain characteristics that sim-
plify the word segmentation rules.
</bodyText>
<subsectionHeader confidence="0.831598">
2.1 Orthography
</subsectionHeader>
<bodyText confidence="0.999956956521739">
Hindi is written in the partially-phonemic Devana-
gari script. Most consonant clusters that occur in
the language are represented by characters and lig-
atures, while a very few are represented as diacrit-
ics. Vowels that follow consonants or consonant
clusters are marked with diacritics. However, each
consonant in the Devanagari script also carries an
implicit vowel a1 unless its absence is marked by a
special diacritic “halant”. Vowels are represented
by vowel characters when they occur at the head
of a word or after another vowel.
The y sound sometimes does not surface in the
pronunciation when it occurs between two vow-
els. So suffixes where the y is followed by e or I
can be written in two ways, with or without the y
sound in them. For instance the suffix ie can also
be written as iye.
Certain stemming rules will therefore need to
be duplicated in order to accommodate the differ-
ent spelling possibilities and the different vowel
representations in Hindi. The character encoding
also plays a small but significant role in the ease
of stemming of Hindi word-forms.
</bodyText>
<subsectionHeader confidence="0.999388">
2.2 Unicode Representation
</subsectionHeader>
<bodyText confidence="0.999944153846154">
We used Unicode to encode Hindi characters. The
Unicode representation of Devanagari treats sim-
ple consonants and vowels as separate units and so
makes it easier to match substrings at consonant-
vowel boundaries. Ligatures and diacritical forms
of consonants are therefore represented by the
same character code and they can be equated very
simply.
However, when using Unicode as the charac-
ter encoding, it must be borne in mind that there
are different character codes for the vowel diacrit-
ics and for the vowel characters for one and the
same vowel sound, and that the long and short
</bodyText>
<footnote confidence="0.528132">
1In the discussion in Section 2 and in Table 1 and
Table 2, we have used a loose phonetic transcription
that resembles ITRANS (developed by Avinash Chopde
http://www.aczoom.com/itrans/).
</footnote>
<table confidence="0.999680625">
Word Form Derivational Segmentation Root
karnA kar + nA kar
karAnA kar + A + nA kar
karvAnA kar + vA + nA kar
Word Form Inflectional Segmentation Root
karnA kar + nA kar
karAnA karA + nA karA
karvAnA karvA + nA karvA
</table>
<tableCaption confidence="0.96596">
Table 1: Morpheme Segmentation
</tableCaption>
<table confidence="0.999727">
laDkA Nominative Oblique
Singular laDkA laDke
Plural laDke laDkon
laDkI Nominative Oblique
Singular laDkI laDkI
Plural laDkI laDkiyAn
</table>
<tableCaption confidence="0.996887">
Table 2: Sample Paradigms
</tableCaption>
<bodyText confidence="0.999953333333333">
forms of the vowels are represented by different
codes. These artifacts of the character encoding
need to be compensated for when using substring
matches to identify the short vowel sound as being
part of the corresponding prolonged vowel sound
and when stemming.
</bodyText>
<subsectionHeader confidence="0.994421">
2.3 Morphology
</subsectionHeader>
<bodyText confidence="0.999991909090909">
The inflectional morphology of Hindi does not
permit agglutination. This helps keep the num-
ber of inflectional morphological rules manage-
able. However, the derivational suffixes are agglu-
tinative, leading to an explosion in the number of
root word-forms in the inflectional root lexicon.
The example in Table 1 shows that verbs can
take one of the two causative suffixes A and vA.
These being derivational suffixes are not stemmed
in our system and cause the verb lexicon to be
larger than it would have otherwise.
</bodyText>
<subsectionHeader confidence="0.984723">
2.4 Paradigms
</subsectionHeader>
<bodyText confidence="0.999684777777778">
Nouns, verbs and adjectives are the main POS cat-
egories that undergo inflection in Hindi according
to regular paradigm rules.
For example, Hindi nouns inflect for case and
number. The inflections for the paradigms that the
words laDkA (meaning boy) and laDkI (mean-
ing girl) belong to are shown in Table 2. The root
word-forms are laDkA and laDkI respectively
(the singular and nominative forms).
</bodyText>
<page confidence="0.996457">
122
</page>
<bodyText confidence="0.999738666666667">
Hindi verbs are inflected by gender, number,
person, mood and tense. Hindi adjectives take
inflections for gender and case. The number of
inflected forms in different POS categories varies
considerably, with verbs tending to have a lot more
inflections than other POS categories.
</bodyText>
<sectionHeader confidence="0.984911" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.997342285714286">
In order to construct a morphological lexicon, we
used a rule-based approach combined with heuris-
tics for stem and root selection. When used in
concert with a POS tagger, they could extract a
very accurate morphological lexicon from a raw
text corpus. Our system therefore consists of the
following components:
</bodyText>
<listItem confidence="0.96821725">
1. A raw text corpus in the Hindi language large
enough to contain a few hundred thousand
unique word-forms and a smaller labelled
corpus to train a POS tagger with.
2. A list of rules comprising suffix strings and
constraints on the word-forms and POS cate-
gories that they can be applied to.
3. A stemmer that uses the above rules, and
some heuristics to identify and reduce in-
flected word-forms to stems.
4. A POS tagger to identify the POS category or
categories that the word forms in the raw text
corpus can belong to.
5. A root selector that identifies a root word-
form and its paradigm from a stem and a set
of inflections of the stem.
</listItem>
<bodyText confidence="0.9995255">
The components of the system are described in
more detail below.
</bodyText>
<subsectionHeader confidence="0.999043">
3.1 Text Corpora
</subsectionHeader>
<bodyText confidence="0.999692692307692">
Rules alone are not always sufficient to identify
the best stem or root for a word-form, when the
words being stemmed have very few inflectional
forms or when a word might be stemmed in one
of many ways. In that case, a raw text corpus can
provide important clues for identifying them.
The raw text corpus that we use is the Web-
Duniya corpus which consists of 1.4 million sen-
tences of newswire and 21.8 million words. The
corpus, being newswire, is clearly not balanced.
It has a preponderance of third-person forms
whereas first and second person inflectional forms
are under-represented.
</bodyText>
<table confidence="0.9995718">
Name POS Paradigm Suffixes Root
laDkA noun {‘A’,‘e’,‘on’} ‘A’
laDkI noun {‘I’,‘iyAn’} ‘I’
dho verb {‘’,‘yogI’,‘nA’,... } ‘’
chal verb {‘’,‘ogI’,‘nA’,... } ‘’
</table>
<tableCaption confidence="0.999674">
Table 3: Sample Paradigm Suffix Sets
</tableCaption>
<bodyText confidence="0.999900166666666">
Since Hindi word boundaries are clearly marked
with punctuation and spaces, tokenization was
an easy task. The raw text corpus yielded ap-
proximately 331000 unique word-forms. When
words beginning with numbers were removed, we
were left with about 316000 unique word-forms of
which almost half occurred only once in the cor-
pus.
In addition, we needed a corpus of 45,000
words labelled with POS categories using the IL-
POST tagset (Sankaran et al., 2008) for the POS
tagger.
</bodyText>
<subsectionHeader confidence="0.997779">
3.2 Rules
</subsectionHeader>
<bodyText confidence="0.999787772727273">
The morphological rules input into the system are
used to recognize word-forms that together be-
long to a paradigm. Paradigms can be treated as a
set of suffixes that can be used to generate inflec-
tional word-forms from a stem. The set of suffixes
that constitutes a paradigm defines an equivalence
class on the set of unique word-forms in the cor-
pus.
For example, the laDkA paradigm in Table 2
would be represented by the set of suffix strings
{‘A’, ‘e’, ‘on’} derived from the word-forms
laDkA, laDke and laDkon. A few paradigms
are listed in Table 3.
The suffix set formalism of a paradigm closely
resembles the one used in a previous attempt at
unsupervised paradigm extraction (Zeman, 2007)
but differs from it in that Zeman (2007) considers
the set of word-forms that match the paradigm to
be a part of the paradigm definition.
In our system, we represent the morphological
rules by a list of suffix add-delete rules. Each rule
in our method is a five-tuple {α, Q, -y, S, E} where:
</bodyText>
<listItem confidence="0.999275666666667">
• α is the suffix string to be matched for the
rule to apply.
• Q is the portion of the suffix string after which
the stem ends.
• -y is a POS category in which the string α is a
valid suffix.
</listItem>
<page confidence="0.995871">
123
</page>
<table confidence="0.9951844">
α Q ry 6 E
‘A’ ‘’ Noun N1 ‘A’
‘on’ ‘’ Noun N1,N3 ‘A’
‘e’ ‘’ Noun N1 ‘A’
‘oyogI’ ‘o’ Verb V5 ‘o’
</table>
<tableCaption confidence="0.941054">
Table 4: Sample Paradigm Rules
</tableCaption>
<table confidence="0.999127090909091">
BSE Word-forms Accuracy
1 20.5% 79%
2 20.0% 70%
3 13.2% 70%
4 10.8% 81%
5 &amp; more 35.5% 80%
Word Form α Match Stem Root
laDkA laDk + A laDk o laDkA
laDkon laDk + on laDk laDkA
laDke laDk + e laDk laDkA
dhoyogI dh + oyogI dh + dho
</table>
<tableCaption confidence="0.9486625">
Table 5: Rule Application
Table 6: % Frequency and Accuracy by BSE
</tableCaption>
<table confidence="0.7559565">
BSE Nouns Verbs Others
1 292 6 94
2 245 2 136
3 172 15 66
4 120 16 71
5 &amp; more 103 326 112
</table>
<listItem confidence="0.998563333333333">
• 6 is a list of paradigms that contain the suffix
string α.
• E is the root suffix
</listItem>
<bodyText confidence="0.999963380952381">
The sample paradigm rules shown in Table 4
would match the words laDkA, laDkon, laDke
and dhoyogI respectively and cause them to be
stemmed and assigned roots as shown in Table 5.
The rules by themselves can identify word-and-
paradigm entries from the raw text corpus if a suf-
ficient number of inflectional forms were present.
For instance, if the words laDkA and laDkon
were present in the corpus, by taking the intersec-
tion of the paradigms associated with the match-
ing rules in Table 4, it would be possible to infer
that the root word-form was laDkA and that the
paradigm was N1.
We needed to create about 300 rules for Hindi.
The rules could be stored in a list indexed by the
suffix in the case of Hindi because the number of
possible suffixes was small. For highly aggluti-
native languages, such as Tamil and Malayalam,
which can have thousands of suffixes, it would be
necessary to use a Finite State Machine represen-
tation of the rules.
</bodyText>
<subsectionHeader confidence="0.998328">
3.3 Suffix Evidence
</subsectionHeader>
<bodyText confidence="0.999433125">
We define the term ‘suffix evidence’ for a poten-
tial stem as the number of word-forms in the cor-
pus that are composed of a concatenation of the
stem and any valid suffix. For instance, the suf-
fix evidence for the stem laDk is 2 if the word-
forms laDkA and laDkon are the only word-
forms with the prefix laDk that exist in the corpus
and A and on are both valid suffixes.
</bodyText>
<tableCaption confidence="0.986968">
Table 7: Frequency by POS Category
</tableCaption>
<bodyText confidence="0.979605347826087">
Table 6 presents word-form counts for differ-
ent suffix evidence values for the WebDuniya cor-
pus. Since the real stems for the word-forms were
not known, the prefix substring with the highest
suffix evidence was used as the stem. We shall
call this heuristically selected stem the best-suffix-
evidence stem and its suffix evidence as the best-
suffix-evidence (BSE).
It will be seen from Table 6 that about 20% of
the words have a BSE of only 1. Altogether about
40% of the words have a BSE of 1 or 2. Note
that all words have a BSE of atleast 1 since the
empty string is also considered a valid suffix. The
fraction is even higher for nouns as shown in Table
7.
It must be noted that the number of nouns with
a BSE of 5 or more is in the hundreds only be-
cause of erroneous concatenations of suffixes with
stems. Nouns in Hindi do not usually have more
than four inflectional forms.
The scarcity of suffix evidence for most word-
forms poses a huge obstacle to the extraction of a
high-coverage lexicon because:
</bodyText>
<listItem confidence="0.9966192">
1. There are usually multiple ways to pick a
stem from word-forms with a BSE of 1 or 2.
2. Spurious stems cannot be detected easily
when there is no overwhelming suffix evi-
dence in favour of the correct stem.
</listItem>
<subsectionHeader confidence="0.968315">
3.4 Gold Standard
</subsectionHeader>
<bodyText confidence="0.999813">
The gold standard consists of one thousand word-
forms picked at random from the intersection of
</bodyText>
<page confidence="0.993331">
124
</page>
<bodyText confidence="0.999987870967742">
the unique word-forms in the unlabelled Web-
Duniya corpus and the POS labelled corpus. Each
word-form in the gold standard was manually ex-
amined and a stem and a root word-form found for
it.
For word-forms associated with multiple POS
categories, the stem and root of a word-form were
listed once for each POS category because the seg-
mentation of a word could depend on its POS cat-
egory. There were 1913 word and POS category
combinations in the gold standard.
The creation of the stem gold standard needed
some arbitrary choices which had to be reflected
in the rules as well. These concerned some words
which could be stemmed in multiple ways. For in-
stance, the noun laDkT meaning ‘girl’ could be
segmented into the morphemes laDk and T or al-
lowed to remain unsegmented as laDkT. This is
because by doing the former, the stems of both
laDkA and laDkT could be conflated whereas
by doing the latter, they could be kept separate
from each other. We arbitrarily made the choice
to keep nouns ending in T unsegmented and made
our rules reflect that choice.
A second gold standard consisting of 1000
word-forms was also created to be used in eval-
uation and as training data for supervised algo-
rithms. The second gold standard contained 1906
word and POS category combinations. Only word-
forms that did not appear in the first gold standard
were included in the second one.
</bodyText>
<subsectionHeader confidence="0.697461">
3.5 Stemmer
</subsectionHeader>
<bodyText confidence="0.998896166666667">
Since the list of valid suffixes is given, the stem-
mer does not need to discover the stems in the lan-
guage but only learn to apply the right one in the
right place. We experimented with three heuristics
for finding the right stem for a word-form. The
heuristics were:
</bodyText>
<listItem confidence="0.998457666666667">
• Longest Suffix Match (LSM) - Picking the
longest suffix that can be applied to the word-
form.
• Highest Suffix Evidence (HSE) - Picking the
suffix which yields the stem with the highest
value for suffix evidence.
• Highest Suffix Evidence with Supervised
Rule Selection (HSE + Sup) - Using labelled
data to modulate suffix matching.
</listItem>
<subsectionHeader confidence="0.760094">
3.5.1 Longest Suffix Match (LSM)
</subsectionHeader>
<bodyText confidence="0.9999382">
In the LSM heuristic, when multiple suffixes can
be applied to a word-form to stem it, we choose
the longest one. Since Hindi has concatenative
morphology with only postfix inflection, we only
need to find one matching suffix to stem it. It is
claimed in the literature that the method of us-
ing the longest suffix match works better than ran-
dom suffix selection (Sarkar and Bandyopadhyay,
2008). This heuristic was used as the baseline for
our experiments.
</bodyText>
<subsectionHeader confidence="0.896728">
3.5.2 Highest Suffix Evidence (HSE)
</subsectionHeader>
<bodyText confidence="0.999993454545454">
In the HSE heuristic, which has been applied be-
fore to unsupervised morphological segmentation
(Goldsmith, 2001), stemming (Pandey and Sid-
diqui, 2008), and automatic paradigm extraction
(Zeman, 2007), when multiple suffixes can be ap-
plied to stem a word-form, the suffix that is picked
is the one that results in the stem with the high-
est suffix evidence. In our case, when computing
the suffix evidence, the following additional con-
straint is applied: all the suffixes used to compute
the suffix evidence score for any stem must be as-
sociated with the same POS category.
For example, the suffix yon is only applicable
to nouns, whereas the suffix ta is only applicable
to verbs. These two suffixes will therefore never
be counted together in computing the suffix evi-
dence for a stem. The algorithm for determining
the suffix evidence computes the suffix evidence
once for each POS category and then returns the
maximum.
In the absence of this constraint, the accuracy
drops as the size of the raw word corpus increases.
</bodyText>
<sectionHeader confidence="0.812296" genericHeader="method">
3.5.3 HSE and Supervised Rule Selection
(HSE + Sup)
</sectionHeader>
<bodyText confidence="0.999979076923077">
The problem with the aforementioned heuristics is
that there are no weights assigned to rules. Since
the rules for the system were written to be as gen-
eral and flexible as possible, false positives were
commonly encountered. We propose a very sim-
ple supervised learning method to circumvent this
problem.
The training data used was a set of 1000 word-
forms sampled, like the gold standard, from the
unique word-forms in the intersection of the raw
text corpus and the POS labelled corpus. The set
of word-forms in the training data was disjoint
from the set of word-forms in the gold standard.
</bodyText>
<page confidence="0.997637">
125
</page>
<table confidence="0.999019333333333">
Rules Accur Prec Recall F-Score
Rules1 73.65% 68.25% 69.4% 68.8%
Rules2 75.0% 69.0% 77.6% 73.0%
</table>
<tableCaption confidence="0.992812">
Table 8: Comparison of Rules
</tableCaption>
<table confidence="0.999869375">
Gold 1 Accur Prec Recall F-Score
LSM 71.6% 65.8% 66.1% 65.9%
HSE 76.7% 70.6% 77.9% 74.1%
HSE+Sup 78.0% 72.3% 79.8% 75.9%
Gold 2 Accur Prec Recall F-Score
LSM 75.7% 70.7% 72.7% 71.7%
HSE 75.0% 69.0% 77.6% 73.0%
HSE+Sup 75.3% 69.3% 78.0% 73.4%
</table>
<tableCaption confidence="0.999819">
Table 9: Comparison of Heuristics
</tableCaption>
<bodyText confidence="0.999390857142857">
The feature set consisted of two features: the
last character (or diacritic) of the word-form, and
the suffix. The POS category was an optional fea-
ture and used when available. If the number of in-
correct splits exceeded the number of correct splits
given a feature set, the rule was assigned a weight
of 0, else it was given a weight of 1.
</bodyText>
<subsectionHeader confidence="0.527141">
3.5.4 Comparison
</subsectionHeader>
<bodyText confidence="0.999782590909091">
We compare the performance of our rules with
the performance of the Lightweight Stemmer for
Hindi (Ramanathan and Rao, 2003) with a re-
ported accuracy of 81.5%. The scores we report
in Table 8 are the average of the LSM scores
on the two gold standards. The stemmer using
the standard rule-set (Rules1) does not perform as
well as the Lightweight Stemmer. We then hand-
crafted a different set of rules (Rules2) with ad-
justments to maximize its performance. The ac-
curacy was better than Rules1 but not quite equal
to the Lightweight Stemmer. However, since our
gold standard is different from that used to eval-
uate the Lightweight Stemmer, the comparison is
not necessarily very meaningful.
As shown in Table 9, in F-score comparisons,
HSE seems to outperform LSM and HSE+Sup
seems to outperform HSE, but the improvement
in performance is not very large in the case of the
second gold standard. In terms of accuracy scores,
LSM outperforms HSE and HSE+Sup when eval-
uated against the second gold standard.
</bodyText>
<table confidence="0.9981448">
POS Correct Incorrect POS Errors
Noun 749 231 154
Verb 324 108 0
Adjective 227 49 13
Others 136 82 35
</table>
<tableCaption confidence="0.998299">
Table 10: Errors by POS Category
</tableCaption>
<subsectionHeader confidence="0.525036">
3.5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999090333333333">
Table 10 lists the number of correct stems, in-
correct stems, and finally a count of those incor-
rect stems that the HSE+Sup heuristic would have
gotten right if the POS category had been avail-
able. From the numbers it appears that a size-
able fraction of the errors, especially with noun
word-forms, is caused when a suffix of the wrong
POS category is applied to a word-form. More-
over, prior work in Bangla (Sarkar and Bandy-
opadhyay, 2008) indicates that POS category in-
formation could improve the accuracy of stem-
ming.
Assigning POS categories to word-forms re-
quires a POS tagger and a substantial amount of
POS labelled data as described below.
</bodyText>
<subsectionHeader confidence="0.846769">
3.5.6 POS Tagging
</subsectionHeader>
<bodyText confidence="0.9999956">
The POS tagset used was the hierarchical tagset
IL-POST (Sankaran et al., 2008). The hierarchical
tagset supports broad POS categories like nouns
and verbs, less broad POS types like common and
proper nouns and finally, at its finest granularity,
attributes like gender, number, case and mood.
We found that with a training corpus of about
45,000 tagged words (2366 sentences), it was pos-
sible to produce a reasonably accurate POS tag-
ger2, use it to label the raw text corpus with broad
POS tags, and consequently improve the accuracy
of stemming. For our experiments, we used both
the full training corpus of 45,000 words and a sub-
set of the same consisting of about 20,000 words.
The POS tagging accuracies obtained were ap-
proximately 87% and 65% respectively.
The reason for repeating the experiment using
the 20,000 word subset of the training data was to
demonstrate that a mere 20,000 words of labelled
data, which does not take a very great amount of
</bodyText>
<footnote confidence="0.977298166666667">
2The Part-of-Speech tagger used was an implementa-
tion of a Cyclic Dependency Network Part-of-Speech tagger
(Toutanova et al., 2003). The following feature set was used
in the tagger: tag of previous word, tag of next word, word
prefixes and suffixes of length exactly four, bigrams and the
presence of numbers or symbols.
</footnote>
<page confidence="0.997351">
126
</page>
<bodyText confidence="0.999986111111111">
time and effort to create, can produce significant
improvements in stemming performance.
In order to assign tags to the words of the gold
standard, sentences from the raw text corpus con-
taining word-forms present in the gold standard
were tagged using a POS tagger. The POS cate-
gories assigned to each word-form were then read
off and stored in a table.
Once POS tags were associated with all the
words, a more restrictive criterion for matching a
rule to a word-form could be used to calculate the
BSE in order to determine the stem of the word-
form. When searching for rules, and consequently
the suffixes, to be applied to a word-form, only
rules whose -y value matches the word-form’s POS
category were considered. We shall call the HSE
heuristic that uses POS information in this way
HSE+Pos.
</bodyText>
<subsectionHeader confidence="0.99525">
3.6 Root Selection
</subsectionHeader>
<bodyText confidence="0.9876452">
The stem lexicon obtained by the process de-
scribed above had to be converted into a root word-
form lexicon. A root word-form lexicon is in some
cases more useful than a stem lexicon, for the fol-
lowing reasons:
</bodyText>
<listItem confidence="0.979632333333333">
1. Morphological lexicons are traditionally in-
dexed by root word-forms
2. Multiple root word-forms may map to one
stem and be conflated.
3. Tools that use the morphological lexicon may
expect the lexicon to consist of roots instead
of stems.
4. Multiple root word-forms may map to one
stem and be conflated.
5. Stems are entirely dependent on the way
stemming rules are crafted. Roots are inde-
pendent of the stemming rules.
</listItem>
<bodyText confidence="0.984953">
The stem lexicon can be converted into a root
lexicon using the raw text corpus and the morpho-
logical rules that were used for stemming, as fol-
lows:
</bodyText>
<listItem confidence="0.969231833333334">
1. For any word-form and its stem, list all rules
that match.
2. Generate all the root word-forms possible
from the matching rules and stems.
3. From the choices, select the root word-form
with the highest frequency in the corpus.
</listItem>
<bodyText confidence="0.996033285714286">
Relative frequencies of word-forms have been
used in previous work to detect incorrect affix at-
tachments in Bengali and English (Dasgupta and
Ng, 2007). Our evaluation of the system showed
that relative frequencies could be very effective
predictors of root word-forms when applied within
the framework of a rule-based system.
</bodyText>
<sectionHeader confidence="0.996526" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99996015">
The goal of our experiment was to build a high-
coverage morphological lexicon for Hindi and to
evaluate the same. Having developed a multi-stage
system for lexicon extraction with a POS tagging
step following by stemming and root word-form
discovery, we proceeded to evaluate it as follows.
The stemming and the root discovery module
were evaluated against the gold standard of 1000
word-forms. In the first experiment, the precision
and recall of stemming using the HSE+Pos algo-
rithm were measured at different POS tagging ac-
curacies.
In the second experiment the root word-form
discovery module was provided the entire raw
word corpus to use in determining the best pos-
sible candidate for a root and tested using the gold
standard. The scores obtained reflect the perfor-
mance of the overall system.
For stemming, the recall was calculated as the
fraction of stems and suffixes in the gold standard
that were returned by the stemmer for each word-
form examined. The precision was calculated as
the fraction of stems and suffixes returned by the
stemmer that matched the gold standard. The F-
score was calculated as the harmonic mean of the
precision and recall.
The recall of the root lexicon was measured as
the fraction of gold standard roots that were in the
lexicon. The precision was calculated as the frac-
tion of roots in the lexicon that were also in the
gold standard. Accuracy was the percentage of
gold word-forms’ roots that were matched exactly.
In order to approximately estimate the accuracy
of a stemmer or morphological analyzer that used
such a lexicon, we also calculated the accuracy
weighted by the frequency of the word-forms in
a small corpus of running text. The gold standard
tokens were seen in this corpus about 4400 times.
We only considered content words (nouns, verbs,
adjectives and adverbs) in this calculation.
</bodyText>
<page confidence="0.988278">
127
</page>
<table confidence="0.999399833333333">
Gold1 Accur Prec Recall F-Sco
POS 86.7% 82.4% 86.2% 84.2%
Sup+POS 88.2% 85.2% 87.3% 86.3%
Gold2 Accur Prec Recall F-Sco
POS 81.8% 77.8% 82.0% 79.8%
Sup+POS 83.5% 80.2% 82.6% 81.3%
</table>
<tableCaption confidence="0.972008">
Table 11: Stemming Performance Comparisons
</tableCaption>
<table confidence="0.9998196">
Gold 1 Accur Prec Recall F-Sco
No POS 76.7% 70.6% 77.9% 74.1%
65% POS 82.3% 77.5% 81.4% 79.4%
87% POS 85.4% 80.8% 85.1% 82.9%
Gold POS 86.7% 82.4% 86.2% 84.2%
</table>
<tableCaption confidence="0.954835">
Table 12: Stemming Performance at Different
POS Tagger Accuracies
</tableCaption>
<sectionHeader confidence="0.999875" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999754363636364">
The performance of our system using POS tag in-
formation is comparable to that obtained by Sarkar
and Bandyopadhyay (2008). Sarkar and Bandy-
opadhyay (2008) obtained stemming accuracies of
90.2% for Bangla using gold POS tags. So in the
comparisons in Table 11, we use gold POS tags
(row two) and also supervised learning (row three)
using the other gold corpus as the labelled training
corpus. We present the scores for the two gold
standards separately. It must be noted that Sarkar
and Bandyopadhyay (2008) conducted their ex-
periments on Bangla, and so the results are not
exactly comparable.
We also evaluate the performance of stemming
using HSE with POS tagging by a real tagger at
two different tagging accuracies - approximately
65% and 87% - as shown in Table 12. We com-
pare the performance with gold POS tags and a
baseline system which does not use POS tags. We
do not use labelled training data for this section of
the experiments and only evaluate against the first
gold standard.
</bodyText>
<tableCaption confidence="0.694645">
Table 13 compares the F-scores for root discov-
</tableCaption>
<table confidence="0.9995896">
Gold 1 Accur Prec Recall F-Sco
No POS 71.7% 77.6% 78.8% 78.1%
65% POS 82.5% 87.2% 88.9% 88.0%
87% POS 87.0% 94.1% 95.3% 94.6%
Gold POS 89.1% 95.4% 97.9% 96.6%
</table>
<tableCaption confidence="0.946379">
Table 13: Root Finding Accuracy
</tableCaption>
<table confidence="0.999393">
Gold 1 Stemming Root Finding
65% POS 85.6% 87.0%
87% POS 87.5% 90.6%
Gold POS 88.5% 90.2%
</table>
<tableCaption confidence="0.9179205">
Table 14: Weighted Stemming and Root Finding
Accuracies (only Content Words)
</tableCaption>
<bodyText confidence="0.998693714285714">
ery at different POS tagging accuracies against a
baseline which excludes the use of POS tags alto-
gether. There seems to be very little prior work
that we can use for comparison here. To our
knowledge, the closest comparable work is a sys-
tem built by Oliver and Tadi´c (2004) in order to
enlarge a Croatian Morphological Lexicon. The
overall performance reported by Tadi´c et al was
as follows: (precision=86.13%, recall=35.36%,
F1=50.14%).
Lastly, Table 14 shows the accuracy of stem-
ming and root finding weighted by the frequencies
of the words in a running text corpus. This was
calculated only for content words.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999845">
We have described a system for automatically con-
structing a root word-form lexicon from a raw
text corpus. The system is rule-based and uti-
lizes a POS tagger. Though preliminary, our re-
sults demonstrate that it is possible, using this
method, to extract a high-precision and high-recall
root word-form lexicon. Specifically, we show
that with a POS tagger capable of labelling word-
forms with POS categories at an accuracy of about
88%, we can extract root word-forms with an ac-
curacy of about 87% and a precision and recall of
94.1% and 95.3% respectively.
Though the system has been evaluated on Hindi,
the techniques described herein can probably be
applied to other inflectional languages. The rules
selected by the system and applied to the word-
forms also contain information that can be used to
determine the paradigm membership of each root
word-form. Further work could evaluate the accu-
racy with which we can accomplish this task.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99946625">
We would like to thank our colleagues Priyanka
Biswas, Kalika Bali and Shalini Hada, of Mi-
crosoft Research India, for their assistance in the
creation of the Hindi root and stem gold standards.
</bodyText>
<page confidence="0.997365">
128
</page>
<sectionHeader confidence="0.995865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99915815942029">
Lionel Cl´ement, Benoit Sagot and Bernard Lang.
2004. Morphology based automatic acquisition of
large-coverage lexica. In Proceedings of LREC
2004, Lisbon, Portugal.
Sajib Dasgupta and Vincent Ng. 2007. High-
Performance, Language-Independent Morphologi-
cal Segmentation. In Main Proceedings of NAACL
HLT 2007, Rochester, NY, USA.
Markus Forsberg, Harald Hammarstr¨om and Aarne
Ranta. 2006. Morphological Lexicon Extraction
from Raw Text Data. In Proceedings of the 5th In-
ternational Conference on Advances in Natural Lan-
guage Processing, FinTAL, Finland.
John A. Goldsmith. 2001. Linguistica: An Automatic
Morphological Analyzer. In Arika Okrent and John
Boyle, editors, CLS 36: The Main Session, volume
36-1, Chicago Linguistic Society, Chicago.
Erika de Lima. 1998. Induction of a Stem Lexicon for
Two-Level Morphological Analysis. In Proceedings
of the Joint Conferences on New Methods in Lan-
guage Processing and Computational Natural Lan-
guage Learning, NeMLaP3/CoNLL98, pp 267-268,
Sydney, Australia.
Antoni Oliver, Marko Tadi´c. 2004. Enlarging the
Croatian Morphological Lexicon by Automatic Lex-
ical Acquisition from Raw Corpora. In Proceedings
of LREC 2004, Lisbon, Portugal.
Amaresh Kumar Pandey and Tanveer J. Siddiqui.
2008. An Unsupervised Hindi Stemmer with
Heuristic Improvements. In Proceedings of the Sec-
ond Workshop on Analytics for Noisy Unstructured
Text Data, AND 2008, pp 99-105, Singapore.
A Ramanathan and D. D. Rao. 2003. A Lightweight
Stemmer for Hindi. Presented at EACL 2003, Bu-
dapest, Hungary.
Benoit Sagot. 2005. Automatic Acquisition of a
Slovak Lexicon from a Raw Corpus. In Lecture
Notes in Artificial Intelligence 3658, Proceedings of
TSD’05, Karlovy Vary, Czech Republic.
Benoit Sagot. 2007. Building a Morphosyntactic Lexi-
con and a Pre-Syntactic Processing Chain for Polish.
In Proceedings of LTC 2007, Pozna´n, Poland.
Benoit Sagot, Lionel Cl´ement, ´Eric Villemonte de la
Clergerie and Pierre Boullier. 2006. The Lefff 2
Syntactic Lexicon for French: Architecture, Acqui-
sition, Use. In Proceedings of LREC’06, Genoa,
Italy.
Baskaran Sankaran, Kalika Bali, Monojit Choudhury,
Tanmoy Bhattacharya, Pushpak Bhattacharyya,
Girish Nath Jha, S. Rajendran, K. Saravanan, L.
Sobha and K.V. Subbarao. 2008. A Common Parts-
of-Speech Tagset Framework for Indian Languages.
In Proceedings of the Sixth International Language
Resources and Evaluation (LREC’08), Marrakech,
Morocco.
Sandipan Sarkar and Sivaji Bandyopadhyay. 2008.
Design of a Rule-based Stemmer for Natural Lan-
guage Text in Bengali. In Proceedings of the
IJCNLP-08 Workshop on NLP for Less Privileged
Languages, Hyderabad, India.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependependency
Network In Proceedings of HLT-NAACL 2003
pages 252-259.
Daniel Zeman. 2007. Unsupervised Acquisition of
Morphological Paradigms from Tokenized Text. In
Working Notes for the Cross Language Evaluation
Forum CLEF 2007 Workshop, Budapest, Hungary.
</reference>
<page confidence="0.9985">
129
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.868781">
<title confidence="0.998208">Large-Coverage Root Lexicon Extraction for Hindi</title>
<author confidence="0.941426">Cohan Sujay Carlos Monojit Choudhury Sandipan Dandapat</author>
<affiliation confidence="0.999564">Microsoft Research India</affiliation>
<email confidence="0.999791">monojitc@microsoft.com</email>
<abstract confidence="0.994383785714286">This paper describes a method using morphological rules and heuristics, for the automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus. We cast the problem of high-coverage lexicon extraction as one of stemming followed by root word-form selection. We examine the use of POS tagging to improve precision and recall of stemming and thereby the coverage of the lexicon. We present accuracy, precision and recall scores for the system on a Hindi corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lionel Cl´ement</author>
<author>Benoit Sagot</author>
<author>Bernard Lang</author>
</authors>
<title>Morphology based automatic acquisition of large-coverage lexica.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Cl´ement, Sagot, Lang, 2004</marker>
<rawString>Lionel Cl´ement, Benoit Sagot and Bernard Lang. 2004. Morphology based automatic acquisition of large-coverage lexica. In Proceedings of LREC 2004, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>HighPerformance, Language-Independent Morphological Segmentation.</title>
<date>2007</date>
<booktitle>In Main Proceedings of NAACL HLT 2007,</booktitle>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="24650" citStr="Dasgupta and Ng, 2007" startWordPosition="4253" endWordPosition="4256">endent on the way stemming rules are crafted. Roots are independent of the stemming rules. The stem lexicon can be converted into a root lexicon using the raw text corpus and the morphological rules that were used for stemming, as follows: 1. For any word-form and its stem, list all rules that match. 2. Generate all the root word-forms possible from the matching rules and stems. 3. From the choices, select the root word-form with the highest frequency in the corpus. Relative frequencies of word-forms have been used in previous work to detect incorrect affix attachments in Bengali and English (Dasgupta and Ng, 2007). Our evaluation of the system showed that relative frequencies could be very effective predictors of root word-forms when applied within the framework of a rule-based system. 4 Evaluation The goal of our experiment was to build a highcoverage morphological lexicon for Hindi and to evaluate the same. Having developed a multi-stage system for lexicon extraction with a POS tagging step following by stemming and root word-form discovery, we proceeded to evaluate it as follows. The stemming and the root discovery module were evaluated against the gold standard of 1000 word-forms. In the first expe</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. HighPerformance, Language-Independent Morphological Segmentation. In Main Proceedings of NAACL HLT 2007, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Forsberg</author>
<author>Harald Hammarstr¨om</author>
<author>Aarne Ranta</author>
</authors>
<title>Morphological Lexicon Extraction from Raw Text Data.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Advances in Natural Language Processing,</booktitle>
<location>FinTAL, Finland.</location>
<marker>Forsberg, Hammarstr¨om, Ranta, 2006</marker>
<rawString>Markus Forsberg, Harald Hammarstr¨om and Aarne Ranta. 2006. Morphological Lexicon Extraction from Raw Text Data. In Proceedings of the 5th International Conference on Advances in Natural Language Processing, FinTAL, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>Linguistica: An Automatic Morphological Analyzer.</title>
<date>2001</date>
<booktitle>In Arika Okrent and</booktitle>
<volume>volume</volume>
<pages>36--1</pages>
<editor>John Boyle, editors,</editor>
<publisher>Linguistic Society,</publisher>
<location>Chicago</location>
<contexts>
<context position="17248" citStr="Goldsmith, 2001" startWordPosition="2979" endWordPosition="2980">atch (LSM) In the LSM heuristic, when multiple suffixes can be applied to a word-form to stem it, we choose the longest one. Since Hindi has concatenative morphology with only postfix inflection, we only need to find one matching suffix to stem it. It is claimed in the literature that the method of using the longest suffix match works better than random suffix selection (Sarkar and Bandyopadhyay, 2008). This heuristic was used as the baseline for our experiments. 3.5.2 Highest Suffix Evidence (HSE) In the HSE heuristic, which has been applied before to unsupervised morphological segmentation (Goldsmith, 2001), stemming (Pandey and Siddiqui, 2008), and automatic paradigm extraction (Zeman, 2007), when multiple suffixes can be applied to stem a word-form, the suffix that is picked is the one that results in the stem with the highest suffix evidence. In our case, when computing the suffix evidence, the following additional constraint is applied: all the suffixes used to compute the suffix evidence score for any stem must be associated with the same POS category. For example, the suffix yon is only applicable to nouns, whereas the suffix ta is only applicable to verbs. These two suffixes will therefor</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John A. Goldsmith. 2001. Linguistica: An Automatic Morphological Analyzer. In Arika Okrent and John Boyle, editors, CLS 36: The Main Session, volume 36-1, Chicago Linguistic Society, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erika de Lima</author>
</authors>
<title>Induction of a Stem Lexicon for Two-Level Morphological Analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning, NeMLaP3/CoNLL98,</booktitle>
<pages>267--268</pages>
<location>Sydney, Australia.</location>
<marker>de Lima, 1998</marker>
<rawString>Erika de Lima. 1998. Induction of a Stem Lexicon for Two-Level Morphological Analysis. In Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning, NeMLaP3/CoNLL98, pp 267-268, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoni Oliver</author>
<author>Marko Tadi´c</author>
</authors>
<title>Enlarging the Croatian Morphological Lexicon by Automatic Lexical Acquisition from Raw Corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Oliver, Tadi´c, 2004</marker>
<rawString>Antoni Oliver, Marko Tadi´c. 2004. Enlarging the Croatian Morphological Lexicon by Automatic Lexical Acquisition from Raw Corpora. In Proceedings of LREC 2004, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amaresh Kumar Pandey</author>
<author>Tanveer J Siddiqui</author>
</authors>
<title>An Unsupervised Hindi Stemmer with Heuristic Improvements.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data, AND</booktitle>
<pages>99--105</pages>
<contexts>
<context position="17286" citStr="Pandey and Siddiqui, 2008" startWordPosition="2982" endWordPosition="2986">tic, when multiple suffixes can be applied to a word-form to stem it, we choose the longest one. Since Hindi has concatenative morphology with only postfix inflection, we only need to find one matching suffix to stem it. It is claimed in the literature that the method of using the longest suffix match works better than random suffix selection (Sarkar and Bandyopadhyay, 2008). This heuristic was used as the baseline for our experiments. 3.5.2 Highest Suffix Evidence (HSE) In the HSE heuristic, which has been applied before to unsupervised morphological segmentation (Goldsmith, 2001), stemming (Pandey and Siddiqui, 2008), and automatic paradigm extraction (Zeman, 2007), when multiple suffixes can be applied to stem a word-form, the suffix that is picked is the one that results in the stem with the highest suffix evidence. In our case, when computing the suffix evidence, the following additional constraint is applied: all the suffixes used to compute the suffix evidence score for any stem must be associated with the same POS category. For example, the suffix yon is only applicable to nouns, whereas the suffix ta is only applicable to verbs. These two suffixes will therefore never be counted together in computi</context>
</contexts>
<marker>Pandey, Siddiqui, 2008</marker>
<rawString>Amaresh Kumar Pandey and Tanveer J. Siddiqui. 2008. An Unsupervised Hindi Stemmer with Heuristic Improvements. In Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data, AND 2008, pp 99-105, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ramanathan</author>
<author>D D Rao</author>
</authors>
<title>A Lightweight Stemmer for Hindi. Presented at EACL</title>
<date>2003</date>
<location>Budapest, Hungary.</location>
<contexts>
<context position="19688" citStr="Ramanathan and Rao, 2003" startWordPosition="3396" endWordPosition="3399">r Prec Recall F-Score LSM 75.7% 70.7% 72.7% 71.7% HSE 75.0% 69.0% 77.6% 73.0% HSE+Sup 75.3% 69.3% 78.0% 73.4% Table 9: Comparison of Heuristics The feature set consisted of two features: the last character (or diacritic) of the word-form, and the suffix. The POS category was an optional feature and used when available. If the number of incorrect splits exceeded the number of correct splits given a feature set, the rule was assigned a weight of 0, else it was given a weight of 1. 3.5.4 Comparison We compare the performance of our rules with the performance of the Lightweight Stemmer for Hindi (Ramanathan and Rao, 2003) with a reported accuracy of 81.5%. The scores we report in Table 8 are the average of the LSM scores on the two gold standards. The stemmer using the standard rule-set (Rules1) does not perform as well as the Lightweight Stemmer. We then handcrafted a different set of rules (Rules2) with adjustments to maximize its performance. The accuracy was better than Rules1 but not quite equal to the Lightweight Stemmer. However, since our gold standard is different from that used to evaluate the Lightweight Stemmer, the comparison is not necessarily very meaningful. As shown in Table 9, in F-score comp</context>
</contexts>
<marker>Ramanathan, Rao, 2003</marker>
<rawString>A Ramanathan and D. D. Rao. 2003. A Lightweight Stemmer for Hindi. Presented at EACL 2003, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
</authors>
<title>Automatic Acquisition of a Slovak Lexicon from a Raw Corpus.</title>
<date>2005</date>
<booktitle>In Lecture Notes in Artificial Intelligence 3658, Proceedings of TSD’05, Karlovy Vary,</booktitle>
<location>Czech Republic.</location>
<contexts>
<context position="2156" citStr="Sagot, 2005" startWordPosition="330" endWordPosition="331">h Asia. Since a POS tagger, another basic tool, was available along with POS tagged data to train it, and since the error patterns indicated that POS tagging could greatly improve the accuracy of the lexicon, we used the POS tagger in our experiments on lexicon extraction. Previous work in morphological lexicon extraction from a raw corpus often does not achieve very high precision and recall (de Lima, 1998; Oliver and Tadi´c, 2004). In some previous work the process of lexicon extraction involves incremental or post-construction manual validation of the entire lexicon (Cl´ement et al., 2004; Sagot, 2005; Forsberg et al., 2006; Sagot et al., 2006; Sagot, 2007). Our method attempts to improve on and extend the previous work by increasing the precision and recall of the system to such a point that manual validation might even be rendered unnecessary. Yet another difference, to our knowledge, is that in our method we cast the problem of lexicon extraction as two subproblems: that of stemming and following it, that of root word-form selection. The input resources for our system are as follows: a) raw text corpus, b) morphological rules, c) POS tagger and d) word-segmentation labelled data. We out</context>
</contexts>
<marker>Sagot, 2005</marker>
<rawString>Benoit Sagot. 2005. Automatic Acquisition of a Slovak Lexicon from a Raw Corpus. In Lecture Notes in Artificial Intelligence 3658, Proceedings of TSD’05, Karlovy Vary, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
</authors>
<title>Building a Morphosyntactic Lexicon and a Pre-Syntactic Processing Chain for Polish.</title>
<date>2007</date>
<booktitle>In Proceedings of LTC 2007,</booktitle>
<location>Pozna´n, Poland.</location>
<contexts>
<context position="2213" citStr="Sagot, 2007" startWordPosition="341" endWordPosition="342">lable along with POS tagged data to train it, and since the error patterns indicated that POS tagging could greatly improve the accuracy of the lexicon, we used the POS tagger in our experiments on lexicon extraction. Previous work in morphological lexicon extraction from a raw corpus often does not achieve very high precision and recall (de Lima, 1998; Oliver and Tadi´c, 2004). In some previous work the process of lexicon extraction involves incremental or post-construction manual validation of the entire lexicon (Cl´ement et al., 2004; Sagot, 2005; Forsberg et al., 2006; Sagot et al., 2006; Sagot, 2007). Our method attempts to improve on and extend the previous work by increasing the precision and recall of the system to such a point that manual validation might even be rendered unnecessary. Yet another difference, to our knowledge, is that in our method we cast the problem of lexicon extraction as two subproblems: that of stemming and following it, that of root word-form selection. The input resources for our system are as follows: a) raw text corpus, b) morphological rules, c) POS tagger and d) word-segmentation labelled data. We output a stem lexicon and a root wordform lexicon. We take a</context>
</contexts>
<marker>Sagot, 2007</marker>
<rawString>Benoit Sagot. 2007. Building a Morphosyntactic Lexicon and a Pre-Syntactic Processing Chain for Polish. In Proceedings of LTC 2007, Pozna´n, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Lionel Cl´ement</author>
</authors>
<title>Eric Villemonte de la Clergerie and Pierre Boullier.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’06,</booktitle>
<location>Genoa, Italy.</location>
<marker>Sagot, Cl´ement, 2006</marker>
<rawString>Benoit Sagot, Lionel Cl´ement, ´Eric Villemonte de la Clergerie and Pierre Boullier. 2006. The Lefff 2 Syntactic Lexicon for French: Architecture, Acquisition, Use. In Proceedings of LREC’06, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Kalika Bali</author>
<author>Monojit Choudhury</author>
<author>Tanmoy Bhattacharya</author>
<author>Pushpak Bhattacharyya</author>
<author>Girish Nath Jha</author>
<author>S Rajendran</author>
<author>K Saravanan</author>
<author>L Sobha</author>
<author>K V Subbarao</author>
</authors>
<title>A Common Partsof-Speech Tagset Framework for Indian Languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="10115" citStr="Sankaran et al., 2008" startWordPosition="1662" endWordPosition="1665">es Root laDkA noun {‘A’,‘e’,‘on’} ‘A’ laDkI noun {‘I’,‘iyAn’} ‘I’ dho verb {‘’,‘yogI’,‘nA’,... } ‘’ chal verb {‘’,‘ogI’,‘nA’,... } ‘’ Table 3: Sample Paradigm Suffix Sets Since Hindi word boundaries are clearly marked with punctuation and spaces, tokenization was an easy task. The raw text corpus yielded approximately 331000 unique word-forms. When words beginning with numbers were removed, we were left with about 316000 unique word-forms of which almost half occurred only once in the corpus. In addition, we needed a corpus of 45,000 words labelled with POS categories using the ILPOST tagset (Sankaran et al., 2008) for the POS tagger. 3.2 Rules The morphological rules input into the system are used to recognize word-forms that together belong to a paradigm. Paradigms can be treated as a set of suffixes that can be used to generate inflectional word-forms from a stem. The set of suffixes that constitutes a paradigm defines an equivalence class on the set of unique word-forms in the corpus. For example, the laDkA paradigm in Table 2 would be represented by the set of suffix strings {‘A’, ‘e’, ‘on’} derived from the word-forms laDkA, laDke and laDkon. A few paradigms are listed in Table 3. The suffix set f</context>
<context position="21465" citStr="Sankaran et al., 2008" startWordPosition="3706" endWordPosition="3709">p heuristic would have gotten right if the POS category had been available. From the numbers it appears that a sizeable fraction of the errors, especially with noun word-forms, is caused when a suffix of the wrong POS category is applied to a word-form. Moreover, prior work in Bangla (Sarkar and Bandyopadhyay, 2008) indicates that POS category information could improve the accuracy of stemming. Assigning POS categories to word-forms requires a POS tagger and a substantial amount of POS labelled data as described below. 3.5.6 POS Tagging The POS tagset used was the hierarchical tagset IL-POST (Sankaran et al., 2008). The hierarchical tagset supports broad POS categories like nouns and verbs, less broad POS types like common and proper nouns and finally, at its finest granularity, attributes like gender, number, case and mood. We found that with a training corpus of about 45,000 tagged words (2366 sentences), it was possible to produce a reasonably accurate POS tagger2, use it to label the raw text corpus with broad POS tags, and consequently improve the accuracy of stemming. For our experiments, we used both the full training corpus of 45,000 words and a subset of the same consisting of about 20,000 word</context>
</contexts>
<marker>Sankaran, Bali, Choudhury, Bhattacharya, Bhattacharyya, Jha, Rajendran, Saravanan, Sobha, Subbarao, 2008</marker>
<rawString>Baskaran Sankaran, Kalika Bali, Monojit Choudhury, Tanmoy Bhattacharya, Pushpak Bhattacharyya, Girish Nath Jha, S. Rajendran, K. Saravanan, L. Sobha and K.V. Subbarao. 2008. A Common Partsof-Speech Tagset Framework for Indian Languages. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandipan Sarkar</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Design of a Rule-based Stemmer for Natural Language Text in Bengali.</title>
<date>2008</date>
<booktitle>In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<location>Hyderabad,</location>
<contexts>
<context position="17037" citStr="Sarkar and Bandyopadhyay, 2008" startWordPosition="2946" endWordPosition="2949">icking the suffix which yields the stem with the highest value for suffix evidence. • Highest Suffix Evidence with Supervised Rule Selection (HSE + Sup) - Using labelled data to modulate suffix matching. 3.5.1 Longest Suffix Match (LSM) In the LSM heuristic, when multiple suffixes can be applied to a word-form to stem it, we choose the longest one. Since Hindi has concatenative morphology with only postfix inflection, we only need to find one matching suffix to stem it. It is claimed in the literature that the method of using the longest suffix match works better than random suffix selection (Sarkar and Bandyopadhyay, 2008). This heuristic was used as the baseline for our experiments. 3.5.2 Highest Suffix Evidence (HSE) In the HSE heuristic, which has been applied before to unsupervised morphological segmentation (Goldsmith, 2001), stemming (Pandey and Siddiqui, 2008), and automatic paradigm extraction (Zeman, 2007), when multiple suffixes can be applied to stem a word-form, the suffix that is picked is the one that results in the stem with the highest suffix evidence. In our case, when computing the suffix evidence, the following additional constraint is applied: all the suffixes used to compute the suffix evid</context>
<context position="21160" citStr="Sarkar and Bandyopadhyay, 2008" startWordPosition="3655" endWordPosition="3659">ed against the second gold standard. POS Correct Incorrect POS Errors Noun 749 231 154 Verb 324 108 0 Adjective 227 49 13 Others 136 82 35 Table 10: Errors by POS Category 3.5.5 Error Analysis Table 10 lists the number of correct stems, incorrect stems, and finally a count of those incorrect stems that the HSE+Sup heuristic would have gotten right if the POS category had been available. From the numbers it appears that a sizeable fraction of the errors, especially with noun word-forms, is caused when a suffix of the wrong POS category is applied to a word-form. Moreover, prior work in Bangla (Sarkar and Bandyopadhyay, 2008) indicates that POS category information could improve the accuracy of stemming. Assigning POS categories to word-forms requires a POS tagger and a substantial amount of POS labelled data as described below. 3.5.6 POS Tagging The POS tagset used was the hierarchical tagset IL-POST (Sankaran et al., 2008). The hierarchical tagset supports broad POS categories like nouns and verbs, less broad POS types like common and proper nouns and finally, at its finest granularity, attributes like gender, number, case and mood. We found that with a training corpus of about 45,000 tagged words (2366 sentence</context>
<context position="27263" citStr="Sarkar and Bandyopadhyay (2008)" startWordPosition="4685" endWordPosition="4688">s, adjectives and adverbs) in this calculation. 127 Gold1 Accur Prec Recall F-Sco POS 86.7% 82.4% 86.2% 84.2% Sup+POS 88.2% 85.2% 87.3% 86.3% Gold2 Accur Prec Recall F-Sco POS 81.8% 77.8% 82.0% 79.8% Sup+POS 83.5% 80.2% 82.6% 81.3% Table 11: Stemming Performance Comparisons Gold 1 Accur Prec Recall F-Sco No POS 76.7% 70.6% 77.9% 74.1% 65% POS 82.3% 77.5% 81.4% 79.4% 87% POS 85.4% 80.8% 85.1% 82.9% Gold POS 86.7% 82.4% 86.2% 84.2% Table 12: Stemming Performance at Different POS Tagger Accuracies 5 Results The performance of our system using POS tag information is comparable to that obtained by Sarkar and Bandyopadhyay (2008). Sarkar and Bandyopadhyay (2008) obtained stemming accuracies of 90.2% for Bangla using gold POS tags. So in the comparisons in Table 11, we use gold POS tags (row two) and also supervised learning (row three) using the other gold corpus as the labelled training corpus. We present the scores for the two gold standards separately. It must be noted that Sarkar and Bandyopadhyay (2008) conducted their experiments on Bangla, and so the results are not exactly comparable. We also evaluate the performance of stemming using HSE with POS tagging by a real tagger at two different tagging accuracies - </context>
</contexts>
<marker>Sarkar, Bandyopadhyay, 2008</marker>
<rawString>Sandipan Sarkar and Sivaji Bandyopadhyay. 2008. Design of a Rule-based Stemmer for Natural Language Text in Bengali. In Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Partof-Speech Tagging with a Cyclic Dependependency Network In</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="22475" citStr="Toutanova et al., 2003" startWordPosition="3875" endWordPosition="3878">xt corpus with broad POS tags, and consequently improve the accuracy of stemming. For our experiments, we used both the full training corpus of 45,000 words and a subset of the same consisting of about 20,000 words. The POS tagging accuracies obtained were approximately 87% and 65% respectively. The reason for repeating the experiment using the 20,000 word subset of the training data was to demonstrate that a mere 20,000 words of labelled data, which does not take a very great amount of 2The Part-of-Speech tagger used was an implementation of a Cyclic Dependency Network Part-of-Speech tagger (Toutanova et al., 2003). The following feature set was used in the tagger: tag of previous word, tag of next word, word prefixes and suffixes of length exactly four, bigrams and the presence of numbers or symbols. 126 time and effort to create, can produce significant improvements in stemming performance. In order to assign tags to the words of the gold standard, sentences from the raw text corpus containing word-forms present in the gold standard were tagged using a POS tagger. The POS categories assigned to each word-form were then read off and stored in a table. Once POS tags were associated with all the words, a</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning and Yoram Singer. 2003. Feature-Rich Partof-Speech Tagging with a Cyclic Dependependency Network In Proceedings of HLT-NAACL 2003 pages 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Unsupervised Acquisition of Morphological Paradigms from Tokenized Text. In Working Notes for the Cross Language Evaluation Forum CLEF</title>
<date>2007</date>
<location>Workshop, Budapest, Hungary.</location>
<contexts>
<context position="10840" citStr="Zeman, 2007" startWordPosition="1790" endWordPosition="1791">t together belong to a paradigm. Paradigms can be treated as a set of suffixes that can be used to generate inflectional word-forms from a stem. The set of suffixes that constitutes a paradigm defines an equivalence class on the set of unique word-forms in the corpus. For example, the laDkA paradigm in Table 2 would be represented by the set of suffix strings {‘A’, ‘e’, ‘on’} derived from the word-forms laDkA, laDke and laDkon. A few paradigms are listed in Table 3. The suffix set formalism of a paradigm closely resembles the one used in a previous attempt at unsupervised paradigm extraction (Zeman, 2007) but differs from it in that Zeman (2007) considers the set of word-forms that match the paradigm to be a part of the paradigm definition. In our system, we represent the morphological rules by a list of suffix add-delete rules. Each rule in our method is a five-tuple {α, Q, -y, S, E} where: • α is the suffix string to be matched for the rule to apply. • Q is the portion of the suffix string after which the stem ends. • -y is a POS category in which the string α is a valid suffix. 123 α Q ry 6 E ‘A’ ‘’ Noun N1 ‘A’ ‘on’ ‘’ Noun N1,N3 ‘A’ ‘e’ ‘’ Noun N1 ‘A’ ‘oyogI’ ‘o’ Verb V5 ‘o’ Table 4: Sampl</context>
<context position="17335" citStr="Zeman, 2007" startWordPosition="2991" endWordPosition="2992">em it, we choose the longest one. Since Hindi has concatenative morphology with only postfix inflection, we only need to find one matching suffix to stem it. It is claimed in the literature that the method of using the longest suffix match works better than random suffix selection (Sarkar and Bandyopadhyay, 2008). This heuristic was used as the baseline for our experiments. 3.5.2 Highest Suffix Evidence (HSE) In the HSE heuristic, which has been applied before to unsupervised morphological segmentation (Goldsmith, 2001), stemming (Pandey and Siddiqui, 2008), and automatic paradigm extraction (Zeman, 2007), when multiple suffixes can be applied to stem a word-form, the suffix that is picked is the one that results in the stem with the highest suffix evidence. In our case, when computing the suffix evidence, the following additional constraint is applied: all the suffixes used to compute the suffix evidence score for any stem must be associated with the same POS category. For example, the suffix yon is only applicable to nouns, whereas the suffix ta is only applicable to verbs. These two suffixes will therefore never be counted together in computing the suffix evidence for a stem. The algorithm </context>
</contexts>
<marker>Zeman, 2007</marker>
<rawString>Daniel Zeman. 2007. Unsupervised Acquisition of Morphological Paradigms from Tokenized Text. In Working Notes for the Cross Language Evaluation Forum CLEF 2007 Workshop, Budapest, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>