<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9773858">
The FINITE STRING Newsletter Abstracts of Current Literature
The Third Annual Meeting of the Cognitive Science
Society will be held in Berkeley, California, August
19-21, 1981. Information about the meeting can be
obtained from:
</note>
<author confidence="0.864111">
Robert Wilensky
</author>
<affiliation confidence="0.994876333333333">
Computer Science Division
Department of EECS
University of California, Berkeley
</affiliation>
<address confidence="0.341501">
Berkeley, California 94720
</address>
<bodyText confidence="0.833055166666667">
The Seventh International Joint Conference on Arti-
ficial Intelligence will be held in Vancouver, British
Columbia, Canada, August 24-28, 1981. [See AJCL
6:3-4, pg. 194.]
Questions about the technical program should be
addressed to:
</bodyText>
<figure confidence="0.867441636363636">
Roger C. Schank
Program Chairman, IJCAI-81
Department of Computer Science
Box 2158 Yale Station
Yale University
New Haven, Connecticut 06520
(203) 436-0606
General questions about the conference may be
addressed to:
Pat Hayes
General Chairman, IJCAI-81
</figure>
<affiliation confidence="0.885133">
Department of Computer Science
Mathematical Sciences Building
University of Rochester
Rochester, New York 14627
</affiliation>
<bodyText confidence="0.814321923076923">
The 1981 Annual Conference of the British Society
for the Philosophy of Science will be held at Wivenhoe
House at the University of Essex, September 18-20,
1981. [See AJCL 6:3-4, pg. 194.] Further details of
the conference may be obtained from:
Liaison Officer
University of Essex
Wivenhoe Park
Colchester C04 3SQ, Essex, ENGLAND
The 1981 ACM Annual Conference will be held at
the Bonaventure Hotel in Los Angeles, California,
November 9-11, 1981. [See AJCL 6:3-4, pg. 194.]
For further information contact:
</bodyText>
<footnote confidence="0.8380962">
Mrs. A.C. Toni Shelter
Xerox Corporation, A3-49
701 South Aviation Boulevard
El Segundo, California 90245
(213) 679-4511 x1968
</footnote>
<note confidence="0.7838388">
Abstracts of Current Literature
The Transformational Question Answering
(TQA) System: Description, Operating
Experience, and Implications
Fred J. Damerau
</note>
<subsectionHeader confidence="0.370168333333333">
Mathematical Sciences Department
IBM Thomas J. Watson Research Center
Yorktown Heights, New York 10598
</subsectionHeader>
<subsubsectionHeader confidence="0.655162">
Research Report RC 8287, May 1980.
</subsubsectionHeader>
<bodyText confidence="0.999898125">
This paper sketches the structure of the TQA sys-
tem with some examples, discusses some of the results
of running the system in a user site during the year
1978, and outlines a few of the implications of a natu-
ral language query system for larger and more diverse
data bases. It is apparent that a number of problems
remain to be solved, but it also seems likely that a
useful system can be constructed for some domains.
</bodyText>
<subsectionHeader confidence="0.545107">
Design for an Intelligent Office System
Janet L. Kolodner
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.509613">
IBM Research Report RC 8385, July 1980.
</subsubsectionHeader>
<bodyText confidence="0.999788352941176">
This paper addresses the problem of designing an
intelligent office system to help principals (managerial,
administrative, or professional level office workers)
with their work. A system is described which would
use normative models of semi-routine office proce-
dures to anticipate the user&apos;s next steps, to provide
him automatically with files, documents, and forms he
needs to complete a task, and to perform automated
subtasks at the correct times. A number of intelligent
functions that would perform some of the tasks secre-
taries generally do (scheduling, reminding, filing) are
described, along with the knowledge necessary for
performing those functions. In addition, an approach
to building a more intelligent system which would
understand natural language commands given by the
user and understand and deal with routine mail is pres-
ented.
</bodyText>
<subsectionHeader confidence="0.930007714285714">
Memory Organization and Search
Processes for Narratives
Michael G. Dyer and Wendy G. Lehnert
Department of Computer Science
Yale University
New Haven, Connecticut 06520
Research Report 175, April 1980.
</subsectionHeader>
<bodyText confidence="0.958556083333333">
BORIS represents the first system to integrate the
knowledge-based inference techniques of scripts,
plans, goals, and themes, within a single narrative un-
derstanding program. This report discusses techniques
used by BORIS for memory representation and memo-
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 51
The FINITE STRING Newsletter Abstracts of Current Literature
ry retrieval. Emphasis is placed on human question
answering abilities, and the heuristics needed to simu-
late these phenomena. An example narrative proc-
essed by BORIS is discussed in detail and used to il-
lustrate design decisions.
</bodyText>
<subsectionHeader confidence="0.9895498">
What Does it Mean to Understand Language?
Terry Winograd
Computer Science Department
Stanford University
Stanford, California 94305
</subsectionHeader>
<subsubsectionHeader confidence="0.517561">
Cognitive Science 4, 3 (July-Sept. 1980), 209-241.
</subsubsectionHeader>
<bodyText confidence="0.999867311111111">
In its earliest drafts, this paper was a structured
argument, presenting a comprehensive view of cogni-
tive science, criticizing prevailing approaches to the
study of language and thought and advocating a new
way of looking at things. Although I strongly believed
in the approach it outlined, somehow it didn&apos;t have the
convincingness on paper that it had in my own reflec-
tion. After some discouraging attempts at reorganiza-
tion and rewriting, I realized that there was a mis-
match between the nature of what I wanted to say and
the form in which I was trying to communicate.
The understanding on which it was based does not
have the form of a carefully structured framework into
which all of cognitive science can be placed. It is
more an orientation — a way of approaching the phe-
nomena — that has grown out of many different expe-
riences and influences and that bears the marks of its
history. I found myself wanting to describe a path
rather than justify its destination, finding that in the
flow, the ideas came across more clearly. Since this
collection was envisioned as a panorama of contrasting
individual views, I have taken the liberty of making
this chapter explicitly personal and describing the evo-
lution of my own understanding.
My interests have centered around natural lan-
guage. I have been engaged in the design of computer
programs that in some sense could be said to
&amp;quot;understand language,&amp;quot; and this has led to looking at
many aspects of the problems, including theories of
meaning, representation formalisms, and the design
and construction of complex computer systems. There
has been a continuous evolution in my understanding
of just what it means to say that a person or computer
&amp;quot;understands,&amp;quot; and this story can be read as recount-
ing that evolution. It is long, because it is still too
early to look back and say &amp;quot;what I was really getting
at for all those years was the one basic idea that ...&amp;quot; I
am too close and too involved in its continuation to
see beyond the twists and turns. The last sections of
the paper describe a viewpoint that differs in signifi-
cant ways from most current approaches, and that
offers new possibilities for a deeper understanding of
language and a grasp on some previously intractable or
unrecognized problems. I hope that it will give some
sense of where the path is headed.
</bodyText>
<subsectionHeader confidence="0.8619098">
Language and Memory
Roger C. Schank
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.665404">
Cognitive Science 4, 3 (July-Sept. 1980), 243-284.
</subsubsectionHeader>
<bodyText confidence="0.999967846153846">
This paper outlines some of the issues and basic
philosophy that have guided my work and that of my
students in the last ten years. It describes the progres-
sion of conceptual representational theories developed
during that time, as well as some of the research mod-
els built to implement those theories. The paper con-
cludes with a discussion of my most recent work in the
area of modelling memory. It presents a theory of
MOPs (Memory Organization Packets), which serve as
both processors and organizers of information in mem-
ory. This enables effective categorization of experi-
ences in episodic memory, which in turn enables better
predictive understanding of new experiences.
</bodyText>
<subsectionHeader confidence="0.900619833333333">
&amp;quot;You Can&apos;t Miss RI&amp;quot;:
Judging the Clarity of Directions
Christopher K. Riesbeck
Department of Computer Science
Yale University
New Haven, Connecticut 06520
</subsectionHeader>
<subsubsectionHeader confidence="0.63578">
Cognitive Science 4, 3 (July-Sept. 1980), 285-303.
</subsubsectionHeader>
<bodyText confidence="0.99994136">
Recently there has been some very interesting Al
work done on the representation of knowledge of large
scale maps and their use in the construction of routes
through some space (Kuipers, 1978; McDermott,
1980). The stress in this work has been on how spatial
information is best represented and how reasoning is
performed with the assumed representation. In this
paper I would like to address the natural language
processing of texts giving directions and make a claim
that during a casual first reading, there actually does
not need to be much spatial reasoning going on at all.
When I read a written set of directions, I am primarily
interested whether the directions seem clear and sensi-
ble - not in constructing a map or program specifying
all the turns, distances, and locates that will be in-
volved. Certainly there are times when I have to make
some kind of route or map structure, such as when I
am the subject in a spatial reasoning experiment, or
when I am lost and ask someone on the street for di-
rections. But I do not need to work so hard when I
am given a piece of paper with a set of directions, and
I know that I will have this piece of paper with me
when I actually make the trip. I know that I can al-
ways read the directions again to figure out what to
do, when I actually set out on the trip.
</bodyText>
<page confidence="0.553945">
52 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
<note confidence="0.412457">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<title confidence="0.214401">
On Throwing Out the Baby with the Bathwater:
A Reply to Black and Wilensky&apos;s Evaluation of
Story Grammars
</title>
<author confidence="0.550265">
Jean M. Mendler
</author>
<affiliation confidence="0.712490833333333">
Department of Psychology
University of California, San Diego
La Jolla, California 92093
Nancy S. Johnson
State University of New York at Buffalo
Amherst, New York 14226
</affiliation>
<subsubsectionHeader confidence="0.795063">
Cognitive Science 4, 3 (July-Sept. 1980), 305-312.
</subsubsectionHeader>
<bodyText confidence="0.999870090909091">
A number of criticisms of a recent paper by Black
and Wilensky (1979) are made. (1) In attempting to
assess the observational adequacy of story grammars,
they state that a context-free grammar cannot handle
discontinuous elements; however, they do not show
that such elements occur in the domain to which the
grammars apply. Further, they do not present ade-
quate evidence for their claim that there are accepta-
ble stories not accounted for by existing grammars and
that the grammars will accept non-stories such as pro-
cedures. (2) They state that it has been proven that
under natural conditions children cannot learn trans-
formational grammars, which is a misrepresentation of
the learnability proofs which have been offered. (3)
Most important, they take an unduly narrow approach
to story understanding by claiming that people only
understand story content and do not have knowledge
of story structure which is useful in comprehension or
memory. Counter-evidence from the literature is cited
which indicates that such knowledge is both useful and
used, and a number of methods for assessing the psy-
chological adequacy of structural models are discussed.
</bodyText>
<subsectionHeader confidence="0.656219">
On Evaluating Story Grammars
</subsectionHeader>
<table confidence="0.90229425">
David E. Rumelhart
University of California, San Diego
La Jolla, California 92093
Cognitive Science 4, 3 (July-Sept. 1980), 313-316.
</table>
<bodyText confidence="0.999614321428571">
In their recent article entitled &amp;quot;An Evaluation of
Story Grammars,&amp;quot; Black and Wilensky (1979) offer a
critique of the recent work on this topic. They argue
that story grammars (or story schemata as I prefer to
call them) are not a productive approach to the study
of story understanding, and they offer three main lines
of argumentation. First, they argue that story gram-
mars are not formally adequate in as much as most of
them are represented as a set of context free rewrite
rules which are known to be inadequate even for sen-
tence grammars. Second, they argue that story gram-
mars are not empirically adequate in as much as there
are stories which do not seem to follow story gram-
mars and there are nonstories which do. Finally, they
argue that story grammars could not form an adequate
basis f or a comprehension model since in order to
apply the grammar you need to have interpreted the
story. These arguments are in my opinion, indicative
of a misunderstanding of the enterprise that I and
others working on these issues have been engaged in.
I believe that they are all based on a misunderstanding
about what grammars might be good for and about
how comprehension might occur. In this response, I
wish to clarify the nature of story schemata as I under-
stand them, clarify the nature of Black and Wilensky&apos;s
misunderstandings and show how each of their argu-
ments fails to address the important issues about story
grammars and story schemata.
</bodyText>
<table confidence="0.497016333333333">
Natural Language Understanding
Avron Barr
Department of Computer Science
Stanford University
Palo Alto, California 94305
Al Magazine 1, 1 (Spring 1980), 5-10.
</table>
<bodyText confidence="0.9996806875">
This is an excerpt from the Handbook of Artificial
Intelligence, a compendium of hundreds of articles
about Al ideas, techniques, and programs being pre-
pared at Stanford University by Al researchers and
students from across the country. This article, which
is from the chapter on Natural Language Understand-
ing, presents a brief sketch of the history of natural
language processing research and gives an idea of the
current state of the art. The other articles in the NL
chapter of the Handbook include a historical sketch of
machine translation, technical articles on grammars
and parsing techniques, and an article on text genera-
tion. Finally, there are several articles describing the
NL programs themselves. The Handbook also includes
chapters on speech understanding and knowledge rep-
resentation.
</bodyText>
<table confidence="0.791618285714286">
Utterance and Objective: Issues in
Natural Language Communication
Barbara J. Grosz
Artificial Intelligence Center
SRI International
Menlo Park, California 94025
Al Magazine 1, 1 (Spring 1980), 11-20.
</table>
<bodyText confidence="0.986697358974359">
Two premises, reflected in the title, underlie the
perspective from which I will consider research in nat-
ural language processing in this paper. First, progress
on building computer systems that process natural
languages in any meaningful sense (i.e., systems that
interact reasonably with people in natural language)
requires considering language as part of a larger com-
municative situation. In this larger situation, the par-
ticipants in a conversation and their states of mind are
as important to the interpretation of an utterance as
the linguistic expressions from which it is formed. A
central concern when language is considered as com-
munication is its function in building and using shared
models of the world.
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 53
The FINITE STRING Newsletter Abstracts of Current Literature
Second, as the phrase &amp;quot;utterance and objective&amp;quot;
suggests, regarding language as communication re-
quires consideration of what is said literally, what is
intended, and the relationship between the two. Re-
cently, the emphasis in research in natural language
processing has begun to shift from an analysis of ut-
terances as isolated linguistic phenomena to a consid-
eration of how people use utterances to achieve cer-
tain objectives. But, in considering objectives, it is
important not to ignore the utterances themselves. A
consideration of a speaker&apos;s underlying goals and moti-
vations is critical, but so is an analysis of the particular
way in which that speaker expresses his thoughts.
This paper examines three consequences of these
claims for the development of language processing
theories and the construction of language processing
programs: (1) language processing requires a combi-
nation of language-specific mechanisms and general
common-sense reasoning mechanisms, (2) language
systems must be able to represent the beliefs and
knowledge of multiple individual agents, and (3) utter-
ances must be viewed as having effects along multiple
dimensions.
</bodyText>
<subsectionHeader confidence="0.864576">
Conversation as Planned Behavior
Jerry R. Hobbs and David A. Evans
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<bodyText confidence="0.987765833333333">
Technical Note 203, Dec. 1979.
In this paper, planning models developed in artifi-
cial intelligence are applied to the kind of planning
that must be carried out by participants in a conversa-
tion. A planning mechanism is defined, and a short
fragment of a free-flowing videotaped conversation is
described. The bulk of the paper is then devoted to
an attempt to understand the conversation in terms of
the planning mechanism. This microanalysis suggests
ways in which the planning mechanisms must be aug-
mented, and reveals several important conversational
phenomena that deserve further investigation.
</bodyText>
<sectionHeader confidence="0.617278333333333" genericHeader="method">
Metaphor, Metaphor Schemata
and Selective Inferencing
Jerry R. Hobbs
</sectionHeader>
<subsectionHeader confidence="0.90554125">
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, California 94025
</subsectionHeader>
<bodyText confidence="0.988556454545455">
Technical Note 204, Dec. 1979.
The importance of spatial and other metaphors is
demonstrated. An approach to handling metaphor in a
computational framework is described, based on the
idea of selective inferencing. Three examples of meta-
phors are examined in detail in this light — a simple
metaphor, a spatial metaphor schema, and a novel
metaphor. Finally, there is a discussion, from this
perspective, of the analogical processes that underlie
metaphor in this approach and what the approach says
about several classical questions about metaphor.
</bodyText>
<subsectionHeader confidence="0.894991">
Representation of Task-Specific Knowledge
in a Gracefully Interacting User Interface
Eugene Ball and Phil Hayes
Department of Computer Science
Carnegie-Mellon University
Schenley Park
Pittsburgh, Pennsylvania 15213
Technical Report CMU-CS-80-123, April 1980.
</subsectionHeader>
<bodyText confidence="0.999973458333333">
Command interfaces to current interactive systems
often appear inflexible and unfriendly to casual and
expert users alike. We are constructing an interface
that will behave more cooperatively (by correcting
spelling and grammatical errors, asking the user to
resolve ambiguities in subparts of commands, etc.).
Given that present-day interfaces often absorb a major
portion of implementation effort, such a gracefully
interacting interface can only be practical if it is inde-
pendent of the specific tool or functional subsystem
with which it is used.
Our interface is tool-independent in the sense that
all its information about a particular tool is expressed
in a declarative tool description. This tool description
contains schemas for each operation that the tool can
perform, and for each kind of object known to the
system. The operation schemas describe the relevant
parameters, their types and defaults, and the object
schemas give corresponding structural descriptions in
terms of defining and derived subcomponents. The
schemas also include input syntax, display formats,
and explanatory text. We discuss how these schemas
can be used by the tool-independent interface to pro-
vide a graceful interface to the tool they describe.
</bodyText>
<subsectionHeader confidence="0.942038285714286">
Flexible Parsing
Phil Hayes and George Mouradian
Department of Computer Science
Carnegie-Mellon University
Schenley Park
Pittsburgh, Pennsylvania 15213
Technical Report CMU-CS-80-122, May 1980.
</subsectionHeader>
<bodyText confidence="0.9999799">
When people use natural language in natural set-
tings, they often use it ungrammatically, missing out or
repeating words, breaking-off and restarting, speaking
in fragments, etc. Their human listeners are usually
able to cope with these deviations with little difficulty.
If a computer system wishes to accept natural language
input from its users on a routine basis, it must display
a similar indifference. In this paper, we outline a set
of parsing flexibilities that such a system should pro-
vide. We go on to describe FlexP, a bottom-up
</bodyText>
<page confidence="0.660611">
54 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
<note confidence="0.644804">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.998937">
pattern-matching parser that we have designed and
implemented to provide these flexibilities for restricted
natural language input to a limited-domain computer
system.
</bodyText>
<subsectionHeader confidence="0.906635857142857">
A Parsing System for Montague Grammar
with Lexical Transformations
Steven E. Tolkin
Computer and Communication Sciences
2076 Frieze Building
University of Michigan
Ann Arbor, Michigan 48109
</subsectionHeader>
<subsubsectionHeader confidence="0.639104">
Computer Studies in Formal Ling. N-24, Sept. 1980.
</subsubsectionHeader>
<bodyText confidence="0.992776363636364">
David Dowty&apos;s proposal to extend the grammar of
Montague&apos;s The proper treatment of quantification in
ordinary English by lexical transformations of verbs
was implemented by modifying an existing system.
Fundamental changes were made to the structure of
the lexicon and to the ATN representation of the
grammar. The viability of this implementation sup-
ports Dowty&apos;s category-changing approach to lexically
governed transformations, and provides a base upon
which further extensions to the grammar may be test-
ed.
</bodyText>
<subsectionHeader confidence="0.6352452">
The ATN and the Sausage Machine:
Which One is Baloney?
Eric Wanner
University of Sussex
Brighton BN1 9QL, ENGLAND
</subsectionHeader>
<bodyText confidence="0.988714360655738">
Cognition 8, 2 (June 1980), 209-225.
In a recent issue of Cognition, Lyn Frazier and Jan-
et Dean Fodor proposed a new two-stage parsing mod-
el, dubbed the Sausage Machine (Frazier and Fodor,
1978). One of the major results which Frazier and
Fodor bring forward in support of their proposal con-
cerns a parsing strategy which, following Kimball
(1973), they call Right Association. The center-piece
of their argument concerns an interaction between this
parsing strategy and another one, which they call Min-
imal Attachment. Frazier and Fodor (henceforth FF)
provide interesting evidence that the language user
makes tacit use of both strategies to resolve temporary
syntactic ambiguities that arise during parsing. FF
then proceed to argue that the existence of these stra-
tegies, as well as the apparent interaction between
them, can be fully explained if we assume that the
language user&apos;s parsing system is configured along the
lines of the Sausage Machine. In FF&apos;s view, the Aug-
mented Transition Network (ATN) runs a very poor
second to the Sausage Machine, for according to FF&apos;s
argument, it is impossible even to describe the two
parsing strategies within the ATN framework. In ef-
fect then, FF are claiming that the Sausage Machine
achieves explanation adequacy in this case while the
ATN fails to reach the level of descriptive adequacy.
These are strong and potentially important claims
If correct, they obviously provide grounds for pursuing
parsing models built along the lines of the Sausage
Machine rather than the ATN. However, when FF&apos;s
arguments are examined at close range, the compari-
son between parsing systems comes out rather differ-
ently than they claim. In particular, it appears that the
Sausage Machine explanation of Right Association and
its interaction with Minimal Attachment is empirically
incorrect. The inadequacy of this explanation com-
pletely cancels the Sausage Machine&apos;s ability to de-
scribe the interaction between strategies that FF have
observed. This follows because FF aspire to an expla-
nation that renders independent description of the
parsing strategies unnecessary. The Sausage Machine
contains no apparatus for describing strategies.
Hence, the failure to achieve explanatory adequacy
automatically entails descriptive failure as well. In
contrast, and in contradiction of FF&apos;s negative claim,
the ATN can provide a perfectly general description
for each strategy in terms of scheduling principles that
constrain the order in which arcs in an ATN grammar
are attempted. Moreover, when these scheduling prin-
ciples are coupled with an ATN version of the gram-
mar FF tacitly employed to generate their pivotal cas-
es, FF&apos;s observations about the interactions between
strategies are completely accounted for. Thus, al-
though the ATN framework does not provide an ex-
planation for either parsing strategy, it appears to
achieve descriptive adequacy. Moreover, the descrip-
tive framework of the ATN makes it possible to dis-
cern just what phenomena require explanation and to
speculate in a reasonable way about the explanatory
principles that underlie the parsing strategies FF have
discovered.
</bodyText>
<subsectionHeader confidence="0.862040833333333">
An Improved Context-Free Recognizer
Susan L. Graham, Michael A. Harrison,
and Walter L. Ruzzo
Computer Science Division
University of California
Berkeley, California 94720
</subsectionHeader>
<subsubsectionHeader confidence="0.496073">
ACM Trans. Frog. Lang. Syst. 2, 3 (July 1980), 415-462.
</subsubsectionHeader>
<bodyText confidence="0.955514882352941">
A new algorithm for recognizing and parsing arbi-
trary context-free languages is presented, and several
new results are given on the computational complexity
of these problems. The new algorithm is of both prac-
tical and theoretical interest. It is conceptually simple
and allows a variety of efficient implementations,
which are worked out in detail. Two versions are
given which run in faster than cubic time. Surprisingly
close connections between the Cocke-Kasami-Younger
and Earley algorithms are established which reveal
that the two algorithms are &amp;quot;almost&amp;quot; identical. One
significant use of general context-free methods is as
part of a system of processing natural languages such
American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 55
The FINITE STRING Newsletter Abstracts of Current Literature
as English, viewing the grammar/parser as a conven-
ient control structure for directing the analysis.
</bodyText>
<subsectionHeader confidence="0.700043875">
The Hearsay-II Speech-Understanding System:
Integrating Knowledge to Resolve Uncertainty
Lee D. Erman
USC/Information Sciences Institute
Marina del Rey, California 90291
Frederick Hayes-Roth
The RAND Corporation
Santa Monica, California 90406
</subsectionHeader>
<author confidence="0.433054">
Victor R. Lesser
</author>
<affiliation confidence="0.3735795">
University of Massachusetts
Amherst, Massachusetts 01003
</affiliation>
<sectionHeader confidence="0.213594" genericHeader="method">
D. Raj Reddy
</sectionHeader>
<subsectionHeader confidence="0.3317825">
Carnegie-Mellon University
Pittsburgh, Pennsylvania 15213
</subsectionHeader>
<bodyText confidence="0.983936717391304">
Computing Surveys 12, 2 (June 1980), 213-253.
The Hearsay-II system, developed at Carnegie-
Mellon University during the DARPA-sponsored five-
year speech-understanding research program, repre-
sents both a specific solution to the speech-
understanding problem and a general framework for
coordinating independent processes to achieve cooper-
ative problem-solving behavior. Speech-understand-
ing, as a computational problem, reflects a large num-
ber of intrinsically interesting issues. Spoken sounds
are achieved by a long chain of successive transforma-
tions, from intentions, through semantic and syntactic
structuring, eventually resulting in audible acoustic
waves. As a consequence, interpreting speech means
effectively inverting these transformations to recover
the speaker&apos;s intention from the sound. At each step
in the interpretive process, ambiguity and uncertainty
arise.
The Hearsay-II problem-solving framework recon-
structs an intention from hypothetical interpretations
formulated at various levels of abstraction. In addi-
tion, it allocates limited processing resources first to
the most promising incremental actions. The final
configuration of the Hearsay-II system comprises
problem-solving components to generate and evaluate
speech hypotheses, and a focus-of-control mechanism
to identify potential actions of greatest value. Many
of these specific procedures reveal novel approaches to
speech problems. Most importantly, the system suc-
cessfully integrates and coordinates all of these inde-
pendent activities to resolve uncertainty and control
combinatorics. Several adaptations of the Hearsay-II
framework have already been undertaken in other
problem domains and we anticipate this trend will
continue; many future systems necessarily will inte-
grate diverse sources of knowledge to solve complex
problems cooperatively.
This paper discusses the characteristics of the
speech problem in particular, the special kinds of
problem-solving uncertainty in that domain, the struc-
ture of the Hearsay-II system developed to cope with
that uncertainty, and the relationship between
Hearsay-II&apos;s structure and those of other speech-
understanding systems. The paper is intended for the
general computer science audience and presupposes no
speech or artificial intelligence background.
</bodyText>
<reference confidence="0.879070857142857">
Morphosemantic Analysis of -ITIS Forms
in Medical Language
M.G. Pacak, L.M. Norton, and G.S. Dunham
Division of Computer Research and Technology
National Institutes of Health
Bethesda, Maryland 20205
Meth. Inform. Med. 19, 2 (April 1980), 99-105.
</reference>
<bodyText confidence="0.999332666666667">
This paper describes an automated procedure for
morphosemantic analysis and semantic interpretation
of medical compound word forms ending in -ITIS.
The requirements for morphosemantic analysis of -
ITIS forms include: a) semantic classification of mor-
phosemantic constituents forming -ITIS words forms,
b) establishment of morphosemantic distribution pat-
terns occurring in -ITIS form, c) preparation of para-
phrasing rules.
</bodyText>
<subsectionHeader confidence="0.96040225">
Lexical Analysis of German Texts
Ingeborg Steinacker and Harald Trost
Department of Medical Cybernetics
University of Vienna
</subsectionHeader>
<bodyText confidence="0.98444325">
SIGLASH Newsletter 13, 3 (Sept. 1980), 6-12.
We present a computerized method for reducing
inflected German words to their stem. The German
language has many possibilities of inflecting words
(nouns have case endings, endings of adjectives de-
pend on case and gender of the noun they belong to,
etc.) Therefore it frequently happens that an inflected
word can be reduced to more than one stem. In this
case all possible dictionary entries to which the word
can be reduced are stored and the final selection can
only be done by semantic means. The program is
written in PASCAL and runs on a 16 bit machine.
</bodyText>
<page confidence="0.829562">
56 American Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978229">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<author confidence="0.592396">Annual Meeting of the Cognitive Science</author>
<note confidence="0.280189333333333">be held in Berkeley, California, August 19-21, 1981. Information about the meeting can be obtained from:</note>
<author confidence="0.999496">Robert Wilensky</author>
<affiliation confidence="0.997599333333333">Computer Science Division Department of EECS University of California, Berkeley</affiliation>
<address confidence="0.998906">Berkeley, California 94720</address>
<note confidence="0.31719625">International Joint Conference on Arti- Intelligence be held in Vancouver, British Canada, August 24-28, 1981. [See 194.]</note>
<title confidence="0.4415545">Questions about the technical program should be addressed to:</title>
<author confidence="0.899676">Roger C Schank</author>
<affiliation confidence="0.652694">Program Chairman, IJCAI-81 Department of Computer Science</affiliation>
<address confidence="0.979521">Box 2158 Yale Station</address>
<affiliation confidence="0.876018">Yale University</affiliation>
<address confidence="0.999669">New Haven, Connecticut 06520</address>
<phone confidence="0.997856">(203) 436-0606</phone>
<note confidence="0.711332">General questions about the conference may be addressed to:</note>
<author confidence="0.549872">Pat Hayes General Chairman</author>
<author confidence="0.549872">IJCAI-</author>
<affiliation confidence="0.997102">Department of Computer Science Mathematical Sciences Building University of Rochester</affiliation>
<address confidence="0.998487">Rochester, New York 14627</address>
<note confidence="0.893977">Annual Conference of the British Society the Philosophy of Science be held at Wivenhoe House at the University of Essex, September 18-20, [See 6:3-4, 194.] Further details of the conference may be obtained from:</note>
<author confidence="0.956129">Liaison Officer</author>
<affiliation confidence="0.99984">University of Essex</affiliation>
<address confidence="0.847056">Wivenhoe Park Colchester C04 3SQ, Essex, ENGLAND</address>
<note confidence="0.91497625">ACM Annual Conference be held at the Bonaventure Hotel in Los Angeles, California, 9-11, 1981. [See 6:3-4, 194.] For further information contact:</note>
<author confidence="0.707598">A C Toni Shelter</author>
<address confidence="0.704873666666667">Xerox Corporation, A3-49 701 South Aviation Boulevard El Segundo, California 90245</address>
<phone confidence="0.894985">(213) 679-4511 x1968</phone>
<title confidence="0.980109">Abstracts of Current Literature The Transformational Question Answering (TQA) System: Description, Operating Experience, and Implications</title>
<author confidence="0.999999">Fred J Damerau</author>
<affiliation confidence="0.9954775">Mathematical Sciences Department IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.927529">Yorktown Heights, New York 10598</address>
<note confidence="0.7362895">Research Report RC 8287, May 1980. This paper sketches the structure of the TQA sys-</note>
<abstract confidence="0.983155428571428">tem with some examples, discusses some of the results of running the system in a user site during the year 1978, and outlines a few of the implications of a natural language query system for larger and more diverse data bases. It is apparent that a number of problems remain to be solved, but it also seems likely that a useful system can be constructed for some domains.</abstract>
<title confidence="0.996952">Design for an Intelligent Office System</title>
<author confidence="0.999945">Janet L Kolodner</author>
<affiliation confidence="0.9820865">Department of Computer Science Yale University</affiliation>
<address confidence="0.991286">New Haven, Connecticut 06520</address>
<note confidence="0.8110915">IBM Research Report RC 8385, July 1980. This paper addresses the problem of designing an</note>
<abstract confidence="0.995437625">intelligent office system to help principals (managerial, administrative, or professional level office workers) with their work. A system is described which would use normative models of semi-routine office procedures to anticipate the user&apos;s next steps, to provide him automatically with files, documents, and forms he needs to complete a task, and to perform automated subtasks at the correct times. A number of intelligent functions that would perform some of the tasks secretaries generally do (scheduling, reminding, filing) are described, along with the knowledge necessary for performing those functions. In addition, an approach to building a more intelligent system which would understand natural language commands given by the user and understand and deal with routine mail is presented.</abstract>
<title confidence="0.9934745">Memory Organization and Search Processes for Narratives</title>
<author confidence="0.999989">Michael G Dyer</author>
<author confidence="0.999989">Wendy G Lehnert</author>
<affiliation confidence="0.982132">Department of Computer Science Yale University</affiliation>
<address confidence="0.990866">New Haven, Connecticut 06520</address>
<note confidence="0.942946">Research Report 175, April 1980.</note>
<abstract confidence="0.895515333333333">BORIS represents the first system to integrate the knowledge-based inference techniques of scripts, plans, goals, and themes, within a single narrative understanding program. This report discusses techniques by BORIS for memory representation and memo- Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 The FINITE STRING Newsletter Abstracts of Current Literature ry retrieval. Emphasis is placed on human question answering abilities, and the heuristics needed to simulate these phenomena. An example narrative processed by BORIS is discussed in detail and used to illustrate design decisions.</abstract>
<affiliation confidence="0.962325333333333">What Does it Mean to Understand Language? Computer Science Department Stanford University</affiliation>
<address confidence="0.999636">Stanford, California 94305</address>
<abstract confidence="0.993327586956521">Cognitive Science 4, 3 (July-Sept. 1980), 209-241. In its earliest drafts, this paper was a structured argument, presenting a comprehensive view of cognitive science, criticizing prevailing approaches to the study of language and thought and advocating a new way of looking at things. Although I strongly believed in the approach it outlined, somehow it didn&apos;t have the convincingness on paper that it had in my own reflection. After some discouraging attempts at reorganization and rewriting, I realized that there was a mismatch between the nature of what I wanted to say and the form in which I was trying to communicate. The understanding on which it was based does not have the form of a carefully structured framework into which all of cognitive science can be placed. It is more an orientation — a way of approaching the phenomena — that has grown out of many different experiences and influences and that bears the marks of its history. I found myself wanting to describe a path rather than justify its destination, finding that in the flow, the ideas came across more clearly. Since this collection was envisioned as a panorama of contrasting individual views, I have taken the liberty of making this chapter explicitly personal and describing the evolution of my own understanding. My interests have centered around natural language. I have been engaged in the design of computer programs that in some sense could be said to &amp;quot;understand language,&amp;quot; and this has led to looking at many aspects of the problems, including theories of meaning, representation formalisms, and the design and construction of complex computer systems. There has been a continuous evolution in my understanding of just what it means to say that a person or computer &amp;quot;understands,&amp;quot; and this story can be read as recounting that evolution. It is long, because it is still too to look back and say &amp;quot;what I was at for all those years was the one basic idea that ...&amp;quot; I am too close and too involved in its continuation to see beyond the twists and turns. The last sections of the paper describe a viewpoint that differs in significant ways from most current approaches, and that offers new possibilities for a deeper understanding of language and a grasp on some previously intractable or unrecognized problems. I hope that it will give some sense of where the path is headed.</abstract>
<title confidence="0.760306">Language and Memory</title>
<author confidence="0.471663">C</author>
<affiliation confidence="0.9815425">Department of Computer Science Yale University</affiliation>
<address confidence="0.990642">New Haven, Connecticut 06520</address>
<abstract confidence="0.944730714285714">Cognitive Science 4, 3 (July-Sept. 1980), 243-284. This paper outlines some of the issues and basic philosophy that have guided my work and that of my students in the last ten years. It describes the progression of conceptual representational theories developed during that time, as well as some of the research models built to implement those theories. The paper concludes with a discussion of my most recent work in the area of modelling memory. It presents a theory of MOPs (Memory Organization Packets), which serve as both processors and organizers of information in memory. This enables effective categorization of experiences in episodic memory, which in turn enables better predictive understanding of new experiences.</abstract>
<title confidence="0.9457195">Miss RI&amp;quot;: Judging the Clarity of Directions</title>
<author confidence="0.999973">Christopher K Riesbeck</author>
<affiliation confidence="0.9822635">Department of Computer Science Yale University</affiliation>
<address confidence="0.995873">New Haven, Connecticut 06520</address>
<abstract confidence="0.981343">Cognitive Science 4, 3 (July-Sept. 1980), 285-303. there has been some very Al work done on the representation of knowledge of large scale maps and their use in the construction of routes through some space (Kuipers, 1978; McDermott, 1980). The stress in this work has been on how spatial information is best represented and how reasoning is performed with the assumed representation. In this paper I would like to address the natural language processing of texts giving directions and make a claim casual first actually does not need to be much spatial reasoning going on at all. When I read a written set of directions, I am primarily interested whether the directions seem clear and sensible not in constructing a map or program specifying all the turns, distances, and locates that will be involved. Certainly there are times when I have to make some kind of route or map structure, such as when I am the subject in a spatial reasoning experiment, or when I am lost and ask someone on the street for directions. But I do not need to work so hard when I am given a piece of paper with a set of directions, and I know that I will have this piece of paper with me when I actually make the trip. I know that I can always read the directions again to figure out what to I actually set out on the trip.</abstract>
<note confidence="0.921071">Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981</note>
<title confidence="0.998511">The FINITE STRING Newsletter Abstracts of Current Literature On Throwing Out the Baby with the Bathwater: A Reply to Black and Wilensky&apos;s Evaluation of Story Grammars</title>
<author confidence="0.999996">Jean M Mendler</author>
<affiliation confidence="0.999967">Department of Psychology University of California, San Diego</affiliation>
<address confidence="0.994495">Jolla, California</address>
<author confidence="0.999724">Nancy S Johnson</author>
<affiliation confidence="0.999906">State University of New York at Buffalo</affiliation>
<address confidence="0.998269">Amherst, New York 14226</address>
<note confidence="0.4388735">Cognitive Science 4, 3 (July-Sept. 1980), 305-312. A number of criticisms of a recent paper by Black</note>
<abstract confidence="0.998838428571429">and Wilensky (1979) are made. (1) In attempting to assess the observational adequacy of story grammars, they state that a context-free grammar cannot handle discontinuous elements; however, they do not show that such elements occur in the domain to which the grammars apply. Further, they do not present adequate evidence for their claim that there are acceptable stories not accounted for by existing grammars and that the grammars will accept non-stories such as procedures. (2) They state that it has been proven that under natural conditions children cannot learn transformational grammars, which is a misrepresentation of the learnability proofs which have been offered. (3) Most important, they take an unduly narrow approach to story understanding by claiming that people only understand story content and do not have knowledge of story structure which is useful in comprehension or memory. Counter-evidence from the literature is cited which indicates that such knowledge is both useful and used, and a number of methods for assessing the psychological adequacy of structural models are discussed.</abstract>
<title confidence="0.999673">On Evaluating Story Grammars</title>
<author confidence="0.999984">David E Rumelhart</author>
<affiliation confidence="0.999991">University of California, San Diego</affiliation>
<address confidence="0.992217">La Jolla, California 92093</address>
<abstract confidence="0.979104379310345">Cognitive Science 4, 3 (July-Sept. 1980), 313-316. In their recent article entitled &amp;quot;An Evaluation of Story Grammars,&amp;quot; Black and Wilensky (1979) offer a critique of the recent work on this topic. They argue that story grammars (or story schemata as I prefer to call them) are not a productive approach to the study of story understanding, and they offer three main lines of argumentation. First, they argue that story gramare not in as much as most of them are represented as a set of context free rewrite rules which are known to be inadequate even for sentence grammars. Second, they argue that story gramare not in as much as there are stories which do not seem to follow story grammars and there are nonstories which do. Finally, they argue that story grammars could not form an adequate basis f or a comprehension model since in order to apply the grammar you need to have interpreted the story. These arguments are in my opinion, indicative of a misunderstanding of the enterprise that I and others working on these issues have been engaged in. I believe that they are all based on a misunderstanding about what grammars might be good for and about how comprehension might occur. In this response, I wish to clarify the nature of story schemata as I understand them, clarify the nature of Black and Wilensky&apos;s misunderstandings and show how each of their arguments fails to address the important issues about story grammars and story schemata.</abstract>
<title confidence="0.416495">Natural Language Understanding</title>
<affiliation confidence="0.9990045">Department of Computer Science Stanford University</affiliation>
<address confidence="0.998568">Palo Alto, California 94305</address>
<abstract confidence="0.981437411764706">Al Magazine 1, 1 (Spring 1980), 5-10. is an excerpt from the of Artificial compendium of hundreds of articles about Al ideas, techniques, and programs being prepared at Stanford University by Al researchers and students from across the country. This article, which is from the chapter on Natural Language Understanding, presents a brief sketch of the history of natural language processing research and gives an idea of the current state of the art. The other articles in the NL chapter of the Handbook include a historical sketch of machine translation, technical articles on grammars and parsing techniques, and an article on text generation. Finally, there are several articles describing the NL programs themselves. The Handbook also includes chapters on speech understanding and knowledge representation.</abstract>
<title confidence="0.986576">Utterance and Objective: Issues in Natural Language Communication</title>
<author confidence="0.999998">Barbara J Grosz</author>
<affiliation confidence="0.999874">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.999201">Menlo Park, California 94025</address>
<abstract confidence="0.966770375">Al Magazine 1, 1 (Spring 1980), 11-20. Two premises, reflected in the title, underlie the perspective from which I will consider research in natural language processing in this paper. First, progress on building computer systems that process natural languages in any meaningful sense (i.e., systems that interact reasonably with people in natural language) requires considering language as part of a larger communicative situation. In this larger situation, the participants in a conversation and their states of mind are as important to the interpretation of an utterance as the linguistic expressions from which it is formed. A central concern when language is considered as communication is its function in building and using shared models of the world. Journal of Computational Linguistics, Volume 7, Number 1, January-March 53 The FINITE STRING Newsletter Abstracts of Current Literature Second, as the phrase &amp;quot;utterance and objective&amp;quot; suggests, regarding language as communication requires consideration of what is said literally, what is intended, and the relationship between the two. Recently, the emphasis in research in natural language processing has begun to shift from an analysis of utterances as isolated linguistic phenomena to a consideration of how people use utterances to achieve certain objectives. But, in considering objectives, it is important not to ignore the utterances themselves. A consideration of a speaker&apos;s underlying goals and motivations is critical, but so is an analysis of the particular way in which that speaker expresses his thoughts. This paper examines three consequences of these claims for the development of language processing theories and the construction of language processing programs: (1) language processing requires a combination of language-specific mechanisms and general common-sense reasoning mechanisms, (2) language systems must be able to represent the beliefs and knowledge of multiple individual agents, and (3) utterances must be viewed as having effects along multiple dimensions.</abstract>
<title confidence="0.963081">Conversation as Planned Behavior</title>
<author confidence="0.982212">R</author>
<author confidence="0.982212">David A Evans</author>
<affiliation confidence="0.999559">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9978875">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.98186625">Technical Note 203, Dec. 1979. In this paper, planning models developed in artificial intelligence are applied to the kind of planning that must be carried out by participants in a conversation. A planning mechanism is defined, and a short fragment of a free-flowing videotaped conversation is described. The bulk of the paper is then devoted to an attempt to understand the conversation in terms of the planning mechanism. This microanalysis suggests ways in which the planning mechanisms must be augmented, and reveals several important conversational phenomena that deserve further investigation.</abstract>
<title confidence="0.973242">Metaphor, Metaphor Schemata and Selective Inferencing</title>
<author confidence="0.999949">Jerry R Hobbs</author>
<affiliation confidence="0.999738">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9978055">333 Ravenswood Avenue Menlo Park, California 94025</address>
<abstract confidence="0.912473909090909">Technical Note 204, Dec. 1979. The importance of spatial and other metaphors is demonstrated. An approach to handling metaphor in a computational framework is described, based on the idea of selective inferencing. Three examples of metaphors are examined in detail in this light — a simple metaphor, a spatial metaphor schema, and a novel metaphor. Finally, there is a discussion, from this perspective, of the analogical processes that underlie metaphor in this approach and what the approach says about several classical questions about metaphor.</abstract>
<title confidence="0.9576125">Representation of Task-Specific Knowledge in a Gracefully Interacting User Interface</title>
<author confidence="0.999584">Eugene Ball</author>
<author confidence="0.999584">Phil Hayes</author>
<affiliation confidence="0.9997905">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.8923975">Schenley Park Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.776175">Technical Report CMU-CS-80-123, April 1980.</note>
<title confidence="0.376069">Command interfaces to current interactive systems</title>
<abstract confidence="0.999524173913043">often appear inflexible and unfriendly to casual and expert users alike. We are constructing an interface that will behave more cooperatively (by correcting spelling and grammatical errors, asking the user to resolve ambiguities in subparts of commands, etc.). Given that present-day interfaces often absorb a major portion of implementation effort, such a gracefully interacting interface can only be practical if it is independent of the specific tool or functional subsystem with which it is used. Our interface is tool-independent in the sense that all its information about a particular tool is expressed in a declarative tool description. This tool description contains schemas for each operation that the tool can perform, and for each kind of object known to the system. The operation schemas describe the relevant parameters, their types and defaults, and the object schemas give corresponding structural descriptions in terms of defining and derived subcomponents. The schemas also include input syntax, display formats, and explanatory text. We discuss how these schemas can be used by the tool-independent interface to provide a graceful interface to the tool they describe.</abstract>
<title confidence="0.990069">Flexible Parsing</title>
<author confidence="0.999849">Phil Hayes</author>
<author confidence="0.999849">George Mouradian</author>
<affiliation confidence="0.9997495">Department of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.892155">Schenley Park Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.919059">Technical Report CMU-CS-80-122, May 1980.</note>
<abstract confidence="0.9674546875">When people use natural language in natural settings, they often use it ungrammatically, missing out or repeating words, breaking-off and restarting, speaking in fragments, etc. Their human listeners are usually able to cope with these deviations with little difficulty. If a computer system wishes to accept natural language input from its users on a routine basis, it must display a similar indifference. In this paper, we outline a set of parsing flexibilities that such a system should provide. We go on to describe FlexP, a bottom-up Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 The FINITE STRING Newsletter Abstracts of Current Literature pattern-matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system.</abstract>
<title confidence="0.9203195">A Parsing System for Montague Grammar with Lexical Transformations</title>
<author confidence="0.999982">Steven E Tolkin</author>
<affiliation confidence="0.992614">Computer and Communication Sciences</affiliation>
<address confidence="0.99707">2076 Frieze Building</address>
<affiliation confidence="0.996092">University of Michigan</affiliation>
<address confidence="0.988859">Ann Arbor, Michigan 48109</address>
<note confidence="0.5917715">Computer Studies in Formal Ling. N-24, Sept. 1980. David Dowty&apos;s proposal to extend the grammar of</note>
<abstract confidence="0.9964388">proper treatment of quantification in English lexical transformations of verbs was implemented by modifying an existing system. Fundamental changes were made to the structure of the lexicon and to the ATN representation of the grammar. The viability of this implementation supports Dowty&apos;s category-changing approach to lexically governed transformations, and provides a base upon which further extensions to the grammar may be tested.</abstract>
<title confidence="0.951155">The ATN and the Sausage Machine: Which One is Baloney?</title>
<author confidence="0.999957">Eric Wanner</author>
<affiliation confidence="0.99976">University of Sussex</affiliation>
<address confidence="0.988468">Brighton BN1 9QL, ENGLAND</address>
<abstract confidence="0.992131557377049">Cognition 8, 2 (June 1980), 209-225. a recent issue of Frazier and Janet Dean Fodor proposed a new two-stage parsing model, dubbed the Sausage Machine (Frazier and Fodor, 1978). One of the major results which Frazier and Fodor bring forward in support of their proposal concerns a parsing strategy which, following Kimball (1973), they call Right Association. The center-piece of their argument concerns an interaction between this parsing strategy and another one, which they call Minimal Attachment. Frazier and Fodor (henceforth FF) provide interesting evidence that the language user makes tacit use of both strategies to resolve temporary syntactic ambiguities that arise during parsing. FF then proceed to argue that the existence of these strategies, as well as the apparent interaction between them, can be fully explained if we assume that the language user&apos;s parsing system is configured along the lines of the Sausage Machine. In FF&apos;s view, the Augmented Transition Network (ATN) runs a very poor second to the Sausage Machine, for according to FF&apos;s argument, it is impossible even to describe the two parsing strategies within the ATN framework. In effect then, FF are claiming that the Sausage Machine achieves explanation adequacy in this case while the ATN fails to reach the level of descriptive adequacy. These are strong and potentially important claims If correct, they obviously provide grounds for pursuing parsing models built along the lines of the Sausage Machine rather than the ATN. However, when FF&apos;s arguments are examined at close range, the comparison between parsing systems comes out rather differently than they claim. In particular, it appears that the Sausage Machine explanation of Right Association and its interaction with Minimal Attachment is empirically incorrect. The inadequacy of this explanation completely cancels the Sausage Machine&apos;s ability to describe the interaction between strategies that FF have observed. This follows because FF aspire to an explanation that renders independent description of the parsing strategies unnecessary. The Sausage Machine contains no apparatus for describing strategies. Hence, the failure to achieve explanatory adequacy automatically entails descriptive failure as well. In contrast, and in contradiction of FF&apos;s negative claim, the ATN can provide a perfectly general description for each strategy in terms of scheduling principles that constrain the order in which arcs in an ATN grammar are attempted. Moreover, when these scheduling principles are coupled with an ATN version of the grammar FF tacitly employed to generate their pivotal cases, FF&apos;s observations about the interactions between strategies are completely accounted for. Thus, although the ATN framework does not provide an explanation for either parsing strategy, it appears to achieve descriptive adequacy. Moreover, the descriptive framework of the ATN makes it possible to discern just what phenomena require explanation and to speculate in a reasonable way about the explanatory principles that underlie the parsing strategies FF have discovered.</abstract>
<title confidence="0.994488">An Improved Context-Free Recognizer</title>
<author confidence="0.9940495">Susan L Graham</author>
<author confidence="0.9940495">Michael A Harrison</author>
<author confidence="0.9940495">Walter L Ruzzo</author>
<affiliation confidence="0.9999705">Computer Science Division University of California</affiliation>
<address confidence="0.999865">Berkeley, California 94720</address>
<note confidence="0.793138">ACM Trans. Frog. Lang. Syst. 2, 3 (July 1980), 415-462.</note>
<abstract confidence="0.965191235294118">A new algorithm for recognizing and parsing arbitrary context-free languages is presented, and several new results are given on the computational complexity of these problems. The new algorithm is of both practical and theoretical interest. It is conceptually simple and allows a variety of efficient implementations, which are worked out in detail. Two versions are given which run in faster than cubic time. Surprisingly close connections between the Cocke-Kasami-Younger and Earley algorithms are established which reveal that the two algorithms are &amp;quot;almost&amp;quot; identical. One significant use of general context-free methods is as a system of processing natural languages such Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981 The FINITE STRING Newsletter Abstracts of Current Literature as English, viewing the grammar/parser as a convenient control structure for directing the analysis.</abstract>
<title confidence="0.994727">The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty</title>
<author confidence="0.999984">Lee D Erman</author>
<affiliation confidence="0.999826">USC/Information Sciences Institute</affiliation>
<address confidence="0.998574">Marina del Rey, California 90291</address>
<author confidence="0.997806">Frederick Hayes-Roth</author>
<affiliation confidence="0.983633">The RAND Corporation</affiliation>
<address confidence="0.999429">Santa Monica, California 90406</address>
<author confidence="0.999776">Victor R Lesser</author>
<affiliation confidence="0.999988">University of Massachusetts</affiliation>
<address confidence="0.999962">Amherst, Massachusetts 01003</address>
<author confidence="0.997479">D Raj Reddy</author>
<affiliation confidence="0.999995">Carnegie-Mellon University</affiliation>
<address confidence="0.99894">Pittsburgh, Pennsylvania 15213</address>
<note confidence="0.7096875">Computing Surveys 12, 2 (June 1980), 213-253. The Hearsay-II system, developed at Carnegie-</note>
<abstract confidence="0.987138613636363">Mellon University during the DARPA-sponsored fiveyear speech-understanding research program, represents both a specific solution to the speechunderstanding problem and a general framework for coordinating independent processes to achieve cooperative problem-solving behavior. Speech-understanding, as a computational problem, reflects a large number of intrinsically interesting issues. Spoken sounds are achieved by a long chain of successive transformations, from intentions, through semantic and syntactic structuring, eventually resulting in audible acoustic waves. As a consequence, interpreting speech means effectively inverting these transformations to recover the speaker&apos;s intention from the sound. At each step in the interpretive process, ambiguity and uncertainty arise. The Hearsay-II problem-solving framework reconstructs an intention from hypothetical interpretations formulated at various levels of abstraction. In addition, it allocates limited processing resources first to the most promising incremental actions. The final configuration of the Hearsay-II system comprises problem-solving components to generate and evaluate speech hypotheses, and a focus-of-control mechanism to identify potential actions of greatest value. Many of these specific procedures reveal novel approaches to speech problems. Most importantly, the system successfully integrates and coordinates all of these independent activities to resolve uncertainty and control combinatorics. Several adaptations of the Hearsay-II framework have already been undertaken in other problem domains and we anticipate this trend will continue; many future systems necessarily will integrate diverse sources of knowledge to solve complex problems cooperatively. This paper discusses the characteristics of the speech problem in particular, the special kinds of problem-solving uncertainty in that domain, the structure of the Hearsay-II system developed to cope with that uncertainty, and the relationship between Hearsay-II&apos;s structure and those of other speechunderstanding systems. The paper is intended for the general computer science audience and presupposes no speech or artificial intelligence background.</abstract>
<title confidence="0.9902695">Morphosemantic Analysis of -ITIS Forms in Medical Language</title>
<author confidence="0.99435">L M Pacak</author>
<author confidence="0.99435">G S Dunham</author>
<affiliation confidence="0.999722">Division of Computer Research and Technology National Institutes of Health</affiliation>
<address confidence="0.997179">Bethesda, Maryland 20205</address>
<abstract confidence="0.9660842">Meth. Inform. Med. 19, 2 (April 1980), 99-105. This paper describes an automated procedure for morphosemantic analysis and semantic interpretation of medical compound word forms ending in -ITIS. The requirements for morphosemantic analysis of - ITIS forms include: a) semantic classification of morphosemantic constituents forming -ITIS words forms, b) establishment of morphosemantic distribution patterns occurring in -ITIS form, c) preparation of paraphrasing rules.</abstract>
<title confidence="0.906738">Lexical Analysis of German Texts</title>
<author confidence="0.999297">Ingeborg Steinacker</author>
<author confidence="0.999297">Harald Trost</author>
<affiliation confidence="0.9944905">Department of Medical Cybernetics University of Vienna</affiliation>
<abstract confidence="0.983895833333333">SIGLASH Newsletter 13, 3 (Sept. 1980), 6-12. We present a computerized method for reducing inflected German words to their stem. The German language has many possibilities of inflecting words (nouns have case endings, endings of adjectives depend on case and gender of the noun they belong to, etc.) Therefore it frequently happens that an inflected word can be reduced to more than one stem. In this case all possible dictionary entries to which the word can be reduced are stored and the final selection can only be done by semantic means. The program is written in PASCAL and runs on a 16 bit machine.</abstract>
<note confidence="0.833257">Journal of Computational Linguistics, Volume 7, Number 1, January-March 1981</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Morphosemantic Analysis of -ITIS Forms</title>
<note>in Medical Language</note>
<marker></marker>
<rawString>Morphosemantic Analysis of -ITIS Forms in Medical Language</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Pacak</author>
<author>L M Norton</author>
<author>G S</author>
</authors>
<date></date>
<booktitle>Dunham Division of Computer Research and Technology National Institutes of Health</booktitle>
<location>Bethesda, Maryland</location>
<marker>Pacak, Norton, S, </marker>
<rawString>M.G. Pacak, L.M. Norton, and G.S. Dunham Division of Computer Research and Technology National Institutes of Health Bethesda, Maryland 20205</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inform Med</author>
</authors>
<date>1980</date>
<volume>19</volume>
<pages>99--105</pages>
<marker>Med, 1980</marker>
<rawString>Meth. Inform. Med. 19, 2 (April 1980), 99-105.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>