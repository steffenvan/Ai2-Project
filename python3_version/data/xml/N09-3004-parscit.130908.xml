<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009305">
<title confidence="0.980549">
Sentence Realisation from Bag of Words with dependency constraints
</title>
<author confidence="0.983923">
Karthik Gali, Sriram Venkatapathy
</author>
<affiliation confidence="0.9486725">
Language Technologies Research Centre,
IIIT-Hyderabad, Hyderabad, India
</affiliation>
<email confidence="0.909423">
{karthikg@students,sriram@research}.iiit.ac.in
</email>
<sectionHeader confidence="0.99798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999472533333333">
In this paper, we present five models for sentence
realisation from a bag-of-words containing mini-
mal syntactic information. It has a large variety
of applications ranging from Machine Translation
to Dialogue systems. Our models employ simple
and efficient techniques based on n-gram Language
modeling.
We evaluated the models by comparing the syn-
thesized sentences with reference sentences using
the standard BLEU metric(Papineni et al., 2001).
We obtained higher results (BLEU score of 0.8156)
when compared to the state-of-art results. In fu-
ture, we plan to incorporate our sentence realiser in
Machine Translation and observe its effect on the
translation accuracies.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966263157895">
In applications such as Machine Translation (MT)
and Dialogue Systems, sentence realisation is a ma-
jor step. Sentence realisation involves generating a
well-formed sentence from a bag of lexical items.
These lexical items may be syntactically related to
each other. The level of syntactic information at-
tached to the lexical items might vary with applica-
tion. In order to appeal to the wide range of applica-
tions that use sentence realisation, our experiments
assume only basic syntactic information, such as un-
labeled dependency relationships between the lexi-
cal items.
In this paper, we present different models for sen-
tence realisation. These models consider a bag of
words with unlabelled dependency relations as input
and apply simple n-gram language modeling tech-
niques to get a well-formed sentence.
We now present the role of a sentence realiser
in the task of MT. In transfer-based approaches for
</bodyText>
<page confidence="0.988064">
19
</page>
<bodyText confidence="0.998522393939394">
MT1 (Lavie et al., 2003), the source sentence is
first analyzed by a parser (a phrase-structure or a
dependency-based parser). Then the source lexical
items are transferred to the target language using a
bi-lingual dictionary. The target language sentence
is finally realised by applying transfer-rules that map
the grammar of both the languages. Generally, these
transfer rules make use of rich analysis on the source
side such as dependency labels etc. The accuracy of
having such rich analysis (dependency labeling ) is
low and hence, might affect the performance of the
sentence realiser. Also, the approach of manually
constructing transfer rules is costly, especially for
divergent language pairs such as English and Hindi
or English and Japanese. Our models can be used
in this scenario, providing a robust alternative to the
transfer rules.
A sentence realiser can also be used in the frame-
work of a two-step statistical machine translation.
In the two-step framework, the semantic transfer
and sentence realisation are decoupled into indepen-
dent modules. This provides an opporunity to de-
velop simple and efficient modules for each of the
steps. The model for Global Lexical Selection and
Sentence Re-construction (Bangalore et al., 2007)
is one such approach. In this approach, discrimi-
native techniques are used to first transfer semantic
information of the source sentence by looking at the
source sentence globally, this obtaining a accurate
bag-of-words in the target language. The words in
the bag might be attached with mild syntactic infor-
mation (ie., the words they modify) (Venkatapathy
and Bangalore, 2007). We propose models that take
</bodyText>
<footnote confidence="0.994096">
1http://www.isi.edu/natural-language/mteval/html/412.html
</footnote>
<note confidence="0.8860245">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19–24,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999972625">
this information as input and produce the target sen-
tence. We can also use our sentence realiser as an
ordering module in other approaches such as (Quirk
et al., 2005), where the goal is to order an unordered
bag (of treelets in this case) with dependency links.
In Natural Language Generation applications
such as Dialogue systems etc, the set of concepts
and the dependencies between the concepts is ob-
tained first which is known as text planning. These
concepts are then realized into words resulting in a
bag of words with syntactic relations (Bangalore and
Rambow, 2000). This is known as sentence plan-
ning. In the end, the surface string can be obtained
by our models.
In this paper, we do not test our models with any
of the applications mentioned above. However, we
plan to test our models with these applications, es-
pecially on the two-stage statistical MT approach
using the bag-of-words obtained by Global Lexi-
cal Selection (Bangalore et al., 2007),(Venkatapathy
and Bangalore, 2007). Here, we test our models in-
dependent of any application, by beginning with a
given bag-of-words (with dependency links).
The structure of the paper is as follows. We give
an overview of the related work in section 2. In sec-
tion 3, we talk about the effect of dependency con-
straints and gives details of the experimental setup in
section 4. In section 5, we describe about the exper-
iments that have been conducted. In section 6, our
experimental results are presented. In section 7, we
talk about the possible future work and we conclude
with section 8.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999555773584906">
There have been approaches for sentence realisation
such as FUF/SURGE (Elhadad, 1991), OpenCCG
(White, 2004) and XLE (Crouch et al., 2007)
that apply hand-crafted grammars based on partic-
ular linguistic theories. These approaches expect
rich syntactic information as input in order to re-
alise the sentence. There are other approaches in
which the generation grammars are extracted semi-
automatically (Belz, 2007) or automatically (such as
HPSG (Nakanishi and Miyao, 2005), LFG (Cahill
and van Genabith, 2006; Hogan et al., 2007) and
CCG (White et al., 2007)). The limitation of these
approaches is that these cannot be incorporated into
a wide range of applications as they rely on rich
syntactic information for generation. On the con-
trary, we use simple n-gram models to realise (or lin-
earize) a bag-of-words where the only information
available is the presence of various links between the
words.
Our work is similar to a recently published work
by Guo (Guo et al., 2008). They use n-gram models
to realise sentences from the f-structures of HPSG
(equivalent to labeled dependency structure). Their
models rely heavily on the dependency relation la-
bels (also called grammatical roles) available in
HPSG. However, the dependency role information
(of any dependency formalism) is either not read-
ily available in a variety of applications in NLP. We
propose to explore the realisation of a sentence us-
ing minimal syntactic information. Apart from de-
pendency links, we also make use of part-of-speech
tags which are easily available and hence, our sen-
tence realiser can be plugged much easily into var-
ious applications. Guo (Guo et al., 2008) conduct
their experiments by considering gold data as input.
Apart from using gold data as input, we also con-
duct experiments by assuming noisy input data to
test the robustness of our models. The search al-
gorithm used by both Guo and us is locally greedy
i.e., we compute the best string at every node. Guo
uses the Viterbi algorithm to get best string whereas
we consider and score all permutations to obtain the
best string.
There has been burgeoning interest in the prob-
abilistic models for sentence realisation, especially
for realisation ranking in a two stage sentence real-
isation architecture where in the first stage a set of
sentence realisations are generated and then a real-
isation ranker will choose the best of them (Banga-
lore and Rambow, 2000).
One major observation in our experiments was
that the POS tags held immensely in the task of sen-
tence realisation.
</bodyText>
<sectionHeader confidence="0.956792" genericHeader="method">
3 Effect of Dependency Constraints
</sectionHeader>
<bodyText confidence="0.999973">
There is a major advantage in using dependency
constraints for sentence realisation. The search
space reduces drastically when the constraints are
applied. These constraints state that the realised sen-
tences should be projective with respect to the de-
</bodyText>
<page confidence="0.967654">
20
</page>
<bodyText confidence="0.999127857142857">
pendency structure (unordered) of the input bag-of-
words ie.., any word and its children in the depen-
dency tree should project as a contiguous unit in the
realised sentence. This is a safe assumption to make
as the non-projectivity in English is only used to
account for Long-Distance Dependencies and such
cases are low in number (Guo et al., 2008).
</bodyText>
<figureCaption confidence="0.979912">
Figure 1: Bag of words with dependency constraints
and head marked
</figureCaption>
<bodyText confidence="0.999978461538461">
We now present an example to show how the de-
pendency constraints reduce the search space. For
example, consider an unordered dependency tree in
Figure 1, which has five words. If we don’t use the
constraints provided by the dependency tree then the
search space is 5! (120). But, if we use the con-
straints provided by the dependency tree then the
search space is 2! + 4! = 28. There is a huge reduc-
tion in the search space if we use the constraints pro-
vided by the dependency tree. Further, it has been
shown in (Chang and Toutanova, 2007) that apply-
ing the constraints also aids for the synthesis of bet-
ter constructed sentences.
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.968043625">
For the experiments, we use the WSJ portion of the
Penn tree bank (Marcus et al., 1993), using the stan-
dard train/development/test splits, viz 39,832 sen-
tences from 2-21 sections, 2416 sentences from sec-
tion 23 for testing and 1,700 sentences from sec-
tion 22 for development. The input to our sen-
tence realiser are bag of words with dependency
constraints which are automatically extracted from
the Penn treebank using head percolation rules used
in (Magerman, 1995), which do not contain any or-
der information. We also use the provided part-of-
speech tags in some experiments.
In a typical application, the input to the sentence
realiser is noisy. To test the robustness of our models
in such scenarios, we also conduct experiments with
noisy input data. We parse the test data with an un-
labelled projective dependency parser (Nivre et al.,
2006) and drop the order information to obtain the
input to our sentence realiser. However we still use
the correct bag of words. We propose to test this as-
pect in future by plugging our sentence realiser in
Machine Translation.
Table 1 shows the number of nodes having a par-
ticular number of children in the test data.
</bodyText>
<table confidence="0.9984975">
Children countNodes Children countNodes
0 30219 5 1017
1 13649 6 685
2 5887 7 269
3 3207 8 106
4 1526 &gt; 8 119
</table>
<tableCaption confidence="0.8481445">
Table 1: The number of nodes having a particular
number of children in the test data
</tableCaption>
<bodyText confidence="0.999486">
From Table 1, we can see that more than 96% of
the internal nodes of the trees contain five or less
children. It means that for almost all the nodes, the
reordering complexity is minimal. This makes this
approach very feasible if the order of a sub-tree is
computed after the order of the sub-trees of its chil-
dren is fixed. Hence, the approaches that we present
in the next section use bottom-up traversal of the
tree. During the traversal, the appropriate order of
every sub-tree is fixed.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999502857142857">
The task here is to realise a well formed sentence
from a bag of words with dependency constraints
(unordered dependency tree) for which we propose
five models using n-gram based Language modeling
techinque. We train the language models of order 3
using Good-Turning smoothing on the training data
of Penn Treebank.
</bodyText>
<subsectionHeader confidence="0.966367">
5.1 Model 1 : Sentential Language Model
</subsectionHeader>
<bodyText confidence="0.976194666666667">
We traverse the tree in bottom up manner and find
the best phrase at each subtree. The best phrase cor-
responding to the subtree is assigned to the root node
of the sub-tree during the traversal.
Let the node n have N children represented as cz
(1 &lt; i &lt; N). During the bottom up traversal, the
</bodyText>
<figure confidence="0.836093333333333">
Ram to school
is
going
</figure>
<page confidence="0.994892">
21
</page>
<bodyText confidence="0.999652222222222">
children ci are assigned best phrases before process-
ing node n. Let the best phrases corresponding to the
children be p(ci). The best phrase corresponding to
the node n is computed by exploring the permuta-
tions of n and the best phrases p(ci) corresponding
to the children ci. The total number of permutations
that are explored are (N +1)!. A sentential language
model is applied on each of the candidate phrases to
select the best phrase.
</bodyText>
<equation confidence="0.495594">
p(n) = bestPhrase ( perm (n, ∀ i p(ci)) o LM )
</equation>
<bodyText confidence="0.988096">
In Sentential Language Model, we used a LM that
is trained on complete sentences of the training cor-
pus to score the permutations.
</bodyText>
<subsectionHeader confidence="0.6433385">
5.2 Model 2 : Subtree-type based Language
Models(STLM)
</subsectionHeader>
<bodyText confidence="0.999957590909091">
The major problem with model 1 is that we are us-
ing a common sentential language model (trained on
complete sentences) to score phrases corresponding
to various sub-tree types. In this model, we build
different LMs for phrases corresponding to different
subtree-types.
To build STLMs, the training data is parsed first.
Each subtree in the parse structure is represented
by the part-of-speech tag of its head. Different lan-
guage models are created for each of the POS tags.
We have 44 different language models each corre-
sponding to a particular POS tag. For example, a
IN language model contains phrases like in hour, of
chaos, after crash, in futures, etc and VBD language
model contains phrases like were criticized, never
resumed while training.
So, in this model we realise a sentence from a
unordered dependency tree by traversing the depen-
dency tree in bottom-up manner as we did in model
1; but while scoring the permuted phrases we use
different language models for subtrees headed by
words of various pos tags.
</bodyText>
<equation confidence="0.737153">
p(n) = bestPhrase (perm (n, ∀ i p(ci)) o LMPOS(n) )
</equation>
<bodyText confidence="0.996847">
Here, LMPOS(n) represents the language model
associated with the part-of-speech of the node n.
</bodyText>
<subsectionHeader confidence="0.972651">
5.3 Model 3 : Head-word STLM
</subsectionHeader>
<bodyText confidence="0.999963833333333">
In the models presented earlier, a node and its chil-
dren are ordered using the best phrases of the chil-
dren. For example, the best phrase assigned to the
node ‘was’ is computed by taking of the permutation
of ‘was’ and its children ‘The equity market’, ‘illiq-
uid’ and ‘.’ and then applying the language model.
In model 3, instead of considering best phrases while
ordering, the heads of the the children ci are consid-
ered. For example, the best phrase assigned to the
node ‘was’ is computed by first permuting the nodes
‘was’, ‘market’, ‘illiquid’ and ‘.’ and then apply-
ing the language models trained on the treelets (head
and children) and not on entire sub-trees.
The major advantage of using this model is that
order at a node is independent of the best phrases of
its descendants and also any mistakes in computa-
tion of best phrases of descendants doesn’t effect the
choice of reordering decision at a particular node.
</bodyText>
<subsectionHeader confidence="0.949418">
5.4 Model 4 : POS based STLM
</subsectionHeader>
<bodyText confidence="0.99998895">
We now experiment by using Part-Of-Speech (POS)
tags of words for ordering the nodes. In the previ-
ous approaches, the language models were trained
on the words which were then used to compute the
best strings associated with various nodes. Here,
we order the node and its children using a language
model trained on POS tag sequences. The motiva-
tion behind buliding such kind of Language models
is that it deals with unseen words effectively. Hence,
in this model, the best phrase corresponding to the
node ‘was’ is obtained by permuting the POS tags
of the words ‘was’, ‘market’, ’illiquid and ’.’ which
are ‘VBZ’, ‘NN’, ‘NN’ and ‘.’ respectively. As the
best POS tag sequence might correspond to several
orderings of the treelet, a word based STLM is ap-
plied to choose the correct ordering.
The major advantages of this model is that it is
more general and it deals with unseen words effec-
tively. Also, it is much faster than earlier models as
this model is a POS tag based model.
</bodyText>
<subsectionHeader confidence="0.995942">
5.5 Model 5: Head-marked POS based STLM
</subsectionHeader>
<bodyText confidence="0.999928">
In POS based STLM, the head of a particular node
isn’t marked while applying the language model.
Hence, all the nodes of the treelet are treated equally
while applying the LM. For example, in Figure 2, the
structures of treelets is not taken into account while
applying the head-POS based language model. Both
are treated in the same manner while applying TLM.
In this model, we experiment by marking the head
</bodyText>
<page confidence="0.991332">
22
</page>
<bodyText confidence="0.9124897">
information for the POS of the head word which
treats the treelets in Figure 2 in a different manner to
obtain the best phrase. As the best POS tag sequence
might correspond to several orderings of the treelet,
we test various word-based approaches to choose the
best ordering among the many possibilities. The best
approach was the one where head-word of the treelet
had the POS tag attached to it.
Figure 2: Two different treelets which would have
same best POS tag sequence
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="method">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9995621">
To evaluate our models, we compare the system gen-
erated sentences with reference sentences and get
the BLEU score. As mentioned in section 4, We
evaluate our models on two different types of in-
put. In the first input type, we have bag of words
with dependency constraints extracted from tree-
bank and in the second input type, the dependency
constraints among the bag of words are extracted
from the parser which are noisy. Table 2 shows the
results of model 1-5.
</bodyText>
<table confidence="0.773123666666667">
Model Treebank(gold) Parser(noisy)
Model 1 0.5472 0.5514
Model 2 0.6886 0.6870
Model 3 0.7284 0.7227
Model 4 0.7890 0.7783
Model 5 0.8156 0.8027
</table>
<tableCaption confidence="0.987806">
Table 2: The results of Model 1-5
</tableCaption>
<bodyText confidence="0.999951727272727">
We can observe that in model 1, BLEU score of
the parser input is high when compared to Treebank
input. This might be because, the parser input is pro-
jective (as we used projective parsing) whereas the
treebank input might contain some non-projective
cases. In general, for all the models, the results with
noisy dependency links are comparable to the cases
where gold dependency links are used which is en-
couraging.
We have taken the Table-3 from (Guo et al.,
2008), which shows the BLEU scores of different
</bodyText>
<table confidence="0.9968185">
Paper BLEU score
Langkilde(2002) 0.757
Nakanishi(2005) 0.705
Cahill(2006) 0.6651
Hogan(2007) 0.6882
White(2007) 0.5768
Guo(2008) 0.7440
Our Model 0.8156
</table>
<tableCaption confidence="0.9725455">
Table 3: Comparsion of results for English WSJ sec-
tion 23
</tableCaption>
<bodyText confidence="0.998049266666667">
systems on section 23 of PTB. Its really difficult to
compare sentence realisers as the information con-
tained in the input vaires greatly between systems.
But, we can clearly see that the our system performs
better than all the systems. The main observations
from the results are, (1) Searching the entire space of
O(n!) helps, (2) Treelet LM capture characteristics
of phrases headed by various POS tags, in contrast to
sentential LM which is a general LM, (3) POS tags
can play an important role in ordering nodes of a de-
pendency structure, (4) The head models performed
better than the models that used all the nodes of the
sub-tree, and (5) Marking the head of a treelet pro-
vides vital clues to the language model for reorder-
ing.
</bodyText>
<sectionHeader confidence="0.997681" genericHeader="method">
7 Future Experiments
</sectionHeader>
<bodyText confidence="0.99991">
Although the results of the proposed models are
much higher when compared to other methods, the
major constraint with our models is the computa-
tional complexity, which is O(n!). However, our ap-
proach is still tractable because of the low values of
n. We plan to reduce the search space complexity by
using Viterbi search (Guo et al., 2008), and examine
the drop in results because of that.
The models proposed in paper, consider only the
locally best phrases (local to the sub-tree) at every
step. In order to retain the globally best possibilities
at every step, we plan to use beam search, where we
retain K-best best phrases for every sub-tree.
Also, the goal is to test the approach for
morphologically-rich languages such as Hindi.
Also, it would require us to expand our features set.
We also plan to test the factored models.
The most important experiment that we plan to
</bodyText>
<figure confidence="0.938575">
VB
VBP
VB NN
VBP NN
</figure>
<page confidence="0.995042">
23
</page>
<bodyText confidence="0.999886">
perform is to test our system in the context of MT,
where the input is more real and noisy.
To train more robust language models, we plan to
use the much larger data on a web scale.
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999746714285714">
In this paper, we had experimented with five ngram
based models for sentence realisation from bag of
words with dependency constraints. We have evalu-
ated our models on two different types of input(gold
and noisy). From the results, we can conclude that
the model ’Marked Head-POS based LM’ works
best in both cases.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999905">
The authors of this work were supported by ILMT
grant 11(10)/2006-HCC(TDIL) and EILMT grant
11(9)/2006HCC(TDIL). We would also like to thank
the four reviewers for their valuable reviews.
</bodyText>
<sectionHeader confidence="0.996265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993367684931507">
S. Bangalore and O. Rambow. 2000. Exploiting a proba-
bilistic hierarchical model for generation. Proceedings
of the 18th conference on Computational linguistics.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statisti-
cal Machine Translation through Global Lexical Selec-
tion and Sentence Reconstruction. In Annual Meeting
- ACL, volume 45.
A. Belz. 2007. Probabilistic Generation of Weather
Forecast Texts. In Proceedings ofNAACL HLT.
A. Cahill and J. van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LINGUIS-
TICS, volume 44.
P.C. Chang and K. Toutanova. 2007. A Discriminative
Syntactic Word Order Model for Machine Translation.
Proceedings of the 45th Annual Meeting of the ACL.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman. 2007. XLE documen-
tation. Available on-line.
M. Elhadad. 1991. FUF: The universal unifier user man-
ual version 5.0. Department of Computer Science,
Columbia University. New York.
Y. Guo, J. van Genabith, and H. Wang. 2008.
Dependency-Based N-Gram Models for General Pur-
pose Sentence Realisation. Proceedings of the 22nd
conference on Computational linguistics.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Gen-
abith. 2007. Exploiting Multi-Word Units in History-
Based Probabilistic Generation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,
A.F. Llitj´os, R. Reynolds, J. Carbonell, and R. Cohen.
2003. Experiments with a Hindi-to-English transfer-
based MT system under a miserly data scenario. ACM-
TALIP, 2(2).
D.M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings ofthe 33rd annual meeting
on ACL. ACL Morristown, NJ, USA.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics, 19(2).
H. Nakanishi and Y. Miyao. 2005. Probabilistic models
for disambiguation of an HPSG-based chart generator.
In Proceedings of the International Workshop on Pars-
ing Technology.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proceedings of the
Tenth CoNLL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Proceedings of the 40th Annual Meeting
on ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal SMT. Proceedings of the 43rd Annual Meet-
ing ofACL.
S. Venkatapathy and S. Bangalore. 2007. Three mod-
els for discriminative machine translation using Global
Lexical Selection and Sentence Reconstruction. In
Proceedings ofSSST, NAACLHLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages
152–159.
M. White, R. Rajkumar, and S. Martin. 2007. Towards
Broad Coverage Surface Realization with CCG. In
Proceedings of the Workshop on Using Corpora for
NLG: Language Generation and Machine Translation
(UCNLG+ MT).
M. White. 2004. Reining in CCG Chart Realization.
LECTURE NOTES IN COMPUTER SCIENCE.
</reference>
<page confidence="0.999139">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.825280">
<title confidence="0.99756">Sentence Realisation from Bag of Words with dependency constraints</title>
<author confidence="0.919374">Karthik Gali</author>
<author confidence="0.919374">Sriram</author>
<affiliation confidence="0.935309">Language Technologies Research</affiliation>
<address confidence="0.883485">IIIT-Hyderabad, Hyderabad,</address>
<abstract confidence="0.999248">In this paper, we present five models for sentence realisation from a bag-of-words containing minimal syntactic information. It has a large variety of applications ranging from Machine Translation to Dialogue systems. Our models employ simple and efficient techniques based on n-gram Language modeling. We evaluated the models by comparing the synthesized sentences with reference sentences using the standard BLEU metric(Papineni et al., 2001). We obtained higher results (BLEU score of 0.8156) when compared to the state-of-art results. In future, we plan to incorporate our sentence realiser in Machine Translation and observe its effect on the translation accuracies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th conference on Computational linguistics.</booktitle>
<contexts>
<context position="4286" citStr="Bangalore and Rambow, 2000" startWordPosition="644" endWordPosition="647">9. c�2009 Association for Computational Linguistics this information as input and produce the target sentence. We can also use our sentence realiser as an ordering module in other approaches such as (Quirk et al., 2005), where the goal is to order an unordered bag (of treelets in this case) with dependency links. In Natural Language Generation applications such as Dialogue systems etc, the set of concepts and the dependencies between the concepts is obtained first which is known as text planning. These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). This is known as sentence planning. In the end, the surface string can be obtained by our models. In this paper, we do not test our models with any of the applications mentioned above. However, we plan to test our models with these applications, especially on the two-stage statistical MT approach using the bag-of-words obtained by Global Lexical Selection (Bangalore et al., 2007),(Venkatapathy and Bangalore, 2007). Here, we test our models independent of any application, by beginning with a given bag-of-words (with dependency links). The structure of the paper is as follows. We give an overv</context>
<context position="7674" citStr="Bangalore and Rambow, 2000" startWordPosition="1206" endWordPosition="1210">suming noisy input data to test the robustness of our models. The search algorithm used by both Guo and us is locally greedy i.e., we compute the best string at every node. Guo uses the Viterbi algorithm to get best string whereas we consider and score all permutations to obtain the best string. There has been burgeoning interest in the probabilistic models for sentence realisation, especially for realisation ranking in a two stage sentence realisation architecture where in the first stage a set of sentence realisations are generated and then a realisation ranker will choose the best of them (Bangalore and Rambow, 2000). One major observation in our experiments was that the POS tags held immensely in the task of sentence realisation. 3 Effect of Dependency Constraints There is a major advantage in using dependency constraints for sentence realisation. The search space reduces drastically when the constraints are applied. These constraints state that the realised sentences should be projective with respect to the de20 pendency structure (unordered) of the input bag-ofwords ie.., any word and its children in the dependency tree should project as a contiguous unit in the realised sentence. This is a safe assump</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. Proceedings of the 18th conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>P Haffner</author>
<author>S Kanthak</author>
</authors>
<title>Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction.</title>
<date>2007</date>
<booktitle>In Annual Meeting - ACL,</booktitle>
<volume>45</volume>
<contexts>
<context position="3072" citStr="Bangalore et al., 2007" startWordPosition="460" endWordPosition="463">constructing transfer rules is costly, especially for divergent language pairs such as English and Hindi or English and Japanese. Our models can be used in this scenario, providing a robust alternative to the transfer rules. A sentence realiser can also be used in the framework of a two-step statistical machine translation. In the two-step framework, the semantic transfer and sentence realisation are decoupled into independent modules. This provides an opporunity to develop simple and efficient modules for each of the steps. The model for Global Lexical Selection and Sentence Re-construction (Bangalore et al., 2007) is one such approach. In this approach, discriminative techniques are used to first transfer semantic information of the source sentence by looking at the source sentence globally, this obtaining a accurate bag-of-words in the target language. The words in the bag might be attached with mild syntactic information (ie., the words they modify) (Venkatapathy and Bangalore, 2007). We propose models that take 1http://www.isi.edu/natural-language/mteval/html/412.html Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19–24, Boulder, Colorado, June 2009. c�2009 Ass</context>
<context position="4670" citStr="Bangalore et al., 2007" startWordPosition="710" endWordPosition="713"> the set of concepts and the dependencies between the concepts is obtained first which is known as text planning. These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). This is known as sentence planning. In the end, the surface string can be obtained by our models. In this paper, we do not test our models with any of the applications mentioned above. However, we plan to test our models with these applications, especially on the two-stage statistical MT approach using the bag-of-words obtained by Global Lexical Selection (Bangalore et al., 2007),(Venkatapathy and Bangalore, 2007). Here, we test our models independent of any application, by beginning with a given bag-of-words (with dependency links). The structure of the paper is as follows. We give an overview of the related work in section 2. In section 3, we talk about the effect of dependency constraints and gives details of the experimental setup in section 4. In section 5, we describe about the experiments that have been conducted. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related </context>
</contexts>
<marker>Bangalore, Haffner, Kanthak, 2007</marker>
<rawString>S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction. In Annual Meeting - ACL, volume 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Probabilistic Generation of Weather Forecast Texts.</title>
<date>2007</date>
<booktitle>In Proceedings ofNAACL HLT.</booktitle>
<contexts>
<context position="5686" citStr="Belz, 2007" startWordPosition="877" endWordPosition="878">out the experiments that have been conducted. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related Work There have been approaches for sentence realisation such as FUF/SURGE (Elhadad, 1991), OpenCCG (White, 2004) and XLE (Crouch et al., 2007) that apply hand-crafted grammars based on particular linguistic theories. These approaches expect rich syntactic information as input in order to realise the sentence. There are other approaches in which the generation grammars are extracted semiautomatically (Belz, 2007) or automatically (such as HPSG (Nakanishi and Miyao, 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007)). The limitation of these approaches is that these cannot be incorporated into a wide range of applications as they rely on rich syntactic information for generation. On the contrary, we use simple n-gram models to realise (or linearize) a bag-of-words where the only information available is the presence of various links between the words. Our work is similar to a recently published work by Guo (Guo et al., 2008). They use n-gram models to realise se</context>
</contexts>
<marker>Belz, 2007</marker>
<rawString>A. Belz. 2007. Probabilistic Generation of Weather Forecast Texts. In Proceedings ofNAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>J van Genabith</author>
</authors>
<title>Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations.</title>
<date>2006</date>
<booktitle>In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>44</volume>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>A. Cahill and J. van Genabith. 2006. Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations. In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Chang</author>
<author>K Toutanova</author>
</authors>
<title>A Discriminative Syntactic Word Order Model for Machine Translation.</title>
<date>2007</date>
<booktitle>Proceedings of the 45th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="9036" citStr="Chang and Toutanova, 2007" startWordPosition="1443" endWordPosition="1446">(Guo et al., 2008). Figure 1: Bag of words with dependency constraints and head marked We now present an example to show how the dependency constraints reduce the search space. For example, consider an unordered dependency tree in Figure 1, which has five words. If we don’t use the constraints provided by the dependency tree then the search space is 5! (120). But, if we use the constraints provided by the dependency tree then the search space is 2! + 4! = 28. There is a huge reduction in the search space if we use the constraints provided by the dependency tree. Further, it has been shown in (Chang and Toutanova, 2007) that applying the constraints also aids for the synthesis of better constructed sentences. 4 Experimental Set-up For the experiments, we use the WSJ portion of the Penn tree bank (Marcus et al., 1993), using the standard train/development/test splits, viz 39,832 sentences from 2-21 sections, 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development. The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn treebank using head percolation rules used in (Magerman, 1995), which do not conta</context>
</contexts>
<marker>Chang, Toutanova, 2007</marker>
<rawString>P.C. Chang and K. Toutanova. 2007. A Discriminative Syntactic Word Order Model for Machine Translation. Proceedings of the 45th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Crouch</author>
<author>M Dalrymple</author>
<author>R Kaplan</author>
<author>T King</author>
<author>J Maxwell</author>
<author>P Newman</author>
</authors>
<title>XLE documentation. Available on-line.</title>
<date>2007</date>
<contexts>
<context position="5413" citStr="Crouch et al., 2007" startWordPosition="835" endWordPosition="838">-of-words (with dependency links). The structure of the paper is as follows. We give an overview of the related work in section 2. In section 3, we talk about the effect of dependency constraints and gives details of the experimental setup in section 4. In section 5, we describe about the experiments that have been conducted. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related Work There have been approaches for sentence realisation such as FUF/SURGE (Elhadad, 1991), OpenCCG (White, 2004) and XLE (Crouch et al., 2007) that apply hand-crafted grammars based on particular linguistic theories. These approaches expect rich syntactic information as input in order to realise the sentence. There are other approaches in which the generation grammars are extracted semiautomatically (Belz, 2007) or automatically (such as HPSG (Nakanishi and Miyao, 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007)). The limitation of these approaches is that these cannot be incorporated into a wide range of applications as they rely on rich syntactic information for generation. On the contrar</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2007</marker>
<rawString>D. Crouch, M. Dalrymple, R. Kaplan, T. King, J. Maxwell, and P. Newman. 2007. XLE documentation. Available on-line.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>FUF: The universal unifier user manual version 5.0.</title>
<date>1991</date>
<institution>Department of Computer Science, Columbia University.</institution>
<location>New York.</location>
<contexts>
<context position="5360" citStr="Elhadad, 1991" startWordPosition="828" endWordPosition="829"> any application, by beginning with a given bag-of-words (with dependency links). The structure of the paper is as follows. We give an overview of the related work in section 2. In section 3, we talk about the effect of dependency constraints and gives details of the experimental setup in section 4. In section 5, we describe about the experiments that have been conducted. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related Work There have been approaches for sentence realisation such as FUF/SURGE (Elhadad, 1991), OpenCCG (White, 2004) and XLE (Crouch et al., 2007) that apply hand-crafted grammars based on particular linguistic theories. These approaches expect rich syntactic information as input in order to realise the sentence. There are other approaches in which the generation grammars are extracted semiautomatically (Belz, 2007) or automatically (such as HPSG (Nakanishi and Miyao, 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007)). The limitation of these approaches is that these cannot be incorporated into a wide range of applications as they rely on rich</context>
</contexts>
<marker>Elhadad, 1991</marker>
<rawString>M. Elhadad. 1991. FUF: The universal unifier user manual version 5.0. Department of Computer Science, Columbia University. New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Guo</author>
<author>J van Genabith</author>
<author>H Wang</author>
</authors>
<title>Dependency-Based N-Gram Models for General Purpose Sentence Realisation.</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd conference on Computational linguistics.</booktitle>
<marker>Guo, van Genabith, Wang, 2008</marker>
<rawString>Y. Guo, J. van Genabith, and H. Wang. 2008. Dependency-Based N-Gram Models for General Purpose Sentence Realisation. Proceedings of the 22nd conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hogan</author>
<author>C Cafferkey</author>
<author>A Cahill</author>
<author>J van Genabith</author>
</authors>
<title>Exploiting Multi-Word Units in HistoryBased Probabilistic Generation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<marker>Hogan, Cafferkey, Cahill, van Genabith, 2007</marker>
<rawString>D. Hogan, C. Cafferkey, A. Cahill, and J. van Genabith. 2007. Exploiting Multi-Word Units in HistoryBased Probabilistic Generation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>S Vogel</author>
<author>L Levin</author>
<author>E Peterson</author>
<author>K Probst</author>
<author>A F Llitj´os</author>
<author>R Reynolds</author>
<author>J Carbonell</author>
<author>R Cohen</author>
</authors>
<title>Experiments with a Hindi-to-English transferbased MT system under a miserly data scenario.</title>
<date>2003</date>
<journal>ACMTALIP,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Lavie, Vogel, Levin, Peterson, Probst, Llitj´os, Reynolds, Carbonell, Cohen, 2003</marker>
<rawString>A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A.F. Llitj´os, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Experiments with a Hindi-to-English transferbased MT system under a miserly data scenario. ACMTALIP, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings ofthe 33rd annual meeting on ACL. ACL</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9616" citStr="Magerman, 1995" startWordPosition="1540" endWordPosition="1541">hown in (Chang and Toutanova, 2007) that applying the constraints also aids for the synthesis of better constructed sentences. 4 Experimental Set-up For the experiments, we use the WSJ portion of the Penn tree bank (Marcus et al., 1993), using the standard train/development/test splits, viz 39,832 sentences from 2-21 sections, 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development. The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn treebank using head percolation rules used in (Magerman, 1995), which do not contain any order information. We also use the provided part-ofspeech tags in some experiments. In a typical application, the input to the sentence realiser is noisy. To test the robustness of our models in such scenarios, we also conduct experiments with noisy input data. We parse the test data with an unlabelled projective dependency parser (Nivre et al., 2006) and drop the order information to obtain the input to our sentence realiser. However we still use the correct bag of words. We propose to test this aspect in future by plugging our sentence realiser in Machine Translati</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D.M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings ofthe 33rd annual meeting on ACL. ACL Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9237" citStr="Marcus et al., 1993" startWordPosition="1478" endWordPosition="1481">ered dependency tree in Figure 1, which has five words. If we don’t use the constraints provided by the dependency tree then the search space is 5! (120). But, if we use the constraints provided by the dependency tree then the search space is 2! + 4! = 28. There is a huge reduction in the search space if we use the constraints provided by the dependency tree. Further, it has been shown in (Chang and Toutanova, 2007) that applying the constraints also aids for the synthesis of better constructed sentences. 4 Experimental Set-up For the experiments, we use the WSJ portion of the Penn tree bank (Marcus et al., 1993), using the standard train/development/test splits, viz 39,832 sentences from 2-21 sections, 2416 sentences from section 23 for testing and 1,700 sentences from section 22 for development. The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn treebank using head percolation rules used in (Magerman, 1995), which do not contain any order information. We also use the provided part-ofspeech tags in some experiments. In a typical application, the input to the sentence realiser is noisy. To test the robustness of our models in</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the penn treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakanishi</author>
<author>Y Miyao</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technology.</booktitle>
<contexts>
<context position="5745" citStr="Nakanishi and Miyao, 2005" startWordPosition="884" endWordPosition="887">. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related Work There have been approaches for sentence realisation such as FUF/SURGE (Elhadad, 1991), OpenCCG (White, 2004) and XLE (Crouch et al., 2007) that apply hand-crafted grammars based on particular linguistic theories. These approaches expect rich syntactic information as input in order to realise the sentence. There are other approaches in which the generation grammars are extracted semiautomatically (Belz, 2007) or automatically (such as HPSG (Nakanishi and Miyao, 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007)). The limitation of these approaches is that these cannot be incorporated into a wide range of applications as they rely on rich syntactic information for generation. On the contrary, we use simple n-gram models to realise (or linearize) a bag-of-words where the only information available is the presence of various links between the words. Our work is similar to a recently published work by Guo (Guo et al., 2008). They use n-gram models to realise sentences from the f-structures of HPSG (equivalent to labele</context>
</contexts>
<marker>Nakanishi, Miyao, 2005</marker>
<rawString>H. Nakanishi and Y. Miyao. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings of the International Workshop on Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryigit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth CoNLL.</booktitle>
<contexts>
<context position="9996" citStr="Nivre et al., 2006" startWordPosition="1603" endWordPosition="1606">1,700 sentences from section 22 for development. The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn treebank using head percolation rules used in (Magerman, 1995), which do not contain any order information. We also use the provided part-ofspeech tags in some experiments. In a typical application, the input to the sentence realiser is noisy. To test the robustness of our models in such scenarios, we also conduct experiments with noisy input data. We parse the test data with an unlabelled projective dependency parser (Nivre et al., 2006) and drop the order information to obtain the input to our sentence realiser. However we still use the correct bag of words. We propose to test this aspect in future by plugging our sentence realiser in Machine Translation. Table 1 shows the number of nodes having a particular number of children in the test data. Children countNodes Children countNodes 0 30219 5 1017 1 13649 6 685 2 5887 7 269 3 3207 8 106 4 1526 &gt; 8 119 Table 1: The number of nodes having a particular number of children in the test data From Table 1, we can see that more than 96% of the internal nodes of the trees contain fiv</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the Tenth CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>Proceedings of the 40th Annual Meeting on ACL.</booktitle>
<contexts>
<context position="673" citStr="Papineni et al., 2001" startWordPosition="83" endWordPosition="86">ncy constraints Karthik Gali, Sriram Venkatapathy Language Technologies Research Centre, IIIT-Hyderabad, Hyderabad, India {karthikg@students,sriram@research}.iiit.ac.in Abstract In this paper, we present five models for sentence realisation from a bag-of-words containing minimal syntactic information. It has a large variety of applications ranging from Machine Translation to Dialogue systems. Our models employ simple and efficient techniques based on n-gram Language modeling. We evaluated the models by comparing the synthesized sentences with reference sentences using the standard BLEU metric(Papineni et al., 2001). We obtained higher results (BLEU score of 0.8156) when compared to the state-of-art results. In future, we plan to incorporate our sentence realiser in Machine Translation and observe its effect on the translation accuracies. 1 Introduction In applications such as Machine Translation (MT) and Dialogue Systems, sentence realisation is a major step. Sentence realisation involves generating a well-formed sentence from a bag of lexical items. These lexical items may be syntactically related to each other. The level of syntactic information attached to the lexical items might vary with applicatio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Proceedings of the 40th Annual Meeting on ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting ofACL.</booktitle>
<contexts>
<context position="3878" citStr="Quirk et al., 2005" startWordPosition="577" endWordPosition="580">obtaining a accurate bag-of-words in the target language. The words in the bag might be attached with mild syntactic information (ie., the words they modify) (Venkatapathy and Bangalore, 2007). We propose models that take 1http://www.isi.edu/natural-language/mteval/html/412.html Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19–24, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics this information as input and produce the target sentence. We can also use our sentence realiser as an ordering module in other approaches such as (Quirk et al., 2005), where the goal is to order an unordered bag (of treelets in this case) with dependency links. In Natural Language Generation applications such as Dialogue systems etc, the set of concepts and the dependencies between the concepts is obtained first which is known as text planning. These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). This is known as sentence planning. In the end, the surface string can be obtained by our models. In this paper, we do not test our models with any of the applications mentioned above. Howev</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: syntactically informed phrasal SMT. Proceedings of the 43rd Annual Meeting ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Venkatapathy</author>
<author>S Bangalore</author>
</authors>
<title>Three models for discriminative machine translation using Global Lexical Selection and Sentence Reconstruction.</title>
<date>2007</date>
<booktitle>In Proceedings ofSSST, NAACLHLT/AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="3451" citStr="Venkatapathy and Bangalore, 2007" startWordPosition="519" endWordPosition="522">nsfer and sentence realisation are decoupled into independent modules. This provides an opporunity to develop simple and efficient modules for each of the steps. The model for Global Lexical Selection and Sentence Re-construction (Bangalore et al., 2007) is one such approach. In this approach, discriminative techniques are used to first transfer semantic information of the source sentence by looking at the source sentence globally, this obtaining a accurate bag-of-words in the target language. The words in the bag might be attached with mild syntactic information (ie., the words they modify) (Venkatapathy and Bangalore, 2007). We propose models that take 1http://www.isi.edu/natural-language/mteval/html/412.html Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19–24, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics this information as input and produce the target sentence. We can also use our sentence realiser as an ordering module in other approaches such as (Quirk et al., 2005), where the goal is to order an unordered bag (of treelets in this case) with dependency links. In Natural Language Generation applications such as Dialogue systems etc, the</context>
<context position="4705" citStr="Venkatapathy and Bangalore, 2007" startWordPosition="713" endWordPosition="716">the dependencies between the concepts is obtained first which is known as text planning. These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). This is known as sentence planning. In the end, the surface string can be obtained by our models. In this paper, we do not test our models with any of the applications mentioned above. However, we plan to test our models with these applications, especially on the two-stage statistical MT approach using the bag-of-words obtained by Global Lexical Selection (Bangalore et al., 2007),(Venkatapathy and Bangalore, 2007). Here, we test our models independent of any application, by beginning with a given bag-of-words (with dependency links). The structure of the paper is as follows. We give an overview of the related work in section 2. In section 3, we talk about the effect of dependency constraints and gives details of the experimental setup in section 4. In section 5, we describe about the experiments that have been conducted. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related Work There have been approaches for</context>
</contexts>
<marker>Venkatapathy, Bangalore, 2007</marker>
<rawString>S. Venkatapathy and S. Bangalore. 2007. Three models for discriminative machine translation using Global Lexical Selection and Sentence Reconstruction. In Proceedings ofSSST, NAACLHLT/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>R Rajkumar</author>
<author>S Martin</author>
</authors>
<title>Towards Broad Coverage Surface Realization with CCG.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT).</booktitle>
<contexts>
<context position="5831" citStr="White et al., 2007" startWordPosition="900" endWordPosition="903">ble future work and we conclude with section 8. 2 Related Work There have been approaches for sentence realisation such as FUF/SURGE (Elhadad, 1991), OpenCCG (White, 2004) and XLE (Crouch et al., 2007) that apply hand-crafted grammars based on particular linguistic theories. These approaches expect rich syntactic information as input in order to realise the sentence. There are other approaches in which the generation grammars are extracted semiautomatically (Belz, 2007) or automatically (such as HPSG (Nakanishi and Miyao, 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007)). The limitation of these approaches is that these cannot be incorporated into a wide range of applications as they rely on rich syntactic information for generation. On the contrary, we use simple n-gram models to realise (or linearize) a bag-of-words where the only information available is the presence of various links between the words. Our work is similar to a recently published work by Guo (Guo et al., 2008). They use n-gram models to realise sentences from the f-structures of HPSG (equivalent to labeled dependency structure). Their models rely heavily on the dependency relation labels (</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>M. White, R. Rajkumar, and S. Martin. 2007. Towards Broad Coverage Surface Realization with CCG. In Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Reining in CCG Chart Realization.</title>
<date>2004</date>
<journal>LECTURE NOTES IN COMPUTER SCIENCE.</journal>
<contexts>
<context position="5383" citStr="White, 2004" startWordPosition="831" endWordPosition="832">nning with a given bag-of-words (with dependency links). The structure of the paper is as follows. We give an overview of the related work in section 2. In section 3, we talk about the effect of dependency constraints and gives details of the experimental setup in section 4. In section 5, we describe about the experiments that have been conducted. In section 6, our experimental results are presented. In section 7, we talk about the possible future work and we conclude with section 8. 2 Related Work There have been approaches for sentence realisation such as FUF/SURGE (Elhadad, 1991), OpenCCG (White, 2004) and XLE (Crouch et al., 2007) that apply hand-crafted grammars based on particular linguistic theories. These approaches expect rich syntactic information as input in order to realise the sentence. There are other approaches in which the generation grammars are extracted semiautomatically (Belz, 2007) or automatically (such as HPSG (Nakanishi and Miyao, 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007)). The limitation of these approaches is that these cannot be incorporated into a wide range of applications as they rely on rich syntactic information </context>
</contexts>
<marker>White, 2004</marker>
<rawString>M. White. 2004. Reining in CCG Chart Realization. LECTURE NOTES IN COMPUTER SCIENCE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>