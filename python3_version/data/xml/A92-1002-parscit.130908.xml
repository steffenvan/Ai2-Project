<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.99823">
A Dialog Control Algorithm and Its Performance
</title>
<author confidence="0.99581">
Ronnie W. Smith*
</author>
<affiliation confidence="0.83559">
Dept. of Computer Science
Duke Universtiy
</affiliation>
<address confidence="0.85272">
Durham, NC 27706
</address>
<email confidence="0.997668">
rws@cs.duke.edu
</email>
<author confidence="0.979251">
D. Richard Hipp
</author>
<affiliation confidence="0.8350805">
Dept. of Computer Science
Duke Universtiy
</affiliation>
<address confidence="0.852374">
Durham, NC 27706
</address>
<email confidence="0.997449">
drh@cs.duke.edu
</email>
<author confidence="0.923988">
Alan W. Biermann
</author>
<affiliation confidence="0.832973">
Dept. of Computer Science
Duke Universtiy
</affiliation>
<address confidence="0.851511">
Durham, NC 27706
</address>
<email confidence="0.997991">
awb@cs.duke.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993448">
A pragmatic architecture for voice dialog ma-
chines aimed at the equipment repair problem
has been implemented which exhibits a num-
ber of behaviors required for efficient human-
machine dialog. These behaviors include:
</bodyText>
<listItem confidence="0.994426647058824">
(1) problem solving to achieve a target
goal,
(2) the ability to carry out subdialogs to
achieve appropriate subgoals and to
pass control arbitrarily from one sub-
dialog to another,
(3) the use of a user model to enable use-
ful verbal exchanges and to inhibit un-
necessary ones,
(4) the ability to change initiative from
strongly computer controlled to
strongly user controlled or somewhere
in between, and
(5) the ability to use context dependent
expectations to correct speech recog-
nition and track user movement to
new subdialogs.
</listItem>
<bodyText confidence="0.999773666666667">
A description of the implemented dialog control
algorithm is given; an example shows the fun-
damental mechanisms for achieving the listed
behaviors. The system implementation is de-
scribed, and results from its performance in 141
problem solving sessions are given.
</bodyText>
<sectionHeader confidence="0.991728" genericHeader="method">
1 A Voice-Interactive Dialog Machine
</sectionHeader>
<bodyText confidence="0.919461214285714">
A limited vocabulary voice dialog system designed to aid
a user in the repair of electronic circuits has been con-
structed in our laboratory. The system contains a model
of the device to be repaired, debugging and repair proce-
dures, voice recognition and sentence processing mech-
anisms, a user model, language generation capabilities,
and a dialog control system which orchestrates the be-
haviors of the various parts. This paper describes the
â€¢This research was supported by National Science Foun-
dation Grant Number NSF-IRI-88-03802 and by Duke
University.
dialog control algorithm and the performance of the to-
tal system in aiding a series of subjects in the repair of
an electronic circuit.
</bodyText>
<sectionHeader confidence="0.992814" genericHeader="method">
2 Target Behaviors
</sectionHeader>
<bodyText confidence="0.991755463414634">
The purpose of the dialog control algorithm is to direct
the activities of the many parts of the system to obtain
efficient human-machine dialog. Specifically, it is aimed
at achieving the following behaviors.
Convergence to a goal. Efficient dialog requires that
each participant understand the purpose of the interac-
tion and have the necessary prerequisites to cooperate
in its achievement. This is the intentional structure of
[Grosz and Sidner, 1986], the goal-oriented mechanism
that gives direction to the interaction. The primary re-
quired facilities are a problem solver that can deduce
the necessary action sequences and a set of subsystems
capable of carrying out those sequences.
Subdialogs and effective movement between them. Ef-
ficient human dialog is usually segmented into utter-
ance sequences, subdialogs, that are individually aimed
at achieving relevant subgoals ([Grosz, 1978], [Linde and
Goguen, 1978], [Polanyi and Scha, 1983], and [Reichman,
1985]). These are called &amp;quot;segments&amp;quot; by [Grosz and Sid-
ner, 1986] and constitute the linguistic structure defined
in their paper. The global goal is approached by a series
of attempts at subgoals each of which involves a set of
interactions, the subdialogs.
An aggressive strategy for global success is to choose
the most likely subgoals needed for success and carry out
their associated subdialogs. As the system proceeds on
a given subdialog, it should always be ready to abruptly
drop it if some other sub dialog suddenly seems more ap-
propriate. This leads to the fragmented style that so
commonly appears in efficient human communication.
A subdialog is opened which leads to another, then an-
other, then a jump to a previously opened subdialog,
and so forth, in an unpredictable order until the neces-
sary subgoals have been solved for an overall success.
Accounting for user knowledge and abilities. Cooper-
ative problem solving involves maintaining a dynamic
profile of user knowledge, termed a user model. This
concept is described for example in [Kobsa and Wahlster,
1988] and [Kobsa and Wahlster, 1989], [Chin, 1989], [Co-
-hen and Jones, 1989], [Finin, 1989], [Lehman and Car-
bonell, 1989], [Monk, 1989], and [Paris, 1988]. The user
</bodyText>
<page confidence="0.995515">
9
</page>
<bodyText confidence="0.999969892857143">
model specifies information needed for efficient interac-
tion with the conversational partner. Its purpose is to
indicate what needs to be said to the user to enable the
user to function effectively. It also indicates what should
be omitted because of existing user knowledge.
Because considerable information is exchanged during
the dialog, the user model changes continuously. Men-
tioned facts are stored in the model as known to the
user and are not repeated. Previously unmentioned in-
formation may be assumed to be unknown and may be
explained as needed. Questions from the user may indi-
cate lack of knowledge and result in the removal of items
from the user model.
Change of initiative. A real possibility in a coopera-
tive interaction is that the user&apos;s problem solving ability,
either on a given subgoal or on the global task, may ex-
ceed that of the machine. When this occurs, an efficient
interaction requires that the machine yield control so
that the more competent partner can lead the way to
the fastest possible solution. Thus, the machine must be
able to carry out its own problem solving process and di-
rect the actions to task completion or yield to the user&apos;s
control and respond cooperatively to his or her requests.
This is a mixed-initiative dialog as studied by [Kitano
and Van Ess-Dykema, 1991], [Novick, 1988], [Whittaker
and Stenton, 1988], and [Walker and Whittaker, 1990].
As a pragmatic issue, we have found that at least four
initiative modes are useful:
</bodyText>
<listItem confidence="0.9985198125">
(1) directive - The computer has complete dialog
control. It recommends a subgoal for comple-
tion and will use whatever dialog is necessary
to obtain the needed item of knowledge related
to the subgoal.
(2) suggestive - The computer still has dialog con-
trol, but not as strongly. The computer will
make suggestions about the subgoal to perform
next, but it is also willing to change the di-
rection of the dialog according to stated user
preferences.
(3) declarative - The user has dialog control, but
the computer is free to mention relevant,
though not required, facts as a response to the
user&apos;s statements.
(4) passive - The user has complete dialog control.
</listItem>
<bodyText confidence="0.927034818181818">
The computer responds directly to user ques-
tions and passively acknowledges user state-
ments without recommending a subgoal as the
next course of action.
Expectation of user input. Since all interactions occur
in the context of a current subdialog, the user&apos;s input
is far more predictable than would be indicated by a
general grammar for English. In fact, the current subdi-
alog specifies the focus of the interaction, the set of all
objects and actions that are locally appropriate. This is
the attentional structure described by [Grosz and Sidner,
19861 and its most important function in our system is
to predict the meaning structures the user is likely to
communicate in a response. For illustration, the open-
ing of a chassis cover plate will often evoke comments
about the objects behind the cover; the measurement of
a voltage is likely to include references to a voltmeter,
leads, voltage range, and the locations of measurement
points.
The subdialog structure thus provides a set of ex-
pected utterances at each point in the conversation and
these have two important roles:
</bodyText>
<listItem confidence="0.884119409090909">
(1) The expected utterances provide strong guid-
ance for the speech recognition system so that
error correction can be maximized. Where
ambiguity arises, recognition can be biased
in the direction of meaningful statements in
the current context. Earlier researchers who
have investigated this insight are [Erman et
al., 1980], [Walker, 1978], [Fink and Biermann,
1986], [Mudler and Paulus, 1988], [Carbonell
and Pierrel, 19881, [Young et a/., 19891, and
[Young and Proctor, 1989].
(2) The expected utterances from subdialogs other
than the current one can be used to indicate
that one of those others is being invoked. Thus,
expectations are one of the primary mecha-
nisms needed for tracking the conversation as
it jumps from subdialog to subdialog. This is
known elsewhere as the plan recognition prob-
lem and it has received much attention in re-
cent years. See, for example, [Allen, 1983], [Lit-
man and Allen, 1987], [Pollack, 1986], and [Car-
berry, 1990].
</listItem>
<bodyText confidence="0.9987525">
Systems capable of all of the above behaviors are rare
as has been observed by [Allen et al., 1989]: &amp;quot;no one
knows how to fit all of the pieces together.&amp;quot; One of the
contributions of the current work is to present an archi-
tecture that can provide them all in the limited domain
of electric circuit repair. [Allen et a/., 1989] describe
their own architecture which concentrates on represen-
tations for subdialog mechanisms and their interactionE
with sentence level processing. Our work differs frorr
theirs on many dimensions including our emphasis on
achieving mixed-initiative, real time, voice interactive
dialog which utilizes a user model.
</bodyText>
<sectionHeader confidence="0.980905" genericHeader="method">
3 The Zero Level Model
</sectionHeader>
<bodyText confidence="0.999983333333333">
The system implemented in our laboratory (described
in great detail in [Smith, 1991]) achieves the above be-
haviors sufficiently to enable efficient human-machine di.
alogs in the electric circuit repair environment. The corn.
plexity of the system prevents its complete descriptior
here. However, a zero level model has been devised fo]
pedagogical purposes which illustrates its principles o
operation. This model mimicks the mechanism of th(
dialog machine while omitting the huge array of detail:
necessary to make a real system work. A later sectior
in this paper describes the actual implementation an
some of its major modules.
The zero level model is the recursive subroutine Zmod
Subdialog shown in figure 1. This routine is entered witl
a single argument, a goal to be proven, and its action:
are to carry out a Prolog-style proof of the goal. A sid:
effect of the proof may be a resort to voice interaction:
with the user to supply necessary missing axioms. II
</bodyText>
<page confidence="0.948986">
10
</page>
<figure confidence="0.5754178">
Recursive subdialog routine (enter with a goal to prove)
ZmodSubdialog (Goal)
Create subdialog data structures
While there are rules available which may achieve Goal
Grab next available rule R from knowledge; unify with Goal
If R trivially satisfies Goal, return with success
If R is vocalize(X) then
Execute verbal output X (mode)
Record expectation
Receive response (mode)
Record implicit and explicit meanings for response
Transfer control depending on which expected response was received
Successful response: Return with success
Negative response: No action
Confused response: Modify rule for clarification; prioritize rule for execution
</figure>
<bodyText confidence="0.9592168">
Interrupt: Match response to expected response of another subdialog;
go to that subdialog (mode)
If R is a general rule then
Store its antecedents
While there are more antecedents to process Do
Grab the next one and enter ZmodSubdialog with it
If the ZmodSubdialog exits with failure then terminate processing on rule R
If all antecedents of R succeed, return with success
NOTE: SUCCESSFUL COMPLETION OF THIS ROUTINE DOES NOT NECESSARILY MEAN TRANSFER OF CONTROL
TO THE CALLING ROUTINE. CONTROL PASSES TO THE SUBDIALOGUE SELECTED BY THE DIALOG CONTROLLER.
</bodyText>
<figureCaption confidence="0.998747">
Figure 1: Zero Level Model
</figureCaption>
<bodyText confidence="0.999667684210526">
fact, the only voice interactions the system undertakes
are those called for by the theorem proving machinery.
The ZmodSubdialog routine has a unique capabil-
ity needed for proper modeling of subdialog behaviors.
Specifically, its actions may be suspended at any time
so that control may be passed to another subdialog,
another instantiation of ZmodSubdialog that is aimed
at achieving another goal. However, control may at a
later time return to the current instantiation to continue
its execution. In fact, a typical computation involves
a set of ZmodSubdialog instantiations all advanced to
some point in the proofs of their respective goals; con-
trol passes back and forth between them looking for the
most likely chances of success until, finally, enough goals
are proven to complete the global proof.
The algorithm becomes understandable if an example
is followed through in full detail. Assume that the fol-
lowing database of Prolog-like rules are contained in the
system knowledge base.
</bodyText>
<subsectionHeader confidence="0.613856">
General Debugging Rules
</subsectionHeader>
<bodyText confidence="0.47754875">
set(knob,Y) &lt;-- find(knob), adjust(knob,Y)
General Dialog Rules
Y &lt;-- usercan(Y), vocalize(Y)
vocalize(X)
</bodyText>
<subsectionHeader confidence="0.641936">
User Model Rules
</subsectionHeader>
<bodyText confidence="0.9945710625">
find(knob)
usercan(adjust(knob,X))
Further assume that ZmodSubdialog is entered with
argument Goal = set(knob,10). Then in Prolog fashion,
the routine grabs the rule R:
set(knob,Y) +-- find(knob),adjust(knob,Y),
and attempts to prove set(knob,10). The algorithm of-
fers three choices depending on whether R is trivial (a
single predicate which is not vocalize(X)), R is vocal-
ize(X), or R is a rule with antecedents as is the case here.
Thus, in this case the third alternative is followed, and
the two antecedents are queued for sequential execution
(find(knob) and adjust(knob,10)). Then the first an-
tecedent is selected and ZmodSubdialog is entered with
argument Goal = find(knob).
The new subdialog to achieve find(knob) is short, how-
ever, since the user model indicates the user already
knows how to find the knob because find(knob) ex-
ists as an available rule. In fact, ZmodSubdialog finds
find(knob) in the rule base, enters the first choice (triv-
ial) and returns with success. If find(knob) had not been
found in the user model, the system might have engaged
in dialog to achieve this goal. The subdialog control
mechanism is not obligated to pass control back to the
calling routine, but in this example we will assume that
it does.
Satisfactory proof of find(knob) means that the next
antecedent at this level, adjust(knob,10), can be at-
tempted, and ZmodSubdialog is entered with this goal.
Here, our model selects R = Y usercan(Y), vocal-
ize(Y), unifying Y with adjust(knob,10). Then a deeper
call to ZmodSubdialog finds usercan(adjust(knob,X)) in
</bodyText>
<page confidence="0.99539">
11
</page>
<bodyText confidence="0.948557284313725">
the user model which means control passes to the an-
tecedent vocalize(adjust (knob ,10)) . This yields another
entry to ZmodSubdialog and a selection of the second
branch. Here the system executes a voice output to
satisfy vocalize(adjust(knob,10)): &amp;quot;Set the knob to one
zero.&amp;quot;
The handling of the goal find(knob) illustrates how
the user model can act to satisfy the theorem proving
process and prevent the enunciation of unneeded infor-
mation. As the theorem proving process proceeds, the
fact that a user knows something is represented in the
knowledge base as an achieved goal. Theorem proving
will encounter this and proceed without voice interac-
tion. In the example, the model already indicates that
the user knows how to find the knob so no voice interac-
tion is needed for this goal.
The handling of the goal adjust(knob,10) illustrates
how the user model supports the theorem proving pro-
cess by enabling voice dialog when it is needed. This
goal could not be proven by application of rules available
in the database and the proof was blocked from further
progress. In our terminology, there was a &amp;quot;missing ax-
iom&amp;quot; for the proof. So the system must either give up on
this proof or try to fill in the missing axiom by resorting
to voice dialog. In the current case, voice dialog was en-
abled by the user model fact usercan(adjust(knob,X)).
This fact opens the way for a query to the user. If the
query is positively answered, then the missing axiom is
made available.
The role of the user model is thus to supply or fail to
supply axioms at the bottom of the proof tree. It both
inhibits extraneous verbalism and enables interactions
necessary to prove the theorem.
Returning to the example computation of ZmodSub-
dialog, a voice output has just been given, and the sys-
tem then records, for this output, the set of expected
responses:
user(adjust(knob,10))
assertion(knob,position,10)
trivialresponse(affirmation)
trivialresponse(negative)
query(location(knob))
query(color(knob))
Expected responses are compiled from the domain
knowledge base and from the dialog controller knowl-
edge base.
The user&apos;s response is then matched (unified) against
the expected meanings and subsequent actions depend
on which meaning fits best. Four different types of ac-
tions may occur at this point.
(1) The user might respond with some paraphrase
of &amp;quot;I set the knob to one zero,&amp;quot; or &amp;quot;Okay&amp;quot;,
which would be interpreted as successful re-
sponses. The routine ZmodSub dialog would re-
turn with success.
(2) The user might also answer &amp;quot;No&amp;quot; yielding a
failure and another cycle around the theorem
proving loop looking for an applicable rule.
(3) The user might respond with &amp;quot;What color is
the knob?&amp;quot; indicating there may be a chance
for a success here if there is further dialog.
In fact, our system handles such a need for
clarification by dynamically modifying the rule
and reexecuting with a newly required clarifi-
cation subdialog. Here the rule set(knob,10)
find(knob), adjust(knob,10) becomes mod-
ified to set(knob,10) find(knob), vocal-
ize(knob , white) , adj ust (knob ,10) . Reexecution
will then yield an explanation of the knob color
followed by the previous request reenunciated:
&amp;quot;Set the knob to one zero.&amp;quot;
(4) The user might respond with an utterance that
matches no local expectation. Here the sys-
tem examines expectations of other sub dialogs
and searches for an acceptable match. If one
is found, control will pass to that subdialog.
For example, if the response is, &amp;quot;The LED is
flashing seven,&amp;quot; and if this is an appropriate
response in some other subdialog, control will
pass to it.
The control of initiative in ZmodSub dialog is handled
by special processing at the steps marked &amp;quot;mode.&amp;quot; Thus,
in strongly directive mode, verbal outputs will be very
positively stated, responses will be expected to be obedi-
ent, and interrupts to other subdialogs will be restricted.
In less demanding modes, outputs will be weaker or
merely suggestive, a wider range of responses will be al-
lowed, and transitions to other subdialogs will be more
freely permitted. In the most passive mode, there are
few outputs except answers to questions, and an inter-
rupt to any subdialog is acceptable.
A very important part of the dialog system is the do-
main processor, the application dependent portion of
the system. It receives all information related to the
current problem and suggests debugging steps to carry
the process further. We model calls to this processor
with the rule: debug(X) (debuggingstep(X))* . This
rule is called upon to debug device X with the predi-
cate debug(X) and its effect is to specify a sequence of
debugging steps which will be specified dynamically as
the problem unfolds and which will lead to repair of the
device.
</bodyText>
<sectionHeader confidence="0.999525" genericHeader="method">
4 The Implementation
</sectionHeader>
<subsectionHeader confidence="0.999614">
4.1 An Integrated Architecture
</subsectionHeader>
<bodyText confidence="0.999970714285714">
The implemented system is based on a computational
model for dialog processing presented in [Smith, 1991].
The model is applicable to the general class of task-
oriented dialogs, dialogs where the computer is assisting
the user in completing a task as the dialog ensues. ThE
derived implementation consists of the following mod-
ules:
dialog controller - This is the supervisor of the
dialog processing system. It provides the sub-
dialog processing highlighted in the zero level
model. In addition, it provides the complex al-
gorithm required to properly handle arbitrary
clarification subdialogs and interrupts as well
as provide the dialog expectations needed to
</bodyText>
<page confidence="0.997195">
12
</page>
<bodyText confidence="0.999909235294118">
assist with input interpretation. It also main-
tains all dialog information shared by the other
modules and controls their activation.
domain processor - As previously mentioned,
it provides suggestions about debugging steps
to pursue. In our system the domain proces-
sor assists users in circuit repair. It receives
user-supplied domain information from the di-
alog controller and returns suggested debug-
ging subgoals to the controller for considera-
tion. The subgoal selection process weighs the
level of expected usefulness of a subgoal with
the number of times the subgoal has been previ-
ously selected. Consequently, the module may
recommend challenging previously reported in-
formation if no noticeable progress in the task
is being made. In this manner, the module and
system can recover from erroneous inputs. If
the dialog system is to be used to repair other
devices, this is the module that needs to be re-
placed.
knowledge - This is the repository of in-
formation about task-oriented dialogs includ-
ing: (1) rules for proving completion of goals;
(2) user model information including a set of
rules for inferring user model information from
user input; (3) rules for determining when a
clarification subdialog should be initiated; and
(4) rules for defining the expectations for the
user&apos;s response as a function of the type of goal
being attempted. Note that the predefined in-
formation of this module is easily modified with-
out requiring changes to the dialog controller.
theorem prover - This provides the general rea-
soning capability of the system. In order to
allow the dialog controller to control the poten-
tially arbitrary movement among subdialogs,
the theorem prover has been made interruptible
and put under the supervision of the dialog con-
troller. Consequently, the theorem prover can
maintain a set of partially completed proofs,
and can activate different proofs as instructed
by the dialog controller. It can also dynami-
cally modify the proof structure, a key feature
for handling clarification subdialogs. Foremost,
the theorem prover is able to suspend itself
when it encounters a missing axiom, permit-
ting natural language interaction to assist in
axiom acquisition. This contrasts with tradi-
tional theorem proving approaches (e.g. Pro-
log) which simply engage in backtracking when
a missing axiom is encountered.
linguistic interface - This consists of the gen-
eration and recognition modules. They use in-
formation on context and expectation as pro-
vided by the dialog controller. The linguistic
generator was developed by Robin Gambill. It
uses a rule-driven approach that takes as input
an utterance specification which encapsulates
the desired meaning and produces as output an
English string that is sent to a text-to-speech
converter for enunciation. Various approaches
to generation have been described in [Danlos,
1987], [Hovy, 1988], [Jacobs, 1987], [McKeown,
1985], and [Patten, 1988]. The recognition
module was designed to be able to recover from
ungrammatical inputs. It will be described in
greater detail below.
</bodyText>
<subsectionHeader confidence="0.925234">
4.2 Minimum distance parsing
</subsectionHeader>
<bodyText confidence="0.788492083333333">
A challenging design problem in any natural language
system is the development of a parser which will trans-
late the lattice of words output by the speech recognizer
into a phrase of some synthetic language (Prolog in our
instance) which encapsulates the meaning of what was
originally spoken. The design difficulty is exacerbated
by the fact that due to speech recognition errors, the
word lattice output by the speech recognizer is proba-
bly different from what the user spoke, and the fact that
users will sometimes speak with a word order or sen-
tence construction which the designers of the parser&apos;s
grammar did not forsee. (These, and other problems
associated with robust parsing of speech are further de-
scribed in [Eastman and McLean, 1981], [Hayes et al.,
1986], and [Young et al., 1989].) In our system, pars-
ing is accomplished using a minimum-distance parsing
algorithm, similar to algorithms described in [Aho and
Peterson, 1972], [Lyon, 1974], and [Levinson, 1985], but
extended to accept a lattice as input, instead of a simple
word list, and also extended to simultaneously perform
syntax-directed translation [Aho and Ullman, 1969] into
the target language. Minimum-distance parsing finds
strings in a given context-free language which can be
converted to a string of the input lattice with a mini-
mum number of weighted insertions and deletions. When
two or more strings are equidistant from the input, dia-
log expectation is used to break the tie. The minimum-
distance parsing algorithm is very robust â€” it always finds
at least one parse â€” but it is also computationally expen-
sive. Our algorithm is faster than previously reported
parsing algorithms of the same generality, but it still re-
quired careful hand-crafting and optimization on a fast
machine in order to provide real-time response. It should
also be noted that our recognition system is capable of
selective verification of meaning. This is highlighted in
the following sample dialogs.
</bodyText>
<sectionHeader confidence="0.800718" genericHeader="method">
5 Sample Dialogs
</sectionHeader>
<bodyText confidence="0.967335166666667">
Figure 2 shows two dialogs obtained from actual usage
of the system. These dialogs can be seen in [Hipp and
Smith, 1991] that is part of the video presentation of
the conference. The words in parentheses found beside
certain user statements are the words actually provided
by the speech recognizer. Where the discrepancy be-
tween the output of the speech recognizer and the words
produced by the user is insignificant, they have been
omitted.
The first dialog is an entire dialog conducted in declar-
ative mode. Particular points to notice from this user-
controlled dialog are:
</bodyText>
<page confidence="0.997881">
13
</page>
<bodyText confidence="0.828785">
Dialog 1 - Declarative Mode (C denotes the computer, U the user, and R the recognizer)
</bodyText>
<listItem confidence="0.99106425">
1. C: This is the circuit fix it shop. How may I help you?
2. U: I want to fix this rs111.
3. C: I am familiar with that circuit. The LED is supposed to be displaying alternately
flashing one and seven.
4. U: The LED is displaying an alternately flashing one and the top corner of a seven.
5. C: There is supposed to be a wire between connector nine nine and connector one zero zero.
6. U: It is there.
7. C: There is supposed to be a wire between connector nine nine and connector one zero two.
8. U: There is no wire on connector one zero four. (R: stays know wire i connector one zero for)
9. C: There is supposed to be a wire between connector one zero four and connector one zero two.
[User adds wire and verifies the circuit is repaired, a three step process.]
10. U: The circuit is working.
11. C: Glad to have been of assistance. Good-bye.
Dialog 2 - Directive Mode excerpt (begins just after computer describes what circuit does)
8. U: The LED is displaying nothing.
9. C: What is the switch at when the LED is off?
10: U: The switch is down.
11. C: Put the switch up.
12. U: Done.
13. C: Put the knob to one zero.
14. U: Which knob?
15. C: Near the top boundary and in the center is a green region. Now, in the middle
right of this green region is the control knob.
16. U: The knob is at one zero.
17. C: What is the LED displaying?
18. U: The LED is displaying a one and seven at the same time.
19. C: Put the knob to zero.
20. U: Done.
21. C: What is the LED displaying?
22. U: The same thing. (R: the same same)
23. C: Did you mean to say the LED is displaying the same thing?
24. U: Yes.
</listItem>
<bodyText confidence="0.942461">
[Eventually, a missing wire is detected and added, and the computer explicitly guides
the user through the verification process. The entire dialog lasts 51 utterances.]
</bodyText>
<figureCaption confidence="0.961528">
Figure 2: Sample Dialogs
</figureCaption>
<listItem confidence="0.991011833333333">
(1) Successful processing of anaphoric reference at
utterance 6.
(2) Successful shifts to user-initiated subdialogs at
utterances 8 and 10.
(3) Successful recovery from speech recognition er-
rors at utterance 8.
</listItem>
<bodyText confidence="0.9995708">
The second dialog is an excerpt from a dialog con-
ducted in directive mode (strongly computer-controlled
dialog). The total dialog lasted 51 utterances in contrast
to the 11 utterance declarative mode dialog. Particular
points to notice from this excerpt include:
</bodyText>
<listItem confidence="0.9984945">
(1) Computer responses which are more directed
and forceful in content than in dialog 1.
(2) Successful handling of a clarification subdialog
(utterances 13-16).
(3) Successful verification of the implicit meaning
of a user utterance in the presence of speech
</listItem>
<bodyText confidence="0.9975768">
recognition errors in utterance 22. In contrast
with utterance 8 of dialog 1, the system decided
an explicit verification subdialog was required
to ascertain the meaning of the user&apos;s utter-
ance.
</bodyText>
<sectionHeader confidence="0.993229" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.994687428571429">
The system has been implemented on a Sun 4 worksta-
tion with the majority of the code written in Quintus
Prolog&apos;, and the parser in C. Speech recognition is per-
formed by a Verbex 6000 running on an IBM PC and
speech production is performed by a DECtalk2 DTC01
text-to-speech converter. The users are restricted to a
125 word vocabulary in the connected speech system.
</bodyText>
<footnote confidence="0.9971455">
1Quintus Prolog is a trademark of Quintus Computer Sys-
tems, Incorporated
2DECtalk is a trademark of Digital Equipment
Corporation.
</footnote>
<page confidence="0.999128">
14
</page>
<bodyText confidence="0.999988344827586">
The implemented domain processor has been loaded with
a model for a particular experimental circuit assembled
on a Radio Shack 160-in-One Electronic Project Kit.
After testing system prototypes with a few volunteers,
eight subjects used the system during the formal exper-
imental phase. After a warmup session where the sub-
ject trained on the speech recognizer and practiced us-
ing the system, each subject participated in two sessions
where up to ten problems were attempted. The system
ran in declarative mode (user-controlled dialogs) during
one session and in directive mode (strongly computer-
controlled dialog) in the other session. Subjects at-
tempted a total of 141 dialogs of which 118 or 84% were
completed successfully.&apos; Subjects spoke a total of 2840
user utterances, with 81.5% correctly interpreted by the
system although only 50.0% were correctly recognized
word for word by the speech recognizer. The average
speech rate was 2.8 sentences per minute, and the aver-
age task completion times for successful dialogs were 4.5
and 8.5 minutes, respectively, for declarative and direc-
tive modes. The average number of user utterances per
successful dialog was 10.7 in declarative mode and 27.6 in
directive mode. A detailed description of the experiment
and results is given in [Smith, 1991]. The substantially
shorter completion times for users in declarative mode
can be attributed to the fact that the subjects learned
many of the debugging procedures during the experiment
and did not need the detailed descriptions given in the
directive mode.
</bodyText>
<sectionHeader confidence="0.998652" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.999901413793103">
A voice interactive dialog architecture has been devel-
oped which achieves simultaneously a variety of behav-
iors believed to be necessary for efficient human-machine
dialog. Goal oriented behavior is supplied by the theo-
rem proving paradigm. Subdialogs and movement be-
tween them is implemented with an interruptible theo-
rem prover that maintains a set of partially completed
proofs and can work on the most appropriate one at
any given time. A user model is provided by a contin-
uously changing set of rules that are referenced in the
theorem proving process either to enable or inhibit voice
dialog. Mixed initiative is made possible by variable
types of processing by the output and input routines
and by restricting or releasing the ability to interrupt to
a new subdialog. Expectation is associated with indi-
vidual subdialogs, is compiled from domain and dialog
information related to each specific output, and is used
to improve voice recognition and enable movement be-
tween subdialogs.
&apos;Of the 23 dialogs which were not completed, 22 were ter-
minated prematurely due to excessive time being spent on
the dialog. Misunderstandings due to misrecognition were
the cause in 13 of these failures. Misunderstandings due to
inadequate grammar coverage occurred in 3 of the failures.
In 4 of the failures the subject misconnected a wire. In one
failure there was confusion by the subject about when the
circuit was working, and in another failure there were prob-
lems with the system software. A hardware failure caused
termination of the final dialog.
</bodyText>
<sectionHeader confidence="0.995393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991198185185185">
[Aho and Peterson, 1972] Alfred V. Aho and Thomas G.
Peterson. A minimum distance error-correcting parser
for context-free languages. SIAM Journal on Compu-
tation, 1(4):305-312, 1972.
[Aho and Ullman, 1969] A. V. Aho and J. D. Ullman.
Properties of syntax directed translations. Journal of
Computer and System Sciences, 3(3):319-334, 1969.
[Allen et al., 1989] J. Allen, S. Guez, L. Hoebel,
E. Hinkelman, K. Jackson, A. Kyburg, and D. Traum.
The discourse system project. Technical Report 317,
University of Rochester, November 1989.
[Allen, 19831 J.F. Allen. Recognizing intentions from
natural language utterances. In M. Brady and R.C.
Berwick, editors, Computational Models of Discourse,
pages 107-166. MIT Press, Cambridge, Mass., 1983.
[Carberry, 1990] S. Carberry. Plan Recognition in Natu-
ral Language Dialogue. MIT Press, Cambridge, Mass.,
1990.
[Carbonell and Pierrel, 19881 N. Carbonell and J.M.
Pierrel. Task-oriented dialogue processing in human-
computer voice communication. In H. Niemann,
M. Lang, and G. Sagerer, editors, Recent Advances
in Speech Understanding and Dialog Systems, pages
491-496. Springer-Verlag, New York, 1988.
[Chin, 1989] D.N. Chin. KNOME: Modeling what the
user knows in UC. In A. Kobsa and W. Wahlster,
editors, User Models in Dialog Systems, pages 74-107.
Springer-Verlag, New York, 1989.
[Cohen and Jones, 1989] R. Cohen and M. Jones. In-
corporating user models into expert systems for ed-
ucational diagnosis. In A. Kobsa and W. Wahlster,
editors, User Models in Dialog Systems, pages 313-
333. Springer-Verlag, New York, 1989.
[Danlos, 1987] L. Danlos. The Linguistic Basis of Text
Generation. Cambridge University Press, New York,
1987.
[Eastman and McLean, 1981] C. M. Eastman and D. S.
McLean. On the need for parsing ill-formed in-
put. American Journal of Computational Linguistics,
7(4):257, 1981.
[Erman et a/., 1980] L.D. Erman, F. Hayes-Roth, V.R.
Lesser, and D.R. Reddy. The Hearsay-II speech-
understanding system: Integrating knowledge to re-
solve uncertainty. ACM Computing Surveys, pages
213-253, June 1980.
[Finin, 1989] T.W. Finin. GUMS: A general user model-
ing shell. In A. Kobsa and W. Wahlster, editors, User
Models in Dialog Systems, pages 411-430. Springer-
Verlag, New York, 1989.
[Fink and Biermann, 1986] P.E. Fink and A.W. Bier-
mann. The correction of ill-formed input using
history-based expectation with applications to speech
understanding. Computational Linguistics, 12(1):13â€”
36, 1986.
</reference>
<page confidence="0.985774">
15
</page>
<reference confidence="0.99607181981982">
[Grosz and Sidner, 19861 B.J. Grosz and C.L. Sidner.
Attentions, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175-204, 1986.
[Grosz, 19781 B.J. Grosz. Discourse analysis. In
D.E. Walker, editor, Understanding Spoken Language,
pages 235-268. North-Holland, New York, 1978.
[Hayes et al., 1986] Philip J. Hayes, Alexander G
Hauptmann, Jaime G. Carbonell, and Masaru Tomita.
Parsing spoken language: A semantic caseframe ap-
proach. In COLING-86: Proceedings of the 11th In-
ternational Conference on Computational Linguistics,
pages 587-592, Bonn, August 1986.
[Hipp and Smith, 19911 D. R. Hipp and R. W. Smith.
A demonstration of the &amp;quot;circuit fix-it shoppe&amp;quot;. A 12
minute videotape available from the authors at Duke
University, Durham, NC 27706, August 1991.
[Hovy, 1988] E.H. Hovy. Generating Natural Language
under Pragmatic Constraints. Lawrence Erlbaum As-
sociates, Hillsdale, NJ, 1988.
[Jacobs, 1987] P.S. Jacobs. KING: a knowledge-
intensive natural language generator. In Gerard Kem-
pen, editor, Natural Language Generation, pages 219-
230. Martinus Nijhoff Publishers, Boston, 1987.
[Kitano and Van Ess-Dykema, 19911 H. Kitano and
C. Van Ess-Dykema. Toward a plan-based under-
standing model for mixed-initiative dialogues. In Pro-
ceedings of the 29th Annual Meeting of the Association
for Computational Linguistics, pages 25-32, 1991.
[Kobsa and Wahlster, 1988] A. Kobsa and W. Wahlster,
editors. Special Issue on User Modeling. MIT Press,
Cambridge, Mass., September 1988. A special issue of
Computational Lingusitics.
[Kobsa and Wahlster, 19891 A. Kobsa and W. Wahlster,
editors. User Models in Dialog Systems. Springer-
Verlag, New York, 1989.
[Lehman and Carbonell, 19891 J.F. Lehman and J.G.
Carbonell. Learning the user&apos;s language: A step to-
wards automated creation of user models. In A. Kobsa
and W. Wahlster, editors, User Models in Dialog
Systems, pages 163-194. Springer-Verlag, New York,
1989.
[Levinson, 19851 Stephen E. Levinson. Structural meth-
ods in automatic speech recognition. Proceeding of the
IEEE, 73(11):1625-1650, 1985.
[Linde and Goguen, 19781 C. Linde and J. Goguen.
Structure of planning discourse. J. Social Biol. Struct.,
1:219-251, 1978.
[Litman and Allen, 19871 D.J. Litman and J.F. Allen. A
plan recognition model for subdialogues in conversa-
tions. Cognitive Science, 11(2):163-200, 1987.
[Lyon, 1974] Gordon Lyon. Syntax-directed least errors
analysis for context-free languages. Communcations
of the ACM, 17(0:3-14, 1974.
[McKeown, 1985] K.R. McKeown. Text Generation.
Cambridge University Press, New York, 1985.
[Monk, 1989] K. Monk. User models and conversationa:
settings: Modeling the user&apos;s wants. In A. Kobsa and
W. Wahlster, editors, User Models in Dialog Systems
pages 364-385. Springer-Verlag, New York, 1989.
[Mudler and Paulus, 19881 J. Mudler and E. Paulus
Expectation-based speech recognition. In H. Niemann
M. Lang, and G. Sagerer, editors, Recent Advances ii
Speech Understanding and Dialog Systems, pages 473-
477. Springer-Verlag, New York, 1988.
[Novick, 19881 D.G. Novick. Control of Mixed-Initiativl
Discourse Through Meta-Locutionary Acts: A Com
putational Model. PhD thesis, University of Oregon
1988.
[Paris, 1988] C.L. Paris. Tailoring object descriptions t(
a user&apos;s level of expertise. Computational Linguistics
14(3):64-78, 1988.
[Patten, 1988] T. Patten. Systemic Text Generation a
Problem Solving. Cambridge University Press, Nes,
York, 1988.
[Polanyi and Scha, 19831 L. Polanyi and R. Scha. 01
the recursive structure of discourse. In K. Ehlich an
H. van Riemsdijk, editors, Connectedness in Sentence
Discourse and Text, pages 141-178. Tilburg Univer
sity, 1983.
[Pollack, 19861 M.E. Pollack. A model of plan inferenc,
that distinguishes between the beliefs of actors am
observers. In Proceedings of the 24th Annual Meet
ing of the Association for Computational Linguistic
pages 207-214, 1986.
[Reichman, 1985] R. Reichman. Getting Computers t
Talk Like You and Me. MIT Press, Cambridge, Mass
1985.
[Smith, 19911 R.W. Smith. A Computational Model c
Expectation-Driven Mixed-Initiative Dialog Process
ing. PhD thesis, Duke University, 1991.
[Walker and Whittaker, 1990] M. Walker and S. Whit
taker. Mixed initiative in dialogue: An investigatio
into discourse segmentation. In Proceedings of the 28t
Annual Meeting of the Association for Computation4
Linguistics, pages 70-78, 1990.
[Walker, 1978] D.E. Walker, editor. Understanding Spc
ken Language. North-Holland, New York, 1978.
[Whittaker and Stenton, 1988] S. Whittaker and
P. Stenton. Cues and control in expert-client di
logues. In Proceedings of the 26th Annual Meeting
the Association for Computational Linguistics, pap
123-130, 1988.
[Young and Proctor, 1989] S.J. Young and C.E. Pro(
tor. The design and implementation of dialogue cor
trol in voice operated database inquiry systems. Con
puter Speech and Language, 3:329-353, 1989.
[Young et al., 1989] S.R. Young, A.G. Hauptman]
W.H. Ward, E.T. Smith, and P. Werner. High lev,
knowledge sources in usable speech recognition syl
tems. Communications of the ACM, pages 183-19,
August 1989.
</reference>
<page confidence="0.998693">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.999117">A Dialog Control Algorithm and Its Performance</title>
<author confidence="0.999978">Ronnie W Smith</author>
<affiliation confidence="0.900499666666667">Dept. of Computer Science Duke Universtiy Durham, NC 27706</affiliation>
<email confidence="0.999141">rws@cs.duke.edu</email>
<author confidence="0.998868">D Richard Hipp</author>
<affiliation confidence="0.864206666666667">Dept. of Computer Science Duke Universtiy Durham, NC 27706</affiliation>
<email confidence="0.998868">drh@cs.duke.edu</email>
<author confidence="0.99998">Alan W Biermann</author>
<affiliation confidence="0.896323666666667">Dept. of Computer Science Duke Universtiy Durham, NC 27706</affiliation>
<email confidence="0.999162">awb@cs.duke.edu</email>
<abstract confidence="0.982927436464089">A pragmatic architecture for voice dialog machines aimed at the equipment repair problem has been implemented which exhibits a number of behaviors required for efficient humanmachine dialog. These behaviors include: (1) problem solving to achieve a target goal, (2) the ability to carry out subdialogs to achieve appropriate subgoals and to pass control arbitrarily from one subdialog to another, (3) the use of a user model to enable useful verbal exchanges and to inhibit unnecessary ones, (4) the ability to change initiative from strongly computer controlled to strongly user controlled or somewhere in between, and (5) the ability to use context dependent expectations to correct speech recognition and track user movement to new subdialogs. A description of the implemented dialog control algorithm is given; an example shows the fundamental mechanisms for achieving the listed behaviors. The system implementation is described, and results from its performance in 141 problem solving sessions are given. 1 A Voice-Interactive Dialog Machine A limited vocabulary voice dialog system designed to aid a user in the repair of electronic circuits has been constructed in our laboratory. The system contains a model of the device to be repaired, debugging and repair procedures, voice recognition and sentence processing mechanisms, a user model, language generation capabilities, and a dialog control system which orchestrates the behaviors of the various parts. This paper describes the â€¢This research was supported by National Science Foundation Grant Number NSF-IRI-88-03802 and by Duke University. dialog control algorithm and the performance of the total system in aiding a series of subjects in the repair of an electronic circuit. 2 Target Behaviors The purpose of the dialog control algorithm is to direct the activities of the many parts of the system to obtain efficient human-machine dialog. Specifically, it is aimed at achieving the following behaviors. to a goal. dialog requires that each participant understand the purpose of the interaction and have the necessary prerequisites to cooperate its achievement. This is the structure [Grosz and Sidner, 1986], the goal-oriented mechanism that gives direction to the interaction. The primary required facilities are a problem solver that can deduce the necessary action sequences and a set of subsystems capable of carrying out those sequences. and effective movement between them. Efficient human dialog is usually segmented into uttersequences, are individually aimed at achieving relevant subgoals ([Grosz, 1978], [Linde and Goguen, 1978], [Polanyi and Scha, 1983], and [Reichman, 1985]). These are called &amp;quot;segments&amp;quot; by [Grosz and Sid- 1986] and constitute the structure in their paper. The global goal is approached by a series of attempts at subgoals each of which involves a set of interactions, the subdialogs. An aggressive strategy for global success is to choose the most likely subgoals needed for success and carry out their associated subdialogs. As the system proceeds on a given subdialog, it should always be ready to abruptly drop it if some other sub dialog suddenly seems more appropriate. This leads to the fragmented style that so commonly appears in efficient human communication. A subdialog is opened which leads to another, then another, then a jump to a previously opened subdialog, and so forth, in an unpredictable order until the necessary subgoals have been solved for an overall success. for user knowledge and abilities. Cooperative problem solving involves maintaining a dynamic of user knowledge, termed a model. concept is described for example in [Kobsa and Wahlster, 1988] and [Kobsa and Wahlster, 1989], [Chin, 1989], [Coand Jones, 1989], [Finin, 1989], [Lehman and Carbonell, 1989], [Monk, 1989], and [Paris, 1988]. The user 9 model specifies information needed for efficient interaction with the conversational partner. Its purpose is to indicate what needs to be said to the user to enable the user to function effectively. It also indicates what should be omitted because of existing user knowledge. Because considerable information is exchanged during the dialog, the user model changes continuously. Mentioned facts are stored in the model as known to the user and are not repeated. Previously unmentioned information may be assumed to be unknown and may be explained as needed. Questions from the user may indicate lack of knowledge and result in the removal of items from the user model. of initiative. real possibility in a cooperative interaction is that the user&apos;s problem solving ability, either on a given subgoal or on the global task, may exceed that of the machine. When this occurs, an efficient interaction requires that the machine yield control so that the more competent partner can lead the way to the fastest possible solution. Thus, the machine must be able to carry out its own problem solving process and direct the actions to task completion or yield to the user&apos;s control and respond cooperatively to his or her requests. This is a mixed-initiative dialog as studied by [Kitano and Van Ess-Dykema, 1991], [Novick, 1988], [Whittaker and Stenton, 1988], and [Walker and Whittaker, 1990]. As a pragmatic issue, we have found that at least four useful: directive computer has complete dialog control. It recommends a subgoal for completion and will use whatever dialog is necessary to obtain the needed item of knowledge related to the subgoal. suggestive computer still has dialog control, but not as strongly. The computer will make suggestions about the subgoal to perform next, but it is also willing to change the direction of the dialog according to stated user preferences. declarative user has dialog control, but the computer is free to mention relevant, though not required, facts as a response to the user&apos;s statements. passive user has complete dialog control. The computer responds directly to user questions and passively acknowledges user statements without recommending a subgoal as the next course of action. of user input. all interactions occur in the context of a current subdialog, the user&apos;s input is far more predictable than would be indicated by a general grammar for English. In fact, the current subdispecifies the the interaction, the set of objects and actions that are locally appropriate. This is structure by [Grosz and Sidner, 19861 and its most important function in our system is to predict the meaning structures the user is likely to communicate in a response. For illustration, the opening of a chassis cover plate will often evoke comments about the objects behind the cover; the measurement of a voltage is likely to include references to a voltmeter, leads, voltage range, and the locations of measurement points. The subdialog structure thus provides a set of expected utterances at each point in the conversation and these have two important roles: (1) The expected utterances provide strong guidance for the speech recognition system so that error correction can be maximized. Where ambiguity arises, recognition can be biased in the direction of meaningful statements in the current context. Earlier researchers who investigated this insight are [Erman [Walker, 1978], [Fink and Biermann, 1986], [Mudler and Paulus, 1988], [Carbonell Pierrel, 19881, [Young 19891, and [Young and Proctor, 1989]. (2) The expected utterances from subdialogs other than the current one can be used to indicate that one of those others is being invoked. Thus, expectations are one of the primary mechanisms needed for tracking the conversation as it jumps from subdialog to subdialog. This is elsewhere as the recognition probit has received much attention in recent years. See, for example, [Allen, 1983], [Litman and Allen, 1987], [Pollack, 1986], and [Carberry, 1990]. Systems capable of all of the above behaviors are rare has been observed by [Allen 1989]: &amp;quot;no one knows how to fit all of the pieces together.&amp;quot; One of the contributions of the current work is to present an architecture that can provide them all in the limited domain electric circuit repair. [Allen 1989] describe their own architecture which concentrates on representations for subdialog mechanisms and their interactionE with sentence level processing. Our work differs frorr theirs on many dimensions including our emphasis on achieving mixed-initiative, real time, voice interactive dialog which utilizes a user model. 3 The Zero Level Model The system implemented in our laboratory (described in great detail in [Smith, 1991]) achieves the above behaviors sufficiently to enable efficient human-machine di. alogs in the electric circuit repair environment. The corn. plexity of the system prevents its complete descriptior However, a level model been devised fo] pedagogical purposes which illustrates its principles o operation. This model mimicks the mechanism of th( dialog machine while omitting the huge array of detail: necessary to make a real system work. A later sectior in this paper describes the actual implementation an some of its major modules. The zero level model is the recursive subroutine Zmod Subdialog shown in figure 1. This routine is entered witl a single argument, a goal to be proven, and its action: are to carry out a Prolog-style proof of the goal. A sid: effect of the proof may be a resort to voice interaction: the user to supply necessary axioms. 10 Recursive subdialog routine (enter with a goal to prove) ZmodSubdialog (Goal) Create subdialog data structures While there are rules available which may achieve Goal Grab next available rule R from knowledge; unify with Goal If R trivially satisfies Goal, return with success If R is vocalize(X) then Execute verbal output X (mode) Record expectation Receive response (mode) Record implicit and explicit meanings for response Transfer control depending on which expected response was received Successful response: Return with success Negative response: No action Confused response: Modify rule for clarification; prioritize rule for execution Interrupt: Match response to expected response of another subdialog; go to that subdialog (mode) If R is a general rule then Store its antecedents While there are more antecedents to process Do Grab the next one and enter ZmodSubdialog with it If the ZmodSubdialog exits with failure then terminate processing on rule R If all antecedents of R succeed, return with success NOTE: SUCCESSFUL COMPLETION OF THIS ROUTINE DOES NOT NECESSARILY MEAN TRANSFER OF CONTROL TO THE CALLING ROUTINE. CONTROL PASSES TO THE SUBDIALOGUE SELECTED BY THE DIALOG CONTROLLER. Figure 1: Zero Level Model fact, the only voice interactions the system undertakes are those called for by the theorem proving machinery. The ZmodSubdialog routine has a unique capability needed for proper modeling of subdialog behaviors. Specifically, its actions may be suspended at any time so that control may be passed to another subdialog, another instantiation of ZmodSubdialog that is aimed at achieving another goal. However, control may at a later time return to the current instantiation to continue its execution. In fact, a typical computation involves a set of ZmodSubdialog instantiations all advanced to some point in the proofs of their respective goals; control passes back and forth between them looking for the most likely chances of success until, finally, enough goals are proven to complete the global proof. The algorithm becomes understandable if an example is followed through in full detail. Assume that the following database of Prolog-like rules are contained in the system knowledge base. General Debugging Rules set(knob,Y) &lt;-find(knob), adjust(knob,Y) General Dialog Rules Y &lt;-usercan(Y), vocalize(Y) vocalize(X) User Model Rules find(knob) usercan(adjust(knob,X)) Further assume that ZmodSubdialog is entered with argument Goal = set(knob,10). Then in Prolog fashion, the routine grabs the rule R: set(knob,Y) +-find(knob),adjust(knob,Y), and attempts to prove set(knob,10). The algorithm offers three choices depending on whether R is trivial (a single predicate which is not vocalize(X)), R is vocalize(X), or R is a rule with antecedents as is the case here. Thus, in this case the third alternative is followed, and the two antecedents are queued for sequential execution (find(knob) and adjust(knob,10)). Then the first antecedent is selected and ZmodSubdialog is entered with argument Goal = find(knob). The new subdialog to achieve find(knob) is short, however, since the user model indicates the user already knows how to find the knob because find(knob) exists as an available rule. In fact, ZmodSubdialog finds find(knob) in the rule base, enters the first choice (trivial) and returns with success. If find(knob) had not been found in the user model, the system might have engaged in dialog to achieve this goal. The subdialog control mechanism is not obligated to pass control back to the calling routine, but in this example we will assume that it does. Satisfactory proof of find(knob) means that the next antecedent at this level, adjust(knob,10), can be attempted, and ZmodSubdialog is entered with this goal. our model selects R = Y usercan(Y), ize(Y), unifying Y with adjust(knob,10). Then a deeper call to ZmodSubdialog finds usercan(adjust(knob,X)) in 11 the user model which means control passes to the antecedent vocalize(adjust (knob ,10)) . This yields another entry to ZmodSubdialog and a selection of the second branch. Here the system executes a voice output to satisfy vocalize(adjust(knob,10)): &amp;quot;Set the knob to one zero.&amp;quot; The handling of the goal find(knob) illustrates how the user model can act to satisfy the theorem proving process and prevent the enunciation of unneeded information. As the theorem proving process proceeds, the fact that a user knows something is represented in the knowledge base as an achieved goal. Theorem proving will encounter this and proceed without voice interaction. In the example, the model already indicates that the user knows how to find the knob so no voice interaction is needed for this goal. The handling of the goal adjust(knob,10) illustrates how the user model supports the theorem proving process by enabling voice dialog when it is needed. This goal could not be proven by application of rules available in the database and the proof was blocked from further progress. In our terminology, there was a &amp;quot;missing axiom&amp;quot; for the proof. So the system must either give up on this proof or try to fill in the missing axiom by resorting to voice dialog. In the current case, voice dialog was enabled by the user model fact usercan(adjust(knob,X)). This fact opens the way for a query to the user. If the query is positively answered, then the missing axiom is made available. The role of the user model is thus to supply or fail to supply axioms at the bottom of the proof tree. It both inhibits extraneous verbalism and enables interactions necessary to prove the theorem. Returning to the example computation of ZmodSubdialog, a voice output has just been given, and the system then records, for this output, the set of expected responses: user(adjust(knob,10)) assertion(knob,position,10) trivialresponse(affirmation) trivialresponse(negative) query(location(knob)) query(color(knob)) Expected responses are compiled from the domain knowledge base and from the dialog controller knowledge base. user&apos;s response is then (unified) the expected meanings and subsequent actions depend on which meaning fits best. Four different types of actions may occur at this point. (1) The user might respond with some paraphrase of &amp;quot;I set the knob to one zero,&amp;quot; or &amp;quot;Okay&amp;quot;, which would be interpreted as successful responses. The routine ZmodSub dialog would return with success. (2) The user might also answer &amp;quot;No&amp;quot; yielding a failure and another cycle around the theorem proving loop looking for an applicable rule. (3) The user might respond with &amp;quot;What color is the knob?&amp;quot; indicating there may be a chance for a success here if there is further dialog. In fact, our system handles such a need for clarification by dynamically modifying the rule and reexecuting with a newly required clarification subdialog. Here the rule set(knob,10) adjust(knob,10) becomes modto set(knob,10) find(knob), ize(knob , white) , adj ust (knob ,10) . Reexecution will then yield an explanation of the knob color followed by the previous request reenunciated: &amp;quot;Set the knob to one zero.&amp;quot; (4) The user might respond with an utterance that matches no local expectation. Here the system examines expectations of other sub dialogs and searches for an acceptable match. If one is found, control will pass to that subdialog. For example, if the response is, &amp;quot;The LED is flashing seven,&amp;quot; and if this is an appropriate response in some other subdialog, control will pass to it. The control of initiative in ZmodSub dialog is handled by special processing at the steps marked &amp;quot;mode.&amp;quot; Thus, in strongly directive mode, verbal outputs will be very positively stated, responses will be expected to be obedient, and interrupts to other subdialogs will be restricted. In less demanding modes, outputs will be weaker or merely suggestive, a wider range of responses will be allowed, and transitions to other subdialogs will be more freely permitted. In the most passive mode, there are few outputs except answers to questions, and an interrupt to any subdialog is acceptable. very important part of the dialog system is the doprocessor, application dependent portion of system. It receives related to the current problem and suggests debugging steps to carry the process further. We model calls to this processor the rule: (debuggingstep(X))* . This rule is called upon to debug device X with the predicate debug(X) and its effect is to specify a sequence of debugging steps which will be specified dynamically as the problem unfolds and which will lead to repair of the device. 4 The Implementation 4.1 An Integrated Architecture The implemented system is based on a computational model for dialog processing presented in [Smith, 1991]. The model is applicable to the general class of taskoriented dialogs, dialogs where the computer is assisting the user in completing a task as the dialog ensues. ThE derived implementation consists of the following modules: controller is the supervisor of the dialog processing system. It provides the subdialog processing highlighted in the zero level model. In addition, it provides the complex algorithm required to properly handle arbitrary clarification subdialogs and interrupts as well as provide the dialog expectations needed to 12 assist with input interpretation. It also maintains all dialog information shared by the other modules and controls their activation. processor previously mentioned, it provides suggestions about debugging steps to pursue. In our system the domain processor assists users in circuit repair. It receives user-supplied domain information from the dialog controller and returns suggested debugging subgoals to the controller for consideration. The subgoal selection process weighs the level of expected usefulness of a subgoal with the number of times the subgoal has been previously selected. Consequently, the module may recommend challenging previously reported information if no noticeable progress in the task is being made. In this manner, the module and system can recover from erroneous inputs. If the dialog system is to be used to repair other devices, this is the module that needs to be replaced. is the repository of information about task-oriented dialogs including: (1) rules for proving completion of goals; (2) user model information including a set of rules for inferring user model information from user input; (3) rules for determining when a clarification subdialog should be initiated; and (4) rules for defining the expectations for the user&apos;s response as a function of the type of goal attempted. Note that predefined information of this module is easily modified without requiring changes to the dialog controller. prover provides the general reasoning capability of the system. In order to allow the dialog controller to control the potentially arbitrary movement among subdialogs, the theorem prover has been made interruptible and put under the supervision of the dialog controller. Consequently, the theorem prover can maintain a set of partially completed proofs, and can activate different proofs as instructed by the dialog controller. It can also dynamically modify the proof structure, a key feature for handling clarification subdialogs. Foremost, the theorem prover is able to suspend itself when it encounters a missing axiom, permitting natural language interaction to assist in axiom acquisition. This contrasts with traditional theorem proving approaches (e.g. Prolog) which simply engage in backtracking when a missing axiom is encountered. interface consists of the generation and recognition modules. They use information on context and expectation as provided by the dialog controller. The linguistic generator was developed by Robin Gambill. It uses a rule-driven approach that takes as input an utterance specification which encapsulates the desired meaning and produces as output an English string that is sent to a text-to-speech converter for enunciation. Various approaches to generation have been described in [Danlos, 1987], [Hovy, 1988], [Jacobs, 1987], [McKeown, 1985], and [Patten, 1988]. The recognition module was designed to be able to recover from ungrammatical inputs. It will be described in greater detail below. 4.2 Minimum distance parsing A challenging design problem in any natural language is the development of a will translate the lattice of words output by the speech recognizer into a phrase of some synthetic language (Prolog in our instance) which encapsulates the meaning of what was originally spoken. The design difficulty is exacerbated by the fact that due to speech recognition errors, the word lattice output by the speech recognizer is probably different from what the user spoke, and the fact that users will sometimes speak with a word order or sentence construction which the designers of the parser&apos;s grammar did not forsee. (These, and other problems associated with robust parsing of speech are further dein [Eastman and McLean, 1981], [Hayes al., and [Young al., In our system, parsis accomplished using a parsing algorithm, similar to algorithms described in [Aho and Peterson, 1972], [Lyon, 1974], and [Levinson, 1985], but extended to accept a lattice as input, instead of a simple word list, and also extended to simultaneously perform translation and Ullman, 1969] into the target language. Minimum-distance parsing finds strings in a given context-free language which can be converted to a string of the input lattice with a minimum number of weighted insertions and deletions. When two or more strings are equidistant from the input, dialog expectation is used to break the tie. The minimumdistance parsing algorithm is very robust â€” it always finds at least one parse â€” but it is also computationally expensive. Our algorithm is faster than previously reported parsing algorithms of the same generality, but it still required careful hand-crafting and optimization on a fast machine in order to provide real-time response. It should also be noted that our recognition system is capable of selective verification of meaning. This is highlighted in the following sample dialogs. 5 Sample Dialogs Figure 2 shows two dialogs obtained from actual usage of the system. These dialogs can be seen in [Hipp and Smith, 1991] that is part of the video presentation of the conference. The words in parentheses found beside certain user statements are the words actually provided by the speech recognizer. Where the discrepancy between the output of the speech recognizer and the words produced by the user is insignificant, they have been omitted. The first dialog is an entire dialog conducted in declarative mode. Particular points to notice from this usercontrolled dialog are: 13 Dialog 1 - Declarative Mode (C denotes the computer, U the user, and R the recognizer) 1. C: This is the circuit fix it shop. How may I help you? 2. U: I want to fix this rs111. 3. C: I am familiar with that circuit. The LED is supposed to be displaying alternately flashing one and seven. 4. U: The LED is displaying an alternately flashing one and the top corner of a seven. 5. C: There is supposed to be a wire between connector nine nine and connector one zero zero. 6. U: It is there. 7. C: There is supposed to be a wire between connector nine nine and connector one zero two. 8. U: There is no wire on connector one zero four. (R: stays know wire i connector one zero for) 9. C: There is supposed to be a wire between connector one zero four and connector one zero two. [User adds wire and verifies the circuit is repaired, a three step process.] 10. U: The circuit is working. 11. C: Glad to have been of assistance. Good-bye.</abstract>
<note confidence="0.871916647058824">Dialog 2 - Directive Mode excerpt (begins just after computer describes what circuit does) 8. U: The LED is displaying nothing. 9. C: What is the switch at when the LED is off? 10: U: The switch is down. 11. C: Put the switch up. 12. U: Done. 13. C: Put the knob to one zero. 14. U: Which knob? 15. C: Near the top boundary and in the center is a green region. Now, in the middle right of this green region is the control knob. 16. U: The knob is at one zero. 17. C: What is the LED displaying? 18. U: The LED is displaying a one and seven at the same time. 19. C: Put the knob to zero. 20. U: Done. 21. C: What is the LED displaying? 22. U: The same thing. (R: the same same)</note>
<abstract confidence="0.976182514285714">23. C: Did you mean to say the LED is displaying the same thing? 24. U: Yes. [Eventually, a missing wire is detected and added, and the computer explicitly guides the user through the verification process. The entire dialog lasts 51 utterances.] Figure 2: Sample Dialogs (1) Successful processing of anaphoric reference at utterance 6. (2) Successful shifts to user-initiated subdialogs at utterances 8 and 10. (3) Successful recovery from speech recognition errors at utterance 8. The second dialog is an excerpt from a dialog conducted in directive mode (strongly computer-controlled dialog). The total dialog lasted 51 utterances in contrast to the 11 utterance declarative mode dialog. Particular points to notice from this excerpt include: (1) Computer responses which are more directed and forceful in content than in dialog 1. (2) Successful handling of a clarification subdialog (utterances 13-16). (3) Successful verification of the implicit meaning of a user utterance in the presence of speech recognition errors in utterance 22. In contrast with utterance 8 of dialog 1, the system decided explicit verification subdialog to ascertain the meaning of the user&apos;s utterance. 6 Experimental Results The system has been implemented on a Sun 4 workstation with the majority of the code written in Quintus Prolog&apos;, and the parser in C. Speech recognition is perby a Verbex 6000 running on an and production is performed by a DTC01 text-to-speech converter. The users are restricted to a 125 word vocabulary in the connected speech system.</abstract>
<note confidence="0.8698748">Prolog is a trademark of Quintus Computer Systems, Incorporated is a trademark of Digital Equipment Corporation. 14</note>
<abstract confidence="0.99596186440678">The implemented domain processor has been loaded with a model for a particular experimental circuit assembled on a Radio Shack 160-in-One Electronic Project Kit. After testing system prototypes with a few volunteers, eight subjects used the system during the formal experimental phase. After a warmup session where the subject trained on the speech recognizer and practiced using the system, each subject participated in two sessions where up to ten problems were attempted. The system ran in declarative mode (user-controlled dialogs) during one session and in directive mode (strongly computercontrolled dialog) in the other session. Subjects attempted a total of 141 dialogs of which 118 or 84% were completed successfully.&apos; Subjects spoke a total of 2840 user utterances, with 81.5% correctly interpreted by the system although only 50.0% were correctly recognized word for word by the speech recognizer. The average speech rate was 2.8 sentences per minute, and the average task completion times for successful dialogs were 4.5 and 8.5 minutes, respectively, for declarative and directive modes. The average number of user utterances per successful dialog was 10.7 in declarative mode and 27.6 in directive mode. A detailed description of the experiment and results is given in [Smith, 1991]. The substantially shorter completion times for users in declarative mode can be attributed to the fact that the subjects learned many of the debugging procedures during the experiment and did not need the detailed descriptions given in the directive mode. 7 Summary A voice interactive dialog architecture has been developed which achieves simultaneously a variety of behaviors believed to be necessary for efficient human-machine dialog. Goal oriented behavior is supplied by the theorem proving paradigm. Subdialogs and movement between them is implemented with an interruptible theorem prover that maintains a set of partially completed proofs and can work on the most appropriate one at any given time. A user model is provided by a continuously changing set of rules that are referenced in the theorem proving process either to enable or inhibit voice dialog. Mixed initiative is made possible by variable types of processing by the output and input routines and by restricting or releasing the ability to interrupt to a new subdialog. Expectation is associated with individual subdialogs, is compiled from domain and dialog information related to each specific output, and is used to improve voice recognition and enable movement between subdialogs. &apos;Of the 23 dialogs which were not completed, 22 were terminated prematurely due to excessive time being spent on the dialog. Misunderstandings due to misrecognition were the cause in 13 of these failures. Misunderstandings due to inadequate grammar coverage occurred in 3 of the failures. In 4 of the failures the subject misconnected a wire. In one failure there was confusion by the subject about when the circuit was working, and in another failure there were problems with the system software. A hardware failure caused termination of the final dialog.</abstract>
<note confidence="0.9484105078125">References [Aho and Peterson, 1972] Alfred V. Aho and Thomas G. Peterson. A minimum distance error-correcting parser context-free languages. Journal on Compu- 1972. [Aho and Ullman, 1969] A. V. Aho and J. D. Ullman. of syntax directed translations. of and System Sciences, 1969. 1989] J. Allen, S. Guez, L. Hoebel, E. Hinkelman, K. Jackson, A. Kyburg, and D. Traum. The discourse system project. Technical Report 317, University of Rochester, November 1989. [Allen, 19831 J.F. Allen. Recognizing intentions from natural language utterances. In M. Brady and R.C. editors, Models of Discourse, pages 107-166. MIT Press, Cambridge, Mass., 1983. 1990] S. Carberry. Recognition in Natu- Language Dialogue. Press, Cambridge, Mass., 1990. [Carbonell and Pierrel, 19881 N. Carbonell and J.M. Pierrel. Task-oriented dialogue processing in humancomputer voice communication. In H. Niemann, Lang, and G. Sagerer, editors, Advances Speech Understanding and Dialog Systems, 491-496. Springer-Verlag, New York, 1988. [Chin, 1989] D.N. Chin. KNOME: Modeling what the user knows in UC. In A. Kobsa and W. Wahlster, Models in Dialog Systems, 74-107. Springer-Verlag, New York, 1989. [Cohen and Jones, 1989] R. Cohen and M. Jones. Incorporating user models into expert systems for educational diagnosis. In A. Kobsa and W. Wahlster, Models in Dialog Systems, 313- 333. Springer-Verlag, New York, 1989. 1987] L. Danlos. Linguistic Basis of Text University Press, New York, 1987. [Eastman and McLean, 1981] C. M. Eastman and D. S. McLean. On the need for parsing ill-formed in- Journal of Computational Linguistics, 7(4):257, 1981. 1980] L.D. Erman, F. Hayes-Roth, V.R. Lesser, and D.R. Reddy. The Hearsay-II speechunderstanding system: Integrating knowledge to reuncertainty. Computing Surveys, 213-253, June 1980. [Finin, 1989] T.W. Finin. GUMS: A general user modelshell. In A. Kobsa and W. Wahlster, editors, in Dialog Systems, 411-430. Springer- Verlag, New York, 1989. [Fink and Biermann, 1986] P.E. Fink and A.W. Biermann. The correction of ill-formed input using history-based expectation with applications to speech Linguistics, 36, 1986. 15 [Grosz and Sidner, 19861 B.J. Grosz and C.L. Sidner. Attentions, intentions, and the structure of discourse. Linguistics, 1986. [Grosz, 19781 B.J. Grosz. Discourse analysis. In Walker, editor, Spoken Language, pages 235-268. North-Holland, New York, 1978. al., Philip J. Hayes, Alexander G Hauptmann, Jaime G. Carbonell, and Masaru Tomita. Parsing spoken language: A semantic caseframe ap- In Proceedings of the 11th International Conference on Computational Linguistics, pages 587-592, Bonn, August 1986. [Hipp and Smith, 19911 D. R. Hipp and R. W. Smith. A demonstration of the &amp;quot;circuit fix-it shoppe&amp;quot;. A 12 minute videotape available from the authors at Duke University, Durham, NC 27706, August 1991. 1988] E.H. Hovy. Natural Language Pragmatic Constraints. Erlbaum Associates, Hillsdale, NJ, 1988. 1987] P.S. Jacobs. KING: a knowledgeintensive natural language generator. In Gerard Kemeditor, Language Generation, 219- 230. Martinus Nijhoff Publishers, Boston, 1987. [Kitano and Van Ess-Dykema, 19911 H. Kitano and C. Van Ess-Dykema. Toward a plan-based undermodel for mixed-initiative dialogues. In Proceedings of the 29th Annual Meeting of the Association Computational Linguistics, 25-32, 1991. [Kobsa and Wahlster, 1988] A. Kobsa and W. Wahlster, Issue on User Modeling. Press, Cambridge, Mass., September 1988. A special issue of Computational Lingusitics. [Kobsa and Wahlster, 19891 A. Kobsa and W. Wahlster, Models in Dialog Systems. Springer- Verlag, New York, 1989. [Lehman and Carbonell, 19891 J.F. Lehman and J.G. Carbonell. Learning the user&apos;s language: A step towards automated creation of user models. In A. Kobsa W. Wahlster, editors, Models in Dialog 163-194. Springer-Verlag, New York, 1989. [Levinson, 19851 Stephen E. Levinson. Structural methin automatic speech recognition. of the 1985. [Linde and Goguen, 19781 C. Linde and J. Goguen. of planning discourse. Social Biol. Struct., 1:219-251, 1978. [Litman and Allen, 19871 D.J. Litman and J.F. Allen. A plan recognition model for subdialogues in conversa- Science, 1987. [Lyon, 1974] Gordon Lyon. Syntax-directed least errors for context-free languages. the ACM, 1974. 1985] K.R. McKeown. Generation. Cambridge University Press, New York, 1985. [Monk, 1989] K. Monk. User models and conversationa: settings: Modeling the user&apos;s wants. In A. Kobsa and Wahlster, editors, Models in Dialog Systems pages 364-385. Springer-Verlag, New York, 1989. [Mudler and Paulus, 19881 J. Mudler and E. Paulus Expectation-based speech recognition. In H. Niemann Lang, and G. Sagerer, editors, Advances Understanding and Dialog Systems, 473- 477. Springer-Verlag, New York, 1988. 19881 D.G. Novick. of Mixed-Initiativl Discourse Through Meta-Locutionary Acts: A Com Model. thesis, University of Oregon 1988. [Paris, 1988] C.L. Paris. Tailoring object descriptions t( user&apos;s level of expertise. Linguistics 14(3):64-78, 1988. 1988] T. Patten. Text Generation a</note>
<affiliation confidence="0.943871">Solving. University Press,</affiliation>
<address confidence="0.919703">York, 1988.</address>
<note confidence="0.963294689655172">[Polanyi and Scha, 19831 L. Polanyi and R. Scha. 01 the recursive structure of discourse. In K. Ehlich an van Riemsdijk, editors, in Sentence and Text, 141-178. Tilburg Univer sity, 1983. 19861 M.E. Pollack. A model of plan that distinguishes between the beliefs of actors am In of the 24th Annual Meet ing of the Association for Computational Linguistic pages 207-214, 1986. 1985] R. Reichman. Computers t Like You and Me. Press, Cambridge, 1985. 19911 R.W. Smith. Computational Model c Expectation-Driven Mixed-Initiative Dialog Process thesis, Duke University, 1991. [Walker and Whittaker, 1990] M. Walker and S. Whit taker. Mixed initiative in dialogue: An investigatio discourse segmentation. In of the 28t Annual Meeting of the Association for Computation4 70-78, 1990. 1978] D.E. Walker, editor. Spc Language. New York, 1978. [Whittaker and Stenton, 1988] S. Whittaker and P. Stenton. Cues and control in expert-client di In of the 26th Annual Meeting Association for Computational Linguistics, 123-130, 1988. [Young and Proctor, 1989] S.J. Young and C.E. Pro(</note>
<abstract confidence="0.848813875">tor. The design and implementation of dialogue cor in voice operated database inquiry systems. Speech 1989. 1989] S.R. Young, A.G. Hauptman] W.H. Ward, E.T. Smith, and P. Werner. High lev, knowledge sources in usable speech recognition syl of the ACM, August 1989.</abstract>
<intro confidence="0.519281">16</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>A minimum distance error-correcting parser for context-free languages.</title>
<date>1972</date>
<journal>SIAM Journal on Computation,</journal>
<pages>1--4</pages>
<marker>1972</marker>
<rawString>[Aho and Peterson, 1972] Alfred V. Aho and Thomas G. Peterson. A minimum distance error-correcting parser for context-free languages. SIAM Journal on Computation, 1(4):305-312, 1972.</rawString>
</citation>
<citation valid="true">
<title>Properties of syntax directed translations.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>3--3</pages>
<marker>1969</marker>
<rawString>[Aho and Ullman, 1969] A. V. Aho and J. D. Ullman. Properties of syntax directed translations. Journal of Computer and System Sciences, 3(3):319-334, 1969.</rawString>
</citation>
<citation valid="true">
<title>The discourse system project.</title>
<date>1989</date>
<tech>Technical Report 317,</tech>
<institution>University of Rochester,</institution>
<marker>1989</marker>
<rawString>[Allen et al., 1989] J. Allen, S. Guez, L. Hoebel, E. Hinkelman, K. Jackson, A. Kyburg, and D. Traum. The discourse system project. Technical Report 317, University of Rochester, November 1989.</rawString>
</citation>
<citation valid="true">
<title>Recognizing intentions from natural language utterances.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>107--166</pages>
<editor>In M. Brady and R.C. Berwick, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<marker>1983</marker>
<rawString>[Allen, 19831 J.F. Allen. Recognizing intentions from natural language utterances. In M. Brady and R.C. Berwick, editors, Computational Models of Discourse, pages 107-166. MIT Press, Cambridge, Mass., 1983.</rawString>
</citation>
<citation valid="true">
<title>Plan Recognition in Natural Language Dialogue.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<marker>1990</marker>
<rawString>[Carberry, 1990] S. Carberry. Plan Recognition in Natural Language Dialogue. MIT Press, Cambridge, Mass., 1990.</rawString>
</citation>
<citation valid="true">
<title>Task-oriented dialogue processing in humancomputer voice communication. In</title>
<date>1988</date>
<booktitle>Recent Advances in Speech Understanding and Dialog Systems,</booktitle>
<pages>491--496</pages>
<editor>H. Niemann, M. Lang, and G. Sagerer, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York,</location>
<marker>1988</marker>
<rawString>[Carbonell and Pierrel, 19881 N. Carbonell and J.M. Pierrel. Task-oriented dialogue processing in humancomputer voice communication. In H. Niemann, M. Lang, and G. Sagerer, editors, Recent Advances in Speech Understanding and Dialog Systems, pages 491-496. Springer-Verlag, New York, 1988.</rawString>
</citation>
<citation valid="true">
<title>KNOME: Modeling what the user knows</title>
<date>1989</date>
<booktitle>User Models in Dialog Systems,</booktitle>
<pages>74--107</pages>
<editor>in UC. In A. Kobsa and W. Wahlster, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York,</location>
<marker>1989</marker>
<rawString>[Chin, 1989] D.N. Chin. KNOME: Modeling what the user knows in UC. In A. Kobsa and W. Wahlster, editors, User Models in Dialog Systems, pages 74-107. Springer-Verlag, New York, 1989.</rawString>
</citation>
<citation valid="true">
<title>Incorporating user models into expert systems for educational diagnosis.</title>
<date>1989</date>
<booktitle>User Models in Dialog Systems,</booktitle>
<pages>313--333</pages>
<editor>In A. Kobsa and W. Wahlster, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York,</location>
<marker>1989</marker>
<rawString>[Cohen and Jones, 1989] R. Cohen and M. Jones. Incorporating user models into expert systems for educational diagnosis. In A. Kobsa and W. Wahlster, editors, User Models in Dialog Systems, pages 313-333. Springer-Verlag, New York, 1989.</rawString>
</citation>
<citation valid="true">
<title>The Linguistic Basis of Text Generation.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>New York,</location>
<marker>1987</marker>
<rawString>[Danlos, 1987] L. Danlos. The Linguistic Basis of Text Generation. Cambridge University Press, New York, 1987.</rawString>
</citation>
<citation valid="true">
<title>On the need for parsing ill-formed input.</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>7</volume>
<issue>4</issue>
<marker>1981</marker>
<rawString>[Eastman and McLean, 1981] C. M. Eastman and D. S. McLean. On the need for parsing ill-formed input. American Journal of Computational Linguistics, 7(4):257, 1981.</rawString>
</citation>
<citation valid="true">
<title>The Hearsay-II speechunderstanding system: Integrating knowledge to resolve uncertainty.</title>
<date>1980</date>
<journal>ACM Computing Surveys,</journal>
<pages>213--253</pages>
<marker>1980</marker>
<rawString>[Erman et a/., 1980] L.D. Erman, F. Hayes-Roth, V.R. Lesser, and D.R. Reddy. The Hearsay-II speechunderstanding system: Integrating knowledge to resolve uncertainty. ACM Computing Surveys, pages 213-253, June 1980.</rawString>
</citation>
<citation valid="true">
<title>A general user modeling shell.</title>
<date>1989</date>
<booktitle>User Models in Dialog Systems,</booktitle>
<pages>411--430</pages>
<editor>T.W. Finin. GUMS:</editor>
<publisher>SpringerVerlag,</publisher>
<location>New York,</location>
<marker>1989</marker>
<rawString>[Finin, 1989] T.W. Finin. GUMS: A general user modeling shell. In A. Kobsa and W. Wahlster, editors, User Models in Dialog Systems, pages 411-430. SpringerVerlag, New York, 1989.</rawString>
</citation>
<citation valid="true">
<title>The correction of ill-formed input using history-based expectation with applications to speech understanding.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>1</issue>
<marker>1986</marker>
<rawString>[Fink and Biermann, 1986] P.E. Fink and A.W. Biermann. The correction of ill-formed input using history-based expectation with applications to speech understanding. Computational Linguistics, 12(1):13â€” 36, 1986.</rawString>
</citation>
<citation valid="true">
<title>Attentions, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<marker>1986</marker>
<rawString>[Grosz and Sidner, 19861 B.J. Grosz and C.L. Sidner. Attentions, intentions, and the structure of discourse. Computational Linguistics, 12(3):175-204, 1986.</rawString>
</citation>
<citation valid="true">
<title>Discourse analysis.</title>
<date>1978</date>
<booktitle>Understanding Spoken Language,</booktitle>
<pages>235--268</pages>
<editor>In D.E. Walker, editor,</editor>
<publisher>North-Holland,</publisher>
<location>New York,</location>
<marker>1978</marker>
<rawString>[Grosz, 19781 B.J. Grosz. Discourse analysis. In D.E. Walker, editor, Understanding Spoken Language, pages 235-268. North-Holland, New York, 1978.</rawString>
</citation>
<citation valid="true">
<title>Parsing spoken language: A semantic caseframe approach.</title>
<date>1986</date>
<booktitle>In COLING-86: Proceedings of the 11th International Conference on Computational Linguistics,</booktitle>
<pages>587--592</pages>
<location>Bonn,</location>
<marker>1986</marker>
<rawString>[Hayes et al., 1986] Philip J. Hayes, Alexander G Hauptmann, Jaime G. Carbonell, and Masaru Tomita. Parsing spoken language: A semantic caseframe approach. In COLING-86: Proceedings of the 11th International Conference on Computational Linguistics, pages 587-592, Bonn, August 1986.</rawString>
</citation>
<citation valid="true">
<title>A demonstration of the &amp;quot;circuit fix-it shoppe&amp;quot;. A 12 minute videotape available from the authors at Duke University,</title>
<date>1991</date>
<pages>27706</pages>
<location>Durham, NC</location>
<marker>1991</marker>
<rawString>[Hipp and Smith, 19911 D. R. Hipp and R. W. Smith. A demonstration of the &amp;quot;circuit fix-it shoppe&amp;quot;. A 12 minute videotape available from the authors at Duke University, Durham, NC 27706, August 1991.</rawString>
</citation>
<citation valid="true">
<title>E.H. Hovy. Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum Associates,</title>
<date>1988</date>
<location>Hillsdale, NJ,</location>
<marker>1988</marker>
<rawString>[Hovy, 1988] E.H. Hovy. Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum Associates, Hillsdale, NJ, 1988.</rawString>
</citation>
<citation valid="true">
<title>knowledgeintensive natural language generator.</title>
<date>1987</date>
<booktitle>Natural Language Generation,</booktitle>
<pages>219--230</pages>
<editor>P.S. Jacobs. KING: a</editor>
<publisher>Martinus Nijhoff Publishers,</publisher>
<location>Boston,</location>
<marker>1987</marker>
<rawString>[Jacobs, 1987] P.S. Jacobs. KING: a knowledgeintensive natural language generator. In Gerard Kempen, editor, Natural Language Generation, pages 219-230. Martinus Nijhoff Publishers, Boston, 1987.</rawString>
</citation>
<citation valid="true">
<title>Ess-Dykema. Toward a plan-based understanding model for mixed-initiative dialogues.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<marker>1991</marker>
<rawString>[Kitano and Van Ess-Dykema, 19911 H. Kitano and C. Van Ess-Dykema. Toward a plan-based understanding model for mixed-initiative dialogues. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 25-32, 1991.</rawString>
</citation>
<citation valid="true">
<title>A special issue of Computational Lingusitics.</title>
<date>1988</date>
<booktitle>Special Issue on User Modeling.</booktitle>
<editor>A. Kobsa and W. Wahlster, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<marker>1988</marker>
<rawString>[Kobsa and Wahlster, 1988] A. Kobsa and W. Wahlster, editors. Special Issue on User Modeling. MIT Press, Cambridge, Mass., September 1988. A special issue of Computational Lingusitics.</rawString>
</citation>
<citation valid="true">
<date>1989</date>
<booktitle>User Models in Dialog Systems.</booktitle>
<editor>A. Kobsa and W. Wahlster, editors.</editor>
<publisher>SpringerVerlag,</publisher>
<location>New York,</location>
<marker>1989</marker>
<rawString>[Kobsa and Wahlster, 19891 A. Kobsa and W. Wahlster, editors. User Models in Dialog Systems. SpringerVerlag, New York, 1989.</rawString>
</citation>
<citation valid="true">
<title>Learning the user&apos;s language: A step towards automated creation of user models.</title>
<date>1989</date>
<booktitle>User Models in Dialog Systems,</booktitle>
<pages>163--194</pages>
<editor>In A. Kobsa and W. Wahlster, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York,</location>
<marker>1989</marker>
<rawString>[Lehman and Carbonell, 19891 J.F. Lehman and J.G. Carbonell. Learning the user&apos;s language: A step towards automated creation of user models. In A. Kobsa and W. Wahlster, editors, User Models in Dialog Systems, pages 163-194. Springer-Verlag, New York, 1989.</rawString>
</citation>
<citation valid="true">
<title>Structural methods in automatic speech recognition.</title>
<date>1985</date>
<booktitle>Proceeding of the IEEE,</booktitle>
<pages>73--11</pages>
<marker>1985</marker>
<rawString>[Levinson, 19851 Stephen E. Levinson. Structural methods in automatic speech recognition. Proceeding of the IEEE, 73(11):1625-1650, 1985.</rawString>
</citation>
<citation valid="true">
<title>Structure of planning discourse.</title>
<date>1978</date>
<journal>J. Social Biol. Struct.,</journal>
<pages>1--219</pages>
<marker>1978</marker>
<rawString>[Linde and Goguen, 19781 C. Linde and J. Goguen. Structure of planning discourse. J. Social Biol. Struct., 1:219-251, 1978.</rawString>
</citation>
<citation valid="true">
<title>A plan recognition model for subdialogues in conversations.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<pages>11--2</pages>
<marker>1987</marker>
<rawString>[Litman and Allen, 19871 D.J. Litman and J.F. Allen. A plan recognition model for subdialogues in conversations. Cognitive Science, 11(2):163-200, 1987.</rawString>
</citation>
<citation valid="true">
<title>Gordon Lyon. Syntax-directed least errors analysis for context-free languages.</title>
<date>1974</date>
<journal>Communcations of the ACM,</journal>
<pages>17--0</pages>
<marker>1974</marker>
<rawString>[Lyon, 1974] Gordon Lyon. Syntax-directed least errors analysis for context-free languages. Communcations of the ACM, 17(0:3-14, 1974.</rawString>
</citation>
<citation valid="true">
<title>Text Generation.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>New York,</location>
<marker>1985</marker>
<rawString>[McKeown, 1985] K.R. McKeown. Text Generation. Cambridge University Press, New York, 1985.</rawString>
</citation>
<citation valid="true">
<title>User models and conversationa: settings: Modeling the user&apos;s wants.</title>
<date>1989</date>
<booktitle>User Models in Dialog Systems</booktitle>
<pages>364--385</pages>
<editor>In A. Kobsa and W. Wahlster, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York,</location>
<marker>1989</marker>
<rawString>[Monk, 1989] K. Monk. User models and conversationa: settings: Modeling the user&apos;s wants. In A. Kobsa and W. Wahlster, editors, User Models in Dialog Systems pages 364-385. Springer-Verlag, New York, 1989.</rawString>
</citation>
<citation valid="true">
<title>Expectation-based speech recognition. In</title>
<date>1988</date>
<booktitle>Recent Advances ii Speech Understanding and Dialog Systems,</booktitle>
<pages>473--477</pages>
<editor>J. Mudler and E. Paulus</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York,</location>
<marker>1988</marker>
<rawString>[Mudler and Paulus, 19881 J. Mudler and E. Paulus Expectation-based speech recognition. In H. Niemann M. Lang, and G. Sagerer, editors, Recent Advances ii Speech Understanding and Dialog Systems, pages 473-477. Springer-Verlag, New York, 1988.</rawString>
</citation>
<citation valid="true">
<title>Control of Mixed-Initiativl Discourse Through Meta-Locutionary Acts: A Com putational Model.</title>
<date>1988</date>
<tech>PhD thesis,</tech>
<institution>University of Oregon</institution>
<marker>1988</marker>
<rawString>[Novick, 19881 D.G. Novick. Control of Mixed-Initiativl Discourse Through Meta-Locutionary Acts: A Com putational Model. PhD thesis, University of Oregon 1988.</rawString>
</citation>
<citation valid="true">
<title>C.L. Paris. Tailoring object descriptions t( a user&apos;s level of expertise.</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<pages>14--3</pages>
<marker>1988</marker>
<rawString>[Paris, 1988] C.L. Paris. Tailoring object descriptions t( a user&apos;s level of expertise. Computational Linguistics 14(3):64-78, 1988.</rawString>
</citation>
<citation valid="true">
<title>Systemic Text Generation a Problem Solving.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Nes, York,</location>
<marker>1988</marker>
<rawString>[Patten, 1988] T. Patten. Systemic Text Generation a Problem Solving. Cambridge University Press, Nes, York, 1988.</rawString>
</citation>
<citation valid="true">
<title>01 the recursive structure of discourse. In</title>
<date>1983</date>
<booktitle>Connectedness in Sentence Discourse and Text,</booktitle>
<pages>141--178</pages>
<editor>K. Ehlich an H. van Riemsdijk, editors,</editor>
<marker>1983</marker>
<rawString>[Polanyi and Scha, 19831 L. Polanyi and R. Scha. 01 the recursive structure of discourse. In K. Ehlich an H. van Riemsdijk, editors, Connectedness in Sentence Discourse and Text, pages 141-178. Tilburg Univer sity, 1983.</rawString>
</citation>
<citation valid="true">
<title>A model of plan inferenc, that distinguishes between the beliefs of actors am observers.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meet ing of the Association for Computational Linguistic</booktitle>
<pages>207--214</pages>
<marker>1986</marker>
<rawString>[Pollack, 19861 M.E. Pollack. A model of plan inferenc, that distinguishes between the beliefs of actors am observers. In Proceedings of the 24th Annual Meet ing of the Association for Computational Linguistic pages 207-214, 1986.</rawString>
</citation>
<citation valid="true">
<title>Getting Computers t Talk Like You and Me.</title>
<date>1985</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass</location>
<marker>1985</marker>
<rawString>[Reichman, 1985] R. Reichman. Getting Computers t Talk Like You and Me. MIT Press, Cambridge, Mass 1985.</rawString>
</citation>
<citation valid="true">
<title>A Computational Model c Expectation-Driven Mixed-Initiative Dialog Process ing.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>Duke University,</institution>
<marker>1991</marker>
<rawString>[Smith, 19911 R.W. Smith. A Computational Model c Expectation-Driven Mixed-Initiative Dialog Process ing. PhD thesis, Duke University, 1991.</rawString>
</citation>
<citation valid="true">
<title>Mixed initiative in dialogue: An investigatio into discourse segmentation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28t Annual Meeting of the Association for Computation4 Linguistics,</booktitle>
<pages>70--78</pages>
<marker>1990</marker>
<rawString>[Walker and Whittaker, 1990] M. Walker and S. Whit taker. Mixed initiative in dialogue: An investigatio into discourse segmentation. In Proceedings of the 28t Annual Meeting of the Association for Computation4 Linguistics, pages 70-78, 1990.</rawString>
</citation>
<citation valid="false">
<date>1978</date>
<editor>D.E. Walker, editor. Understanding Spc ken Language.</editor>
<publisher>North-Holland,</publisher>
<location>New York,</location>
<marker>1978</marker>
<rawString>[Walker, 1978] D.E. Walker, editor. Understanding Spc ken Language. North-Holland, New York, 1978.</rawString>
</citation>
<citation valid="true">
<title>Cues and control in expert-client di logues.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting the Association for Computational Linguistics,</booktitle>
<pages>123--130</pages>
<marker>1988</marker>
<rawString>[Whittaker and Stenton, 1988] S. Whittaker and P. Stenton. Cues and control in expert-client di logues. In Proceedings of the 26th Annual Meeting the Association for Computational Linguistics, pap 123-130, 1988.</rawString>
</citation>
<citation valid="true">
<title>The design and implementation of dialogue cor trol in voice operated database inquiry systems. Con puter Speech and Language,</title>
<date>1989</date>
<pages>3--329</pages>
<marker>1989</marker>
<rawString>[Young and Proctor, 1989] S.J. Young and C.E. Pro( tor. The design and implementation of dialogue cor trol in voice operated database inquiry systems. Con puter Speech and Language, 3:329-353, 1989.</rawString>
</citation>
<citation valid="true">
<title>High lev, knowledge sources in usable speech recognition syl tems.</title>
<date>1989</date>
<journal>Communications of the ACM,</journal>
<pages>183--19</pages>
<marker>1989</marker>
<rawString>[Young et al., 1989] S.R. Young, A.G. Hauptman] W.H. Ward, E.T. Smith, and P. Werner. High lev, knowledge sources in usable speech recognition syl tems. Communications of the ACM, pages 183-19, August 1989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>