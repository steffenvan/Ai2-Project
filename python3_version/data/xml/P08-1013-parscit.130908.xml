<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014908">
<title confidence="0.9986715">
Applying a Grammar-based Language Model
to a Simplified Broadcast-News Transcription Task
</title>
<author confidence="0.92853">
Tobias Kaufmann
</author>
<affiliation confidence="0.834179">
Speech Processing Group
</affiliation>
<address confidence="0.8480675">
ETH Z¨urich
Z¨urich, Switzerland
</address>
<email confidence="0.995512">
kaufmann@tik.ee.ethz.ch
</email>
<author confidence="0.723406">
Beat Pfister
</author>
<affiliation confidence="0.654635">
Speech Processing Group
</affiliation>
<address confidence="0.824717">
ETH Z¨urich
Z¨urich, Switzerland
</address>
<email confidence="0.997216">
pfister@tik.ee.ethz.ch
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999175666666667">
We propose a language model based on
a precise, linguistically motivated grammar
(a hand-crafted Head-driven Phrase Structure
Grammar) and a statistical model estimating
the probability of a parse tree. The language
model is applied by means of an N-best rescor-
ing step, which allows to directly measure the
performance gains relative to the baseline sys-
tem without rescoring. To demonstrate that
our approach is feasible and beneficial for
non-trivial broad-domain speech recognition
tasks, we applied it to a simplified German
broadcast-news transcription task. We report
a significant reduction in word error rate com-
pared to a state-of-the-art baseline system.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958823529412">
It has repeatedly been pointed out that N-grams
model natural language only superficially: an Nth-
order Markov chain is a very crude model of the
complex dependencies between words in an utter-
ance. More accurate statistical models of natural
language have mainly been developed in the field
of statistical parsing, e.g. Collins (2003), Charniak
(2000) and Ratnaparkhi (1999). Other linguistically
inspired language models like Chelba and Jelinek
(2000) and Roark (2001) have been applied to con-
tinuous speech recognition.
These models have in common that they explic-
itly or implicitly use a context-free grammar induced
from a treebank, with the exception of Chelba and
Jelinek (2000). The probability of a rule expansion
or parser operation is conditioned on various con-
textual information and the derivation history. An
</bodyText>
<page confidence="0.978668">
106
</page>
<bodyText confidence="0.997278705882353">
important reason for the success of these models is
the fact that they are lexicalized: the probability dis-
tributions are also conditioned on the actual words
occuring in the utterance, and not only on their parts
of speech. Most statistical parsers achieve a high ro-
bustness with respect to out-of-grammar sentences
by allowing for arbitrary derivations and rule expan-
sions. On the other hand, they are not suited to reli-
ably decide on the grammaticality of a given phrase,
as they do not accurately model the linguistic con-
straints inherent in natural language.
We take a completely different position. In the
first place, we want our language model to reliably
distinguish between grammatical and ungrammati-
cal phrases. To this end, we have developed a pre-
cise, linguistically motivated grammar. To distin-
guish between common and uncommon phrases, we
use a statistical model that estimates the probability
of a phrase based on the syntactic dependencies es-
tablished by the parser. We achieve some degree of
robustness by letting the grammar accept arbitrary
sequences of words and phrases. To keep the gram-
mar restrictive, such sequences are penalized by the
statistical model.
Accurate hand-crafted grammars have been ap-
plied to speech recognition before, e.g. Kiefer et
al. (2000) and van Noord et al. (1999). However,
they primarily served as a basis for a speech un-
derstanding component and were applied to narrow-
domain tasks such as appointment scheduling or
public transport information. We are mainly con-
cerned with speech recognition performance on
broad-domain recognition tasks.
Beutler et al. (2005) pursued a similar approach.
</bodyText>
<note confidence="0.821603">
Proceedings of ACL-08: HLT, pages 106–113,
</note>
<page confidence="0.548073">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999953315789474">
However, their grammar-based language model did
not make use of a probabilistic component, and it
was applied to a rather simple recognition task (dic-
tation texts for pupils read and recorded under good
acoustic conditions, no out-of-vocabulary words).
Besides proposing an improved language model,
this paper presents experimental results for a much
more difficult and realistic task and compares them
to the performance of a state-of-the-art baseline sys-
tem.
In the following Section, we will first describe our
grammar-based language model. Next, we will turn
to the linguistic components of the model, namely
the grammar, the lexicon and the parser. We will
point out some of the challenges arising from the
broad-domain speech recognition application and
propose ways to deal with them. Finally, we will de-
scribe our experiments on broadcast news data and
discuss the results.
</bodyText>
<sectionHeader confidence="0.997822" genericHeader="method">
2 Language Model
</sectionHeader>
<subsectionHeader confidence="0.999313">
2.1 The General Approach
</subsectionHeader>
<bodyText confidence="0.99979075">
Speech recognizers choose the word sequence
W� which maximizes the posterior probability
P(W |O), where O is the acoustic observation. This
is achieved by optimizing
</bodyText>
<equation confidence="0.794086">
P(O|W ) · P(W)λ · ip|W |(1)
</equation>
<bodyText confidence="0.9999355">
The language model weight A and the word inser-
tion penalty ip lead to a better performance in prac-
tice, but they have no theoretical justification. Our
grammar-based language model is incorporated into
the above expression as an additional probability
Pgram(W), weighted by a parameter p:
</bodyText>
<equation confidence="0.949474833333333">
P(O|W)·P(W)λ·Pgram(W)µ·ip|W|
(2)
Pgram(W) is defined as the probability of the most
likely parse tree of a word sequence W:
Pgram(W) = max
TEparses(W)
</equation>
<bodyText confidence="0.999869">
To determine Pgram(W) is an expensive operation
as it involves parsing. For this reason, we pursue an
N-best rescoring approach. We first produce the N
best hypotheses according to the criterion in equa-
tion (1). From these hypotheses we then choose the
final recognition result according to equation (2).
</bodyText>
<subsectionHeader confidence="0.997777">
2.2 The Probability of a Parse Tree
</subsectionHeader>
<bodyText confidence="0.999071636363636">
The parse trees produced by our parser are binary-
branching and rather deep. In order to compute the
probability of a parse tree, it is transformed to a flat
dependency tree similar to the syntax graph repre-
sentation used in the TIGER treebank Brants et al
(2002). An inner node of such a dependency tree
represents a constituent or phrase. Typically, it di-
rectly connects to a leaf node representing the most
important word of the phrase, the head child. The
other children represent phrases or words directly
depending on the head child. To give an example,
the immediate children of a sentence node are the
finite verb (the head child), the adverbials, the sub-
ject and the all other (verbal and non-verbal) com-
plements.
This flat structure has the advantage that the in-
formation which is most relevant for the head child
is represented within the locality of an inner node.
Assuming statistical independence between the in-
ternal structures of the inner nodes ni, we can factor
P(T) much like it is done for probabilistic context-
free grammars:
</bodyText>
<equation confidence="0.9856335">
P(T) ≈ � P(childtags(ni)  |tag(ni)) (4)
ni
</equation>
<bodyText confidence="0.999979954545455">
In the above equation, tag(ni) is simply the label
assigned to the tree node ni, and childtags(ni) de-
notes the tags assigned to the child nodes of ni.
Our statistical model for German sentences distin-
guishes between eight different tags. Three tags are
used for different types of noun phrases: pronomi-
nal NPs, non-pronominal NPs and prenominal gen-
itives. Prenominal genitives were given a dedicated
tag because they are much more restricted than or-
dinary NPs. Another two tags were used to dis-
tinguish between clauses with sentence-initial finite
verbs (main clauses) and clauses with sentence-final
finite verbs (subordinate clauses). Finally, there are
specific tags for infinitive verb phrases, adjective
phrases and prepositional phrases.
P was modeled by means of a dedicated prob-
ability distribution for each conditioning tag. The
probability of the internal structure of a sentence
was modeled as the trigram probability of the cor-
responding tag sequence (the sequence of the sen-
tence node’s child tags). The probability of an ad-
jective phrase was decomposed into the probability
</bodyText>
<equation confidence="0.9999414">
W� = argmax
W
W� = argmax
W
P(T) (3)
</equation>
<page confidence="0.987331">
107
</page>
<bodyText confidence="0.999991705882353">
of the adjective type (participle or non-participle and
attributive, adverbial or predicative) and the proba-
bility of its length in words given the adjective type.
This allows the model to directly penalize long ad-
jective phrases, which are very rare. The model for
noun phrases is based on the joint probability of the
head type (either noun, adjective or proper name),
the presence of a determiner and the presence of pre-
and postnominal modifiers. The probabilities of var-
ious other events are conditioned on those four vari-
ables, namely the number of prepositional phrases,
relative clauses and adjectives, as well as the pres-
ence of appositions and prenominal or postnominal
genitives.
The resulting probability distributions were
trained on the German TIGER treebank which con-
sists of about 50000 sentences of newspaper text.
</bodyText>
<subsectionHeader confidence="0.997974">
2.3 Robustness Issues
</subsectionHeader>
<bodyText confidence="0.983092918918919">
A major problem of grammar-based approaches
to language modeling is how to deal with out-of-
grammar utterances. Obviously, the utterance to be
recognized may be ungrammatical, or it could be
grammatical but not covered by the given grammar.
But even if the utterance is both grammatical and
covered by the grammar, the correct word sequence
may not be among the N best hypotheses due to
out-of-vocabulary words or bad acoustic conditions.
In all these cases, the best hypothesis available is
likely to be out-of-grammar, but the language model
should nevertheless prefer it to competing hypothe-
ses. To make things worse, it is not unlikely that
some of the competing hypotheses are grammatical.
It is therefore important that our language model
is robust with respect to out-of-grammar sentences.
In particular this means that it should provide a rea-
sonable parse tree for any possible word sequence
W. However, our approach is to use an accurate,
linguistically motivated grammar, and it is undesir-
able to weaken the constraints encoded in the gram-
mar. Instead, we allow the parser to attach any se-
quence of words or correct phrases to the root node,
where each attachment is penalized by the proba-
bilistic model P(T). This can be thought of as
adding two probabilistic context-free rules:
S −� S&apos; S with probability q
S −� S&apos; with probability 1−q
In order to guarantee that all possible word se-
quences are parseable, S&apos; can produce both satu-
rated phrases and arbitrary words. To include such
a productive set of rules into the grammar would
lead to serious efficiency problems. For this reason,
these rules were actually implemented as a dynamic
programming pass: after the parser has identified
all correct phrases, the most probable sequence of
phrases or words is computed.
</bodyText>
<subsectionHeader confidence="0.999164">
2.4 Model Parameters
</subsectionHeader>
<bodyText confidence="0.99998725">
Besides the distributions required to specify P(T),
our language model has three parameters: the lan-
guage model weight p, the attachment probability
q and the number of hypotheses N. The parame-
ters p and q are considered to be task-dependent.
For instance, if the utterances are well-covered by
the grammar and the acoustic conditions are good,
it can be expected that p is relatively large and that
q is relatively small. The choice of N is restricted
by the available computing power. For our experi-
ments, we chose N = 100. The influence of N on
the word error rate is discussed in the results section.
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="method">
3 Linguistic Resources
</sectionHeader>
<subsectionHeader confidence="0.999965">
3.1 Particularities of the Recognizer Output
</subsectionHeader>
<bodyText confidence="0.999996545454545">
The linguistic resources presented in this Section
are partly influenced by the form of the recog-
nizer output. In particular, the speech recognizer
does not always transcribe numbers, compounds
and acronyms as single words. For instance, the
word “einundzwanzig” (twenty-one) is transcribed
as “ein und zwanzig”, “Kriegspl¨ane” (war plans) as
“Kriegs Pl¨ane” and ”BMW” as “B. M. W.” These
transcription variants are considered to be correct
by our evaluation scheme. Therefore, the grammar
should accept them as well.
</bodyText>
<subsectionHeader confidence="0.99929">
3.2 Grammar and Parser
</subsectionHeader>
<bodyText confidence="0.999975625">
We used the Head-driven Phrase Structure Grammar
(HPSG, see Pollard and Sag (1994)) formalism to
develop a precise large-coverage grammar for Ger-
man. HPSG is an unrestricted grammar (Chomsky
type 0) which is based on a context-free skeleton
and the unification of complex feature structures.
There are several variants of HPSG which mainly
differ in the formal tools they provide for stating lin-
</bodyText>
<page confidence="0.996487">
108
</page>
<bodyText confidence="0.999988352941177">
guistic constraints. Our particular variant requires
that constituents (phrases) be continuous, but it pro-
vides a mechanism for dealing with discontinuities
as present e.g. in the German main clause, see
Kaufmann and Pfister (2007). HPSG typically dis-
tinguishes between immediate dominance schemata
(rough equivalents of phrase structure rules, but
making no assumptions about constituent order) and
linear precedence rules (constraints on constituent
order). We do not make this distinction but rather let
immediate dominance schemata specify constituent
order. Further, the formalism allows to express com-
plex linguistic constraints by means of predicates or
relational constraints. At parse time, predicates are
backed by program code that can perform arbitrary
computations to check or specify feature structures.
We have implemented an efficient Java parser for
our variant of the HPSG formalism. The parser sup-
ports ambiguity packing, which is a technique for
merging constituents with different derivational his-
tories but identical syntactic properties. This is es-
sential for parsing long and ambiguous sentences.
Our grammar incorporates many ideas from ex-
isting linguistic work, e.g. M¨uller (2007), M¨uller
(1999), Crysmann (2005), Crysmann (2003). In ad-
dition, we have modeled a few constructions which
occur frequently but are often neglected in formal
syntactic theories. Among them are prenominal and
postnominal genitives, expressions of quantity and
expressions of date and time. Further, we have
implemented dedicated subgrammars for analyzing
written numbers, compounds and acronyms that are
written as separate words. To reduce ambiguity, only
noun-noun compounds are covered by the grammar.
Noun-noun compounds are by far the most produc-
tive compound type.
The grammar consists of 17 rules for gen-
eral linguistic phenomena (e.g. subcategorization,
modification and extraction), 12 rules for model-
ing the German verbal complex and another 13
construction-specific rules (relative clauses, genitive
attributes, optional determiners, nominalized adjec-
tives, etc.). The various subgrammars (expressions
of date and time, written numbers, noun-noun com-
pounds and acronyms) amount to a total of 43 rules.
The grammar allows the derivation of “interme-
diate products” which cannot be regarded as com-
plete phrases. We consider complete phrases to be
sentences, subordinate clauses, relative and interrog-
ative clauses, noun phrases, prepositional phrases,
adjective phrases and expressions of date and time.
</bodyText>
<subsectionHeader confidence="0.993546">
3.3 Lexicon
</subsectionHeader>
<bodyText confidence="0.999977139534883">
The lexicon was created manually based on a list of
more than 5000 words appearing in the N-best lists
of our experiment. As the domain of our recognition
task is very broad, we attempted to include any pos-
sible reading of a given word. Our main source of
dictionary information was Duden (1999).
Each word was annotated with precise morpho-
logical and syntactic information. For example, the
roughly 2700 verbs were annotated with over 7000
valency frames. We distinguish 86 basic valency
frames, for most of which the complement types can
be further specified.
A major difficulty was the acquisition of multi-
word lexemes. Slightly deviating from the common
notion, we use the following definition: A syntac-
tic unit consisting of two or more words is a multi-
word lexeme, if the grammar cannot derive it from
its parts. English examples are idioms like “by and
large” and phrasal verbs such as “to call sth off”.
Such multi-word lexemes have to be entered into the
lexicon, but they cannot directly be identified in the
word list. Therefore, they have to be extracted from
supplementary resources. For our work, we used a
newspaper text corpus of 230M words (Frankfurter
Rundschau and Neue Z¨urcher Zeitung). This cor-
pus included only articles which are dated before the
first broadcast news show used in the experiment. In
the next few paragraphs we will discuss some types
of multiword lexemes and our methods of extracting
them.
There is a large and very productive class of Ger-
man prefix verbs whose prefixes can appear sepa-
rated from the verb, similar to English phrasal verbs.
For example, the prefix of the verb “untergehen” (to
sink) is separated in “das Schiff geht unter” (the ship
sinks) and attached in “weil das Schiff untergeht”
(because the ship sinks). The set of possible va-
lency frames of a prefix verb has to be looked up
in a dictionary as it cannot be derived systematically
from its parts. Exploiting the fact that prefixes are at-
tached to their verb under certain circumstances, we
extracted a list of prefix verbs from the above news-
paper text corpus. As the number of prefix verbs is
</bodyText>
<page confidence="0.998374">
109
</page>
<bodyText confidence="0.999970528301887">
very large, a candidate prefix verb was included into
the lexicon only if there is a recognizer hypothesis
in which both parts are present. Note that this pro-
cedure does not amount to optimizing on test data:
when parsing a hypothesis, the parser chart contains
only those multiword lexemes for which all parts are
present in the hypothesis.
Other multi-word lexemes are fixed word clus-
ters of various types. For instance, some preposi-
tional phrases appearing in support verb construc-
tions lack an otherwise mandatory determiner, e.g.
“unter Beschuss” (under fire). Many multi-word
lexemes are adverbials, e.g. “nach wie vor” (still),
“auf die Dauer” (in the long run). To extract such
word clusters we used suffix arrays proposed in Ya-
mamoto and Church (2001) and the pointwise mu-
tual information measure, see Church and Hanks
(1990). Again, it is feasible to consider only those
clusters appearing in some recognizer hypothesis.
The list of candidate clusters was reduced using dif-
ferent filter heuristics and finally checked manually.
For our task, split compounds are to be consid-
ered as multi-word lexemes as well. As our gram-
mar only models noun-noun compounds, other com-
pounds such as “unionsgef¨uhrt” (led by the union)
have to be entered into the lexicon. We applied
the decompounding algorithm proposed in Adda-
Decker (2003) to our corpus to extract such com-
pounds. The resulting candidate list was again fil-
tered manually.
We observed that many proper nouns (e.g. per-
sonal names and geographic names) are identical to
some noun, adjective or verb form. For example,
about 40% of the nouns in our lexicon share in-
flected forms with personal names. Proper nouns
considerably contribute to ambiguity, as most of
them do not require a determiner. Therefore, a
proper noun which is a homograph of an open-class
word was entered only if it is “relevant” for our
task. The “relevant” proper nouns were extracted
automatically from our text corpus. We used small
databases of unambiguous given names and forms
of address to spot personal names in significant bi-
grams. Relevant geographic names were extracted
by considering capitalized words which significantly
often follow certain local prepositions.
The final lexicon contains about 2700 verbs (in-
cluding 1900 verbs with separable prefixes), 3500
nouns, 450 adjectives, 570 closed-class words and
220 multiword lexemes. All lexicon entries amount
to a total of 137500 full forms. Noun-noun com-
pounds are not included in these numbers, as they
are handled in a morphological analysis component.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997035">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999983083333333">
The experiment was designed to measure how much
a given speech recognition system can benefit from
our grammar-based language model. To this end,
we used a baseline speech recognition system which
provided the N best hypotheses of an utterance
along with their respective scores. The grammar-
based language model was then applied to the N
best hypotheses as described in Section 2.1, yielding
a new best hypothesis. For a given test set we could
then compare the word error rate of the baseline sys-
tem with that of the extended system employing the
grammar-based language model.
</bodyText>
<subsectionHeader confidence="0.991666">
4.2 Data and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999993695652174">
Our experiments are based on word lattice out-
put from the LIMSI German broadcast news tran-
scription system (McTait and Adda-Decker, 2003),
which employs 4-gram backoff language models.
From the experiment reported in McTait and Adda-
Decker (2003), we used the first three broadcast
news shows1 which corresponds to a signal length
of roughly 50 minutes.
Rather than applying our model to the origi-
nal broadcast-news transcription task, we used the
above data to create an artificial recognition task
with manageable complexity. Our primary aim was
to design a task which allows us to investigate the
properties of our grammar-based approach and to
compare its performance with that of a competitive
baseline system.
As a first simplification, we assumed perfect sen-
tence segmentation. We manually split the original
word lattices at the sentence boundaries and merged
them where a sentence crossed a lattice boundary.
This resulted in a set of 636 lattices (sentences). Sec-
ond, we classified the sentences with respect to con-
tent type and removed those classes with an excep-
</bodyText>
<footnote confidence="0.9928235">
1The 8 o’clock broadcasts of the “Tagesschau” from the
14th of April, 213t of April and 7th of Mai 2002.
</footnote>
<page confidence="0.997444">
110
</page>
<bodyText confidence="0.9999716">
tionally high baseline word error rate. These classes
are interviews (a word error rate of 36.1%), sports
reports (28.4%) and press conferences (25.7%). The
baseline word error rate of the remaining 447 lattices
(sentences) is 11.8%.
From each of these 447 lattices, the 100 best hy-
potheses were extracted. We next compiled a list
containing all words present in the recognizer hy-
potheses. These words were entered into the lexicon
as described in Section 3.3. Finally, all extracted
recognizer hypotheses were parsed. Only 25 of the
44000 hypotheses2 caused an early termination of
the parser due to the imposed memory limits. How-
ever, the inversion of ambiguity packing (see Sec-
tion 3.2) turned out to be a bottleneck. As P(T)
does not directly apply to parse trees, all possible
readings have to be unpacked. For 24 of the 447
lattices, some of the N best hypotheses contained
phrases with more than 1000 readings. For these lat-
tices the grammar-based language model was sim-
ply switched off in the experiment, as no parse trees
were produced for efficiency reasons.
To assess the difficulty of our task, we inspected
the reference transcriptions, the word lattices and
the N-best lists for the 447 selected utterances. We
found that for only 59% of the utterances the correct
transcription is among the 100-best hypotheses. The
first-best hypothesis is completely correct for 34%
of the utterances. The out-of-vocabulary rate (es-
timated from the number of reference transcription
words which do not appear in any of the lattices) is
1.7%. The first-best word error rate is 11.79%, and
the 100-best oracle word error rate is 4.8%.
We further attempted to judge the grammatical-
ity of the reference transcriptions. We considered
only 1% of the sentences to be clearly ungrammat-
ical. 19% of the remaining sentences were found
to contain general grammatical constructions which
are not handled by our grammar. Some of these
constructions (most notably ellipses, which are om-
nipresent in broadcast-news reports) are notoriously
difficult as they would dramatically increase ambi-
guity when implemented in a grammar. About 45%
of the reference sentences were correctly analyzed
by the grammar.
</bodyText>
<footnote confidence="0.865017">
2Some of the word lattices contain less than 100 different
hypotheses.
</footnote>
<subsectionHeader confidence="0.991485">
4.3 Training and Testing
</subsectionHeader>
<bodyText confidence="0.999992611111111">
The parameter N, the maximum number of hy-
potheses to be considered, was set to 100 (the ef-
fect of choosing different values of N will be dis-
cussed in section 4.4). The remaining parameters
µ and q were trained using the leave-one-out cross-
validation method: each of the 447 utterances served
as the single test item once, whereas the remaining
446 utterances were used for training. As the er-
ror landscape is complex and discrete, we could not
use gradient-based optimization methods. Instead,
we chose µ and q from 500 equidistant points within
the intervals [0, 20] and [0, 0.25], respectively. The
word error rate was evaluated for each possible pair
of parameter values.
The evaluation scheme was taken from McTait
and Adda-Decker (2003). It ignores capitalization,
and written numbers, compounds and acronyms
need not be written as single words.
</bodyText>
<subsectionHeader confidence="0.889599">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999971192307692">
As shown in Table 1, the grammar-based language
model reduced the word error rate by 9.2% rela-
tive over the baseline system. This improvement
is statistically significant on a level of &lt; 0.1% for
both the Matched Pairs Sentence-Segment Word Er-
ror test (MAPSSWE) and McNemar’s test (Gillick
and Cox, 1989). If the parameters are optimized on
all 447 sentences (i.e. on the test data), the word
error rate is reduced by 10.7% relative.
For comparison, we redefined the probabilistic
model as P(T) _ (1 − q)qk−1, where k is the num-
ber of phrases attached to the root node. This re-
duced model only considers the grammaticality of
a phrase, completely ignoring the probability of its
internal structure. It achieved a relative word error
reduction of 5.9%, which is statistically significant
on a level of &lt; 0.1% for both tests. The improve-
ment of the full model compared to the reduced
model is weakly significant on a level of 2.6% for
the MAPSSWE test.
For both models, the optimal value of q was 0.001
for almost all training runs. The language model
weight µ of the reduced model was about 60%
smaller than the respective value for the full model,
which confirms that the full model provides more
reliable information.
</bodyText>
<page confidence="0.994036">
111
</page>
<table confidence="0.994425142857143">
AWe�R (el
experiment word error rate
baseline 11.79%
grammar, no statistics 11.09% (-5.9% rel.)
grammar 10.70% (-9.2% rel.)
grammar, cheating 10.60% (-10.7% rel.)
100-best oracle 4.80%
</table>
<tableCaption confidence="0.8785972">
Table 1: The impact of the grammar-based language
model on the word error rate. For comparison, the results
for alternative experiments are shown. In the experiment
“grammar, cheating”, the parameters were optimized on
test data.
</tableCaption>
<bodyText confidence="0.998522473684211">
Figure 1 shows the effect of varying N (the max-
imum number of hypotheses) on the word error rate
both for leave-one-out training and for optimizing
the parameters on test data. The similar shapes of
the two curves suggest that the observed variations
are partly due to the problem structure. In fact, if N
is increased and new hypotheses with a high value
of Pgram(W) appear, the benefit of the grammar-
based language model can increase (if the hypothe-
ses are predominantly good with respect to word er-
ror rate) or decrease (if they are bad). This horizon
effect tends to be reduced with increasing N (with
the exception of 89 &lt; N &lt; 93) because hypothe-
ses with high ranks need a much higher Pgram(W)
in order to compensate for their lower value of
P(O|W) · P(W)λ. For small N, the parameter esti-
mation is more severely affected by the rather acci-
dental horizon effects and therefore is prone to over-
fitting.
</bodyText>
<sectionHeader confidence="0.996852" genericHeader="conclusions">
5 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999972428571429">
We have presented a language model based on a pre-
cise, linguistically motivated grammar, and we have
successfully applied it to a difficult broad-domain
task.
It is a well-known fact that natural language is
highly ambiguous: a correct and seemingly unam-
biguous sentence may have an enormous number of
readings. A related – and for our approach even
more relevant – phenomenon is that many weird-
looking and seemingly incorrect word sequences are
in fact grammatical. This obviously reduces the ben-
efit of pure grammaticality information. A solution
is to use additional information to asses how “natu-
ral” a reading of a word sequence is. We have done a
</bodyText>
<figure confidence="0.98415">
0 20 40 60 80 100
N
</figure>
<figureCaption confidence="0.999967">
Figure 1: The word error rate as a function of the maxi-
</figureCaption>
<bodyText confidence="0.944079444444444">
mum number of best hypotheses N.
first step in this direction by estimating the probabil-
ity of a parse tree. However, our model only looks at
the structure of a parse tree and does not take the ac-
tual words into account. As N-grams and statistical
parsers demonstrate, word information can be very
valuable. It would therefore be interesting to investi-
gate ways of introducing word information into our
grammar-based model.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9983698">
This work was supported by the Swiss National Sci-
ence Foundation. We cordially thank Jean-Luc Gau-
vain of LIMSI for providing us with word lattices
from their German broadcast news transcription sys-
tem.
</bodyText>
<figure confidence="0.994036111111111">
−10
−12
−2
−4
−6
−8
0
leave−one−out
optimized on test data
</figure>
<page confidence="0.986661">
112
</page>
<sectionHeader confidence="0.994742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702333333334">
M. Adda-Decker. 2003. A corpus-based decompounding
algorithm for German lexical modeling in LVCSR. In
Proceedings of Eurospeech, pages 257–260, Geneva,
Switzerland.
R. Beutler, T. Kaufmann, and B. Pfister. 2005. Integrat-
ing a non-probabilistic grammar into large vocabulary
continuous speech recognition. In Proceedings of the
IEEEASRU 2005 Workshop, pages 104–109, San Juan
(Puerto Rico).
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories, So-
zopol, Bulgaria.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the NAACL, pages 132–139, San
Francisco, USA.
C. Chelba and F. Jelinek. 2000. Structured language
modeling. Computer Speech &amp; Language, 14(4):283–
332.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22–29.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589–637.
B. Crysmann. 2003. On the efficient implementation of
German verb placement in HPSG. In Proceedings of
RANLP.
B. Crysmann. 2005. Relative clause extraposition in
German: An efficient and portable implementation.
Research on Language and Computation, 3(1):61–82.
Duden. 1999. – Das große W¨orterbuch der deutschen
Sprache in zehn B¨anden. Dudenverlag, dritte Auflage.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of the ICASSP, pages 532–535.
T. Kaufmann and B. Pfister. 2007. Applying licenser
rules to a grammar with continuous constituents. In
Stefan M¨uller, editor, The Proceedings of the 14th In-
ternational Conference on Head-Driven Phrase Struc-
ture Grammar, pages 150–162, Stanford, USA. CSLI
Publications.
B. Kiefer, H.-U. Krieger, and M.-J. Nederhof. 2000. Ef-
ficient and robust parsing of word hypotheses graphs.
In Wolfgang Wahlster, editor, Verbmobil. Founda-
tions of Speech-to-Speech Translation, pages 280–295.
Springer, Berlin, Germany, artificial intelligence edi-
tion.
K. McTait and M. Adda-Decker. 2003. The 300k LIMSI
German broadcast news transcription system. In Pro-
ceedings of Eurospeech, Geneva, Switzerland.
S. M¨uller. 1999. Deutsche Syntax deklarativ. Head-
Driven Phrase Structure Grammar f¨ur das Deutsche.
Number 394 in Linguistische Arbeiten. Max Niemeyer
Verlag, T¨ubingen.
S. M¨uller. 2007. Head-Driven Phrase Structure Gram-
mar: Eine Einf¨uhrung. Stauffenburg Einf¨uhrungen,
Nr. 17. Stauffenburg Verlag, T¨ubingen.
G. Van Noord, G. Bouma, R. Koeling, and M.-J. Neder-
hof. 1999. Robust grammatical analysis for spo-
ken dialogue systems. Natural Language Engineer-
ing, 5(1):45–93.
C. J. Pollard and I. A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151–175.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
M. Yamamoto and K. W. Church. 2001. Using suffix
arrays to compute term frequency and document fre-
quency for all substrings in a corpus. Computational
Linguistics, 27(1):1–30.
</reference>
<page confidence="0.999315">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557749">
<title confidence="0.997784">Applying a Grammar-based Language Model to a Simplified Broadcast-News Transcription Task</title>
<author confidence="0.999788">Tobias Kaufmann</author>
<affiliation confidence="0.9627935">Speech Processing Group ETH Z¨urich</affiliation>
<address confidence="0.939171">Z¨urich, Switzerland</address>
<email confidence="0.954486">kaufmann@tik.ee.ethz.ch</email>
<author confidence="0.97036">Beat Pfister</author>
<affiliation confidence="0.9576545">Speech Processing Group ETH Z¨urich</affiliation>
<address confidence="0.939391">Z¨urich, Switzerland</address>
<email confidence="0.975883">pfister@tik.ee.ethz.ch</email>
<abstract confidence="0.9889669375">We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree. The language model is applied by means of an N-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring. To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate compared to a state-of-the-art baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Adda-Decker</author>
</authors>
<title>A corpus-based decompounding algorithm for German lexical modeling in LVCSR.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>257--260</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="19910" citStr="Adda-Decker, 2003" startWordPosition="3163" endWordPosition="3164">e model. To this end, we used a baseline speech recognition system which provided the N best hypotheses of an utterance along with their respective scores. The grammarbased language model was then applied to the N best hypotheses as described in Section 2.1, yielding a new best hypothesis. For a given test set we could then compare the word error rate of the baseline system with that of the extended system employing the grammar-based language model. 4.2 Data and Preprocessing Our experiments are based on word lattice output from the LIMSI German broadcast news transcription system (McTait and Adda-Decker, 2003), which employs 4-gram backoff language models. From the experiment reported in McTait and AddaDecker (2003), we used the first three broadcast news shows1 which corresponds to a signal length of roughly 50 minutes. Rather than applying our model to the original broadcast-news transcription task, we used the above data to create an artificial recognition task with manageable complexity. Our primary aim was to design a task which allows us to investigate the properties of our grammar-based approach and to compare its performance with that of a competitive baseline system. As a first simplificat</context>
<context position="23981" citStr="Adda-Decker (2003)" startWordPosition="3827" endWordPosition="3828"> of N will be discussed in section 4.4). The remaining parameters µ and q were trained using the leave-one-out crossvalidation method: each of the 447 utterances served as the single test item once, whereas the remaining 446 utterances were used for training. As the error landscape is complex and discrete, we could not use gradient-based optimization methods. Instead, we chose µ and q from 500 equidistant points within the intervals [0, 20] and [0, 0.25], respectively. The word error rate was evaluated for each possible pair of parameter values. The evaluation scheme was taken from McTait and Adda-Decker (2003). It ignores capitalization, and written numbers, compounds and acronyms need not be written as single words. 4.4 Results As shown in Table 1, the grammar-based language model reduced the word error rate by 9.2% relative over the baseline system. This improvement is statistically significant on a level of &lt; 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemar’s test (Gillick and Cox, 1989). If the parameters are optimized on all 447 sentences (i.e. on the test data), the word error rate is reduced by 10.7% relative. For comparison, we redefined the probabilist</context>
</contexts>
<marker>Adda-Decker, 2003</marker>
<rawString>M. Adda-Decker. 2003. A corpus-based decompounding algorithm for German lexical modeling in LVCSR. In Proceedings of Eurospeech, pages 257–260, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Beutler</author>
<author>T Kaufmann</author>
<author>B Pfister</author>
</authors>
<title>Integrating a non-probabilistic grammar into large vocabulary continuous speech recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEEASRU 2005 Workshop,</booktitle>
<pages>104--109</pages>
<location>San Juan (Puerto Rico).</location>
<contexts>
<context position="3407" citStr="Beutler et al. (2005)" startWordPosition="514" endWordPosition="517">ve some degree of robustness by letting the grammar accept arbitrary sequences of words and phrases. To keep the grammar restrictive, such sequences are penalized by the statistical model. Accurate hand-crafted grammars have been applied to speech recognition before, e.g. Kiefer et al. (2000) and van Noord et al. (1999). However, they primarily served as a basis for a speech understanding component and were applied to narrowdomain tasks such as appointment scheduling or public transport information. We are mainly concerned with speech recognition performance on broad-domain recognition tasks. Beutler et al. (2005) pursued a similar approach. Proceedings of ACL-08: HLT, pages 106–113, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics However, their grammar-based language model did not make use of a probabilistic component, and it was applied to a rather simple recognition task (dictation texts for pupils read and recorded under good acoustic conditions, no out-of-vocabulary words). Besides proposing an improved language model, this paper presents experimental results for a much more difficult and realistic task and compares them to the performance of a state-of-the-art bas</context>
</contexts>
<marker>Beutler, Kaufmann, Pfister, 2005</marker>
<rawString>R. Beutler, T. Kaufmann, and B. Pfister. 2005. Integrating a non-probabilistic grammar into large vocabulary continuous speech recognition. In Proceedings of the IEEEASRU 2005 Workshop, pages 104–109, San Juan (Puerto Rico).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="5721" citStr="Brants et al (2002)" startWordPosition="882" endWordPosition="885">max TEparses(W) To determine Pgram(W) is an expensive operation as it involves parsing. For this reason, we pursue an N-best rescoring approach. We first produce the N best hypotheses according to the criterion in equation (1). From these hypotheses we then choose the final recognition result according to equation (2). 2.2 The Probability of a Parse Tree The parse trees produced by our parser are binarybranching and rather deep. In order to compute the probability of a parse tree, it is transformed to a flat dependency tree similar to the syntax graph representation used in the TIGER treebank Brants et al (2002). An inner node of such a dependency tree represents a constituent or phrase. Typically, it directly connects to a leaf node representing the most important word of the phrase, the head child. The other children represent phrases or words directly depending on the head child. To give an example, the immediate children of a sentence node are the finite verb (the head child), the adverbials, the subject and the all other (verbal and non-verbal) complements. This flat structure has the advantage that the information which is most relevant for the head child is represented within the locality of a</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>132--139</pages>
<location>San Francisco, USA.</location>
<contexts>
<context position="1319" citStr="Charniak (2000)" startWordPosition="186" endWordPosition="187">asible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate compared to a state-of-the-art baseline system. 1 Introduction It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003), Charniak (2000) and Ratnaparkhi (1999). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000). The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history. An 106 important reason for the success of these models is the fact that they are lexicalized: the probability distributions are als</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the NAACL, pages 132–139, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>332</pages>
<contexts>
<context position="1420" citStr="Chelba and Jelinek (2000)" startWordPosition="197" endWordPosition="200">to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate compared to a state-of-the-art baseline system. 1 Introduction It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003), Charniak (2000) and Ratnaparkhi (1999). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000). The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history. An 106 important reason for the success of these models is the fact that they are lexicalized: the probability distributions are also conditioned on the actual words occuring in the utterance, and not only on their parts of speech. M</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>C. Chelba and F. Jelinek. 2000. Structured language modeling. Computer Speech &amp; Language, 14(4):283– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="17416" citStr="Church and Hanks (1990)" startWordPosition="2760" endWordPosition="2763">en parsing a hypothesis, the parser chart contains only those multiword lexemes for which all parts are present in the hypothesis. Other multi-word lexemes are fixed word clusters of various types. For instance, some prepositional phrases appearing in support verb constructions lack an otherwise mandatory determiner, e.g. “unter Beschuss” (under fire). Many multi-word lexemes are adverbials, e.g. “nach wie vor” (still), “auf die Dauer” (in the long run). To extract such word clusters we used suffix arrays proposed in Yamamoto and Church (2001) and the pointwise mutual information measure, see Church and Hanks (1990). Again, it is feasible to consider only those clusters appearing in some recognizer hypothesis. The list of candidate clusters was reduced using different filter heuristics and finally checked manually. For our task, split compounds are to be considered as multi-word lexemes as well. As our grammar only models noun-noun compounds, other compounds such as “unionsgef¨uhrt” (led by the union) have to be entered into the lexicon. We applied the decompounding algorithm proposed in AddaDecker (2003) to our corpus to extract such compounds. The resulting candidate list was again filtered manually. W</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1302" citStr="Collins (2003)" startWordPosition="184" endWordPosition="185">r approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate compared to a state-of-the-art baseline system. 1 Introduction It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003), Charniak (2000) and Ratnaparkhi (1999). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000). The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history. An 106 important reason for the success of these models is the fact that they are lexicalized: the probability dist</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crysmann</author>
</authors>
<title>On the efficient implementation of German verb placement in HPSG.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="13197" citStr="Crysmann (2003)" startWordPosition="2079" endWordPosition="2080">s of predicates or relational constraints. At parse time, predicates are backed by program code that can perform arbitrary computations to check or specify feature structures. We have implemented an efficient Java parser for our variant of the HPSG formalism. The parser supports ambiguity packing, which is a technique for merging constituents with different derivational histories but identical syntactic properties. This is essential for parsing long and ambiguous sentences. Our grammar incorporates many ideas from existing linguistic work, e.g. M¨uller (2007), M¨uller (1999), Crysmann (2005), Crysmann (2003). In addition, we have modeled a few constructions which occur frequently but are often neglected in formal syntactic theories. Among them are prenominal and postnominal genitives, expressions of quantity and expressions of date and time. Further, we have implemented dedicated subgrammars for analyzing written numbers, compounds and acronyms that are written as separate words. To reduce ambiguity, only noun-noun compounds are covered by the grammar. Noun-noun compounds are by far the most productive compound type. The grammar consists of 17 rules for general linguistic phenomena (e.g. subcateg</context>
</contexts>
<marker>Crysmann, 2003</marker>
<rawString>B. Crysmann. 2003. On the efficient implementation of German verb placement in HPSG. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crysmann</author>
</authors>
<title>Relative clause extraposition in German: An efficient and portable implementation.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="13180" citStr="Crysmann (2005)" startWordPosition="2077" endWordPosition="2078">nstraints by means of predicates or relational constraints. At parse time, predicates are backed by program code that can perform arbitrary computations to check or specify feature structures. We have implemented an efficient Java parser for our variant of the HPSG formalism. The parser supports ambiguity packing, which is a technique for merging constituents with different derivational histories but identical syntactic properties. This is essential for parsing long and ambiguous sentences. Our grammar incorporates many ideas from existing linguistic work, e.g. M¨uller (2007), M¨uller (1999), Crysmann (2005), Crysmann (2003). In addition, we have modeled a few constructions which occur frequently but are often neglected in formal syntactic theories. Among them are prenominal and postnominal genitives, expressions of quantity and expressions of date and time. Further, we have implemented dedicated subgrammars for analyzing written numbers, compounds and acronyms that are written as separate words. To reduce ambiguity, only noun-noun compounds are covered by the grammar. Noun-noun compounds are by far the most productive compound type. The grammar consists of 17 rules for general linguistic phenome</context>
</contexts>
<marker>Crysmann, 2005</marker>
<rawString>B. Crysmann. 2005. Relative clause extraposition in German: An efficient and portable implementation. Research on Language and Computation, 3(1):61–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duden</author>
</authors>
<title>Das große W¨orterbuch der deutschen Sprache in zehn B¨anden. Dudenverlag, dritte Auflage.</title>
<date>1999</date>
<contexts>
<context position="14765" citStr="Duden (1999)" startWordPosition="2316" endWordPosition="2317">ules. The grammar allows the derivation of “intermediate products” which cannot be regarded as complete phrases. We consider complete phrases to be sentences, subordinate clauses, relative and interrogative clauses, noun phrases, prepositional phrases, adjective phrases and expressions of date and time. 3.3 Lexicon The lexicon was created manually based on a list of more than 5000 words appearing in the N-best lists of our experiment. As the domain of our recognition task is very broad, we attempted to include any possible reading of a given word. Our main source of dictionary information was Duden (1999). Each word was annotated with precise morphological and syntactic information. For example, the roughly 2700 verbs were annotated with over 7000 valency frames. We distinguish 86 basic valency frames, for most of which the complement types can be further specified. A major difficulty was the acquisition of multiword lexemes. Slightly deviating from the common notion, we use the following definition: A syntactic unit consisting of two or more words is a multiword lexeme, if the grammar cannot derive it from its parts. English examples are idioms like “by and large” and phrasal verbs such as “t</context>
</contexts>
<marker>Duden, 1999</marker>
<rawString>Duden. 1999. – Das große W¨orterbuch der deutschen Sprache in zehn B¨anden. Dudenverlag, dritte Auflage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>S Cox</author>
</authors>
<title>Some statistical issues in the comparison of speech recognition algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings of the ICASSP,</booktitle>
<pages>532--535</pages>
<contexts>
<context position="24407" citStr="Gillick and Cox, 1989" startWordPosition="3894" endWordPosition="3897"> the intervals [0, 20] and [0, 0.25], respectively. The word error rate was evaluated for each possible pair of parameter values. The evaluation scheme was taken from McTait and Adda-Decker (2003). It ignores capitalization, and written numbers, compounds and acronyms need not be written as single words. 4.4 Results As shown in Table 1, the grammar-based language model reduced the word error rate by 9.2% relative over the baseline system. This improvement is statistically significant on a level of &lt; 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemar’s test (Gillick and Cox, 1989). If the parameters are optimized on all 447 sentences (i.e. on the test data), the word error rate is reduced by 10.7% relative. For comparison, we redefined the probabilistic model as P(T) _ (1 − q)qk−1, where k is the number of phrases attached to the root node. This reduced model only considers the grammaticality of a phrase, completely ignoring the probability of its internal structure. It achieved a relative word error reduction of 5.9%, which is statistically significant on a level of &lt; 0.1% for both tests. The improvement of the full model compared to the reduced model is weakly signif</context>
</contexts>
<marker>Gillick, Cox, 1989</marker>
<rawString>L. Gillick and S. Cox. 1989. Some statistical issues in the comparison of speech recognition algorithms. In Proceedings of the ICASSP, pages 532–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kaufmann</author>
<author>B Pfister</author>
</authors>
<title>Applying licenser rules to a grammar with continuous constituents. In</title>
<date>2007</date>
<contexts>
<context position="12171" citStr="Kaufmann and Pfister (2007)" startWordPosition="1931" endWordPosition="1934">used the Head-driven Phrase Structure Grammar (HPSG, see Pollard and Sag (1994)) formalism to develop a precise large-coverage grammar for German. HPSG is an unrestricted grammar (Chomsky type 0) which is based on a context-free skeleton and the unification of complex feature structures. There are several variants of HPSG which mainly differ in the formal tools they provide for stating lin108 guistic constraints. Our particular variant requires that constituents (phrases) be continuous, but it provides a mechanism for dealing with discontinuities as present e.g. in the German main clause, see Kaufmann and Pfister (2007). HPSG typically distinguishes between immediate dominance schemata (rough equivalents of phrase structure rules, but making no assumptions about constituent order) and linear precedence rules (constraints on constituent order). We do not make this distinction but rather let immediate dominance schemata specify constituent order. Further, the formalism allows to express complex linguistic constraints by means of predicates or relational constraints. At parse time, predicates are backed by program code that can perform arbitrary computations to check or specify feature structures. We have imple</context>
</contexts>
<marker>Kaufmann, Pfister, 2007</marker>
<rawString>T. Kaufmann and B. Pfister. 2007. Applying licenser rules to a grammar with continuous constituents. In</rawString>
</citation>
<citation valid="false">
<booktitle>The Proceedings of the 14th International Conference on Head-Driven Phrase Structure Grammar,</booktitle>
<pages>150--162</pages>
<editor>Stefan M¨uller, editor,</editor>
<publisher>CSLI Publications.</publisher>
<location>Stanford, USA.</location>
<marker></marker>
<rawString>Stefan M¨uller, editor, The Proceedings of the 14th International Conference on Head-Driven Phrase Structure Grammar, pages 150–162, Stanford, USA. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H-U Krieger</author>
<author>M-J Nederhof</author>
</authors>
<title>Efficient and robust parsing of word hypotheses graphs.</title>
<date>2000</date>
<contexts>
<context position="3079" citStr="Kiefer et al. (2000)" startWordPosition="463" endWordPosition="466">istinguish between grammatical and ungrammatical phrases. To this end, we have developed a precise, linguistically motivated grammar. To distinguish between common and uncommon phrases, we use a statistical model that estimates the probability of a phrase based on the syntactic dependencies established by the parser. We achieve some degree of robustness by letting the grammar accept arbitrary sequences of words and phrases. To keep the grammar restrictive, such sequences are penalized by the statistical model. Accurate hand-crafted grammars have been applied to speech recognition before, e.g. Kiefer et al. (2000) and van Noord et al. (1999). However, they primarily served as a basis for a speech understanding component and were applied to narrowdomain tasks such as appointment scheduling or public transport information. We are mainly concerned with speech recognition performance on broad-domain recognition tasks. Beutler et al. (2005) pursued a similar approach. Proceedings of ACL-08: HLT, pages 106–113, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics However, their grammar-based language model did not make use of a probabilistic component, and it was applied to a rath</context>
</contexts>
<marker>Kiefer, Krieger, Nederhof, 2000</marker>
<rawString>B. Kiefer, H.-U. Krieger, and M.-J. Nederhof. 2000. Efficient and robust parsing of word hypotheses graphs.</rawString>
</citation>
<citation valid="false">
<title>Foundations of Speech-to-Speech Translation,</title>
<pages>280--295</pages>
<editor>In Wolfgang Wahlster, editor, Verbmobil.</editor>
<publisher>Springer,</publisher>
<location>Berlin, Germany,</location>
<note>artificial intelligence edition.</note>
<marker></marker>
<rawString>In Wolfgang Wahlster, editor, Verbmobil. Foundations of Speech-to-Speech Translation, pages 280–295. Springer, Berlin, Germany, artificial intelligence edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McTait</author>
<author>M Adda-Decker</author>
</authors>
<title>The 300k LIMSI German broadcast news transcription system.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="19910" citStr="McTait and Adda-Decker, 2003" startWordPosition="3161" endWordPosition="3164">sed language model. To this end, we used a baseline speech recognition system which provided the N best hypotheses of an utterance along with their respective scores. The grammarbased language model was then applied to the N best hypotheses as described in Section 2.1, yielding a new best hypothesis. For a given test set we could then compare the word error rate of the baseline system with that of the extended system employing the grammar-based language model. 4.2 Data and Preprocessing Our experiments are based on word lattice output from the LIMSI German broadcast news transcription system (McTait and Adda-Decker, 2003), which employs 4-gram backoff language models. From the experiment reported in McTait and AddaDecker (2003), we used the first three broadcast news shows1 which corresponds to a signal length of roughly 50 minutes. Rather than applying our model to the original broadcast-news transcription task, we used the above data to create an artificial recognition task with manageable complexity. Our primary aim was to design a task which allows us to investigate the properties of our grammar-based approach and to compare its performance with that of a competitive baseline system. As a first simplificat</context>
<context position="23981" citStr="McTait and Adda-Decker (2003)" startWordPosition="3825" endWordPosition="3828">rent values of N will be discussed in section 4.4). The remaining parameters µ and q were trained using the leave-one-out crossvalidation method: each of the 447 utterances served as the single test item once, whereas the remaining 446 utterances were used for training. As the error landscape is complex and discrete, we could not use gradient-based optimization methods. Instead, we chose µ and q from 500 equidistant points within the intervals [0, 20] and [0, 0.25], respectively. The word error rate was evaluated for each possible pair of parameter values. The evaluation scheme was taken from McTait and Adda-Decker (2003). It ignores capitalization, and written numbers, compounds and acronyms need not be written as single words. 4.4 Results As shown in Table 1, the grammar-based language model reduced the word error rate by 9.2% relative over the baseline system. This improvement is statistically significant on a level of &lt; 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemar’s test (Gillick and Cox, 1989). If the parameters are optimized on all 447 sentences (i.e. on the test data), the word error rate is reduced by 10.7% relative. For comparison, we redefined the probabilist</context>
</contexts>
<marker>McTait, Adda-Decker, 2003</marker>
<rawString>K. McTait and M. Adda-Decker. 2003. The 300k LIMSI German broadcast news transcription system. In Proceedings of Eurospeech, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨uller</author>
</authors>
<title>Deutsche Syntax deklarativ. HeadDriven Phrase Structure Grammar f¨ur das Deutsche.</title>
<date>1999</date>
<journal>Number</journal>
<booktitle>in Linguistische Arbeiten.</booktitle>
<volume>394</volume>
<publisher>Max Niemeyer Verlag, T¨ubingen.</publisher>
<marker>M¨uller, 1999</marker>
<rawString>S. M¨uller. 1999. Deutsche Syntax deklarativ. HeadDriven Phrase Structure Grammar f¨ur das Deutsche. Number 394 in Linguistische Arbeiten. Max Niemeyer Verlag, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨uller</author>
</authors>
<title>Head-Driven Phrase Structure Grammar: Eine Einf¨uhrung.</title>
<date>2007</date>
<journal>Stauffenburg Einf¨uhrungen, Nr.</journal>
<volume>17</volume>
<publisher>Stauffenburg Verlag, T¨ubingen.</publisher>
<marker>M¨uller, 2007</marker>
<rawString>S. M¨uller. 2007. Head-Driven Phrase Structure Grammar: Eine Einf¨uhrung. Stauffenburg Einf¨uhrungen, Nr. 17. Stauffenburg Verlag, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Van Noord</author>
<author>G Bouma</author>
<author>R Koeling</author>
<author>M-J Nederhof</author>
</authors>
<title>Robust grammatical analysis for spoken dialogue systems.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Van Noord, Bouma, Koeling, Nederhof, 1999</marker>
<rawString>G. Van Noord, G. Bouma, R. Koeling, and M.-J. Nederhof. 1999. Robust grammatical analysis for spoken dialogue systems. Natural Language Engineering, 5(1):45–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="11623" citStr="Pollard and Sag (1994)" startWordPosition="1846" endWordPosition="1849">he linguistic resources presented in this Section are partly influenced by the form of the recognizer output. In particular, the speech recognizer does not always transcribe numbers, compounds and acronyms as single words. For instance, the word “einundzwanzig” (twenty-one) is transcribed as “ein und zwanzig”, “Kriegspl¨ane” (war plans) as “Kriegs Pl¨ane” and ”BMW” as “B. M. W.” These transcription variants are considered to be correct by our evaluation scheme. Therefore, the grammar should accept them as well. 3.2 Grammar and Parser We used the Head-driven Phrase Structure Grammar (HPSG, see Pollard and Sag (1994)) formalism to develop a precise large-coverage grammar for German. HPSG is an unrestricted grammar (Chomsky type 0) which is based on a context-free skeleton and the unification of complex feature structures. There are several variants of HPSG which mainly differ in the formal tools they provide for stating lin108 guistic constraints. Our particular variant requires that constituents (phrases) be continuous, but it provides a mechanism for dealing with discontinuities as present e.g. in the German main clause, see Kaufmann and Pfister (2007). HPSG typically distinguishes between immediate dom</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. J. Pollard and I. A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1342" citStr="Ratnaparkhi (1999)" startWordPosition="189" endWordPosition="190">l for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate compared to a state-of-the-art baseline system. 1 Introduction It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003), Charniak (2000) and Ratnaparkhi (1999). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000). The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history. An 106 important reason for the success of these models is the fact that they are lexicalized: the probability distributions are also conditioned on the ac</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1437" citStr="Roark (2001)" startWordPosition="202" endWordPosition="203">st-news transcription task. We report a significant reduction in word error rate compared to a state-of-the-art baseline system. 1 Introduction It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003), Charniak (2000) and Ratnaparkhi (1999). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000). The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history. An 106 important reason for the success of these models is the fact that they are lexicalized: the probability distributions are also conditioned on the actual words occuring in the utterance, and not only on their parts of speech. Most statistical p</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yamamoto</author>
<author>K W Church</author>
</authors>
<title>Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="17342" citStr="Yamamoto and Church (2001)" startWordPosition="2747" endWordPosition="2751">sent. Note that this procedure does not amount to optimizing on test data: when parsing a hypothesis, the parser chart contains only those multiword lexemes for which all parts are present in the hypothesis. Other multi-word lexemes are fixed word clusters of various types. For instance, some prepositional phrases appearing in support verb constructions lack an otherwise mandatory determiner, e.g. “unter Beschuss” (under fire). Many multi-word lexemes are adverbials, e.g. “nach wie vor” (still), “auf die Dauer” (in the long run). To extract such word clusters we used suffix arrays proposed in Yamamoto and Church (2001) and the pointwise mutual information measure, see Church and Hanks (1990). Again, it is feasible to consider only those clusters appearing in some recognizer hypothesis. The list of candidate clusters was reduced using different filter heuristics and finally checked manually. For our task, split compounds are to be considered as multi-word lexemes as well. As our grammar only models noun-noun compounds, other compounds such as “unionsgef¨uhrt” (led by the union) have to be entered into the lexicon. We applied the decompounding algorithm proposed in AddaDecker (2003) to our corpus to extract s</context>
</contexts>
<marker>Yamamoto, Church, 2001</marker>
<rawString>M. Yamamoto and K. W. Church. 2001. Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus. Computational Linguistics, 27(1):1–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>