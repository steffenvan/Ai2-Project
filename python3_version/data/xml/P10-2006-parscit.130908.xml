<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018787">
<title confidence="0.99876">
Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
</title>
<author confidence="0.94288">
Graeme Blackwood, Adri`a de Gispert, William Byrne
</author>
<affiliation confidence="0.9509785">
Machine Intelligence Laboratory
Cambridge University Engineering Department
</affiliation>
<address confidence="0.861029">
Trumpington Street, CB2 1PZ, U.K.
</address>
<email confidence="0.99804">
{gwb24|ad465|wjb31}@cam.ac.uk
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978692307692">
This paper presents an efficient imple-
mentation of linearised lattice minimum
Bayes-risk decoding using weighted finite
state transducers. We introduce transduc-
ers to efficiently count lattice paths con-
taining n-grams and use these to gather
the required statistics. We show that these
procedures can be implemented exactly
through simple transformations of word
sequences to sequences of n-grams. This
yields a novel implementation of lattice
minimum Bayes-risk decoding which is
fast and exact even for very large lattices.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928666666667">
This paper focuses on an exact implementation
of the linearised form of lattice minimum Bayes-
risk (LMBR) decoding using general purpose
weighted finite state transducer (WFST) opera-
tions1. The LMBR decision rule in Tromble et al.
(2008) has the form
</bodyText>
<equation confidence="0.984910333333333">
E = argmax{ 00  |E′  |+
E′EE l uEN
(1)
</equation>
<bodyText confidence="0.998979166666667">
where E is a lattice of translation hypotheses, N
is the set of all n-grams in the lattice (typically,
n = 1... 4), and the parameters θ are constants
estimated on held-out data. The quantity p(u|E)
we refer to as the path posterior probability of the
n-gram u. This particular posterior is defined as
</bodyText>
<equation confidence="0.9143965">
p(u|E) = p(Eu|E) = X P(E|F), (2)
EEEu
</equation>
<bodyText confidence="0.99875">
where Eu = {E ∈ E : #u(E) &gt; 0} is the sub-
set of lattice paths containing the n-gram u at least
</bodyText>
<footnote confidence="0.925432">
1We omit an introduction to WFSTs for space reasons.
See Mohri et al. (2008) for details of the general purpose
WFST operations used in this paper.
</footnote>
<bodyText confidence="0.995189777777778">
once. It is the efficient computation of these path
posterior n-gram probabilities that is the primary
focus of this paper. We will show how general
purpose WFST algorithms can be employed to ef-
ficiently compute p(u|E) for all u ∈ N.
Tromble et al. (2008) use Equation (1) as an
approximation to the general form of statistical
machine translation MBR decoder (Kumar and
Byrne, 2004):
</bodyText>
<equation confidence="0.962604333333333">
X
E� = argmin L(E, E′)P(E|F) (3)
E′EE EEE
</equation>
<bodyText confidence="0.999952333333333">
The approximation replaces the sum over all paths
in the lattice by a sum over lattice n-grams. Even
though a lattice may have many n-grams, it is
possible to extract and enumerate them exactly
whereas this is often impossible for individual
paths. Therefore, while the Tromble et al. (2008)
linearisation of the gain function in the decision
rule is an approximation, Equation (1) can be com-
puted exactly even over very large lattices. The
challenge is to do so efficiently.
If the quantity p(u|E) had the form of a condi-
tional expected count
</bodyText>
<equation confidence="0.9819345">
c(u|E) = X #u(E)P(E|F), (4)
EEE
</equation>
<bodyText confidence="0.999433307692308">
it could be computed efficiently using counting
transducers (Allauzen et al., 2003). The statis-
tic c(u|E) counts the number of times an n-gram
occurs on each path, accumulating the weighted
count over all paths. By contrast, what is needed
by the approximation in Equation (1) is to iden-
tify all paths containing an n-gram and accumulate
their probabilities. The accumulation of probabil-
ities at the path level, rather than the n-gram level,
makes the exact computation of p(u|E) hard.
Tromble et al. (2008) approach this problem by
building a separate word sequence acceptor for
each n-gram in N and intersecting this acceptor
</bodyText>
<equation confidence="0.799867">
�θu#u(E′)p(u|E)
</equation>
<page confidence="0.961424">
27
</page>
<note confidence="0.4972175">
Proceedings of the ACL 2010 Conference Short Papers, pages 27–32,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999016675">
with the lattice to discard all paths that do not con-
tain the n-gram; they then sum the probabilities of
all paths in the filtered lattice. We refer to this as
the sequential method, since p(u|E) is calculated
separately for each u in sequence.
Allauzen et al. (2010) introduce a transducer
for simultaneous calculation of p(u|E) for all un-
igrams u ∈ N1 in a lattice. This transducer is
effective for finding path posterior probabilities of
unigrams because there are relatively few unique
unigrams in the lattice. As we will show, however,
it is less efficient for higher-order n-grams.
Allauzen et al. (2010) use exact statistics for
the unigram path posterior probabilities in Equa-
tion (1), but use the conditional expected counts
of Equation (4) for higher-order n-grams. Their
hybrid MBR decoder has the form
where k determines the range of n-gram orders
at which the path posterior probabilities p(u|E)
of Equation (2) and conditional expected counts
c(u|E) of Equation (4) are used to compute the
expected gain. For k &lt; 4, Equation (5) is thus
an approximation to the approximation. In many
cases it will be perfectly fine, depending on how
closely p(u|E) and c(u|E) agree for higher-order
n-grams. Experimentally, Allauzen et al. (2010)
find this approximation works well at k = 1 for
MBR decoding of statistical machine translation
lattices. However, there may be scenarios in which
p(u|E) and c(u|E) differ so that Equation (5) is no
longer useful in place of the original Tromble et
al. (2008) approximation.
In the following sections, we present an efficient
method for simultaneous calculation of p(u|E) for
n-grams of a fixed order. While other fast MBR
approximations are possible (Kumar et al., 2009),
we show how the exact path posterior probabilities
can be calculated and applied in the implementa-
tion of Equation (1) for efficient MBR decoding
over lattices.
</bodyText>
<sectionHeader confidence="0.94854" genericHeader="method">
2 N-gram Mapping Transducer
</sectionHeader>
<bodyText confidence="0.999955705882353">
We make use of a trick to count higher-order n-
grams. We build transducer fin to map word se-
quences to n-gram sequences of order n. fin has a
similar form to the WFST implementation of an n-
gram language model (Allauzen et al., 2003). fin
includes for each n-gram u = wn1 arcs of the form:
The n-gram lattice of order n is called En and is
found by composing E ◦fin, projecting on the out-
put, removing ǫ-arcs, determinizing, and minimis-
ing. The construction of En is fast even for large
lattices and is memory efficient. En itself may
have more states than E due to the association of
distinct n-gram histories with states. However, the
counting transducer for unigrams is simpler than
the corresponding counting transducer for higher-
order n-grams. As a result, counting unigrams in
En is easier than counting n-grams in E.
</bodyText>
<sectionHeader confidence="0.991362" genericHeader="method">
3 Efficient Path Counting
</sectionHeader>
<bodyText confidence="0.99998025">
Associated with each En we have a transducer XFn
which can be used to calculate the path posterior
probabilities p(u|E) for all u ∈ Nn. In Figures
1 and 2 we give two possible forms2 of XFn that
can be used to compute path posterior probabilities
over n-grams u1,2 ∈ Nn for some n. No modifica-
tion to the ρ-arc matching mechanism is required
even in counting higher-order n-grams since all n-
grams are represented as individual symbols after
application of the mapping transducer fin.
Transducer Xpn is used by Allauzen et al. (2010)
to compute the exact unigram contribution to the
conditional expected gain in Equation (5). For ex-
ample, in counting paths that contain u1, XFn re-
tains the first occurrence of u1 and maps every
other symbol to ǫ. This ensures that in any path
containing a given u, only the first u is counted,
avoiding multiple counting of paths.
We introduce an alternative path counting trans-
ducer XpR n that effectively deletes all symbols ex-
cept the last occurrence of u on any path by en-
suring that any paths in composition which count
earlier instances of u do not end in a final state.
Multiple counting is avoided by counting only the
last occurrence of each symbol u on a path.
We note that initial ǫ:ǫ arcs in n effectively
create |Nn |copies of En in composition while
searching for the first occurrence of each u. Com-
</bodyText>
<footnote confidence="0.957497333333333">
2The special composition symbol σ matches any arc; p
matches any arc other than those with an explicit transition.
See the OpenFst documentation: http://openfst.org
</footnote>
<figure confidence="0.9572959">
E = argmax{ 00  |E′ |
E′E£ l
+
uEN:1&lt;JuJ&lt;k
� 1 0u#u(E′)c(u|E) , (5)
+
uEN:k&lt;JuJ&lt;4
0u#u(E′)p(u|E)
-1 wn:u n
w1 w2
</figure>
<page confidence="0.899872">
28
</page>
<figureCaption confidence="0.99269675">
Figure 1: Path counting transducer XpLn matching
first (left-most) occurrence of each u E Nn.
Figure 2: Path counting transducer XpRnmatching
last (right-most) occurrence of each u E Nn.
</figureCaption>
<bodyText confidence="0.999706">
posing with XpR n creates a single copy of £n while
searching for the last occurrence of u; we find this
to be much more efficient for large Nn.
Path posterior probabilities are calculated over
each £n by composing with XFn in the log semir-
ing, projecting on the output, removing ǫ-arcs, de-
terminizing, minimising, and pushing weights to
the initial state (Allauzen et al., 2010). Using ei-
ther �L n or Rn ,the resulting counts acceptor is Xn.
It has a compact form with one arc from the start
state for each ui E Nn:
</bodyText>
<subsectionHeader confidence="0.998644">
3.1 Efficient Path Posterior Calculation
</subsectionHeader>
<bodyText confidence="0.999982027027027">
Although Xn has a convenient and elegant form,
it can be difficult to build for large Nn because
the composition £n o XFn results in millions of
states and arcs. The log semiring ǫ-removal and
determinization required to sum the probabilities
of paths labelled with each u can be slow.
However, if we use the proposed XpRn, then each
path in £n o XpR n has only one non-ǫ output la-
bel u and all paths leading to a given final state
share the same u. A modified forward algorithm
can be used to calculate p(u|£) without the costly
ǫ-removal and determinization. The modification
simply requires keeping track of which symbol
u is encountered along each path to a final state.
More than one final state may gather probabilities
for the same u; to compute p(u|£) these proba-
bilities are added. The forward algorithm requires
that £noj1R n be topologically sorted; although sort-
ing can be slow, it is still quicker than log semiring
ǫ-removal and determinization.
The statistics gathered by the forward algo-
rithm could also be gathered under the expectation
semiring (Eisner, 2002) with suitably defined fea-
tures. We take the view that the full complexity of
that approach is not needed here, since only one
symbol is introduced per path and per exit state.
Unlike £n o XpRn, the composition £n o q&apos;Ln does
not segregate paths by u such that there is a di-
rect association between final states and symbols.
The forward algorithm does not readily yield the
per-symbol probabilities, although an arc weight
vector indexed by symbols could be used to cor-
rectly aggregate the required statistics (Riley et al.,
2009). For large Nn this would be memory in-
tensive. The association between final states and
symbols could also be found by label pushing, but
we find this slow for large £n o XFn.
</bodyText>
<sectionHeader confidence="0.989582" genericHeader="method">
4 Efficient Decoder Implementation
</sectionHeader>
<bodyText confidence="0.997734666666667">
In contrast to Equation (5), we use the exact values
of p(u|£) for all u E Nn at orders n = 1... 4 to
compute
</bodyText>
<equation confidence="0.945714333333333">
4 l
E = argmin{ 00|E′ |+ �gn(E, E′) }, (6)
E′∈E l J
</equation>
<bodyText confidence="0.999968916666667">
where gn(E, E′) = Eu∈Arn 0u#u(E′)p(u|£) us-
ing the exact path posterior probabilities at each
order. We make acceptors Qn such that £ o Qn
assigns order n partial gain gn(E, E′) to all paths
E E £. Qn is derived from 4bn directly by assign-
ing arc weight 0uxp(u|£) to arcs with output label
u and then projecting on the input labels. For each
n-gram u = wn1 in Nn arcs of Qn have the form:
To apply 00 we make a copy of £, called £0,
with fixed weight 00 on all arcs. The decoder is
formed as the composition £0 o Q1 o Q2 o Q3 o Q4
and E� is extracted as the maximum cost string.
</bodyText>
<sectionHeader confidence="0.969638" genericHeader="method">
5 Lattice Generation for LMBR
</sectionHeader>
<bodyText confidence="0.9496655">
Lattice MBR decoding performance and effi-
ciency is evaluated in the context of the NIST
</bodyText>
<figure confidence="0.994649892857143">
ρ:ǫ
ǫ:ǫ
1
u1:u1
σ:ǫ
0
ǫ:ǫ
ρ:ǫ
u2:u2
3
2
ρ:ǫ
σ:ǫ
u1:u1
1
u1:ǫ
2
0
u2:u2
ρ:ǫ
3
u2:ǫ
4
ui/- log p(ui|E)
0 i
n=1
wn-1 wn/θu � p(u|E) wn
1 2
</figure>
<page confidence="0.994877">
29
</page>
<table confidence="0.999690285714286">
mt0205tune mt0205test mt08nw mt08ng
ML 54.2 53.8 51.4 36.3
0 52.6 52.3 49.8 34.5
k 1 54.8 54.4 52.2 36.6
2 54.9 54.5 52.4 36.8
3 54.9 54.5 52.4 36.8
LMBR 55.0 54.6 52.4 36.8
</table>
<tableCaption confidence="0.929297">
Table 1: BLEU scores for Arabic→English maximum likelihood translation (ML), MBR decoding using
the hybrid decision rule of Equation (5) at 0 ≤ k ≤ 3, and regular linearised lattice MBR (LMBR).
</tableCaption>
<table confidence="0.999645">
mt0205tune mt0205test mt08nw mt08ng
Posteriors sequential 3160 3306 2090 3791
�L 6880 7387 4201 8796
n 1746 1789 1182 2787
�R
n
Decoding sequential 4340 4530 2225 4104
XFn 284 319 118 197
Total sequential 7711 8065 4437 8085
�L 7458 8075 4495 9199
n 2321 2348 1468 3149
�R
n
</table>
<tableCaption confidence="0.939033">
Table 2: Time in seconds required for path posterior n-gram probability calculation and LMBR decoding
using sequential method and left-most (XpLn) or right-most (XpRn) counting transducer implementations.
</tableCaption>
<bodyText confidence="0.981767384615384">
Arabic→English machine translation task3. The
development set mt0205tune is formed from the
odd numbered sentences of the NIST MT02–
MT05 testsets; the even numbered sentences form
the validation set mt0205test. Performance on
NIST MT08 newswire (mt08nw) and newsgroup
(mt08ng) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al., 2009), a hierarchical phrase-based
decoder. Word alignments are generated using
MTTK (Deng and Byrne, 2008) over 150M words
of parallel text for the constrained NIST MT08
Arabic→English track. In decoding, a Shallow-
1 grammar with a single level of rule nesting is
used and no pruning is performed in generating
first-pass lattices (Iglesias et al., 2009).
The first-pass language model is a modified
Kneser-Ney (Kneser and Ney, 1995) 4-gram esti-
mated over the English parallel text and an 881M
word subset of the GigaWord Third Edition (Graff
et al., 2007). Prior to LMBR, the lattices are
rescored with large stupid-backoff 5-gram lan-
guage models (Brants et al., 2007) estimated over
more than 6 billion words of English text.
The n-gram factors 00, ... , 04 are set according
to Tromble et al. (2008) using unigram precision
</bodyText>
<footnote confidence="0.65153">
3http://www.itl.nist.gov/iad/mig/tests/mt
</footnote>
<bodyText confidence="0.998736333333333">
p = 0.85 and average recall ratio r = 0.74. Our
translation decoder and MBR procedures are im-
plemented using OpenFst (Allauzen et al., 2007).
</bodyText>
<sectionHeader confidence="0.822121" genericHeader="method">
6 LMBR Speed and Performance
</sectionHeader>
<bodyText confidence="0.9881515">
Lattice MBR decoding performance is shown in
Table 1. Compared to the maximum likelihood
translation hypotheses (row ML), LMBR gives
gains of +0.8 to +1.0 BLEU for newswire data and
+0.5 BLEU for newsgroup data (row LMBR).
The other rows of Table 1 show the performance
of LMBR decoding using the hybrid decision rule
of Equation (5) for 0 ≤ k ≤ 3. When the condi-
tional expected counts c(u|E) are used at all orders
(i.e. k = 0), the hybrid decoder BLEU scores are
considerably lower than even the ML scores. This
poor performance is because there are many un-
igrams u for which c(u|E) is much greater than
p(u|E). The consensus translation maximising the
conditional expected gain is then dominated by
unigram matches, significantly degrading LMBR
decoding performance. Table 1 shows that for
these lattices the hybrid decision rule is an ac-
curate approximation to Equation (1) only when
k ≥ 2 and the exact contribution to the gain func-
tion is computed using the path posterior probabil-
ities at orders n = 1 and n = 2.
</bodyText>
<page confidence="0.995833">
30
</page>
<bodyText confidence="0.999855486486486">
We now analyse the efficiency of lattice MBR
decoding using the exact path posterior probabil-
ities of Equation (2) at all orders. We note that
the sequential method and both simultaneous im-
plementations using path counting transducers Xpn
and XpR n yield the same hypotheses (allowing for
numerical accuracy); they differ only in speed and
memory usage.
Posteriors Efficiency Computation times for
the steps in LMBR are given in Table 2. In calcu-
lating path posterior n-gram probabilities p(u|E),
we find that the use of Xpn is more than twice
as slow as the sequential method. This is due to
the difficulty of counting higher-order n-grams in
large lattices. Xpn is effective for counting uni-
grams, however, since there are far fewer of them.
Using XpR n is almost twice as fast as the sequential
method. This speed difference is due to the sim-
ple forward algorithm. We also observe that for
higher-order n, the composition En ◦ XpR n requires
less memory and produces a smaller machine than
En ◦ Xp�n. It is easier to count paths by the final
occurrence of a symbol than by the first.
Decoding Efficiency Decoding times are signif-
icantly faster using Stn than the sequential method;
average decoding time is around 0.1 seconds per
sentence. The total time required for lattice MBR
is dominated by the calculation of the path pos-
terior n-gram probabilities, and this is a func-
tion of the number of n-grams in the lattice |N |.
For each sentence in mt0205tune, Figure 3 plots
the total LMBR time for the sequential method
(marked ‘o’) and for probabilities computed using
XpR n (marked ‘+’). This compares the two tech-
niques on a sentence-by-sentence basis. As |N|
grows, the simultaneous path counting transducer
is found to be much more efficient.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999951545454546">
We have described an efficient and exact imple-
mentation of the linear approximation to LMBR
using general WFST operations. A simple trans-
ducer was used to map words to sequences of n-
grams in order to simplify the extraction of higher-
order statistics. We presented a counting trans-
ducer XpRn that extracts the statistics required for
all n-grams of order n in a single composition and
allows path posterior probabilities to be computed
efficiently using a modified forward procedure.
We take the view that even approximate search
</bodyText>
<figureCaption confidence="0.996781">
Figure 3: Total time in seconds versus |N|.
</figureCaption>
<bodyText confidence="0.999949461538462">
criteria should be implemented exactly where pos-
sible, so that it is clear exactly what the system is
doing. For machine translation lattices, conflat-
ing the values of p(u|E) and c(u|E) for higher-
order n-grams might not be a serious problem, but
in other scenarios – especially where symbol se-
quences are repeated multiple times on the same
path – it may be a poor approximation.
We note that since much of the time in calcula-
tion is spent dealing with ǫ-arcs that are ultimately
removed, an optimised composition algorithm that
skips over such redundant structure may lead to
further improvements in time efficiency.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99952725">
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996543538461538">
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557–564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11–23. Springer.
Cyril Allauzen, Shankar Kumar, Wolfgang Macherey,
Mehryar Mohri, and Michael Riley. 2010. Expected
</reference>
<figure confidence="0.989360714285714">
70
60
sequential
simultaneous TR
50
40
30
20
10
0
0 1000&apos;0
00020 3000 4000 5000 6000
lattice n-grams
total time (seconds)
</figure>
<page confidence="0.999678">
31
</page>
<reference confidence="0.99927590140845">
sequence similarity maximization. In Human Lan-
guage Technologies 2010: The 11th Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Los Angeles,
California, June.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 858–867.
Yonggang Deng and William Byrne. 2008. HMM
word and phrase alignment for statistical machine
translation. IEEE Transactions on Audio, Speech,
and Language Processing, 16(3):494–507.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting ofthe Association for Com-
putational Linguistics (ACL), pages 1–8, Philadel-
phia, July.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state trans-
ducers. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 433–441, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech,
and Signal Processing, pages 181–184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2004 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 169–176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163–
171, Suntec, Singapore, August. Association for
Computational Linguistics.
M. Mohri, F.C.N. Pereira, and M. Riley. 2008. Speech
recognition with weighted finite-state transducers.
Handbook on Speech Processing and Speech Com-
munication.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. OpenFst: An Open-Source, Weighted Finite-
State Transducer Library and its Applications to
Speech and Language. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9–10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620–629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
</reference>
<page confidence="0.999298">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.849692">
<title confidence="0.999352">Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</title>
<author confidence="0.9998">Graeme Blackwood</author>
<author confidence="0.9998">Adri`a de_Gispert</author>
<author confidence="0.9998">William Byrne</author>
<affiliation confidence="0.995377">Machine Intelligence Laboratory Cambridge University Engineering Department</affiliation>
<address confidence="0.867244">Trumpington Street, CB2 1PZ, U.K.</address>
<abstract confidence="0.999290428571428">This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths conand use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word to sequences of This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="2809" citStr="Allauzen et al., 2003" startWordPosition="455" endWordPosition="458">er all paths in the lattice by a sum over lattice n-grams. Even though a lattice may have many n-grams, it is possible to extract and enumerate them exactly whereas this is often impossible for individual paths. Therefore, while the Tromble et al. (2008) linearisation of the gain function in the decision rule is an approximation, Equation (1) can be computed exactly even over very large lattices. The challenge is to do so efficiently. If the quantity p(u|E) had the form of a conditional expected count c(u|E) = X #u(E)P(E|F), (4) EEE it could be computed efficiently using counting transducers (Allauzen et al., 2003). The statistic c(u|E) counts the number of times an n-gram occurs on each path, accumulating the weighted count over all paths. By contrast, what is needed by the approximation in Equation (1) is to identify all paths containing an n-gram and accumulate their probabilities. The accumulation of probabilities at the path level, rather than the n-gram level, makes the exact computation of p(u|E) hard. Tromble et al. (2008) approach this problem by building a separate word sequence acceptor for each n-gram in N and intersecting this acceptor �θu#u(E′)p(u|E) 27 Proceedings of the ACL 2010 Conferen</context>
<context position="5660" citStr="Allauzen et al., 2003" startWordPosition="924" endWordPosition="927">on. In the following sections, we present an efficient method for simultaneous calculation of p(u|E) for n-grams of a fixed order. While other fast MBR approximations are possible (Kumar et al., 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. 2 N-gram Mapping Transducer We make use of a trick to count higher-order ngrams. We build transducer fin to map word sequences to n-gram sequences of order n. fin has a similar form to the WFST implementation of an ngram language model (Allauzen et al., 2003). fin includes for each n-gram u = wn1 arcs of the form: The n-gram lattice of order n is called En and is found by composing E ◦fin, projecting on the output, removing ǫ-arcs, determinizing, and minimising. The construction of En is fast even for large lattices and is memory efficient. En itself may have more states than E due to the association of distinct n-gram histories with states. However, the counting transducer for unigrams is simpler than the corresponding counting transducer for higherorder n-grams. As a result, counting unigrams in En is easier than counting n-grams in E. 3 Efficie</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: a general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th International Conference on Implementation and Application of Automata,</booktitle>
<pages>11--23</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13687" citStr="Allauzen et al., 2007" startWordPosition="2339" endWordPosition="2342">s a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion words of English text. The n-gram factors 00, ... , 04 are set according to Tromble et al. (2008) using unigram precision 3http://www.itl.nist.gov/iad/mig/tests/mt p = 0.85 and average recall ratio r = 0.74. Our translation decoder and MBR procedures are implemented using OpenFst (Allauzen et al., 2007). 6 LMBR Speed and Performance Lattice MBR decoding performance is shown in Table 1. Compared to the maximum likelihood translation hypotheses (row ML), LMBR gives gains of +0.8 to +1.0 BLEU for newswire data and +0.5 BLEU for newsgroup data (row LMBR). The other rows of Table 1 show the performance of LMBR decoding using the hybrid decision rule of Equation (5) for 0 ≤ k ≤ 3. When the conditional expected counts c(u|E) are used at all orders (i.e. k = 0), the hybrid decoder BLEU scores are considerably lower than even the ML scores. This poor performance is because there are many unigrams u f</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: a general and efficient weighted finite-state transducer library. In Proceedings of the 9th International Conference on Implementation and Application of Automata, pages 11–23. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>Expected sequence similarity maximization.</title>
<date>2010</date>
<booktitle>In Human Language Technologies 2010: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles, California,</location>
<contexts>
<context position="3789" citStr="Allauzen et al. (2010)" startWordPosition="615" endWordPosition="618"> makes the exact computation of p(u|E) hard. Tromble et al. (2008) approach this problem by building a separate word sequence acceptor for each n-gram in N and intersecting this acceptor �θu#u(E′)p(u|E) 27 Proceedings of the ACL 2010 Conference Short Papers, pages 27–32, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics with the lattice to discard all paths that do not contain the n-gram; they then sum the probabilities of all paths in the filtered lattice. We refer to this as the sequential method, since p(u|E) is calculated separately for each u in sequence. Allauzen et al. (2010) introduce a transducer for simultaneous calculation of p(u|E) for all unigrams u ∈ N1 in a lattice. This transducer is effective for finding path posterior probabilities of unigrams because there are relatively few unique unigrams in the lattice. As we will show, however, it is less efficient for higher-order n-grams. Allauzen et al. (2010) use exact statistics for the unigram path posterior probabilities in Equation (1), but use the conditional expected counts of Equation (4) for higher-order n-grams. Their hybrid MBR decoder has the form where k determines the range of n-gram orders at whic</context>
<context position="6809" citStr="Allauzen et al. (2010)" startWordPosition="1124" endWordPosition="1127">, counting unigrams in En is easier than counting n-grams in E. 3 Efficient Path Counting Associated with each En we have a transducer XFn which can be used to calculate the path posterior probabilities p(u|E) for all u ∈ Nn. In Figures 1 and 2 we give two possible forms2 of XFn that can be used to compute path posterior probabilities over n-grams u1,2 ∈ Nn for some n. No modification to the ρ-arc matching mechanism is required even in counting higher-order n-grams since all ngrams are represented as individual symbols after application of the mapping transducer fin. Transducer Xpn is used by Allauzen et al. (2010) to compute the exact unigram contribution to the conditional expected gain in Equation (5). For example, in counting paths that contain u1, XFn retains the first occurrence of u1 and maps every other symbol to ǫ. This ensures that in any path containing a given u, only the first u is counted, avoiding multiple counting of paths. We introduce an alternative path counting transducer XpR n that effectively deletes all symbols except the last occurrence of u on any path by ensuring that any paths in composition which count earlier instances of u do not end in a final state. Multiple counting is a</context>
<context position="8469" citStr="Allauzen et al., 2010" startWordPosition="1415" endWordPosition="1418">) , (5) + uEN:k&lt;JuJ&lt;4 0u#u(E′)p(u|E) -1 wn:u n w1 w2 28 Figure 1: Path counting transducer XpLn matching first (left-most) occurrence of each u E Nn. Figure 2: Path counting transducer XpRnmatching last (right-most) occurrence of each u E Nn. posing with XpR n creates a single copy of £n while searching for the last occurrence of u; we find this to be much more efficient for large Nn. Path posterior probabilities are calculated over each £n by composing with XFn in the log semiring, projecting on the output, removing ǫ-arcs, determinizing, minimising, and pushing weights to the initial state (Allauzen et al., 2010). Using either �L n or Rn ,the resulting counts acceptor is Xn. It has a compact form with one arc from the start state for each ui E Nn: 3.1 Efficient Path Posterior Calculation Although Xn has a convenient and elegant form, it can be difficult to build for large Nn because the composition £n o XFn results in millions of states and arcs. The log semiring ǫ-removal and determinization required to sum the probabilities of paths labelled with each u can be slow. However, if we use the proposed XpRn, then each path in £n o XpR n has only one non-ǫ output label u and all paths leading to a given f</context>
</contexts>
<marker>Allauzen, Kumar, Macherey, Mohri, Riley, 2010</marker>
<rawString>Cyril Allauzen, Shankar Kumar, Wolfgang Macherey, Mehryar Mohri, and Michael Riley. 2010. Expected sequence similarity maximization. In Human Language Technologies 2010: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="13347" citStr="Brants et al., 2007" startWordPosition="2285" endWordPosition="2288">alignments are generated using MTTK (Deng and Byrne, 2008) over 150M words of parallel text for the constrained NIST MT08 Arabic→English track. In decoding, a Shallow1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion words of English text. The n-gram factors 00, ... , 04 are set according to Tromble et al. (2008) using unigram precision 3http://www.itl.nist.gov/iad/mig/tests/mt p = 0.85 and average recall ratio r = 0.74. Our translation decoder and MBR procedures are implemented using OpenFst (Allauzen et al., 2007). 6 LMBR Speed and Performance Lattice MBR decoding performance is shown in Table 1. Compared to the maximum likelihood translation hypotheses (row ML), LMBR gives gains of +0.8 to +1.0 BLEU for newswire data and +0.5 BLEU for newsgroup data (row LMBR). The ot</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="12785" citStr="Deng and Byrne, 2008" startWordPosition="2192" endWordPosition="2195">m probability calculation and LMBR decoding using sequential method and left-most (XpLn) or right-most (XpRn) counting transducer implementations. Arabic→English machine translation task3. The development set mt0205tune is formed from the odd numbered sentences of the NIST MT02– MT05 testsets; the even numbered sentences form the validation set mt0205test. Performance on NIST MT08 newswire (mt08nw) and newsgroup (mt08ng) data is also reported. First-pass translation is performed using HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder. Word alignments are generated using MTTK (Deng and Byrne, 2008) over 150M words of parallel text for the constrained NIST MT08 Arabic→English track. In decoding, a Shallow1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion wo</context>
</contexts>
<marker>Deng, Byrne, 2008</marker>
<rawString>Yonggang Deng and William Byrne. 2008. HMM word and phrase alignment for statistical machine translation. IEEE Transactions on Audio, Speech, and Language Processing, 16(3):494–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),</booktitle>
<pages>1--8</pages>
<location>Philadelphia,</location>
<contexts>
<context position="9725" citStr="Eisner, 2002" startWordPosition="1637" endWordPosition="1638">ward algorithm can be used to calculate p(u|£) without the costly ǫ-removal and determinization. The modification simply requires keeping track of which symbol u is encountered along each path to a final state. More than one final state may gather probabilities for the same u; to compute p(u|£) these probabilities are added. The forward algorithm requires that £noj1R n be topologically sorted; although sorting can be slow, it is still quicker than log semiring ǫ-removal and determinization. The statistics gathered by the forward algorithm could also be gathered under the expectation semiring (Eisner, 2002) with suitably defined features. We take the view that the full complexity of that approach is not needed here, since only one symbol is introduced per path and per exit state. Unlike £n o XpRn, the composition £n o q&apos;Ln does not segregate paths by u such that there is a direct association between final states and symbols. The forward algorithm does not readily yield the per-symbol probabilities, although an arc weight vector indexed by symbols could be used to correctly aggregate the required statistics (Riley et al., 2009). For large Nn this would be memory intensive. The association between</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL), pages 1–8, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Third Edition.</title>
<date>2007</date>
<contexts>
<context position="13234" citStr="Graff et al., 2007" startWordPosition="2267" endWordPosition="2270">st-pass translation is performed using HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder. Word alignments are generated using MTTK (Deng and Byrne, 2008) over 150M words of parallel text for the constrained NIST MT08 Arabic→English track. In decoding, a Shallow1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion words of English text. The n-gram factors 00, ... , 04 are set according to Tromble et al. (2008) using unigram precision 3http://www.itl.nist.gov/iad/mig/tests/mt p = 0.85 and average recall ratio r = 0.74. Our translation decoder and MBR procedures are implemented using OpenFst (Allauzen et al., 2007). 6 LMBR Speed and Performance Lattice MBR decoding performance is shown in Table 1. Compared to the maximum likelihood translation hypotheses (row</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrasebased translation with weighted finite state transducers.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>433--441</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Hierarchical phrasebased translation with weighted finite state transducers. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 433–441, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="13111" citStr="Kneser and Ney, 1995" startWordPosition="2245" endWordPosition="2248">m the validation set mt0205test. Performance on NIST MT08 newswire (mt08nw) and newsgroup (mt08ng) data is also reported. First-pass translation is performed using HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder. Word alignments are generated using MTTK (Deng and Byrne, 2008) over 150M words of parallel text for the constrained NIST MT08 Arabic→English track. In decoding, a Shallow1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion words of English text. The n-gram factors 00, ... , 04 are set according to Tromble et al. (2008) using unigram precision 3http://www.itl.nist.gov/iad/mig/tests/mt p = 0.85 and average recall ratio r = 0.74. Our translation decoder and MBR procedures are implemented using OpenFst (Allauzen et al., 2007). 6 LMBR Speed and Perfo</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technologies: The 2004 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="2106" citStr="Kumar and Byrne, 2004" startWordPosition="335" endWordPosition="338">e Eu = {E ∈ E : #u(E) &gt; 0} is the subset of lattice paths containing the n-gram u at least 1We omit an introduction to WFSTs for space reasons. See Mohri et al. (2008) for details of the general purpose WFST operations used in this paper. once. It is the efficient computation of these path posterior n-gram probabilities that is the primary focus of this paper. We will show how general purpose WFST algorithms can be employed to efficiently compute p(u|E) for all u ∈ N. Tromble et al. (2008) use Equation (1) as an approximation to the general form of statistical machine translation MBR decoder (Kumar and Byrne, 2004): X E� = argmin L(E, E′)P(E|F) (3) E′EE EEE The approximation replaces the sum over all paths in the lattice by a sum over lattice n-grams. Even though a lattice may have many n-grams, it is possible to extract and enumerate them exactly whereas this is often impossible for individual paths. Therefore, while the Tromble et al. (2008) linearisation of the gain function in the decision rule is an approximation, Equation (1) can be computed exactly even over very large lattices. The challenge is to do so efficiently. If the quantity p(u|E) had the form of a conditional expected count c(u|E) = X #</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of Human Language Technologies: The 2004 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 169–176.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>163--171</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="5238" citStr="Kumar et al., 2009" startWordPosition="849" endWordPosition="852"> cases it will be perfectly fine, depending on how closely p(u|E) and c(u|E) agree for higher-order n-grams. Experimentally, Allauzen et al. (2010) find this approximation works well at k = 1 for MBR decoding of statistical machine translation lattices. However, there may be scenarios in which p(u|E) and c(u|E) differ so that Equation (5) is no longer useful in place of the original Tromble et al. (2008) approximation. In the following sections, we present an efficient method for simultaneous calculation of p(u|E) for n-grams of a fixed order. While other fast MBR approximations are possible (Kumar et al., 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. 2 N-gram Mapping Transducer We make use of a trick to count higher-order ngrams. We build transducer fin to map word sequences to n-gram sequences of order n. fin has a similar form to the WFST implementation of an ngram language model (Allauzen et al., 2003). fin includes for each n-gram u = wn1 arcs of the form: The n-gram lattice of order n is called En and is found by composing E ◦fin, projecting on the output, removing ǫ-arcs, d</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163– 171, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F C N Pereira</author>
<author>M Riley</author>
</authors>
<title>Speech recognition with weighted finite-state transducers. Handbook on Speech Processing and Speech Communication.</title>
<date>2008</date>
<contexts>
<context position="1651" citStr="Mohri et al. (2008)" startWordPosition="258" endWordPosition="261">rations1. The LMBR decision rule in Tromble et al. (2008) has the form E = argmax{ 00 |E′ |+ E′EE l uEN (1) where E is a lattice of translation hypotheses, N is the set of all n-grams in the lattice (typically, n = 1... 4), and the parameters θ are constants estimated on held-out data. The quantity p(u|E) we refer to as the path posterior probability of the n-gram u. This particular posterior is defined as p(u|E) = p(Eu|E) = X P(E|F), (2) EEEu where Eu = {E ∈ E : #u(E) &gt; 0} is the subset of lattice paths containing the n-gram u at least 1We omit an introduction to WFSTs for space reasons. See Mohri et al. (2008) for details of the general purpose WFST operations used in this paper. once. It is the efficient computation of these path posterior n-gram probabilities that is the primary focus of this paper. We will show how general purpose WFST algorithms can be employed to efficiently compute p(u|E) for all u ∈ N. Tromble et al. (2008) use Equation (1) as an approximation to the general form of statistical machine translation MBR decoder (Kumar and Byrne, 2004): X E� = argmin L(E, E′)P(E|F) (3) E′EE EEE The approximation replaces the sum over all paths in the lattice by a sum over lattice n-grams. Even </context>
</contexts>
<marker>Mohri, Pereira, Riley, 2008</marker>
<rawString>M. Mohri, F.C.N. Pereira, and M. Riley. 2008. Speech recognition with weighted finite-state transducers. Handbook on Speech Processing and Speech Communication.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Riley</author>
<author>Cyril Allauzen</author>
<author>Martin Jansche</author>
</authors>
<title>OpenFst: An Open-Source, Weighted FiniteState Transducer Library and its Applications to Speech and Language.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts,</booktitle>
<pages>9--10</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="10255" citStr="Riley et al., 2009" startWordPosition="1727" endWordPosition="1730">he forward algorithm could also be gathered under the expectation semiring (Eisner, 2002) with suitably defined features. We take the view that the full complexity of that approach is not needed here, since only one symbol is introduced per path and per exit state. Unlike £n o XpRn, the composition £n o q&apos;Ln does not segregate paths by u such that there is a direct association between final states and symbols. The forward algorithm does not readily yield the per-symbol probabilities, although an arc weight vector indexed by symbols could be used to correctly aggregate the required statistics (Riley et al., 2009). For large Nn this would be memory intensive. The association between final states and symbols could also be found by label pushing, but we find this slow for large £n o XFn. 4 Efficient Decoder Implementation In contrast to Equation (5), we use the exact values of p(u|£) for all u E Nn at orders n = 1... 4 to compute 4 l E = argmin{ 00|E′ |+ �gn(E, E′) }, (6) E′∈E l J where gn(E, E′) = Eu∈Arn 0u#u(E′)p(u|£) using the exact path posterior probabilities at each order. We make acceptors Qn such that £ o Qn assigns order n partial gain gn(E, E′) to all paths E E £. Qn is derived from 4bn directl</context>
</contexts>
<marker>Riley, Allauzen, Jansche, 2009</marker>
<rawString>Michael Riley, Cyril Allauzen, and Martin Jansche. 2009. OpenFst: An Open-Source, Weighted FiniteState Transducer Library and its Applications to Speech and Language. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts, pages 9–10, Boulder, Colorado, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum BayesRisk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>620--629</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1089" citStr="Tromble et al. (2008)" startWordPosition="146" endWordPosition="149">sducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices. 1 Introduction This paper focuses on an exact implementation of the linearised form of lattice minimum Bayesrisk (LMBR) decoding using general purpose weighted finite state transducer (WFST) operations1. The LMBR decision rule in Tromble et al. (2008) has the form E = argmax{ 00 |E′ |+ E′EE l uEN (1) where E is a lattice of translation hypotheses, N is the set of all n-grams in the lattice (typically, n = 1... 4), and the parameters θ are constants estimated on held-out data. The quantity p(u|E) we refer to as the path posterior probability of the n-gram u. This particular posterior is defined as p(u|E) = p(Eu|E) = X P(E|F), (2) EEEu where Eu = {E ∈ E : #u(E) &gt; 0} is the subset of lattice paths containing the n-gram u at least 1We omit an introduction to WFSTs for space reasons. See Mohri et al. (2008) for details of the general purpose WF</context>
<context position="2441" citStr="Tromble et al. (2008)" startWordPosition="393" endWordPosition="396">imary focus of this paper. We will show how general purpose WFST algorithms can be employed to efficiently compute p(u|E) for all u ∈ N. Tromble et al. (2008) use Equation (1) as an approximation to the general form of statistical machine translation MBR decoder (Kumar and Byrne, 2004): X E� = argmin L(E, E′)P(E|F) (3) E′EE EEE The approximation replaces the sum over all paths in the lattice by a sum over lattice n-grams. Even though a lattice may have many n-grams, it is possible to extract and enumerate them exactly whereas this is often impossible for individual paths. Therefore, while the Tromble et al. (2008) linearisation of the gain function in the decision rule is an approximation, Equation (1) can be computed exactly even over very large lattices. The challenge is to do so efficiently. If the quantity p(u|E) had the form of a conditional expected count c(u|E) = X #u(E)P(E|F), (4) EEE it could be computed efficiently using counting transducers (Allauzen et al., 2003). The statistic c(u|E) counts the number of times an n-gram occurs on each path, accumulating the weighted count over all paths. By contrast, what is needed by the approximation in Equation (1) is to identify all paths containing an</context>
<context position="5026" citStr="Tromble et al. (2008)" startWordPosition="817" endWordPosition="820">rior probabilities p(u|E) of Equation (2) and conditional expected counts c(u|E) of Equation (4) are used to compute the expected gain. For k &lt; 4, Equation (5) is thus an approximation to the approximation. In many cases it will be perfectly fine, depending on how closely p(u|E) and c(u|E) agree for higher-order n-grams. Experimentally, Allauzen et al. (2010) find this approximation works well at k = 1 for MBR decoding of statistical machine translation lattices. However, there may be scenarios in which p(u|E) and c(u|E) differ so that Equation (5) is no longer useful in place of the original Tromble et al. (2008) approximation. In the following sections, we present an efficient method for simultaneous calculation of p(u|E) for n-grams of a fixed order. While other fast MBR approximations are possible (Kumar et al., 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. 2 N-gram Mapping Transducer We make use of a trick to count higher-order ngrams. We build transducer fin to map word sequences to n-gram sequences of order n. fin has a similar form to the WFST implementation of an ngram lang</context>
<context position="13480" citStr="Tromble et al. (2008)" startWordPosition="2310" endWordPosition="2313">lish track. In decoding, a Shallow1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion words of English text. The n-gram factors 00, ... , 04 are set according to Tromble et al. (2008) using unigram precision 3http://www.itl.nist.gov/iad/mig/tests/mt p = 0.85 and average recall ratio r = 0.74. Our translation decoder and MBR procedures are implemented using OpenFst (Allauzen et al., 2007). 6 LMBR Speed and Performance Lattice MBR decoding performance is shown in Table 1. Compared to the maximum likelihood translation hypotheses (row ML), LMBR gives gains of +0.8 to +1.0 BLEU for newswire data and +0.5 BLEU for newsgroup data (row LMBR). The other rows of Table 1 show the performance of LMBR decoding using the hybrid decision rule of Equation (5) for 0 ≤ k ≤ 3. When the cond</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum BayesRisk decoding for statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620–629, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>