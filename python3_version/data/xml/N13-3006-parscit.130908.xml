<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028939">
<title confidence="0.9868315">
KooSHO: Japanese Text Input Environment
based on Aerial Hand Writing
</title>
<author confidence="0.96359">
Masato Hagiwara Soh Masuko
</author>
<affiliation confidence="0.986322">
Rakuten Institute of Technology Rakuten Institute of Technology
</affiliation>
<address confidence="0.987119">
215 Park Avenue South, 4-13-9 Higashi-shinagawa
New York, NY, USA 10003 Shinagawa-ku, Tokyo, JAPAN 140-0002
</address>
<email confidence="0.999414">
masato.hagiwara@mail.rakuten.com so.masuko@mail.rakuten.com
</email>
<sectionHeader confidence="0.996575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998592294117647">
Hand gesture-based input systems have
been in active research, yet most of them
focus only on single character recognition.
We propose KooSHO: an environment for
Japanese input based on aerial hand ges-
tures. The system provides an integrated
experience of character input, Kana-Kanji
conversion, and search result visualization.
To achieve faster input, users only have to
input consonant, which is then converted
directly to Kanji sequences by direct conso-
nant decoding. The system also shows sug-
gestions to complete the user input. The
comparison with voice recognition and a
screen keyboard showed that KooSHO can
be a more practical solution compared to
the existing system.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947">
In mobile computing, intuitive and natural
text input is crucial for successful user experi-
ence, and there have been many methods and
systems proposed as the alternatives to tradi-
tional keyboard-and-mouse input devices. One
of the most widely used input technologies is
voice recognition such as Apple Inc.’s Siri. How-
ever, it has some drawbacks such as being vul-
nerable to ambient noise and privacy issues
when being overheared. Virtual keyboardsi
require extensive practice and could be error-
prone compared to physical keyboards.
</bodyText>
<footnote confidence="0.963812">
1http://www.youtube.com/watch?v=h9htRy0-sUw
</footnote>
<figureCaption confidence="0.994038333333333">
Figure 1: Overall text input procedure using
KooSHO — (a) Character recognition (b) Kana-
Kanji conversion results (c) Search results
</figureCaption>
<bodyText confidence="0.999944961538461">
In order to address these issues, many gesture-
based text input interfaces have been proposed,
including a magnet-based hand writing device
(Han et al., 2007). Because these systems re-
quire users to wear or hold special devices, hand
gesture recognition systems based on video cam-
eras are proposed, such as Yoon et al. (1999)
and Sonoda and Muraoka (2003). However, a
large portion of the literature only focuses on
single character input. One must consider over-
all text input experience when users are typing
words and phrases. This problem is pronounced
for languages which require explicit conversion
from Romanized forms to ideographic writing
systems such as Japanese.
In this paper, we propose KooSHO: an in-
tegrated environment for Japanese text input
based on aerial hand gestures. It provides an
integrated experience of character input, Kana-
Kanji conversion, i.e., conversion from Roman-
ized forms to ideographic (Kanji) ones, and
search result visualization. Figure 1 shows the
overall procedure using KooSHO. First, (a) a
user draws alphabetical shapes in the air, whose
hand position is captured by Microsoft Kinect.
KooSHO then recognizes characters, and after
</bodyText>
<figure confidence="0.827452">
(a) (b) (c)
</figure>
<page confidence="0.937668">
24
</page>
<note confidence="0.33143">
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 24–27,
Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.731256">
Kinect
</figure>
<figureCaption confidence="0.99985">
Figure 2: Confiugration of the KooSHO system
</figureCaption>
<bodyText confidence="0.98796021875">
Kana-Kanji conversion, the results are shown in
a circle centered at the user’s shoulder (b). Fi-
nally, the user can choose one of the candidates
by “touching” it, and (c) the search result using
the chosen word as the query is shown in circle
again for the user to choose.
KooSHO has several novelties to achieve
seamless yet robust text input, including:
Non-restrictive character forms — the
system does not restrict on the input character
forms, unlike Graffiti 22.
Robust continuous recognition and con-
version — Aerial handwriting poses special dif-
ficulties since the system cannot obtain individ-
ual strokes. We solve this problem by employing
a discriminative Kana-Kanji conversion model
trained on the specific domain.
Faster input by suggestions and con-
sonant input — KooSHO shows suggestions
to predict the words the user is about to in-
put, while it allows users to type only conso-
nants, similar to Tanaka-Ishii et al. (2001).
We propose direct consonant decoding, which
runs Viterbi search directly on the input con-
sonant sequence without converting them back
into Kana candidates.
We conducted the evaluations on character
recognition and Kana-Kanji conversion accu-
racy to measure KooSHO’s performance. We
also ran an overall user experience test, compar-
ing its performance with the voice recognition
software Siri and a screen keyboard.
</bodyText>
<sectionHeader confidence="0.914718" genericHeader="method">
2 Character Recognition
</sectionHeader>
<bodyText confidence="0.996759333333333">
Figure 2 describes the overall configuration. A
user draws alphabetical shapes in the air, which
is captured by Kinect and sent to KooSHO. We
</bodyText>
<footnote confidence="0.706645">
2http://en.wikipedia.org/wiki/Graffiti_2
</footnote>
<figureCaption confidence="0.999176">
Figure 3: Lattice search based on consonants
</figureCaption>
<bodyText confidence="0.999935423076923">
used the skeleton recognition functionalities in-
cluded in Kinect for Windows SDK v1.5.1. The
system consists of the front-end and back-end
parts, which are responsible for character recog-
nition and user interface, and Kana-Kanji con-
version and suggestion, respectively.
We continuously match the drawn trajectory
to templates (training examples) using dynamic
programming. The trajectory and the templates
are both represented by 8 direction features to
facilitate the match, and the distance is cal-
culated based on how apart the directions are.
This coding system is robust to scaling of char-
acters and a slight variation of writing speed,
while not robust to stroke order. This is re-
peated every frame to produce the distance be-
tween the trajectory ending at the current frame
and each template. If the distance is below a cer-
tain threshold, the character is considered to be
the one the user has just drawn.
If more than one characters are detected and
their periods overlap, they are both sent as al-
ternative. The result is represented as a lattice,
with alternation and concatenation. To each let-
ter a confidence score (the inverse of the mini-
mum distance from the template) is assigned.
</bodyText>
<sectionHeader confidence="0.998323" genericHeader="method">
3 Kana-Kanji Conversion
</sectionHeader>
<bodyText confidence="0.999913181818182">
In this section, we describe the Kana-Kanji
conversion model we employed to achieve the
consonant-to-Kanji conversion. As we men-
tioned, the input to the back-end part passed
from the front-end part is a lattice of possi-
ble consonant sequences. We therefore have to
“guess” the possibly omitted vowels somehow
and convert the sequences back into intended
Kanji sequences. However, it would be an ex-
ponentially large number if we expand the in-
put consonant sequence to all possible Kana se-
</bodyText>
<figure confidence="0.97430515">
Character Recognition
&amp; User Interface
Kana-Kanji Conversion
&amp; Suggestion
Front-End
Back-End
The Internet
KooSHO Engine
Screen
H K B K R
BOS EOS
Tf (hu)
AR (huku)
6 (huku)
fro% (hokubu)
M1113 (kobukuro)
ltO (buki)
V i(bukuro)
L*L (kore)
X (ro)
</figure>
<page confidence="0.98514">
25
</page>
<bodyText confidence="0.99998445">
quences. Therefore, instead of attempting to re-
store all possible Kana sequences, we directly
“decode” the input consonant sequence to ob-
tain the Kanji sequence. We call this process
direct consonant decoding, shown in Figure 3. It
is basically the same as the vanilla Viterbi search
often used for Japanese morphological analysis
(Kudo et al., 2004), except that it runs on a con-
sonant sequence. The key change to this Viterbi
search is to make it possible to look up the dic-
tionary directly by consonant substrings. To do
this, we convert dictionary entries to possible
consonant sequences referring to Microsoft IME
Kana Table3 when the dictionary structure is
loaded onto the memory. For example, for a dic-
tionary entry;KR/7 hukubukuro, possi-
ble consonant sequences such as “hkbkr,” “huk-
bkr,” “hkubkr,” “hukubkr,” “hkbukr,”... are
stored in the index structure.
As for the conversion model, we employed the
discriminative Kana-Kanji conversion model by
Tokunaga (2011). The basic algorithm is the
same except that the Viterbi search also runs on
consonant sequences rather than Kana ones. We
used surface unigram, surface + class (PoS tags)
unigram, surface + reading unigram, class bi-
gram, surface bigram as features. The red lines
in Figure 3 illustrate the finally chosen path.
The suggestion candidates, which is to show
candidates such as hukubukuro (lucky bag) and
hontai (body) for an input “H,” are chosen from
2000 most frequent query fragments issued in
2011 at Rakuten Ichiba4. We annotate each
query with Kana pronunciation, which is con-
verted into possible consonant sequence as in
the previous section. At run-time, prefix search
is perfomed on this consonant trie to obtain the
candidate list. The candidate list is sorted by
the frequency, and shown to the user supple-
menting the Kana-Kanji conversion results.
</bodyText>
<sectionHeader confidence="0.999019" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997136">
In this section, we compare KooSHO with
Siri and a software keyboard system. We
used the following three training corpora: 1)
</bodyText>
<footnote confidence="0.9995255">
3http://support.microsoft.com/kb/883232/ja
4http://www.rakuten.co.jp/
</footnote>
<bodyText confidence="0.999563113636364">
BCCWJ-CORE (60,374 sentences and 1,286,899
tokens)5, 2) EC corpus, consists of 1,230 product
titles and descriptions randomly sampled from
Rakuten Ichiba (118,355 tokens). 3) EC query
log (2000 most frequent query fragments issued
in 2011 at Rakuten Ichiba) As the dictionary,
we used UniDic6.
Character Recognition Firstly, we evaluate
the accuracy of the character recognition model.
For each letter from “A” to “Z,” two subjects
attempted to type the letter for three times, and
the accuracy how many times the character was
correctly recognized was measured.
We observed recognition accuracy varies from
letter to letter. Letters which have similar
forms, such as “F” and “T” can be easily mis-
recognized, leading lower accuracy. For some
of the cases where the letter shape completely
contains a shape of the other, e.g., “F” and “E,”
recognition error is inevitable. The overall char-
acter recognition accuracy was 0.76.
Kana-Kanji Conversion Secondly, we eval-
uate the accuracy of the Kana-Kanji conversion
algorithm. We used ACC (averaged Top-1 ac-
curacy), MFS (mean F score), and MRR (mean
reciprocal rank) as evaluation measures (Li et
al., 2009). We used a test set consisting of
100 words and phrases which are randomly ex-
tracted from Rakuten Ichiba, popular products
and query logs. The result was ACC = 0.24,
MFS = 0.50, and MRR = 0.30, which suggests
the right choice comes at the top 24 percent of
the time, about half (50%) the characters of the
top candidate match the answer, and the aver-
age position of the answer is 1 / MRR = 3.3. No-
tice that this is a very strict evaluation since it
does not allow partial input. For example, even
if “7-f fittonesushu-zu (fitness
shoes) does not come up at the top, one could
obtain the same result by inputting “7-f���
7,” (fitness) and “����” (shoes) separately.
Also, we found that some spelling variations
such as I-D6f and I-D-� (both meaning eye-
lashes) lower the evaluation result, even though
</bodyText>
<footnote confidence="0.9999685">
5http://www.ninjal.ac.jp/corpus_center/bccwj/
6http://www.tokuteicorpus.jp/dist/
</footnote>
<page confidence="0.997403">
26
</page>
<bodyText confidence="0.998246243902439">
they are not a problem in practice.
Overall Evaluation Lastly, we evaluate the
overall input accuracy, speed, and user experi-
ence comparing Siri, a screen keyboard (Tablet
PC Input Panel) controlled by Kinect using
KinEmote7, and KooSHO.
First, we measured the recognition accuracy
of Siri based on the 100 test queries. The accu-
racy turned out to be 85%, and the queries were
recognized within three to four seconds. How-
ever, about 14% of the queries cannot be recog-
nized even after many attempts. There are es-
pecially two types of queries where voice recog-
nition performed poorly — the first one is rel-
atively new, unknown words such as �-0�
✓F&apos; (ogaland), which obviously depends on the
recognition system’s language models and the
vocabulary set. The second the is homonyms,
i.e., voice recognition is, in principle, unable to
discern multiple words with the same pronun-
ciation, such as “�V” (package) and “MCA”
(broadcast) housou, and “�Fj� ✓” (alum) and
“MR” (tomorrow evening) myouban. This is
where KooSHO-like visual feedback on the con-
version results has a clear advantage.
Second, we tried the screen keyboard con-
trolled by Kinect. Using a screen keyboard was
extremely difficult, almost impossible, since it
requires fine movement of hands in order to
place the cursor over the desired keys. There-
fore, only the time required to place the cursor
on the desired keys in order was measured. The
fact that users have to type out all the characters
including vowels is making the matter worse.
This is also where KooSHO excels.
Finally, we measured the time taken for
KooSHO to complete each query. The result
varied depending on query, but the ones which
contain characters with low recognition accu-
racy such as “C” (e.g., “9�--X” (cheese)) took
longer. The average was 35 seconds.
</bodyText>
<sectionHeader confidence="0.90579" genericHeader="conclusions">
Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.998718333333333">
In this paper, we proposed a novel environ-
ment for Japanese text input based on aerial
hand gestures called KooSHO, which provides
</bodyText>
<footnote confidence="0.859542">
7http://www.kinemote.net/
</footnote>
<bodyText confidence="0.999955473684211">
an integrated experience of character input,
Kana-Kanji conversion, and search result vi-
sualization. This is the first to propose a
Japanese text input system beyond single char-
acters based on hand gestures. The system has
several novelties, including 1) non-restrictive
character forms, 2) robust continuous recogni-
tion and Kana-Kanji conversion, and 3) faster
input by suggestions and consonant input. The
comparison with voice recognition and a screen
keyboard showed KooSHO can be a more prac-
tical solution compared to the screen keyboard.
Since KooSHO is an integrated Japanese in-
put environment, not just a character recog-
nition software, many features implemented in
modern input methods, such as fuzzy match-
ing and user personalization, can also be im-
plemented. In particular, how to let the user
modify the mistaken input is a great challenge.
</bodyText>
<sectionHeader confidence="0.998066" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999458517241379">
Xinying Han, Hiroaki Seki, Yoshitsugu kamiya, and
Masatoshi Hikizu. 2007. Wearable handwriting
input device using magnetic field. In Proc. of
SICE, pages 365–368.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proc. of
EMNLP, pages 230–237.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proc. of NEWS,
pages 1–18.
Tomonari Sonoda and Yoishic Muraoka. 2003. A
letter input system of handwriting gesture (in
Japanese). The Transactions of the Institute of
Electronics, Information and Communication En-
gineers D-II, J86-D-II:1015–1025.
Kumiko Tanaka-Ishii, Yusuke Inutsuka, and Masato
Takeichi. 2001. Japanese text input system with
digits. In Proc. of HLT, pages 1–8.
Hiroyuki Tokunaga, Daisuke Okanohara, and Shin-
suke Mori. 2011. Discriminative method for
Japanese kana-kanji input method. In Proc. of
WTIM.
Ho-Sub Yoon, Jung Soh, Byung-Woo Min, and
Hyun Seung Yang. 1999. Recognition of al-
phabetical hand gestures using hidden markov
model. IEICE Trans. Fundamentals, E82-
A(7):1358–1366.
</reference>
<page confidence="0.998802">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844883">
<title confidence="0.997826">KooSHO: Japanese Text Input based on Aerial Hand Writing</title>
<author confidence="0.925022">Masato Hagiwara Soh Masuko</author>
<affiliation confidence="0.999798">Rakuten Institute of Technology Rakuten Institute of Technology</affiliation>
<address confidence="0.975898">215 Park Avenue South, 4-13-9 Higashi-shinagawa New York, NY, USA 10003 Shinagawa-ku, Tokyo, JAPAN 140-0002</address>
<email confidence="0.99603">masato.hagiwara@mail.rakuten.comso.masuko@mail.rakuten.com</email>
<abstract confidence="0.997406444444444">Hand gesture-based input systems have been in active research, yet most of them focus only on single character recognition. We propose KooSHO: an environment for Japanese input based on aerial hand gestures. The system provides an integrated experience of character input, Kana-Kanji conversion, and search result visualization. To achieve faster input, users only have to input consonant, which is then converted to Kanji sequences by conso- The system also shows suggestions to complete the user input. The comparison with voice recognition and a screen keyboard showed that KooSHO can be a more practical solution compared to the existing system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xinying Han</author>
<author>Hiroaki Seki</author>
<author>Yoshitsugu kamiya</author>
<author>Masatoshi Hikizu</author>
</authors>
<title>Wearable handwriting input device using magnetic field.</title>
<date>2007</date>
<booktitle>In Proc. of SICE,</booktitle>
<pages>365--368</pages>
<contexts>
<context position="1896" citStr="Han et al., 2007" startWordPosition="272" endWordPosition="275">input technologies is voice recognition such as Apple Inc.’s Siri. However, it has some drawbacks such as being vulnerable to ambient noise and privacy issues when being overheared. Virtual keyboardsi require extensive practice and could be errorprone compared to physical keyboards. 1http://www.youtube.com/watch?v=h9htRy0-sUw Figure 1: Overall text input procedure using KooSHO — (a) Character recognition (b) KanaKanji conversion results (c) Search results In order to address these issues, many gesturebased text input interfaces have been proposed, including a magnet-based hand writing device (Han et al., 2007). Because these systems require users to wear or hold special devices, hand gesture recognition systems based on video cameras are proposed, such as Yoon et al. (1999) and Sonoda and Muraoka (2003). However, a large portion of the literature only focuses on single character input. One must consider overall text input experience when users are typing words and phrases. This problem is pronounced for languages which require explicit conversion from Romanized forms to ideographic writing systems such as Japanese. In this paper, we propose KooSHO: an integrated environment for Japanese text input </context>
</contexts>
<marker>Han, Seki, kamiya, Hikizu, 2007</marker>
<rawString>Xinying Han, Hiroaki Seki, Yoshitsugu kamiya, and Masatoshi Hikizu. 2007. Wearable handwriting input device using magnetic field. In Proc. of SICE, pages 365–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="7033" citStr="Kudo et al., 2004" startWordPosition="1089" endWordPosition="1092"> all possible Kana seCharacter Recognition &amp; User Interface Kana-Kanji Conversion &amp; Suggestion Front-End Back-End The Internet KooSHO Engine Screen H K B K R BOS EOS Tf (hu) AR (huku) 6 (huku) fro% (hokubu) M1113 (kobukuro) ltO (buki) V i(bukuro) L*L (kore) X (ro) 25 quences. Therefore, instead of attempting to restore all possible Kana sequences, we directly “decode” the input consonant sequence to obtain the Kanji sequence. We call this process direct consonant decoding, shown in Figure 3. It is basically the same as the vanilla Viterbi search often used for Japanese morphological analysis (Kudo et al., 2004), except that it runs on a consonant sequence. The key change to this Viterbi search is to make it possible to look up the dictionary directly by consonant substrings. To do this, we convert dictionary entries to possible consonant sequences referring to Microsoft IME Kana Table3 when the dictionary structure is loaded onto the memory. For example, for a dictionary entry;KR/7 hukubukuro, possible consonant sequences such as “hkbkr,” “hukbkr,” “hkubkr,” “hukubkr,” “hkbukr,”... are stored in the index structure. As for the conversion model, we employed the discriminative Kana-Kanji conversion mo</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proc. of EMNLP, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>Report of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proc. of NEWS,</booktitle>
<pages>1--18</pages>
<contexts>
<context position="9876" citStr="Li et al., 2009" startWordPosition="1533" endWordPosition="1536">y recognized was measured. We observed recognition accuracy varies from letter to letter. Letters which have similar forms, such as “F” and “T” can be easily misrecognized, leading lower accuracy. For some of the cases where the letter shape completely contains a shape of the other, e.g., “F” and “E,” recognition error is inevitable. The overall character recognition accuracy was 0.76. Kana-Kanji Conversion Secondly, we evaluate the accuracy of the Kana-Kanji conversion algorithm. We used ACC (averaged Top-1 accuracy), MFS (mean F score), and MRR (mean reciprocal rank) as evaluation measures (Li et al., 2009). We used a test set consisting of 100 words and phrases which are randomly extracted from Rakuten Ichiba, popular products and query logs. The result was ACC = 0.24, MFS = 0.50, and MRR = 0.30, which suggests the right choice comes at the top 24 percent of the time, about half (50%) the characters of the top candidate match the answer, and the average position of the answer is 1 / MRR = 3.3. Notice that this is a very strict evaluation since it does not allow partial input. For example, even if “7-f fittonesushu-zu (fitness shoes) does not come up at the top, one could obtain the same result </context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min Zhang. 2009. Report of news 2009 machine transliteration shared task. In Proc. of NEWS, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomonari Sonoda</author>
<author>Yoishic Muraoka</author>
</authors>
<title>A letter input system of handwriting gesture (in Japanese).</title>
<date>2003</date>
<booktitle>The Transactions of the Institute of Electronics, Information and Communication Engineers D-II,</booktitle>
<pages>86--1015</pages>
<contexts>
<context position="2093" citStr="Sonoda and Muraoka (2003)" startWordPosition="306" endWordPosition="309"> keyboardsi require extensive practice and could be errorprone compared to physical keyboards. 1http://www.youtube.com/watch?v=h9htRy0-sUw Figure 1: Overall text input procedure using KooSHO — (a) Character recognition (b) KanaKanji conversion results (c) Search results In order to address these issues, many gesturebased text input interfaces have been proposed, including a magnet-based hand writing device (Han et al., 2007). Because these systems require users to wear or hold special devices, hand gesture recognition systems based on video cameras are proposed, such as Yoon et al. (1999) and Sonoda and Muraoka (2003). However, a large portion of the literature only focuses on single character input. One must consider overall text input experience when users are typing words and phrases. This problem is pronounced for languages which require explicit conversion from Romanized forms to ideographic writing systems such as Japanese. In this paper, we propose KooSHO: an integrated environment for Japanese text input based on aerial hand gestures. It provides an integrated experience of character input, KanaKanji conversion, i.e., conversion from Romanized forms to ideographic (Kanji) ones, and search result vi</context>
</contexts>
<marker>Sonoda, Muraoka, 2003</marker>
<rawString>Tomonari Sonoda and Yoishic Muraoka. 2003. A letter input system of handwriting gesture (in Japanese). The Transactions of the Institute of Electronics, Information and Communication Engineers D-II, J86-D-II:1015–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Tanaka-Ishii</author>
<author>Yusuke Inutsuka</author>
<author>Masato Takeichi</author>
</authors>
<title>Japanese text input system with digits.</title>
<date>2001</date>
<booktitle>In Proc. of HLT,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4065" citStr="Tanaka-Ishii et al. (2001)" startWordPosition="616" endWordPosition="619">hieve seamless yet robust text input, including: Non-restrictive character forms — the system does not restrict on the input character forms, unlike Graffiti 22. Robust continuous recognition and conversion — Aerial handwriting poses special difficulties since the system cannot obtain individual strokes. We solve this problem by employing a discriminative Kana-Kanji conversion model trained on the specific domain. Faster input by suggestions and consonant input — KooSHO shows suggestions to predict the words the user is about to input, while it allows users to type only consonants, similar to Tanaka-Ishii et al. (2001). We propose direct consonant decoding, which runs Viterbi search directly on the input consonant sequence without converting them back into Kana candidates. We conducted the evaluations on character recognition and Kana-Kanji conversion accuracy to measure KooSHO’s performance. We also ran an overall user experience test, comparing its performance with the voice recognition software Siri and a screen keyboard. 2 Character Recognition Figure 2 describes the overall configuration. A user draws alphabetical shapes in the air, which is captured by Kinect and sent to KooSHO. We 2http://en.wikipedi</context>
</contexts>
<marker>Tanaka-Ishii, Inutsuka, Takeichi, 2001</marker>
<rawString>Kumiko Tanaka-Ishii, Yusuke Inutsuka, and Masato Takeichi. 2001. Japanese text input system with digits. In Proc. of HLT, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Tokunaga</author>
<author>Daisuke Okanohara</author>
<author>Shinsuke Mori</author>
</authors>
<title>Discriminative method for Japanese kana-kanji input method.</title>
<date>2011</date>
<booktitle>In Proc. of WTIM.</booktitle>
<marker>Tokunaga, Okanohara, Mori, 2011</marker>
<rawString>Hiroyuki Tokunaga, Daisuke Okanohara, and Shinsuke Mori. 2011. Discriminative method for Japanese kana-kanji input method. In Proc. of WTIM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ho-Sub Yoon</author>
<author>Jung Soh</author>
<author>Byung-Woo Min</author>
<author>Hyun Seung Yang</author>
</authors>
<title>Recognition of alphabetical hand gestures using hidden markov model.</title>
<date>1999</date>
<journal>IEICE Trans. Fundamentals,</journal>
<pages>82--7</pages>
<contexts>
<context position="2063" citStr="Yoon et al. (1999)" startWordPosition="301" endWordPosition="304">ing overheared. Virtual keyboardsi require extensive practice and could be errorprone compared to physical keyboards. 1http://www.youtube.com/watch?v=h9htRy0-sUw Figure 1: Overall text input procedure using KooSHO — (a) Character recognition (b) KanaKanji conversion results (c) Search results In order to address these issues, many gesturebased text input interfaces have been proposed, including a magnet-based hand writing device (Han et al., 2007). Because these systems require users to wear or hold special devices, hand gesture recognition systems based on video cameras are proposed, such as Yoon et al. (1999) and Sonoda and Muraoka (2003). However, a large portion of the literature only focuses on single character input. One must consider overall text input experience when users are typing words and phrases. This problem is pronounced for languages which require explicit conversion from Romanized forms to ideographic writing systems such as Japanese. In this paper, we propose KooSHO: an integrated environment for Japanese text input based on aerial hand gestures. It provides an integrated experience of character input, KanaKanji conversion, i.e., conversion from Romanized forms to ideographic (Kan</context>
</contexts>
<marker>Yoon, Soh, Min, Yang, 1999</marker>
<rawString>Ho-Sub Yoon, Jung Soh, Byung-Woo Min, and Hyun Seung Yang. 1999. Recognition of alphabetical hand gestures using hidden markov model. IEICE Trans. Fundamentals, E82-A(7):1358–1366.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>