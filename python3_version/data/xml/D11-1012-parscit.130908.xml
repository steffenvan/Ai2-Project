<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<title confidence="0.967456">
A Joint Model for Extended Semantic Role Labeling
</title>
<author confidence="0.99087">
Vivek Srikumar and Dan Roth
</author>
<affiliation confidence="0.996481">
University of Illinois, Urbana-Champaign
</affiliation>
<address confidence="0.603408">
Urbana, IL 61801
</address>
<email confidence="0.978608">
{vsrikum2, danr}@illinois.edu
</email>
<bodyText confidence="0.9996075">
However, sentences express semantic relations
through other linguistic phenomena. For example,
consider the following sentence:
(1) The field goal by Brien changed the game in the
fourth quarter.
Verb centered semantic role labeling would identify
the arguments of the predicate change as (a) The
field goal by Brien (A0, the causer of the change),
(b) the game (A1, the thing changing), and (c) in
the fourth quarter (temporal modifier). However,
this does not tell us that the scorer of the field goal
was Brien, which is expressed by the preposition by.
Also, note that the in indicates a temporal relation,
which overlaps with the verb’s analysis.
In this paper, we propose an extension of the stan-
dard semantic role labeling task to include relations
expressed by lexical items other than verbs and nom-
inalizations. Further, we argue that there are interac-
tions between the different phenomena which sug-
gest that there is a benefit in studying them together.
However, one key challenge is that large jointly la-
beled corpora do not exist. This motivates the need
for novel learning and inference schemes that ad-
dress the data problem and can still benefit from the
interactions among the phenomena.
This paper has two main contributions.
1. From the machine learning standpoint, we pro-
pose a joint inference scheme to combine exist-
ing structure predictors for multiple linguistic
phenomena. We do so using hard constraints
that involve only the labels of the phenomena.
The strength of our model is that it is easily
</bodyText>
<sectionHeader confidence="0.667061" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949047619048">
This paper presents a model that extends se-
mantic role labeling. Existing approaches in-
dependently analyze relations expressed by
verb predicates or those expressed as nominal-
izations. However, sentences express relations
via other linguistic phenomena as well. Fur-
thermore, these phenomena interact with each
other, thus restricting the structures they artic-
ulate. In this paper, we use this intuition to
define a joint inference model that captures
the inter-dependencies between verb seman-
tic role labeling and relations expressed us-
ing prepositions. The scarcity of jointly la-
beled data presents a crucial technical chal-
lenge for learning a joint model. The key
strength of our model is that we use existing
structure predictors as black boxes. By en-
forcing consistency constraints between their
predictions, we show improvements in the per-
formance of both tasks without retraining the
individual models.
</bodyText>
<sectionHeader confidence="0.982218" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9982776">
The identification of semantic relations between
sentence constituents has been an important task in
NLP research. It finds applications in various natural
language understanding tasks that require complex
inference going beyond the surface representation.
In the literature, semantic role extraction has been
studied mostly in the context of verb predicates, us-
ing the Propbank annotation of Palmer et al. (2005),
and also for nominal predicates, using the Nombank
corpus of Meyers et al. (2004).
</bodyText>
<page confidence="0.980274">
129
</page>
<note confidence="0.9578875">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 129–139,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997002">
extensible, since adding new phenomena does
not require fully retraining the joint model from
scratch. Furthermore, our approach minimizes
the need for extensive jointly labeled corpora
and, instead, uses existing predictors as black
boxes.
2. From an NLP perspective, we motivate the ex-
tension of semantic role labeling beyond verbs
and nominalizations. We instantiate our joint
model for the case of extracting preposition and
verb relations together. Our model uses exist-
ing systems that identify verb semantic roles
and preposition object roles and jointly pre-
dicts the output of the two systems in the pres-
ence of linguistic constraints that enforce co-
herence between the predictions. We show that
using constraints to combine models improves
the performance on both tasks. Furthermore,
since the constraints depend only on the labels
of the two tasks and not on any specific dataset,
our experiments also demonstrate that enforc-
ing them allows for better domain adaptation.
The rest of the paper is organized as follows: We
motivate the need for extending semantic role label-
ing and the necessity for joint inference in Section 2.
In Section 3, we describe the component verb SRL
and preposition role systems. The global model is
defined in Section 4. Section 5 provides details on
the coherence constraints we use and demonstrates
the effectiveness of the joint model through experi-
ments. Section 6 discusses our approach in compar-
ison to existing work and Section 7 provides con-
cluding remarks.
</bodyText>
<sectionHeader confidence="0.886" genericHeader="introduction">
2 Problem Definition and Motivation
</sectionHeader>
<bodyText confidence="0.998865714285714">
Semantic Role Labeling has been extensively stud-
ied in the context of verbs and nominalizations.
While this analysis is crucial to understanding a
sentence, it is clear that in many natural language
sentences, information is conveyed via other lexi-
cal items. Consider, for example, the following sen-
tences:
</bodyText>
<listItem confidence="0.9612108">
(2) Einstein’s theory of relativity changed physics.
(3) The plays of Shakespeare are widely read.
(4) The bus, which was heading for Nairobi
in Kenya, crashed in the Kabale district of
Uganda.
</listItem>
<bodyText confidence="0.999846555555556">
The examples contain information that cannot be
captured by analyzing the verbs and the nominaliza-
tions. In sentence (2), the possessive form tells us
that the theory of relativity was discovered by Ein-
stein. Furthermore, the theory is on the subject of
relativity. The usage of the preposition of is dif-
ferent in sentence (3), where it indicates a creator-
creation relationship. In the last sentence, the same
preposition tells us that the Kabale district is located
in Uganda. Prepositions, compound nouns, posses-
sives, adjectival forms and punctuation marks of-
ten express relations, the identification of which is
crucial for text understanding tasks like recognizing
textual entailment, paraphrasing and question an-
swering.
The relations expressed by different linguistic
phenomena often overlap. For example, consider the
following sentence:
</bodyText>
<listItem confidence="0.901573">
(5) Construction of the library began in 1968.
</listItem>
<bodyText confidence="0.999078058823529">
The relation expressed by the nominalization con-
struction recognizes the library as the argument of
the predicate construct. However, the same analy-
sis can also be obtained by identifying the sense of
the preposition of, which tells us that the subject of
the preposition is a nominalization of the underlying
verb. A similar redundancy can be observed with
analyses of the verb began and the preposition in.
The above example motivates the following key in-
tuition: The correct interpretation of a sentence is
the one that gives a consistent analysis across all
the linguistic phenomena expressed in it.
An inference mechanism that simultaneously pre-
dicts the structure for different phenomena should
account for consistency between the phenomena. A
model designed to address this has the following
desiderata:
</bodyText>
<listItem confidence="0.99940975">
1. It should account for the dependencies between
phenomena.
2. It should be extensible to allow easy addition of
new linguistic phenomena.
</listItem>
<page confidence="0.986563">
130
</page>
<bodyText confidence="0.9889988">
3. It should be able to leverage existing state-of-
the-art models with minimal use of jointly la-
beled data, which is expensive to obtain.
Systems that are trained on each task indepen-
dently do not account for the interplay between
them. One approach for tackling this is to define
pipelines, where the predictions for one of the tasks
acts as the input for another. However, a pipeline
does not capture the two-way dependency between
the tasks. Training a fully joint model from scratch
is also unrealistic because it requires text that is an-
notated with all the tasks, thus making joint train-
ing implausible from a learning theoretic perspective
(See Punyakanok et al. (2005) for a discussion about
the learning theoretic requirements of joint training.)
</bodyText>
<sectionHeader confidence="0.979334" genericHeader="method">
3 Tasks and Individual Systems
</sectionHeader>
<bodyText confidence="0.9996108">
Before defining our proposed model that captures
the requirements listed in the previous section, we
introduce the tasks we consider and their indepen-
dently trained systems that we improve using the
joint system. Though the model proposed here is
general and can be extended to several linguistic
phenomena, in this paper, we focus on relations ex-
pressed by verbs and prepositions. This section de-
scribes the tasks, the data sets we used for our exper-
iments and the current state-of-the-art systems for
these tasks.
We use the following sentence as our running ex-
ample to illustrate the phenomena: The company
calculated the price trends on the major stock mar-
kets on Monday.
</bodyText>
<subsectionHeader confidence="0.999658">
3.1 Preposition Relations
</subsectionHeader>
<bodyText confidence="0.999815272727273">
Prepositions indicate a relation between the attach-
ment point of the preposition and its object. As we
have seen, the same preposition can indicate dif-
ferent types of relations. In the literature, the pol-
ysemy of prepositions is addressed by The Prepo-
sition Project1 of Litkowski and Hargraves (2005),
which is a large lexical resource for English that la-
bels prepositions with their sense. This sense inven-
tory formed the basis of the SemEval-2007 task of
preposition word sense disambiguation of Litkowski
and Hargraves (2007). In our example, the first on
</bodyText>
<footnote confidence="0.89016">
1http://www.clres.com/prepositions.html
</footnote>
<bodyText confidence="0.99994430952381">
would be labeled with the sense 8(3) which identifies
the object of the preposition as the topic, while the
second instance would be labeled as 17(8), which
indicates that argument is the day of the occurrence.
The preposition sense inventory, while useful to
identify the fine grained distinctions between prepo-
sition usage, defines a unique sense label for each
preposition by indexing the definitions of the prepo-
sitions in the Oxford Dictionary of English. For ex-
ample, in the phrase at noon, the at would be labeled
with the sense 2(2), while the preposition in I will
see you in an hour will be labeled 4(3). Note that
both these (and also the second on in our running ex-
ample) indicate a temporal relation, but are assigned
different labels based on the preposition. To counter
this problem we collapsed preposition senses that
are semantically similar to define a new label space,
which we refer to as Preposition Roles.
We retrained classifiers for preposition sense for
the new label space. Before describing the prepo-
sition role dataset, we briefly describe the datasets
and the features for the sense problem. The best
performing system at the SemEval-2007 shared task
of preposition sense disambiguation (Ye and Bald-
win (2007)) achieves a mean precision of 69.3% for
predicting the fine grained senses. Tratz and Hovy
(2009) and Hovy et al. (2010) attained significant
improvements in performance using features derived
from the preposition’s neighbors in the parse tree.
We extended the feature set defined in the former
for our independent system. Table 1 summarizes the
rules for identifying the syntactically related words
for each preposition. We used dependencies from
the easy-first dependency parser of Goldberg and El-
hadad (2010).
For each word extracted from these rules, the fea-
tures include the word itself, its lemma, the POS
tag, synonyms and hypernyms of the first WordNet
sense and an indicator for capitalization. These fea-
tures improved the accuracy of sense identification
to 75.1% on the SemEval test set. In addition, we
also added the following new features for each word:
</bodyText>
<listItem confidence="0.993304">
1. Indicators for gerunds and nominalizations of
verbs.
2. The named entity tag (Person, Location or Or-
ganization) associated with a word, if any. We
</listItem>
<page confidence="0.990555">
131
</page>
<bodyText confidence="0.8114653">
Id. Feature
Head noun/verb that dominates the
preposition along with its modifiers
Head noun/verb that is dominated by
the preposition along with its modifiers
Subject, negator and object(s) of the
immediately dominating verb
Heads of sibling prepositions
Words withing a window of 5 centered
at the preposition
</bodyText>
<tableCaption confidence="0.68707">
Table 1: Features for preposition relation from Tratz and
Hovy (2009). These rules were used to identify syntacti-
cally related words for each preposition.
</tableCaption>
<bodyText confidence="0.948226206896552">
used the state-of-the-art named entity tagger of
Ratinov and Roth (2009) to label the text.
3. Gazetteer features, which are active if a word is
a part of a phrase that belongs to a gazetteer list.
We used the gazetteer lists which were used
by the NER system. We also used the CBC
word clusters of Pantel and Lin (2002) as ad-
ditional gazetteers and Brown cluster features
as used by Ratinov and Roth (2009) and Koo et
al. (2008).
Dahlmeier et al. (2009) annotated senses for the
prepositions at, for, in, of, on, to and with in the sec-
tions 2-4 and 23 of the Wall Street Journal portion of
the Penn Treebank2. We trained sense classifiers on
both datasets using the Averaged Perceptron algo-
rithm with the one-vs-all scheme using the Learning
Based Java framework of Rizzolo and Roth (2010)3.
Table 2 reports the performance of our sense disam-
biguation systems for the Treebank prepositions.
As mentioned earlier, we collapsed the sense la-
bels onto the newly defined preposition role labels.
Table 3 shows this label set along with frequencies
of the labels in the Treebank dataset. According to
this labeling scheme, the first on in our running ex-
ample will be labeled TOPIC and the second one will
2This dataset does not annotate all prepositions and re-
stricts itself mainly to prepositions that start a Propbank ar-
gument. The data is available at http://nlp.comp.nus.
edu.sg/corpora
</bodyText>
<footnote confidence="0.903445">
3Learning Based Java can be downloaded from http://
cogcomp.cs.illinois.edu.
</footnote>
<table confidence="0.99862175">
Train Test set
Treebank Sec. 23 SemEval
Penn Treebank 61.41 38.22
SemEval 47.00 78.25
</table>
<tableCaption confidence="0.921098">
Table 2: Preposition sense performance. This table re-
ports accuracy of sense prediction on the prepositions that
have been annotated for the Penn Treebank dataset.
</tableCaption>
<table confidence="0.99992147826087">
Role Train Test
ACTIVITY 57 23
ATTRIBUTE 119 51
BENEFICIARY 78 17
CAUSE 255 116
CONCOMITANT 156 74
ENDCONDITION 88 66
EXPERIENCER 88 42
INSTRUMENT 37 19
LOCATION 1141 414
MEDIUMOFCOMMUNICATION 39 30
NUMERIC/LEVEL 301 174
OBJECTOFVERB 365 112
OTHER 65 49
PARTWHOLE 485 133
PARTICIPANT/ACCOMPANIER 122 58
PHYSICALSUPPORT 32 18
POSSESSOR 195 56
PROFESSIONALASPECT 24 10
RECIPIENT 150 70
SPECIES 240 58
TEMPORAL 582 270
TOPIC 148 54
</table>
<tableCaption confidence="0.979039">
Table 3: Preposition role data statistics for the Penn Tree-
bank preposition dataset.
</tableCaption>
<bodyText confidence="0.999848833333333">
be labeled TEMPORAL4. We re-trained the sense
disambiguation system to predict preposition roles.
When trained on the Treebank data, our system at-
tains an accuracy of 67.82% on Section 23 of the
Treebank. We use this system as our independent
baseline for preposition role identification.
</bodyText>
<subsectionHeader confidence="0.998434">
3.2 Verb SRL
</subsectionHeader>
<bodyText confidence="0.999938">
The goal of verb Semantic Role Labeling (SRL)
is to identify the predicate-argument structure de-
fined by verbs in sentences. The CoNLL Shared
Tasks of 2004 and 2005 (See Carreras and M`arquez
</bodyText>
<footnote confidence="0.933001333333333">
4The mapping from the preposition senses to the roles de-
fines a new dataset and is available for download at http:
//cogcomp.cs.illinois.edu/.
</footnote>
<page confidence="0.99557">
132
</page>
<bodyText confidence="0.999913155555555">
(2004), Carreras and M`arquez (2005)) studied the
identification of the predicate-argument structure of
verbs using the PropBank corpus of Palmer et al.
(2005). Punyakanok et al. (2008) and Toutanova et
al. (2008) used global inference to ensure that the
predictions across all arguments of the same predi-
cate are coherent. We re-implemented the system of
Punyakanok et al. (2008), which we briefly describe
here, to serve as our baseline verb semantic role la-
beler 5. We refer the reader to the original paper for
further details.
The verb SRL system of Punyakanok et al. (2008)
consists of four stages – candidate generation, argu-
ment identification, argument classification and in-
ference. The candidate generation stage involves us-
ing the heuristic of Xue and Palmer (2004) to gener-
ate an over-complete set of argument candidates for
each predicate. The identification stage uses a clas-
sifier to prune the candidates. In the argument clas-
sification step, the candidates that remain after the
identification step are assigned scores for the SRL
arguments using a multiclass classifier. One of the
labels of the classifier is 0, which indicates that the
candidate is, in fact, not an argument. The inference
step produces a combined prediction for all argu-
ment candidates of a verb proposition by enforcing
global constraints.
The inference enforces the following structural
and linguistic constraints: (1) Each candidate can
have at most one label. (2) No duplicate core argu-
ments. (3) No overlapping or embedding arguments.
(4) Given the predicate, some argument classes are
illegal. (5) If a candidate is labeled as an R-arg,
then there should be one labeled as arg. (6) If a
candidate is labeled as a C-arg, there should be one
labeled arg that occurs before the C-arg.
Instead of using the identifier to filter candidates
for the classifier, in our SRL system, we added
the identifier to the global inference and enforced
consistency constraints between the identifier and
the argument classifier predictions – the identifier
should predict that a candidate is an argument if,
and only if, the argument classifier does not predict
the label 0. This change is in keeping with the idea
of using joint inference to combine independently
</bodyText>
<footnote confidence="0.978407">
5The verb SRL system be downloaded from http://
cogcomp.cs.illinois.edu/page/software
</footnote>
<bodyText confidence="0.999567857142857">
learned systems, in this case, the argument identifier
and the role classifier. Furthermore, we do not need
to explicitly tune the identifier for high recall.
We phrase the inference task as an integer lin-
ear program (ILP) following the approach devel-
oped in Roth and Yih (2004). Integer linear pro-
grams were used by Roth and Yih (2005) to add gen-
eral constraints for inference with conditional ran-
dom fields. ILPs have since been used successfully
in many NLP applications involving complex struc-
tures – Punyakanok et al. (2008) for semantic role
labeling, Riedel and Clarke (2006) and Martins et al.
(2009) for dependency parsing and several others6.
Let vCi,a be the Boolean indicator variable that de-
notes that the ith argument candidate for a predicate
is assigned a label a and let ΘC i,a represent the score
assigned by the argument classifier for this decision.
Similarly, let vIi denote the identifier decision for the
ith argument candidate of the predicate and ΘIi de-
note its identifier score. Then, the objective of infer-
ence is to maximize the total score of the assignment
</bodyText>
<equation confidence="0.994426">
max ΘC i,avC i,a + ΘIi vI (1)
i
vC,vI
i,a i
</equation>
<bodyText confidence="0.999959125">
Here, vC and vI denote all the argument classifier
and identifier variables respectively. This maximiza-
tion is subject to the constraints described above,
which can be transformed to linear (in)equalities.
We denote these constraints as CSRL. In addition
to CSRL which were defined by Punyakanok et al.
(2008), we also have the constraints linking the pre-
dictions of the identifier and classifier:
</bodyText>
<equation confidence="0.987954">
vCv,i,∅ + vIv,i = 1; ∀v, i. (2)
</equation>
<bodyText confidence="0.999506571428571">
Inference in our baseline SRL system is, thus, the
maximization of the objective defined in (1) sub-
ject to constraints CSRL, the identifier-classifier con-
straints defined in (2) and the restriction of the vari-
ables to take values in {0, 1}.
To train the classifiers, we used parse trees from
the Charniak and Johnson (2005) parser with the
</bodyText>
<footnote confidence="0.927157333333333">
6The primary advantage of using ILP for inference is that
this representation enables us to add arbitrary coherence con-
straints between the phenomena. If the underlying optimization
problem itself is tractable, then so is the corresponding integer
program. However, other approaches to solve the constrained
maximization problem can also be used for inference.
</footnote>
<page confidence="0.997661">
133
</page>
<bodyText confidence="0.999859666666667">
same feature representation as in the original sys-
tem. We trained the classifiers on the standard
Propbank training set using the one-vs-all extension
of the average Perceptron algorithm. As with the
preposition roles, we implemented our system using
Learning Based Java of Rizzolo and Roth (2010).
We normalized all classifier scores using the soft-
max function. Compared to the 76.29% F1 score
reported by Punyakanok et al. (2008) using single
parse tree predictions from the parser, our system
obtained 76.22% F1 score on section 23 of the Penn
Treebank.
</bodyText>
<sectionHeader confidence="0.969277" genericHeader="method">
4 A Joint Model for Verbs and
Prepositions
</sectionHeader>
<bodyText confidence="0.999697842105263">
We now introduce our model that captures the needs
identified in Section 2. The approach we develop
in this paper follows the one proposed by Roth and
Yih (2004) of training individual models and com-
bining them at inference time. Our joint model
is a Constrained Conditional Model (See Chang et
al. (2011)), which allows us to build upon existing
learned models using declarative constraints.
We represent our component inference problems
as integer linear program instances. As we saw in
Section 3.2, the inference for SRL is instantiated as
an ILP problem. The problem of predicting prepo-
sition roles can be easily transformed into an ILP
instance. Let vRp,r denote the decision variable that
encodes the prediction that the preposition p is as-
signed a role r and let ΘRp,r denote its score. Let
vR denote all the role variables for a sentence. Then
role prediction is equivalent to the following maxi-
mization problem:
</bodyText>
<equation confidence="0.9956915">
E
max ΘRp,r · vR (3)
p,r
VR p,r
subj. to E vR p,r = 1, bp (4)
r
R
vp ,r E 10, 11, bp, r. (5)
</equation>
<bodyText confidence="0.999163476190476">
In general, let p denote a linguistic structure pre-
diction task of interest and let P denote all such
tasks. Let Zp denote the set of labels that the parts
of the structure associated with phenomenon p can
take. For example, for the SRL argument classifica-
tion component, the parts of the structure are all the
candidates that need to be labeled for a given sen-
tence and the set Zp is the set of all argument labels.
For each phenomenon p E P, we use vp to denote
its set of inference variables for a given sentence.
Each inference variable vpZ,y E vp corresponds to
the prediction that the part y has the label Z in the
final structure. Each variable is associated with a
score ΘpZ,y that is obtained from a learned score pre-
dictor. Let Cp denote the structural constraints that
are “local” to the phenomenon. Thus, for verb SRL,
these would be the constraints defined in the previ-
ous section, and for preposition role, the only local
constraint would be the constraint (4) defined above.
The independent inference problem for the phe-
nomenon p is the following integer program:
</bodyText>
<equation confidence="0.9972392">
vp p ( )
Z,y Θ 6
Z,y,
subj. to Cp(vp), (7)
vpZ,y E 10, 11, bvpZ,y. (8)
</equation>
<bodyText confidence="0.9999475">
As a technical point, this defines one inference
problem per sentence, rather than per predicate
as in the verb SRL system of Punyakanok et al.
(2008). This simple extension enabled Surdeanu et
al. (2007) to study the impact of incorporating cross-
predicate constraints for verb SRL. In this work, this
extension allows us to incorporate cross-phenomena
inference.
</bodyText>
<subsectionHeader confidence="0.992865">
4.1 Joint inference
</subsectionHeader>
<bodyText confidence="0.981602">
We consider the problem of jointly predicting sev-
eral phenomena incorporating linguistic knowledge
that enforce consistency between the output labels.
Suppose p1 and p2 are two phenomena. If zp1
1 is a la-
bel associated with the former and zp2
</bodyText>
<equation confidence="0.996769">
1 , zp2
2 , · · · are
</equation>
<bodyText confidence="0.995431">
labels associated with the latter, we consider con-
straints of the form
</bodyText>
<equation confidence="0.8913145">
zp1 (9)
1 �zp2
1 V zp2
2 V ··· V zp2 n
</equation>
<bodyText confidence="0.999668714285714">
We expand this language of constraints by allowing
the specification of pre-conditions for a constraint to
apply. This allows us to enforce constraints of the
form “If an argument that starts with the preposi-
tion ‘at’ is labeled AM-TMP, then the preposition
can be labeled either NUMERIC/LEVEL or TEMPO-
RAL.” This constraint is universally quantified for
</bodyText>
<figure confidence="0.881432666666667">
E
vP
max
VP
E
Z∈ZP
</figure>
<page confidence="0.993447">
134
</page>
<bodyText confidence="0.836302">
all arguments that satisfy the precondition of start-
ing with the preposition at.
Given a first-order constraint in this form and an
input sentence, suppose the inference variable vp1
1 is
a grounding of zp1
</bodyText>
<equation confidence="0.726967333333333">
1 and vp2
1 , vp2
2 , · · · are groundings
</equation>
<bodyText confidence="0.999737666666667">
of the right hand labels such that the preconditions
are satisfied, then the constraint can be phrased as
the following linear inequality.
</bodyText>
<equation confidence="0.992793">
X
_v1 +
i
i
</equation>
<bodyText confidence="0.99998988">
In the context of the preposition role and verb
SRL, we consider constraints between labels for a
preposition and SRL argument candidates that begin
with that preposition. This restriction forms the pre-
condition for all the joint constraints considered in
this paper. Since the joint constraints involve only
the labels, they can be derived either manually from
the definition of the tasks or using statistical rela-
tion learning techniques. In addition to mining con-
straints of the form (9), we also use manually spec-
ified joint constraints. The constraints used in our
experiments are described further in Section 5.
In general, let J denote a set of pairwise joint
constraints. The joint inference problem can be
phrased as that of maximizing the score of the as-
signment subject to the structural constraints of each
phenomenon (Cp) and the joint linguistic constraints
(J). However, since, the individual tasks were not
trained on the same datasets, the scoring functions
need not be in the same numeric scale. In our model,
each label Z for a phenomenon p is associated with a
scoring function ΘpZ,y for a part y. To scale the scor-
ing functions, we associate each label with a param-
eter ApZ. This gives us the following integer linear
program for joint inference:
</bodyText>
<equation confidence="0.8598285">
vpZ,y · ΘZ,y! , (10)
subj. to Cp(vp), bp E P (11)
J(v), (12)
vpZ,y E {0, 11, bvpZ,y. (13)
</equation>
<bodyText confidence="0.999509">
Here, v is the vector of inference variables which
is obtained by stacking all the inference variables of
each phenomena.
For our experiments, we use a cutting plane solver
to solve the integer linear program as in Riedel
(2009). This allows us to solve the inference prob-
lem without explicitly having to instantiate all the
joint constraints.
</bodyText>
<subsectionHeader confidence="0.995045">
4.2 Learning to rescale the individual systems
</subsectionHeader>
<bodyText confidence="0.999958388888889">
Given the individual models and the constraints, we
only need to learn the scaling parameters ApZ. Note
that the number of scaling parameters is the total
number of labels. When we jointly predict verb SRL
and preposition role, we have 22 preposition roles
(from table 3), one SRL identifier label and 54 SRL
argument classifier labels. Thus we learn only 77
parameters for our joint model. This means that we
only need a very small dataset that is jointly anno-
tated with all the phenomena.
We use the Structure Perceptron of Collins (2002)
to learn the scaling weights. Note that for learning
the scaling weights, we need each label to be associ-
ated with a real-valued feature. Given an assignment
of the inference variables v, the value of the feature
corresponding to the label Z of task p is given by the
sum of scores of all parts in the structure for p that
have been assigned this label, i.e. P
</bodyText>
<equation confidence="0.60531">
yP
</equation>
<bodyText confidence="0.9980115">
feature is computed for the gold and the predicted
structures and is used for updating the weights.
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999305">
In this section, we describe our experimental setup
and evaluate the performance of our approach. The
research question addressed by the experiments is
the following: Given independently trained systems
for verb SRL and preposition roles, can their per-
formance be improved using joint inference between
the two tasks? To address this, we report the results
of the following two experiments:
</bodyText>
<listItem confidence="0.9153537">
1. First, we compare the joint system against the
baseline systems and with pipelines in both di-
rections. In this setting, both base systems are
trained on the Penn Treebank data.
2. Second, we show that using joint inference can
provide strong a performance gain even when
the underlying systems are trained on different
domains.
In all experiments, we report the F1 measure for
the verb SRL performance using the CoNLL 2005
</listItem>
<equation confidence="0.950616615384615">
vp2
i &gt; 0
P
Ap
Z
yP
P
p∈P
max
V
P
Z∈ZP
vp Z,y·ΘpZ,y. This
</equation>
<page confidence="0.994869">
135
</page>
<bodyText confidence="0.9974685">
evaluation metric and the accuracy for the preposi-
tion role labeling task.
</bodyText>
<subsectionHeader confidence="0.994693">
5.1 Data and Constraints
</subsectionHeader>
<bodyText confidence="0.999990851851852">
For both the verb SRL and preposition roles, we
used the first 500 sentences of section 2 of the Penn
Treebank corpus to train our scaling parameters. For
the first set of experiments, we trained our underly-
ing systems on the rest of the available Penn Tree-
bank training data for each task. For the adaptation
experiment, we train the role classifier on the Se-
mEval data (restricted to the same Treebank prepo-
sitions). In both cases, we report performance on
section 23 of the Treebank.
We mined consistency constraints from the sec-
tions 2, 3 and 4 of the Treebank data. As mentioned
in Section 4.1, we considered joint constraints re-
lating preposition roles to verb argument candidates
that start with the preposition. We identified the fol-
lowing types of constraints: (1) For each preposi-
tion, the set of invalid verb arguments and prepo-
sition roles. (2) For each preposition role, the set
of allowed verb argument labels if the role occurred
more than ten times in the data, and (3) For each
verb argument, the set of allowed preposition roles,
similarly with a support of ten. Note that, while the
constraints were obtained from jointly labeled data,
the constraints could be written down because they
encode linguistic intuition about the labels.
The following is a constraint extracted from the
data, which applies to the preposition with:
</bodyText>
<equation confidence="0.998960571428571">
srlarg(A2) -+ prep-role(ATTRIBUTE)
V prep-role(CAUSE)
V prep-role(INSTRUMENT)
V prep-role(OBJECTOFVERB)
V prep-role(PARTWHOLE)
V prep-role(PARTICIPANT/ACCOMPAINER)
V prep-role(PROFESSIONALASPECT).
</equation>
<bodyText confidence="0.999824533333333">
This constraint says that if any candidate that starts
with with is labeled as an A2, then the preposition
can be labeled only with one of the roles on the right
hand side.
Some of the mined constraints have negated vari-
ables to enforce that a role or an argument label
should not be allowed. These can be similarly con-
verted to linear inequalities. See Rizzolo and Roth
(2010) for a further discussion about converting log-
ical expressions into linear constraints.
In addition to these constraints that were mined
from data, we also enforce the following hand-
written constraints: (1) If the role of a verb at-
tached preposition is labeled TEMPORAL, then there
should be a verb predicate for which this preposi-
tional phrase is labeled AM-TMP. (2) For verb at-
tached prepositions, if the preposition is labeled with
one of ACTIVITY, ENDCONDITION, INSTRUMENT
or PROFESSIONALASPECT, there should be at least
one predicate for which the corresponding preposi-
tional phrase is not labeled 0.
The conversion of the first constraint to a linear
inequality is similar to the earlier cases. For each
of the roles in the second constraint, let r denote a
role variable that assigns the label to some prepo-
sition. Suppose there are n SRL candidates across
all verb predicates begin with that preposition, and
let s1, s2, · · · , sn denote the SRL variables that as-
sign these candidates to the label 0. Then the second
constraint corresponds to the following inequality:
</bodyText>
<equation confidence="0.977196333333333">
n
r + sz &lt; n
z=1
</equation>
<subsectionHeader confidence="0.995932">
5.2 Results of joint learning
</subsectionHeader>
<bodyText confidence="0.999278904761905">
First, we compare our approach to the performance
of the baseline independent systems and to pipelines
in both directions in Table 4. For one pipeline, we
added the prediction of the baseline preposition role
system as an additional feature to both the identifier
and the argument classifier for argument candidates
that start with a preposition. Similarly, for the sec-
ond pipeline, we added the SRL predictions as fea-
tures for prepositions that were the first word of an
SRL argument. In all cases, we performed five-fold
cross validation to train the classifiers.
The results show that both pipelines improve per-
formance. This justifies the need for a joint sys-
tem because the pipeline can improve only one of
the tasks. The last line of the table shows that the
joint inference system improves upon both the base-
lines. We achieve this improvement without retrain-
ing the underlying models, as done in the case of the
pipelines.
On analyzing the output of the systems, we found
that the SRL precision improved by 2.75% but the
</bodyText>
<page confidence="0.995507">
136
</page>
<table confidence="0.999542142857143">
Setting SRL Preposition Role
(F1) (Accuracy)
Baseline SRL 76.22 –
Baseline Prep. – 67.82
Prep. SRL 76.84 –
SRL Prep. – 68.55
Joint inference 77.07 68.39
</table>
<tableCaption confidence="0.760617666666667">
Table 4: Performance of the joint system, compared to
the individual systems and the pipelines. All performance
measures are reported on Section 23 of the Penn Tree-
</tableCaption>
<bodyText confidence="0.936678225806452">
bank. The verb SRL systems were trained on sections
2-21, while the preposition role classifiers were trained
on sections 2-4. For the joint inference system, the scal-
ing parameters were trained on the first 500 sentences of
section 2, which were held out. All the improvements in
this table are statistically significant at the 0.05 level.
recall decreased by 0.98%, contributing to the over-
all F1 improvement. The decrease in recall is due to
the joint hard constraints that prohibit certain assign-
ments to the variables which would have otherwise
been possible. Note that, for a given sentence, even
if the joint constraints affect only a few argument
candidates directly, they can alter the labels of the
other candidates via the “local” SRL constraints.
Consider the following example of the system
output which highlights the effect of the constraints.
(6) Weatherford said market conditions led to the
cancellation of the planned exchange.
The independent preposition role system incor-
rectly identifies the to as a LOCATION. The semantic
role labeling component identifies the phrase to the
cancellation of the planned exchange as the A2 of
the verb led. One of the constraints mined from the
data prohibits the label LOCATION for the preposi-
tion to if the argument it starts is labeled A2. This
forces the system to change the preposition label
to the correct one, namely ENDCONDITION. Both
the independent and the joint systems also label the
preposition of as OBJECTOFVERB, which indicates
that the phrase the planned exchange is the object of
the deverbal noun cancellation.
</bodyText>
<subsectionHeader confidence="0.999657">
5.3 Effect of constraints on adaptation
</subsectionHeader>
<bodyText confidence="0.999844363636364">
Our second experiment compares the performance
of the preposition role classifier that has been trained
on the SemEval dataset with and without joint con-
straints. Note that Table 2 in Section 3, shows
the drop in performance when applying the prepo-
sition sense classifier. We see that the SemEval-
trained preposition role classifier (baseline in the ta-
ble) achieves an accuracy of 53.29% when tested on
the Treebank dataset. Using this classifier jointly
with the verb SRL classifier via joint constraints gets
an improvement of almost 3 percent in accuracy.
</bodyText>
<table confidence="0.98963125">
Setting Preposition Role
(Accuracy)
Baseline 53.29
Joint inference 56.22
</table>
<tableCaption confidence="0.6818936">
Table 5: Performance of the SemEval-trained preposition
role classifier, when tested on the Treebank dataset with
and without joint inference with the verb SRL system.
The improvement, in this case is statistically significant
at the 0.01 level using the sign test.
</tableCaption>
<bodyText confidence="0.9999668">
The primary reason for this improvement, even
without re-training the classifier, is that the con-
straints are defined using only the labels of the sys-
tems. This avoids the standard adaptation problems
of differing vocabularies and unseen features.
</bodyText>
<sectionHeader confidence="0.997032" genericHeader="discussions">
6 Discussion and Related work
</sectionHeader>
<bodyText confidence="0.999965095238095">
Roth and Yih (2004) formulated the problem of ex-
tracting entities and relations as an integer linear
program, allowing them to use global structural con-
straints at inference time even though the component
classifiers were trained independently. In this pa-
per, we use this idea to combine classifiers that were
trained for two different tasks on different datasets
using constraints to encode linguistic knowledge.
In the recent years, we have seen several joint
models that combine two or more NLP tasks . An-
drew et al. (2004) studied verb subcategorization
and sense disambiguation of verbs by treating it as
a problem of learning with partially labeled struc-
tures and proposed to use EM to train the joint
model. Finkel and Manning (2009) modeled the task
of named entity recognition together with parsing.
Meza-Ruiz and Riedel (2009) modeled verb SRL,
predicate identification and predicate sense recogni-
tion jointly using Markov Logic. Henderson et al.
(2008) was designed for jointly learning to predict
syntactic and semantic dependencies. Dahlmeier et
</bodyText>
<page confidence="0.993242">
137
</page>
<bodyText confidence="0.99977274">
al. (2009) addressed the problem of jointly learning
verb SRL and preposition sense using the Penn Tree-
bank annotation that was introduced in that work.
The key difference between these and the model
presented in this paper lies in the simplicity of our
model and its easy extensibility because it leverages
existing trained systems. Moreover, our model has
the advantage that the complexity of the joint param-
eters is small, hence does not require a large jointly
labeled dataset to train the scaling parameters.
Our approach is conceptually similar to that of
Rush et al. (2010), which combined separately
trained models by enforcing agreement using global
inference and solving its linear programming relax-
ation. They applied this idea to jointly predict de-
pendency and phrase structure parse trees and on the
task of predicting full parses together with part-of-
speech tags. The main difference in our approach is
that we treat the scaling problem as a separate learn-
ing problem in itself and train a joint model specifi-
cally for re-scaling the output of the trained systems.
The SRL combination system of Surdeanu et al.
(2007) studied the combination of three different
SRL systems using constraints and also by training
secondary scoring functions over the individual sys-
tems. Their approach is similar to the one presented
in this paper in that, unlike standard reranking, as
in Collins (2000), we entertain all possible solutions
during inference, while reranking approaches train
a discriminative scorer for the top-K solutions of
an underlying system. Unlike the SRL combination
system, however, our approach spans multiple phe-
nomena. Moreover, in contrast to their re-scoring
approaches, we do not define joint features drawn
from the predictions of the underlying components
to define our global model.
We consider the tasks verb SRL and preposition
roles and combine their predictions to provide a
richer semantic annotation of text. This approach
can be easily extended to include systems that pre-
dict structures for other linguistic phenomena be-
cause we do not retrain the underlying systems. The
semantic relations can be enriched by incorporating
more linguistic phenomena such as nominal SRL,
defined by the Nombank annotation scheme of Mey-
ers et al. (2004), the preposition function analysis
of O’Hara and Wiebe (2009) and noun compound
analysis as defined by Girju (2007) and Girju et al.
(2009) and others. This presents an exciting direc-
tion for future work.
</bodyText>
<sectionHeader confidence="0.996411" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999988176470588">
This paper presents a strategy for extending seman-
tic role labeling without the need for extensive re-
training or data annotation. While standard seman-
tic role labeling focuses on verb and nominal re-
lations, sentences can express relations using other
lexical items also. Moreover, the different relations
interact with each other and constrain the possible
structures that they can take. We use this intuition
to define a joint model for inference. We instanti-
ate our model using verb semantic role labeling and
preposition role labeling and show that, using lin-
guistic constraints between the tasks and minimal
joint learning, we can improve the performance of
both tasks. The main advantage of our approach
is that we can use existing trained models without
re-training them, thus making it easy to extend this
work to include other linguistic phenomena.
</bodyText>
<sectionHeader confidence="0.997237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999973333333333">
The authors thank the members of the Cognitive
Computation Group at the University of Illinois for
insightful discussions and the anonymous reviewers
for valuable feedback.
This research is supported by the Defense Ad-
vanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-
09-C-0181. Any opinions, ndings, and conclusion
or recommendations expressed in this material are
those of the authors and do not necessarily reect the
view of the DARPA, AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990909">
G. Andrew, T. Grenager, and C. D. Manning. 2004.
Verb sense and subcategorization: Using joint infer-
ence to improve performance on complementary tasks.
In Proceedings of EMNLP.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 shared tasks: Semantic role labeling. In
Proceedings of CoNLL-2004.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-2005.
</reference>
<page confidence="0.980847">
138
</page>
<reference confidence="0.999911233009709">
M. Chang, L. Ratinov, and D. Roth. 2011. Structured
learning with constrained conditional models. Ma-
chine Learning (To appear).
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In ICML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources and
Evaluation.
R. Girju. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In ACL.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In NAACL.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A
latent variable model of synchronous parsing for syn-
tactic and semantic dependencies. In CoNLL.
D. Hovy, S. Tratz, and E. Hovy. 2010. What’s in a prepo-
sition? dimensions of sense disambiguation for an in-
teresting word class. In Coling 2010: Posters.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
K. Litkowski and O. Hargraves. 2005. The preposition
project. In Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Preposi-
tions and their Use in Computational Linguistics For-
malisms and Applications.
K. Litkowski and O. Hargraves. 2007. Semeval-2007
task 06: Word-sense disambiguation of prepositions.
In SemEval-2007: 4th International Workshop on Se-
mantic Evaluations.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation.
I Meza-Ruiz and S. Riedel. 2009. Jointly identifying
predicates, arguments and senses using markov logic.
In NAACL.
T. O’Hara and J. Wiebe. 2009. Exploiting semantic role
resources for preposition disambiguation. Computa-
tional Linguistics, 35(2), June.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In The Eighth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJ-
CAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
S. Riedel. 2009. Cutting plane map inference for markov
logic. In SRL 2009.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Language Re-
sources and Evaluation.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
CoNLL.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP. Association for Computational Linguistics.
M. Surdeanu, L. M`arquez, X. Carreras, and P. R. Comas.
2007. Combination strategies for semantic role label-
ing. J. Artif. Int. Res., 29:105–151, June.
K. Toutanova, A. Haghighi, and C. D. Manning. 2008. A
global joint model for semantic role labeling. Compu-
tational Linguistics, 34(2).
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features.
In NAACL: Student Research Workshop and Doctoral
Consortium.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP.
P. Ye and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Features.
In SemEval-2007.
</reference>
<page confidence="0.998864">
139
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.777864">
<title confidence="0.999789">A Joint Model for Extended Semantic Role Labeling</title>
<author confidence="0.981844">Vivek Srikumar</author>
<author confidence="0.981844">Dan</author>
<affiliation confidence="0.999998">University of Illinois,</affiliation>
<address confidence="0.988134">Urbana, IL</address>
<abstract confidence="0.996075703703704">However, sentences express semantic relations through other linguistic phenomena. For example, consider the following sentence: (1) The field goal by Brien changed the game in the fourth quarter. Verb centered semantic role labeling would identify arguments of the predicate (a) goal by Brien the causer of the change), game the thing changing), and (c) fourth quarter modifier). However, this does not tell us that the scorer of the field goal Brien, which is expressed by the preposition note that the a temporal relation, which overlaps with the verb’s analysis. In this paper, we propose an extension of the standard semantic role labeling task to include relations expressed by lexical items other than verbs and nominalizations. Further, we argue that there are interactions between the different phenomena which suggest that there is a benefit in studying them together. However, one key challenge is that large jointly labeled corpora do not exist. This motivates the need for novel learning and inference schemes that address the data problem and can still benefit from the interactions among the phenomena. This paper has two main contributions. 1. From the machine learning standpoint, we proa joint inference scheme to combine existpredictors for multiple linguistic phenomena. We do so using hard constraints that involve only the labels of the phenomena. The strength of our model is that it is easily Abstract This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>Verb sense and subcategorization: Using joint inference to improve performance on complementary tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="35250" citStr="Andrew et al. (2004)" startWordPosition="5786" endWordPosition="5790">ptation problems of differing vocabularies and unseen features. 6 Discussion and Related work Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et 137 al. (2009) addressed the problem of jointly learning verb SRL a</context>
</contexts>
<marker>Andrew, Grenager, Manning, 2004</marker>
<rawString>G. Andrew, T. Grenager, and C. D. Manning. 2004. Verb sense and subcategorization: Using joint inference to improve performance on complementary tasks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared tasks: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>X. Carreras and L. M`arquez. 2004. Introduction to the CoNLL-2004 shared tasks: Semantic role labeling. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Structured learning with constrained conditional models.</title>
<date>2011</date>
<journal>Machine Learning</journal>
<note>(To appear).</note>
<contexts>
<context position="20386" citStr="Chang et al. (2011)" startWordPosition="3267" endWordPosition="3270">Roth (2010). We normalized all classifier scores using the softmax function. Compared to the 76.29% F1 score reported by Punyakanok et al. (2008) using single parse tree predictions from the parser, our system obtained 76.22% F1 score on section 23 of the Penn Treebank. 4 A Joint Model for Verbs and Prepositions We now introduce our model that captures the needs identified in Section 2. The approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at inference time. Our joint model is a Constrained Conditional Model (See Chang et al. (2011)), which allows us to build upon existing learned models using declarative constraints. We represent our component inference problems as integer linear program instances. As we saw in Section 3.2, the inference for SRL is instantiated as an ILP problem. The problem of predicting preposition roles can be easily transformed into an ILP instance. Let vRp,r denote the decision variable that encodes the prediction that the preposition p is assigned a role r and let ΘRp,r denote its score. Let vR denote all the role variables for a sentence. Then role prediction is equivalent to the following maximi</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2011</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2011. Structured learning with constrained conditional models. Machine Learning (To appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19099" citStr="Charniak and Johnson (2005)" startWordPosition="3060" endWordPosition="3063">constraints described above, which can be transformed to linear (in)equalities. We denote these constraints as CSRL. In addition to CSRL which were defined by Punyakanok et al. (2008), we also have the constraints linking the predictions of the identifier and classifier: vCv,i,∅ + vIv,i = 1; ∀v, i. (2) Inference in our baseline SRL system is, thus, the maximization of the objective defined in (1) subject to constraints CSRL, the identifier-classifier constraints defined in (2) and the restriction of the variables to take values in {0, 1}. To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the 6The primary advantage of using ILP for inference is that this representation enables us to add arbitrary coherence constraints between the phenomena. If the underlying optimization problem itself is tractable, then so is the corresponding integer program. However, other approaches to solve the constrained maximization problem can also be used for inference. 133 same feature representation as in the original system. We trained the classifiers on the standard Propbank training set using the one-vs-all extension of the average Perceptron algorithm. As with the preposition roles,</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="37187" citStr="Collins (2000)" startWordPosition="6098" endWordPosition="6099">ure parse trees and on the task of predicting full parses together with part-ofspeech tags. The main difference in our approach is that we treat the scaling problem as a separate learning problem in itself and train a joint model specifically for re-scaling the output of the trained systems. The SRL combination system of Surdeanu et al. (2007) studied the combination of three different SRL systems using constraints and also by training secondary scoring functions over the individual systems. Their approach is similar to the one presented in this paper in that, unlike standard reranking, as in Collins (2000), we entertain all possible solutions during inference, while reranking approaches train a discriminative scorer for the top-K solutions of an underlying system. Unlike the SRL combination system, however, our approach spans multiple phenomena. Moreover, in contrast to their re-scoring approaches, we do not define joint features drawn from the predictions of the underlying components to define our global model. We consider the tasks verb SRL and preposition roles and combine their predictions to provide a richer semantic annotation of text. This approach can be easily extended to include syste</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="26071" citStr="Collins (2002)" startWordPosition="4270" endWordPosition="4271">all the joint constraints. 4.2 Learning to rescale the individual systems Given the individual models and the constraints, we only need to learn the scaling parameters ApZ. Note that the number of scaling parameters is the total number of labels. When we jointly predict verb SRL and preposition role, we have 22 preposition roles (from table 3), one SRL identifier label and 54 SRL argument classifier labels. Thus we learn only 77 parameters for our joint model. This means that we only need a very small dataset that is jointly annotated with all the phenomena. We use the Structure Perceptron of Collins (2002) to learn the scaling weights. Note that for learning the scaling weights, we need each label to be associated with a real-valued feature. Given an assignment of the inference variables v, the value of the feature corresponding to the label Z of task p is given by the sum of scores of all parts in the structure for p that have been assigned this label, i.e. P yP feature is computed for the gold and the predicted structures and is used for updating the weights. 5 Experiments In this section, we describe our experimental setup and evaluate the performance of our approach. The research question a</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
<author>T Schultz</author>
</authors>
<title>Joint learning of preposition senses and semantic roles of prepositional phrases.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12475" citStr="Dahlmeier et al. (2009)" startWordPosition="1982" endWordPosition="1985">e preposition Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2. We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3. Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along with frequencies of the lab</context>
</contexts>
<marker>Dahlmeier, Ng, Schultz, 2009</marker>
<rawString>D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint learning of preposition senses and semantic roles of prepositional phrases. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="35464" citStr="Finkel and Manning (2009)" startWordPosition="5822" endWordPosition="5825">ing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et 137 al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy exte</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Joint parsing and named entity recognition. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>P Nakov</author>
<author>V Nastase</author>
<author>S Szpakowicz</author>
<author>P Turney</author>
<author>D Yuret</author>
</authors>
<title>Classification of semantic relations between nominals. Language Resources and Evaluation.</title>
<date>2009</date>
<contexts>
<context position="38202" citStr="Girju et al. (2009)" startWordPosition="6253" endWordPosition="6256"> our global model. We consider the tasks verb SRL and preposition roles and combine their predictions to provide a richer semantic annotation of text. This approach can be easily extended to include systems that predict structures for other linguistic phenomena because we do not retrain the underlying systems. The semantic relations can be enriched by incorporating more linguistic phenomena such as nominal SRL, defined by the Nombank annotation scheme of Meyers et al. (2004), the preposition function analysis of O’Hara and Wiebe (2009) and noun compound analysis as defined by Girju (2007) and Girju et al. (2009) and others. This presents an exciting direction for future work. 7 Conclusion This paper presents a strategy for extending semantic role labeling without the need for extensive retraining or data annotation. While standard semantic role labeling focuses on verb and nominal relations, sentences can express relations using other lexical items also. Moreover, the different relations interact with each other and constrain the possible structures that they can take. We use this intuition to define a joint model for inference. We instantiate our model using verb semantic role labeling and prepositi</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2009</marker>
<rawString>R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret. 2009. Classification of semantic relations between nominals. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Improving the interpretation of noun phrases with cross-linguistic information.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="38178" citStr="Girju (2007)" startWordPosition="6250" endWordPosition="6251">ponents to define our global model. We consider the tasks verb SRL and preposition roles and combine their predictions to provide a richer semantic annotation of text. This approach can be easily extended to include systems that predict structures for other linguistic phenomena because we do not retrain the underlying systems. The semantic relations can be enriched by incorporating more linguistic phenomena such as nominal SRL, defined by the Nombank annotation scheme of Meyers et al. (2004), the preposition function analysis of O’Hara and Wiebe (2009) and noun compound analysis as defined by Girju (2007) and Girju et al. (2009) and others. This presents an exciting direction for future work. 7 Conclusion This paper presents a strategy for extending semantic role labeling without the need for extensive retraining or data annotation. While standard semantic role labeling focuses on verb and nominal relations, sentences can express relations using other lexical items also. Moreover, the different relations interact with each other and constrain the possible structures that they can take. We use this intuition to define a joint model for inference. We instantiate our model using verb semantic rol</context>
</contexts>
<marker>Girju, 2007</marker>
<rawString>R. Girju. 2007. Improving the interpretation of noun phrases with cross-linguistic information. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="11043" citStr="Goldberg and Elhadad (2010)" startWordPosition="1740" endWordPosition="1744">e best performing system at the SemEval-2007 shared task of preposition sense disambiguation (Ye and Baldwin (2007)) achieves a mean precision of 69.3% for predicting the fine grained senses. Tratz and Hovy (2009) and Hovy et al. (2010) attained significant improvements in performance using features derived from the preposition’s neighbors in the parse tree. We extended the feature set defined in the former for our independent system. Table 1 summarizes the rules for identifying the syntactically related words for each preposition. We used dependencies from the easy-first dependency parser of Goldberg and Elhadad (2010). For each word extracted from these rules, the features include the word itself, its lemma, the POS tag, synonyms and hypernyms of the first WordNet sense and an indicator for capitalization. These features improved the accuracy of sense identification to 75.1% on the SemEval test set. In addition, we also added the following new features for each word: 1. Indicators for gerunds and nominalizations of verbs. 2. The named entity tag (Person, Location or Organization) associated with a word, if any. We 131 Id. Feature Head noun/verb that dominates the preposition along with its modifiers Head n</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Y. Goldberg and M. Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>P Merlo</author>
<author>G Musillo</author>
<author>I Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="35687" citStr="Henderson et al. (2008)" startWordPosition="5854" endWordPosition="5857">s on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et 137 al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that the complexity of the joint parameters is small, hence does not require a large jointly labeled dataset to train the scalin</context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hovy</author>
<author>S Tratz</author>
<author>E Hovy</author>
</authors>
<title>What’s in a preposition? dimensions of sense disambiguation for an interesting word class. In Coling</title>
<date>2010</date>
<publisher>Posters.</publisher>
<contexts>
<context position="10652" citStr="Hovy et al. (2010)" startWordPosition="1684" endWordPosition="1687">abels based on the preposition. To counter this problem we collapsed preposition senses that are semantically similar to define a new label space, which we refer to as Preposition Roles. We retrained classifiers for preposition sense for the new label space. Before describing the preposition role dataset, we briefly describe the datasets and the features for the sense problem. The best performing system at the SemEval-2007 shared task of preposition sense disambiguation (Ye and Baldwin (2007)) achieves a mean precision of 69.3% for predicting the fine grained senses. Tratz and Hovy (2009) and Hovy et al. (2010) attained significant improvements in performance using features derived from the preposition’s neighbors in the parse tree. We extended the feature set defined in the former for our independent system. Table 1 summarizes the rules for identifying the syntactically related words for each preposition. We used dependencies from the easy-first dependency parser of Goldberg and Elhadad (2010). For each word extracted from these rules, the features include the word itself, its lemma, the POS tag, synonyms and hypernyms of the first WordNet sense and an indicator for capitalization. These features i</context>
</contexts>
<marker>Hovy, Tratz, Hovy, 2010</marker>
<rawString>D. Hovy, S. Tratz, and E. Hovy. 2010. What’s in a preposition? dimensions of sense disambiguation for an interesting word class. In Coling 2010: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12450" citStr="Koo et al. (2008)" startWordPosition="1978" endWordPosition="1981">of 5 centered at the preposition Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2. We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3. Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along wi</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
<author>O Hargraves</author>
</authors>
<title>The preposition project.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications.</booktitle>
<contexts>
<context position="8993" citStr="Litkowski and Hargraves (2005)" startWordPosition="1414" endWordPosition="1417">ositions. This section describes the tasks, the data sets we used for our experiments and the current state-of-the-art systems for these tasks. We use the following sentence as our running example to illustrate the phenomena: The company calculated the price trends on the major stock markets on Monday. 3.1 Preposition Relations Prepositions indicate a relation between the attachment point of the preposition and its object. As we have seen, the same preposition can indicate different types of relations. In the literature, the polysemy of prepositions is addressed by The Preposition Project1 of Litkowski and Hargraves (2005), which is a large lexical resource for English that labels prepositions with their sense. This sense inventory formed the basis of the SemEval-2007 task of preposition word sense disambiguation of Litkowski and Hargraves (2007). In our example, the first on 1http://www.clres.com/prepositions.html would be labeled with the sense 8(3) which identifies the object of the preposition as the topic, while the second instance would be labeled as 17(8), which indicates that argument is the day of the occurrence. The preposition sense inventory, while useful to identify the fine grained distinctions be</context>
</contexts>
<marker>Litkowski, Hargraves, 2005</marker>
<rawString>K. Litkowski and O. Hargraves. 2005. The preposition project. In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
<author>O Hargraves</author>
</authors>
<title>Semeval-2007 task 06: Word-sense disambiguation of prepositions.</title>
<date>2007</date>
<booktitle>In SemEval-2007: 4th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="9221" citStr="Litkowski and Hargraves (2007)" startWordPosition="1450" endWordPosition="1453">e company calculated the price trends on the major stock markets on Monday. 3.1 Preposition Relations Prepositions indicate a relation between the attachment point of the preposition and its object. As we have seen, the same preposition can indicate different types of relations. In the literature, the polysemy of prepositions is addressed by The Preposition Project1 of Litkowski and Hargraves (2005), which is a large lexical resource for English that labels prepositions with their sense. This sense inventory formed the basis of the SemEval-2007 task of preposition word sense disambiguation of Litkowski and Hargraves (2007). In our example, the first on 1http://www.clres.com/prepositions.html would be labeled with the sense 8(3) which identifies the object of the preposition as the topic, while the second instance would be labeled as 17(8), which indicates that argument is the day of the occurrence. The preposition sense inventory, while useful to identify the fine grained distinctions between preposition usage, defines a unique sense label for each preposition by indexing the definitions of the prepositions in the Oxford Dictionary of English. For example, in the phrase at noon, the at would be labeled with the</context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>K. Litkowski and O. Hargraves. 2007. Semeval-2007 task 06: Word-sense disambiguation of prepositions. In SemEval-2007: 4th International Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Martins</author>
<author>N A Smith</author>
<author>E Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="17823" citStr="Martins et al. (2009)" startWordPosition="2842" endWordPosition="2845">age/software learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6. Let vCi,a be the Boolean indicator variable that denotes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let vIi denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment max ΘC i,avC i,a + ΘIi vI (1) i vC,vI i,a i Here, vC and vI denote all the argument classifier and identifier variables r</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. Martins, N. A. Smith, and E. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="3116" citStr="Meyers et al. (2004)" startWordPosition="484" endWordPosition="487">predictions, we show improvements in the performance of both tasks without retraining the individual models. 1 Introduction The identification of semantic relations between sentence constituents has been an important task in NLP research. It finds applications in various natural language understanding tasks that require complex inference going beyond the surface representation. In the literature, semantic role extraction has been studied mostly in the context of verb predicates, using the Propbank annotation of Palmer et al. (2005), and also for nominal predicates, using the Nombank corpus of Meyers et al. (2004). 129 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 129–139, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics extensible, since adding new phenomena does not require fully retraining the joint model from scratch. Furthermore, our approach minimizes the need for extensive jointly labeled corpora and, instead, uses existing predictors as black boxes. 2. From an NLP perspective, we motivate the extension of semantic role labeling beyond verbs and nominalizations. We instantiate our joint model for the case </context>
<context position="38062" citStr="Meyers et al. (2004)" startWordPosition="6229" endWordPosition="6233">in contrast to their re-scoring approaches, we do not define joint features drawn from the predictions of the underlying components to define our global model. We consider the tasks verb SRL and preposition roles and combine their predictions to provide a richer semantic annotation of text. This approach can be easily extended to include systems that predict structures for other linguistic phenomena because we do not retrain the underlying systems. The semantic relations can be enriched by incorporating more linguistic phenomena such as nominal SRL, defined by the Nombank annotation scheme of Meyers et al. (2004), the preposition function analysis of O’Hara and Wiebe (2009) and noun compound analysis as defined by Girju (2007) and Girju et al. (2009) and others. This presents an exciting direction for future work. 7 Conclusion This paper presents a strategy for extending semantic role labeling without the need for extensive retraining or data annotation. While standard semantic role labeling focuses on verb and nominal relations, sentences can express relations using other lexical items also. Moreover, the different relations interact with each other and constrain the possible structures that they can</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank project: An interim report. In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Meza-Ruiz</author>
<author>S Riedel</author>
</authors>
<title>Jointly identifying predicates, arguments and senses using markov logic.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="35560" citStr="Meza-Ruiz and Riedel (2009)" startWordPosition="5836" endWordPosition="5839">sifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et 137 al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage t</context>
</contexts>
<marker>Meza-Ruiz, Riedel, 2009</marker>
<rawString>I Meza-Ruiz and S. Riedel. 2009. Jointly identifying predicates, arguments and senses using markov logic. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T O’Hara</author>
<author>J Wiebe</author>
</authors>
<title>Exploiting semantic role resources for preposition disambiguation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>2</issue>
<marker>O’Hara, Wiebe, 2009</marker>
<rawString>T. O’Hara and J. Wiebe. 2009. Exploiting semantic role resources for preposition disambiguation. Computational Linguistics, 35(2), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>P Kingsbury</author>
<author>D Gildea</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="3033" citStr="Palmer et al. (2005)" startWordPosition="470" endWordPosition="473">ture predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models. 1 Introduction The identification of semantic relations between sentence constituents has been an important task in NLP research. It finds applications in various natural language understanding tasks that require complex inference going beyond the surface representation. In the literature, semantic role extraction has been studied mostly in the context of verb predicates, using the Propbank annotation of Palmer et al. (2005), and also for nominal predicates, using the Nombank corpus of Meyers et al. (2004). 129 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 129–139, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics extensible, since adding new phenomena does not require fully retraining the joint model from scratch. Furthermore, our approach minimizes the need for extensive jointly labeled corpora and, instead, uses existing predictors as black boxes. 2. From an NLP perspective, we motivate the extension of semantic role labe</context>
<context position="15048" citStr="Palmer et al. (2005)" startWordPosition="2392" endWordPosition="2395">tion 23 of the Treebank. We use this system as our independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. 132 (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5. We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue </context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>M. Palmer, P. Kingsbury, and D. Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In The Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="12341" citStr="Pantel and Lin (2002)" startWordPosition="1958" endWordPosition="1961">t, negator and object(s) of the immediately dominating verb Heads of sibling prepositions Words withing a window of 5 centered at the preposition Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2. We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3. Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we col</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In The Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning and inference over constrained output.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="7880" citStr="Punyakanok et al. (2005)" startWordPosition="1234" endWordPosition="1237">he-art models with minimal use of jointly labeled data, which is expensive to obtain. Systems that are trained on each task independently do not account for the interplay between them. One approach for tackling this is to define pipelines, where the predictions for one of the tasks acts as the input for another. However, a pipeline does not capture the two-way dependency between the tasks. Training a fully joint model from scratch is also unrealistic because it requires text that is annotated with all the tasks, thus making joint training implausible from a learning theoretic perspective (See Punyakanok et al. (2005) for a discussion about the learning theoretic requirements of joint training.) 3 Tasks and Individual Systems Before defining our proposed model that captures the requirements listed in the previous section, we introduce the tasks we consider and their independently trained systems that we improve using the joint system. Though the model proposed here is general and can be extended to several linguistic phenomena, in this paper, we focus on relations expressed by verbs and prepositions. This section describes the tasks, the data sets we used for our experiments and the current state-of-the-ar</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learning and inference over constrained output. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="15074" citStr="Punyakanok et al. (2008)" startWordPosition="2396" endWordPosition="2399">k. We use this system as our independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. 132 (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5. We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to gener</context>
<context position="17744" citStr="Punyakanok et al. (2008)" startWordPosition="2829" endWordPosition="2832">endently 5The verb SRL system be downloaded from http:// cogcomp.cs.illinois.edu/page/software learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6. Let vCi,a be the Boolean indicator variable that denotes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let vIi denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment max ΘC i,avC i,a + ΘIi vI (1) i vC,vI i,a </context>
<context position="19912" citStr="Punyakanok et al. (2008)" startWordPosition="3185" endWordPosition="3188">imization problem itself is tractable, then so is the corresponding integer program. However, other approaches to solve the constrained maximization problem can also be used for inference. 133 same feature representation as in the original system. We trained the classifiers on the standard Propbank training set using the one-vs-all extension of the average Perceptron algorithm. As with the preposition roles, we implemented our system using Learning Based Java of Rizzolo and Roth (2010). We normalized all classifier scores using the softmax function. Compared to the 76.29% F1 score reported by Punyakanok et al. (2008) using single parse tree predictions from the parser, our system obtained 76.22% F1 score on section 23 of the Penn Treebank. 4 A Joint Model for Verbs and Prepositions We now introduce our model that captures the needs identified in Section 2. The approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at inference time. Our joint model is a Constrained Conditional Model (See Chang et al. (2011)), which allows us to build upon existing learned models using declarative constraints. We represent our component inference p</context>
<context position="22395" citStr="Punyakanok et al. (2008)" startWordPosition="3633" endWordPosition="3636">re ΘpZ,y that is obtained from a learned score predictor. Let Cp denote the structural constraints that are “local” to the phenomenon. Thus, for verb SRL, these would be the constraints defined in the previous section, and for preposition role, the only local constraint would be the constraint (4) defined above. The independent inference problem for the phenomenon p is the following integer program: vp p ( ) Z,y Θ 6 Z,y, subj. to Cp(vp), (7) vpZ,y E 10, 11, bvpZ,y. (8) As a technical point, this defines one inference problem per sentence, rather than per predicate as in the verb SRL system of Punyakanok et al. (2008). This simple extension enabled Surdeanu et al. (2007) to study the impact of incorporating crosspredicate constraints for verb SRL. In this work, this extension allows us to incorporate cross-phenomena inference. 4.1 Joint inference We consider the problem of jointly predicting several phenomena incorporating linguistic knowledge that enforce consistency between the output labels. Suppose p1 and p2 are two phenomena. If zp1 1 is a label associated with the former and zp2 1 , zp2 2 , · · · are labels associated with the latter, we consider constraints of the form zp1 (9) 1 �zp2 1 V zp2 2 V ···</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="12093" citStr="Ratinov and Roth (2009)" startWordPosition="1909" endWordPosition="1912">ntity tag (Person, Location or Organization) associated with a word, if any. We 131 Id. Feature Head noun/verb that dominates the preposition along with its modifiers Head noun/verb that is dominated by the preposition along with its modifiers Subject, negator and object(s) of the immediately dominating verb Heads of sibling prepositions Words withing a window of 5 centered at the preposition Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2. We trained sense classifiers on both datasets using the Avera</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17797" citStr="Riedel and Clarke (2006)" startWordPosition="2837" endWordPosition="2840">:// cogcomp.cs.illinois.edu/page/software learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6. Let vCi,a be the Boolean indicator variable that denotes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let vIi denote the identifier decision for the ith argument candidate of the predicate and ΘIi denote its identifier score. Then, the objective of inference is to maximize the total score of the assignment max ΘC i,avC i,a + ΘIi vI (1) i vC,vI i,a i Here, vC and vI denote all the argument classifier </context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
</authors>
<title>Cutting plane map inference for markov logic.</title>
<date>2009</date>
<booktitle>In SRL</booktitle>
<contexts>
<context position="25368" citStr="Riedel (2009)" startWordPosition="4152" endWordPosition="4153">s need not be in the same numeric scale. In our model, each label Z for a phenomenon p is associated with a scoring function ΘpZ,y for a part y. To scale the scoring functions, we associate each label with a parameter ApZ. This gives us the following integer linear program for joint inference: vpZ,y · ΘZ,y! , (10) subj. to Cp(vp), bp E P (11) J(v), (12) vpZ,y E {0, 11, bvpZ,y. (13) Here, v is the vector of inference variables which is obtained by stacking all the inference variables of each phenomena. For our experiments, we use a cutting plane solver to solve the integer linear program as in Riedel (2009). This allows us to solve the inference problem without explicitly having to instantiate all the joint constraints. 4.2 Learning to rescale the individual systems Given the individual models and the constraints, we only need to learn the scaling parameters ApZ. Note that the number of scaling parameters is the total number of labels. When we jointly predict verb SRL and preposition role, we have 22 preposition roles (from table 3), one SRL identifier label and 54 SRL argument classifier labels. Thus we learn only 77 parameters for our joint model. This means that we only need a very small data</context>
<context position="35560" citStr="Riedel (2009)" startWordPosition="5838" endWordPosition="5839">rained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem of learning with partially labeled structures and proposed to use EM to train the joint model. Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. Meza-Ruiz and Riedel (2009) modeled verb SRL, predicate identification and predicate sense recognition jointly using Markov Logic. Henderson et al. (2008) was designed for jointly learning to predict syntactic and semantic dependencies. Dahlmeier et 137 al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage t</context>
</contexts>
<marker>Riedel, 2009</marker>
<rawString>S. Riedel. 2009. Cutting plane map inference for markov logic. In SRL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Learning based java for rapid development of nlp systems.</title>
<date>2010</date>
<booktitle>In Language Resources and Evaluation.</booktitle>
<contexts>
<context position="12811" citStr="Rizzolo and Roth (2010)" startWordPosition="2040" endWordPosition="2043">t belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, to and with in the sections 2-4 and 23 of the Wall Street Journal portion of the Penn Treebank2. We trained sense classifiers on both datasets using the Averaged Perceptron algorithm with the one-vs-all scheme using the Learning Based Java framework of Rizzolo and Roth (2010)3. Table 2 reports the performance of our sense disambiguation systems for the Treebank prepositions. As mentioned earlier, we collapsed the sense labels onto the newly defined preposition role labels. Table 3 shows this label set along with frequencies of the labels in the Treebank dataset. According to this labeling scheme, the first on in our running example will be labeled TOPIC and the second one will 2This dataset does not annotate all prepositions and restricts itself mainly to prepositions that start a Propbank argument. The data is available at http://nlp.comp.nus. edu.sg/corpora 3Lea</context>
<context position="19778" citStr="Rizzolo and Roth (2010)" startWordPosition="3163" endWordPosition="3166"> inference is that this representation enables us to add arbitrary coherence constraints between the phenomena. If the underlying optimization problem itself is tractable, then so is the corresponding integer program. However, other approaches to solve the constrained maximization problem can also be used for inference. 133 same feature representation as in the original system. We trained the classifiers on the standard Propbank training set using the one-vs-all extension of the average Perceptron algorithm. As with the preposition roles, we implemented our system using Learning Based Java of Rizzolo and Roth (2010). We normalized all classifier scores using the softmax function. Compared to the 76.29% F1 score reported by Punyakanok et al. (2008) using single parse tree predictions from the parser, our system obtained 76.22% F1 score on section 23 of the Penn Treebank. 4 A Joint Model for Verbs and Prepositions We now introduce our model that captures the needs identified in Section 2. The approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at inference time. Our joint model is a Constrained Conditional Model (See Chang et al</context>
<context position="29449" citStr="Rizzolo and Roth (2010)" startWordPosition="4834" endWordPosition="4837">the data, which applies to the preposition with: srlarg(A2) -+ prep-role(ATTRIBUTE) V prep-role(CAUSE) V prep-role(INSTRUMENT) V prep-role(OBJECTOFVERB) V prep-role(PARTWHOLE) V prep-role(PARTICIPANT/ACCOMPAINER) V prep-role(PROFESSIONALASPECT). This constraint says that if any candidate that starts with with is labeled as an A2, then the preposition can be labeled only with one of the roles on the right hand side. Some of the mined constraints have negated variables to enforce that a role or an argument label should not be allowed. These can be similarly converted to linear inequalities. See Rizzolo and Roth (2010) for a further discussion about converting logical expressions into linear constraints. In addition to these constraints that were mined from data, we also enforce the following handwritten constraints: (1) If the role of a verb attached preposition is labeled TEMPORAL, then there should be a verb predicate for which this prepositional phrase is labeled AM-TMP. (2) For verb attached prepositions, if the preposition is labeled with one of ACTIVITY, ENDCONDITION, INSTRUMENT or PROFESSIONALASPECT, there should be at least one predicate for which the corresponding prepositional phrase is not label</context>
</contexts>
<marker>Rizzolo, Roth, 2010</marker>
<rawString>N. Rizzolo and D. Roth. 2010. Learning based java for rapid development of nlp systems. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="17493" citStr="Roth and Yih (2004)" startWordPosition="2787" endWordPosition="2790">gument classifier predictions – the identifier should predict that a candidate is an argument if, and only if, the argument classifier does not predict the label 0. This change is in keeping with the idea of using joint inference to combine independently 5The verb SRL system be downloaded from http:// cogcomp.cs.illinois.edu/page/software learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6. Let vCi,a be the Boolean indicator variable that denotes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarl</context>
<context position="20242" citStr="Roth and Yih (2004)" startWordPosition="3243" endWordPosition="3246">xtension of the average Perceptron algorithm. As with the preposition roles, we implemented our system using Learning Based Java of Rizzolo and Roth (2010). We normalized all classifier scores using the softmax function. Compared to the 76.29% F1 score reported by Punyakanok et al. (2008) using single parse tree predictions from the parser, our system obtained 76.22% F1 score on section 23 of the Penn Treebank. 4 A Joint Model for Verbs and Prepositions We now introduce our model that captures the needs identified in Section 2. The approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at inference time. Our joint model is a Constrained Conditional Model (See Chang et al. (2011)), which allows us to build upon existing learned models using declarative constraints. We represent our component inference problems as integer linear program instances. As we saw in Section 3.2, the inference for SRL is instantiated as an ILP problem. The problem of predicting preposition roles can be easily transformed into an ILP instance. Let vRp,r denote the decision variable that encodes the prediction that the preposition p is assigned a role r</context>
<context position="34743" citStr="Roth and Yih (2004)" startWordPosition="5704" endWordPosition="5707">on Role (Accuracy) Baseline 53.29 Joint inference 56.22 Table 5: Performance of the SemEval-trained preposition role classifier, when tested on the Treebank dataset with and without joint inference with the verb SRL system. The improvement, in this case is statistically significant at the 0.01 level using the sign test. The primary reason for this improvement, even without re-training the classifier, is that the constraints are defined using only the labels of the systems. This avoids the standard adaptation problems of differing vocabularies and unseen features. 6 Discussion and Related work Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently. In this paper, we use this idea to combine classifiers that were trained for two different tasks on different datasets using constraints to encode linguistic knowledge. In the recent years, we have seen several joint models that combine two or more NLP tasks . Andrew et al. (2004) studied verb subcategorization and sense disambiguation of verbs by treating it as a problem</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="17551" citStr="Roth and Yih (2005)" startWordPosition="2798" endWordPosition="2801">ict that a candidate is an argument if, and only if, the argument classifier does not predict the label 0. This change is in keeping with the idea of using joint inference to combine independently 5The verb SRL system be downloaded from http:// cogcomp.cs.illinois.edu/page/software learned systems, in this case, the argument identifier and the role classifier. Furthermore, we do not need to explicitly tune the identifier for high recall. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). Integer linear programs were used by Roth and Yih (2005) to add general constraints for inference with conditional random fields. ILPs have since been used successfully in many NLP applications involving complex structures – Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others6. Let vCi,a be the Boolean indicator variable that denotes that the ith argument candidate for a predicate is assigned a label a and let ΘC i,a represent the score assigned by the argument classifier for this decision. Similarly, let vIi denote the identifier decision for the ith argu</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36367" citStr="Rush et al. (2010)" startWordPosition="5964" endWordPosition="5967">mantic dependencies. Dahlmeier et 137 al. (2009) addressed the problem of jointly learning verb SRL and preposition sense using the Penn Treebank annotation that was introduced in that work. The key difference between these and the model presented in this paper lies in the simplicity of our model and its easy extensibility because it leverages existing trained systems. Moreover, our model has the advantage that the complexity of the joint parameters is small, hence does not require a large jointly labeled dataset to train the scaling parameters. Our approach is conceptually similar to that of Rush et al. (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. They applied this idea to jointly predict dependency and phrase structure parse trees and on the task of predicting full parses together with part-ofspeech tags. The main difference in our approach is that we treat the scaling problem as a separate learning problem in itself and train a joint model specifically for re-scaling the output of the trained systems. The SRL combination system of Surdeanu et al. (2007) studied the combination of three different SRL s</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>P R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>J. Artif. Int. Res.,</journal>
<pages>29--105</pages>
<marker>Surdeanu, M`arquez, Carreras, Comas, 2007</marker>
<rawString>M. Surdeanu, L. M`arquez, X. Carreras, and P. R. Comas. 2007. Combination strategies for semantic role labeling. J. Artif. Int. Res., 29:105–151, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="15102" citStr="Toutanova et al. (2008)" startWordPosition="2401" endWordPosition="2404">independent baseline for preposition role identification. 3.2 Verb SRL The goal of verb Semantic Role Labeling (SRL) is to identify the predicate-argument structure defined by verbs in sentences. The CoNLL Shared Tasks of 2004 and 2005 (See Carreras and M`arquez 4The mapping from the preposition senses to the roles defines a new dataset and is available for download at http: //cogcomp.cs.illinois.edu/. 132 (2004), Carreras and M`arquez (2005)) studied the identification of the predicate-argument structure of verbs using the PropBank corpus of Palmer et al. (2005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5. We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of </context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>K. Toutanova, A. Haghighi, and C. D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tratz</author>
<author>D Hovy</author>
</authors>
<title>Disambiguation of preposition sense using linguistically motivated features.</title>
<date>2009</date>
<booktitle>In NAACL: Student Research Workshop and Doctoral Consortium.</booktitle>
<contexts>
<context position="10629" citStr="Tratz and Hovy (2009)" startWordPosition="1679" endWordPosition="1682">t are assigned different labels based on the preposition. To counter this problem we collapsed preposition senses that are semantically similar to define a new label space, which we refer to as Preposition Roles. We retrained classifiers for preposition sense for the new label space. Before describing the preposition role dataset, we briefly describe the datasets and the features for the sense problem. The best performing system at the SemEval-2007 shared task of preposition sense disambiguation (Ye and Baldwin (2007)) achieves a mean precision of 69.3% for predicting the fine grained senses. Tratz and Hovy (2009) and Hovy et al. (2010) attained significant improvements in performance using features derived from the preposition’s neighbors in the parse tree. We extended the feature set defined in the former for our independent system. Table 1 summarizes the rules for identifying the syntactically related words for each preposition. We used dependencies from the easy-first dependency parser of Goldberg and Elhadad (2010). For each word extracted from these rules, the features include the word itself, its lemma, the POS tag, synonyms and hypernyms of the first WordNet sense and an indicator for capitaliz</context>
<context position="11935" citStr="Tratz and Hovy (2009)" startWordPosition="1885" endWordPosition="1888">emEval test set. In addition, we also added the following new features for each word: 1. Indicators for gerunds and nominalizations of verbs. 2. The named entity tag (Person, Location or Organization) associated with a word, if any. We 131 Id. Feature Head noun/verb that dominates the preposition along with its modifiers Head noun/verb that is dominated by the preposition along with its modifiers Subject, negator and object(s) of the immediately dominating verb Heads of sibling prepositions Words withing a window of 5 centered at the preposition Table 1: Features for preposition relation from Tratz and Hovy (2009). These rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. 3. Gazetteer features, which are active if a word is a part of a phrase that belongs to a gazetteer list. We used the gazetteer lists which were used by the NER system. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. (2008). Dahlmeier et al. (2009) annotated senses for the prepositions at, for, in, of, on, </context>
</contexts>
<marker>Tratz, Hovy, 2009</marker>
<rawString>S. Tratz and D. Hovy. 2009. Disambiguation of preposition sense using linguistically motivated features. In NAACL: Student Research Workshop and Doctoral Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="15665" citStr="Xue and Palmer (2004)" startWordPosition="2493" endWordPosition="2496">005). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al. (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler 5. We refer the reader to the original paper for further details. The verb SRL system of Punyakanok et al. (2008) consists of four stages – candidate generation, argument identification, argument classification and inference. The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of argument candidates for each predicate. The identification stage uses a classifier to prune the candidates. In the argument classification step, the candidates that remain after the identification step are assigned scores for the SRL arguments using a multiclass classifier. One of the labels of the classifier is 0, which indicates that the candidate is, in fact, not an argument. The inference step produces a combined prediction for all argument candidates of a verb proposition by enforcing global constraints. The inference enforces the following structural </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating features for semantic role labeling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ye</author>
<author>T Baldwin</author>
</authors>
<title>MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features.</title>
<date>2007</date>
<booktitle>In SemEval-2007.</booktitle>
<contexts>
<context position="10531" citStr="Ye and Baldwin (2007)" startWordPosition="1662" endWordPosition="1666">e that both these (and also the second on in our running example) indicate a temporal relation, but are assigned different labels based on the preposition. To counter this problem we collapsed preposition senses that are semantically similar to define a new label space, which we refer to as Preposition Roles. We retrained classifiers for preposition sense for the new label space. Before describing the preposition role dataset, we briefly describe the datasets and the features for the sense problem. The best performing system at the SemEval-2007 shared task of preposition sense disambiguation (Ye and Baldwin (2007)) achieves a mean precision of 69.3% for predicting the fine grained senses. Tratz and Hovy (2009) and Hovy et al. (2010) attained significant improvements in performance using features derived from the preposition’s neighbors in the parse tree. We extended the feature set defined in the former for our independent system. Table 1 summarizes the rules for identifying the syntactically related words for each preposition. We used dependencies from the easy-first dependency parser of Goldberg and Elhadad (2010). For each word extracted from these rules, the features include the word itself, its le</context>
</contexts>
<marker>Ye, Baldwin, 2007</marker>
<rawString>P. Ye and T. Baldwin. 2007. MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features. In SemEval-2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>