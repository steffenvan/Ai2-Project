<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005192">
<title confidence="0.98467">
Hypertagging: Supertagging for Surface Realization with CCG
</title>
<author confidence="0.997498">
Dominic Espinosa and Michael White and Dennis Mehay
</author>
<affiliation confidence="0.9950035">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.928458">
Columbus, OH, USA
</address>
<email confidence="0.999574">
{espinosa,mwhite,mehay}@ling.osu.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881111111111">
In lexicalized grammatical formalisms, it is
possible to separate lexical category assign-
ment from the combinatory processes that
make use of such categories, such as pars-
ing and realization. We adapt techniques
from supertagging — a relatively recent tech-
nique that performs complex lexical tagging
before full parsing (Bangalore and Joshi,
1999; Clark, 2002) — for chart realization
in OpenCCG, an open-source NLP toolkit for
CCG. We call this approach hypertagging, as
it operates at a level “above” the syntax, tag-
ging semantic representations with syntactic
lexical categories. Our results demonstrate
that a hypertagger-informed chart realizer can
achieve substantial improvements in realiza-
tion speed (being approximately twice as fast)
with superior realization quality.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822739130435">
In lexicalized grammatical formalisms such as Lex-
icalized Tree Adjoining Grammar (Schabes et al.,
1988, LTAG), Combinatory Categorial Grammar
(Steedman, 2000, CCG) and Head-Driven Phrase-
Structure Grammar (Pollard and Sag, 1994, HPSG),
it is possible to separate lexical category assign-
ment — the assignment of informative syntactic cat-
egories to linguistic objects such as words or lex-
ical predicates — from the combinatory processes
that make use of such categories — such as pars-
ing and surface realization. One way of performing
lexical assignment is simply to hypothesize all pos-
sible lexical categories and then search for the best
combination thereof, as in the CCG parser in (Hock-
enmaier, 2003) or the chart realizer in (Carroll and
Oepen, 2005). A relatively recent technique for lex-
ical category assignment is supertagging (Bangalore
and Joshi, 1999), a preprocessing step to parsing that
assigns likely categories based on word and part-of-
speech (POS) contextual information. Supertagging
was dubbed “almost parsing” by these authors, be-
cause an oracle supertagger left relatively little work
for their parser, while speeding up parse times con-
siderably. Supertagging has been more recently ex-
tended to a multitagging paradigm in CCG (Clark,
2002; Curran et al., 2006), leading to extremely ef-
ficient parsing with state-of-the-art dependency re-
covery (Clark and Curran, 2007).
We have adapted this multitagging approach to
lexical category assignment for realization using the
CCG-based natural language toolkit OpenCCG.1 In-
stead of basing category assignment on linear word
and POS context, however, we predict lexical cat-
egories based on contexts within a directed graph
structure representing the logical form (LF) of a
proposition to be realized. Assigned categories are
instantiated in OpenCCG’s chart realizer where, to-
gether with a treebank-derived syntactic grammar
(Hockenmaier and Steedman, 2007) and a factored
language model (Bilmes and Kirchhoff, 2003), they
constrain the English word-strings that are chosen to
express the LF. We have dubbed this approach hy-
pertagging, as it operates at a level “above” the syn-
tax, moving from semantic representations to syn-
tactic categories.
We evaluate this hypertagger in two ways: first,
</bodyText>
<footnote confidence="0.974724">
1http://openccg.sourceforge.net.
</footnote>
<page confidence="0.963102">
183
</page>
<note confidence="0.698477">
Proceedings ofACL-08: HLT, pages 183–191,
</note>
<page confidence="0.495071">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999650428571428">
we evaluate it as a tagger, where the hypertagger
achieves high single-best (93.6%) and multitagging
labelling accuracies (95.8–99.4% with category per
lexical predication ratios ranging from 1.1 to 3.9).2
Second, we compare a hypertagger-augmented ver-
sion of OpenCCG’s chart realizer with the pre-
existing chart realizer (White et al., 2007) that sim-
ply instantiates the chart with all possible CCG cat-
egories (subject to frequency cutoffs) for each in-
put LF predicate. The hypertagger-seeded realizer
runs approximately twice as fast as the pre-existing
OpenCCG realizer and finds a larger number of
complete realizations, resorting less to chart frag-
ment assembly in order to produce an output within
a 15 second per-sentence time limit. Moreover, the
overall BLEU (Papineni et al., 2002) and METEOR
(Lavie and Agarwal, 2007) scores, as well as num-
bers of exact string matches (as measured against to
the original sentences in the CCGbank) are higher
for the hypertagger-seeded realizer than for the pre-
existing realizer.
This paper is structured as follows: Section 2 pro-
vides background on chart realization in OpenCCG
using a corpus-derived grammar. Section 3 de-
scribes our hypertagging approach and how it is in-
tegrated into the realizer. Section 4 describes our
results, followed by related work in Section 5 and
our conclusions in Section 6.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.997554">
2.1 Surface Realization with OpenCCG
</subsectionHeader>
<bodyText confidence="0.993467928571429">
The OpenCCG surface realizer is based on Steed-
man’s (2000) version of CCG elaborated with
Baldridge and Kruijff’s multi-modal extensions for
lexically specified derivation control (Baldridge,
2002; Baldridge and Kruijff, 2003) and hybrid
logic dependency semantics (Baldridge and Kruijff,
2002). OpenCCG implements a symbolic-statistical
chart realization algorithm (Kay, 1996; Carroll et al.,
1999; White, 2006b) combining (1) a theoretically
grounded approach to syntax and semantic composi-
tion with (2) factored language models (Bilmes and
Kirchhoff, 2003) for making choices among the op-
tions left open by the grammar.
In OpenCCG, the search for complete realizations
</bodyText>
<footnote confidence="0.955755">
2Note that the multitagger is “correct” if the correct tag is
anywhere in the multitag set.
</footnote>
<figureCaption confidence="0.807457">
Figure 1: Semantic dependency graph from the CCGbank
for He has a point he wants to make [... ]
</figureCaption>
<bodyText confidence="0.9998785">
makes use of n-gram language models over words
represented as vectors of factors, including surface
form, part of speech, supertag and semantic class.
The search proceeds in one of two modes, anytime
or two-stage (packing/unpacking). In the anytime
mode, a best-first search is performed with a con-
figurable time limit: the scores assigned by the n-
gram model determine the order of the edges on
the agenda, and thus have an impact on realization
speed. In the two-stage mode, a packed forest of
all possible realizations is created in the first stage;
in the second stage, the packed representation is un-
packed in bottom-up fashion, with scores assigned
to the edge for each sign as it is unpacked, much
as in (Langkilde, 2000). Edges are grouped into
equivalence classes when they have the same syn-
tactic category and cover the same parts of the in-
put logical form. Pruning takes place within equiv-
alence classes of edges. Additionally, to realize a
wide range of paraphrases, OpenCCG implements
an algorithm for efficiently generating from disjunc-
tive logical forms (White, 2006a).
To illustrate the input to OpenCCG, consider the
semantic dependency graph in Figure 1, which is
taken from section 00 of a Propbank-enhanced ver-
sion of the CCGbank (Boxwell and White, 2008).
In the graph, each node has a lexical predica-
tion (e.g. make.03) and a set of semantic features
(e.g. (NUM)sg); nodes are connected via depen-
dency relations (e.g. (ARG0)). Internally, such
</bodyText>
<figure confidence="0.992223826086957">
he
&lt;Arg1&gt;
p1
point
h2
&lt;NUM&gt;sg
&lt;Det&gt;
have.03
&lt;TENSE&gt;pres
&lt;Arg0&gt; &lt;Arg1&gt;
&lt;GenRel&gt;
h1
want.01
a1
a
w1
&lt;TENSE&gt;pres
&lt;Arg0&gt;
&lt;Arg1&gt;
h3
he
m1
make.03
</figure>
<page confidence="0.997301">
184
</page>
<bodyText confidence="0.999920741935484">
graphs are represented using Hybrid Logic Depen-
dency Semantics (HLDS), a dependency-based ap-
proach to representing linguistic meaning developed
by Baldridge and Kruijff (2002). In HLDS, hy-
brid logic (Blackburn, 2000) terms are used to de-
scribe dependency graphs. These graphs have been
suggested as representations for discourse structure,
and have their own underlying semantics (White,
2006b).
To more robustly support broad coverage surface
realization, OpenCCG has recently been enhanced
to greedily assemble fragments in the event that the
realizer fails to find a complete realization. The frag-
ment assembly algorithm begins with the edge for
the best partial realization, i.e. the one that covers
the most elementary predications in the input logi-
cal form, with ties broken according to the n-gram
score. (Larger fragments are preferred under the
assumption that they are more likely to be gram-
matical.) Next, the chart and agenda are greedily
searched for the best edge whose semantic coverage
is disjoint from those selected so far; this process re-
peats until no further edges can be added to the set
of selected fragments. In the final step, these frag-
ments are concatenated, again in a greedy fashion,
this time according to the n-gram score of the con-
catenated edges: starting with the original best edge,
the fragment whose concatenation on the left or right
side yields the highest score is chosen as the one to
concatenate next, until all the fragments have been
concatenated into a single output.
</bodyText>
<subsectionHeader confidence="0.999159">
2.2 Realization from an Enhanced CCGbank
</subsectionHeader>
<bodyText confidence="0.999989076923077">
White et al. (2007) describe an ongoing effort to en-
gineer a grammar from the CCGbank (Hockenmaier
and Steedman, 2007) — a corpus of CCG deriva-
tions derived from the Penn Treebank — suitable for
realization with OpenCCG. This process involves
converting the corpus to reflect more precise anal-
yses, where feasible, and adding semantic represen-
tations to the lexical categories. In the first step, the
derivations in the CCGbank are revised to reflect the
desired syntactic derivations. Changes to the deriva-
tions are necessary to reflect the lexicalized treat-
ment of coordination and punctuation assumed by
the multi-modal version of CCG that is implemented
in OpenCCG. Further changes are necessary to sup-
port semantic dependencies rather than surface syn-
tactic ones; in particular, the features and unifica-
tion constraints in the categories related to semanti-
cally empty function words such complementizers,
infinitival-to, expletive subjects, and case-marking
prepositions are adjusted to reflect their purely syn-
tactic status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with logi-
cal forms. Categories and unary type changing rules
(corresponding to zero morphemes) are sorted by
frequency and extracted if they meet the specified
frequency thresholds.
A separate transformation then uses around two
dozen generalized templates to add logical forms
to the categories, in a fashion reminiscent of (Bos,
2005). The effect of this transformation is illustrated
below. Example (1) shows how numbered seman-
tic roles, taken from PropBank (Palmer et al., 2005)
when available, are added to the category of an ac-
tive voice, past tense transitive verb, where *pred*
is a placeholder for the lexical predicate; examples
(2) and (3) show how more specific relations are in-
troduced in the category for determiners and the cat-
egory for the possessive ’s, respectively.
</bodyText>
<equation confidence="0.999259285714286">
(1) s1:dcl\np2/np3 =⇒
s1:dcl,x1\np2:x2/np3:x3 : @x1(*pred* ∧
hTENSEipres ∧ hARG0ix2 ∧ hARG1ix3)
(2) np1/n1 =⇒
np1:x1/n1:x1 : @x1(hDETi(d ∧ *pred*))
(3) np1/n1\np2 =⇒
np1:x1/n1:x1\np2:x2 : @x1(hGENOWNix2)
</equation>
<bodyText confidence="0.999174357142857">
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse the
sentences in the CCGbank according to the gold-
standard derivation. If the derivation can be success-
fully followed, the parse yields a logical form which
is saved along with the corpus sentence in order to
later test the realizer. The algorithm for following
corpus derivations attempts to continue processing if
it encounters a blocked derivation due to sentence-
internal punctuation. While punctuation has been
partially reanalyzed to use lexical categories, many
problem cases remain due to the CCGbank’s re-
liance on punctuation-specific binary rules that are
not supported in OpenCCG.
</bodyText>
<page confidence="0.994685">
185
</page>
<bodyText confidence="0.9999395">
Currently, the algorithm succeeds in creating log-
ical forms for 97.7% of the sentences in the devel-
opment section (Sect. 00) of the converted CCG-
bank, and 96.1% of the sentences in the test section
(Sect. 23). Of these, 76.6% of the development log-
ical forms are semantic dependency graphs with a
single root, while 76.7% of the test logical forms
have a single root. The remaining cases, with multi-
ple roots, are missing one or more dependencies re-
quired to form a fully connected graph. These miss-
ing dependencies usually reflect inadequacies in the
current logical form templates.
</bodyText>
<subsectionHeader confidence="0.996656">
2.3 Factored Language Models
</subsectionHeader>
<bodyText confidence="0.999966607142857">
Following White et al. (2007), we use factored tri-
gram models over words, part-of-speech tags and
supertags to score partial and complete realiza-
tions. The language models were created using the
SRILM toolkit (Stolcke, 2002) on the standard train-
ing sections (2–21) of the CCGbank, with sentence-
initial words (other than proper names) uncapital-
ized. While these models are considerably smaller
than the ones used in (Langkilde-Geary, 2002; Vell-
dal and Oepen, 2005), the training data does have
the advantage of being in the same domain and
genre (using larger n-gram models remains for fu-
ture investigation). The models employ interpolated
Kneser-Ney smoothing with the default frequency
cutoffs. The best performing model interpolates a
word trigram model with a trigram model that chains
a POS model with a supertag model, where the POS
model conditions on the previous two POS tags, and
the supertag model conditions on the previous two
POS tags as well as the current one.
Note that the use of supertags in the factored lan-
guage model to score possible realizations is distinct
from the prediction of supertags for lexical category
assignment: the former takes the words in the local
context into account (as in supertagging for parsing),
while the latter takes features of the logical form into
account. It is this latter process which we call hyper-
tagging, and to which we now turn.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="method">
3 The Approach
</sectionHeader>
<subsectionHeader confidence="0.999894">
3.1 Lexical Smoothing and Search Errors
</subsectionHeader>
<bodyText confidence="0.944394">
In White et al.’s (2007) initial investigation of scal-
ing up OpenCCG for broad coverage realization,
</bodyText>
<table confidence="0.95464275">
test set grammar complete
oracle / best
dev (00) dev 49.1% / 47.8%
train 37.5% / 22.6%
</table>
<tableCaption confidence="0.995617333333333">
Table 1: Percentage of complete realizations using an or-
acle n-gram model versus the best performing factored
language model.
</tableCaption>
<bodyText confidence="0.999600382352941">
all categories observed more often than a thresh-
old frequency were instantiated for lexical predi-
cates; for unseen words, a simple smoothing strategy
based on the part of speech was employed, assign-
ing the most frequent categories for the POS. This
approach turned out to suffer from a large number
of search errors, where the realizer failed to find a
complete realization before timing out even in cases
where the grammar supported one. To confirm that
search errors had become a significant issue, White
et al. compared the percentage of complete realiza-
tions (versus fragmentary ones) with their top scor-
ing model against an oracle model that uses a simpli-
fied BLEU score based on the target string, which is
useful for regression testing as it guides the best-first
search to the reference sentence. The comparison
involved both a medium-sized (non-blind) grammar
derived from the development section and a large
grammar derived from the training sections (the lat-
ter with slightly higher thresholds). As shown in
Table 1, with the large grammar derived from the
training sections, many fewer complete realizations
are found (before timing out) using the factored lan-
guage model than are possible, as indicated by the
results of using the oracle model. By contrast, the
difference is small with the medium-sized grammar
derived from the development section. This result is
not surprising when one considers that a large num-
ber of common words are observed to have many
possible categories.
In the next section, we show that a supertag-
ger for CCG realization, or hypertagger, can reduce
the problem of search errors by focusing the search
space on the most likely lexical categories.
</bodyText>
<subsectionHeader confidence="0.99683">
3.2 Maximum Entropy Hypertagging
</subsectionHeader>
<bodyText confidence="0.9998865">
As supertagging for parsing involves studying a
given input word and its local context, the concep-
</bodyText>
<page confidence="0.995054">
186
</page>
<bodyText confidence="0.998334326530612">
tual equivalent for a lexical predicate in the LF is to
study a given node and its local graph structure. Our
implementation makes use of three general types of
features: lexicalized features, which are simply the
names of the parent and child elementary predica-
tion nodes, graph structural features, such as the
total number of edges emanating from a node, the
number of argument and non-argument dependents,
and the names of the relations of the dependent
nodes to the parent node, and syntactico-semantic
attributes of nodes, such as the tense and number.
For example, in the HLDS graph shown in Figure 1,
the node representing want has two dependents, and
the relational type of make with respect to want is
ARG1.
Clark (2002) notes in his parsing experiments that
the POS tags of the surrounding words are highly in-
formative. As discussed below, a significant gain in
hypertagging accuracy resulted from including fea-
tures sensitive to the POS tags of a node’s parent, the
node itself, and all of its arguments and modifiers.
Predicting these tags requires the use of a separate
POS tagger, which operates in a manner similar to
the hypertagger itself, though exploiting a slightly
different set of features (e.g., including features cor-
responding to the four-character prefixes and suf-
fixes of rare logical predication names). Follow-
ing the (word) supertagging experiments of (Cur-
ran et al., 2006) we assigned potentially multiple
POS tags to each elementary predication. The POS
tags assigned are all those that are some factor Q
of the highest ranked tag,3 giving an average of 1.1
POS tags per elementary predication. The values of
the corresponding feature functions are the POS tag
probabilities according to the POS tagger. At this
ambiguity level, the POS tagger is correct Pt� 92% of
the time.
Features for the hypertagger were extracted from
semantic dependency graphs extracted from sections
2 through 21 of the CCGbank. In total, 37,168
dependency graphs were derived from the corpus,
yielding 468,628 feature parameters.
The resulting contextual features and gold-
standard supertag for each predication were then
used to train a maximum entropy classifier model.
3I.e., all tags t whose probabilities p(t) &gt; Q · p*, where p*
is the highest ranked tag’s probability.
Maximum entropy models describe a set of proba-
bility distributions of the form:
</bodyText>
<equation confidence="0.9745485">
n
p(o  |x) = Z(x) &apos; exp( Aifi(o,x))
</equation>
<bodyText confidence="0.94191747826087">
where o is an outcome, x is a context, the fi are
feature functions, the Ai are the respective weights
of the feature functions, and Z(x) is a normalizing
sum over all competing outcomes. More concretely,
given an elementary predication labeled want (as in
Figure 1), a feature function over this node could be:
� 1, if o is (s[dcl]\np)/(s[adj]\np) and
f(o, x) = number of LF dependents(x) = 2
0, otherwise.
We used Zhang Le’s maximum entropy toolkit4
for training the hypertagging model, which uses an
implementation of Limited-memory BFGS, an ap-
proximate quasi-Newton optimization method from
the numerical optimization literature (Liu and No-
cedal, 1989). Using L-BFGS allowed us to include
continuous feature function values where appropri-
ate (e.g., the probabilities of automatically-assigned
POS tags). We trained each hypertagging model to
275 iterations and our POS tagging model to 400 it-
erations. We used no feature frequency cut-offs, but
rather employed Gaussian priors with global vari-
ances of 100 and 75, respectively, for the hypertag-
ging and POS tagging models.
</bodyText>
<subsectionHeader confidence="0.997577">
3.3 Iterative Q-Best Realization
</subsectionHeader>
<bodyText confidence="0.999960923076923">
During realization, the hypertagger serves to prob-
abilistically filter the categories assigned to an ele-
mentary predication, as well as to propose categories
for rare or unseen predicates. Given a predication,
the tagger returns a Q-best list of supertags in order
of decreasing probability. Increasing the number of
categories returned clearly increases the likelihood
that the most-correct supertag is among them, but at
a corresponding cost in chart size. Accordingly, the
hypertagger begins with a highly restrictive value for
Q, and backs off to progressively less-restrictive val-
ues if no complete realization could be found using
the set of supertags returned. The search is restarted
</bodyText>
<footnote confidence="0.9577545">
4http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
</footnote>
<equation confidence="0.567673">
i=1
</equation>
<page confidence="0.997145">
187
</page>
<tableCaption confidence="0.9518476">
Table 2: Hypertagger accuracy on Sections 00 and 23.
Results (in percentages) are for per-logical-predication
(PR) and per-whole-graph (GRPH) tagging accurcies.
Difference between best-only and baselines (b.l.) is sig-
nificant (p &lt; 2 · 10−16) by McNemar’s x2 test.
</tableCaption>
<table confidence="0.999614230769231">
Sect00 Sect23
Q Tags PR GRPH PR GRPH
����
b.l. 1 1 68.7 1.8 68.7 2.3
b.l. 2 2 84.3 9.9 84.4 10.9
1.0 1 93.6 40.4 93.6 38.2
0.16 1.1 95.8 55.7 96.2 56.8
0.05 1.2 96.6 63.8 97.3 66.0
0.0058 1.5 97.9 74.8 98.3 76.9
1.75e-3 1.8 98.4 78.9 98.7 81.8
6.25e-4 2.2 98.7 82.5 99.0 84.3
1.25e-4 3.2 99.0 85.7 99.3 88.5
5.8e-5 3.9 99.1 87.2 99.4 89.9
</table>
<bodyText confidence="0.9997534">
from scratch with the next Q value, though in prin-
ciple the same chart could be expanded. The iter-
ative, Q-best search for a complete realization uses
the realizer’s packing mode, which can more quickly
determine whether a complete realization is possi-
ble. If the halfway point of the overall time limit
is reached with no complete realization, the search
switches to best-first mode, ultimately assembling
fragments if no complete realization can be found
during the remaining time.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9999169">
Several experiments were performed in training and
applying the hypertagger. Three different models
were created using 1) non-lexicalized features only,
2) all features excluding POS tags, 3) all, 3) all
features except syntactico-semantic attributes such
as tense and number and 4) all features available.
Models trained on these feature subsets were tested
against one another on Section 00, and then the best
performing model was run on both Section 00 and
23.
</bodyText>
<subsectionHeader confidence="0.962876">
4.1 Feature Ablation Testing
</subsectionHeader>
<bodyText confidence="0.9985675">
The the whole feature set was found in feature abla-
tion testing on the development set to outperform all
other feature subsets significantly (p &lt; 2.2 · 10−16).
These results listed in Table 3. As we can see, taking
</bodyText>
<tableCaption confidence="0.5916465">
Table 3: Hypertagger feature ablation testing results on
Section 00. The full feature set outperforms all others sig-
nificantly (p &lt; 2.2 · 10−16). Results for per-predication
(PR) and per-whole-graph (GRPH) tagging percentage
accuracies are listed. (Key: no-POS=no POS features;
no-attr=no syntactico-semantic attributes such as tense
and number; non-lex=non-lexicalized features only (no
predication names).
</tableCaption>
<table confidence="0.966194">
FEATURESET PR GRPH
full 93.6 40.37
no-POS 91.3 29.5
no-attr 91.8 31.2
non-lex 91.5 28.7
</table>
<bodyText confidence="0.999569416666667">
away any one class of features leads to drop in per-
predication tagging accuracy of at least 1.8% and a
drop per-whole-graph accuracy of at least 9.2%. As
expected from previous work in supertagging (for
parsing), POS features resulted in a large improve-
ment in overall accuracy (1.8%). Although the POS
tagger by itself is only 92% accurate (as a multi-
tagger of 1.1 PO�
word average ambiguity) — well be-
low the state-of-the-art for the tagging of words —
its predictions are still quite valuable to the hyper-
tagger.
</bodyText>
<subsectionHeader confidence="0.987563">
4.2 Best Model Hypertagger Accuracy
</subsectionHeader>
<bodyText confidence="0.9999755">
The results for the full feature set on Sections 00
and 23 are outlined in Table 2. Included in this
table are accuracy data for a baseline dummy tag-
ger which simply assigns the most-frequently-seen
tag(s) for a given predication and backs off to the
overall most frequent tag(s) when confronted with
an unseen predication. The development set (00)
was used to tune the Q parameter to obtain reason-
able hypertag ambiguity levels; the model was not
otherwise tuned to it. The hypertagger achieves high
per-predication and whole-graph accuracies even at
small ambiguity levels.
</bodyText>
<subsectionHeader confidence="0.999027">
4.3 Realizer Performance
</subsectionHeader>
<bodyText confidence="0.999884333333333">
Tables 4 and 5 show how the hypertagger improves
realization performance on the development and test
sections of the CCGbank. As Table 4 indicates, us-
ing the hypertagger in an iterative beta-best fash-
ion more than doubles the number of grammati-
cally complete realizations found within the time
</bodyText>
<page confidence="0.999093">
188
</page>
<tableCaption confidence="0.9972345">
Table 5: Realization quality metrics exact match, BLEU and METEOR, on complete realizations only and overall,
with and without hypertagger, on Sections 00 and 23.
</tableCaption>
<table confidence="0.9998055">
Sec- Hyper- Complete Exact Overall
tion tagger BLEU METEOR BLEU METEOR
00 with 0.8137 0.9153 15.3% 0.6567 0.8494
w/o 0.6864 0.8585 11.3% 0.5902 0.8209
23 with 0.8149 0.9162 16.0% 0.6701 0.8557
w/o 0.6910 0.8606 12.3% 0.6022 0.8273
</table>
<tableCaption confidence="0.98078275">
Table 4: Percentage of grammatically complete realiza-
tions, runtimes for complete realizations and overall run-
times, with and without hypertagger, on Sections 00 and
23.
</tableCaption>
<table confidence="0.998957666666667">
Sec- Hyper- Percent Complete Overall
tion tagger Complete Time Time
00 with 47.4% 1.2s 4.5s
w/o 22.6% 8.7s 9.5s
23 with 48.5% 1.2s 4.4s
w/o 23.5% 8.9s 9.6s
</table>
<bodyText confidence="0.999695588235294">
limit; on the development set, this improvement eli-
mates more than the number of known search errors
(cf. Table 1). Additionally, by reducing the search
space, the hypertagger cuts overall realization times
by more than half, and in the cases where complete
realizations are found, realization times are reduced
by a factor of four, down to 1.2 seconds per sentence
on a desktop Linux PC.
Table 5 shows that increasing the number of com-
plete realizations also yields improved BLEU and
METEOR scores, as well as more exact matches. In
particular, the hypertagger makes possible a more
than 6-point improvement in the overall BLEU score
on both the development and test sections, and a
more than 12-point improvement on the sentences
with complete realizations.
As the effort to engineer a grammar suitable for
realization from the CCGbank proceeds in paral-
lel to our work on hypertagging, we expect the
hypertagger-seeded realizer to continue to improve,
since a more complete and precise extracted gram-
mar should enable more complete realizations to be
found, and richer semantic representations should
simplify the hypertagging task. Even with the cur-
rent incomplete set of semantic templates, the hy-
pertagger brings realizer performance roughly up to
state-of-the-art levels, as our overall test set BLEU
score (0.6701) slightly exceeds that of Cahill and
van Genabith (2006), though at a coverage of 96%
instead of 98%. We caution, however, that it remains
unclear how meaningful it is to directly compare
these scores when the realizer inputs vary consider-
ably in their specificity, as Langkilde-Geary’s (2002)
experiments dramatically illustrate.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999936521739131">
Our approach follows Langkilde-Geary (2002) and
Callaway (2003) in aiming to leverage the Penn
Treebank to develop a broad-coverage surface re-
alizer for English. However, while these earlier,
generation-only approaches made use of converters
for transforming the outputs of Treebank parsers to
inputs for realization, our approach instead employs
a shared bidirectional grammar, so that the input to
realization is guaranteed to be the same logical form
constructed by the parser. In this regard, our ap-
proach is more similar to the ones pursued more re-
cently by Carroll, Oepen and Velldal (2005; 2005;
2006), Nakanishi et al. (2005) and Cahill and van
Genabith (2006) with HPSG and LFG grammars.
While we consider our approach to be the first to
employ a supertagger for realization, or hypertagger,
the approach is clearly reminiscent of the LTAG tree
models of Srinivas and Rambow (2000). The main
difference between the approaches is that ours con-
sists of a multitagging step followed by the bottom-
up construction of a realization chart, while theirs
involves the top-down selection of the single most
likely supertag for each node that is grammatically
</bodyText>
<page confidence="0.996822">
189
</page>
<bodyText confidence="0.999990477272727">
compatible with the parent node, with the proba-
bility conditioned only on the child nodes. Note
that although their approach does involve a subse-
quent lattice construction step, it requires making
non-standard assumptions about the TAG; in con-
trast, ours follows the chart realization tradition of
working with the same operations of grammatical
combination as in parsing, including a well-defined
notion of semantic composition. Additionally, as
our tagger employs maximum entropy modeling, it
is able to take into account a greater variety of con-
textual features, including those derived from parent
nodes.
In comparison to other recent chart realization ap-
proaches, Nakanishi et al.’s is similar to ours in that
it employs an iterative beam search, dynamically
changing the beam size in order to cope with the
large search space. However, their log-linear selec-
tion models have been adapted from ones used in
parsing, and do not condition choices based on fea-
tures of the input semantics to the same extent. In
particular, while they employ a baseline maximum
likelihood model that conditions the probability of
a lexical entry upon its predicate argument struc-
ture (PAS) — that is, the set of elementary predi-
cations introduced by the lexical item — this prob-
ability does not take into account other elements of
the local context, including parents and modifiers,
and their lexical predicates. Similarly, Cahill and
van Genabith condition the probability of their lex-
ical rules on the set of feature-value pairs linked to
the RHS of the rule, but do not take into account any
additional context. Since their probabilistic mod-
els involve independence assumptions like those in
a PCFG, and since they do not employ n-grams for
scoring alternative realizations, their approach only
keeps the single most likely edge in an equivalence
class, rather than packing them into a forest. Car-
roll, Oepen and Velldal’s approach is like Nakanishi
et al.’s in that they adapt log-linear parsing models
to the realization task; however, they employ manu-
ally written grammars on much smaller corpora, and
perhaps for this reason they have not faced the need
to employ an iterative beam search.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995857142857">
We have introduced a novel type of supertagger,
which we have dubbed a hypertagger, that assigns
CCG category labels to elementary predications in
a structured semantic representation with high accu-
racy at several levels of tagging ambiguity in a fash-
ion reminiscent of (Bangalore and Rambow, 2000).
To our knowledge, we are the first to report tag-
ging results in the semantic-to-syntactic direction.
We have also shown that, by integrating this hy-
pertagger with a broad-coverage CCG chart real-
izer, considerably faster realization times are possi-
ble (approximately twice as fast as compared with
a realizer that performs simple lexical look-ups)
with higher BLEU, METEOR and exact string match
scores. Moreover, the hypertagger-augmented real-
izer finds more than twice the number of complete
realizations, and further analysis revealed that the
realization quality (as per modified BLEU and ME-
TEOR) is higher in the cases when the realizer finds
a complete realization. This suggests that further
improvements to the hypertagger will lead to more
complete realizations, hence more high-quality re-
alizations. Finally, further efforts to engineer a
grammar suitable for realization from the CCGbank
should provide richer feature sets, which, as our fea-
ture ablation study suggests, are useful for boosting
hypertagging performance, hence for finding better
and more complete realizations.
</bodyText>
<sectionHeader confidence="0.997364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999614333333333">
The authors thank the anonymous reviewers, Chris
Brew, Detmar Meurers and Eric Fosler-Lussier for
helpful comments and discussion.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9795777">
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proc.
ACL-03.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
</reference>
<page confidence="0.989365">
190
</page>
<reference confidence="0.999373838095238">
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237–265.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proce. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Patrick Blackburn. 2000. Representation, reasoning, and
relational structures: a hybrid logic manifesto. Logic
Journal of the IGPL, 8(3):339–625.
Johan Bos. 2005. Towards wide-coverage semantic in-
terpretation. In Proc. IWCS-6.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-08.
To appear.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proc. COLING-ACL ’06.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI-03.
John Carroll and Stefan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proc. IJCNLP-05.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Pozna´nski. 1999. An efficient chart generator for
(semi-) lexicalist grammars. In Proc. ENLG-99.
Stephen Clark and James Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4).
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th Inter-
national Workshop on Tree Adjoining Grammars and
Related Frameworks (TAG+6), pages 19–24, Venice,
Italy.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697–704, Sydney, Aus-
tralia.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proc. NAACL-00.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of Workshop on Statistical Machine Translation at the
45th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2007), Prague.
D C Liu and Jorge Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathematical
Programming B, 45(3).
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Philadelphia, PA.
Carl J Pollard and Ivan A Sag. 1994. Head-Driven
Phrase Structure Grammar. University Of Chicago
Press.
Yves Schabes, Anne Abeill´e, and Aravind K. Joshi.
1988. Parsing strategies with ’lexicalized’ grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 121h International Conference on Compu-
tational Linguistics (COLING-88), Budapest.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts, USA.
Andreas Stolcke. 2002. SRILM — An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia, July.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Michael White. 2006a. CCG chart realization from dis-
junctive inputs. In Proceedings, INLG 2006.
Michael White. 2006b. Efficient realization of coordi-
nate structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39–75.
</reference>
<page confidence="0.998429">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.952969">
<title confidence="0.999597">Hypertagging: Supertagging for Surface Realization with CCG</title>
<author confidence="0.998466">Espinosa White Mehay</author>
<affiliation confidence="0.9997745">Department of Linguistics The Ohio State University</affiliation>
<address confidence="0.999989">Columbus, OH, USA</address>
<abstract confidence="0.997521894736842">In lexicalized grammatical formalisms, it is to separate category assignthe combinatory processes that make use of such categories, such as parsing and realization. We adapt techniques a relatively recent technique that performs complex lexical tagging before full parsing (Bangalore and Joshi, 1999; Clark, 2002) — for chart realization in OpenCCG, an open-source NLP toolkit for We call this approach as it operates at a level “above” the syntax, tagging semantic representations with syntactic lexical categories. Our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed (being approximately twice as fast) with superior realization quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan Kruijff</author>
</authors>
<title>Coupling CCG and Hybrid Logic Dependency Semantics. In</title>
<date>2002</date>
<booktitle>Proc. ACL-02.</booktitle>
<contexts>
<context position="5128" citStr="Baldridge and Kruijff, 2002" startWordPosition="758" endWordPosition="761">on 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [... ] makes use of n-gram language model</context>
<context position="7479" citStr="Baldridge and Kruijff (2002)" startWordPosition="1139" endWordPosition="1142">h in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. (NUM)sg); nodes are connected via dependency relations (e.g. (ARG0)). Internally, such he &lt;Arg1&gt; p1 point h2 &lt;NUM&gt;sg &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; &lt;GenRel&gt; h1 want.01 a1 a w1 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h3 he m1 make.03 184 graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff (2002). In HLDS, hybrid logic (Blackburn, 2000) terms are used to describe dependency graphs. These graphs have been suggested as representations for discourse structure, and have their own underlying semantics (White, 2006b). To more robustly support broad coverage surface realization, OpenCCG has recently been enhanced to greedily assemble fragments in the event that the realizer fails to find a complete realization. The fragment assembly algorithm begins with the edge for the best partial realization, i.e. the one that covers the most elementary predications in the input logical form, with ties b</context>
</contexts>
<marker>Baldridge, Kruijff, 2002</marker>
<rawString>Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling CCG and Hybrid Logic Dependency Semantics. In Proc. ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Geert-Jan Kruijff</author>
</authors>
<title>MultiModal Combinatory Categorial Grammar. In</title>
<date>2003</date>
<booktitle>Proc. ACL-03.</booktitle>
<contexts>
<context position="5060" citStr="Baldridge and Kruijff, 2003" startWordPosition="749" endWordPosition="752">the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency graph from the CCGbank for He ha</context>
</contexts>
<marker>Baldridge, Kruijff, 2003</marker>
<rawString>Jason Baldridge and Geert-Jan Kruijff. 2003. MultiModal Combinatory Categorial Grammar. In Proc. ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>Lexically Specified Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<tech>Ph.D. thesis,</tech>
<volume>25</volume>
<issue>2</issue>
<institution>School of Informatics, University of Edinburgh. Srinivas Bangalore</institution>
<contexts>
<context position="5030" citStr="Baldridge, 2002" startWordPosition="747" endWordPosition="748">ealizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency gr</context>
</contexts>
<marker>Baldridge, 2002</marker>
<rawString>Jason Baldridge. 2002. Lexically Specified Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, School of Informatics, University of Edinburgh. Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An Approach to Almost Parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proce. COLING-00.</booktitle>
<contexts>
<context position="29924" citStr="Bangalore and Rambow, 2000" startWordPosition="4743" endWordPosition="4746"> into a forest. Carroll, Oepen and Velldal’s approach is like Nakanishi et al.’s in that they adapt log-linear parsing models to the realization task; however, they employ manually written grammars on much smaller corpora, and perhaps for this reason they have not faced the need to employ an iterative beam search. 6 Conclusion We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000). To our knowledge, we are the first to report tagging results in the semantic-to-syntactic direction. We have also shown that, by integrating this hypertagger with a broad-coverage CCG chart realizer, considerably faster realization times are possible (approximately twice as fast as compared with a realizer that performs simple lexical look-ups) with higher BLEU, METEOR and exact string match scores. Moreover, the hypertagger-augmented realizer finds more than twice the number of complete realizations, and further analysis revealed that the realization quality (as per modified BLEU and METEOR</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proce. COLING-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and general parallelized backoff.</title>
<date>2003</date>
<booktitle>In Proc. HLT-03.</booktitle>
<contexts>
<context position="2997" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="436" endWordPosition="439">ncy recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treebank-derived syntactic grammar (Hockenmaier and Steedman, 2007) and a factored language model (Bilmes and Kirchhoff, 2003), they constrain the English word-strings that are chosen to express the LF. We have dubbed this approach hypertagging, as it operates at a level “above” the syntax, moving from semantic representations to syntactic categories. We evaluate this hypertagger in two ways: first, 1http://openccg.sourceforge.net. 183 Proceedings ofACL-08: HLT, pages 183–191, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we evaluate it as a tagger, where the hypertagger achieves high single-best (93.6%) and multitagging labelling accuracies (95.8–99.4% with category per lexical pre</context>
<context position="5393" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="794" endWordPosition="797">ons in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [... ] makes use of n-gram language models over words represented as vectors of factors, including surface form, part of speech, supertag and semantic class. The search proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a conf</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language models and general parallelized backoff. In Proc. HLT-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
</authors>
<title>Representation, reasoning, and relational structures: a hybrid logic manifesto.</title>
<date>2000</date>
<journal>Logic Journal of the IGPL,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="7520" citStr="Blackburn, 2000" startWordPosition="1148" endWordPosition="1149">opbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. (NUM)sg); nodes are connected via dependency relations (e.g. (ARG0)). Internally, such he &lt;Arg1&gt; p1 point h2 &lt;NUM&gt;sg &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; &lt;GenRel&gt; h1 want.01 a1 a w1 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h3 he m1 make.03 184 graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff (2002). In HLDS, hybrid logic (Blackburn, 2000) terms are used to describe dependency graphs. These graphs have been suggested as representations for discourse structure, and have their own underlying semantics (White, 2006b). To more robustly support broad coverage surface realization, OpenCCG has recently been enhanced to greedily assemble fragments in the event that the realizer fails to find a complete realization. The fragment assembly algorithm begins with the edge for the best partial realization, i.e. the one that covers the most elementary predications in the input logical form, with ties broken according to the n-gram score. (Lar</context>
</contexts>
<marker>Blackburn, 2000</marker>
<rawString>Patrick Blackburn. 2000. Representation, reasoning, and relational structures: a hybrid logic manifesto. Logic Journal of the IGPL, 8(3):339–625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Towards wide-coverage semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proc. IWCS-6.</booktitle>
<contexts>
<context position="10309" citStr="Bos, 2005" startWordPosition="1588" endWordPosition="1589">s related to semantically empty function words such complementizers, infinitival-to, expletive subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status. In the second step, a grammar is extracted from the converted CCGbank and augmented with logical forms. Categories and unary type changing rules (corresponding to zero morphemes) are sorted by frequency and extracted if they meet the specified frequency thresholds. A separate transformation then uses around two dozen generalized templates to add logical forms to the categories, in a fashion reminiscent of (Bos, 2005). The effect of this transformation is illustrated below. Example (1) shows how numbered semantic roles, taken from PropBank (Palmer et al., 2005) when available, are added to the category of an active voice, past tense transitive verb, where *pred* is a placeholder for the lexical predicate; examples (2) and (3) show how more specific relations are introduced in the category for determiners and the category for the possessive ’s, respectively. (1) s1:dcl\np2/np3 =⇒ s1:dcl,x1\np2:x2/np3:x3 : @x1(*pred* ∧ hTENSEipres ∧ hARG0ix2 ∧ hARG1ix3) (2) np1/n1 =⇒ np1:x1/n1:x1 : @x1(hDETi(d ∧ *pred*)) (3)</context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>Johan Bos. 2005. Towards wide-coverage semantic interpretation. In Proc. IWCS-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boxwell</author>
<author>Michael White</author>
</authors>
<title>Projecting Propbank roles onto the CCGbank.</title>
<date>2008</date>
<booktitle>In Proc. LREC-08.</booktitle>
<note>To appear.</note>
<contexts>
<context position="6968" citStr="Boxwell and White, 2008" startWordPosition="1059" endWordPosition="1062">ned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Pruning takes place within equivalence classes of edges. Additionally, to realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. (NUM)sg); nodes are connected via dependency relations (e.g. (ARG0)). Internally, such he &lt;Arg1&gt; p1 point h2 &lt;NUM&gt;sg &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; &lt;GenRel&gt; h1 want.01 a1 a w1 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h3 he m1 make.03 184 graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff (2002). In HLDS, hybrid logic (Blackburn, 2000) terms are used to describe dependency graphs. T</context>
</contexts>
<marker>Boxwell, White, 2008</marker>
<rawString>Stephen Boxwell and Michael White. 2008. Projecting Propbank roles onto the CCGbank. In Proc. LREC-08. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Robust PCFG-based generation using automatically acquired LFG approximations.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL ’06.</booktitle>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>Aoife Cahill and Josef van Genabith. 2006. Robust PCFG-based generation using automatically acquired LFG approximations. In Proc. COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proc. IJCAI-03.</booktitle>
<contexts>
<context position="26332" citStr="Callaway (2003)" startWordPosition="4166" endWordPosition="4167"> hypertagging task. Even with the current incomplete set of semantic templates, the hypertagger brings realizer performance roughly up to state-of-the-art levels, as our overall test set BLEU score (0.6701) slightly exceeds that of Cahill and van Genabith (2006), though at a coverage of 96% instead of 98%. We caution, however, that it remains unclear how meaningful it is to directly compare these scores when the realizer inputs vary considerably in their specificity, as Langkilde-Geary’s (2002) experiments dramatically illustrate. 5 Related Work Our approach follows Langkilde-Geary (2002) and Callaway (2003) in aiming to leverage the Penn Treebank to develop a broad-coverage surface realizer for English. However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. In this regard, our approach is more similar to the ones pursued more recently by Carroll, Oepen and Velldal (2005; 2005; 2006), Nakanishi et al. (2005) and Cahill and van Genabith (</context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proc. IJCAI-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Stefan Oepen</author>
</authors>
<title>High efficiency realization for a wide-coverage unification grammar.</title>
<date>2005</date>
<booktitle>In Proc. IJCNLP-05.</booktitle>
<contexts>
<context position="1774" citStr="Carroll and Oepen, 2005" startWordPosition="257" endWordPosition="260">orial Grammar (Steedman, 2000, CCG) and Head-Driven PhraseStructure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment — the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates — from the combinatory processes that make use of such categories — such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency </context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>John Carroll and Stefan Oepen. 2005. High efficiency realization for a wide-coverage unification grammar. In Proc. IJCNLP-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Pozna´nski</author>
</authors>
<title>An efficient chart generator for (semi-) lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proc. ENLG-99.</booktitle>
<marker>Carroll, Copestake, Flickinger, Pozna´nski, 1999</marker>
<rawString>John Carroll, Ann Copestake, Dan Flickinger, and Victor Pozna´nski. 1999. An efficient chart generator for (semi-) lexicalist grammars. In Proc. ENLG-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2407" citStr="Clark and Curran, 2007" startWordPosition="351" endWordPosition="354">vely recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treebank-derived syntactic grammar (Hockenmaier and Steedman, 2007) and a factored language model (Bilmes and Kirchhoff, 2003), they con</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Supertagging for combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6),</booktitle>
<pages>pages</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="2278" citStr="Clark, 2002" startWordPosition="334" endWordPosition="335">ion thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treeban</context>
<context position="16575" citStr="Clark (2002)" startWordPosition="2607" endWordPosition="2608">tation makes use of three general types of features: lexicalized features, which are simply the names of the parent and child elementary predication nodes, graph structural features, such as the total number of edges emanating from a node, the number of argument and non-argument dependents, and the names of the relations of the dependent nodes to the parent node, and syntactico-semantic attributes of nodes, such as the tense and number. For example, in the HLDS graph shown in Figure 1, the node representing want has two dependents, and the relational type of make with respect to want is ARG1. Clark (2002) notes in his parsing experiments that the POS tags of the surrounding words are highly informative. As discussed below, a significant gain in hypertagging accuracy resulted from including features sensitive to the POS tags of a node’s parent, the node itself, and all of its arguments and modifiers. Predicting these tags requires the use of a separate POS tagger, which operates in a manner similar to the hypertagger itself, though exploiting a slightly different set of features (e.g., including features corresponding to the four-character prefixes and suffixes of rare logical predication names</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>Stephen Clark. 2002. Supertagging for combinatory categorial grammar. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6), pages 19–24, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>697--704</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2300" citStr="Curran et al., 2006" startWordPosition="336" endWordPosition="339">as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treebank-derived syntactic gr</context>
<context position="17248" citStr="Curran et al., 2006" startWordPosition="2712" endWordPosition="2716">f the surrounding words are highly informative. As discussed below, a significant gain in hypertagging accuracy resulted from including features sensitive to the POS tags of a node’s parent, the node itself, and all of its arguments and modifiers. Predicting these tags requires the use of a separate POS tagger, which operates in a manner similar to the hypertagger itself, though exploiting a slightly different set of features (e.g., including features corresponding to the four-character prefixes and suffixes of rare logical predication names). Following the (word) supertagging experiments of (Curran et al., 2006) we assigned potentially multiple POS tags to each elementary predication. The POS tags assigned are all those that are some factor Q of the highest ranked tag,3 giving an average of 1.1 POS tags per elementary predication. The values of the corresponding feature functions are the POS tag probabilities according to the POS tagger. At this ambiguity level, the POS tagger is correct Pt� 92% of the time. Features for the hypertagger were extracted from semantic dependency graphs extracted from sections 2 through 21 of the CCGbank. In total, 37,168 dependency graphs were derived from the corpus, y</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R. Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06), pages 697–704, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="2938" citStr="Hockenmaier and Steedman, 2007" startWordPosition="427" endWordPosition="430">ng to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treebank-derived syntactic grammar (Hockenmaier and Steedman, 2007) and a factored language model (Bilmes and Kirchhoff, 2003), they constrain the English word-strings that are chosen to express the LF. We have dubbed this approach hypertagging, as it operates at a level “above” the syntax, moving from semantic representations to syntactic categories. We evaluate this hypertagger in two ways: first, 1http://openccg.sourceforge.net. 183 Proceedings ofACL-08: HLT, pages 183–191, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we evaluate it as a tagger, where the hypertagger achieves high single-best (93.6%) and multitagging lab</context>
<context position="8976" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1384" endWordPosition="1387"> this process repeats until no further edges can be added to the set of selected fragments. In the final step, these fragments are concatenated, again in a greedy fashion, this time according to the n-gram score of the concatenated edges: starting with the original best edge, the fragment whose concatenation on the left or right side yields the highest score is chosen as the one to concatenate next, until all the fragments have been concatenated into a single output. 2.2 Realization from an Enhanced CCGbank White et al. (2007) describe an ongoing effort to engineer a grammar from the CCGbank (Hockenmaier and Steedman, 2007) — a corpus of CCG derivations derived from the Penn Treebank — suitable for realization with OpenCCG. This process involves converting the corpus to reflect more precise analyses, where feasible, and adding semantic representations to the lexical categories. In the first step, the derivations in the CCGbank are revised to reflect the desired syntactic derivations. Changes to the derivations are necessary to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG. Further changes are necessary to support semanti</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1723" citStr="Hockenmaier, 2003" startWordPosition="249" endWordPosition="251">chabes et al., 1988, LTAG), Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven PhraseStructure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment — the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates — from the combinatory processes that make use of such categories — such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely </context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proc. ACL-96. Irene Langkilde-Geary.</booktitle>
<contexts>
<context position="5210" citStr="Kay, 1996" startWordPosition="769" endWordPosition="770">scribes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [... ] makes use of n-gram language models over words represented as vectors of factors, including surface form, part of sp</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart generation. In Proc. ACL-96. Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proc. INLG-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proc. NAACL-00.</booktitle>
<contexts>
<context position="6421" citStr="Langkilde, 2000" startWordPosition="973" endWordPosition="974">of speech, supertag and semantic class. The search proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configurable time limit: the scores assigned by the ngram model determine the order of the edges on the agenda, and thus have an impact on realization speed. In the two-stage mode, a packed forest of all possible realizations is created in the first stage; in the second stage, the packed representation is unpacked in bottom-up fashion, with scores assigned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Pruning takes place within equivalence classes of edges. Additionally, to realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proc. NAACL-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of Workshop on Statistical Machine Translation at the 45th Annual Meeting of the Association of Computational Linguistics (ACL-2007),</booktitle>
<location>Prague.</location>
<contexts>
<context position="4261" citStr="Lavie and Agarwal, 2007" startWordPosition="625" endWordPosition="628">.2 Second, we compare a hypertagger-augmented version of OpenCCG’s chart realizer with the preexisting chart realizer (White et al., 2007) that simply instantiates the chart with all possible CCG categories (subject to frequency cutoffs) for each input LF predicate. The hypertagger-seeded realizer runs approximately twice as fast as the pre-existing OpenCCG realizer and finds a larger number of complete realizations, resorting less to chart fragment assembly in order to produce an output within a 15 second per-sentence time limit. Moreover, the overall BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCGbank) are higher for the hypertagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realize</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of Workshop on Statistical Machine Translation at the 45th Annual Meeting of the Association of Computational Linguistics (ACL-2007), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="18899" citStr="Liu and Nocedal, 1989" startWordPosition="2981" endWordPosition="2985">ext, the fi are feature functions, the Ai are the respective weights of the feature functions, and Z(x) is a normalizing sum over all competing outcomes. More concretely, given an elementary predication labeled want (as in Figure 1), a feature function over this node could be: � 1, if o is (s[dcl]\np)/(s[adj]\np) and f(o, x) = number of LF dependents(x) = 2 0, otherwise. We used Zhang Le’s maximum entropy toolkit4 for training the hypertagging model, which uses an implementation of Limited-memory BFGS, an approximate quasi-Newton optimization method from the numerical optimization literature (Liu and Nocedal, 1989). Using L-BFGS allowed us to include continuous feature function values where appropriate (e.g., the probabilities of automatically-assigned POS tags). We trained each hypertagging model to 275 iterations and our POS tagging model to 400 iterations. We used no feature frequency cut-offs, but rather employed Gaussian priors with global variances of 100 and 75, respectively, for the hypertagging and POS tagging models. 3.3 Iterative Q-Best Realization During realization, the hypertagger serves to probabilistically filter the categories assigned to an elementary predication, as well as to propose</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D C Liu and Jorge Nocedal. 1989. On the limited memory method for large scale optimization. Mathematical Programming B, 45(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic methods for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proc. IWPT-05.</booktitle>
<contexts>
<context position="26902" citStr="Nakanishi et al. (2005)" startWordPosition="4255" endWordPosition="4258">ch follows Langkilde-Geary (2002) and Callaway (2003) in aiming to leverage the Penn Treebank to develop a broad-coverage surface realizer for English. However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. In this regard, our approach is more similar to the ones pursued more recently by Carroll, Oepen and Velldal (2005; 2005; 2006), Nakanishi et al. (2005) and Cahill and van Genabith (2006) with HPSG and LFG grammars. While we consider our approach to be the first to employ a supertagger for realization, or hypertagger, the approach is clearly reminiscent of the LTAG tree models of Srinivas and Rambow (2000). The main difference between the approaches is that ours consists of a multitagging step followed by the bottomup construction of a realization chart, while theirs involves the top-down selection of the single most likely supertag for each node that is grammatically 189 compatible with the parent node, with the probability conditioned only </context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic methods for disambiguation of an HPSG-based chart generator. In Proc. IWPT-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="10455" citStr="Palmer et al., 2005" startWordPosition="1609" endWordPosition="1612">re adjusted to reflect their purely syntactic status. In the second step, a grammar is extracted from the converted CCGbank and augmented with logical forms. Categories and unary type changing rules (corresponding to zero morphemes) are sorted by frequency and extracted if they meet the specified frequency thresholds. A separate transformation then uses around two dozen generalized templates to add logical forms to the categories, in a fashion reminiscent of (Bos, 2005). The effect of this transformation is illustrated below. Example (1) shows how numbered semantic roles, taken from PropBank (Palmer et al., 2005) when available, are added to the category of an active voice, past tense transitive verb, where *pred* is a placeholder for the lexical predicate; examples (2) and (3) show how more specific relations are introduced in the category for determiners and the category for the possessive ’s, respectively. (1) s1:dcl\np2/np3 =⇒ s1:dcl,x1\np2:x2/np3:x3 : @x1(*pred* ∧ hTENSEipres ∧ hARG0ix2 ∧ hARG1ix3) (2) np1/n1 =⇒ np1:x1/n1:x1 : @x1(hDETi(d ∧ *pred*)) (3) np1/n1\np2 =⇒ np1:x1/n1:x1\np2:x2 : @x1(hGENOWNix2) After logical form insertion, the extracted and augmented grammar is loaded and used to parse</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4224" citStr="Papineni et al., 2002" startWordPosition="619" endWordPosition="622">ion ratios ranging from 1.1 to 3.9).2 Second, we compare a hypertagger-augmented version of OpenCCG’s chart realizer with the preexisting chart realizer (White et al., 2007) that simply instantiates the chart with all possible CCG categories (subject to frequency cutoffs) for each input LF predicate. The hypertagger-seeded realizer runs approximately twice as fast as the pre-existing OpenCCG realizer and finds a larger number of complete realizations, resorting less to chart fragment assembly in order to produce an output within a 15 second per-sentence time limit. Moreover, the overall BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCGbank) are higher for the hypertagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization wit</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University Of Chicago Press.</publisher>
<contexts>
<context position="1248" citStr="Pollard and Sag, 1994" startWordPosition="169" endWordPosition="172"> open-source NLP toolkit for CCG. We call this approach hypertagging, as it operates at a level “above” the syntax, tagging semantic representations with syntactic lexical categories. Our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed (being approximately twice as fast) with superior realization quality. 1 Introduction In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar (Schabes et al., 1988, LTAG), Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven PhraseStructure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment — the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates — from the combinatory processes that make use of such categories — such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is superta</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl J Pollard and Ivan A Sag. 1994. Head-Driven Phrase Structure Grammar. University Of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abeill´e</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with ’lexicalized’ grammars: Application to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proceedings of the 121h International Conference on Computational Linguistics (COLING-88),</booktitle>
<location>Budapest.</location>
<marker>Schabes, Abeill´e, Joshi, 1988</marker>
<rawString>Yves Schabes, Anne Abeill´e, and Aravind K. Joshi. 1988. Parsing strategies with ’lexicalized’ grammars: Application to tree adjoining grammars. In Proceedings of the 121h International Conference on Computational Linguistics (COLING-88), Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="1179" citStr="Steedman, 2000" startWordPosition="161" endWordPosition="162">shi, 1999; Clark, 2002) — for chart realization in OpenCCG, an open-source NLP toolkit for CCG. We call this approach hypertagging, as it operates at a level “above” the syntax, tagging semantic representations with syntactic lexical categories. Our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed (being approximately twice as fast) with superior realization quality. 1 Introduction In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar (Schabes et al., 1988, LTAG), Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven PhraseStructure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment — the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates — from the combinatory processes that make use of such categories — such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A r</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM — An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP-02.</booktitle>
<contexts>
<context position="12488" citStr="Stolcke, 2002" startWordPosition="1933" endWordPosition="1934">6% of the development logical forms are semantic dependency graphs with a single root, while 76.7% of the test logical forms have a single root. The remaining cases, with multiple roots, are missing one or more dependencies required to form a fully connected graph. These missing dependencies usually reflect inadequacies in the current logical form templates. 2.3 Factored Language Models Following White et al. (2007), we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2–21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates a word trigram model with a trigram model that chains a POS model with a supertag model,</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM — An extensible language modeling toolkit. In Proc. ICSLP-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum entropy models for realization ranking.</title>
<date>2005</date>
<booktitle>In Proc. MT Summit X.</booktitle>
<contexts>
<context position="12728" citStr="Velldal and Oepen, 2005" startWordPosition="1968" endWordPosition="1972">ed to form a fully connected graph. These missing dependencies usually reflect inadequacies in the current logical form templates. 2.3 Factored Language Models Following White et al. (2007), we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2–21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates a word trigram model with a trigram model that chains a POS model with a supertag model, where the POS model conditions on the previous two POS tags, and the supertag model conditions on the previous two POS tags as well as the current one. Note that the use of supertags in the factored language model to score possible realiza</context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>Erik Velldal and Stephan Oepen. 2005. Maximum entropy models for realization ranking. In Proc. MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Statistical ranking in tactical generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia,</location>
<marker>Velldal, Oepen, 2006</marker>
<rawString>Erik Velldal and Stephan Oepen. 2006. Statistical ranking in tactical generation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards broad coverage surface realization with CCG.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</booktitle>
<contexts>
<context position="3775" citStr="White et al., 2007" startWordPosition="548" endWordPosition="551">yntax, moving from semantic representations to syntactic categories. We evaluate this hypertagger in two ways: first, 1http://openccg.sourceforge.net. 183 Proceedings ofACL-08: HLT, pages 183–191, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics we evaluate it as a tagger, where the hypertagger achieves high single-best (93.6%) and multitagging labelling accuracies (95.8–99.4% with category per lexical predication ratios ranging from 1.1 to 3.9).2 Second, we compare a hypertagger-augmented version of OpenCCG’s chart realizer with the preexisting chart realizer (White et al., 2007) that simply instantiates the chart with all possible CCG categories (subject to frequency cutoffs) for each input LF predicate. The hypertagger-seeded realizer runs approximately twice as fast as the pre-existing OpenCCG realizer and finds a larger number of complete realizations, resorting less to chart fragment assembly in order to produce an output within a 15 second per-sentence time limit. Moreover, the overall BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCGbank)</context>
<context position="8877" citStr="White et al. (2007)" startWordPosition="1368" endWordPosition="1371">rched for the best edge whose semantic coverage is disjoint from those selected so far; this process repeats until no further edges can be added to the set of selected fragments. In the final step, these fragments are concatenated, again in a greedy fashion, this time according to the n-gram score of the concatenated edges: starting with the original best edge, the fragment whose concatenation on the left or right side yields the highest score is chosen as the one to concatenate next, until all the fragments have been concatenated into a single output. 2.2 Realization from an Enhanced CCGbank White et al. (2007) describe an ongoing effort to engineer a grammar from the CCGbank (Hockenmaier and Steedman, 2007) — a corpus of CCG derivations derived from the Penn Treebank — suitable for realization with OpenCCG. This process involves converting the corpus to reflect more precise analyses, where feasible, and adding semantic representations to the lexical categories. In the first step, the derivations in the CCGbank are revised to reflect the desired syntactic derivations. Changes to the derivations are necessary to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-mo</context>
<context position="12293" citStr="White et al. (2007)" startWordPosition="1901" endWordPosition="1904">m succeeds in creating logical forms for 97.7% of the sentences in the development section (Sect. 00) of the converted CCGbank, and 96.1% of the sentences in the test section (Sect. 23). Of these, 76.6% of the development logical forms are semantic dependency graphs with a single root, while 76.7% of the test logical forms have a single root. The remaining cases, with multiple roots, are missing one or more dependencies required to form a fully connected graph. These missing dependencies usually reflect inadequacies in the current logical form templates. 2.3 Factored Language Models Following White et al. (2007), we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2–21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ in</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White, Rajakrishnan Rajkumar, and Scott Martin. 2007. Towards broad coverage surface realization with CCG. In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>CCG chart realization from disjunctive inputs.</title>
<date>2006</date>
<booktitle>In Proceedings, INLG</booktitle>
<contexts>
<context position="5245" citStr="White, 2006" startWordPosition="775" endWordPosition="776"> and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [... ] makes use of n-gram language models over words represented as vectors of factors, including surface form, part of speech, supertag and semantic class. </context>
<context position="6774" citStr="White, 2006" startWordPosition="1029" endWordPosition="1030"> mode, a packed forest of all possible realizations is created in the first stage; in the second stage, the packed representation is unpacked in bottom-up fashion, with scores assigned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Pruning takes place within equivalence classes of edges. Additionally, to realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. (NUM)sg); nodes are connected via dependency relations (e.g. (ARG0)). Internally, such he &lt;Arg1&gt; p1 point h2 &lt;NUM&gt;sg &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; &lt;GenRel&gt; h1 want.01 a1 a w1 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h3 he m1 make.03 184 graphs are represented using Hybrid Logic Dependency Semantics (HLDS),</context>
</contexts>
<marker>White, 2006</marker>
<rawString>Michael White. 2006a. CCG chart realization from disjunctive inputs. In Proceedings, INLG 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Efficient realization of coordinate structures in Combinatory Categorial Grammar.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5245" citStr="White, 2006" startWordPosition="775" endWordPosition="776"> and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [... ] makes use of n-gram language models over words represented as vectors of factors, including surface form, part of speech, supertag and semantic class. </context>
<context position="6774" citStr="White, 2006" startWordPosition="1029" endWordPosition="1030"> mode, a packed forest of all possible realizations is created in the first stage; in the second stage, the packed representation is unpacked in bottom-up fashion, with scores assigned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Pruning takes place within equivalence classes of edges. Additionally, to realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. (NUM)sg); nodes are connected via dependency relations (e.g. (ARG0)). Internally, such he &lt;Arg1&gt; p1 point h2 &lt;NUM&gt;sg &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; &lt;GenRel&gt; h1 want.01 a1 a w1 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h3 he m1 make.03 184 graphs are represented using Hybrid Logic Dependency Semantics (HLDS),</context>
</contexts>
<marker>White, 2006</marker>
<rawString>Michael White. 2006b. Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation, 4(1):39–75.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>