<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984032">
Incremental Interpretation of Categorial Grammar*
</title>
<author confidence="0.985309">
David Milward
</author>
<affiliation confidence="0.937812333333333">
Centre for Cognitive Science
University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, U.K.
</affiliation>
<email confidence="0.695126">
davidmOcogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.960313" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976475">
The paper describes a parser for Catego-
rial Grammar which provides fully word
by word incremental interpretation. The
parser does not require fragments of sen-
tences to form constituents, and thereby
avoids problems of spurious ambiguity.
The paper includes a brief discussion of
the relationship between basic Catego-
rial Grammar and other formalisms such
as HPSG, Dependency Grammar and
the Lambek Calculus. It also includes
a discussion of some of the issues which
arise when parsing lexicalised grammars,
and the possibilities for using statistical
techniques for tuning to particular lan-
guages.
</bodyText>
<sectionHeader confidence="0.995595" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985126934782609">
There is a large body of psycholinguistic evidence
which suggests that meaning can be extracted be-
fore the end of a sentence, and before the end
of phrasal constituents (e.g. Marslen-Wilson 1973,
Tanenhaus et al. 1990). There is also recent evi-
dence suggesting that, during speech processing,
partial interpretations can be built extremely ra-
pidly, even before words are completed (Spivey-
Knowlton et al. 1994)&apos; . There are also potential
computational applications for incremental inter-
pretation, including early parse filtering using sta-
tistics based on logical form plausibility, and in-
terpretation of fragments of dialogues (a survey
is provided by Milward and Cooper, 1994, hence-
forth referred to as M&amp;C).
In the current computational and psycholingui-
stic literature there are two main approaches to
the incremental construction of logical forms. One
approach is to use a grammar with &apos;non-standard&apos;
*This research was supported by the U.K. Science
and Engineering Research Council, grant RR30718.
I am grateful to Patrick Sturt, Carl Vogel, and the
reviewers for comments on an earlier version.
&apos;Spivey-Knowlton et al. reported 3 experiments,
One showed effects before the end of a word when
there was no other appropriate word with the same
initial phonology. Another showed on-line effects
from adjectives and determiners during noun phrase
processing.
constituency, so that an initial fragment of a sen-
tence, such as John likes, can be treated as a con-
stituent, and hence be assigned a type and a se-
mantics. This approach is exemplified by Com-
binatory Categorial Grammar, CCG (Steedman
1991), which takes a basic CG with just applica-
tion, and adds various new ways of combining ele-
ments together2. Incremental interpretation can
then be achieved using a standard bottom-up shift
reduce parser, working from left to right along
the sentence. The alternative approach, exempli-
fied by the work of Stabler on top-down parsing
(Stabler 1991), and Pulman on left-corner parsing
(Pulman 1986) is to associate a semantics directly
with the partial structures formed during a top-
down or left-corner parse. For example, a syntax
tree missing a noun phrase, such as the following
</bodyText>
<equation confidence="0.7340464">
/\
np vp
John / \
V np-
likes
</equation>
<bodyText confidence="0.99996915">
can be given a semantics as a function from enti-
ties to truth values i.e. Ax. likes(john,x), with-
out having to say that John likes is a constituent.
Neither approach is without problems. If a
grammar is augmented with operations which are
powerful enough to make most initial fragments
constituents, then there may be unwanted inter-
actions with the rest of the grammar (examples
of this in the case of CCG and the Lambek Cal-
culus are given in Section 2). The addition of
extra operations also means that, for any given
reading of a sentence there will generally be many
different possible derivations (so-called &apos;spurious&apos;
ambiguity), making simple parsing strategies such
as shift-reduce highly inefficient.
The limitations of the parsing approaches be-
come evident when we consider grammars with
left recursion. In such cases a simple top-down
parser will be incomplete, and a left corner parser
will resort to buffering the input (so won&apos;t be fully
</bodyText>
<footnote confidence="0.6026225">
&apos;Note that CCG doesn&apos;t provide a type for all in-
itial fragments of sentences. For example, it gives a
type to John thinks Mary, but not to John thinks each.
In contrast the Lambek Calculus (Lambek 1958) pro-
vides an infinite number of types for any initial sen-
tence fragment.
</footnote>
<page confidence="0.998396">
119
</page>
<bodyText confidence="0.925753904761905">
word-by-word). M&amp;C illustrate the problem by
considering the fragment Mary thinks John. This
has a small number of possible semantic repre-
sentations (the exact number depending upon the
grammar) e.g.
AP. thinks (mary,P (john))
AP.AQ. Q(thinks(mary,P(john)))
AFAR. (R(Ax.thinks(x,P(john))))(mary)
The second representation is appropriate if the
sentence finishes with a sentential modifier. The
third allows there to be a verb phrase modifier.
If the semantic representation is to be read off
syntactic structure, then the parser must provide
a single syntax tree (possibly with empty nodes).
However, there are actually any number of such
syntax trees corresponding to, for example, the
first semantic representation, since the np and the
s can be arbitrarily far apart. The following tree is
suitable for the sentence Mary thinks John shaves
but not for e.g. Mary thinks John coming here was
a mistake.
</bodyText>
<equation confidence="0.992643714285714">
/ \
np vp
Mary / \
V s
thinks / \
np VP_
John
</equation>
<bodyText confidence="0.952830238095238">
M&amp;C suggest various possibilities for packing the
partial syntax trees, including using Tree Adjoi-
ning Grammar (Joshi 1987) or Description Theory
(Marcus et al. 1983). One further possibility is to
choose a single syntax tree, and to use destructive
tree operations later in the parse3.
The approach which we will adopt here is based
on Milward (1992, 1994). Partial syntax trees can
be regarded as performing two main roles. The
first is to provide syntactic information which gui-
des how the rest of the sentence can be integrated
into the tree. The second is to provide a basis for a
semantic representation. The first role can be cap-
tured using syntactic types, where each type corre-
sponds to a potentially infinite number of partial
syntax trees. The second role can be captured by
the parser constructing semantic representations
directly. The general processing model therefore
consists of transitions of the form:
Syntactic type, _÷ Syntactic typei+i
Semantic rep, Semantic repi+i
</bodyText>
<footnote confidence="0.929501777777778">
3This might turn out to be similar to one view of
Tree Adjoining Grammar, where adjunction adds into
a pre-existing well-formed tree structure. It is also
closer to some methods for incremental adaptation of
discourse structures, where additions are allowed to
the right-frontier of a tree structure (e.g. Polanyi and
Scha 1984). There are however problems with this
kind of approach when features are considered (see
e.g. Vijay-Shanker 1992).
</footnote>
<bodyText confidence="0.99995536">
This provides a state-transition or dynamic model
of processing, with each state being a pair of a
syntactic type and a semantic value.
The main difference between our approach and
that of Milward (1992, 1994) is that it is based
on a more expressive grammar formalism, Appli-
cative Categorial Grammar, as opposed to Lexi-
calised Dependency Grammar. Applicative Cate-
gorial Grammars allow categories to have argu-
ments which are themselves functions (e.g. very
can be treated as a function of a function, and gi-
ven the type (n/n)/(n/n) when used as an adjec-
tival modifier). The ability to deal with functions
of functions has advantages in enabling more ele-
gant linguistic descriptions, and in providing one
kind of robust parsing: the parser never fails until
the last word, since there could always be a final
word which is a function over all the constituents
formed so far. However, there is a corresponding
problem of far greater non-determinism, with even
unambiguous words allowing many possible tran-
sitions. It therefore becomes crucial to either per-
form some kind of ambiguity packing, or language
tuning. This will be discussed in the final section
of the paper.
</bodyText>
<sectionHeader confidence="0.976835" genericHeader="method">
2 Applicative Categorial Grammar
</sectionHeader>
<bodyText confidence="0.9913384">
Applicative Categorial Grammar is the most ba-
sic form of Categorial Grammar, with just a single
combination rule corresponding to function appli-
cation. It was first applied to linguistic descrip-
tion by Adjukiewicz and Bar-Hillel in the 1950s.
Although it is still used for linguistic description
(e.g. Houma and van Noord, 1994), it has been
somewhat overshadowed in recent years by HPSG
(Pollard and Sag 1994), and by Lambek Cate-
gorial Grammars (Lambek 1958). It is therefore
worth giving some brief indications of how it fits
in with these developments.
The first directed Applicative CG was proposed
by Bar-Hillel (1953). Functional types included a
list of arguments to the left, and a list of argu-
ments to the right. Translating Bar-Hillel&apos;s nota-
tion into a feature based notation similar to that
in HPSG (Pollard and Sag 1994), we obtain the
following category for a ditransitive verb such as
put:
</bodyText>
<equation confidence="0.915415">
[s
i(np)
r(np, pp)
</equation>
<bodyText confidence="0.999185">
The list of arguments to the left are gathered un-
der the feature, 1, and those to the right, an np
and a pp in that order, under the feature r.
Bar-Hillel employed a single application rule,
which corresponds to the following:
</bodyText>
<page confidence="0.878285">
120
</page>
<equation confidence="0.815872666666667">
... L1[X
1(1,1 ... LTh) R1 ... Rn, X
r(Ri . • • Rrt)
</equation>
<bodyText confidence="0.996354581395349">
The result was a system which comes very close to
the formalised dependency grammars of Gaifman
(1965) and Hays (1964). The only real difference
is that Bar-Hillel allowed arguments to themsel-
ves be functions. For example, an adverb such as
slowly could be given the type4
An unfortunate aspect of Bar-Hillel&apos;s first system
was that the application rule only ever resulted
in a primitive type. Hence, arguments with fun-
ctional types had to correspond to single lexical
items: there was no way to form the type np \ s5
for a non-lexical verb phrase such as likes Mary.
Rather than adapting the Application Rule to
allow functions to be applied to one argument at a
time, Bar-Hillel&apos;s second system (often called AB
Categorial Grammar, or Adjukiewicz/Bar-Hillel
CG, Bar-Hillel 1964) adopted a &apos;Curried&apos; nota-
tion, and this has been adopted by most CGs
since. To represent a function which requires an
np on the left, and an np and a pp to the right,
there is a choice of the following three types using
Curried notation:
np \ ((s/pp)/np)
(np \ (s/pp))/np
((np\s)/pp)/np
Most CGs either choose the third of these (to give
a vp structure), or include a rule of Associativity
which means that the types are interchangeable
(in the Lambek Calculus, Associativity is a conse-
quence of the calculus, rather than being specified
separately).
The main impetus to change Applicative CG
came from the work of Ades and Steedman (1982).
Ades and Steedman noted that the use of function
composition allows CGs to deal with unbounded
dependency constructions. Function composition
enables a function to be applied to its argument,
even if that argument is incomplete e.g.
s/pp + pp/np s/np
This allows peripheral extraction, where the &apos;gap&apos;
is at the start or the end of e.g. a relative clause.
Variants of the composition rule were proposed
in order to deal with non-peripheral extraction,
</bodyText>
<footnote confidence="0.4076705">
4The reformulation is not entirely faithful here to
Bar-Hillel, who used a slightly problematic &apos;double
slash&apos; notation for functions of functions.
5Lambek notation (Lambek 1958).
</footnote>
<bodyText confidence="0.989231140350877">
but this led to unwanted effects elsewhere in the
grammar (Houma 1987). Subsequent treatments
of non-peripheral extraction based on the Lambek
Calculus (where standard composition is built in:
it is a rule which can be proven from the calcu-
lus) have either introduced an alternative to the
forward and backward slashes i.e. / and \ for nor-
mal args, t for wh-args (Moortgat 1988), or have
introduced so called modal operators on the wh-
argument (Morrill et al. 1990). Both techniques
can be thought of as marking the wh-arguments as
requiring special treatment, and therefore do not
lead to unwanted effects elsewhere in the gram-
mar.
However, there are problems with having just
composition, the most basic of the non-applicative
operations. In CGs which contain functions of
functions (such as very, or slowly), the addition of
composition adds both new analyses of sentences,
and new strings to the language. This is due to
the fact that composition can be used to form a
function, which can then be used as an argument
to a function of a function. For example, if the
two types, n/n and n/n are composed to give the
type n/n, then this can be modified by an adjec-
tival modifier of type (n/n)/(n/n). Thus, the
noun very old dilapidated car can get the unac-
ceptable bracketing, [[very [old dilapidated]] car].
Associative CGs with Composition, or the Lam-
bek Calculus also allow strings such as boy with
the to be given the type n/n predicting very boy
with the car to be an acceptable noun. Although
individual examples might be possible to rule out
using appropriate features, it is difficult to see how
to do this in general whilst retaining a calculus
suitable for incremental interpretation.
If wh-arguments need to be treated specially
anyway (to deal with non-peripheral extraction),
and if composition as a general rule is proble-
matic, this suggests we should perhaps return to
grammars which use just Application as a gene-
ral operation, but have a special treatment for
wh-arguments. Using the non-Curried notation
of Bar-Hillel, it is more natural to use a separate
wh-list than to mark wh-arguments individually.
For example, the category appropriate for relative
clauses with a noun phrase gap would be:
r()
w(np)
It is then possible to specify operations which act
as purely applicative operations with respect to
the left and right arguments lists, but more like
composition with respect to the wh-list. This is
very similar to the way in which wh-movement
is dealt with in GPSG (Gazdar et al. 1985) and
HPSG, where wh-arguments are treated using
slash mechanisms or feature inheritance principles
</bodyText>
<page confidence="0.990289">
121
</page>
<bodyText confidence="0.999942285714286">
which correspond closely to function composition.
Given that our arguments have produced a cate-
gorial grammar which looks very similar to HPSG,
why not use HPSG rather than Applicative CG?
The main reason is that Applicative CG is a much
simpler formalism, which can be given a very sim-
ple syntax semantics interface, with function ap-
plication in syntax mapping to function applica-
tion in semantics&amp;quot;. This in turn makes it relati-
vely easy to provide proofs of soundness and com-
pleteness for an incremental parsing algorithm.
Ultimately, some of the techniques developed here
should be able to be extended to more complex
formalisms such as HPSG.
</bodyText>
<sectionHeader confidence="0.9965545" genericHeader="method">
3 AB Categorial grammar with
Associativity (AACG)
</sectionHeader>
<bodyText confidence="0.999896428571429">
In this section we define a grammar similar to Bar-
Hillel&apos;s first grammar. However, unlike Bar-Hillel,
we allow one argument to be absorbed at a time.
The resulting grammar is equivalent to AB Cate-
gorial Grammar plus associativity.
The categories of the grammar are defined as
follows:
</bodyText>
<equation confidence="0.88658">
I. If X is a syntactic type (e.g. s, np), then
X
10 is a category.
2. If X is a syntactic type, and L and R are lists
of categories, then
L 1xL1 is a category.
rR
</equation>
<bodyText confidence="0.973549909090909">
Application to the right is defined by the rule8:
60ne area where application based approaches to
semantic combination gain in simplicity over unifica-
tion based approaches is in providing semantics for
functions of functions. Moore (1989) provides a treat-
ment of functions of functions in a unification based
approach, but only by explicitly incorporating lambda
expressions. Pollard and Sag (1994) deal with some
functions of functions, such as non-intersective adjec-
tives, by explicit set construction.
7As discussed above, wh-movement requires some-
thing more like composition than application. A sim-
ple syntax semantics interface can be retained if the
same operation is used in both syntax and semantics.
Wh-arguments can be treated as similar to other ar-
guments i.e. as lambda abstracted in the semantics.
For example, the fragment: John found a woman who
Mary can be given the semantics AP.Dx. woman(x)
&amp; found(john,x) &amp; P(mary,x), where P is a fun-
ction from a left argument Mary of type e and a wh-
argument, also of type e.
s&apos;•&apos; is list concatenation e.g. (np)•(s) equals (np,$).
</bodyText>
<equation confidence="0.950567">
X
r(Ri)eR
rR
1L + 1L
[
X
Application to the left is defined by the rule:
X X
L1 + [1(L1).L1 = 1L
rR rR
</equation>
<bodyText confidence="0.99988775">
The basic grammar provides some spurious deri-
vations, since sentences such as John likes Mary
can be bracketed as either ((John likes) Mary)
or (John (likes Mary)). However, we will see
that these spurious derivations do not translate
into spurious ambiguity in the parser, which maps
from strings of words directly to semantic repre-
sentations.
</bodyText>
<sectionHeader confidence="0.985692" genericHeader="method">
4 An Incremental Parser
</sectionHeader>
<bodyText confidence="0.956928743589744">
Most parsers which work left to right along an
input string can be described in terms of state
transitions i.e. by rules which say how the current
parsing state (e.g. a stack of categories, or a chart)
can be transformed by the next word into a new
state. Here this will be made particularly explicit,
with the parser described in terms of just two rules
which take a state, a new word and create a new
state&apos;. There are two unusual features. Firstly,
there is nothing equivalent to a stack mechanism:
at all times the state is characterised by a single
syntactic type, and a single semantic value, not
by some stack of semantic values or syntax trees
which are waiting to be connected together. Se-
condly, all transitions between states occur on the
input of a new word: there are no &apos;empty&apos; tran-
sitions (such as the reduce step of a shift-reduce
parser).
The two rules, which are given in Figure 110, are
difficult to understand in their most general form.
Here we will work upto the rules gradually, by con-
sidering which kinds of rules we might need in par-
ticular instances. Consider the following pairing
of sentence fragments with their simplest possible
CG type:
Mary thinks: s/s
Mary thinks John: sAnp \
Mary thinks John likes: s/np
Mary thinks John likes Sue: s
Now consider taking each type as a description of
the state that the parser is in after absorbing the
fragment. We obtain a sequence of transitions as
follows:
°This approach is described in greater detail in Mil-
ward (1994), where parsers are specified formally in
terms of their dynamics.
FL, Hi are lists of categories. lz and r, are lists
of variables, of the same length as the corresponding
L, and R.
</bodyText>
<page confidence="0.972542">
122
</page>
<figure confidence="0.9893035">
State-Application:
10
• R2
h()
X
IL()
where W: rRi•Ro
h()
&amp;quot;AA,&amp;quot;
r(
1.LieL
where W: rRi 4■R
h()
10
) • Lo
) • R2
• Ho
X
1L
rR
h()
rRo
1L
rR
110
- -
1(
h(
rRi. • (
State-Prediction:
10 X ) • R2 It)1
r( 1L1 .L0
h() rRo
hLi *Ho
h()
Ari.(Ah. F(A11. (h( r (((G ri)r)10))))
</figure>
<figureCaption confidence="0.8536175">
Figure 1: Transition Rules
&amp;quot;John&amp;quot; &amp;quot;likes&amp;quot; &amp;quot;Sue&amp;quot;
</figureCaption>
<bodyText confidence="0.933562028571429">
s/s s/(np\s) sinp s
If an embedded sentence such as John likes Sue
is a mapping from an s/s to an s, this suggests
that it might be possible to treat all sentences as
mapping from some category expecting an s to
that category i.e. from X/s to X. Similarly, all
noun phrases might be treated as mappings from
an X/np to an X.
Now consider individual transitions. The sim-
plest of these is where the type of argument ex-
pected by the state is matched by the next word
i.e.
where: Sue: np
This can be generalised to the following rule,
which is similar to Function Application in stan-
dard CGil
“Axp,
X/Y 4 X where: W: Y
A similar transition occurs for likes. Here an np\s
was expected, but likes only provides part of this:
&amp;quot;It differs in not being a rule of grammar here
the functor is a state category and the argument is a
lexical category. In standard CG function application,
the functor and argument can correspond to a word
or a phrase.
it requires an np to the right to form an np\s.
Thus after likes is absorbed the state category will
need to expect an np. The rule required is similar
to Function Composition in CG i.e.
«vr
X/Y X/Z where: W: Y/Z
Considering this informally in terms of tree struc-
tures, what is happening is the replacement of an
empty node in a partial tree by a second partial
tree i.e.
</bodyText>
<equation confidence="0.99993175">
X X
/\ / \
U Y- .&gt; U Y
V Z- V Z-
</equation>
<bodyText confidence="0.999959083333333">
The two rules specified so far need to be further
generalised to allow for the case where a lexical
item has more than one argument (e.g. if we re-
place likes by a di-transitive such as gives or a
tri-transitive such as bets). This is relatively tri-
vial using a non-curried notation similar to that
used for AACG. What we obtain is the single rule
of State-Application, which corresponds to appli-
cation when the list of arguments, R1, is empty,
to function composition when R1 is of length one,
and to n-ary composition when R1 is of length n.
The only change needed from AACG notation is
</bodyText>
<equation confidence="0.757577">
&amp;quot;Sue&amp;quot;
s/np s
</equation>
<page confidence="0.869645">
123
</page>
<figure confidence="0.999528952380953">
10
10 &amp;quot;John&amp;quot;
r(s)
h()
A-Q.Q
10
r(np)
ho
AY.likes;(john&apos;,Y)
10
r()
h()
les&apos;(-john&apos;,sue&apos;)
&amp;quot;S4e&amp;quot;
&amp;quot;likes&amp;quot;
1(np)
r()
h(np)
ho
(H(john&apos;))
r(
</figure>
<figureCaption confidence="0.999782">
Figure 2: Possible state transitions
</figureCaption>
<bodyText confidence="0.957867928571429">
the inclusion of an extra feature list, the h list,
which stores information about which arguments
are waiting for a head (the reasons for this will be
explained later). The lexicon is identical to that
for a standard AACG, except for having h-lists
which are always set to empty.
Now consider the first transition. Here a sen-
tence was expected, but what was encountered
was a noun phrase, John. The appropriate rule
in CG notation would be:
X/Y X/(Z\Y) where: W: Z
This rule states that if looking for a Y and get a
Z then look for a Y which is missing a Z. In tree
structure terms we have:
</bodyText>
<equation confidence="0.9972924">
X
/\
U Y
/ \
Z \ Y&amp;quot;
</equation>
<bodyText confidence="0.965122130434783">
The rule of State-Prediction is obtained by further
generalising to allow the lexical item to have mis-
sing arguments, and for the expected argument to
have missing arguments.
State-Application and State-Prediction to-
gether provide the basis of a sound and complete
parser12. Parsing of sentences is achieved by star-
ting in a state expecting a sentence, and apply-
ing the rules non-deterministically as each word
is input. A successful parse is achieved if the fi-
nal state expects no more arguments. As an ex-
ample, reconsider the string John likes Sue. The
sequence of transitions corresponding to John li-
kes Sue being a sentence, is given in Figure 2.
The transition on encountering John is determini-
stic: State-Application cannot apply, and State-
Prediction can only be instantiated one way. The
result is a new state expecting an argument which,
given an np could give an s i.e. an np\s.
&apos;The parser accepts the same strings as the gram-
mar and assigns them the same semantic values. This
is slightly different from the standard notion of so-
undness and completeness of a parser, where the par-
ser accepts the same strings as the grammar and as-
signs them the same syntax trees.
The transition on input of likes is non-
deterministic. State-Application can apply, as in
Figure 2. However, State-Prediction can also ap-
ply, and can be instantiated in four ways (these
correspond to different ways of cutting up the
left and right subcategorisation lists of the le-
xical entry, likes, i.e. as (np) • () or () • (np)).
One possibility corresponds to the prediction of
an s \ s modifier, a second to the prediction of an
(np\s)\(np\s) modifier (i.e. a verb phrase mo-
difier), a third to there being a function which
takes the subject and the verb as separate argu-
ments, and the fourth corresponds to there being a
function which requires an s/np argument. The
second of these is perhaps the most interesting,
and is given in Figure 3. It is the choice of this
particular transition at this point which allows
verb phrase modification, and hence, assuming the
next word is Sue, an implicit bracketing of the
string fragment as (John (likes Sue)). Note that
if State-Application is chosen, or the first of the
State-Prediction possibilities, the fragment John
likes Sue retains a flat structure. If there is to
be no modification of the verb phrase, no verb
phrase structure is introduced. This relates to
there being no spurious ambiguity: each choice of
transition has semantic consequences; each choice
affects whether a particular part of the semantics
is to be modified or not.
Finally, it is worth noting why it is necessary to
use h-lists. These are needed to distinguish bet-
ween cases of real functional arguments (of func-
tions of functions), and functions formed by State-
Prediction. Consider the following trees, where
the np\s node is empty.
/ \
np np\s
/\
(np\s)/(np\s) np\s
Both trees have the same syntactic type, however
in the first case we want to allow for there to be
an s\s modifier of the lower s, but not in the se-
cond. The headed list distinguishes between the
two cases, with only the first having an np on its
</bodyText>
<figure confidence="0.966647208333333">
&gt;
124
1(np)
ro
h()
1(
,np)
&amp;quot;likes&amp;quot;
r(
r(np,
1(np)
where W: r (np)
h()
AY.AX.Irkes&apos;(X,Y)
, rip)
r()
h(
1(np)
r()
h(np)
h()
(H(john&apos;))
h()
AY.A1{.(K(AX.Iikes&apos;(X,Y)))(john)
</figure>
<figureCaption confidence="0.999834">
Figure 3: Example instantiation of State-Prediction
</figureCaption>
<bodyText confidence="0.851487">
headed list, allowing prediction of an s modifier.
</bodyText>
<sectionHeader confidence="0.984353" genericHeader="method">
5 Parsing Lexicalised Grammars
</sectionHeader>
<bodyText confidence="0.999331986486487">
When we consider full sentence processing, as op-
posed to incremental processing, the use of lexi-
calised grammars has a major advantage over the
use of more standard rule based grammars. In
processing a sentence using a lexicalised formalism
we do not have to look at the grammar as a whole,
but only at the grammatical information indexed
by each of the words. Thus increases in the size
of a grammar don&apos;t necessarily effect efficiency of
processing, provided the increase in size is due to
the addition of new words, rather than increased
lexical ambiguity. Once the full set of possible le-
xical entries for a sentence is collected, they can,
if required, then be converted back into a set of
phrase structure rules (which should correspond
to a small subset of the rule based formalism equi-
valent to the whole lexicalised grammar), before
being parsing with a standard algorithm such as
Earley&apos;s (Earley 1970).
In incremental parsing we cannot predict which
words will appear in the sentence, so cannot use
the same technique. However, if we are to base a
parser on the rules given above, it would seem that
we gain further. Instead of grammatical informa-
tion being localised to the sentence as a whole, it
is localised to a particular word in its particular
context: there is no need to consider a pp as a
start of a sentence if it occurs at the end, even if
there is a verb with an entry which allows for a
subject pp.
However there is a major problem. As we noted
in the last paragraph, it is the nature of parsing
incrementally that we don&apos;t know what words are
to come next. But here the parser doesn&apos;t even
use the information that the words are to come
from a lexicon for a particular language. For ex-
ample, given an input of 3 nps, the parser will
happily create a state expecting 3 nps to the left.
This might be a likely state for say a head final
language, but an unlikely state for a language such
as English. Note that incremental interpretation
will be of no use here, since the semantic represen-
tation should be no more or less plausible in the
different languages. In practical terms, a naive in-
teractive parallel Prolog implementation on a cur-
rent workstation fails to be interactive in a real
sense after about 8 words&amp;quot;.
What seems to be needed is some kind of langu-
age tuning&amp;quot;. This could be in the nature of fixed
restrictions to the rules e.g. for English we might
rule out uses of prediction when a noun phrase is
encountered, and two already exist on the left list.
A more appealing alternative is to base the tuning
on statistical methods. This could be achieved by
running the parser over corpora to provide pro-
babilities of particular transitions given particu-
lar words. These transitions would capture the
likelihood of a word having a particular part of
speech, and the probability of a particular transi-
tion being performed with that part of speech.
&amp;quot;This result should however be treated with some
caution: in this implementation there was no attempt
to perform any packing of different possible transiti-
ons, and the algorithm has exponential complexity. In
contrast, a packed recogniser based on a similar, but
much simpler, incremental parser for Lexicalised De-
pendency Grammar has 0(n3) time complexity (Mil-
ward 1994) and good practical performance, taking a
couple of seconds on 30 word sentences.
&amp;quot;The usage of the term language tuning is perhaps
broader here than its use in the psycholinguistic lite-
rature to refer to different structural preferences bet-
ween languages e.g. for high versus low attachment
(Mitchell et al. 1992).
</bodyText>
<page confidence="0.995718">
125
</page>
<bodyText confidence="0.9993539375">
There has already been some early work done on
providing statistically based parsing using transi-
tions between recursively structured syntactic ca-
tegories (Tugwell 1995)15. Unlike a simple Markov
process, there are a potentially infinite number of
states, so there is inevitably a problem of sparse
data. It is therefore necessary to make various
generalisations over the states, for example by ig-
noring the R.2 lists.
The full processing model can then be either
serial, exploring the most highly ranked transiti-
ons first (but allowing backtracking if the seman-
tic plausibility of the current interpretation drops
too low), or ranked parallel, exploring just the n
paths ranked highest according to the transition
probabilities and semantic plausibility.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999928428571429">
The paper has presented a method for providing
interpretations word by word for basic Categorial
Grammar. The final section contrasted parsing
with lexicalised and rule based grammars, and ar-
gued that statistical language tuning is particu-
larly suitable for incremental, lexicalised parsing
strategies.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.978929901960785">
Ades, A. &amp; Steedman, M.: 1972, &apos;On the Order of
Words&apos;, Linguistics e_4 Philosophy 4, 517-558.
Bar-Hillel, Y.: 1953, &apos;A Quasi-Arithmetical Notation
for Syntactic Description&apos;, Language 29, 47-58.
Bar-Hillel, Y.: 1964, Language e4 Information:
Selected Essays on Their Theory e4 Application,
Addison-Wesley.
Bouma, G.: 1987, &apos;A Unification-Based Analysis of
Unbounded Dependencies&apos;, in Proceedings of the
6th Amsterdam Colloquium, ITLI, University of
Amsterdam.
Bouma, G. &amp; van Noord, G.: 1994, &apos;Constraint-Based
Categorial Grammar&apos;, in Proceedings of the 32nd
ACL, Las Cruces, U.S.A.
Earley, J.: 1970, &apos;An Efficient Context-free Parsing
Algorithm&apos;, ACM Communications 13(2), 94-102.
Gaifman, H.: 1965, &apos;Dependency Systems &amp; Phrase
Structure Systems&apos;, Information E4 Control 8: 304-
337.
Gazdar, G., Klein, E., Pullum, G.K., &amp; Sag,
I.A.: 1985, Generalized Phrase Structure Gram-
mar, Blackwell, Oxford.
Hays, D.G.: 1964, &apos;Dependency Theory: A Forma-
lism &amp; Some Observations&apos;, Language 40, 511-525.
Joshi, A.K.: 1987, &apos;An Introduction to Tree Adjoining
Grammars&apos;, in Manaster-Ramer (ed.), Mathema-
tics of Language, John Benjamins, Amsterdam.
15Tugwell&apos;s approach does however differ in that the
state transitions are not limited by the rules of State-
Prediction and State-Application. This has advanta-
ges in allowing the grammar to learn phenomena such
as heavy NP shift, but has the disadvantage of suf-
fering from greater sparse data problems. A compro-
mise system using the rules here, but allowing reorde-
ring of the r-lists might be preferable.
Lambek, J.: 1958, &apos;The Mathematics of Sentence
Structure&apos;, American Mathematical Monthly 65,
154-169.
Marcus, M., Hindle, D., &amp; Fleck, M.: 1983, &apos;D-
Theory: Talking about Talking about Trees&apos;, in
Proceedings of the 21st ACL, Cambridge, Mass.
Marslen-Wilson, W.: 1973, &apos;Linguistic Structure &amp;
Speech Shadowing at Very Short Latencies&apos;, Na-
ture 244, 522-523.
Milward, D.: 1992, &apos;Dynamics, Dependency Gram-
mar &amp; Incremental Interpretation&apos;, in Proceedings
of COLING 92, Nantes, vol 4, 1095-1099.
Milward, D. &amp; Cooper, R.: 1994, &apos;Incremental Inter-
pretation: Applications, Theory &amp; Relationship to
Dynamic Semantics&apos;, in Proceedings of COLING
94, Kyoto, Japan, 748-754.
Milward, D.: 1994, &apos;Dynamic Dependency Grammar&apos;,
to appear in Linguistics &amp; Philosophy 17, 561-605.
Mitchell, D.C., Cuetos, F., &amp; Corley, M.M.B.: 1992,
&apos;Statistical versus linguistic determinants of par-
sing bias: cross-linguistic evidence&apos;. Paper presen-
ted at the 5th Annual CUNY Conference on Hu-
man Sentence Processing, New York.
Moore, R.C.: 1989, &apos;Unification-Based Semantic In-
terpretation&apos;, in Proceedings of the 27th ACL, Van-
couver.
Moortgat, M.: 1988, Categorial Investigations: Logi-
cal e4 Linguistic Aspects of the Lambek Calculus,
Foris, Dordrecht.
Morrill, G., Leslie, N., Hepple, M. &amp; Barry,G.: 1990,
&apos;Categorial Deductions &amp; Structural Operations&apos;,
in Barry, G. &amp; Morrill, G. (eds.), Studies in Ca-
tegorial Grammar, Edinburgh Working Papers in
Cognitive Science, 5.
Polanyi, L. &amp; Scha, R.: 1984, &apos;A Syntactic Approach
to Discourse Semantics&apos;, in Proceedings of CO-
LING 84, Stanford, 413-419.
Pollard, C. &amp; Sag, I.A.: 1994, Head-Driven Phrase
Structure Grammar, University of Chicago Press
&amp; CSLI Publications, Chicago.
Putman, S.G.: 1986, &apos;Grammars, Parsers, &amp; Memory
Limitations&apos;, Language &amp; Cognitive Processes 1(3),
197-225.
Spivey-Knowlton, M., Sedivy, J., Eberhard, K., &amp; Ta-
nenhaus, M.: 1994, `Psycholinguistic Study of the
Interaction Between Language &amp; Vision&apos;, in Pro-
ceedings of the 12th National Conference on Al,
AAAI-94.
Stabler, E.P.: 1991, &apos;Avoid the Pedestrian&apos;s Paradox&apos;,
in Berwick, R.C. et al. (eds.), Principle-Based Par-
sing: Computation E4 Psycholinguistics, Kluwer,
Netherlands, 199-237.
Steedman, M.J.: 1991, &apos;Type-Raising &amp; Directiona-
lity in Combinatory Grammar&apos;, in Proceedings of
the 29th ACL, Berkeley, U.S.A.
Tanenhaus, M.K., Garnsey, S., &amp; Boland, J.: 1990,
&apos;Combinatory Lexical Information &amp; Language
Comprehension&apos;, in Altmann, G.T.M. Cognitive
Models of Speech Processing, MIT Press, Cam-
bridge Ma.
Tugwell, D.: 1995, &apos;A State-Transition Grammar for
Data-Oriented Parsing&apos;, in Proceedings of the 7th
Conference of the European ACL, EACL-95, Dub-
lin, this volume.
Vijay-Shanker, K.: 1992, &apos;Using Descriptions of Trees
in a Tree Adjoining Grammar&apos;, Computational
Linguistics 18(4), 481-517.
</reference>
<page confidence="0.998524">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.825627">
<title confidence="0.999632">Incremental Interpretation of Categorial Grammar*</title>
<author confidence="0.999879">David Milward</author>
<affiliation confidence="0.9999225">Centre for Cognitive Science University of Edinburgh</affiliation>
<address confidence="0.954706">2 Buccleuch Place, Edinburgh, EH8 9LW, U.K.</address>
<email confidence="0.999269">davidmOcogsci.ed.ac.uk</email>
<abstract confidence="0.992066235294118">The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation. The parser does not require fragments of sentences to form constituents, and thereby avoids problems of spurious ambiguity. The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus. It also includes a discussion of some of the issues which arise when parsing lexicalised grammars, and the possibilities for using statistical techniques for tuning to particular languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ades</author>
<author>M Steedman</author>
</authors>
<title>On the Order of Words&apos;,</title>
<date>1972</date>
<journal>Linguistics e_4 Philosophy</journal>
<volume>4</volume>
<pages>517--558</pages>
<marker>Ades, Steedman, 1972</marker>
<rawString>Ades, A. &amp; Steedman, M.: 1972, &apos;On the Order of Words&apos;, Linguistics e_4 Philosophy 4, 517-558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
</authors>
<title>A Quasi-Arithmetical Notation for Syntactic Description&apos;,</title>
<date>1953</date>
<journal>Language</journal>
<volume>29</volume>
<pages>47--58</pages>
<contexts>
<context position="8448" citStr="Bar-Hillel (1953)" startWordPosition="1354" endWordPosition="1355">ative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Houma and van Noord, 1994), it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994), and by Lambek Categorial Grammars (Lambek 1958). It is therefore worth giving some brief indications of how it fits in with these developments. The first directed Applicative CG was proposed by Bar-Hillel (1953). Functional types included a list of arguments to the left, and a list of arguments to the right. Translating Bar-Hillel&apos;s notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994), we obtain the following category for a ditransitive verb such as put: [s i(np) r(np, pp) The list of arguments to the left are gathered under the feature, 1, and those to the right, an np and a pp in that order, under the feature r. Bar-Hillel employed a single application rule, which corresponds to the following: 120 ... L1[X 1(1,1 ... LTh) R1 ... Rn, X r(Ri . • • Rrt) The result was a</context>
</contexts>
<marker>Bar-Hillel, 1953</marker>
<rawString>Bar-Hillel, Y.: 1953, &apos;A Quasi-Arithmetical Notation for Syntactic Description&apos;, Language 29, 47-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
</authors>
<date>1964</date>
<booktitle>Language e4 Information: Selected Essays on Their Theory e4 Application,</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="9812" citStr="Bar-Hillel 1964" startWordPosition="1593" endWordPosition="1594">allowed arguments to themselves be functions. For example, an adverb such as slowly could be given the type4 An unfortunate aspect of Bar-Hillel&apos;s first system was that the application rule only ever resulted in a primitive type. Hence, arguments with functional types had to correspond to single lexical items: there was no way to form the type np \ s5 for a non-lexical verb phrase such as likes Mary. Rather than adapting the Application Rule to allow functions to be applied to one argument at a time, Bar-Hillel&apos;s second system (often called AB Categorial Grammar, or Adjukiewicz/Bar-Hillel CG, Bar-Hillel 1964) adopted a &apos;Curried&apos; notation, and this has been adopted by most CGs since. To represent a function which requires an np on the left, and an np and a pp to the right, there is a choice of the following three types using Curried notation: np \ ((s/pp)/np) (np \ (s/pp))/np ((np\s)/pp)/np Most CGs either choose the third of these (to give a vp structure), or include a rule of Associativity which means that the types are interchangeable (in the Lambek Calculus, Associativity is a consequence of the calculus, rather than being specified separately). The main impetus to change Applicative CG came fr</context>
</contexts>
<marker>Bar-Hillel, 1964</marker>
<rawString>Bar-Hillel, Y.: 1964, Language e4 Information: Selected Essays on Their Theory e4 Application, Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>A Unification-Based Analysis of Unbounded Dependencies&apos;,</title>
<date>1987</date>
<booktitle>in Proceedings of the 6th Amsterdam Colloquium, ITLI,</booktitle>
<institution>University of Amsterdam.</institution>
<marker>Bouma, 1987</marker>
<rawString>Bouma, G.: 1987, &apos;A Unification-Based Analysis of Unbounded Dependencies&apos;, in Proceedings of the 6th Amsterdam Colloquium, ITLI, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G van Noord</author>
</authors>
<title>Constraint-Based Categorial Grammar&apos;,</title>
<date>1994</date>
<booktitle>in Proceedings of the 32nd ACL,</booktitle>
<location>Las Cruces, U.S.A.</location>
<marker>Bouma, van Noord, 1994</marker>
<rawString>Bouma, G. &amp; van Noord, G.: 1994, &apos;Constraint-Based Categorial Grammar&apos;, in Proceedings of the 32nd ACL, Las Cruces, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-free Parsing Algorithm&apos;,</title>
<date>1970</date>
<journal>ACM Communications</journal>
<volume>13</volume>
<issue>2</issue>
<pages>94--102</pages>
<contexts>
<context position="25461" citStr="Earley 1970" startWordPosition="4330" endWordPosition="4331">at the grammatical information indexed by each of the words. Thus increases in the size of a grammar don&apos;t necessarily effect efficiency of processing, provided the increase in size is due to the addition of new words, rather than increased lexical ambiguity. Once the full set of possible lexical entries for a sentence is collected, they can, if required, then be converted back into a set of phrase structure rules (which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar), before being parsing with a standard algorithm such as Earley&apos;s (Earley 1970). In incremental parsing we cannot predict which words will appear in the sentence, so cannot use the same technique. However, if we are to base a parser on the rules given above, it would seem that we gain further. Instead of grammatical information being localised to the sentence as a whole, it is localised to a particular word in its particular context: there is no need to consider a pp as a start of a sentence if it occurs at the end, even if there is a verb with an entry which allows for a subject pp. However there is a major problem. As we noted in the last paragraph, it is the nature of</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J.: 1970, &apos;An Efficient Context-free Parsing Algorithm&apos;, ACM Communications 13(2), 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<title>Dependency Systems &amp; Phrase Structure Systems&apos;,</title>
<date>1965</date>
<journal>Information E4 Control</journal>
<volume>8</volume>
<pages>304--337</pages>
<contexts>
<context position="9134" citStr="Gaifman (1965)" startWordPosition="1481" endWordPosition="1482">of arguments to the right. Translating Bar-Hillel&apos;s notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994), we obtain the following category for a ditransitive verb such as put: [s i(np) r(np, pp) The list of arguments to the left are gathered under the feature, 1, and those to the right, an np and a pp in that order, under the feature r. Bar-Hillel employed a single application rule, which corresponds to the following: 120 ... L1[X 1(1,1 ... LTh) R1 ... Rn, X r(Ri . • • Rrt) The result was a system which comes very close to the formalised dependency grammars of Gaifman (1965) and Hays (1964). The only real difference is that Bar-Hillel allowed arguments to themselves be functions. For example, an adverb such as slowly could be given the type4 An unfortunate aspect of Bar-Hillel&apos;s first system was that the application rule only ever resulted in a primitive type. Hence, arguments with functional types had to correspond to single lexical items: there was no way to form the type np \ s5 for a non-lexical verb phrase such as likes Mary. Rather than adapting the Application Rule to allow functions to be applied to one argument at a time, Bar-Hillel&apos;s second system (ofte</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, H.: 1965, &apos;Dependency Systems &amp; Phrase Structure Systems&apos;, Information E4 Control 8: 304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G K Pullum</author>
<author>Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar,</title>
<date>1985</date>
<location>I.A.:</location>
<contexts>
<context position="13596" citStr="Gazdar et al. 1985" startWordPosition="2224" endWordPosition="2227"> which use just Application as a general operation, but have a special treatment for wh-arguments. Using the non-Curried notation of Bar-Hillel, it is more natural to use a separate wh-list than to mark wh-arguments individually. For example, the category appropriate for relative clauses with a noun phrase gap would be: r() w(np) It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists, but more like composition with respect to the wh-list. This is very similar to the way in which wh-movement is dealt with in GPSG (Gazdar et al. 1985) and HPSG, where wh-arguments are treated using slash mechanisms or feature inheritance principles 121 which correspond closely to function composition. Given that our arguments have produced a categorial grammar which looks very similar to HPSG, why not use HPSG rather than Applicative CG? The main reason is that Applicative CG is a much simpler formalism, which can be given a very simple syntax semantics interface, with function application in syntax mapping to function application in semantics&amp;quot;. This in turn makes it relatively easy to provide proofs of soundness and completeness for an inc</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., Klein, E., Pullum, G.K., &amp; Sag, I.A.: 1985, Generalized Phrase Structure Grammar, Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Dependency Theory: A Formalism &amp; Some Observations&apos;,</title>
<date>1964</date>
<journal>Language</journal>
<volume>40</volume>
<pages>511--525</pages>
<contexts>
<context position="9150" citStr="Hays (1964)" startWordPosition="1484" endWordPosition="1485"> right. Translating Bar-Hillel&apos;s notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994), we obtain the following category for a ditransitive verb such as put: [s i(np) r(np, pp) The list of arguments to the left are gathered under the feature, 1, and those to the right, an np and a pp in that order, under the feature r. Bar-Hillel employed a single application rule, which corresponds to the following: 120 ... L1[X 1(1,1 ... LTh) R1 ... Rn, X r(Ri . • • Rrt) The result was a system which comes very close to the formalised dependency grammars of Gaifman (1965) and Hays (1964). The only real difference is that Bar-Hillel allowed arguments to themselves be functions. For example, an adverb such as slowly could be given the type4 An unfortunate aspect of Bar-Hillel&apos;s first system was that the application rule only ever resulted in a primitive type. Hence, arguments with functional types had to correspond to single lexical items: there was no way to form the type np \ s5 for a non-lexical verb phrase such as likes Mary. Rather than adapting the Application Rule to allow functions to be applied to one argument at a time, Bar-Hillel&apos;s second system (often called AB Cate</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>Hays, D.G.: 1964, &apos;Dependency Theory: A Formalism &amp; Some Observations&apos;, Language 40, 511-525.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A K Joshi</author>
</authors>
<title>An Introduction to Tree Adjoining Grammars&apos;,</title>
<date>1987</date>
<editor>in Manaster-Ramer (ed.),</editor>
<contexts>
<context position="5314" citStr="Joshi 1987" startWordPosition="847" endWordPosition="848">resentation is to be read off syntactic structure, then the parser must provide a single syntax tree (possibly with empty nodes). However, there are actually any number of such syntax trees corresponding to, for example, the first semantic representation, since the np and the s can be arbitrarily far apart. The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake. / \ np vp Mary / \ V s thinks / \ np VP_ John M&amp;C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al. 1983). One further possibility is to choose a single syntax tree, and to use destructive tree operations later in the parse3. The approach which we will adopt here is based on Milward (1992, 1994). Partial syntax trees can be regarded as performing two main roles. The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree. The second is to provide a basis for a semantic representation. The first role can be captured using syntactic types, where each type corresponds to a potentially infinite numb</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Joshi, A.K.: 1987, &apos;An Introduction to Tree Adjoining Grammars&apos;, in Manaster-Ramer (ed.), Mathematics of Language, John Benjamins, Amsterdam. 15Tugwell&apos;s approach does however differ in that the state transitions are not limited by the rules of StatePrediction and State-Application. This has advantages in allowing the grammar to learn phenomena such as heavy NP shift, but has the disadvantage of suffering from greater sparse data problems. A compromise system using the rules here, but allowing reordering of the r-lists might be preferable.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>The Mathematics of Sentence Structure&apos;,</title>
<date>1958</date>
<journal>American Mathematical Monthly</journal>
<volume>65</volume>
<pages>154--169</pages>
<contexts>
<context position="4164" citStr="Lambek 1958" startWordPosition="662" endWordPosition="663">there will generally be many different possible derivations (so-called &apos;spurious&apos; ambiguity), making simple parsing strategies such as shift-reduce highly inefficient. The limitations of the parsing approaches become evident when we consider grammars with left recursion. In such cases a simple top-down parser will be incomplete, and a left corner parser will resort to buffering the input (so won&apos;t be fully &apos;Note that CCG doesn&apos;t provide a type for all initial fragments of sentences. For example, it gives a type to John thinks Mary, but not to John thinks each. In contrast the Lambek Calculus (Lambek 1958) provides an infinite number of types for any initial sentence fragment. 119 word-by-word). M&amp;C illustrate the problem by considering the fragment Mary thinks John. This has a small number of possible semantic representations (the exact number depending upon the grammar) e.g. AP. thinks (mary,P (john)) AP.AQ. Q(thinks(mary,P(john))) AFAR. (R(Ax.thinks(x,P(john))))(mary) The second representation is appropriate if the sentence finishes with a sentential modifier. The third allows there to be a verb phrase modifier. If the semantic representation is to be read off syntactic structure, then the p</context>
<context position="8284" citStr="Lambek 1958" startWordPosition="1328" endWordPosition="1329">r perform some kind of ambiguity packing, or language tuning. This will be discussed in the final section of the paper. 2 Applicative Categorial Grammar Applicative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Houma and van Noord, 1994), it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994), and by Lambek Categorial Grammars (Lambek 1958). It is therefore worth giving some brief indications of how it fits in with these developments. The first directed Applicative CG was proposed by Bar-Hillel (1953). Functional types included a list of arguments to the left, and a list of arguments to the right. Translating Bar-Hillel&apos;s notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994), we obtain the following category for a ditransitive verb such as put: [s i(np) r(np, pp) The list of arguments to the left are gathered under the feature, 1, and those to the right, an np and a pp in that order, under the fea</context>
<context position="11083" citStr="Lambek 1958" startWordPosition="1803" endWordPosition="1804">noted that the use of function composition allows CGs to deal with unbounded dependency constructions. Function composition enables a function to be applied to its argument, even if that argument is incomplete e.g. s/pp + pp/np s/np This allows peripheral extraction, where the &apos;gap&apos; is at the start or the end of e.g. a relative clause. Variants of the composition rule were proposed in order to deal with non-peripheral extraction, 4The reformulation is not entirely faithful here to Bar-Hillel, who used a slightly problematic &apos;double slash&apos; notation for functions of functions. 5Lambek notation (Lambek 1958). but this led to unwanted effects elsewhere in the grammar (Houma 1987). Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \ for normal args, t for wh-args (Moortgat 1988), or have introduced so called modal operators on the whargument (Morrill et al. 1990). Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted </context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>Lambek, J.: 1958, &apos;The Mathematics of Sentence Structure&apos;, American Mathematical Monthly 65, 154-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>D Hindle</author>
<author>M Fleck</author>
</authors>
<title>DTheory: Talking about Talking about Trees&apos;,</title>
<date>1983</date>
<booktitle>in Proceedings of the 21st ACL,</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="5357" citStr="Marcus et al. 1983" startWordPosition="852" endWordPosition="855">ctic structure, then the parser must provide a single syntax tree (possibly with empty nodes). However, there are actually any number of such syntax trees corresponding to, for example, the first semantic representation, since the np and the s can be arbitrarily far apart. The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake. / \ np vp Mary / \ V s thinks / \ np VP_ John M&amp;C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al. 1983). One further possibility is to choose a single syntax tree, and to use destructive tree operations later in the parse3. The approach which we will adopt here is based on Milward (1992, 1994). Partial syntax trees can be regarded as performing two main roles. The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree. The second is to provide a basis for a semantic representation. The first role can be captured using syntactic types, where each type corresponds to a potentially infinite number of partial syntax trees. The second role</context>
</contexts>
<marker>Marcus, Hindle, Fleck, 1983</marker>
<rawString>Marcus, M., Hindle, D., &amp; Fleck, M.: 1983, &apos;DTheory: Talking about Talking about Trees&apos;, in Proceedings of the 21st ACL, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Marslen-Wilson</author>
</authors>
<title>Linguistic Structure &amp; Speech Shadowing at Very Short Latencies&apos;,</title>
<date>1973</date>
<journal>Nature</journal>
<volume>244</volume>
<pages>522--523</pages>
<contexts>
<context position="1000" citStr="Marslen-Wilson 1973" startWordPosition="146" endWordPosition="147">nd thereby avoids problems of spurious ambiguity. The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus. It also includes a discussion of some of the issues which arise when parsing lexicalised grammars, and the possibilities for using statistical techniques for tuning to particular languages. 1 Introduction There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence, and before the end of phrasal constituents (e.g. Marslen-Wilson 1973, Tanenhaus et al. 1990). There is also recent evidence suggesting that, during speech processing, partial interpretations can be built extremely rapidly, even before words are completed (SpiveyKnowlton et al. 1994)&apos; . There are also potential computational applications for incremental interpretation, including early parse filtering using statistics based on logical form plausibility, and interpretation of fragments of dialogues (a survey is provided by Milward and Cooper, 1994, henceforth referred to as M&amp;C). In the current computational and psycholinguistic literature there are two main appr</context>
</contexts>
<marker>Marslen-Wilson, 1973</marker>
<rawString>Marslen-Wilson, W.: 1973, &apos;Linguistic Structure &amp; Speech Shadowing at Very Short Latencies&apos;, Nature 244, 522-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Dynamics, Dependency Grammar &amp; Incremental Interpretation&apos;,</title>
<date>1992</date>
<booktitle>in Proceedings of COLING 92, Nantes,</booktitle>
<volume>4</volume>
<pages>1095--1099</pages>
<contexts>
<context position="5541" citStr="Milward (1992" startWordPosition="886" endWordPosition="887">first semantic representation, since the np and the s can be arbitrarily far apart. The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake. / \ np vp Mary / \ V s thinks / \ np VP_ John M&amp;C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al. 1983). One further possibility is to choose a single syntax tree, and to use destructive tree operations later in the parse3. The approach which we will adopt here is based on Milward (1992, 1994). Partial syntax trees can be regarded as performing two main roles. The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree. The second is to provide a basis for a semantic representation. The first role can be captured using syntactic types, where each type corresponds to a potentially infinite number of partial syntax trees. The second role can be captured by the parser constructing semantic representations directly. The general processing model therefore consists of transitions of the form: Syntactic type, _÷ Syntactic </context>
<context position="6825" citStr="Milward (1992" startWordPosition="1091" endWordPosition="1092">r to one view of Tree Adjoining Grammar, where adjunction adds into a pre-existing well-formed tree structure. It is also closer to some methods for incremental adaptation of discourse structures, where additions are allowed to the right-frontier of a tree structure (e.g. Polanyi and Scha 1984). There are however problems with this kind of approach when features are considered (see e.g. Vijay-Shanker 1992). This provides a state-transition or dynamic model of processing, with each state being a pair of a syntactic type and a semantic value. The main difference between our approach and that of Milward (1992, 1994) is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar. Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a function of a function, and given the type (n/n)/(n/n) when used as an adjectival modifier). The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions, and in providing one kind of robust parsing: the parser never fails until the last word, since there could always be a </context>
</contexts>
<marker>Milward, 1992</marker>
<rawString>Milward, D.: 1992, &apos;Dynamics, Dependency Grammar &amp; Incremental Interpretation&apos;, in Proceedings of COLING 92, Nantes, vol 4, 1095-1099.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
<author>R Cooper</author>
</authors>
<title>Incremental Interpretation: Applications, Theory &amp; Relationship to Dynamic Semantics&apos;,</title>
<date>1994</date>
<booktitle>in Proceedings of COLING 94,</booktitle>
<pages>748--754</pages>
<location>Kyoto, Japan,</location>
<contexts>
<context position="1482" citStr="Milward and Cooper, 1994" startWordPosition="216" endWordPosition="219">hich suggests that meaning can be extracted before the end of a sentence, and before the end of phrasal constituents (e.g. Marslen-Wilson 1973, Tanenhaus et al. 1990). There is also recent evidence suggesting that, during speech processing, partial interpretations can be built extremely rapidly, even before words are completed (SpiveyKnowlton et al. 1994)&apos; . There are also potential computational applications for incremental interpretation, including early parse filtering using statistics based on logical form plausibility, and interpretation of fragments of dialogues (a survey is provided by Milward and Cooper, 1994, henceforth referred to as M&amp;C). In the current computational and psycholinguistic literature there are two main approaches to the incremental construction of logical forms. One approach is to use a grammar with &apos;non-standard&apos; *This research was supported by the U.K. Science and Engineering Research Council, grant RR30718. I am grateful to Patrick Sturt, Carl Vogel, and the reviewers for comments on an earlier version. &apos;Spivey-Knowlton et al. reported 3 experiments, One showed effects before the end of a word when there was no other appropriate word with the same initial phonology. Another sh</context>
</contexts>
<marker>Milward, Cooper, 1994</marker>
<rawString>Milward, D. &amp; Cooper, R.: 1994, &apos;Incremental Interpretation: Applications, Theory &amp; Relationship to Dynamic Semantics&apos;, in Proceedings of COLING 94, Kyoto, Japan, 748-754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Dynamic Dependency Grammar&apos;, to appear in</title>
<date>1994</date>
<journal>Linguistics &amp; Philosophy</journal>
<volume>17</volume>
<pages>561--605</pages>
<contexts>
<context position="17893" citStr="Milward (1994)" startWordPosition="2963" endWordPosition="2965">re given in Figure 110, are difficult to understand in their most general form. Here we will work upto the rules gradually, by considering which kinds of rules we might need in particular instances. Consider the following pairing of sentence fragments with their simplest possible CG type: Mary thinks: s/s Mary thinks John: sAnp \ Mary thinks John likes: s/np Mary thinks John likes Sue: s Now consider taking each type as a description of the state that the parser is in after absorbing the fragment. We obtain a sequence of transitions as follows: °This approach is described in greater detail in Milward (1994), where parsers are specified formally in terms of their dynamics. FL, Hi are lists of categories. lz and r, are lists of variables, of the same length as the corresponding L, and R. 122 State-Application: 10 • R2 h() X IL() where W: rRi•Ro h() &amp;quot;AA,&amp;quot; r( 1.LieL where W: rRi 4■R h() 10 ) • Lo ) • R2 • Ho X 1L rR h() rRo 1L rR 110 - - 1( h( rRi. • ( State-Prediction: 10 X ) • R2 It)1 r( 1L1 .L0 h() rRo hLi *Ho h() Ari.(Ah. F(A11. (h( r (((G ri)r)10)))) Figure 1: Transition Rules &amp;quot;John&amp;quot; &amp;quot;likes&amp;quot; &amp;quot;Sue&amp;quot; s/s s/(np\s) sinp s If an embedded sentence such as John likes Sue is a mapping from an s/s to an </context>
<context position="27802" citStr="Milward 1994" startWordPosition="4744" endWordPosition="4746">abilities of particular transitions given particular words. These transitions would capture the likelihood of a word having a particular part of speech, and the probability of a particular transition being performed with that part of speech. &amp;quot;This result should however be treated with some caution: in this implementation there was no attempt to perform any packing of different possible transitions, and the algorithm has exponential complexity. In contrast, a packed recogniser based on a similar, but much simpler, incremental parser for Lexicalised Dependency Grammar has 0(n3) time complexity (Milward 1994) and good practical performance, taking a couple of seconds on 30 word sentences. &amp;quot;The usage of the term language tuning is perhaps broader here than its use in the psycholinguistic literature to refer to different structural preferences between languages e.g. for high versus low attachment (Mitchell et al. 1992). 125 There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories (Tugwell 1995)15. Unlike a simple Markov process, there are a potentially infinite number of states, so there is inevitably a</context>
</contexts>
<marker>Milward, 1994</marker>
<rawString>Milward, D.: 1994, &apos;Dynamic Dependency Grammar&apos;, to appear in Linguistics &amp; Philosophy 17, 561-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Mitchell</author>
<author>F Cuetos</author>
<author>Corley</author>
</authors>
<title>Statistical versus linguistic determinants of parsing bias: cross-linguistic evidence&apos;.</title>
<date>1992</date>
<booktitle>Paper presented at the 5th Annual CUNY Conference on Human Sentence Processing,</booktitle>
<location>M.M.B.:</location>
<contexts>
<context position="28116" citStr="Mitchell et al. 1992" startWordPosition="4794" endWordPosition="4797">s implementation there was no attempt to perform any packing of different possible transitions, and the algorithm has exponential complexity. In contrast, a packed recogniser based on a similar, but much simpler, incremental parser for Lexicalised Dependency Grammar has 0(n3) time complexity (Milward 1994) and good practical performance, taking a couple of seconds on 30 word sentences. &amp;quot;The usage of the term language tuning is perhaps broader here than its use in the psycholinguistic literature to refer to different structural preferences between languages e.g. for high versus low attachment (Mitchell et al. 1992). 125 There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories (Tugwell 1995)15. Unlike a simple Markov process, there are a potentially infinite number of states, so there is inevitably a problem of sparse data. It is therefore necessary to make various generalisations over the states, for example by ignoring the R.2 lists. The full processing model can then be either serial, exploring the most highly ranked transitions first (but allowing backtracking if the semantic plausibility of the current </context>
</contexts>
<marker>Mitchell, Cuetos, Corley, 1992</marker>
<rawString>Mitchell, D.C., Cuetos, F., &amp; Corley, M.M.B.: 1992, &apos;Statistical versus linguistic determinants of parsing bias: cross-linguistic evidence&apos;. Paper presented at the 5th Annual CUNY Conference on Human Sentence Processing, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>Unification-Based Semantic Interpretation&apos;,</title>
<date>1989</date>
<booktitle>in Proceedings of the 27th ACL,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="15081" citStr="Moore (1989)" startWordPosition="2475" endWordPosition="2476">However, unlike Bar-Hillel, we allow one argument to be absorbed at a time. The resulting grammar is equivalent to AB Categorial Grammar plus associativity. The categories of the grammar are defined as follows: I. If X is a syntactic type (e.g. s, np), then X 10 is a category. 2. If X is a syntactic type, and L and R are lists of categories, then L 1xL1 is a category. rR Application to the right is defined by the rule8: 60ne area where application based approaches to semantic combination gain in simplicity over unification based approaches is in providing semantics for functions of functions. Moore (1989) provides a treatment of functions of functions in a unification based approach, but only by explicitly incorporating lambda expressions. Pollard and Sag (1994) deal with some functions of functions, such as non-intersective adjectives, by explicit set construction. 7As discussed above, wh-movement requires something more like composition than application. A simple syntax semantics interface can be retained if the same operation is used in both syntax and semantics. Wh-arguments can be treated as similar to other arguments i.e. as lambda abstracted in the semantics. For example, the fragment: </context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Moore, R.C.: 1989, &apos;Unification-Based Semantic Interpretation&apos;, in Proceedings of the 27th ACL, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>Categorial Investigations: Logical e4 Linguistic Aspects of the Lambek Calculus,</title>
<date>1988</date>
<location>Foris, Dordrecht.</location>
<contexts>
<context position="11463" citStr="Moortgat 1988" startWordPosition="1867" endWordPosition="1868">oposed in order to deal with non-peripheral extraction, 4The reformulation is not entirely faithful here to Bar-Hillel, who used a slightly problematic &apos;double slash&apos; notation for functions of functions. 5Lambek notation (Lambek 1958). but this led to unwanted effects elsewhere in the grammar (Houma 1987). Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \ for normal args, t for wh-args (Moortgat 1988), or have introduced so called modal operators on the whargument (Morrill et al. 1990). Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted effects elsewhere in the grammar. However, there are problems with having just composition, the most basic of the non-applicative operations. In CGs which contain functions of functions (such as very, or slowly), the addition of composition adds both new analyses of sentences, and new strings to the language. This is due to the fact that composition can be used to form a functi</context>
</contexts>
<marker>Moortgat, 1988</marker>
<rawString>Moortgat, M.: 1988, Categorial Investigations: Logical e4 Linguistic Aspects of the Lambek Calculus, Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<title>Categorial Deductions &amp; Structural Operations&apos;,</title>
<date>1990</date>
<booktitle>Studies in Categorial Grammar, Edinburgh Working Papers in Cognitive Science,</booktitle>
<volume>5</volume>
<editor>Morrill, G., Leslie, N., Hepple, M. &amp; Barry,G.:</editor>
<marker>1990</marker>
<rawString>Morrill, G., Leslie, N., Hepple, M. &amp; Barry,G.: 1990, &apos;Categorial Deductions &amp; Structural Operations&apos;, in Barry, G. &amp; Morrill, G. (eds.), Studies in Categorial Grammar, Edinburgh Working Papers in Cognitive Science, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>R Scha</author>
</authors>
<title>A Syntactic Approach to Discourse Semantics&apos;,</title>
<date>1984</date>
<booktitle>in Proceedings of COLING 84,</booktitle>
<pages>413--419</pages>
<contexts>
<context position="6507" citStr="Polanyi and Scha 1984" startWordPosition="1038" endWordPosition="1041">o a potentially infinite number of partial syntax trees. The second role can be captured by the parser constructing semantic representations directly. The general processing model therefore consists of transitions of the form: Syntactic type, _÷ Syntactic typei+i Semantic rep, Semantic repi+i 3This might turn out to be similar to one view of Tree Adjoining Grammar, where adjunction adds into a pre-existing well-formed tree structure. It is also closer to some methods for incremental adaptation of discourse structures, where additions are allowed to the right-frontier of a tree structure (e.g. Polanyi and Scha 1984). There are however problems with this kind of approach when features are considered (see e.g. Vijay-Shanker 1992). This provides a state-transition or dynamic model of processing, with each state being a pair of a syntactic type and a semantic value. The main difference between our approach and that of Milward (1992, 1994) is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar. Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a functio</context>
</contexts>
<marker>Polanyi, Scha, 1984</marker>
<rawString>Polanyi, L. &amp; Scha, R.: 1984, &apos;A Syntactic Approach to Discourse Semantics&apos;, in Proceedings of COLING 84, Stanford, 413-419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<date>1994</date>
<publisher>Press &amp; CSLI Publications,</publisher>
<institution>Head-Driven Phrase Structure Grammar, University of Chicago</institution>
<location>Chicago.</location>
<contexts>
<context position="8235" citStr="Pollard and Sag 1994" startWordPosition="1318" endWordPosition="1321">ossible transitions. It therefore becomes crucial to either perform some kind of ambiguity packing, or language tuning. This will be discussed in the final section of the paper. 2 Applicative Categorial Grammar Applicative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Houma and van Noord, 1994), it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994), and by Lambek Categorial Grammars (Lambek 1958). It is therefore worth giving some brief indications of how it fits in with these developments. The first directed Applicative CG was proposed by Bar-Hillel (1953). Functional types included a list of arguments to the left, and a list of arguments to the right. Translating Bar-Hillel&apos;s notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994), we obtain the following category for a ditransitive verb such as put: [s i(np) r(np, pp) The list of arguments to the left are gathered under the feature, 1, and those to the r</context>
<context position="15241" citStr="Pollard and Sag (1994)" startWordPosition="2497" endWordPosition="2500">ivity. The categories of the grammar are defined as follows: I. If X is a syntactic type (e.g. s, np), then X 10 is a category. 2. If X is a syntactic type, and L and R are lists of categories, then L 1xL1 is a category. rR Application to the right is defined by the rule8: 60ne area where application based approaches to semantic combination gain in simplicity over unification based approaches is in providing semantics for functions of functions. Moore (1989) provides a treatment of functions of functions in a unification based approach, but only by explicitly incorporating lambda expressions. Pollard and Sag (1994) deal with some functions of functions, such as non-intersective adjectives, by explicit set construction. 7As discussed above, wh-movement requires something more like composition than application. A simple syntax semantics interface can be retained if the same operation is used in both syntax and semantics. Wh-arguments can be treated as similar to other arguments i.e. as lambda abstracted in the semantics. For example, the fragment: John found a woman who Mary can be given the semantics AP.Dx. woman(x) &amp; found(john,x) &amp; P(mary,x), where P is a function from a left argument Mary of type e an</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C. &amp; Sag, I.A.: 1994, Head-Driven Phrase Structure Grammar, University of Chicago Press &amp; CSLI Publications, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Putman</author>
</authors>
<title>Grammars, Parsers,</title>
<date>1986</date>
<journal>Memory Limitations&apos;, Language &amp; Cognitive Processes</journal>
<volume>1</volume>
<issue>3</issue>
<pages>197--225</pages>
<marker>Putman, 1986</marker>
<rawString>Putman, S.G.: 1986, &apos;Grammars, Parsers, &amp; Memory Limitations&apos;, Language &amp; Cognitive Processes 1(3), 197-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Spivey-Knowlton</author>
<author>J Sedivy</author>
<author>K Eberhard</author>
<author>M Tanenhaus</author>
</authors>
<title>Psycholinguistic Study of the Interaction Between Language &amp; Vision&apos;,</title>
<date>1994</date>
<booktitle>in Proceedings of the 12th National Conference on Al, AAAI-94.</booktitle>
<marker>Spivey-Knowlton, Sedivy, Eberhard, Tanenhaus, 1994</marker>
<rawString>Spivey-Knowlton, M., Sedivy, J., Eberhard, K., &amp; Tanenhaus, M.: 1994, `Psycholinguistic Study of the Interaction Between Language &amp; Vision&apos;, in Proceedings of the 12th National Conference on Al, AAAI-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E P Stabler</author>
</authors>
<title>Avoid the Pedestrian&apos;s Paradox&apos;,</title>
<date>1991</date>
<booktitle>Principle-Based Parsing: Computation E4 Psycholinguistics,</booktitle>
<editor>in Berwick, R.C. et al. (eds.),</editor>
<location>Kluwer, Netherlands,</location>
<contexts>
<context position="2750" citStr="Stabler 1991" startWordPosition="421" endWordPosition="422">ring noun phrase processing. constituency, so that an initial fragment of a sentence, such as John likes, can be treated as a constituent, and hence be assigned a type and a semantics. This approach is exemplified by Combinatory Categorial Grammar, CCG (Steedman 1991), which takes a basic CG with just application, and adds various new ways of combining elements together2. Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence. The alternative approach, exemplified by the work of Stabler on top-down parsing (Stabler 1991), and Pulman on left-corner parsing (Pulman 1986) is to associate a semantics directly with the partial structures formed during a topdown or left-corner parse. For example, a syntax tree missing a noun phrase, such as the following /\ np vp John / \ V nplikes can be given a semantics as a function from entities to truth values i.e. Ax. likes(john,x), without having to say that John likes is a constituent. Neither approach is without problems. If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents, then there may be unwanted interactions</context>
</contexts>
<marker>Stabler, 1991</marker>
<rawString>Stabler, E.P.: 1991, &apos;Avoid the Pedestrian&apos;s Paradox&apos;, in Berwick, R.C. et al. (eds.), Principle-Based Parsing: Computation E4 Psycholinguistics, Kluwer, Netherlands, 199-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Steedman</author>
</authors>
<title>Type-Raising &amp; Directionality in Combinatory Grammar&apos;,</title>
<date>1991</date>
<booktitle>in Proceedings of the 29th ACL,</booktitle>
<location>Berkeley, U.S.A.</location>
<contexts>
<context position="2405" citStr="Steedman 1991" startWordPosition="366" endWordPosition="367">, grant RR30718. I am grateful to Patrick Sturt, Carl Vogel, and the reviewers for comments on an earlier version. &apos;Spivey-Knowlton et al. reported 3 experiments, One showed effects before the end of a word when there was no other appropriate word with the same initial phonology. Another showed on-line effects from adjectives and determiners during noun phrase processing. constituency, so that an initial fragment of a sentence, such as John likes, can be treated as a constituent, and hence be assigned a type and a semantics. This approach is exemplified by Combinatory Categorial Grammar, CCG (Steedman 1991), which takes a basic CG with just application, and adds various new ways of combining elements together2. Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence. The alternative approach, exemplified by the work of Stabler on top-down parsing (Stabler 1991), and Pulman on left-corner parsing (Pulman 1986) is to associate a semantics directly with the partial structures formed during a topdown or left-corner parse. For example, a syntax tree missing a noun phrase, such as the following /\ np vp John / \ V np</context>
</contexts>
<marker>Steedman, 1991</marker>
<rawString>Steedman, M.J.: 1991, &apos;Type-Raising &amp; Directionality in Combinatory Grammar&apos;, in Proceedings of the 29th ACL, Berkeley, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Tanenhaus</author>
<author>S Garnsey</author>
<author>J Boland</author>
</authors>
<title>Combinatory Lexical Information &amp; Language Comprehension&apos;,</title>
<date>1990</date>
<booktitle>in Altmann, G.T.M. Cognitive Models of Speech Processing,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge Ma.</location>
<contexts>
<context position="1024" citStr="Tanenhaus et al. 1990" startWordPosition="148" endWordPosition="151">blems of spurious ambiguity. The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus. It also includes a discussion of some of the issues which arise when parsing lexicalised grammars, and the possibilities for using statistical techniques for tuning to particular languages. 1 Introduction There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence, and before the end of phrasal constituents (e.g. Marslen-Wilson 1973, Tanenhaus et al. 1990). There is also recent evidence suggesting that, during speech processing, partial interpretations can be built extremely rapidly, even before words are completed (SpiveyKnowlton et al. 1994)&apos; . There are also potential computational applications for incremental interpretation, including early parse filtering using statistics based on logical form plausibility, and interpretation of fragments of dialogues (a survey is provided by Milward and Cooper, 1994, henceforth referred to as M&amp;C). In the current computational and psycholinguistic literature there are two main approaches to the incrementa</context>
</contexts>
<marker>Tanenhaus, Garnsey, Boland, 1990</marker>
<rawString>Tanenhaus, M.K., Garnsey, S., &amp; Boland, J.: 1990, &apos;Combinatory Lexical Information &amp; Language Comprehension&apos;, in Altmann, G.T.M. Cognitive Models of Speech Processing, MIT Press, Cambridge Ma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tugwell</author>
</authors>
<title>A State-Transition Grammar for Data-Oriented Parsing&apos;,</title>
<date>1995</date>
<booktitle>in Proceedings of the 7th Conference of the European ACL, EACL-95,</booktitle>
<location>Dublin, this</location>
<note>volume.</note>
<contexts>
<context position="28291" citStr="Tugwell 1995" startWordPosition="4821" endWordPosition="4822"> a similar, but much simpler, incremental parser for Lexicalised Dependency Grammar has 0(n3) time complexity (Milward 1994) and good practical performance, taking a couple of seconds on 30 word sentences. &amp;quot;The usage of the term language tuning is perhaps broader here than its use in the psycholinguistic literature to refer to different structural preferences between languages e.g. for high versus low attachment (Mitchell et al. 1992). 125 There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories (Tugwell 1995)15. Unlike a simple Markov process, there are a potentially infinite number of states, so there is inevitably a problem of sparse data. It is therefore necessary to make various generalisations over the states, for example by ignoring the R.2 lists. The full processing model can then be either serial, exploring the most highly ranked transitions first (but allowing backtracking if the semantic plausibility of the current interpretation drops too low), or ranked parallel, exploring just the n paths ranked highest according to the transition probabilities and semantic plausibility. 6 Conclusion </context>
</contexts>
<marker>Tugwell, 1995</marker>
<rawString>Tugwell, D.: 1995, &apos;A State-Transition Grammar for Data-Oriented Parsing&apos;, in Proceedings of the 7th Conference of the European ACL, EACL-95, Dublin, this volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>Using Descriptions of Trees in a Tree Adjoining Grammar&apos;,</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<issue>4</issue>
<pages>481--517</pages>
<contexts>
<context position="6621" citStr="Vijay-Shanker 1992" startWordPosition="1057" endWordPosition="1058">emantic representations directly. The general processing model therefore consists of transitions of the form: Syntactic type, _÷ Syntactic typei+i Semantic rep, Semantic repi+i 3This might turn out to be similar to one view of Tree Adjoining Grammar, where adjunction adds into a pre-existing well-formed tree structure. It is also closer to some methods for incremental adaptation of discourse structures, where additions are allowed to the right-frontier of a tree structure (e.g. Polanyi and Scha 1984). There are however problems with this kind of approach when features are considered (see e.g. Vijay-Shanker 1992). This provides a state-transition or dynamic model of processing, with each state being a pair of a syntactic type and a semantic value. The main difference between our approach and that of Milward (1992, 1994) is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar. Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a function of a function, and given the type (n/n)/(n/n) when used as an adjectival modifier). The ability to deal with fun</context>
</contexts>
<marker>Vijay-Shanker, 1992</marker>
<rawString>Vijay-Shanker, K.: 1992, &apos;Using Descriptions of Trees in a Tree Adjoining Grammar&apos;, Computational Linguistics 18(4), 481-517.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>