<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002838">
<title confidence="0.955563">
Adaptive Compression-based Approach for Chinese Pinyin Input
</title>
<author confidence="0.599082">
Jin Hu Huang and David Powers
</author>
<affiliation confidence="0.3925155">
School of Informatics and Engineering
Flinders University of South Australia
</affiliation>
<address confidence="0.3619">
GPO Box 2100, SA 5001
Australia
</address>
<email confidence="0.993725">
{jin.huang,powers}@infoeng.flinders.edu.au
</email>
<sectionHeader confidence="0.97979" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917333333334">
This article presents a compression-based adap-
tive algorithm for Chinese Pinyin input. There
are many different input methods for Chinese
character text and the phonetic Pinyin in-
put method is the one most commonly used.
Compression by Partial Match (PPM) is an
adaptive statistical modelling technique that
is widely used in the field of text compres-
sion. Compression-based approaches are able to
build models very efficiently and incrementally.
Experiments show that adaptive compression-
based approach for Pinyin input outperforms
modified Kneser-Ney smoothing method im-
plemented by SRILM language tools (Stolcke,
2002).
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875285714286">
Chinese words comprise ideographic and picto-
graphic characters. Unlike English, these char-
acters can’t be entered by keyboard directly.
They have to be transliterated from keyboard
input based on different input methods. There
are two main approaches: phonetic-based input
methods such as Pinyin input and structure-
based input methods such as WBZX. Pinyin
input is the easiest to learn and most widely
used. WBZX is more difficult as the user has to
remember all the radical parts of each character,
but it is faster.
Early products using Pinyin input methods
are very slow because of the large number of
homonyms in the Chinese language. The user
has to choose the correct character after each
Pinyin has been entered. The situation in cur-
rent products such as Microsoft IME for Chi-
nese and Chinese Star has been improved with
the progress in language modelling (Goodman,
2001) but users are still not satisfied.
</bodyText>
<sectionHeader confidence="0.846156" genericHeader="method">
2 Statistical Language Modelling
</sectionHeader>
<bodyText confidence="0.9856534">
Statistical language modelling has been success-
fully applied to Chinese Pinyin input (Gao et
al., 2002). The task of statistical language mod-
elling is to determine the probability of a se-
quence of words.
</bodyText>
<equation confidence="0.999071">
P(wi|w1 ... wi−1) ≈ P(wi|wi−2wi−1) (2)
</equation>
<bodyText confidence="0.99962325">
The key difficulty with using n-gram language
models is that of data sparsity. One can never
have enough training data to cover all the n-
grams. Therefore some mechanism for assigning
non-zero probability to novel n-grams is a key
issue in statistical language modelling. Smooth-
ing is used to adjust the probabilities and
make distributions more uniform. Chen and
Goodman (Chen and Goodman, 1999) made a
complete comparison of most smoothing tech-
niques and found that the modified Kneser-Ney
smoothing(equation 3) outperformed others.
</bodyText>
<equation confidence="0.99991075">
i−n+1))
pKN(wi|wi−1
i−n+1) = c(wi−1
i−n+1) − D(c(wi−1
i−1
Pwi c(wi−n+1)
+γ(i−1 i−1
wi−n+1)pKN(wi |wi−n+2) (3)
</equation>
<bodyText confidence="0.707461">
where
</bodyText>
<equation confidence="0.999353894736842">
0 if c = 0
D1 if c = 1
D2 if c = 2
D3+ if c ≥ 3
γ(i−1
wi−n+1) =
D1N1(wi−1
i−n+1·)+D2N2(wi−1
i−n+1·)+D3+N3+(wi−1
i−n+1·)
P (4)
wi c(wi−1
i−n+1)
⎧
⎨⎪⎪
⎪⎪⎩
D(c) =
P(w1 ... wi) = P(w1)*P(w2|w1)*···*P(wi|w1 ... wi−1)
(1)
</equation>
<bodyText confidence="0.9998465">
Given the previous i-1 words, it is difficult to
compute the conditional probability if i is very
large. An n-gram Markov model approximates
this probability by assuming that only words
relevant to predict are previous n-1 words. The
most commonly used is trigram.
</bodyText>
<equation confidence="0.998800461538462">
N1(wi−1
i−n+1·) =  |{wi : c(wi−1
i−n+1wi) = 11 |(5)
n1
Y=
n1 + 2n2
− 2Y n2
n1
− 3Y n3
n2
− 4Y n4
n3
(6)
</equation>
<bodyText confidence="0.996482">
The process of Pinyin input can be formu-
lated as follows.
</bodyText>
<equation confidence="0.995675">
Pr(W |A) (7)
</equation>
<bodyText confidence="0.994744">
is the number of tokens that have followed. t is
the number of types.
Method A works by allocating a count of one
to the escape symbol.
</bodyText>
<equation confidence="0.9976715">
e = 1 (9)
n + 1
p(φ) = (10)
n + 1
</equation>
<bodyText confidence="0.999370666666667">
Method B makes assumption that the first
occurrence of a particular symbol in a particu-
lar context may be taken as evidence of a novel
symbol appearing in the context, and therefore
does not contribute towards the estimate of the
probability of the symbol which it occurred.
</bodyText>
<equation confidence="0.999280625">
D1 = 1
D2 = 1
D3+ = 1
W = arg max
W
c(φ)
W = arg max Pr(A|W) Pr(W) (8) e = t (11)
W n
</equation>
<bodyText confidence="0.9999872">
We assume each Chinese character has only
one pronunciation in our experiments.
Thus we can use the Viterbi algorithm to find
the word sequences to maximize the language
model according to Pinyin input.
</bodyText>
<sectionHeader confidence="0.71019" genericHeader="method">
3 Prediction by Partial Matching
</sectionHeader>
<bodyText confidence="0.998948482758621">
Prediction by Partial Matching (PPM)(Cleary
and Witten, 1984; Bell et al., 1990) is a symbol-
wise compression scheme for adaptive text com-
pression. PPM generates a prediction for each
input character based on its preceding char-
acters. The prediction is encoded in form of
conditional probability, conditioned on previous
context. PPM maintains predictions, computed
from the training data, for larger context as well
as all shorter con-texts. If PPM cannot pre-
dict the character from current context, it uses
an escape probability to “escape” another con-
text model, usually of length one shorter than
the current context. For novel characters that
have never seen before in any length model, the
algorithm escapes down to a default “order-1”
context model where all possible characters are
present.
PPM escape method can be considered as an
instance of Jelinek-Mercer smoothing. It is de-
fined recursively as a linear interpolation be-
tween the nth-order maximum likelihood and
the (n-1)th-order smoothed model. Various
methods have been proposed for estimating the
escape probability. In the following description
of each method, e is the escape probability and
p(φ) is the conditional probability for symbol φ
, given a context. c(φ) is the number of times
the context was followed by the symbol φ . n
</bodyText>
<equation confidence="0.985844333333333">
c(φ) − 1
p(φ) = (12)
n
</equation>
<bodyText confidence="0.9708946">
Method C (Moffat, 1990) is similar to Method
B, with the distinction that the first observation
of a particular symbol in a particular symbol
in a particular context also contributes to the
probability estimate of the symbol itself. Es-
cape method C is called Witten-Bell smooth-
ing in statistical language modelling. Chen and
Goodman (Chen and Goodman, 1999) reported
it is competitive on very large training data sets
comparing with other smoothing techniques.
</bodyText>
<equation confidence="0.999056">
e = t (13)
n + t
p(φ) = (14)
n + t
</equation>
<bodyText confidence="0.920263">
Method D (Howard, 1993) is minor modifi-
cation to method B. Whenever a novel event
occurs, rather than adding one to the symbol,
half is added instead.
</bodyText>
<equation confidence="0.95975375">
t (15)
2n
p(φ) = (16)
2n
</equation>
<bodyText confidence="0.9999385">
To illustrate the PPM compression modelling
technique, Table 1 shows the model after string
dealornodeal has been processed. In this illus-
tration the maximum order is 2 and each pre-
diction has a count c and a prediction prob-
ability p. The probability is determined from
</bodyText>
<equation confidence="0.9707314">
c(φ)
e =
2c(φ) − 1
Order 2
Prediction c p
</equation>
<table confidence="0.986689586956522">
al → o 1 1/2
→ Esc 1 1/2
de → a 2 3/4
→ Esc 1 1/4
ea → l 2 3/4
→ Esc 1 1/2
lo → r 1 1/2
→ Esc 1 1/2
no → d 1 1/2
→ Esc 1 1/2
od → e 1 1/2
→ Esc 1 1/2
or → n 1 1/2
→ Esc 1 1/2
rn → o 1 1/2
→ Esc 1 1/2
Order 1
Prediction c p
a → l 2 3/4
→ Esc 1 1/4
d → e 2 3/4
→ Esc 1 1/4
e → a 2 3/4
→ Esc 1 1/4
l → o 1 1/2
→ Esc 1 1/2
n → o 1 1/2
→ Esc 1 1/2
o → d 1 1/4
→ r 1 1/4
→ Esc 2 1/2
r → n 1 1/2
→ Esc 1 1/2
Order 0 p
Prediction c
→ a 2 3/24
→ d 2 3/24
→ e 2 3/24
→ l 2 3/24
→ n 1 1/24
→ o 2 3/24
→ r 1 1/24
→ Esc 7 7/24
Order –1
Prediction c p
→ A 1 1/|A|
</table>
<tableCaption confidence="0.710655">
Table 1: PPM model after processing the string
dealornodeal
</tableCaption>
<bodyText confidence="0.994814108108108">
counts associated with the prediction using es-
cape method D(equation 16). |A |is the size the
alphabet which determines the probability for
each unseen character.
Suppose the character following dealornodeal
is o. Since the order-2 context is al and the up-
coming symbol o has already seen in this con-
text, the order-2 model is used to encode the
symbol. The encoding probability is 1/2. If the
next character were i instead of o, it has not
been seen in the current order-2 context (al).
Then an order-2 escape event is emitted with a
probability of 1/2 and the context truncated to
l. Checking the order-1 model, the upcoming
character i has not been seen in this context,
so an order-1 escape event is emitted with a
probability of 1/2 and the context is truncated
to the null context, corresponding to the order-
0 model. As i has not appeared in the string
dealornodeal, a final level of escape is emitted
with a probability of 7/24 and the i will be pre-
dicted with a probability of 1/256 in the order-
–1, assuming that the alphabet size is 256 for
ASCII. Thus i is encoded with a total probabil-
ity1 1 7 1
of 2 ∗ 2 ∗ 24 ∗ 256.
In reality, the alphabet size in the order- –1
model may be reduced by the number of char-
acters in the order-0 model as these characters
will never be predicted in the order- –1 context.
Thus it can be reduced to 249 in this case. Simi-
larly a character that occurs in the higher-order
model will never be encoded in the lower-order
models. So it is not necessary to reserve the
probability space for the character in the lower-
order models. This is called “exclusion”, which
can greatly improve compression.
</bodyText>
<tableCaption confidence="0.837305285714286">
Table 2: Compression results for different com-
pression methods
Table 2 shows the compression result for file
People Daily (9101) with 792964 Bytes using
different compression methods. PPM compres-
sion methods are significantly better than prac-
tical compression utilities like Unix gzip and
</tableCaption>
<table confidence="0.999823307692308">
Compression Method Size Compression Rate
Escape A(order 2) 434228 54.8%
Escape B(order 2) 332278 41.9%
Escape C(order 2) 333791 42.1%
Escape D(order 2) 332829 42.0%
Escape D(order 1) 345841 43.6%
Escape D(order 3) 332932 42.0%
gzip 434220 54.8%
compress 514045 64.8%
People Daily(96) Stories
modified Kneser-Ney 5.82% 14.48%
Static PPM 6.00% 16.55%
Adaptive PPM 4.98% 14.24%
</table>
<bodyText confidence="0.998832538461539">
compress except escape method A but they are
slower during compression. The compression
rates for escape method B and D are both higher
than escape method C. Order-2 model (trigram)
is slightly better that order-1 and order-3 mod-
els for escape method D.
In our experiment we use escape method D
to calculate the escape probability as escape
method D is slightly better than other escape
methods in compressing text although Method
B is the best here. Teahan (Teahan et al., 2000)
has successfully applied escape method D to
segment Chinese text.
</bodyText>
<sectionHeader confidence="0.993025" genericHeader="evaluation">
4 Experiment and Result
</sectionHeader>
<bodyText confidence="0.99995675862069">
We use 220MB People Daily (91-95) as the
training corpus and 58M People Daily (96) and
stories download from Internet (400K) as the
test corpus.
We used SRILM language tools (Stolcke,
2002) to collect trigram counts and applied
modified Kneser-Ney smoothing method to
build the language model. Then we used disam-
big to translate Pinyin to Chinese characters.
In PPM model we used the same count data
collected by SRILM tools. We chose a trie struc-
ture to store the symbol and count. Adaptive
PPM model updates the counts during Pinyin
input. It is similar to a cache model (Kuhn
and De Mori, 1990). We tested both static and
adaptive PPM models on test corpus. PPM
models run twice faster than SRILM tool dis-
ambig. It took 20 hours to translate Pinyin
(People Daily 96) to character on a Sparc with
two CPUs(900Mhz) using SRILM tools. The
following Table 3 shows the results in terms of
character error rate. People Daily(96) is the
same domain as the training corpus. Results ob-
tained testing on People Daily are consistently
much better than Stories. Static PPM is a lit-
tle worse than modified Kneser-Ney smoothing
method. Adaptive PPM model testing on large
corpus is better than small corpus as it takes
time to adapt to the new model.
</bodyText>
<tableCaption confidence="0.827907">
Table 3: Character Error Rates for Kneser-Ney,
Static and Adaptive PPM
</tableCaption>
<sectionHeader confidence="0.985258" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99976275">
We have introduced a method for Pinyin input
based on an adaptive PPM model. Adaptive
PPM model outperforms both static PPM and
modified Kneser-Ney smoothing.
</bodyText>
<sectionHeader confidence="0.9922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999441236842105">
T.C. Bell, J.G. Cleary, and I.H. Witten. 1990.
Text Compression. Prentice Hall.
Stanly Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for
language modeling. Computer Speech and
Language, 13, 10.
J.G. Cleary and I.H. Witten. 1984. Data com-
pression using adaptive coding and partial
string matching. IEEE transactions on Com-
munications, 32(4).
Jianfeng Gao, Juashua Goodman, Mingjing Li,
and Kai Fu Lee. 2002. Toward a unified ap-
proach to statistical language modeling for
chinese. ACM transaction on Asian Lan-
guage information processing, 1(1), March.
Joshua Goodman. 2001. A bit of progress in
language modeling. Technical Report MSR-
TR-2001-72, Microsoft Research.
P.G. Howard. 1993. The design and analysis
of efficient lossless data compression systems.
Ph.D. thesis, Brown University, Providence,
Rhode Island.
R. Kuhn and R. De Mori. 1990. A cache-based
natural language model for speech reproduc-
tion. IEEE Transac-tion on Pattern Analysis
and Machine Intelligence, 6.
Alistair Moffat. 1990. Implement the ppm data
com-pression scheme. IEEE Transaction on
Communications, 38(11):1917–1921.
A. Stolcke. 2002. Srilm – an extensible lan-
guage modeling toolkit. In Proc. Intl. Conf.
on Spoken Lan-guage Processing, volume 2,
pages 901–904, Denver.
W.J. Teahan, Yingying Wen, and I.H. Wit-
ten R. McNab. 2000. A compression-
based algorithm for chinese word segmenta-
tion. Computational Linguistics, 26(3):375–
394, September.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431989">
<title confidence="0.99879">Adaptive Compression-based Approach for Chinese Pinyin Input</title>
<author confidence="0.988742">Jin Hu Huang</author>
<author confidence="0.988742">David</author>
<affiliation confidence="0.980504">School of Informatics and Flinders University of South</affiliation>
<address confidence="0.943726">GPO Box 2100, SA</address>
<abstract confidence="0.999035142857143">This article presents a compression-based adaptive algorithm for Chinese Pinyin input. There are many different input methods for Chinese character text and the phonetic Pinyin input method is the one most commonly used. Compression by Partial Match (PPM) is an adaptive statistical modelling technique that is widely used in the field of text compression. Compression-based approaches are able to build models very efficiently and incrementally. Experiments show that adaptive compressionbased approach for Pinyin input outperforms modified Kneser-Ney smoothing method im-</abstract>
<note confidence="0.530524">plemented by SRILM language tools (Stolcke, 2002).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T C Bell</author>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="4232" citStr="Bell et al., 1990" startWordPosition="710" endWordPosition="713">rticular symbol in a particular context may be taken as evidence of a novel symbol appearing in the context, and therefore does not contribute towards the estimate of the probability of the symbol which it occurred. D1 = 1 D2 = 1 D3+ = 1 W = arg max W c(φ) W = arg max Pr(A|W) Pr(W) (8) e = t (11) W n We assume each Chinese character has only one pronunciation in our experiments. Thus we can use the Viterbi algorithm to find the word sequences to maximize the language model according to Pinyin input. 3 Prediction by Partial Matching Prediction by Partial Matching (PPM)(Cleary and Witten, 1984; Bell et al., 1990) is a symbolwise compression scheme for adaptive text compression. PPM generates a prediction for each input character based on its preceding characters. The prediction is encoded in form of conditional probability, conditioned on previous context. PPM maintains predictions, computed from the training data, for larger context as well as all shorter con-texts. If PPM cannot predict the character from current context, it uses an escape probability to “escape” another context model, usually of length one shorter than the current context. For novel characters that have never seen before in any len</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>T.C. Bell, J.G. Cleary, and I.H. Witten. 1990. Text Compression. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanly Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<pages>10</pages>
<contexts>
<context position="2475" citStr="Chen and Goodman, 1999" startWordPosition="379" endWordPosition="382">guage modelling has been successfully applied to Chinese Pinyin input (Gao et al., 2002). The task of statistical language modelling is to determine the probability of a sequence of words. P(wi|w1 ... wi−1) ≈ P(wi|wi−2wi−1) (2) The key difficulty with using n-gram language models is that of data sparsity. One can never have enough training data to cover all the ngrams. Therefore some mechanism for assigning non-zero probability to novel n-grams is a key issue in statistical language modelling. Smoothing is used to adjust the probabilities and make distributions more uniform. Chen and Goodman (Chen and Goodman, 1999) made a complete comparison of most smoothing techniques and found that the modified Kneser-Ney smoothing(equation 3) outperformed others. i−n+1)) pKN(wi|wi−1 i−n+1) = c(wi−1 i−n+1) − D(c(wi−1 i−1 Pwi c(wi−n+1) +γ(i−1 i−1 wi−n+1)pKN(wi |wi−n+2) (3) where 0 if c = 0 D1 if c = 1 D2 if c = 2 D3+ if c ≥ 3 γ(i−1 wi−n+1) = D1N1(wi−1 i−n+1·)+D2N2(wi−1 i−n+1·)+D3+N3+(wi−1 i−n+1·) P (4) wi c(wi−1 i−n+1) ⎧ ⎨⎪⎪ ⎪⎪⎩ D(c) = P(w1 ... wi) = P(w1)*P(w2|w1)*···*P(wi|w1 ... wi−1) (1) Given the previous i-1 words, it is difficult to compute the conditional probability if i is very large. An n-gram Markov model a</context>
<context position="5833" citStr="Chen and Goodman, 1999" startWordPosition="971" endWordPosition="974"> the escape probability. In the following description of each method, e is the escape probability and p(φ) is the conditional probability for symbol φ , given a context. c(φ) is the number of times the context was followed by the symbol φ . n c(φ) − 1 p(φ) = (12) n Method C (Moffat, 1990) is similar to Method B, with the distinction that the first observation of a particular symbol in a particular symbol in a particular context also contributes to the probability estimate of the symbol itself. Escape method C is called Witten-Bell smoothing in statistical language modelling. Chen and Goodman (Chen and Goodman, 1999) reported it is competitive on very large training data sets comparing with other smoothing techniques. e = t (13) n + t p(φ) = (14) n + t Method D (Howard, 1993) is minor modification to method B. Whenever a novel event occurs, rather than adding one to the symbol, half is added instead. t (15) 2n p(φ) = (16) 2n To illustrate the PPM compression modelling technique, Table 1 shows the model after string dealornodeal has been processed. In this illustration the maximum order is 2 and each prediction has a count c and a prediction probability p. The probability is determined from c(φ) e = 2c(φ) </context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanly Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Data compression using adaptive coding and partial string matching.</title>
<date>1984</date>
<journal>IEEE transactions on Communications,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="4212" citStr="Cleary and Witten, 1984" startWordPosition="706" endWordPosition="709"> first occurrence of a particular symbol in a particular context may be taken as evidence of a novel symbol appearing in the context, and therefore does not contribute towards the estimate of the probability of the symbol which it occurred. D1 = 1 D2 = 1 D3+ = 1 W = arg max W c(φ) W = arg max Pr(A|W) Pr(W) (8) e = t (11) W n We assume each Chinese character has only one pronunciation in our experiments. Thus we can use the Viterbi algorithm to find the word sequences to maximize the language model according to Pinyin input. 3 Prediction by Partial Matching Prediction by Partial Matching (PPM)(Cleary and Witten, 1984; Bell et al., 1990) is a symbolwise compression scheme for adaptive text compression. PPM generates a prediction for each input character based on its preceding characters. The prediction is encoded in form of conditional probability, conditioned on previous context. PPM maintains predictions, computed from the training data, for larger context as well as all shorter con-texts. If PPM cannot predict the character from current context, it uses an escape probability to “escape” another context model, usually of length one shorter than the current context. For novel characters that have never se</context>
</contexts>
<marker>Cleary, Witten, 1984</marker>
<rawString>J.G. Cleary and I.H. Witten. 1984. Data compression using adaptive coding and partial string matching. IEEE transactions on Communications, 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Juashua Goodman</author>
<author>Mingjing Li</author>
<author>Kai Fu Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for chinese.</title>
<date>2002</date>
<journal>ACM transaction on Asian Language information processing,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1940" citStr="Gao et al., 2002" startWordPosition="291" endWordPosition="294"> the user has to remember all the radical parts of each character, but it is faster. Early products using Pinyin input methods are very slow because of the large number of homonyms in the Chinese language. The user has to choose the correct character after each Pinyin has been entered. The situation in current products such as Microsoft IME for Chinese and Chinese Star has been improved with the progress in language modelling (Goodman, 2001) but users are still not satisfied. 2 Statistical Language Modelling Statistical language modelling has been successfully applied to Chinese Pinyin input (Gao et al., 2002). The task of statistical language modelling is to determine the probability of a sequence of words. P(wi|w1 ... wi−1) ≈ P(wi|wi−2wi−1) (2) The key difficulty with using n-gram language models is that of data sparsity. One can never have enough training data to cover all the ngrams. Therefore some mechanism for assigning non-zero probability to novel n-grams is a key issue in statistical language modelling. Smoothing is used to adjust the probabilities and make distributions more uniform. Chen and Goodman (Chen and Goodman, 1999) made a complete comparison of most smoothing techniques and foun</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Jianfeng Gao, Juashua Goodman, Mingjing Li, and Kai Fu Lee. 2002. Toward a unified approach to statistical language modeling for chinese. ACM transaction on Asian Language information processing, 1(1), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<tech>Technical Report MSRTR-2001-72, Microsoft Research.</tech>
<contexts>
<context position="1768" citStr="Goodman, 2001" startWordPosition="267" endWordPosition="268">ed input methods such as Pinyin input and structurebased input methods such as WBZX. Pinyin input is the easiest to learn and most widely used. WBZX is more difficult as the user has to remember all the radical parts of each character, but it is faster. Early products using Pinyin input methods are very slow because of the large number of homonyms in the Chinese language. The user has to choose the correct character after each Pinyin has been entered. The situation in current products such as Microsoft IME for Chinese and Chinese Star has been improved with the progress in language modelling (Goodman, 2001) but users are still not satisfied. 2 Statistical Language Modelling Statistical language modelling has been successfully applied to Chinese Pinyin input (Gao et al., 2002). The task of statistical language modelling is to determine the probability of a sequence of words. P(wi|w1 ... wi−1) ≈ P(wi|wi−2wi−1) (2) The key difficulty with using n-gram language models is that of data sparsity. One can never have enough training data to cover all the ngrams. Therefore some mechanism for assigning non-zero probability to novel n-grams is a key issue in statistical language modelling. Smoothing is used</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A bit of progress in language modeling. Technical Report MSRTR-2001-72, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P G Howard</author>
</authors>
<title>The design and analysis of efficient lossless data compression systems.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University,</institution>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="5995" citStr="Howard, 1993" startWordPosition="1005" endWordPosition="1006">φ) is the number of times the context was followed by the symbol φ . n c(φ) − 1 p(φ) = (12) n Method C (Moffat, 1990) is similar to Method B, with the distinction that the first observation of a particular symbol in a particular symbol in a particular context also contributes to the probability estimate of the symbol itself. Escape method C is called Witten-Bell smoothing in statistical language modelling. Chen and Goodman (Chen and Goodman, 1999) reported it is competitive on very large training data sets comparing with other smoothing techniques. e = t (13) n + t p(φ) = (14) n + t Method D (Howard, 1993) is minor modification to method B. Whenever a novel event occurs, rather than adding one to the symbol, half is added instead. t (15) 2n p(φ) = (16) 2n To illustrate the PPM compression modelling technique, Table 1 shows the model after string dealornodeal has been processed. In this illustration the maximum order is 2 and each prediction has a count c and a prediction probability p. The probability is determined from c(φ) e = 2c(φ) − 1 Order 2 Prediction c p al → o 1 1/2 → Esc 1 1/2 de → a 2 3/4 → Esc 1 1/4 ea → l 2 3/4 → Esc 1 1/2 lo → r 1 1/2 → Esc 1 1/2 no → d 1 1/2 → Esc 1 1/2 od → e 1 1</context>
</contexts>
<marker>Howard, 1993</marker>
<rawString>P.G. Howard. 1993. The design and analysis of efficient lossless data compression systems. Ph.D. thesis, Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>A cache-based natural language model for speech reproduction.</title>
<date>1990</date>
<journal>IEEE Transac-tion on Pattern Analysis and Machine Intelligence,</journal>
<volume>6</volume>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>R. Kuhn and R. De Mori. 1990. A cache-based natural language model for speech reproduction. IEEE Transac-tion on Pattern Analysis and Machine Intelligence, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Moffat</author>
</authors>
<title>Implement the ppm data com-pression scheme.</title>
<date>1990</date>
<journal>IEEE Transaction on Communications,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="5499" citStr="Moffat, 1990" startWordPosition="919" endWordPosition="920">der-1” context model where all possible characters are present. PPM escape method can be considered as an instance of Jelinek-Mercer smoothing. It is defined recursively as a linear interpolation between the nth-order maximum likelihood and the (n-1)th-order smoothed model. Various methods have been proposed for estimating the escape probability. In the following description of each method, e is the escape probability and p(φ) is the conditional probability for symbol φ , given a context. c(φ) is the number of times the context was followed by the symbol φ . n c(φ) − 1 p(φ) = (12) n Method C (Moffat, 1990) is similar to Method B, with the distinction that the first observation of a particular symbol in a particular symbol in a particular context also contributes to the probability estimate of the symbol itself. Escape method C is called Witten-Bell smoothing in statistical language modelling. Chen and Goodman (Chen and Goodman, 1999) reported it is competitive on very large training data sets comparing with other smoothing techniques. e = t (13) n + t p(φ) = (14) n + t Method D (Howard, 1993) is minor modification to method B. Whenever a novel event occurs, rather than adding one to the symbol,</context>
</contexts>
<marker>Moffat, 1990</marker>
<rawString>Alistair Moffat. 1990. Implement the ppm data com-pression scheme. IEEE Transaction on Communications, 38(11):1917–1921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Lan-guage Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver.</location>
<contexts>
<context position="873" citStr="Stolcke, 2002" startWordPosition="120" endWordPosition="121">ticle presents a compression-based adaptive algorithm for Chinese Pinyin input. There are many different input methods for Chinese character text and the phonetic Pinyin input method is the one most commonly used. Compression by Partial Match (PPM) is an adaptive statistical modelling technique that is widely used in the field of text compression. Compression-based approaches are able to build models very efficiently and incrementally. Experiments show that adaptive compressionbased approach for Pinyin input outperforms modified Kneser-Ney smoothing method implemented by SRILM language tools (Stolcke, 2002). 1 Introduction Chinese words comprise ideographic and pictographic characters. Unlike English, these characters can’t be entered by keyboard directly. They have to be transliterated from keyboard input based on different input methods. There are two main approaches: phonetic-based input methods such as Pinyin input and structurebased input methods such as WBZX. Pinyin input is the easiest to learn and most widely used. WBZX is more difficult as the user has to remember all the radical parts of each character, but it is faster. Early products using Pinyin input methods are very slow because o</context>
<context position="10128" citStr="Stolcke, 2002" startWordPosition="1819" endWordPosition="1820">pe method C. Order-2 model (trigram) is slightly better that order-1 and order-3 models for escape method D. In our experiment we use escape method D to calculate the escape probability as escape method D is slightly better than other escape methods in compressing text although Method B is the best here. Teahan (Teahan et al., 2000) has successfully applied escape method D to segment Chinese text. 4 Experiment and Result We use 220MB People Daily (91-95) as the training corpus and 58M People Daily (96) and stories download from Internet (400K) as the test corpus. We used SRILM language tools (Stolcke, 2002) to collect trigram counts and applied modified Kneser-Ney smoothing method to build the language model. Then we used disambig to translate Pinyin to Chinese characters. In PPM model we used the same count data collected by SRILM tools. We chose a trie structure to store the symbol and count. Adaptive PPM model updates the counts during Pinyin input. It is similar to a cache model (Kuhn and De Mori, 1990). We tested both static and adaptive PPM models on test corpus. PPM models run twice faster than SRILM tool disambig. It took 20 hours to translate Pinyin (People Daily 96) to character on a S</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Lan-guage Processing, volume 2, pages 901–904, Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Yingying Wen</author>
<author>I H Witten R McNab</author>
</authors>
<title>A compressionbased algorithm for chinese word segmentation.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>394</pages>
<contexts>
<context position="9848" citStr="Teahan et al., 2000" startWordPosition="1771" endWordPosition="1774">4220 54.8% compress 514045 64.8% People Daily(96) Stories modified Kneser-Ney 5.82% 14.48% Static PPM 6.00% 16.55% Adaptive PPM 4.98% 14.24% compress except escape method A but they are slower during compression. The compression rates for escape method B and D are both higher than escape method C. Order-2 model (trigram) is slightly better that order-1 and order-3 models for escape method D. In our experiment we use escape method D to calculate the escape probability as escape method D is slightly better than other escape methods in compressing text although Method B is the best here. Teahan (Teahan et al., 2000) has successfully applied escape method D to segment Chinese text. 4 Experiment and Result We use 220MB People Daily (91-95) as the training corpus and 58M People Daily (96) and stories download from Internet (400K) as the test corpus. We used SRILM language tools (Stolcke, 2002) to collect trigram counts and applied modified Kneser-Ney smoothing method to build the language model. Then we used disambig to translate Pinyin to Chinese characters. In PPM model we used the same count data collected by SRILM tools. We chose a trie structure to store the symbol and count. Adaptive PPM model updates</context>
</contexts>
<marker>Teahan, Wen, McNab, 2000</marker>
<rawString>W.J. Teahan, Yingying Wen, and I.H. Witten R. McNab. 2000. A compressionbased algorithm for chinese word segmentation. Computational Linguistics, 26(3):375– 394, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>