<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.997099">
Linguistic Redundancy in Twitter
</title>
<author confidence="0.997822">
Fabio Massimo Zanzotto Marco Pennacchiotti Kostas Tsioutsiouliklis
</author>
<affiliation confidence="0.999256">
University of Rome ”Tor Vergata” Yahoo! Labs Yahoo! Labs
</affiliation>
<address confidence="0.547108">
Rome, Italy Sunnyvale, CA, 94089 Sunnyvale, CA, 94089
</address>
<email confidence="0.995245">
zanzotto@info.uniroma2.it pennac@yahoo-inc.com kostas@yahoo-inc.com
</email>
<sectionHeader confidence="0.995579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99961845">
In the last few years, the interest of the re-
search community in micro-blogs and social
media services, such as Twitter, is growing ex-
ponentially. Yet, so far not much attention has
been paid on a key characteristic of micro-
blogs: the high level of information redun-
dancy. The aim of this paper is to systemat-
ically approach this problem by providing an
operational definition of redundancy. We cast
redundancy in the framework of Textual En-
tailment Recognition. We also provide quan-
titative evidence on the pervasiveness of re-
dundancy in Twitter, and describe a dataset
of redundancy-annotated tweets. Finally, we
present a general purpose system for identify-
ing redundant tweets. An extensive quantita-
tive evaluation shows that our system success-
fully solves the redundancy detection task, im-
proving over baseline systems with statistical
significance.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997972095238095">
Micro-blogs and social media services, such as Twit-
ter, have experienced an exponential growth in the
last few years. The interest of the research commu-
nity and the industry in these services has followed
a similar trend. Web companies such as Google, Ya-
hoo, and Bing are integrating more and more social
content to their sites. At the same time, the compu-
tational linguistic community is getting increasingly
interested in studying social and linguistic proper-
ties of Twitter and other micro-blogs (Java et al.,
2007; Krishnamurthy et al., 2008; Kwak et al., 2010;
Zhao et al., 2007; Popescu and Pennacchiotti, 2010;
Petrovi´c et al., 2010; Lin et al., 2010; Liu et al.,
2010; Ritter et al., 2010). Yet, so far, not much
attention has been paid on a key characteristic of
micro-blogs: the high level of information redun-
dancy. Users often post messages with the same, or
very similar, content, especially when reporting or
commenting on news and events. For example, the
following two tweets are part of a large set of redun-
dant tweets issued during the 2010 winter Olympics:
</bodyText>
<equation confidence="0.9950182">
(example 1)
t1 : “Swiss ski jumper Simon Ammann takes first gold of
Vancouver”
t2 : “Swiss (Suisse) get the Gold on Normal Hill skijump.
#Vancouver2010”
</equation>
<bodyText confidence="0.999834833333333">
By performing an editorial study (described later in
the paper) we discovered that a large part of event-
related tweets are indeed redundant.
Detecting information redundancy is important
for various reasons. First, most applications based
on Twitter share the goal of providing tweets that
are both informative and diverse, with respect to an
initial user information need. For example, Twitter
search engines should ideally select the most infor-
mative and diverse set of tweets in return to a user
query. Similarly, a news web portal that attaches
tweets to a given news article should attach those
tweets that provide the broadest and most diverse
set of information, opinions, and updates about the
news item. To keep a high level of diversity, redun-
dant tweets should be removed from the set of tweets
displayed to the user. Figure 1 shows an example of
a Twitter search engine where redundant tweets are
</bodyText>
<page confidence="0.985783">
659
</page>
<note confidence="0.9816605">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 659–669,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999786">
Figure 1: Twitter search: actual Twitter results and desired results after redundancy reduction.
</figureCaption>
<bodyText confidence="0.997879772727273">
present (left) and where they are discarded (right).
Also, from a computational linguistic point of
view, the high redundancy in micro-blogs gives the
unprecedented opportunity to study classical tasks
such as text summarization (Haghighi and Vander-
wende, 2009), textual entailment recognition (Da-
gan et al., 2006) and paraphrase detection (Dolan et
al., 2004) on very large corpora characterized by an
original and emerging linguistic style, pervaded with
ungrammatical and colloquial expressions, abbrevi-
ations, and new linguistic forms.
The aim of this paper is to formally define, for the
first time, the problem of redundancy in micro-blogs
and to systematically approach the task of automatic
redundancy detection. Note that we focus on lin-
guistic redundancy, i.e. tweets that convey the same
information with different wordings, and ignore the
more trivial issue of detecting retweets, which can
be considered the most basic expression of redun-
dancy.
The main contributions of this paper are the fol-
lowing:
</bodyText>
<listItem confidence="0.9942839">
• We formally define the problem of redundancy
detection in micro-blogs within the framework
of Textual Entailment theory;
• We report results from an editorial study and
provide quantitative evidence of the pervasive-
ness of redundancy in Twitter;
• We present a set of simple and effective ma-
chine learning models for solving the task of
redundancy detection;
• We provide promising experimental results that
</listItem>
<bodyText confidence="0.981743692307692">
show that these models outperform baseline ap-
proaches with statistical significance, and we
report a qualitative evaluation revealing the ad-
vantages of the proposed model.
The rest of the paper is organized as follows.
First, we shortly describe related work in Section 2.
Next, we provide our operational definition of re-
dundancy and introduce our editorial study and
dataset in Section 3. In Section 4 we describe our
models for redundancy detection. In Section 5 we
provide a quantitative and qualitative evaluation of
our models. In Section 6 we conclude the paper with
final observations and future work.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999911428571429">
So far, most research on Twitter has focused on
its network structure, the social behavior of its
users (Java et al., 2007; Krishnamurthy et al., 2008;
Kwak et al., 2010), ranking tweets by relevance for
web search (Ramage et al., 2010; Duan et al., 2010),
and the analysis of time series for extracting trending
news, events and facts (Zhao et al., 2007; Popescu
and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin
et al., 2010). Only few studies have specifically fo-
cused on the linguistic content analysis of tweets,
e.g. (Davidov et al., 2010; Barbosa and Feng, 2010).
To date, our paper most closely relates to works on
semantic role labeling (SRL) on social media (Liu et
al., 2010) and conversation modeling (Ritter et al.,
2010).
Liu et al. (2010) present a self-learning SRL sys-
tem for news tweets, with the goal of addressing low
performance caused by the noise and the unstruc-
tured nature of the data. The authors first cluster
together tweets that refer to the same news. Then,
for each cluster, they identify the tweets that are
</bodyText>
<page confidence="0.993924">
660
</page>
<bodyText confidence="0.99970882">
well-formed (i.e. copy-pasted from news), and in-
duce role mappings between well-formed and noisy
tweets in the same cluster by performing word align-
ment. In our paper we are also interested in aligning
and grouping tweets, although our goal is to detect
redundancy, not to perform SRL.
On a different ground, Ritter et al. (2010) pro-
pose a probabilistic model to discover dialogue acts
in Twitter conversations and to classify tweets in a
conversation according to those acts. (A conversa-
tion is defined as a set of tweets in the same re-
ply thread.) The authors define 10 major dialogue
acts for Twitter, including status, question, response
and reaction, and automatically build a probabilis-
tic transition graph for such acts. In our paper, we
also aim at classifying tweets, but our interest is in
information redundancy instead of acts.
In the computational linguistic literature, redun-
dancy detection is studied in multi-document sum-
marization, where the overall document is used
to select the most informative sentences or snip-
pets (Haghighi and Vanderwende, 2009). Since
tweets are short and tweet sets cannot be considered
documents, these methods are hard to apply. A more
convenient setting is paraphrase detection (Dolan et
al., 2004) and textual entailment recognition (Dagan
et al., 2006) (RTE).
In RTE the task is to recognize if a text called
the text T (typically one or two sentences long) en-
tails another text called the hypothesis H. Many ap-
proaches have been proposed for this task, mostly
based on machine learning. Three main classes
of features have been so far explored in RTE: dis-
tance/similarity feature spaces (Corley and Mihal-
cea, 2005; Newman et al., 2005; Haghighi et al.,
2005; Hickl et al., 2006), entailment trigger fea-
ture spaces (de Marneffe et al., 2006; MacCartney
et al., 2006), and pair content feature spaces (Zan-
zotto et al., 2009). Distance/similarity feature spaces
are more suitable to the paraphrase detection task
because they model the similarity between the two
texts. On the other hand, entailment trigger and con-
tent feature spaces model complex relations between
the texts, taking into account first-order entailment
rules, i.e. entailment rules with variables.
In this paper, one of our goals is to explore RTE
techniques and features that are usually used for
classical texts, and check if they can be successfully
adapted to the unstructured, and oftentimes ungram-
matical, Twitter language.
</bodyText>
<sectionHeader confidence="0.986676" genericHeader="method">
3 Redundancy in Twitter
</sectionHeader>
<bodyText confidence="0.960955916666667">
We formally define two tweets as redundant if they
either convey the same information (paraphrase) or
if the information of one tweet subsumes the infor-
mation of the other (textual entailment). For exam-
ple, the pair in (example 1) is redundant. The first
tweet subsumes (i.e. ‘textually entails’) the other;
both tweets state that Switzerland won a Gold Medal
at the Vancouver winter Olympics, but the first one
also specifies the name of the athlete. The follow-
ing pair is, instead, non-redundant, because the two
tweets convey different information, and they do not
subsume each other:
</bodyText>
<equation confidence="0.9968585">
(example 2)
t1 : “Goal! Iniesta scores for #ESP and they have one
hand on the #worldcup”
t2 : “this will be a hardfinal #Esp vs Ned #worldcup”
</equation>
<bodyText confidence="0.9999265">
Our definition of redundancy is grounded on, and in-
spired by, the theory of Textual Entailment, to which
we refer the reader for further details (Dagan et al.,
2006).
</bodyText>
<subsectionHeader confidence="0.999962">
3.1 Quantifying redundancy
</subsectionHeader>
<bodyText confidence="0.999984210526316">
How pervasive is redundancy in Twitter? In order to
answer this question we performed an initial edito-
rial study where human editors were asked to anno-
tate pairs of tweets as being either redundant or non-
redundant. The editorial study also serves as a test
bed for evaluating our redundancy detection models,
as discussed in Section 5.
In the study we focus on ‘informative’ tweets,
i.e. tweets that describe or comment on relevant
events/facts. Indeed, these are the types of tweets
for which redundancy is a critical issue, especially
in view of real applications, e.g. to present a diverse
set of tweets for a given news article. Other types of
tweets, such as status updates, self-promotions, and
personal messages are of less interest in this context.
Dataset extraction. The study is performed on
an automatically built dataset of informative tweets.
The most critical issue for extracting the dataset is
to pre-process tweets and to discard those that are
</bodyText>
<page confidence="0.991565">
661
</page>
<bodyText confidence="0.999982295454546">
not informative. This is not an easy task: a recent
study (Pear-Analytics, 2009) estimates that only 4%
of all tweets are factual news, and only 37% are con-
versations with content. The rest are spam, status
updates and other types of uninformative content.
In order to retain only informative tweets we first
extract buzzy snapshots (Popescu and Pennacchiotti,
2010). A snapshot is defined as a set of tweets that
explicitly mention a specific topic within a speci-
fied time period. A buzzy snapshot is defined as a
snapshot with a large number of tweets, compared
to previous time periods. For example, given the
topic ‘Haiti earthquake’, the snapshot composed by
the tweets mentioning ‘Haiti earthquake’ on January
12th, 2010, will constitute a buzzy snapshot, since in
previous days the topic was not mentioned often.
We use two different topic lists: a celebrity list
containing about 104K celebrity names, crawled
from Wikipedia, including actors, musicians, politi-
cians, and athletes; and an event list composed of
398 hashtags related to 8 major events that hap-
pened between January and July 2010, and listed
in Wikipedia: 1 the earthquake in Haiti, the winter
Olympics, the earthquake in Chile, the death of the
Polish president, the volcano eruption in Iceland, the
oil spill in the Gulf of Mexico, the Greek financial
crisis, and the FIFA world cup.
We extract buzzy snapshots for the above two
topic lists by following the method described
in (Popescu and Pennacchiotti, 2010): we consider
time periods of one day, and call buzzy the snapshots
that mention a given topic α times more than the av-
erage over the previous 2 days. We set α to 20 and 5
respectively for the celebrity list and the event list.
We further exclude irrelevant and spam snapshots
by removing those that have: fewer than 10 tweets;
more than 50% of tweets non-English; and an aver-
age token overlap between tweets of more than 80%,
usually corresponding to spam threads.
The extraction is performed on a Twitter corpus
containing all tweets posted between July 2009 and
August 2010. In all, we extract 972 snapshots for
the celebrity list, containing 205,885 tweets (i.e. av-
erage of 212 tweets per snapshot); and 674 snap-
</bodyText>
<footnote confidence="0.86537175">
1Hashtags are keywords prefixed by ‘#’, that are used by the
Twitter community to mark the topic of a tweet. We collected
our set of hashtags by semi-automatically inspecting the Twitter
stream in the days the major events happened.
</footnote>
<table confidence="0.692969666666667">
redundant 367 (29.5%)
entailment 195 (15.7%)
paraphrase 172 (13.5%)
non-redundant 875 (70.5%)
related 541 (43.6%)
unrelated 334 (26.9%)
</table>
<tableCaption confidence="0.999526">
Table 1: Results of the redundancy editorial study.
</tableCaption>
<bodyText confidence="0.999729894736842">
shots for the event list, containing 393,965 tweets
(584 tweets per snapshot).
The above two final snapshot corpora (i.e. the 972
celebrities’ snapshots and 674 events’ snapshots)
can be considered a good representation of event de-
scriptions and comments on Twitter, thus forming
our initial set of ‘informative’ tweets. From these
two corpora, we extract the final tweet-pair dataset
by randomly sampling 1500 pairs of tweets con-
tained in the same snapshot. Tweet-pairs that con-
tain retweets are excluded.
Dataset annotation. The main editorial task con-
sisted of annotating tweet-pairs as either redundant
or non-redundant. We also asked editors to char-
acterize the specific linguistic relation between the
two tweets of a pair. We consider four relations: en-
tailment (the first tweet entails the second or vice
versa), paraphrase, contradiction (the tweets con-
tradict each other), and related (the tweets are about
the same topic, e.g. the Haiti earthquake, but are
in none of the previous relations). Tweets that were
about different topics were labeled unrelated. An-
notators were asked to base their decisions on the
parts of the tweets that contained information rel-
evant to the selected topic, e.g. the earthquake in
Haiti. These parts were marked in the corpus. Fo-
cusing on these parts is in line with potential appli-
cations of tweet redundancy detection as tweets are
firstly grouped around a topic. Note that pairs that
fall under the entailment or paraphrase relation are
redundant, while unrelated, related, and contradic-
tory tweets are non-redundant.
The annotation was performed in a three stage
process, since tweets are sometimes hard to under-
stand and hence to annotate (misspellings, usage of
slang and abbreviations, lack of discourse context).
In the first step, the 1500 pairs were independently
annotated by a pool of 20 trained editors, super-
</bodyText>
<page confidence="0.990185">
662
</page>
<bodyText confidence="0.999909181818182">
vised by an expert lead. In the second step, the an-
notations were checked by three highly trained ex-
perts with background in computational linguistics:
each pair was independently checked by two ex-
perts. Average kappa agreement in this second step
is kappa = 0.63 (corresponding to ‘good agree-
ment’). In a final step, discordances between the two
experts were resolved by the third expert. Unclear
and unresolved pairs after the three stages were dis-
carded from the dataset, leaving a final set of 1242
pairs. 2
Annotation Results. Table 1 reports the results of
our study. Among the 1242 tweet-pairs, 367 (30%)
are redundant and 875 (70%) are non-redundant.
This shows that redundancy is indeed a pervasive
phenomenon in Twitter, and a critical issue that has
to be solved in order to provide clean and diverse
social content. Most cases of redundancy corre-
spond to tweets that report the same fact using differ-
ent wording, occasionally adding irrelevant personal
comments and sentiments (e.g. ‘Johnny Depp died’
vs. ‘OMG, I am so sad that Johnny Depp is dead’).
</bodyText>
<sectionHeader confidence="0.991641" genericHeader="method">
4 Redundancy detection models
</sectionHeader>
<bodyText confidence="0.999947384615385">
The task of redundancy detection in Twitter is a
tweet-pair classification problem. Given two tweets
t1 and t2, the goal is to classify the pair (t1, t2) as
being either redundant or non-redundant.
In this section we describe different models for
redundancy detection, inspired by existing work in
RTE. We adopt a machine learning approach where a
Support Vector Machine (SVM) is trained on a man-
ually annotated training set to classify incoming test
examples as either redundant or non-redundant. An
evaluation of the different models adopting for train-
ing and testing the dataset described in Section 3, is
presented in Section 5.
</bodyText>
<subsectionHeader confidence="0.980165">
4.1 Bag-of-word model (BOW)
</subsectionHeader>
<bodyText confidence="0.999933">
The bag-of-word model is the most simple approach
for detecting redundancy. It is used as a baseline in
our experiment. The simple intuition of the model
is that if two tweets t1 and t2 have a high lexical
</bodyText>
<footnote confidence="0.97305575">
2At this time, the TwitterTM Terms of Use do not allow
publication of the annotated dataset. Should the Terms of
Use change, the dataset will become available for download at
http://art.uniroma2.it/zanzotto/datasets.
</footnote>
<bodyText confidence="0.999652705882353">
overlap, then they are likely to express the same in-
formation – i.e. they are likely to be redundant. In
this model, the SVM is trained using a single fea-
ture that computes the cosine similarity between the
bag-of-word vectors of the two tweets. The bag-of-
word vector is built using a classical tf*idf weighting
schema over the set of tokens of the pair. This a very
simple baseline as SVM is only learning thresholds
using this single feature.
The bag-of-word model is of course a naive ap-
proach, since in many cases redundant tweets can
have very different lexical content (e.g. the fol-
lowing two tweets: “Farrah Fawcett left out of Os-
car memorial”, “No Farrah Fawcett’s memory at
the Academy Awards”), and non-redundant tweets
can have similar lexical content (e.g. the tweets:
“Johnny Deep is dead”, “Johnny Deep is not dead”).
</bodyText>
<subsectionHeader confidence="0.480056">
4.2 WordNet-based bag-of-word model
</subsectionHeader>
<bodyText confidence="0.9862188">
(WBOW)
The second baseline model was first defined in (Cor-
ley and Mihalcea, 2005) and since then has been
used by many RTE systems. The model extends
BOW by measuring similarity at the semantic level,
instead of the lexical level.
For example, consider the tweet pair: “Oscars
forgot Farrah Fawcett”, “Farrah Fawcett snubbed at
Academy Awards”. This pair is redundant, and,
hence, should be assigned a very high similar-
ity. Yet, BOW would assign a low score, since
many words are not shared across the two tweets.
WBOW fixes this problem by matching ‘Oscar’-
‘Academy Awards’ and ‘forgot’-‘snubbed’ at the se-
mantic level. To provide these matches, WBOW re-
lies on specific word similarity measures over Word-
Net (Miller, 1995), that allow synonymy and hyper-
onymy matches: in our experiments we specifically
use Jiang&amp;Conrath similarity (Jiang and Conrath,
1997).
In practice, we implement WBOW by using the
text similarity measure defined in (Corley and Mi-
halcea, 2005) as the single feature in the SVM clas-
sifier that, as in BOW, learns the threshold on this
single feature.
</bodyText>
<subsectionHeader confidence="0.989589">
4.3 Lexical content model (LEX)
</subsectionHeader>
<bodyText confidence="0.9990685">
This model and the next ones (SYNT and FOR) ex-
plicitly model the content of a tweet pair P =
</bodyText>
<page confidence="0.99575">
663
</page>
<bodyText confidence="0.988356066666667">
(t1, t2) as a whole. This is a radically different ap-
proach with respect to the similarity-based models
explored so far, where the content of t1 and t2 were
treated independently (i.e. each tweet with its own
bag of words), and the SVM used as the single fea-
ture the similarity between the two tweets.
In the LEX model we represent the content of the
tweet pair in a double bag-of-word vector space.
Each pair P = (t1, t2) is represented by two bag-
of-word vectors, (~t1, ~t2). Within this space, we can
then define a specific similarity measure between
pairs using a kernel function in the SVM learning
algorithm. Given two pairs of tweets P(a) and P(b),
the LEX kernel function is defined as follows:
KLEX(P(a), P(b)) = cos(t(a)
</bodyText>
<equation confidence="0.9980175">
1 , t(b)
1 ) + cos(t(a)
</equation>
<bodyText confidence="0.897718444444444">
2 , t(b)
2 )
where cos(·, ·) is the cosine similarity between the
two vectors. The LEX feature space is simple and
can be extremely effective in modeling the content
of tweet pairs. Yet, in principle, it doesn’t model the
relations among words in the tweet. Different con-
tent feature spaces are then needed to capture these
relations.
</bodyText>
<subsectionHeader confidence="0.986107">
4.4 Syntactic content model (SYNT)
</subsectionHeader>
<bodyText confidence="0.999956148148148">
The SYNT model represents a tweet pair using
pairs of syntactic tree fragments from t1 and t2.
Each feature is a pair &lt; fr1, fr2 &gt;, where fr1
and fr2 are syntactic tree fragments (see figure
below). As defined in (Collins and Duffy, 2002),
a syntactic tree fragment fri is active in ti when
fri is a subtree of the syntactic interpretation of
ti. Therefore, these features represent ground rules
connecting the left-hand sides and the right-hand
sides of the tweet pair: each feature is active for a
pair (t1, t2) when the left-hand side fr1 is activated
by the syntactic analysis of t1 and the right-hand
side fr2 is activated by t2. As an example consider
the feature:
that the two tweets are correctly syntactically ana-
lyzed). This feature space models the relations be-
tween words syntactically. Therefore it overcomes
the limitations of the LEX feature space. But it also
introduces a new limitation: the above feature is
in fact also active for the tweet pair (“GM bought
Opel”,“Opel owns GM”). This pair is extremely dif-
ferent from the previous one, thus possibly mislead-
ing the classifier.
This feature space is not represented explicitly,
but it is encoded in a kernel function. Given two
pairs of tweets P(a) and P(b), the SYNT kernel func-
tion is defined as follows:
</bodyText>
<equation confidence="0.999248666666667">
KSY NT(P(a), P(b)) = K(t(a)
1 , t(b)
1 ) + K(t(a)
</equation>
<bodyText confidence="0.73815125">
2 , t(b)
2 )
where K(·, ·) is the tree kernel function described in
(Collins and Duffy, 2002).
</bodyText>
<subsectionHeader confidence="0.842657">
4.5 Syntactic first-order rule content model
</subsectionHeader>
<bodyText confidence="0.986122333333334">
(FOR)
The FOR model overcomes the limitations of SYNT,
by enriching the space with features representing
first-order relations between the two tweets of a
pair. Each feature represents a rule with variables,
i.e. a first order rule that is activated by the tweet
pairs if the variables are unified. This feature space
has been introduced in (Zanzotto and Moschitti,
2006) and shown to improve over the ones above.
Each feature &lt; fr1, fr2 &gt; is a pair of syntactic tree
fragments augmented with variables. The feature
is active for a tweet pair (t1, t2) if the syntactic
interpretations of t1 and t2 can be unified with
&lt; fr1, fr2 &gt;. For example, consider the following
feature:
</bodyText>
<equation confidence="0.786864222222222">
S S
VBP
owns
i
NP Y
NP X VP
NP X VP
h
NP Y
</equation>
<figure confidence="0.734911166666667">
VBP
bought
,
S S
VBP NP VBP NP
bought owns
</figure>
<bodyText confidence="0.995293">
This feature is active for the pair of tweets (“GM
bought Opel”,“GM owns Opel”) since the syntac-
tic analysis of the pair matches the feature (given
This feature is active for the pair (“GM bought
Opel”,“GM owns Opel”), with the variable unifica-
tion X = “GM” and Y = “Opel”. On the contrary,
this feature is not active for the pair (“GM bought
Opel”,“Opel owns GM”) as there is no possibility of
unifying the two variables. Efficient algorithms for
the computation of the related kernel functions can
</bodyText>
<equation confidence="0.5747066">
h
NP VP
,
N VP
i
</equation>
<page confidence="0.990215">
664
</page>
<bodyText confidence="0.8793585">
be found in (Moschitti and Zanzotto, 2007; Zanzotto
and Dell’Arciprete, 2009).
</bodyText>
<sectionHeader confidence="0.995164" genericHeader="conclusions">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999385">
In this section we present an evaluation of the differ-
ent redundancy detection models. First, we define
the experimental setup in Section 5.1. Then, we an-
alyze the results of the experiments in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.967726">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999431545454545">
We experiment with the redundancy detection
dataset described in Section 3. We randomly divide
the corpus into two sets: 50% for training and 50%
for testing. The training set contains 185 positive
tweet-pairs and 416 negative pairs. The test set con-
tains 182 positive pairs and 466 negatives.
We evaluate the performance of the SVM
models using the following feature combina-
tions: LEX+BOW, LEX+WBOW, SYNT+BOW,
SYNT+WBOW, FOR+BOW, FOR+WBOW. We com-
pare to the system baselines BOW and WBOW. 3
The performance of the different models is com-
puted using the Area Under the ROC curve (AROC)
applied to the classification score returned by the
SVM. The ROC curve allows us to study the be-
havior of the classifier in detail, and also provides a
powerful way to compare among systems when the
dataset is unbalanced (as in our case).
To determine the statistical significance of the dif-
ference in the performance of the systems we ana-
lyzed, we use the model described in (Yeh, 2000) as
implemented in (Pad´o, 2006).
We pre-process the dataset with the following
tools: the Charniak Parser (Charniak, 2000) for
parsing sentences, the WordNet similarity pack-
age (Pedersen et al., 2004) for computing WBOW
and for linking the two tweets in a pair, and SVM-
light (Joachims, 1999), extended with the syntac-
tic first-order rule kernels described in (Moschitti
and Zanzotto, 2007) for creating the SYNT and the
FOR feature spaces. We used the Charniak syntactic
parser without any specific adaptation to the Twitter
language.
</bodyText>
<table confidence="0.998807444444444">
Model AROC
BOW 0.592
WBOW 0.578
LEX + BOW 0.725 †
LEX + WBOW 0.728 †
SYNT + BOW 0.736 †
SYNT + WBOW 0.737 †
FOR + BOW 0.739 †
FOR + WBOW 0.747 † t
</table>
<tableCaption confidence="0.8228662">
Table 2: Experimental results of the different systems. †
indicates statistical significance (p &lt; 0.01) with respect
to the two baseline methods BOW and WBOW. ‡ indicates
statistical significance (p &lt; 0.1) with respect to FOR +
BOW
</tableCaption>
<subsectionHeader confidence="0.997779">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999952307692308">
Table 2 reports the results of the experiment. The
first and most important result is that models using
content features (LEX, SYNT, and FOR) along with
similarity features (BOW and WBOW) outperform the
two baseline models using only similarity features
with statistical significance, up to more than 15%
AROC points.
At first glance, WordNet similarities are not use-
ful: the performance of the WBOW model is in-
deed comparable and statistically insignificant with
respect to the pure token based model BOW. This
seems to be intuitive as the language of the tweets
can be far from proper English, i.e. it may contain
many out-of-dictionary words that are not present
in WordNet, thus impairing the similarity measure
used by WBOW.
This trend is also confirmed in the case of content-
based systems like LEX and SYNT. Using BOW
or WBOW in combination with these features has
the same effect on the final performance. Only the
FOR features are positively affected by the WordNet-
based distance. This may be explained by the fact
that in the FOR+WBOW system, the WordNet sim-
ilarity is also used to link words in the two tweets
of a pair. This increases the possibility of finding
reasonable and useful first-order rules. In the quali-
</bodyText>
<footnote confidence="0.563472333333333">
3Note that other feature combinations would not add value,
as BOW and WBOW are interchangeable, and the same stands
for LEX, SYNT and FOR.
</footnote>
<page confidence="0.988428">
665
</page>
<bodyText confidence="0.99825404">
tative analysis that follows, we show some examples ity. These are all examples where BOW and WBOW
that support this intuition. should typically fail, while FOR+WBOW could cap-
On the other hand, syntax plays a key role for de- ture important syntactic first-order rules to overcome
tecting redundancy. The two syntax based models the limitations of the pure similarity-based models.
SYNT and FOR outperform the lexical based models As a first example, both BOW and WBOW fail to
LEX between 1 and 2 AROC points. This is sur- assign a high rank (i.e. low rank number) to the
prising, since the Charniak parser used in the exper- redundant pair o165: in fact, ‘died’ does not lexi-
iments has not been adapted to the Tweet language, cally match ‘rip’, nor are these two words related in
and therefore could have produced many interpreta- WordNet. In contrast, FOR+WBOW assigns a high
tion errors, thus impairing the use of syntax. This rank to this pair, since it may be able to apply the
seems to suggest that if the interpretations of the rule &lt;X died, rip X&gt; that was most probably ac-
part-of speech tags of the unknown words is correct, quired from examples in the training set (the hoax
the syntax of tweets is reasonably similar to the syn- of somebody’s death is pervasive in Twitter, and it
tax of the generic English language. is therefore likely to fire the abovementioned rule in
The best performing model is FOR+WBOW: first- our dataset if enough examples are available).
order rules successfully emerge in tweets and are The third and the fourth pairs (o130 and o21)
positively exploited by the learning system. In the show some commonalities 5 . According to the
next section we report examples that support this ob- WordNet similarity measure we used, ‘recognize’
servation. and ‘snub’ are highly related as well as ‘forget’ and
5.3 Qualitative analysis ‘snub’. Hence, the two tokens are linked as similar.
The experimental results reported in the previ- For o130, the triggering syntactic rule is &lt;(S (NP X)
ous section show that first-order syntactic rules in (VP Y),(VP (V Y) (NP X)&gt; where X and Y are vari-
combination with the WordNet-based bag-of-word ables. For o21, the rule is: &lt;(VP (V X) (NP Y),(VP
(FOR+WBOW) are highly effective in detecting re- (V X) (NP Y)&gt;.
dundancy. In this section, we briefly analyze For the non-redundant pairs (N) at the bottom of
some tweet pairs where the differences between this the table, the first-order rules are less intuitive. Yet,
model and the BOW and WBOW models are evident. it is clear why these pairs have high lexical simi-
Table 3 reports examples of tweet pairs, along larity (and therefore are ranked high by BOW and
with their ranking position in the test set, accord- WBOW): The two tweets in the pair oe387 share
ing to the SVM score, with respect to different mod- ‘volcanic’, ‘ash’, and the hashtag ‘#ashtag’. Tweets
els. The first column represents the editorial gold in oe64 share ‘Icelandic’ and ‘eruption’ but they are
standard (gs) for the tweet pairs we considered: ei- describing different facts. Tweets in the pair oe43
ther redundant (R) or non-redundant (N). Since we are similar since they are sharing the three hashtags
feed the classifiers with ‘redundant’ as the positive ‘#bpoil’, ‘#bp’, and ‘#oilspill’. This example shows
class 4, a classifier is better than another if it ranks that hashtags alone are not very indicative and useful
redundant tweet pairs (R) higher than non-redundant for detecting redundancy in Twitter.
ones (N). The second, the third, and the fourth 6 Conclusions
columns represent the rank given by WBOW+FOR, In this paper we introduced the notion of linguistic
WBOW, and BOW respectively. The fifth column is redundancy in micro-blogs and the task of tweet re-
the tweet-pair identifier in our dataset (id). The last dundancy detection. We also presented an editorial
two columns are the two tweets in each pair. study showing that redundancy is pervasive in Twit-
The table reports interesting examples where re- ter, and that methods for its detection will be key in
dundant pairs have very little lexical similarity while
the non-redundant pairs have a high lexical similar-
5In o130, the common topic is ‘farrah fawcett’: “farrah
fawcett not recognized at the Oscars memorial?” and “snubbed
farrah fawcett. #oscars” are used by the annotators to make the
decision.
4This is just a convention. Results would be the same by
taking non-redundant pairs as the positive class.
</bodyText>
<figure confidence="0.879058710526316">
666
OR+WBOW
WBOW
OW
gs F W B id t1 t2
11 137 130
32 246 239
43 165 158
101 632 641
467 161 155
572 96 92
614 129 124
R
R
R
R
N
N
N
o165 “is that True that johnny depp died???” “Rip johnny depp? This cannot be True”
o942 “sad...jim carrey and jenny mccarthy have “jim carrey &amp; jenny mccarthy broke up!
called it quits...” omg! bummer! they were the cutest crazy
couple ever.”
o130 “farrah fawcett &amp; bea arthur not recognized “i dont understand how they included
at the Oscars memorial? really?” michael jackson in the memorial tribute as an
actor but snubbed farrah fawcett. #oscars”
o21 “Oscars forgot farrah fawcett??” “farrah fawcett snubbed at Oscars appeared
in a movie with best actor Jeff Bridges... dis-
gusting”
oe387 “We may die in volcanic ash today. Choose “# Just heard about the Icelandic volcanic
your final pose soon to look cool for future ash thing, not really interested but it has the
archaeologists. #ashtag” best hashtag ever, #ashtag !”
oe43 “Many Endangered Turtles Dying On “Species Most at Risk Because of the Oil
Texas Gulf Coast http://ow.ly/1FbB8 via Spill http://ow.ly/1FcB7 #bpoil #bp #oil-
@nprnews #bpoil #bp #oilspill” spill”
oe64 “http://bit.ly/d8W7Xw #ashtag IN PIC- “So, who’s going to take a crack at pro-
TURES: Icelandic volcanic eruption” nouncing the part of Iceland the eruption was
in? #ashtag”
</figure>
<tableCaption confidence="0.994963">
Table 3: Ranks of some tweet pairs according to the scores of the different classifiers.
</tableCaption>
<bodyText confidence="0.999987666666667">
the future for the development of accurate Twitter-
based applications. In the second part of the pa-
per we presented some promising models for redun-
dancy detection that show encouraging results when
compared to typical lexical baselines. Even with the
ungrammaticalities used in tweets, syntactic feature
spaces are effective in modeling redundancy, espe-
cially when used in first-order rules.
In future work we plan to improve our system by
adapting existing linguistic tools and resources to
Twitter (e.g. syntactic parsers). We also plan to in-
vestigate the use of semantic roles and contextual in-
formation to improve the models. For example, the
tweets that other users post about the same topic of
the target-pair may be of some help. Finally, we are
investigating the integration of our models into real
applications such a the enrichment of news articles
with related and diverse content from social media.
</bodyText>
<sectionHeader confidence="0.998715" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982485233333333">
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Posters Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 36–44, Beijing, China.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132–139, Seattle, Wash-
ington.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 263–270.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACL Workshop on Empirical Modeling ofSemantic
Equivalence and Entailment, pages 13–18, Ann Arbor,
Michigan.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Quionero-Candela et al., editor, LNAI 3944:
MLCW 2005, pages 177–190, Milan, Italy. Springer-
Verlag.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Posters Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 241–249, Beijing, China.
</reference>
<page confidence="0.98727">
667
</page>
<reference confidence="0.999405198113207">
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher
D. Manning. 2006. Learning to distinguish valid
textual entailments. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics (Coling 2004), pages 350–
356, Geneva, Switzerland.
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 295–303, Beijing, China.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362–370, Boulder, Colorado.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 387–394, Vancouver, British
Columbia, Canada.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recogniz-
ing textual entailment with LCC’s groundhog system.
In Bernardo Magnini and Ido Dagan, editors, Proceed-
ings of the 2nd PASCAL RTE Challenge, Venice, Italy.
Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.
2007. Why we Twitter: understanding microblogging
usage and communities. In Proceedings of the 9th We-
bKDD and 1st SNA-KDD 2007.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the 10th International Conference
on Research in Computational Linguistics ROCLING,
pages 132–139, Tapei, Taiwan.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods-
Support Vector Learning. MIT Press.
Balachander Krishnamurthy, Phillipa Gill, and Martin
Arlitt. 2008. A few chirps about twitter. In Proceed-
ings of the first workshop on Online social networks,
pages 19–24, Seattle, WA, USA.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of WWW ’10: Pro-
ceedings ofthe 19th international conference on World
wide web, pages 591–600, Raleigh, North Carolina,
USA.
Cindy-Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han.
2010. Pet: a statistical model for popular events
tracking in social communities. In Proceedings of
the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 929–
938, Washington, DC, USA.
Xiaohua Liu, Kuan Li, Bo Han, Ming Zhou, Long Jiang,
Zhongyang Xiong, and Changning Huang. 2010. Se-
mantic role labeling for news tweets. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 698–706,
Beijing, China, August.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 41–48, New York City,
USA.
George A. Miller. 1995. WordNet: A lexical database for
English. Communications of the ACM, 38(11):39–41,
November.
Alessandro Moschitti and Fabio Massimo Zanzotto.
2007. Fast and effective kernels for relational learn-
ing from texts. In Proceedings of the International
Conference of Machine Learning (ICML), Corvallis,
Oregon.
Eamonn Newman, Nicola Stokes, John Dunnion, and
Joe Carthy. 2005. Textual entailment recognition us-
ing a linguistically-motivated decision tree classifier.
In Joaquin Qui˜nonero Candela, Ido Dagan, Bernardo
Magnini, and Florence d’Alch´e Buc, editors, MLCW,
volume 3944 of Lecture Notes in Computer Science,
pages 372–384. Springer.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Pear-Analytics. 2009. Twitter study - august 2009.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL 2004, pages 38–41, Boston, MA.
Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 181–189, Los Angeles, Cali-
fornia.
</reference>
<page confidence="0.975485">
668
</page>
<reference confidence="0.998584162162162">
Ana-Maria Popescu and Marco Pennacchiotti. 2010. De-
tecting controversial events from twitter. In In Pro-
ceedings of the 19th ACM international conference on
Information and knowledge management, pages 1873–
1876.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of the International AAAI Conference on We-
blogs and Social Media, pages 130–137.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 172–180, Los An-
geles, California.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947–953, Morristown, NJ, USA.
Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete.
2009. Efficient kernels for sentence pair classification.
In Conference on Empirical Methods on Natural Lan-
guage Processing, pages 91–100, 6-7 August.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proceedings of the 21st Col-
ing and 44th ACL, pages 401–408, Sydney, Australia,
July.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning ap-
proach to textual entailment recognition. Natural Lan-
guage Engineering, 15-04:551–582.
Q. Zhao, P. Mitra, and B. Chen. 2007. Temporal and in-
formation flow based event detection from social text
streams. In Proceedings of the 22nd national confer-
ence on Artificial intelligence, pages 1501–1506, Van-
couver, British Columbia, Canada.
</reference>
<page confidence="0.998528">
669
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945530">
<title confidence="0.999816">Linguistic Redundancy in Twitter</title>
<author confidence="0.996488">Fabio Massimo Zanzotto Marco Pennacchiotti Kostas Tsioutsiouliklis</author>
<affiliation confidence="0.999781">University of Rome ”Tor Vergata” Yahoo! Labs Yahoo! Labs</affiliation>
<address confidence="0.999816">Rome, Italy Sunnyvale, CA, 94089 Sunnyvale, CA, 94089</address>
<email confidence="0.983911">zanzotto@info.uniroma2.itpennac@yahoo-inc.comkostas@yahoo-inc.com</email>
<abstract confidence="0.99825419047619">In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of microblogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Posters Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>36--44</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6214" citStr="Barbosa and Feng, 2010" startWordPosition="978" endWordPosition="981">final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the tweets that are 660 well-formed (i.e. copy-pasted from news), and induce role mappings between well-formed and noisy tweets in the same cluster b</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Posters Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 36–44, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="25094" citStr="Charniak, 2000" startWordPosition="4146" endWordPosition="4147">nd WBOW. 3 The performance of the different models is computed using the Area Under the ROC curve (AROC) applied to the classification score returned by the SVM. The ROC curve allows us to study the behavior of the classifier in detail, and also provides a powerful way to compare among systems when the dataset is unbalanced (as in our case). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pad´o, 2006). We pre-process the dataset with the following tools: the Charniak Parser (Charniak, 2000) for parsing sentences, the WordNet similarity package (Pedersen et al., 2004) for computing WBOW and for linking the two tweets in a pair, and SVMlight (Joachims, 1999), extended with the syntactic first-order rule kernels described in (Moschitti and Zanzotto, 2007) for creating the SYNT and the FOR feature spaces. We used the Charniak syntactic parser without any specific adaptation to the Twitter language. Model AROC BOW 0.592 WBOW 0.578 LEX + BOW 0.725 † LEX + WBOW 0.728 † SYNT + BOW 0.736 † SYNT + WBOW 0.737 † FOR + BOW 0.739 † FOR + WBOW 0.747 † t Table 2: Experimental results of the dif</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="21151" citStr="Collins and Duffy, 2002" startWordPosition="3458" endWordPosition="3461"> 1 ) + cos(t(a) 2 , t(b) 2 ) where cos(·, ·) is the cosine similarity between the two vectors. The LEX feature space is simple and can be extremely effective in modeling the content of tweet pairs. Yet, in principle, it doesn’t model the relations among words in the tweet. Different content feature spaces are then needed to capture these relations. 4.4 Syntactic content model (SYNT) The SYNT model represents a tweet pair using pairs of syntactic tree fragments from t1 and t2. Each feature is a pair &lt; fr1, fr2 &gt;, where fr1 and fr2 are syntactic tree fragments (see figure below). As defined in (Collins and Duffy, 2002), a syntactic tree fragment fri is active in ti when fri is a subtree of the syntactic interpretation of ti. Therefore, these features represent ground rules connecting the left-hand sides and the right-hand sides of the tweet pair: each feature is active for a pair (t1, t2) when the left-hand side fr1 is activated by the syntactic analysis of t1 and the right-hand side fr2 is activated by t2. As an example consider the feature: that the two tweets are correctly syntactically analyzed). This feature space models the relations between words syntactically. Therefore it overcomes the limitations </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment,</booktitle>
<pages>13--18</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="8351" citStr="Corley and Mihalcea, 2005" startWordPosition="1332" endWordPosition="1336">(Haghighi and Vanderwende, 2009). Since tweets are short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE </context>
<context position="18667" citStr="Corley and Mihalcea, 2005" startWordPosition="3023" endWordPosition="3027"> the set of tokens of the pair. This a very simple baseline as SVM is only learning thresholds using this single feature. The bag-of-word model is of course a naive approach, since in many cases redundant tweets can have very different lexical content (e.g. the following two tweets: “Farrah Fawcett left out of Oscar memorial”, “No Farrah Fawcett’s memory at the Academy Awards”), and non-redundant tweets can have similar lexical content (e.g. the tweets: “Johnny Deep is dead”, “Johnny Deep is not dead”). 4.2 WordNet-based bag-of-word model (WBOW) The second baseline model was first defined in (Corley and Mihalcea, 2005) and since then has been used by many RTE systems. The model extends BOW by measuring similarity at the semantic level, instead of the lexical level. For example, consider the tweet pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, BOW would assign a low score, since many words are not shared across the two tweets. WBOW fixes this problem by matching ‘Oscar’- ‘Academy Awards’ and ‘forgot’-‘snubbed’ at the semantic level. To provide these matches, WBOW relies on specific word simi</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proceedings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment, pages 13–18, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>LNAI 3944: MLCW 2005,</booktitle>
<pages>177--190</pages>
<editor>In Quionero-Candela et al., editor,</editor>
<publisher>SpringerVerlag.</publisher>
<location>Milan, Italy.</location>
<contexts>
<context position="3907" citStr="Dagan et al., 2006" startWordPosition="606" endWordPosition="610">eets are 659 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 659–669, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 1: Twitter search: actual Twitter results and desired results after redundancy reduction. present (left) and where they are discarded (right). Also, from a computational linguistic point of view, the high redundancy in micro-blogs gives the unprecedented opportunity to study classical tasks such as text summarization (Haghighi and Vanderwende, 2009), textual entailment recognition (Dagan et al., 2006) and paraphrase detection (Dolan et al., 2004) on very large corpora characterized by an original and emerging linguistic style, pervaded with ungrammatical and colloquial expressions, abbreviations, and new linguistic forms. The aim of this paper is to formally define, for the first time, the problem of redundancy in micro-blogs and to systematically approach the task of automatic redundancy detection. Note that we focus on linguistic redundancy, i.e. tweets that convey the same information with different wordings, and ignore the more trivial issue of detecting retweets, which can be consider</context>
<context position="7989" citStr="Dagan et al., 2006" startWordPosition="1269" endWordPosition="1272"> probabilistic transition graph for such acts. In our paper, we also aim at classifying tweets, but our interest is in information redundancy instead of acts. In the computational linguistic literature, redundancy detection is studied in multi-document summarization, where the overall document is used to select the most informative sentences or snippets (Haghighi and Vanderwende, 2009). Since tweets are short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature</context>
<context position="10053" citStr="Dagan et al., 2006" startWordPosition="1612" endWordPosition="1615">textually entails’) the other; both tweets state that Switzerland won a Gold Medal at the Vancouver winter Olympics, but the first one also specifies the name of the athlete. The following pair is, instead, non-redundant, because the two tweets convey different information, and they do not subsume each other: (example 2) t1 : “Goal! Iniesta scores for #ESP and they have one hand on the #worldcup” t2 : “this will be a hardfinal #Esp vs Ned #worldcup” Our definition of redundancy is grounded on, and inspired by, the theory of Textual Entailment, to which we refer the reader for further details (Dagan et al., 2006). 3.1 Quantifying redundancy How pervasive is redundancy in Twitter? In order to answer this question we performed an initial editorial study where human editors were asked to annotate pairs of tweets as being either redundant or nonredundant. The editorial study also serves as a test bed for evaluating our redundancy detection models, as discussed in Section 5. In the study we focus on ‘informative’ tweets, i.e. tweets that describe or comment on relevant events/facts. Indeed, these are the types of tweets for which redundancy is a critical issue, especially in view of real applications, e.g.</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Quionero-Candela et al., editor, LNAI 3944: MLCW 2005, pages 177–190, Milan, Italy. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Posters Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>241--249</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6189" citStr="Davidov et al., 2010" startWordPosition="974" endWordPosition="977">nclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the tweets that are 660 well-formed (i.e. copy-pasted from news), and induce role mappings between well-formed and noisy twe</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Posters Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 241–249, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher D. Manning. 2006. Learning to distinguish valid textual entailments. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (Coling</booktitle>
<pages>350--356</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3953" citStr="Dolan et al., 2004" startWordPosition="614" endWordPosition="617">e on Empirical Methods in Natural Language Processing, pages 659–669, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 1: Twitter search: actual Twitter results and desired results after redundancy reduction. present (left) and where they are discarded (right). Also, from a computational linguistic point of view, the high redundancy in micro-blogs gives the unprecedented opportunity to study classical tasks such as text summarization (Haghighi and Vanderwende, 2009), textual entailment recognition (Dagan et al., 2006) and paraphrase detection (Dolan et al., 2004) on very large corpora characterized by an original and emerging linguistic style, pervaded with ungrammatical and colloquial expressions, abbreviations, and new linguistic forms. The aim of this paper is to formally define, for the first time, the problem of redundancy in micro-blogs and to systematically approach the task of automatic redundancy detection. Note that we focus on linguistic redundancy, i.e. tweets that convey the same information with different wordings, and ignore the more trivial issue of detecting retweets, which can be considered the most basic expression of redundancy. Th</context>
<context position="7933" citStr="Dolan et al., 2004" startWordPosition="1261" endWordPosition="1264">estion, response and reaction, and automatically build a probabilistic transition graph for such acts. In our paper, we also aim at classifying tweets, but our interest is in information redundancy instead of acts. In the computational linguistic literature, redundancy detection is studied in multi-document summarization, where the overall document is used to select the most informative sentences or snippets (Haghighi and Vanderwende, 2009). Since tweets are short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spa</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics (Coling 2004), pages 350– 356, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Duan</author>
<author>Long Jiang</author>
<author>Tao Qin</author>
<author>Ming Zhou</author>
<author>Heung-Yeung Shum</author>
</authors>
<title>An empirical study on learning to rank of tweets.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>295--303</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5897" citStr="Duan et al., 2010" startWordPosition="926" endWordPosition="929">Section 2. Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance ca</context>
</contexts>
<marker>Duan, Jiang, Qin, Zhou, Shum, 2010</marker>
<rawString>Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and Heung-Yeung Shum. 2010. An empirical study on learning to rank of tweets. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 295–303, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="3854" citStr="Haghighi and Vanderwende, 2009" startWordPosition="598" endWordPosition="602"> 1 shows an example of a Twitter search engine where redundant tweets are 659 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 659–669, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Figure 1: Twitter search: actual Twitter results and desired results after redundancy reduction. present (left) and where they are discarded (right). Also, from a computational linguistic point of view, the high redundancy in micro-blogs gives the unprecedented opportunity to study classical tasks such as text summarization (Haghighi and Vanderwende, 2009), textual entailment recognition (Dagan et al., 2006) and paraphrase detection (Dolan et al., 2004) on very large corpora characterized by an original and emerging linguistic style, pervaded with ungrammatical and colloquial expressions, abbreviations, and new linguistic forms. The aim of this paper is to formally define, for the first time, the problem of redundancy in micro-blogs and to systematically approach the task of automatic redundancy detection. Note that we focus on linguistic redundancy, i.e. tweets that convey the same information with different wordings, and ignore the more trivi</context>
<context position="7758" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1233" endWordPosition="1236"> conversation according to those acts. (A conversation is defined as a set of tweets in the same reply thread.) The authors define 10 major dialogue acts for Twitter, including status, question, response and reaction, and automatically build a probabilistic transition graph for such acts. In our paper, we also aim at classifying tweets, but our interest is in information redundancy instead of acts. In the computational linguistic literature, redundancy detection is studied in multi-document summarization, where the overall document is used to select the most informative sentences or snippets (Haghighi and Vanderwende, 2009). Since tweets are short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newma</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>387--394</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="8395" citStr="Haghighi et al., 2005" startWordPosition="1341" endWordPosition="1344">re short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE techniques and features that are usually use</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 387–394, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCC’s groundhog system.</title>
<date>2006</date>
<booktitle>In Bernardo Magnini and Ido Dagan, editors, Proceedings of the 2nd PASCAL RTE Challenge,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="8416" citStr="Hickl et al., 2006" startWordPosition="1345" endWordPosition="1348"> cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE techniques and features that are usually used for classical texts</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCC’s groundhog system. In Bernardo Magnini and Ido Dagan, editors, Proceedings of the 2nd PASCAL RTE Challenge, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshay Java</author>
<author>Xiaodan Song</author>
<author>Tim Finin</author>
<author>Belle Tseng</author>
</authors>
<title>Why we Twitter: understanding microblogging usage and communities.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th WebKDD and 1st SNA-KDD</booktitle>
<contexts>
<context position="1675" citStr="Java et al., 2007" startWordPosition="250" endWordPosition="253">dancy detection task, improving over baseline systems with statistical significance. 1 Introduction Micro-blogs and social media services, such as Twitter, have experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon</context>
<context position="5764" citStr="Java et al., 2007" startWordPosition="903" endWordPosition="906">ing the advantages of the proposed model. The rest of the paper is organized as follows. First, we shortly describe related work in Section 2. Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritte</context>
</contexts>
<marker>Java, Song, Finin, Tseng, 2007</marker>
<rawString>Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng. 2007. Why we Twitter: understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 10th International Conference on Research in Computational Linguistics ROCLING,</booktitle>
<pages>132--139</pages>
<location>Tapei, Taiwan.</location>
<contexts>
<context position="19445" citStr="Jiang and Conrath, 1997" startWordPosition="3149" endWordPosition="3152">example, consider the tweet pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, BOW would assign a low score, since many words are not shared across the two tweets. WBOW fixes this problem by matching ‘Oscar’- ‘Academy Awards’ and ‘forgot’-‘snubbed’ at the semantic level. To provide these matches, WBOW relies on specific word similarity measures over WordNet (Miller, 1995), that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&amp;Conrath similarity (Jiang and Conrath, 1997). In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. 4.3 Lexical content model (LEX) This model and the next ones (SYNT and FOR) explicitly model the content of a tweet pair P = 663 (t1, t2) as a whole. This is a radically different approach with respect to the similarity-based models explored so far, where the content of t1 and t2 were treated independently (i.e. each tweet with its own bag of words), and the SVM used as the single f</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics ROCLING, pages 132–139, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel MethodsSupport Vector Learning.</booktitle>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="25263" citStr="Joachims, 1999" startWordPosition="4175" endWordPosition="4176"> curve allows us to study the behavior of the classifier in detail, and also provides a powerful way to compare among systems when the dataset is unbalanced (as in our case). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pad´o, 2006). We pre-process the dataset with the following tools: the Charniak Parser (Charniak, 2000) for parsing sentences, the WordNet similarity package (Pedersen et al., 2004) for computing WBOW and for linking the two tweets in a pair, and SVMlight (Joachims, 1999), extended with the syntactic first-order rule kernels described in (Moschitti and Zanzotto, 2007) for creating the SYNT and the FOR feature spaces. We used the Charniak syntactic parser without any specific adaptation to the Twitter language. Model AROC BOW 0.592 WBOW 0.578 LEX + BOW 0.725 † LEX + WBOW 0.728 † SYNT + BOW 0.736 † SYNT + WBOW 0.737 † FOR + BOW 0.739 † FOR + WBOW 0.747 † t Table 2: Experimental results of the different systems. † indicates statistical significance (p &lt; 0.01) with respect to the two baseline methods BOW and WBOW. ‡ indicates statistical significance (p &lt; 0.1) wit</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel MethodsSupport Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balachander Krishnamurthy</author>
<author>Phillipa Gill</author>
<author>Martin Arlitt</author>
</authors>
<title>A few chirps about twitter.</title>
<date>2008</date>
<booktitle>In Proceedings of the first workshop on Online social networks,</booktitle>
<pages>pages</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="1703" citStr="Krishnamurthy et al., 2008" startWordPosition="254" endWordPosition="257">k, improving over baseline systems with statistical significance. 1 Introduction Micro-blogs and social media services, such as Twitter, have experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of </context>
<context position="5792" citStr="Krishnamurthy et al., 2008" startWordPosition="907" endWordPosition="910">of the proposed model. The rest of the paper is organized as follows. First, we shortly describe related work in Section 2. Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. </context>
</contexts>
<marker>Krishnamurthy, Gill, Arlitt, 2008</marker>
<rawString>Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt. 2008. A few chirps about twitter. In Proceedings of the first workshop on Online social networks, pages 19–24, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haewoon Kwak</author>
<author>Changhyun Lee</author>
<author>Hosung Park</author>
<author>Sue Moon</author>
</authors>
<title>What is twitter, a social network or a news media?</title>
<date>2010</date>
<booktitle>In Proceedings of WWW ’10: Proceedings ofthe 19th international conference on World wide web,</booktitle>
<pages>591--600</pages>
<location>Raleigh, North Carolina, USA.</location>
<contexts>
<context position="1722" citStr="Kwak et al., 2010" startWordPosition="258" endWordPosition="261">ystems with statistical significance. 1 Introduction Micro-blogs and social media services, such as Twitter, have experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Sw</context>
<context position="5812" citStr="Kwak et al., 2010" startWordPosition="911" endWordPosition="914">est of the paper is organized as follows. First, we shortly describe related work in Section 2. Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a sel</context>
</contexts>
<marker>Kwak, Lee, Park, Moon, 2010</marker>
<rawString>Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. 2010. What is twitter, a social network or a news media? In Proceedings of WWW ’10: Proceedings ofthe 19th international conference on World wide web, pages 591–600, Raleigh, North Carolina, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cindy-Xide Lin</author>
<author>Bo Zhao</author>
<author>Qiaozhu Mei</author>
<author>Jiawei Han</author>
</authors>
<title>Pet: a statistical model for popular events tracking in social communities.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>929--938</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="1816" citStr="Lin et al., 2010" startWordPosition="274" endWordPosition="277">h as Twitter, have experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get the Gold on Normal Hill skijump. #Vancouver2010” By performing an editorial s</context>
<context position="6072" citStr="Lin et al., 2010" startWordPosition="955" endWordPosition="958">ncy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the twe</context>
</contexts>
<marker>Lin, Zhao, Mei, Han, 2010</marker>
<rawString>Cindy-Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han. 2010. Pet: a statistical model for popular events tracking in social communities. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 929– 938, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Kuan Li</author>
<author>Bo Han</author>
<author>Ming Zhou</author>
<author>Long Jiang</author>
<author>Zhongyang Xiong</author>
<author>Changning Huang</author>
</authors>
<title>Semantic role labeling for news tweets.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>698--706</pages>
<location>Beijing, China,</location>
<contexts>
<context position="1834" citStr="Liu et al., 2010" startWordPosition="278" endWordPosition="281"> experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get the Gold on Normal Hill skijump. #Vancouver2010” By performing an editorial study (described la</context>
<context position="6331" citStr="Liu et al., 2010" startWordPosition="999" endWordPosition="1002">he social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the tweets that are 660 well-formed (i.e. copy-pasted from news), and induce role mappings between well-formed and noisy tweets in the same cluster by performing word alignment. In our paper we are also interested in aligning and grouping tweets, although our goal i</context>
</contexts>
<marker>Liu, Li, Han, Zhou, Jiang, Xiong, Huang, 2010</marker>
<rawString>Xiaohua Liu, Kuan Li, Bo Han, Ming Zhou, Long Jiang, Zhongyang Xiong, and Changning Huang. 2010. Semantic role labeling for news tweets. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698–706, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>41--48</pages>
<location>New York City, USA.</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 41–48, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="19310" citStr="Miller, 1995" startWordPosition="3132" endWordPosition="3133">by many RTE systems. The model extends BOW by measuring similarity at the semantic level, instead of the lexical level. For example, consider the tweet pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, BOW would assign a low score, since many words are not shared across the two tweets. WBOW fixes this problem by matching ‘Oscar’- ‘Academy Awards’ and ‘forgot’-‘snubbed’ at the semantic level. To provide these matches, WBOW relies on specific word similarity measures over WordNet (Miller, 1995), that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&amp;Conrath similarity (Jiang and Conrath, 1997). In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. 4.3 Lexical content model (LEX) This model and the next ones (SYNT and FOR) explicitly model the content of a tweet pair P = 663 (t1, t2) as a whole. This is a radically different approach with respect to the similarity-based models explored so far, </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference of Machine Learning (ICML),</booktitle>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="23704" citStr="Moschitti and Zanzotto, 2007" startWordPosition="3916" endWordPosition="3919">NP X VP NP X VP h NP Y VBP bought , S S VBP NP VBP NP bought owns This feature is active for the pair of tweets (“GM bought Opel”,“GM owns Opel”) since the syntactic analysis of the pair matches the feature (given This feature is active for the pair (“GM bought Opel”,“GM owns Opel”), with the variable unification X = “GM” and Y = “Opel”. On the contrary, this feature is not active for the pair (“GM bought Opel”,“Opel owns GM”) as there is no possibility of unifying the two variables. Efficient algorithms for the computation of the related kernel functions can h NP VP , N VP i 664 be found in (Moschitti and Zanzotto, 2007; Zanzotto and Dell’Arciprete, 2009). 5 Experimental Evaluation In this section we present an evaluation of the different redundancy detection models. First, we define the experimental setup in Section 5.1. Then, we analyze the results of the experiments in Section 5.2. 5.1 Experimental Setup We experiment with the redundancy detection dataset described in Section 3. We randomly divide the corpus into two sets: 50% for training and 50% for testing. The training set contains 185 positive tweet-pairs and 416 negative pairs. The test set contains 182 positive pairs and 466 negatives. We evaluate </context>
<context position="25361" citStr="Moschitti and Zanzotto, 2007" startWordPosition="4187" endWordPosition="4190">s a powerful way to compare among systems when the dataset is unbalanced (as in our case). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pad´o, 2006). We pre-process the dataset with the following tools: the Charniak Parser (Charniak, 2000) for parsing sentences, the WordNet similarity package (Pedersen et al., 2004) for computing WBOW and for linking the two tweets in a pair, and SVMlight (Joachims, 1999), extended with the syntactic first-order rule kernels described in (Moschitti and Zanzotto, 2007) for creating the SYNT and the FOR feature spaces. We used the Charniak syntactic parser without any specific adaptation to the Twitter language. Model AROC BOW 0.592 WBOW 0.578 LEX + BOW 0.725 † LEX + WBOW 0.728 † SYNT + BOW 0.736 † SYNT + WBOW 0.737 † FOR + BOW 0.739 † FOR + WBOW 0.747 † t Table 2: Experimental results of the different systems. † indicates statistical significance (p &lt; 0.01) with respect to the two baseline methods BOW and WBOW. ‡ indicates statistical significance (p &lt; 0.1) with respect to FOR + BOW 5.2 Experimental Results Table 2 reports the results of the experiment. The</context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Alessandro Moschitti and Fabio Massimo Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Proceedings of the International Conference of Machine Learning (ICML), Corvallis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eamonn Newman</author>
<author>Nicola Stokes</author>
<author>John Dunnion</author>
<author>Joe Carthy</author>
</authors>
<title>Textual entailment recognition using a linguistically-motivated decision tree classifier.</title>
<date>2005</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>372--384</pages>
<editor>In Joaquin Qui˜nonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, MLCW,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="8372" citStr="Newman et al., 2005" startWordPosition="1337" endWordPosition="1340">2009). Since tweets are short and tweet sets cannot be considered documents, these methods are hard to apply. A more convenient setting is paraphrase detection (Dolan et al., 2004) and textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE techniques and featur</context>
</contexts>
<marker>Newman, Stokes, Dunnion, Carthy, 2005</marker>
<rawString>Eamonn Newman, Nicola Stokes, John Dunnion, and Joe Carthy. 2005. Textual entailment recognition using a linguistically-motivated decision tree classifier. In Joaquin Qui˜nonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, MLCW, volume 3944 of Lecture Notes in Computer Science, pages 372–384. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation. Pear-Analytics.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation. Pear-Analytics. 2009. Twitter study - august 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLTNAACL</booktitle>
<pages>38--41</pages>
<location>Boston, MA.</location>
<contexts>
<context position="25172" citStr="Pedersen et al., 2004" startWordPosition="4156" endWordPosition="4159">e Area Under the ROC curve (AROC) applied to the classification score returned by the SVM. The ROC curve allows us to study the behavior of the classifier in detail, and also provides a powerful way to compare among systems when the dataset is unbalanced (as in our case). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pad´o, 2006). We pre-process the dataset with the following tools: the Charniak Parser (Charniak, 2000) for parsing sentences, the WordNet similarity package (Pedersen et al., 2004) for computing WBOW and for linking the two tweets in a pair, and SVMlight (Joachims, 1999), extended with the syntactic first-order rule kernels described in (Moschitti and Zanzotto, 2007) for creating the SYNT and the FOR feature spaces. We used the Charniak syntactic parser without any specific adaptation to the Twitter language. Model AROC BOW 0.592 WBOW 0.578 LEX + BOW 0.725 † LEX + WBOW 0.728 † SYNT + BOW 0.736 † SYNT + WBOW 0.737 † FOR + BOW 0.739 † FOR + WBOW 0.747 † t Table 2: Experimental results of the different systems. † indicates statistical significance (p &lt; 0.01) with respect t</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In Demonstration Papers at HLTNAACL 2004, pages 38–41, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>181--189</pages>
<location>Los Angeles, California.</location>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 181–189, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Detecting controversial events from twitter. In</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>1873--1876</pages>
<contexts>
<context position="1774" citStr="Popescu and Pennacchiotti, 2010" startWordPosition="266" endWordPosition="269">1 Introduction Micro-blogs and social media services, such as Twitter, have experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get the Gold on Normal Hill skijump. #V</context>
<context position="6029" citStr="Popescu and Pennacchiotti, 2010" startWordPosition="947" endWordPosition="950">ection 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Th</context>
<context position="11415" citStr="Popescu and Pennacchiotti, 2010" startWordPosition="1833" endWordPosition="1836">personal messages are of less interest in this context. Dataset extraction. The study is performed on an automatically built dataset of informative tweets. The most critical issue for extracting the dataset is to pre-process tweets and to discard those that are 661 not informative. This is not an easy task: a recent study (Pear-Analytics, 2009) estimates that only 4% of all tweets are factual news, and only 37% are conversations with content. The rest are spam, status updates and other types of uninformative content. In order to retain only informative tweets we first extract buzzy snapshots (Popescu and Pennacchiotti, 2010). A snapshot is defined as a set of tweets that explicitly mention a specific topic within a specified time period. A buzzy snapshot is defined as a snapshot with a large number of tweets, compared to previous time periods. For example, given the topic ‘Haiti earthquake’, the snapshot composed by the tweets mentioning ‘Haiti earthquake’ on January 12th, 2010, will constitute a buzzy snapshot, since in previous days the topic was not mentioned often. We use two different topic lists: a celebrity list containing about 104K celebrity names, crawled from Wikipedia, including actors, musicians, pol</context>
</contexts>
<marker>Popescu, Pennacchiotti, 2010</marker>
<rawString>Ana-Maria Popescu and Marco Pennacchiotti. 2010. Detecting controversial events from twitter. In In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1873– 1876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Susan Dumais</author>
<author>Dan Liebling</author>
</authors>
<title>Characterizing microblogs with topic models.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>130--137</pages>
<contexts>
<context position="5877" citStr="Ramage et al., 2010" startWordPosition="922" endWordPosition="925">ribe related work in Section 2. Next, we provide our operational definition of redundancy and introduce our editorial study and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressin</context>
</contexts>
<marker>Ramage, Dumais, Liebling, 2010</marker>
<rawString>Daniel Ramage, Susan Dumais, and Dan Liebling. 2010. Characterizing microblogs with topic models. In Proceedings of the International AAAI Conference on Weblogs and Social Media, pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="1856" citStr="Ritter et al., 2010" startWordPosition="282" endWordPosition="285">ponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get the Gold on Normal Hill skijump. #Vancouver2010” By performing an editorial study (described later in the paper) we d</context>
<context position="6379" citStr="Ritter et al., 2010" startWordPosition="1006" endWordPosition="1009"> 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweets that refer to the same news. Then, for each cluster, they identify the tweets that are 660 well-formed (i.e. copy-pasted from news), and induce role mappings between well-formed and noisy tweets in the same cluster by performing word alignment. In our paper we are also interested in aligning and grouping tweets, although our goal is to detect redundancy, not to perform SRL. On a</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172–180, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>947--953</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="24971" citStr="Yeh, 2000" startWordPosition="4128" endWordPosition="4129">ture combinations: LEX+BOW, LEX+WBOW, SYNT+BOW, SYNT+WBOW, FOR+BOW, FOR+WBOW. We compare to the system baselines BOW and WBOW. 3 The performance of the different models is computed using the Area Under the ROC curve (AROC) applied to the classification score returned by the SVM. The ROC curve allows us to study the behavior of the classifier in detail, and also provides a powerful way to compare among systems when the dataset is unbalanced (as in our case). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pad´o, 2006). We pre-process the dataset with the following tools: the Charniak Parser (Charniak, 2000) for parsing sentences, the WordNet similarity package (Pedersen et al., 2004) for computing WBOW and for linking the two tweets in a pair, and SVMlight (Joachims, 1999), extended with the syntactic first-order rule kernels described in (Moschitti and Zanzotto, 2007) for creating the SYNT and the FOR feature spaces. We used the Charniak syntactic parser without any specific adaptation to the Twitter language. Model AROC BOW 0.592 WBOW 0.578 LEX + BOW 0.725 † LEX + WBOW 0.7</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics, pages 947–953, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Efficient kernels for sentence pair classification.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>91--100</pages>
<marker>Zanzotto, Dell’Arciprete, 2009</marker>
<rawString>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2009. Efficient kernels for sentence pair classification. In Conference on Empirical Methods on Natural Language Processing, pages 91–100, 6-7 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<pages>401--408</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="22750" citStr="Zanzotto and Moschitti, 2006" startWordPosition="3731" endWordPosition="3734">eets P(a) and P(b), the SYNT kernel function is defined as follows: KSY NT(P(a), P(b)) = K(t(a) 1 , t(b) 1 ) + K(t(a) 2 , t(b) 2 ) where K(·, ·) is the tree kernel function described in (Collins and Duffy, 2002). 4.5 Syntactic first-order rule content model (FOR) The FOR model overcomes the limitations of SYNT, by enriching the space with features representing first-order relations between the two tweets of a pair. Each feature represents a rule with variables, i.e. a first order rule that is activated by the tweet pairs if the variables are unified. This feature space has been introduced in (Zanzotto and Moschitti, 2006) and shown to improve over the ones above. Each feature &lt; fr1, fr2 &gt; is a pair of syntactic tree fragments augmented with variables. The feature is active for a tweet pair (t1, t2) if the syntactic interpretations of t1 and t2 can be unified with &lt; fr1, fr2 &gt;. For example, consider the following feature: S S VBP owns i NP Y NP X VP NP X VP h NP Y VBP bought , S S VBP NP VBP NP bought owns This feature is active for the pair of tweets (“GM bought Opel”,“GM owns Opel”) since the syntactic analysis of the pair matches the feature (given This feature is active for the pair (“GM bought Opel”,“GM ow</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, pages 401–408, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A machine learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--04</pages>
<contexts>
<context position="8560" citStr="Zanzotto et al., 2009" startWordPosition="1368" endWordPosition="1372">textual entailment recognition (Dagan et al., 2006) (RTE). In RTE the task is to recognize if a text called the text T (typically one or two sentences long) entails another text called the hypothesis H. Many approaches have been proposed for this task, mostly based on machine learning. Three main classes of features have been so far explored in RTE: distance/similarity feature spaces (Corley and Mihalcea, 2005; Newman et al., 2005; Haghighi et al., 2005; Hickl et al., 2006), entailment trigger feature spaces (de Marneffe et al., 2006; MacCartney et al., 2006), and pair content feature spaces (Zanzotto et al., 2009). Distance/similarity feature spaces are more suitable to the paraphrase detection task because they model the similarity between the two texts. On the other hand, entailment trigger and content feature spaces model complex relations between the texts, taking into account first-order entailment rules, i.e. entailment rules with variables. In this paper, one of our goals is to explore RTE techniques and features that are usually used for classical texts, and check if they can be successfully adapted to the unstructured, and oftentimes ungrammatical, Twitter language. 3 Redundancy in Twitter We </context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. Natural Language Engineering, 15-04:551–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhao</author>
<author>P Mitra</author>
<author>B Chen</author>
</authors>
<title>Temporal and information flow based event detection from social text streams.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd national conference on Artificial intelligence,</booktitle>
<pages>1501--1506</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1741" citStr="Zhao et al., 2007" startWordPosition="262" endWordPosition="265">ical significance. 1 Introduction Micro-blogs and social media services, such as Twitter, have experienced an exponential growth in the last few years. The interest of the research community and the industry in these services has followed a similar trend. Web companies such as Google, Yahoo, and Bing are integrating more and more social content to their sites. At the same time, the computational linguistic community is getting increasingly interested in studying social and linguistic properties of Twitter and other micro-blogs (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010; Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010; Liu et al., 2010; Ritter et al., 2010). Yet, so far, not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. Users often post messages with the same, or very similar, content, especially when reporting or commenting on news and events. For example, the following two tweets are part of a large set of redundant tweets issued during the 2010 winter Olympics: (example 1) t1 : “Swiss ski jumper Simon Ammann takes first gold of Vancouver” t2 : “Swiss (Suisse) get th</context>
<context position="5996" citStr="Zhao et al., 2007" startWordPosition="943" endWordPosition="946">dy and dataset in Section 3. In Section 4 we describe our models for redundancy detection. In Section 5 we provide a quantitative and qualitative evaluation of our models. In Section 6 we conclude the paper with final observations and future work. 2 Related Work So far, most research on Twitter has focused on its network structure, the social behavior of its users (Java et al., 2007; Krishnamurthy et al., 2008; Kwak et al., 2010), ranking tweets by relevance for web search (Ramage et al., 2010; Duan et al., 2010), and the analysis of time series for extracting trending news, events and facts (Zhao et al., 2007; Popescu and Pennacchiotti, 2010; Petrovi´c et al., 2010; Lin et al., 2010). Only few studies have specifically focused on the linguistic content analysis of tweets, e.g. (Davidov et al., 2010; Barbosa and Feng, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al., 2010). Liu et al. (2010) present a self-learning SRL system for news tweets, with the goal of addressing low performance caused by the noise and the unstructured nature of the data. The authors first cluster together tweet</context>
</contexts>
<marker>Zhao, Mitra, Chen, 2007</marker>
<rawString>Q. Zhao, P. Mitra, and B. Chen. 2007. Temporal and information flow based event detection from social text streams. In Proceedings of the 22nd national conference on Artificial intelligence, pages 1501–1506, Vancouver, British Columbia, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>