<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003259">
<title confidence="0.986377">
Probabilistic Network Models for Word Sense Disambiguation
</title>
<author confidence="0.999394">
Gerald Chao and Michael G. Dyer
</author>
<affiliation confidence="0.9978965">
Computer Science Department,
University of California, Los Angeles
</affiliation>
<address confidence="0.861297">
Los Angeles, California 90095
</address>
<email confidence="0.994136">
gerald@cs.ucla.edu, dyerAcs.ucla.edu
</email>
<sectionHeader confidence="0.979676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986">
We present the techniques used in the word sense
disambiguation (WSD) system that was submitted
to the SENSEVAL-2 workshop. The system builds a
probabilistic network per sentence to model the de-
pendencies between the words within the sentence,
and the sense tagging for the entire sentence is com-
puted by performing a query over the network. The
salient context used for disambiguation is based on
sentential structure and not positional information.
The parameters are established automatically and
smoothed via training data, which was compiled
from the SemCor corpus and the WordNet glosses.
Lastly, the One-sense-per-discourse (OSPD) hypoth-
esis is incorporated to test its effectiveness. The re-
sults from two parameterization techniques and the
effects of the OSPD hypothesis are presented.
</bodyText>
<sectionHeader confidence="0.896791" genericHeader="method">
1 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999945153846154">
WSD is treated in this system as a classification
task, where the ith sense (W#i) of a word (W) is
classified as the correct sense tag (M,), given the
word W and usually some surrounding context. In
the SENSEVAL-2 English all-words task, all ambigu-
ous content words (nouns, verbs, adjectives, and ad-
verbs) are to be classified with a sense tag from the
WordNet 1.7 lexical database (Miller, 1990). For
example, the words &amp;quot;great&amp;quot;, &amp;quot;devastated&amp;quot;, and &amp;quot;re-
gion&amp;quot; in the sentence &amp;quot;The great hurricane devas-
tated the region&amp;quot; are classified with the correct sense
tags 2, 2, and 2, respectively. We will refer to this
task using the following notation:
</bodyText>
<equation confidence="0.627235">
= mbee(s) = erg rnaxP(MIS), (1)
</equation>
<bodyText confidence="0.9990506">
where S is the input sentence, and M is the se-
mantic tag assigned to each word. While a context
larger than the sentence S can be and is used in our
model, we will refer to the context as S. In this for-
mulation, each word Wi in the sentence is treated as
a random variable M, taking on the values 11..Nil,
where Ni is the number of senses for the word W.
Therefore, we wish to find instantiations of M such
that P(MIS) is maximized.
To make the computation of
</bodyText>
<equation confidence="0.49964525">
Mbest(S
more tractable, it can be decomposed into
Mbest(S) erg max(HiP(MilS)) where t is
)
</equation>
<bodyText confidence="0.992600642857143">
assumed that each word can be disambiguated
independently. However, this assumption does
i
not always hold, since disambiguating one word
often affects the sense assignment of another
word within the same sentence. Alternatively, the
process can be modeled as a Markov model, e.g.,
Mbest (S) erg max(HiP(Wi x P(MilMi—i)).
While the Markov model requires fewer param-
eters, it is unable to capture the long-distance
dependencies that occur in natural languages.
Although the first decomposition better captures
these dependencies, computing P(MIS) using the
full sentential context is rarely used, since the
number of parameters required grows exponen-
tially with each added context. Therefore, one
can further simplify this model by narrowing the
context to 2n number of surrounding words, i.e.,
P(MIS) &amp;quot;&amp;quot;&apos; P(MilWi—n, •••Wi--i, Wi+i •••Wi+n)•
However, narrowing the context also discards
long-distance relationships, making it closer to a
Markov model.
Without having to artificially limit the size of
the context, another possible simplification is to
make independence assumptions between the con-
text words. In the simplest case, every context is
assumed to be independent from each other, i.e.,
P(MIS) nxP(MilWx), like a Naive Bayes classi-
fier. While the parameters can be simply established
by a set of bi-grams, the independence assumption
is often too strong and thus negatively affects accu-
racy. The difficulty is in choosing the context that
would maximize the accuracy while allowing for re-
liable parameter estimation from training data.
In our model, we aim to strike this balance by
choosing the context words based on structural in-
formation, rather than positional information. The
hypothesis is that an ambiguous word is probabilisti-
cally dependent on its structurally related words and
is independent of the rest of the sentence. There-
fore, long-distance dependencies can still be cap-
tured, while the context is kept small. Further-
</bodyText>
<page confidence="0.960926">
63
</page>
<equation confidence="0.997903166666667">
P(A113, C)
P(BID,F) P(CID)
P(DIE)
P(E)
P(A,B,C,D,E,F)=P(A113,C)xP(BiD,E)xP(CID)
xP(DIE)xP(E)xP(F)
</equation>
<figureCaption confidence="0.94703">
Figure 1: An example of a Bayesian network and
</figureCaption>
<bodyText confidence="0.952302428571429">
the probability tables at each node that define the
relationships between a node and its parents. The
equation at the bottom shows how the distribution
is represented by the network.
more, each word is not classified independently of
each other, but is computed as one single query that
determines all of the sense assignments that result
in the highest overall probability for the whole sen-
tence. Therefore; our model is a combination of the
decompositions described above, by selectively mak-
ing independence assumptions on a per-word basis
to best model P(MdS), while computing Mbest (S)
in one query to allow for interactions between the
word senses M.
</bodyText>
<subsectionHeader confidence="0.995932">
1.1 Bayesian Networks
</subsectionHeader>
<bodyText confidence="0.999641211538461">
This process is achieved by using Bayesian networks
to model the dependencies between each word and
its contextual words, and based on the parame-
terization, compute the best overall sense assign-
ments. A Bayesian network is a directed acyclic
graph G that represents a joint probability distri-
bution P(Xi ...,Xn) across the random variables of
each node in the graph. By making independence
assumptions between variables, each node i is condi-
tionally dependent upon only its parents PA, (Pearl,
1988): P(Xl, X,) = ILP(Xi1PAi). By using this
representation, the number of probabilities needed
to represent the distribution can be significantly re-
duced. Figure 1 shows an example Bayesian net-
work representing the distribution P(A,B,C,D,E,F).
Instead of having one large table with 26 parameters
(with all Boolean nodes), the distribution is repre-
sented by the conditional probability tables (CPTs)
at each node, such as P(B I D, F) at node 13, re-
quiring a total of only 24 parameters for the whole
distribution. Not only do the savings become more
significant with larger networks, but the sparse data
problem becomes more manageable as well. The
training set no longer needs to cover all permuta-
tions of the feature sets, but only smaller subsets
dictated by the sets of variables of the CPTs.
In our model using Bayesian networks for WSD,
each word is represented by the random variable
M., as a node in G. We then find a set of par-
ents PA i that M depends on, based on struc-
tural information. Using this representation, the
number of parameters is significantly reduced. If
the average number of parents per node is 2, and
if the average number of senses per word is 5,
then the joint distribution across the whole sentence
P(Mi • MN) is represented by the Bayesian net-
work with 5(2+1) * N parameters. This is in con-
trast to a full joint distribution table that would con-
tain 5&apos; entries, which is obviously intractable for
any sentence of non-trivial length N. Bayesian net-
works also facilitate the computation of the instanti-
ations for Mi such that P(Mi. MN) is maximum.
Instead of looking for the maximum row in the table
with 5N entries, this computation is made tractable
by using Bayesian networks. Specifically, this query,
called Maximum A Posteriori (MAP), can be com-
puted in 0(5w), where w &lt;&lt; N and indicates the
connectiveness of G.
Using the same notation above, the process of
a whole-sentence word sense disambiguation using
probabilistic networks can be described as the fol-
lowing:
</bodyText>
<equation confidence="0.9627345">
Mbest(S) erg maxIliP(MilWi, WPA ,MPAJ
erg maxIL (P(Mi )P(M21W2, WpA, )). (2)
</equation>
<bodyText confidence="0.9998701">
The first approximation is based on our hypoth-
esis of a word&apos;s sense is dependent only on struc-
turally related words. It is further decomposed in
the second term to minimize the sparse data prob-
lem. This process consists of three major steps: 1)
defining the structure of the Bayesian network G,
2) quantifying the network with probabilities from
training data (P(M, I Wi, WpA,)); and finally, 3) an-
swering the query of the most probable word sense
assignments (erg maxIL (...)).
</bodyText>
<sectionHeader confidence="0.987745" genericHeader="method">
2 Network Structure
</sectionHeader>
<bodyText confidence="0.999386">
The first step in constructing a Bayesian network
is to determine its structure G, which defines each
node&apos;s dependency relationship with the rest of the
network. In our model, we are making these inde-
pendence assumptions based on the structural re-
lationships between words. Specifically, given the
sentence S and its parse tree, we automatically con-
struct a graph G by first creating a node Mi for each
word W. This process is best illustrated by the ex-
ample shown in Figure 2. For each node Mi, an edge
is added to node M. where Mx is the head word of
a verb phrase (board approved), the target of the
modifier Mi (today&apos;s meeting), or the preposition
Mx where Mi is the target or a constituent of the
prepositional phrase (approved at). One can see
that if the parse tree is known, the construction of
netwOrk G is straight-forward. For SENSEVAL-2, the
</bodyText>
<equation confidence="0.528146">
P(F)
</equation>
<page confidence="0.916889">
64
</page>
<figureCaption confidence="0.8284805">
Figure 2: An example of a Bayesian network repre-
senting the inter-dependencies between the words of
the sentence &amp;quot;The board approved its acquisition by
ABC Co. of New York at today&apos;s meeting.&amp;quot;
parse trees provided in Treebank format were used
to build the Bayesian networks&apos; structure.
</figureCaption>
<bodyText confidence="0.9993755">
Once the structure of the Bayesian network is de-
termined, the context, i.e., the parents PA, for each
word is established. Using the same example, the
context for the word &amp;quot;approved&amp;quot; is &amp;quot;board&amp;quot; and &amp;quot;ac-
quisition&apos;&apos;, and for &amp;quot;at&amp;quot; it is &amp;quot;approved&amp;quot; and &amp;quot;meet-
ing&apos;. Our hypothesis is that these structurally re-
lated words, among all of the words within the sen-
tence, provide the best contextual information for
sense disambiguation. That is, given that the par-
ents&apos; word form WpA, and senses MPA are known,
the sense assignment for Mi is independent of all
other words in the sentence. This is, of course, a
simplification due to the constraint in minimizing
the context. However, the use of Bayesian networks
allows for easy expansion of context by establish-
ing more edges between nodes or adding new nodes,
provided that the parameters can be determined re-
liably.
</bodyText>
<sectionHeader confidence="0.965789" genericHeader="method">
3 Establishing the Parameters
</sectionHeader>
<bodyText confidence="0.999951983870968">
Once G is determined, the CPTs at each node
need to be quantified. Using the same exam-
ple above, for the word &amp;quot;approved&amp;quot;, its CPT
P(approved#ilboard#i, acquisition#i) would con-
tain 2 (number of senses for &amp;quot;approved&amp;quot;) x 9 x 4 =
72 entries. For a word without any parents, such as
&amp;quot;today&apos;s&apos;&amp;quot;, its priors are used.
While determining the network structure is rel-
atively simple, establishing accurate parameters is
quite difficult, even with a small context such as
ours. Due to the limited size of SemCor, our only
labeled training data, we used additional sources
to quantify and smooth these parameters. Primar-
ily we deployed the same techniques used in our
Bayesian Hierarchical Disambiguator (BHD) model
(Chao and Dyer, 2000), which uses Internet search
engines to estimate parameters based on permuta-
tions of synonym words, a method first introduced
by Mihalcea and Moldovan (1999). These param-
eters are then smoothed by training data obtained
from SemCor. The details of BHD are omitted here
due to space constraints.
Although BHD was only used on adjective-noun
pairs, the same principles are used to quantify all of
the CPTs in this model. While only one hierarchi-
cal network is needed to smooth the parameter for
adjective-noun pairs, up to three hierarchical net-
works are used for each potential parent. Since the
smoothing computation is very efficient, being linear
in the depth of the network, these additions did not
impact the speed of the model. The majority of the
time was used to query the Internet search engine.
The BHD model, however, did use additional
training data that was collected from the Word-
Net glosses and manually annotated. While it re-
sulted in good accuracy, this was obviously not an
option for SENSEVAL-2. Instead, the example sen-
tences from WordNet are extracted and first tagged
by Brill&apos;s POS tagger (Brill, 1995). Then an ex-
perimental parser and our WSD system were used
to parse and disambiguate the sentences to extract
additional training data. For example, for the 6th
sense of adjective &amp;quot;great&amp;quot;, the pair &amp;quot;great#6 time&amp;quot; is
extracted from the example sentence fragment &amp;quot;had
a great time at the party&amp;quot; and automatically dis-
ambiguated. The labeled pair is then added to the
training set for great#6.
Lastly, the priors in this model are determined
directly from SemCor&apos;s occurrence statistics and
estimated using Maximum Likelihood Estimation
(MLE). This is another simplification over the BHD
model, where the priors were determined using the
hundred most frequent adjective-noun pairs culled
from the Internet and then manually classified. It is
well known that MLE is inaccurate when the num-
ber of events are low, as is in this case when rarer
senses often have only single occurrences.
Nevertheless, we are able to addreSs both of the
manual steps used in the BHD model with auto-
mated processes. However, it is our belief that they
are also the weakest part of our model and contribute
the most to the errors.
</bodyText>
<sectionHeader confidence="0.926606" genericHeader="method">
4 Querying the Network
</sectionHeader>
<bodyText confidence="0.9999549375">
With both the structure G and the parameters
established, the query we pose is to compute
the instantiations for each random variable that
would result in the highest joint probability, i.e.,
arg maxP(Mi IS). This is computed easily using the
Maximum A Posteriori (MAP) query. This was im-
plemented using the JointTree algorithm (Darwiche,
1995) and can be computed in 0(1cr) time, where
jcl is the size of the variable (number of senses), and
w is the tree width. Given that our networks are
sparsely connected, w is usually close to 3, the aver-
age number of parents + 1.
The advantage of using the MAP query is that
it computes variable instantiations that will maxi-
mize the overall probability across the whole sen-
tence, rather than the localized context. Further-
</bodyText>
<page confidence="0.999783">
65
</page>
<tableCaption confidence="0.934737">
Table 1: Precision/recall results of the three models
submitted to SENSEVAL-2.
</tableCaption>
<bodyText confidence="0.9987705">
more, the resulting instantiation and probability is
guaranteed to be maximum. So given the indepen-
dence assumptions made on the context and the es-
timated parameters, MAP will always produce the
most probable sense tagging for every word in the
sentence.
</bodyText>
<sectionHeader confidence="0.951564" genericHeader="method">
5 Beyond Sentential Context
</sectionHeader>
<bodyText confidence="0.999995333333334">
It is well known that word senses are often influ-
enced by contexts larger than the sentence, such as
surrounding sentences or even the whole passage.
We experimented with the One-sense-per-discourse
(OSPD) hypothesis (Yarowsky, 1993) by applying
the probabilities described in Stetina et al. (1998)
to words that have previously appeared in the text
and thus have been disambiguated. The only mod-
ification needed to our model described thus far is
to apply OSPD probabilities, which is dependent on
the distance between the sentences, to each sense
of a re-occurring word before the MAP query. It is
our observation that this incarnation of the OSPD
hypothesis, chosen for its ease of implementation,
tends to propagate erroneous sense tagging from ini-
tial sentences to the remainder of the passage. A
better approach would be to determine the one sense
that would maximize the consensus across the whole
passage, as well as within each individual sentence.
How this can be achieved efficiently in a probabilistic
framework is currently being investigated.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="conclusions">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999908256410257">
For SENSEVAL-2, we submitted three models for
comparison, which differ by their methods of pa-
rameter estimation. Model 2 uses the training data
from SemCor and Hierarchical networks to smooth
the parameters from Internet search engines. Model
3 incorporates additional training data gathered au-
tomatically from the WordNet glosses. Lastly; model
1 combines all training data, as well as the OSPD
hypothesis.
One can see that the model that uses all of the
available data achieved best accuracy (model 1) but
unfortunately also had the lowest recall due to the
added complexity. Some highly polysemous words
were omitted due to time and memory constraints.
Between the 2 training sets, it was unfortunate that
the addition of the automatically generated training
set reduced the accuracy slightly, mainly due to the
noisy data produced by our experimental system.
Nevertheless, we believe that there is a wealth
of information contained within WordNet&apos;s glosses.
Since one of our aims is to use as much automated
processing as possible, we are focusing on improving
the accuracy of the automatically generated train-
ing data. Our goal is that as the WSD accuracy
of our system improves, so will the reliably of these
automatically generated training data. Having im-
proved training data will further improve the sys-
tem&apos;s WSD accuracy, i.e., a bootstrapping system.
We are at the initial stage of this process, but some
fundamental problems such as reliable POS tagging
and parsing of sentence fragments need to be ad-
dressed first. Furthermore, parameter estimation
based on Internet statistics might prove to be too
noisy, so we are currently focusing on learning al-
gorithms such as Expectation Maximization to tune
the parameters. Lastly, if our context is found to
be too limited, additional features can be added to
the Bayesian networks to improve the classification
accuracy.
</bodyText>
<sectionHeader confidence="0.994549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.926434606060606">
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21:722-727.
Gerald Chao and Michael G. Dyer. 2000. Word
sense disambiguation of adjectives using proba-
bilistic networks. In Proceedings of the Eighteenth
International Conference on Computational Lin-
guistics.
Adnan Darwiche. 1995. Conditional algorithms for
exact and approximate inference in causal net-
works. In Proceedings of the Sixth Conference on
Uncertainty in Artificial Intelligence, pages 99-
107.
Sadao Kurohashi Jiri Stetina and Makoto Nagao.
1998. General word sense disambiguation method
based on a full sentential context. In Proceedings
of COLING-ACL Workshop on Usage of Word-
Net in Natural Language Processing, Montreal,
Canada, pages 1-8, July.
Rada Mihalcea and Dan Moldovan. 1999. A method
for word sense disambiguation of unrestricted
text. In Proceedings of the 37th Annual Meeting
of the ACL, pages 152-158, Maryland, NY, June.
G. Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4).
Judea Pearl. 1988. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
David Yarowsky. 1993. One sense per collocation.
In Proceedings of ARPA Human Language Tech-
nology, Princeton, pages 266-271.
</reference>
<figure confidence="0.9714753">
Model
Precision
Recall
1 0.500
2 0.475
3 0.474
0.449
0.454
0.453
66
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000439">
<title confidence="0.997899">Probabilistic Network Models for Word Sense Disambiguation</title>
<author confidence="0.951173">G Chao</author>
<affiliation confidence="0.999437">Computer Science University of California, Los</affiliation>
<address confidence="0.991306">Los Angeles, California</address>
<email confidence="0.999971">gerald@cs.ucla.edu,dyerAcs.ucla.edu</email>
<abstract confidence="0.989761142487047">We present the techniques used in the word sense disambiguation (WSD) system that was submitted to the SENSEVAL-2 workshop. The system builds a probabilistic network per sentence to model the dependencies between the words within the sentence, and the sense tagging for the entire sentence is computed by performing a query over the network. The salient context used for disambiguation is based on sentential structure and not positional information. The parameters are established automatically and smoothed via training data, which was compiled from the SemCor corpus and the WordNet glosses. Lastly, the One-sense-per-discourse (OSPD) hypothesis is incorporated to test its effectiveness. The results from two parameterization techniques and the effects of the OSPD hypothesis are presented. 1 Problem Formulation WSD is treated in this system as a classification where the sense (W#i) of a word (W) is as the correct sense tag the word W and usually some surrounding context. In the SENSEVAL-2 English all-words task, all ambiguous content words (nouns, verbs, adjectives, and adverbs) are to be classified with a sense tag from the WordNet 1.7 lexical database (Miller, 1990). For example, the words &amp;quot;great&amp;quot;, &amp;quot;devastated&amp;quot;, and &amp;quot;region&amp;quot; in the sentence &amp;quot;The great hurricane devastated the region&amp;quot; are classified with the correct sense tags 2, 2, and 2, respectively. We will refer to this task using the following notation: rnaxP(MIS), (1) the input sentence, and the semantic tag assigned to each word. While a context than the sentence be and is used in our we will refer to the context as this formulation, each word Wi in the sentence is treated as random variable on the values 11..Nil, Ni is the number of senses for the word we wish to find instantiations of maximized. To make the computation of more tractable, it can be decomposed into where tis ) assumed that each word can be disambiguated independently. However, this assumption does i not always hold, since disambiguating one word often affects the sense assignment of another word within the same sentence. Alternatively, the process can be modeled as a Markov model, e.g., While the Markov model requires fewer parameters, it is unable to capture the long-distance dependencies that occur in natural languages. Although the first decomposition better captures dependencies, computing the full sentential context is rarely used, since the number of parameters required grows exponentially with each added context. Therefore, one can further simplify this model by narrowing the context to 2n number of surrounding words, i.e., &amp;quot;&amp;quot;&apos; P(MilWi—n, •••Wi--i, Wi+i •••Wi+n)• However, narrowing the context also discards long-distance relationships, making it closer to a Markov model. Without having to artificially limit the size of the context, another possible simplification is to make independence assumptions between the context words. In the simplest case, every context is assumed to be independent from each other, i.e., like a Naive Bayes classifier. While the parameters can be simply established by a set of bi-grams, the independence assumption is often too strong and thus negatively affects accuracy. The difficulty is in choosing the context that would maximize the accuracy while allowing for reliable parameter estimation from training data. In our model, we aim to strike this balance by the context words based on information, rather than positional information. The hypothesis is that an ambiguous word is probabilistically dependent on its structurally related words and is independent of the rest of the sentence. Therefore, long-distance dependencies can still be capwhile the context is kept small. Further- 63 P(A113, C) P(BID,F) P(CID) P(DIE) P(E) P(A,B,C,D,E,F)=P(A113,C)xP(BiD,E)xP(CID) xP(DIE)xP(E)xP(F) Figure 1: An example of a Bayesian network and the probability tables at each node that define the relationships between a node and its parents. The equation at the bottom shows how the distribution is represented by the network. more, each word is not classified independently of each other, but is computed as one single query that determines all of the sense assignments that result in the highest overall probability for the whole senour model is a combination of the decompositions described above, by selectively making independence assumptions on a per-word basis best model P(MdS), while computing (S) in one query to allow for interactions between the senses 1.1 Bayesian Networks This process is achieved by using Bayesian networks to model the dependencies between each word and its contextual words, and based on the parameterization, compute the best overall sense assignments. A Bayesian network is a directed acyclic represents a joint probability distrithe random variables of each node in the graph. By making independence assumptions between variables, each node i is condidependent upon only its parents By using this representation, the number of probabilities needed to represent the distribution can be significantly reduced. Figure 1 shows an example Bayesian network representing the distribution P(A,B,C,D,E,F). of having one large table with parameters (with all Boolean nodes), the distribution is reprethe conditional tables (CPTs) each node, such as P(B ID, F) at node 13, requiring a total of only 24 parameters for the whole distribution. Not only do the savings become more significant with larger networks, but the sparse data problem becomes more manageable as well. The training set no longer needs to cover all permutations of the feature sets, but only smaller subsets dictated by the sets of variables of the CPTs. In our model using Bayesian networks for WSD, each word is represented by the random variable as a node in then find a set of parithat M depends on, based on structural information. Using this representation, the number of parameters is significantly reduced. If the average number of parents per node is 2, and if the average number of senses per word is 5, then the joint distribution across the whole sentence • represented by the Bayesian netwith * This is in contrast to a full joint distribution table that would contain 5&apos; entries, which is obviously intractable for sentence of non-trivial length networks also facilitate the computation of the instantifor such that MN) maximum. Instead of looking for the maximum row in the table entries, this computation is made tractable by using Bayesian networks. Specifically, this query, called Maximum A Posteriori (MAP), can be computed in 0(5w), where w &lt;&lt; N and indicates the of Using the same notation above, the process of a whole-sentence word sense disambiguation using probabilistic networks can be described as the following: Mbest(S) erg maxIliP(MilWi, WPA ,MPAJ )P(M21W2, WpA, (2) The first approximation is based on our hypothesis of a word&apos;s sense is dependent only on structurally related words. It is further decomposed in the second term to minimize the sparse data problem. This process consists of three major steps: 1) the structure of the Bayesian network 2) quantifying the network with probabilities from data Wi, and finally, 3) answering the query of the most probable word sense (...)). 2 Network Structure The first step in constructing a Bayesian network to determine its structure defines each node&apos;s dependency relationship with the rest of the network. In our model, we are making these independence assumptions based on the structural relationships between words. Specifically, given the its parse tree, we automatically cona graph first creating a node Mi for each word W. This process is best illustrated by the exshown in Figure 2. For each node edge added to node is the head word of a verb phrase (board approved), the target of the modifier Mi (today&apos;s meeting), or the preposition where Mi is the target or a constituent of the prepositional phrase (approved at). One can that if the parse tree is known, the construction of straight-forward. For P(F) 64 Figure 2: An example of a Bayesian network representing the inter-dependencies between the words of the sentence &amp;quot;The board approved its acquisition by ABC Co. of New York at today&apos;s meeting.&amp;quot; parse trees provided in Treebank format were used to build the Bayesian networks&apos; structure. Once the structure of the Bayesian network is determined, the context, i.e., the parents PA, for each word is established. Using the same example, the context for the word &amp;quot;approved&amp;quot; is &amp;quot;board&amp;quot; and &amp;quot;acquisition&apos;&apos;, and for &amp;quot;at&amp;quot; it is &amp;quot;approved&amp;quot; and &amp;quot;meeting&apos;. Our hypothesis is that these structurally related words, among all of the words within the sentence, provide the best contextual information for sense disambiguation. That is, given that the parword form senses known, sense assignment for is independent of all other words in the sentence. This is, of course, a simplification due to the constraint in minimizing the context. However, the use of Bayesian networks allows for easy expansion of context by establishing more edges between nodes or adding new nodes, provided that the parameters can be determined reliably. 3 Establishing the Parameters determined, the CPTs at each node need to be quantified. Using the same example above, for the word &amp;quot;approved&amp;quot;, its CPT acquisition#i) contain 2 (number of senses for &amp;quot;approved&amp;quot;) x 9 x 4 = 72 entries. For a word without any parents, such as &amp;quot;today&apos;s&apos;&amp;quot;, its priors are used. While determining the network structure is relatively simple, establishing accurate parameters is quite difficult, even with a small context such as ours. Due to the limited size of SemCor, our only labeled training data, we used additional sources to quantify and smooth these parameters. Primarily we deployed the same techniques used in our Bayesian Hierarchical Disambiguator (BHD) model (Chao and Dyer, 2000), which uses Internet search engines to estimate parameters based on permutations of synonym words, a method first introduced by Mihalcea and Moldovan (1999). These parameters are then smoothed by training data obtained from SemCor. The details of BHD are omitted here due to space constraints. Although BHD was only used on adjective-noun pairs, the same principles are used to quantify all of the CPTs in this model. While only one hierarchical network is needed to smooth the parameter for adjective-noun pairs, up to three hierarchical networks are used for each potential parent. Since the smoothing computation is very efficient, being linear in the depth of the network, these additions did not impact the speed of the model. The majority of the time was used to query the Internet search engine. The BHD model, however, did use additional training data that was collected from the Word- Net glosses and manually annotated. While it resulted in good accuracy, this was obviously not an for the example sentences from WordNet are extracted and first tagged Brill&apos;s POS tagger Then an experimental parser and our WSD system were used to parse and disambiguate the sentences to extract additional training data. For example, for the 6th sense of adjective &amp;quot;great&amp;quot;, the pair &amp;quot;great#6 time&amp;quot; is extracted from the example sentence fragment &amp;quot;had a great time at the party&amp;quot; and automatically disambiguated. The labeled pair is then added to the training set for great#6. Lastly, the priors in this model are determined directly from SemCor&apos;s occurrence statistics and estimated using Maximum Likelihood Estimation (MLE). This is another simplification over the BHD model, where the priors were determined using the hundred most frequent adjective-noun pairs culled from the Internet and then manually classified. It is well known that MLE is inaccurate when the number of events are low, as is in this case when rarer senses often have only single occurrences. Nevertheless, we are able to addreSs both of the manual steps used in the BHD model with automated processes. However, it is our belief that they are also the weakest part of our model and contribute the most to the errors. 4 Querying the Network both the structure the parameters established, the query we pose is to compute the instantiations for each random variable that would result in the highest joint probability, i.e., maxP(Mi is computed easily using the Maximum A Posteriori (MAP) query. This was implemented using the JointTree algorithm (Darwiche, 1995) and can be computed in 0(1cr) time, where jcl is the size of the variable (number of senses), and w is the tree width. Given that our networks are sparsely connected, w is usually close to 3, the average number of parents + 1. The advantage of using the MAP query is that it computes variable instantiations that will maxithe across the whole senrather than the localized context. Further- 65 Table 1: Precision/recall results of the three models to more, the resulting instantiation and probability is guaranteed to be maximum. So given the independence assumptions made on the context and the estimated parameters, MAP will always produce the most probable sense tagging for every word in the sentence. 5 Beyond Sentential Context It is well known that word senses are often influenced by contexts larger than the sentence, such as surrounding sentences or even the whole passage. We experimented with the One-sense-per-discourse (OSPD) hypothesis (Yarowsky, 1993) by applying the probabilities described in Stetina et al. (1998) to words that have previously appeared in the text and thus have been disambiguated. The only modification needed to our model described thus far is to apply OSPD probabilities, which is dependent on the distance between the sentences, to each sense of a re-occurring word before the MAP query. It is our observation that this incarnation of the OSPD hypothesis, chosen for its ease of implementation, tends to propagate erroneous sense tagging from initial sentences to the remainder of the passage. A better approach would be to determine the one sense that would maximize the consensus across the whole passage, as well as within each individual sentence. How this can be achieved efficiently in a probabilistic framework is currently being investigated. 6 Evaluation For SENSEVAL-2, we submitted three models for comparison, which differ by their methods of parameter estimation. Model 2 uses the training data from SemCor and Hierarchical networks to smooth the parameters from Internet search engines. Model 3 incorporates additional training data gathered aufrom the WordNet glosses. model 1 combines all training data, as well as the OSPD hypothesis. One can see that the model that uses all of the available data achieved best accuracy (model 1) but unfortunately also had the lowest recall due to the added complexity. Some highly polysemous words were omitted due to time and memory constraints. Between the 2 training sets, it was unfortunate that the addition of the automatically generated training set reduced the accuracy slightly, mainly due to the noisy data produced by our experimental system. Nevertheless, we believe that there is a wealth of information contained within WordNet&apos;s glosses. Since one of our aims is to use as much automated processing as possible, we are focusing on improving the accuracy of the automatically generated training data. Our goal is that as the WSD accuracy of our system improves, so will the reliably of these automatically generated training data. Having improved training data will further improve the system&apos;s WSD accuracy, i.e., a bootstrapping system. We are at the initial stage of this process, but some fundamental problems such as reliable POS tagging and parsing of sentence fragments need to be addressed first. Furthermore, parameter estimation based on Internet statistics might prove to be too noisy, so we are currently focusing on learning algorithms such as Expectation Maximization to tune the parameters. Lastly, if our context is found to be too limited, additional features can be added to the Bayesian networks to improve the classification accuracy. References Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case in part of speech tagging. Gerald Chao and Michael G. Dyer. 2000. Word sense disambiguation of adjectives using probanetworks. In of the Eighteenth International Conference on Computational Linguistics. Adnan Darwiche. 1995. Conditional algorithms for exact and approximate inference in causal net- In of the Sixth Conference on in Artificial Intelligence, 99- 107. Sadao Kurohashi Jiri Stetina and Makoto Nagao. 1998. General word sense disambiguation method on a full sentential context. In of COLING-ACL Workshop on Usage of Word-</abstract>
<note confidence="0.847255611111111">Net in Natural Language Processing, Montreal, 1-8, July. Rada Mihalcea and Dan Moldovan. 1999. A method for word sense disambiguation of unrestricted In of the 37th Annual Meeting the ACL, 152-158, Maryland, NY, June. G. Miller. 1990. WordNet: An on-line lexical Journal of Lexicography, 3(4). Pearl. 1988. Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA. David Yarowsky. 1993. One sense per collocation. of ARPA Human Language Tech- Princeton, 266-271. Model Precision Recall</note>
<phone confidence="0.652190333333333">1 0.500 2 0.475 3 0.474</phone>
<date confidence="0.462924666666667">0.449 0.454 0.453</date>
<intro confidence="0.416589">66</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--722</pages>
<contexts>
<context position="12009" citStr="Brill, 1995" startWordPosition="1973" endWordPosition="1974">s, up to three hierarchical networks are used for each potential parent. Since the smoothing computation is very efficient, being linear in the depth of the network, these additions did not impact the speed of the model. The majority of the time was used to query the Internet search engine. The BHD model, however, did use additional training data that was collected from the WordNet glosses and manually annotated. While it resulted in good accuracy, this was obviously not an option for SENSEVAL-2. Instead, the example sentences from WordNet are extracted and first tagged by Brill&apos;s POS tagger (Brill, 1995). Then an experimental parser and our WSD system were used to parse and disambiguate the sentences to extract additional training data. For example, for the 6th sense of adjective &amp;quot;great&amp;quot;, the pair &amp;quot;great#6 time&amp;quot; is extracted from the example sentence fragment &amp;quot;had a great time at the party&amp;quot; and automatically disambiguated. The labeled pair is then added to the training set for great#6. Lastly, the priors in this model are determined directly from SemCor&apos;s occurrence statistics and estimated using Maximum Likelihood Estimation (MLE). This is another simplification over the BHD model, where the</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21:722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Chao</author>
<author>Michael G Dyer</author>
</authors>
<title>Word sense disambiguation of adjectives using probabilistic networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="10885" citStr="Chao and Dyer, 2000" startWordPosition="1784" endWordPosition="1787">word &amp;quot;approved&amp;quot;, its CPT P(approved#ilboard#i, acquisition#i) would contain 2 (number of senses for &amp;quot;approved&amp;quot;) x 9 x 4 = 72 entries. For a word without any parents, such as &amp;quot;today&apos;s&apos;&amp;quot;, its priors are used. While determining the network structure is relatively simple, establishing accurate parameters is quite difficult, even with a small context such as ours. Due to the limited size of SemCor, our only labeled training data, we used additional sources to quantify and smooth these parameters. Primarily we deployed the same techniques used in our Bayesian Hierarchical Disambiguator (BHD) model (Chao and Dyer, 2000), which uses Internet search engines to estimate parameters based on permutations of synonym words, a method first introduced by Mihalcea and Moldovan (1999). These parameters are then smoothed by training data obtained from SemCor. The details of BHD are omitted here due to space constraints. Although BHD was only used on adjective-noun pairs, the same principles are used to quantify all of the CPTs in this model. While only one hierarchical network is needed to smooth the parameter for adjective-noun pairs, up to three hierarchical networks are used for each potential parent. Since the smoot</context>
</contexts>
<marker>Chao, Dyer, 2000</marker>
<rawString>Gerald Chao and Michael G. Dyer. 2000. Word sense disambiguation of adjectives using probabilistic networks. In Proceedings of the Eighteenth International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adnan Darwiche</author>
</authors>
<title>Conditional algorithms for exact and approximate inference in causal networks.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>99--107</pages>
<contexts>
<context position="13480" citStr="Darwiche, 1995" startWordPosition="2216" endWordPosition="2217">nly single occurrences. Nevertheless, we are able to addreSs both of the manual steps used in the BHD model with automated processes. However, it is our belief that they are also the weakest part of our model and contribute the most to the errors. 4 Querying the Network With both the structure G and the parameters established, the query we pose is to compute the instantiations for each random variable that would result in the highest joint probability, i.e., arg maxP(Mi IS). This is computed easily using the Maximum A Posteriori (MAP) query. This was implemented using the JointTree algorithm (Darwiche, 1995) and can be computed in 0(1cr) time, where jcl is the size of the variable (number of senses), and w is the tree width. Given that our networks are sparsely connected, w is usually close to 3, the average number of parents + 1. The advantage of using the MAP query is that it computes variable instantiations that will maximize the overall probability across the whole sentence, rather than the localized context. Further65 Table 1: Precision/recall results of the three models submitted to SENSEVAL-2. more, the resulting instantiation and probability is guaranteed to be maximum. So given the indep</context>
</contexts>
<marker>Darwiche, 1995</marker>
<rawString>Adnan Darwiche. 1995. Conditional algorithms for exact and approximate inference in causal networks. In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, pages 99-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>General word sense disambiguation method based on a full sentential context.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Montreal, Canada,</location>
<marker>Stetina, Nagao, 1998</marker>
<rawString>Sadao Kurohashi Jiri Stetina and Makoto Nagao. 1998. General word sense disambiguation method based on a full sentential context. In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing, Montreal, Canada, pages 1-8, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>A method for word sense disambiguation of unrestricted text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>152--158</pages>
<location>Maryland, NY,</location>
<contexts>
<context position="11042" citStr="Mihalcea and Moldovan (1999)" startWordPosition="1808" endWordPosition="1811">out any parents, such as &amp;quot;today&apos;s&apos;&amp;quot;, its priors are used. While determining the network structure is relatively simple, establishing accurate parameters is quite difficult, even with a small context such as ours. Due to the limited size of SemCor, our only labeled training data, we used additional sources to quantify and smooth these parameters. Primarily we deployed the same techniques used in our Bayesian Hierarchical Disambiguator (BHD) model (Chao and Dyer, 2000), which uses Internet search engines to estimate parameters based on permutations of synonym words, a method first introduced by Mihalcea and Moldovan (1999). These parameters are then smoothed by training data obtained from SemCor. The details of BHD are omitted here due to space constraints. Although BHD was only used on adjective-noun pairs, the same principles are used to quantify all of the CPTs in this model. While only one hierarchical network is needed to smooth the parameter for adjective-noun pairs, up to three hierarchical networks are used for each potential parent. Since the smoothing computation is very efficient, being linear in the depth of the network, these additions did not impact the speed of the model. The majority of the time</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 1999. A method for word sense disambiguation of unrestricted text. In Proceedings of the 37th Annual Meeting of the ACL, pages 152-158, Maryland, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1447" citStr="Miller, 1990" startWordPosition="218" endWordPosition="219">y, the One-sense-per-discourse (OSPD) hypothesis is incorporated to test its effectiveness. The results from two parameterization techniques and the effects of the OSPD hypothesis are presented. 1 Problem Formulation WSD is treated in this system as a classification task, where the ith sense (W#i) of a word (W) is classified as the correct sense tag (M,), given the word W and usually some surrounding context. In the SENSEVAL-2 English all-words task, all ambiguous content words (nouns, verbs, adjectives, and adverbs) are to be classified with a sense tag from the WordNet 1.7 lexical database (Miller, 1990). For example, the words &amp;quot;great&amp;quot;, &amp;quot;devastated&amp;quot;, and &amp;quot;region&amp;quot; in the sentence &amp;quot;The great hurricane devastated the region&amp;quot; are classified with the correct sense tags 2, 2, and 2, respectively. We will refer to this task using the following notation: = mbee(s) = erg rnaxP(MIS), (1) where S is the input sentence, and M is the semantic tag assigned to each word. While a context larger than the sentence S can be and is used in our model, we will refer to the context as S. In this formulation, each word Wi in the sentence is treated as a random variable M, taking on the values 11..Nil, where Ni is th</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="5546" citStr="Pearl, 1988" startWordPosition="877" endWordPosition="878">hile computing Mbest (S) in one query to allow for interactions between the word senses M. 1.1 Bayesian Networks This process is achieved by using Bayesian networks to model the dependencies between each word and its contextual words, and based on the parameterization, compute the best overall sense assignments. A Bayesian network is a directed acyclic graph G that represents a joint probability distribution P(Xi ...,Xn) across the random variables of each node in the graph. By making independence assumptions between variables, each node i is conditionally dependent upon only its parents PA, (Pearl, 1988): P(Xl, X,) = ILP(Xi1PAi). By using this representation, the number of probabilities needed to represent the distribution can be significantly reduced. Figure 1 shows an example Bayesian network representing the distribution P(A,B,C,D,E,F). Instead of having one large table with 26 parameters (with all Boolean nodes), the distribution is represented by the conditional probability tables (CPTs) at each node, such as P(B I D, F) at node 13, requiring a total of only 24 parameters for the whole distribution. Not only do the savings become more significant with larger networks, but the sparse data</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of ARPA Human Language Technology,</booktitle>
<pages>266--271</pages>
<location>Princeton,</location>
<contexts>
<context position="14499" citStr="Yarowsky, 1993" startWordPosition="2384" endWordPosition="2385"> context. Further65 Table 1: Precision/recall results of the three models submitted to SENSEVAL-2. more, the resulting instantiation and probability is guaranteed to be maximum. So given the independence assumptions made on the context and the estimated parameters, MAP will always produce the most probable sense tagging for every word in the sentence. 5 Beyond Sentential Context It is well known that word senses are often influenced by contexts larger than the sentence, such as surrounding sentences or even the whole passage. We experimented with the One-sense-per-discourse (OSPD) hypothesis (Yarowsky, 1993) by applying the probabilities described in Stetina et al. (1998) to words that have previously appeared in the text and thus have been disambiguated. The only modification needed to our model described thus far is to apply OSPD probabilities, which is dependent on the distance between the sentences, to each sense of a re-occurring word before the MAP query. It is our observation that this incarnation of the OSPD hypothesis, chosen for its ease of implementation, tends to propagate erroneous sense tagging from initial sentences to the remainder of the passage. A better approach would be to det</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proceedings of ARPA Human Language Technology, Princeton, pages 266-271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>