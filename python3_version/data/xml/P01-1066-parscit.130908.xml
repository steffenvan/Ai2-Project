<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001619">
<title confidence="0.997037">
Quantitative and Qualitative Evaluation of Darpa Communicator
Spoken Dialogue Systems
</title>
<author confidence="0.700356">
Marilyn A. Walker
</author>
<affiliation confidence="0.573616">
AT&amp;T Labs – Research
</affiliation>
<address confidence="0.934023">
180 Park Ave, E103
Florham Park, NJ. 07932
</address>
<email confidence="0.996952">
walker@research.att.com
</email>
<author confidence="0.91916">
Rebecca Passonneau
</author>
<affiliation confidence="0.900498">
AT&amp;T Labs –Research
</affiliation>
<address confidence="0.9648795">
180 Park Ave, D191
Florham Park, NJ. 07932
</address>
<email confidence="0.99802">
becky@research.att.com
</email>
<author confidence="0.975525">
Julie E. Boland
</author>
<affiliation confidence="0.9974415">
Institute of Cognitive Science
University of Louisiana at Lafayette
</affiliation>
<address confidence="0.950285">
Lafayette, LA 70504
</address>
<email confidence="0.999331">
boland@louisiana.edu
</email>
<sectionHeader confidence="0.993911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999656">
This paper describes the application of
the PARADISE evaluation framework
to the corpus of 662 human-computer
dialogues collected in the June 2000
Darpa Communicator data collection.
We describe results based on the stan-
dard logfile metrics as well as results
based on additional qualitative metrics
derived using the DATE dialogue act
tagging scheme. We show that per-
formance models derived via using the
standard metrics can account for 37%
of the variance in user satisfaction, and
that the addition of DATE metrics im-
proved the models by an absolute 5%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983936708861">
The objective of the DARPA COMMUNICATOR
program is to support research on multi-modal
speech-enabled dialogue systems with advanced
conversational capabilities. In order to make this
a reality, it is important to understand the con-
tribution of various techniques to users’ willing-
ness and ability to use a spoken dialogue system.
In June of 2000, we conducted an exploratory
data collection experiment with nine participating
communicator systems. All systems supported
travel planning and utilized some form of mixed-
initiative interaction. However the systems var-
ied in several critical dimensions: (1) They tar-
geted different back-end databases for travel in-
formation; (2) System modules such as ASR,
NLU, TTS and dialogue management were typ-
ically different across systems.
The Evaluation Committee chaired by Walker
(Walker, 2000), with representatives from the
nine COMMUNICATOR sites and from NIST, de-
veloped the experimental design. A logfile stan-
dard was developed by MITRE along with a set
of tools for processing the logfiles (Aberdeen,
2000); the standard and tools were used by all
sites to collect a set of core metrics for making
cross system comparisons. The core metrics were
developed during a workshop of the Evaluation
Committee and included all metrics that anyone
in the committee suggested, that could be imple-
mented consistently across systems. NIST’s con-
tribution was to recruit the human subjects and to
implement the experimental design specified by
the Evaluation Committee.
The experiment was designed to make it possi-
ble to apply the PARADISE evaluation framework
(Walker et al., 2000), which integrates and unifies
previous approaches to evaluation (Price et al.,
1992; Hirschman, 2000). The framework posits
that user satisfaction is the overall objective to be
maximized and that task success and various in-
teraction costs can be used as predictors of user
satisfaction. Our results from applying PARADISE
include that user satisfaction differed consider-
ably across the nine systems. Subsequent model-
ing of user satisfaction gave us some insight into
why each system was more or less satisfactory;
four variables accounted for 37% of the variance
in user-satisfaction: task completion, task dura-
tion, recognition accuracy, and mean system turn
duration.
However, when doing our analysis we were
struck by the extent to which different aspects of
the systems’ dialogue behavior weren’t captured
by the core metrics. For example, the core met-
rics logged the number and duration of system
turns, but didn’t distinguish between turns used
to request or present information, to give instruc-
tions, or to indicate errors. Recent research on
dialogue has been based on the assumption that
dialogue acts provide a useful way of character-
izing dialogue behaviors (Reithinger and Maier,
1995; Isard and Carletta, 1995; Shriberg et al.,
2000; Di Eugenio et al., 1998). Several research
efforts have explored the use of dialogue act tag-
ging schemes for tasks such as improving recog-
nition performance (Reithinger and Maier, 1995;
Shriberg et al., 2000), identifying important parts
of a dialogue (Finke et al., 1998), and as a con-
straint on nominal expression generation (Jordan,
2000). Thus we decided to explore the applica-
tion of a dialogue act tagging scheme to the task
of evaluating and comparing dialogue systems.
Section 2 describes the corpus. Section 3 de-
scribes the dialogue act tagging scheme we de-
veloped and applied to the evaluation of COM-
MUNICATOR dialogues. Section 4 first describes
our results utilizing the standard logged metrics,
and then describes results using the DATE met-
rics. Section 5 discusses future plans.
</bodyText>
<sectionHeader confidence="0.793989" genericHeader="method">
2 The Communicator 2000 Corpus
</sectionHeader>
<bodyText confidence="0.999615666666667">
The corpus consists of 662 dialogues from nine
different travel planning systems with the num-
ber of dialogues per system ranging between 60
and 79. The experimental design is described
in (Walker et al., 2001). Each dialogue consists
of a recording, a logfile consistent with the stan-
dard, transcriptions and recordings of all user ut-
terances, and the output of a web-based user sur-
vey. Metrics collected per call included:
</bodyText>
<table confidence="0.9213366">
Dialogue Efficiency: Task Duration, System turns,
User turns, Total Turns
Dialogue Quality: Word Accuracy, Response latency,
Response latency variance
Task Success: Exact Scenario Completion
</table>
<tableCaption confidence="0.448222">
User Satisfaction: Sum of TTS performance, Task
ease, User expertise, Expected behavior, Future use.
</tableCaption>
<bodyText confidence="0.998578384615384">
The objective metrics focus on measures that
can be automatically logged or computed and a
web survey was used to calculate User Satisfac-
tion (Walker et al., 2001). A ternary definition
of task completion, Exact Scenario Completion
(ESC) was annotated by hand for each call by an-
notators at AT&amp;T. The ESC metric distinguishes
between exact scenario completion (ESC), any
scenario completion (ANY) and no scenario com-
pletion (NOCOMP). This metric arose because
some callers completed an itinerary other than
the one assigned. This could have been due to
users’ inattentiveness, e.g. users didn’t correct the
system when it had misunderstood them. In this
case, the system could be viewed as having done
the best that it could with the information that it
was given. This would argue that task completion
would be the sum of ESC and ANY. However,
examination of the dialogue transcripts suggested
that the ANY category sometimes arose as a ratio-
nal reaction by the caller to repeated recognition
error. Thus we decided to distinguish the cases
where the user completed the assigned task, ver-
sus completing some other task, versus the cases
where they hung up the phone without completing
any itinerary.
</bodyText>
<sectionHeader confidence="0.983247" genericHeader="method">
3 Dialogue Act Tagging for Evaluation
</sectionHeader>
<bodyText confidence="0.9999718125">
The hypothesis underlying the application of di-
alogue act tagging to system evaluation is that
a system’s dialogue behaviors have a strong ef-
fect on the usability of a spoken dialogue sys-
tem. However, each COMMUNICATOR system has
a unique dialogue strategy and a unique way of
achieving particular communicative goals. Thus,
in order to explore this hypothesis, we needed a
way of characterizing system dialogue behaviors
that could be applied uniformly across the nine
different communicator travel planning systems.
We developed a dialogue act tagging scheme for
this purpose which we call DATE (Dialogue Act
Tagging for Evaluation).
In developing DATE, we believed that it was
important to allow for multiple views of each
dialogue act. This would allow us, for ex-
ample, to investigate what part of the task an
utterance contributes to separately from what
speech act function it serves. Thus, a cen-
tral aspect of DATE is that it makes distinc-
tions within three orthogonal dimensions of ut-
terance classification: (1) a SPEECH-ACT dimen-
sion; (2) a TASK-SUBTASK dimension; and (3) a
CONVERSATIONAL-DOMAIN dimension. We be-
lieve that these distinctions are important for us-
ing such a scheme for evaluation. Figure 1 shows
a COMMUNICATOR dialogue with each system ut-
terance classified on these three dimensions. The
tagset for each dimension are briefly described in
the remainder of this section. See (Walker and
Passonneau, 2001) for more detail.
</bodyText>
<subsectionHeader confidence="0.999415">
3.1 Speech Acts
</subsectionHeader>
<bodyText confidence="0.9896817">
In DATE, the SPEECH-ACT dimension hasten cat-
egories. We use familiar speech-act labels, such
as OFFER, REQUEST-INFO, PRESENT-INFO, AC-
KNOWLEDGE, and introduce new ones designed
to help us capture generalizations about commu-
nicative behavior in this domain, on this task,
given the range of system and human behavior
we see in the data. One new one, for example,
is STATUS-REPORT. Examples of each speech-act
type are in Figure 2.
</bodyText>
<table confidence="0.979905294117647">
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGE I will book this leg.
STATUS-REPORT Accessing the database; this
might take a few seconds.
EXPLICIT- You will depart on September 1st.
CONFIRM Is that correct?
IMPLICIT- Leaving from Dallas.
CONFIRM
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn’t understand that.
OPENING/CLOSING Hello. Welcome to the C M U
Communicator.
</table>
<figureCaption confidence="0.963606">
Figure 2: Example Speech Acts
</figureCaption>
<subsectionHeader confidence="0.999638">
3.2 Conversational Domains
</subsectionHeader>
<bodyText confidence="0.99994815625">
The CONVERSATIONAL-DOMAIN dimension in-
volves the domain of discourse that an utterance
is about. Each speech act can occur in any of three
domains of discourse described below.
The ABOUT-TASK domain is necessary for
evaluating a dialogue system’s ability to collab-
orate with a speaker on achieving the task goal of
making reservations for a specific trip. It supports
metrics such as the amount of time/effort the sys-
tem takes to complete a particular phase of mak-
ing an airline reservation, and any ancillary ho-
tel/car reservations.
The ABOUT-COMMUNICATION domain re-
flects the system goal of managing the verbal
channel and providing evidence of what has been
understood (Walker, 1992; Clark and Schaefer,
1989). Utterances of this type are frequent in
human-computer dialogue, where they are moti-
vated by the need to avoid potentially costly er-
rors arising from imperfect speech recognition.
All implicit and explicit confirmations are about
communication; See Figure 1 for examples.
The SITUATION-FRAME domain pertains to the
goal of managing the culturally relevant framing
expectations (Goffman, 1974). The utterances in
this domain are particularly relevant in human-
computer dialogues because the users’ expecta-
tions need to be defined during the course of the
conversation. About frame utterances by the sys-
tem attempt to help the user understand how to in-
teract with the system, what it knows about, and
what it can do. Some examples are in Figure 1.
</bodyText>
<subsectionHeader confidence="0.986943">
3.3 Task Model
</subsectionHeader>
<bodyText confidence="0.999777416666667">
The TASK-SUBTASK dimension refers to a task
model of the domain task that the system sup-
ports and captures distinctions among dialogue
acts that reflect the task structure.1 The motiva-
tion for this dimension is to derive metrics that
quantify the effort expended on particular sub-
tasks.
This dimension distinguishes among 14 sub-
tasks, some of which can also be grouped at
a level below the top level task.2, as described
in Figure 3. The TOP-LEVEL-TRIP task de-
scribes the task which contains as its subtasks the
ORIGIN, DESTINATION, DATE, TIME, AIRLINE,
TRIP-TYPE, RETRIEVAL and ITINERARY tasks.
The GROUND task includes both the HOTEL and
CAR subtasks.
Note that any subtask can involve multiple
speech acts. For example, the DATE subtask can
consist of acts requesting, or implicitly or explic-
itly confirming the date. A similar example is pro-
vided by the subtasks of CAR (rental) and HOTEL,
which include dialogue acts requesting, confirm-
ing or acknowledging arrangements to rent a car
or book a hotel room on the same trip.
</bodyText>
<footnote confidence="0.983044">
1This dimension elaborates of each speech-act type in
other tagging schemes (Reithinger and Maier, 1995).
2In (Walker and Passonneau, 2001) we didn’t distinguish
the price subtask from the itinerary presentation subtask.
</footnote>
<table confidence="0.998944578947369">
Task Example
TOP-LEVEL- What are your travel plans?
TRIP
ORIGIN And, what city are you leavingfrom?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
TRIP-TYPE Will you return to Boston from San Jose?
RETRIEVAL Accessing the database; this might take
a few seconds.
ITINERARY I found 3 flights from Miami to Min-
neapolis.
PRICE The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground ar-
rangements?.
HOTEL Would you like a hotel near downtown
or near the airport?.
CAR Do you need a car in San Jose?
</table>
<figureCaption confidence="0.950207">
Figure 3: Example Utterances for each Subtask
</figureCaption>
<subsectionHeader confidence="0.924511">
3.4 Implementation and Metrics Derivation
</subsectionHeader>
<bodyText confidence="0.999700688311689">
We implemented a dialogue act parser that clas-
sifies each of the system utterances in each dia-
logue in the COMMUNICATOR corpus. Because
the systems used template-based generation and
had only a limited number of ways of saying the
same content, it was possible to achieve 100% ac-
curacy with a parser that tags utterances automat-
ically from a database of patterns and the corre-
sponding relevant tags from each dimension.
A summarizer program then examined each di-
alogue’s labels and summed the total effort ex-
pended on each type of dialogue act over the
dialogue or the percentage of a dialogue given
over to a particular type of dialogue behavior.
These sums and percentages of effort were calcu-
lated along the different dimensions of the tagging
scheme as we explain in more detail below.
We believed that the top level distinction be-
tween different domains of action might be rel-
evant so we calculated percentages of the to-
tal dialogue expended in each conversational do-
main, resulting in metrics of TaskP, FrameP and
CommP (the percentage of the dialogue devoted
to the task, the frame or the communication do-
mains respectively).
We were also interested in identifying differ-
ences in effort expended on different subtasks.
The effort expended on each subtask is repre-
sented by the sum of the length of the utterances
contributing to that subtask. These are the met-
rics: TripC, OrigC, DestC, DateC, TimeC, Air-
lineC, RetrievalC, FlightinfoC, PriceC, GroundC,
BookingC. See Figure 3.
We were particularly interested developing
metrics related to differences in the system’s di-
alogue strategies. One difference that the DATE
scheme can partially capture is differences in con-
firmation strategy by summing the explicit and
implicit confirms. This introduces two metrics
ECon and ICon, which represent the total effort
spent on these two types of confirmation.
Another strategy difference is in the types of
about frame information that the systems pro-
vide. The metric CINSTRUCT counts instances
of instructions, CREQAMB counts descriptions
provided of what the system knows about in the
context of an ambiguity, and CNOINFO counts
the system’s descriptions of what it doesn’t know
about. SITINFO counts dialogue initial descrip-
tions of the system’s capabilities and instructions
for how to interact with the system
A final type of dialogue behavior that the
scheme captures are apologies for misunderstand-
ing (CREJECT), acknowledgements of user re-
quests to start over (SOVER) and acknowledg-
ments of user corrections of the system’s under-
standing (ACOR).
We believe that it should be possible to use
DATE to capture differences in initiative strate-
gies, but currently only capture differences at the
task level using the task metrics above. The TripC
metric counts open ended questions about the
user’s travel plans, whereas other subtasks typi-
cally include very direct requests for information
needed to complete a subtask.
We also counted triples identifying dialogue
acts used in specific situations, e.g. the utterance
Great! I am adding this flight to your itinerary
is the speech act of acknowledge, in the about-
task domain, contributing to the booking subtask.
This combination is the ACKBOOKING metric.
We also keep track of metrics for dialogue acts
of acknowledging a rental car booking or a hotel
booking, and requesting, presenting or confirm-
ing particular items of task information. Below
we describe dialogue act triples that are signifi-
cant predictors of user satisfaction.
</bodyText>
<table confidence="0.9996234">
Metric Coefficient P value
ESC 0.45 0.000
TaskDur -0.15 0.000
Sys Turn Dur 0.12 0.000
Wrd Acc 0.17 0.000
</table>
<tableCaption confidence="0.9819405">
Table 1: Predictive power and significance of
Core Metrics
</tableCaption>
<sectionHeader confidence="0.999623" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999748916666667">
We initially examined differences in cumulative
user satisfaction across the nine systems. An
ANOVA for user satisfaction by Site ID using the
modified Bonferroni statistic for multiple com-
parisons showed that there were statistically sig-
nificant differences across sites, and that there
were four groups of performers with sites 3,2,1,4
in the top group (listed by average user satisfac-
tion), sites 4,5,9,6 in a second group, and sites 8
and 7 defining a third and a fourth group. See
(Walker et al., 2001) for more detail on cross-
system comparisons.
However, our primary goal was to achieve a
better understanding of the role of qualitative as-
pects of each system’s dialogue behavior. We
quantify the extent to which the dialogue act
metrics improve our understanding by applying
the PARADISE framework to develop a model of
user satisfaction and then examining the extent
to which the dialogue act metrics improve the
model (Walker et al., 2000). Section 4.1 describes
the PARADISE models developed using the core
metrics and section 4.2 describes the models de-
rived from adding in the DATE metrics.
</bodyText>
<subsectionHeader confidence="0.948822">
4.1 Results using Logfile Standard Metrics
</subsectionHeader>
<bodyText confidence="0.999344">
We applied PARADISE to develop models of user
satisfaction using the core metrics; the best model
fit accounts for 37% of the variance in user sat-
isfaction. The learned model is that User Sat-
isfaction is the sum of Exact Scenario Comple-
tion, Task Duration, System Turn Duration and
Word Accuracy. Table 1 gives the details of the
model, where the coefficient indicates both the
magnitude and whether the metric is a positive or
negative predictor of user satisfaction, and the P
value indicates the significance of the metric in
the model.
The finding that metrics of task completion and
</bodyText>
<table confidence="0.999879461538462">
Metric Coefficient P value
ESC (Completion) 0.40 0.00
Task Dur -0.31 0.00
Sys Turn Dur 0.14 0.00
Word Accuracy 0.15 0.00
TripC 0.09 0.01
BookingC 0.08 0.03
PriceC 0.11 0.00
AckRent 0.07 0.05
EconTime 0.05 0.13
ReqDate 0.10 0.01
ReqTripType 0.09 0.00
Econ 0.11 0.01
</table>
<tableCaption confidence="0.983748">
Table 2: Predictive power and significance of Di-
alogue Act Metrics
</tableCaption>
<bodyText confidence="0.998378266666667">
recognition performance are significant predic-
tors duplicates results from other experiments ap-
plying PARADISE (Walker et al., 2000). The fact
that task duration is also a significant predictor
may indicate larger differences in task duration in
this corpus than in previous studies.
Note that the PARADISE model indicates that
system turn duration is positively correlated with
user satisfaction. We believed it plausible that this
was due to the fact that flight presentation utter-
ances are longer than other system turns. Thus
this metric simply captures whether or not the sys-
tem got enough information to present some po-
tential flight itineraries to the user. We investigate
this hypothesis further below.
</bodyText>
<subsectionHeader confidence="0.999121">
4.2 Utilizing Dialogue Parser Metrics
</subsectionHeader>
<bodyText confidence="0.99998587628866">
Next, we add in the dialogue act metrics extracted
by our dialogue parser, and retrain our models of
user satisfaction. We find that many of the dia-
logue act metrics are significant predictors of user
satisfaction, and that the model fit for user sat-
isfaction increases from 37% to 42%. The dia-
logue act metrics which are significant predictors
of user satisfaction are detailed in Table 2.
When we examine this model, we note that sev-
eral of the significant dialogue act metrics are cal-
culated along the task-subtask dimension, namely
TripC, BookingC and PriceC. One interpretation
of these metrics are that they are acting as land-
marks in the dialogue for having achieved a par-
ticular set of subtasks. The TripC metric can
be interpreted this way because it includes open
ended questions about the user’s travel plans both
at the beginning of the dialogue and also after
one itinerary has been planned. Other signif-
icant metrics can also be interpreted this way;
for example the ReqDate metric counts utterances
such as Could you tell me what date you wanna
travel? which are typically only produced after
the origin and the destination have been under-
stood. The ReqTripType metric counts utterances
such as From Boston, are you returning to Dal-
las? which are only asked after all the first infor-
mation for the first leg of the trip have been ac-
quired, and in some cases, after this information
has been confirmed. The AckRental metric has a
similar potential interpretation; the car rental task
isn’t attempted until after the flight itinerary has
been accepted by the caller. However, the predic-
tors for the models already include a ternary exact
scenario completion metric (ESC) which speci-
fies whether any task was achieved or not, and
whether the exact task that the user was attempt-
ing to accomplish was achieved. The fact that the
addition of these dialogue metrics improves the fit
of the user satisfaction model suggests that per-
haps a finer grained distinction on how many of
the subtasks of a dialogue were completed is re-
lated to user satisfaction. This makes sense; a user
who the system hung up on immediately should
be less satisfied than one who never could get the
system to understand his destination, and both of
these should be less satisfied than a user who was
able to communicate a complete travel plan but
still did not complete the task.
Other support for the task completion related
nature of some of the significant metrics is that
the coefficient for ESC is smaller in the model
in Table 2 than in the model in Table 1. Note
also that the coefficient for Task Duration is much
larger. If some of the dialogue act metrics that are
significant predictors are mainly so because they
indicate the successful accomplishment of partic-
ular subtasks, then both of these changes would
make sense. Task Duration can be a greater nega-
tive predictor of user satisfaction, only when it is
counteracted by the positive coefficients for sub-
task completion.
The TripC and the PriceC metrics also have
other interpretations. The positive contribution of
the TripC metric to user satisfaction could arise
from a user’s positive response to systems with
open-ended initial greetings which give the user
the initiative. The positive contribution of the
PriceC metric might indicate the users’ positive
response to getting price information, since not
all systems provided price information.
As mentioned above, our goal was to de-
velop metrics that captured differences in dia-
logue strategies. The positive coefficient of the
Econ metric appears to indicate that an explicit
confirmation strategy overall leads to greater user
satisfaction than an implicit confirmation strategy.
This result is interesting, although it is unclear
how general it is. The systems that used an ex-
plicit confirmation strategy did not use it to con-
firm each item of information; rather the strategy
seemed to be to acquire enough information to go
to the database and then confirm all of the param-
eters before accessing the database. The other use
of explicit confirms was when a system believed
that it had repeatedly misunderstood the user.
We also explored the hypothesis that the rea-
son that system turn duration was a predictor of
user satisfaction is that longer turns were used
to present flight information. We removed sys-
tem turn duration from the model, to determine
whether FlightInfoC would become a significant
predictor. However the model fit decreased and
FlightInfoC was not a significant predictor. Thus
it is unclear to us why longer system turn dura-
tions are a significant positive predictor of user
satisfaction.
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999991741935484">
We showed above that the addition of dialogue act
metrics improves the fit of models of user satis-
faction from 37% to 42%. Many of the significant
dialogue act metrics can be viewed as landmarks
in the dialogue for having achieved particular sub-
tasks. These results suggest that a careful defi-
nition of transaction success, based on automatic
analysis of events in a dialogue, such as acknowl-
edging a booking, might serve as a substitute for
the hand-labelling of task completion.
In current work we are exploring the use of tree
models and boosting for modeling user satisfac-
tion. Tree models using dialogue act metrics can
achieve model fits as high as 48% reduction in
error. However, we need to test both these mod-
els and the linear PARADISE models on unseen
data. Furthermore, we intend to explore methods
for deriving additional metrics from dialogue act
tags. In particular, it is possible that sequential or
structural metrics based on particular sequences
or configurations of dialogue acts might capture
differences in dialogue strategies.
We began a second data collection of dialogues
with COMMUNICATOR travel systems in April
2001. In this data collection, the subject pool will
use the systems to plan real trips that they intend
to take. As part of this data collection, we hope
to develop additional metrics related to the qual-
ity of the dialogue, how much initiative the user
can take, and the quality of the solution that the
system presents to the user.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999491">
This work was supported under DARPA GRANT
MDA 972 99 3 0003 to AT&amp;T Labs Research.
Thanks to the evaluation committee members:
J. Aberdeen, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Narayanan, K. Papineni, B. Pellom,
A. Potamianos, A. Rudnicky, G. Sanders, S. Sen-
eff, and D. Stallard who contributed to 2000
COMMUNICATOR data collection.
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996542260869565">
John Aberdeen. 2000. Darpa communicator logfile
standard. http://fofoca.mitre.org/logstandard.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259–
294.
Barbara Di Eugenio, Pamela W. Jordan, Johanna D.
Moore, and Richmond H. Thomason. 1998. An
empirical investigation of collaborative dialogues.
In ACL-COLING98, Proc. of the 36th Conference
of the Association for Computational Linguistics.
M. Finke, M. Lapata, A. Lavie, L. Levin, L. May-
field Tomokiyo, T. Polzin, K. Ries, A. Waibel, and
K. Zechner. 1998. Clarity: Inferring discourse
structure from speech. In AAAI Symposium on
Applying Machine Learning to Discourse Process-
ing.
Erving Goffman. 1974. Frame Analysis: An Essay on
the Organization of Experience. Harper and Row,
New York.
Lynette Hirschman. 2000. Evaluating spoken lan-
guage interaction: Experiences from the darpa spo-
ken language program 1990–1995. In S. Luperfoy,
editor, Spoken Language Discourse. MIT Press,
Cambridge, Mass.
Amy Isard and Jean C. Carletta. 1995. Replicabil-
ity of transaction and action coding in the map task
corpus. In AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Genera-
tion, pages 60–67.
Pamela W. Jordan. 2000. Intentional Influences on
Object Redescriptions in Dialogue: Evidence from
an Empirical Study. Ph.D. thesis, Intelligent Sys-
tems Program, University of Pittsburgh.
Patti Price, Lynette Hirschman, Elizabeth Shriberg,
and Elizabeth Wade. 1992. Subject-based evalu-
ation measures for interactive spoken language sys-
tems. In Proc. of the DARPA Speech and NL Work-
shop, pages 34–39.
Norbert Reithinger and Elisabeth Maier. 1995. Utiliz-
ing statistical speech act processing in verbmobil.
In ACL 95.
E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and
C. Van Ess-Dykema. 2000. Can prosody aid the
automatic classification of dialog acts in conversa-
tional speech. Language and Speech: Special Issue
on Prosody and Conversation.
M. Walker and R. Passonneau. 2001. Date: A dia-
logue act tagging scheme for evaluation. In Human
Language Technology Conference.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general models
of usability with PARADISE. Natural Language
Engineering: Special Issue on Best Practice in Spo-
ken Dialogue Systems.
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayanan,
K. Papineni, B. Pellom, J. Polifroni, A. Potami-
anos, P. Prabhu, A. Rudnicky, G. Sanders, S. Sen-
eff, D. Stallard, and S. Whittaker. 2001. Darpa
communicator dialog travel planning systems: The
june 2000 data collection. In Submitted to EU-
ROSPEECH 2001.
Marilyn A. Walker. 1992. Redundancy in collabora-
tive dialogue. In Fourteenth International Confer-
ence on Computational Linguistics, pages 345–351.
Marilyn Walker. 2000. Communi-
cator evaluation committee webpage.
http://www.research.att.com/ walker/eval/eval.html.
</reference>
<table confidence="0.934281173913043">
Speaker Utterance Speech Act Task C-Domain
SYS Welcome. opening/closing nil task
You are logged in as a guest user. instruct nil frame
You may say repeat, help me out, start over, or, that’s wrong, instruct nil frame
you can also correct and interrupt the system at any time. instruct nil frame
What airport woodja like to fly out of? request-info origin task
USER MIAMI FLORIDA
SYS Leaving from Miami, implicit-conf origin communication
And, what city are you flying to? request-info dest task
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, implicit-conf origin, dest communication
What date would you like to fly? request-info date task
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, implicit-conf date communication
And what time didja wanna leave? request-info time task
USER UH LATE MORNING
SYS From Minneapolis. implicit-conf origin communication
Are you returning to Miami? request-info trip-type task
USER UM
SYS Are you going back to Miami? request-info trip-type task
Please say yes or no. instruct nil frame
USER NO
SYS Okay, it will just take a moment while I look. status-report retrieval task
</table>
<bodyText confidence="0.8253441">
SYS I found 3 flights from Miami to Minneapolis on October 23rd . present-info itinerary task
For option one I have a flight with United departing at 10 A M present-info price task
and arriving at 3 P M with one stopover and offer itinerary task
Ticket price is 390 dollars.
Please say next option, or, flight details, or, I’ll take it.
USER NEXT OPTION
SYS For option two I have a flight with Delta departing at 12 and present-info itinerary task
arriving at 4 P M non-stop. present-info price task
Ticket price is 450 dollars. offer itinerary task
Would you like me to hold this option?
USER NEXT OPTION
SYS For the last option I have a flight with Northwest departing at 2 present-info itinerary task
P M and arriving at 7 P M with 1 stopover and present-info price task
Ticket price is 470 dollars. offer itinerary task
say next option, or, flight details, or, I’ll take it.
USER NEXT OPTION
SYS Here is your third option again. a flight with Northwest departing present-info itinerary task
at 2 P M and arriving at 7 P M with 1 stopover and present-info price task
Ticket price is 470 dollars. offer itinerary task
say next option, or, flight details, or, I’ll take it.
</bodyText>
<table confidence="0.842669833333333">
USER I’LL TAKE IT
SYS Great! I am adding this flight to your itinerary. acknowledge booking task
Okay, Is Minneapolis your final destination? request-info trip-type task
USER YES IT IS
SYS Would you like to make any ground arrangements? offer ground task
USER NO
SYS Now, would you like to hear a summary of your itinerary? offer itinerary task
USER YES PLEASE
SYS Okay, Here’s your itinerary. On October 23rd Northwest Flight present-info itinerary task
123 departs Miami at 2 P M, arrives Pittsburgh at 4 30 P M con-
necting to Northwest Flight 146 that departs Pittsburgh at 5 P M
and arrives Minneapolis at 7 P M.
</table>
<figureCaption confidence="0.9845605">
Figure 1: Dialogue Illustrating the Speech Act, Task-Subtask and Conversational Domain Dimensions
of DATE
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803811">
<title confidence="0.9995725">Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems</title>
<author confidence="0.999987">Marilyn A Walker</author>
<affiliation confidence="0.998547">AT&amp;T Labs – Research</affiliation>
<address confidence="0.9934705">180 Park Ave, E103 Florham Park, NJ. 07932</address>
<email confidence="0.999732">walker@research.att.com</email>
<author confidence="0.998838">Rebecca Passonneau</author>
<affiliation confidence="0.992352">AT&amp;T Labs –Research</affiliation>
<address confidence="0.993258">180 Park Ave, D191 Florham Park, NJ. 07932</address>
<email confidence="0.99981">becky@research.att.com</email>
<author confidence="0.999691">Julie E Boland</author>
<affiliation confidence="0.9999595">Institute of Cognitive Science University of Louisiana at Lafayette</affiliation>
<address confidence="0.999676">Lafayette, LA 70504</address>
<email confidence="0.999719">boland@louisiana.edu</email>
<abstract confidence="0.9889886875">This paper describes the application of the PARADISE evaluation framework to the corpus of 662 human-computer dialogues collected in the June 2000 Darpa Communicator data collection. We describe results based on the standard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme. We show that performance models derived via using the standard metrics can account for 37% of the variance in user satisfaction, and that the addition of DATE metrics improved the models by an absolute 5%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Aberdeen</author>
</authors>
<date>2000</date>
<note>Darpa communicator logfile standard. http://fofoca.mitre.org/logstandard.</note>
<contexts>
<context position="2056" citStr="Aberdeen, 2000" startWordPosition="306" endWordPosition="307">r systems. All systems supported travel planning and utilized some form of mixedinitiative interaction. However the systems varied in several critical dimensions: (1) They targeted different back-end databases for travel information; (2) System modules such as ASR, NLU, TTS and dialogue management were typically different across systems. The Evaluation Committee chaired by Walker (Walker, 2000), with representatives from the nine COMMUNICATOR sites and from NIST, developed the experimental design. A logfile standard was developed by MITRE along with a set of tools for processing the logfiles (Aberdeen, 2000); the standard and tools were used by all sites to collect a set of core metrics for making cross system comparisons. The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be implemented consistently across systems. NIST’s contribution was to recruit the human subjects and to implement the experimental design specified by the Evaluation Committee. The experiment was designed to make it possible to apply the PARADISE evaluation framework (Walker et al., 2000), which integrates and unifies previou</context>
</contexts>
<marker>Aberdeen, 2000</marker>
<rawString>John Aberdeen. 2000. Darpa communicator logfile standard. http://fofoca.mitre.org/logstandard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Edward F Schaefer</author>
</authors>
<title>Contributing to discourse.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<volume>13</volume>
<pages>294</pages>
<contexts>
<context position="9824" citStr="Clark and Schaefer, 1989" startWordPosition="1542" endWordPosition="1545">is about. Each speech act can occur in any of three domains of discourse described below. The ABOUT-TASK domain is necessary for evaluating a dialogue system’s ability to collaborate with a speaker on achieving the task goal of making reservations for a specific trip. It supports metrics such as the amount of time/effort the system takes to complete a particular phase of making an airline reservation, and any ancillary hotel/car reservations. The ABOUT-COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood (Walker, 1992; Clark and Schaefer, 1989). Utterances of this type are frequent in human-computer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. All implicit and explicit confirmations are about communication; See Figure 1 for examples. The SITUATION-FRAME domain pertains to the goal of managing the culturally relevant framing expectations (Goffman, 1974). The utterances in this domain are particularly relevant in humancomputer dialogues because the users’ expectations need to be defined during the course of the conversation. About frame utterances by the s</context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Herbert H. Clark and Edward F. Schaefer. 1989. Contributing to discourse. Cognitive Science, 13:259– 294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Pamela W Jordan</author>
<author>Johanna D Moore</author>
<author>Richmond H Thomason</author>
</authors>
<title>An empirical investigation of collaborative dialogues.</title>
<date>1998</date>
<booktitle>In ACL-COLING98, Proc. of the 36th Conference of the Association for Computational Linguistics.</booktitle>
<marker>Di Eugenio, Jordan, Moore, Thomason, 1998</marker>
<rawString>Barbara Di Eugenio, Pamela W. Jordan, Johanna D. Moore, and Richmond H. Thomason. 1998. An empirical investigation of collaborative dialogues. In ACL-COLING98, Proc. of the 36th Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Finke</author>
<author>M Lapata</author>
<author>A Lavie</author>
<author>L Levin</author>
<author>L Mayfield Tomokiyo</author>
<author>T Polzin</author>
<author>K Ries</author>
<author>A Waibel</author>
<author>K Zechner</author>
</authors>
<title>Clarity: Inferring discourse structure from speech.</title>
<date>1998</date>
<booktitle>In AAAI Symposium on Applying Machine Learning to Discourse Processing.</booktitle>
<contexts>
<context position="4139" citStr="Finke et al., 1998" startWordPosition="632" endWordPosition="635">m turns, but didn’t distinguish between turns used to request or present information, to give instructions, or to indicate errors. Recent research on dialogue has been based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors (Reithinger and Maier, 1995; Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998). Several research efforts have explored the use of dialogue act tagging schemes for tasks such as improving recognition performance (Reithinger and Maier, 1995; Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), and as a constraint on nominal expression generation (Jordan, 2000). Thus we decided to explore the application of a dialogue act tagging scheme to the task of evaluating and comparing dialogue systems. Section 2 describes the corpus. Section 3 describes the dialogue act tagging scheme we developed and applied to the evaluation of COMMUNICATOR dialogues. Section 4 first describes our results utilizing the standard logged metrics, and then describes results using the DATE metrics. Section 5 discusses future plans. 2 The Communicator 2000 Corpus The corpus consists of 662 dialogues from nine d</context>
</contexts>
<marker>Finke, Lapata, Lavie, Levin, Tomokiyo, Polzin, Ries, Waibel, Zechner, 1998</marker>
<rawString>M. Finke, M. Lapata, A. Lavie, L. Levin, L. Mayfield Tomokiyo, T. Polzin, K. Ries, A. Waibel, and K. Zechner. 1998. Clarity: Inferring discourse structure from speech. In AAAI Symposium on Applying Machine Learning to Discourse Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erving Goffman</author>
</authors>
<title>Frame Analysis: An Essay on the Organization of Experience. Harper and Row,</title>
<date>1974</date>
<location>New York.</location>
<contexts>
<context position="10218" citStr="Goffman, 1974" startWordPosition="1601" endWordPosition="1602">and any ancillary hotel/car reservations. The ABOUT-COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood (Walker, 1992; Clark and Schaefer, 1989). Utterances of this type are frequent in human-computer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. All implicit and explicit confirmations are about communication; See Figure 1 for examples. The SITUATION-FRAME domain pertains to the goal of managing the culturally relevant framing expectations (Goffman, 1974). The utterances in this domain are particularly relevant in humancomputer dialogues because the users’ expectations need to be defined during the course of the conversation. About frame utterances by the system attempt to help the user understand how to interact with the system, what it knows about, and what it can do. Some examples are in Figure 1. 3.3 Task Model The TASK-SUBTASK dimension refers to a task model of the domain task that the system supports and captures distinctions among dialogue acts that reflect the task structure.1 The motivation for this dimension is to derive metrics tha</context>
</contexts>
<marker>Goffman, 1974</marker>
<rawString>Erving Goffman. 1974. Frame Analysis: An Essay on the Organization of Experience. Harper and Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>Evaluating spoken language interaction: Experiences from the darpa spoken language program 1990–1995.</title>
<date>2000</date>
<editor>In S. Luperfoy, editor, Spoken Language Discourse.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="2720" citStr="Hirschman, 2000" startWordPosition="410" endWordPosition="411">to collect a set of core metrics for making cross system comparisons. The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be implemented consistently across systems. NIST’s contribution was to recruit the human subjects and to implement the experimental design specified by the Evaluation Committee. The experiment was designed to make it possible to apply the PARADISE evaluation framework (Walker et al., 2000), which integrates and unifies previous approaches to evaluation (Price et al., 1992; Hirschman, 2000). The framework posits that user satisfaction is the overall objective to be maximized and that task success and various interaction costs can be used as predictors of user satisfaction. Our results from applying PARADISE include that user satisfaction differed considerably across the nine systems. Subsequent modeling of user satisfaction gave us some insight into why each system was more or less satisfactory; four variables accounted for 37% of the variance in user-satisfaction: task completion, task duration, recognition accuracy, and mean system turn duration. However, when doing our analys</context>
</contexts>
<marker>Hirschman, 2000</marker>
<rawString>Lynette Hirschman. 2000. Evaluating spoken language interaction: Experiences from the darpa spoken language program 1990–1995. In S. Luperfoy, editor, Spoken Language Discourse. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Isard</author>
<author>Jean C Carletta</author>
</authors>
<title>Replicability of transaction and action coding in the map task corpus.</title>
<date>1995</date>
<booktitle>In AAAI Spring Symposium: Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>60--67</pages>
<contexts>
<context position="3842" citStr="Isard and Carletta, 1995" startWordPosition="584" endWordPosition="587">on, task duration, recognition accuracy, and mean system turn duration. However, when doing our analysis we were struck by the extent to which different aspects of the systems’ dialogue behavior weren’t captured by the core metrics. For example, the core metrics logged the number and duration of system turns, but didn’t distinguish between turns used to request or present information, to give instructions, or to indicate errors. Recent research on dialogue has been based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors (Reithinger and Maier, 1995; Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998). Several research efforts have explored the use of dialogue act tagging schemes for tasks such as improving recognition performance (Reithinger and Maier, 1995; Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), and as a constraint on nominal expression generation (Jordan, 2000). Thus we decided to explore the application of a dialogue act tagging scheme to the task of evaluating and comparing dialogue systems. Section 2 describes the corpus. Section 3 describes the dialogue act tagging scheme we developed a</context>
</contexts>
<marker>Isard, Carletta, 1995</marker>
<rawString>Amy Isard and Jean C. Carletta. 1995. Replicability of transaction and action coding in the map task corpus. In AAAI Spring Symposium: Empirical Methods in Discourse Interpretation and Generation, pages 60–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
</authors>
<title>Intentional Influences on Object Redescriptions in Dialogue: Evidence from an Empirical Study.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="4208" citStr="Jordan, 2000" startWordPosition="645" endWordPosition="646">formation, to give instructions, or to indicate errors. Recent research on dialogue has been based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors (Reithinger and Maier, 1995; Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998). Several research efforts have explored the use of dialogue act tagging schemes for tasks such as improving recognition performance (Reithinger and Maier, 1995; Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), and as a constraint on nominal expression generation (Jordan, 2000). Thus we decided to explore the application of a dialogue act tagging scheme to the task of evaluating and comparing dialogue systems. Section 2 describes the corpus. Section 3 describes the dialogue act tagging scheme we developed and applied to the evaluation of COMMUNICATOR dialogues. Section 4 first describes our results utilizing the standard logged metrics, and then describes results using the DATE metrics. Section 5 discusses future plans. 2 The Communicator 2000 Corpus The corpus consists of 662 dialogues from nine different travel planning systems with the number of dialogues per sys</context>
</contexts>
<marker>Jordan, 2000</marker>
<rawString>Pamela W. Jordan. 2000. Intentional Influences on Object Redescriptions in Dialogue: Evidence from an Empirical Study. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patti Price</author>
<author>Lynette Hirschman</author>
<author>Elizabeth Shriberg</author>
<author>Elizabeth Wade</author>
</authors>
<title>Subject-based evaluation measures for interactive spoken language systems.</title>
<date>1992</date>
<booktitle>In Proc. of the DARPA Speech and NL Workshop,</booktitle>
<pages>34--39</pages>
<contexts>
<context position="2702" citStr="Price et al., 1992" startWordPosition="406" endWordPosition="409">e used by all sites to collect a set of core metrics for making cross system comparisons. The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be implemented consistently across systems. NIST’s contribution was to recruit the human subjects and to implement the experimental design specified by the Evaluation Committee. The experiment was designed to make it possible to apply the PARADISE evaluation framework (Walker et al., 2000), which integrates and unifies previous approaches to evaluation (Price et al., 1992; Hirschman, 2000). The framework posits that user satisfaction is the overall objective to be maximized and that task success and various interaction costs can be used as predictors of user satisfaction. Our results from applying PARADISE include that user satisfaction differed considerably across the nine systems. Subsequent modeling of user satisfaction gave us some insight into why each system was more or less satisfactory; four variables accounted for 37% of the variance in user-satisfaction: task completion, task duration, recognition accuracy, and mean system turn duration. However, whe</context>
</contexts>
<marker>Price, Hirschman, Shriberg, Wade, 1992</marker>
<rawString>Patti Price, Lynette Hirschman, Elizabeth Shriberg, and Elizabeth Wade. 1992. Subject-based evaluation measures for interactive spoken language systems. In Proc. of the DARPA Speech and NL Workshop, pages 34–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Elisabeth Maier</author>
</authors>
<title>Utilizing statistical speech act processing in verbmobil.</title>
<date>1995</date>
<booktitle>In ACL 95.</booktitle>
<contexts>
<context position="3816" citStr="Reithinger and Maier, 1995" startWordPosition="580" endWordPosition="583">-satisfaction: task completion, task duration, recognition accuracy, and mean system turn duration. However, when doing our analysis we were struck by the extent to which different aspects of the systems’ dialogue behavior weren’t captured by the core metrics. For example, the core metrics logged the number and duration of system turns, but didn’t distinguish between turns used to request or present information, to give instructions, or to indicate errors. Recent research on dialogue has been based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors (Reithinger and Maier, 1995; Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998). Several research efforts have explored the use of dialogue act tagging schemes for tasks such as improving recognition performance (Reithinger and Maier, 1995; Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), and as a constraint on nominal expression generation (Jordan, 2000). Thus we decided to explore the application of a dialogue act tagging scheme to the task of evaluating and comparing dialogue systems. Section 2 describes the corpus. Section 3 describes the dialogue act tag</context>
<context position="11718" citStr="Reithinger and Maier, 1995" startWordPosition="1850" endWordPosition="1853">ks the ORIGIN, DESTINATION, DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL and ITINERARY tasks. The GROUND task includes both the HOTEL and CAR subtasks. Note that any subtask can involve multiple speech acts. For example, the DATE subtask can consist of acts requesting, or implicitly or explicitly confirming the date. A similar example is provided by the subtasks of CAR (rental) and HOTEL, which include dialogue acts requesting, confirming or acknowledging arrangements to rent a car or book a hotel room on the same trip. 1This dimension elaborates of each speech-act type in other tagging schemes (Reithinger and Maier, 1995). 2In (Walker and Passonneau, 2001) we didn’t distinguish the price subtask from the itinerary presentation subtask. Task Example TOP-LEVEL- What are your travel plans? TRIP ORIGIN And, what city are you leavingfrom? DESTINATION And, where are you flying to? DATE What day would you like to leave? TIME Departing at what time?. AIRLINE Did you have an airline preference? TRIP-TYPE Will you return to Boston from San Jose? RETRIEVAL Accessing the database; this might take a few seconds. ITINERARY I found 3 flights from Miami to Minneapolis. PRICE The airfare for this trip is 390 dollars. GROUND Di</context>
</contexts>
<marker>Reithinger, Maier, 1995</marker>
<rawString>Norbert Reithinger and Elisabeth Maier. 1995. Utilizing statistical speech act processing in verbmobil. In ACL 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>P Taylor</author>
<author>R Bates</author>
<author>A Stolcke</author>
<author>K Ries</author>
<author>D Jurafsky</author>
<author>N Coccaro</author>
<author>R Martin</author>
<author>M Meteer</author>
<author>C Van Ess-Dykema</author>
</authors>
<title>Can prosody aid the automatic classification of dialog acts in conversational speech. Language and Speech: Special Issue on Prosody and Conversation.</title>
<date>2000</date>
<marker>Shriberg, Taylor, Bates, Stolcke, Ries, Jurafsky, Coccaro, Martin, Meteer, Van Ess-Dykema, 2000</marker>
<rawString>E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries, D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and C. Van Ess-Dykema. 2000. Can prosody aid the automatic classification of dialog acts in conversational speech. Language and Speech: Special Issue on Prosody and Conversation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>R Passonneau</author>
</authors>
<title>Date: A dialogue act tagging scheme for evaluation.</title>
<date>2001</date>
<booktitle>In Human Language Technology Conference.</booktitle>
<contexts>
<context position="8075" citStr="Walker and Passonneau, 2001" startWordPosition="1262" endWordPosition="1265"> the task an utterance contributes to separately from what speech act function it serves. Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and (3) a CONVERSATIONAL-DOMAIN dimension. We believe that these distinctions are important for using such a scheme for evaluation. Figure 1 shows a COMMUNICATOR dialogue with each system utterance classified on these three dimensions. The tagset for each dimension are briefly described in the remainder of this section. See (Walker and Passonneau, 2001) for more detail. 3.1 Speech Acts In DATE, the SPEECH-ACT dimension hasten categories. We use familiar speech-act labels, such as OFFER, REQUEST-INFO, PRESENT-INFO, ACKNOWLEDGE, and introduce new ones designed to help us capture generalizations about communicative behavior in this domain, on this task, given the range of system and human behavior we see in the data. One new one, for example, is STATUS-REPORT. Examples of each speech-act type are in Figure 2. Speech-Act Example REQUEST-INFO And, what city are you flying to? PRESENT-INFO The airfare for this trip is 390 dollars. OFFER Would you </context>
<context position="11753" citStr="Walker and Passonneau, 2001" startWordPosition="1855" endWordPosition="1858">TIME, AIRLINE, TRIP-TYPE, RETRIEVAL and ITINERARY tasks. The GROUND task includes both the HOTEL and CAR subtasks. Note that any subtask can involve multiple speech acts. For example, the DATE subtask can consist of acts requesting, or implicitly or explicitly confirming the date. A similar example is provided by the subtasks of CAR (rental) and HOTEL, which include dialogue acts requesting, confirming or acknowledging arrangements to rent a car or book a hotel room on the same trip. 1This dimension elaborates of each speech-act type in other tagging schemes (Reithinger and Maier, 1995). 2In (Walker and Passonneau, 2001) we didn’t distinguish the price subtask from the itinerary presentation subtask. Task Example TOP-LEVEL- What are your travel plans? TRIP ORIGIN And, what city are you leavingfrom? DESTINATION And, where are you flying to? DATE What day would you like to leave? TIME Departing at what time?. AIRLINE Did you have an airline preference? TRIP-TYPE Will you return to Boston from San Jose? RETRIEVAL Accessing the database; this might take a few seconds. ITINERARY I found 3 flights from Miami to Minneapolis. PRICE The airfare for this trip is 390 dollars. GROUND Did you need to make any ground arran</context>
</contexts>
<marker>Walker, Passonneau, 2001</marker>
<rawString>M. Walker and R. Passonneau. 2001. Date: A dialogue act tagging scheme for evaluation. In Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE. Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems.</title>
<date>2000</date>
<contexts>
<context position="2618" citStr="Walker et al., 2000" startWordPosition="394" endWordPosition="397"> set of tools for processing the logfiles (Aberdeen, 2000); the standard and tools were used by all sites to collect a set of core metrics for making cross system comparisons. The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be implemented consistently across systems. NIST’s contribution was to recruit the human subjects and to implement the experimental design specified by the Evaluation Committee. The experiment was designed to make it possible to apply the PARADISE evaluation framework (Walker et al., 2000), which integrates and unifies previous approaches to evaluation (Price et al., 1992; Hirschman, 2000). The framework posits that user satisfaction is the overall objective to be maximized and that task success and various interaction costs can be used as predictors of user satisfaction. Our results from applying PARADISE include that user satisfaction differed considerably across the nine systems. Subsequent modeling of user satisfaction gave us some insight into why each system was more or less satisfactory; four variables accounted for 37% of the variance in user-satisfaction: task completi</context>
<context position="17159" citStr="Walker et al., 2000" startWordPosition="2734" endWordPosition="2737">s 3,2,1,4 in the top group (listed by average user satisfaction), sites 4,5,9,6 in a second group, and sites 8 and 7 defining a third and a fourth group. See (Walker et al., 2001) for more detail on crosssystem comparisons. However, our primary goal was to achieve a better understanding of the role of qualitative aspects of each system’s dialogue behavior. We quantify the extent to which the dialogue act metrics improve our understanding by applying the PARADISE framework to develop a model of user satisfaction and then examining the extent to which the dialogue act metrics improve the model (Walker et al., 2000). Section 4.1 describes the PARADISE models developed using the core metrics and section 4.2 describes the models derived from adding in the DATE metrics. 4.1 Results using Logfile Standard Metrics We applied PARADISE to develop models of user satisfaction using the core metrics; the best model fit accounts for 37% of the variance in user satisfaction. The learned model is that User Satisfaction is the sum of Exact Scenario Completion, Task Duration, System Turn Duration and Word Accuracy. Table 1 gives the details of the model, where the coefficient indicates both the magnitude and whether th</context>
<context position="18409" citStr="Walker et al., 2000" startWordPosition="2939" endWordPosition="2942">tive predictor of user satisfaction, and the P value indicates the significance of the metric in the model. The finding that metrics of task completion and Metric Coefficient P value ESC (Completion) 0.40 0.00 Task Dur -0.31 0.00 Sys Turn Dur 0.14 0.00 Word Accuracy 0.15 0.00 TripC 0.09 0.01 BookingC 0.08 0.03 PriceC 0.11 0.00 AckRent 0.07 0.05 EconTime 0.05 0.13 ReqDate 0.10 0.01 ReqTripType 0.09 0.00 Econ 0.11 0.01 Table 2: Predictive power and significance of Dialogue Act Metrics recognition performance are significant predictors duplicates results from other experiments applying PARADISE (Walker et al., 2000). The fact that task duration is also a significant predictor may indicate larger differences in task duration in this corpus than in previous studies. Note that the PARADISE model indicates that system turn duration is positively correlated with user satisfaction. We believed it plausible that this was due to the fact that flight presentation utterances are longer than other system turns. Thus this metric simply captures whether or not the system got enough information to present some potential flight itineraries to the user. We investigate this hypothesis further below. 4.2 Utilizing Dialogu</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards developing general models of usability with PARADISE. Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>J Aberdeen</author>
<author>J Boland</author>
<author>E Bratt</author>
<author>J Garofolo</author>
<author>L Hirschman</author>
<author>A Le</author>
<author>S Lee</author>
<author>S Narayanan</author>
<author>K Papineni</author>
<author>B Pellom</author>
<author>J Polifroni</author>
<author>A Potamianos</author>
<author>P Prabhu</author>
<author>A Rudnicky</author>
<author>G Sanders</author>
<author>S Seneff</author>
<author>D Stallard</author>
<author>S Whittaker</author>
</authors>
<title>Darpa communicator dialog travel planning systems:</title>
<date>2001</date>
<booktitle>The</booktitle>
<note>In Submitted to EUROSPEECH</note>
<contexts>
<context position="4900" citStr="Walker et al., 2001" startWordPosition="757" endWordPosition="760">cheme to the task of evaluating and comparing dialogue systems. Section 2 describes the corpus. Section 3 describes the dialogue act tagging scheme we developed and applied to the evaluation of COMMUNICATOR dialogues. Section 4 first describes our results utilizing the standard logged metrics, and then describes results using the DATE metrics. Section 5 discusses future plans. 2 The Communicator 2000 Corpus The corpus consists of 662 dialogues from nine different travel planning systems with the number of dialogues per system ranging between 60 and 79. The experimental design is described in (Walker et al., 2001). Each dialogue consists of a recording, a logfile consistent with the standard, transcriptions and recordings of all user utterances, and the output of a web-based user survey. Metrics collected per call included: Dialogue Efficiency: Task Duration, System turns, User turns, Total Turns Dialogue Quality: Word Accuracy, Response latency, Response latency variance Task Success: Exact Scenario Completion User Satisfaction: Sum of TTS performance, Task ease, User expertise, Expected behavior, Future use. The objective metrics focus on measures that can be automatically logged or computed and a we</context>
<context position="16718" citStr="Walker et al., 2001" startWordPosition="2662" endWordPosition="2665"> Sys Turn Dur 0.12 0.000 Wrd Acc 0.17 0.000 Table 1: Predictive power and significance of Core Metrics 4 Results We initially examined differences in cumulative user satisfaction across the nine systems. An ANOVA for user satisfaction by Site ID using the modified Bonferroni statistic for multiple comparisons showed that there were statistically significant differences across sites, and that there were four groups of performers with sites 3,2,1,4 in the top group (listed by average user satisfaction), sites 4,5,9,6 in a second group, and sites 8 and 7 defining a third and a fourth group. See (Walker et al., 2001) for more detail on crosssystem comparisons. However, our primary goal was to achieve a better understanding of the role of qualitative aspects of each system’s dialogue behavior. We quantify the extent to which the dialogue act metrics improve our understanding by applying the PARADISE framework to develop a model of user satisfaction and then examining the extent to which the dialogue act metrics improve the model (Walker et al., 2000). Section 4.1 describes the PARADISE models developed using the core metrics and section 4.2 describes the models derived from adding in the DATE metrics. 4.1 </context>
</contexts>
<marker>Walker, Aberdeen, Boland, Bratt, Garofolo, Hirschman, Le, Lee, Narayanan, Papineni, Pellom, Polifroni, Potamianos, Prabhu, Rudnicky, Sanders, Seneff, Stallard, Whittaker, 2001</marker>
<rawString>M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman, A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom, J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky, G. Sanders, S. Seneff, D. Stallard, and S. Whittaker. 2001. Darpa communicator dialog travel planning systems: The june 2000 data collection. In Submitted to EUROSPEECH 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Redundancy in collaborative dialogue.</title>
<date>1992</date>
<booktitle>In Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>345--351</pages>
<contexts>
<context position="9797" citStr="Walker, 1992" startWordPosition="1540" endWordPosition="1541"> an utterance is about. Each speech act can occur in any of three domains of discourse described below. The ABOUT-TASK domain is necessary for evaluating a dialogue system’s ability to collaborate with a speaker on achieving the task goal of making reservations for a specific trip. It supports metrics such as the amount of time/effort the system takes to complete a particular phase of making an airline reservation, and any ancillary hotel/car reservations. The ABOUT-COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood (Walker, 1992; Clark and Schaefer, 1989). Utterances of this type are frequent in human-computer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. All implicit and explicit confirmations are about communication; See Figure 1 for examples. The SITUATION-FRAME domain pertains to the goal of managing the culturally relevant framing expectations (Goffman, 1974). The utterances in this domain are particularly relevant in humancomputer dialogues because the users’ expectations need to be defined during the course of the conversation. Abou</context>
</contexts>
<marker>Walker, 1992</marker>
<rawString>Marilyn A. Walker. 1992. Redundancy in collaborative dialogue. In Fourteenth International Conference on Computational Linguistics, pages 345–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
</authors>
<title>Communicator evaluation committee webpage.</title>
<date>2000</date>
<note>http://www.research.att.com/ walker/eval/eval.html.</note>
<contexts>
<context position="1838" citStr="Walker, 2000" startWordPosition="271" endWordPosition="272">nd the contribution of various techniques to users’ willingness and ability to use a spoken dialogue system. In June of 2000, we conducted an exploratory data collection experiment with nine participating communicator systems. All systems supported travel planning and utilized some form of mixedinitiative interaction. However the systems varied in several critical dimensions: (1) They targeted different back-end databases for travel information; (2) System modules such as ASR, NLU, TTS and dialogue management were typically different across systems. The Evaluation Committee chaired by Walker (Walker, 2000), with representatives from the nine COMMUNICATOR sites and from NIST, developed the experimental design. A logfile standard was developed by MITRE along with a set of tools for processing the logfiles (Aberdeen, 2000); the standard and tools were used by all sites to collect a set of core metrics for making cross system comparisons. The core metrics were developed during a workshop of the Evaluation Committee and included all metrics that anyone in the committee suggested, that could be implemented consistently across systems. NIST’s contribution was to recruit the human subjects and to imple</context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>Marilyn Walker. 2000. Communicator evaluation committee webpage. http://www.research.att.com/ walker/eval/eval.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>