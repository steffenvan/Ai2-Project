<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999604">
A Systematic Comparison of Various
Statistical Alignment Models
</title>
<author confidence="0.999858">
Franz Josef Och∗ Hermann Ney†
</author>
<affiliation confidence="0.993862">
University of Southern California RWTH Aachen
</affiliation>
<bodyText confidence="0.999099076923077">
We present and compare various methods for computing word alignments using statistical or
heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della
Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and
refinements. These statistical models are compared with two heuristic models based on the Dice
coefficient. We present different methodsfor combining word alignments to perform a symmetriza-
tion of directed statistical alignment models. As evaluation criterion, we use the quality of the
resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate
the models on the German-English Verbmobil task and the French-English Hansards task. We
perform a detailed analysis of various design decisions of our statistical alignment system and
evaluate these on training corpora of various sizes. An important result is that refined align-
ment models with a first-order dependence and a fertility model yield significantly better results
than simple heuristic models. In the Appendix, we present an efficient training algorithm for the
alignment models presented.
</bodyText>
<sectionHeader confidence="0.997624" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999917789473684">
We address in this article the problem of finding the word alignment of a bilingual
sentence-aligned corpus by using language-independent statistical methods. There is
a vast literature on this topic, and many different systems have been suggested to
solve this problem. Our work follows and extends the methods introduced by Brown,
Della Pietra, Della Pietra, and Mercer (1993) by using refined statistical models for
the translation process. The basic idea of this approach is to develop a model of the
translation process with the word alignment as a hidden variable of this process, to
apply statistical estimation theory to compute the “optimal” model parameters, and
to perform alignment search to compute the best word alignment.
So far, refined statistical alignment models have in general been rarely used. One
reason for this is the high complexity of these models, which makes them difficult
to understand, implement, and tune. Instead, heuristic models are usually used. In
heuristic models, the word alignments are computed by analyzing some association
score metric of a link between a source language word and a target language word.
These models are relatively easy to implement.
In this article, we focus on consistent statistical alignment models suggested in the
literature, but we also describe a heuristic association metric. By providing a detailed
description and a systematic evaluation of these alignment models, we give the reader
various criteria for deciding which model to use for a given task.
</bodyText>
<note confidence="0.5844684">
∗ Information Science Institute (USC/ISI), 4029 Via Marina, Suite 1001, Marina del Rey, CA 90292.
† Lehrstuhl f¨ur Informatik VI, Computer Science Department, RWTH Aachen–University of Technology,
D-52056 Aachen, Germany.
© 2003 Association for Computational Linguistics
Computational Linguistics Volume 29, Number 1
</note>
<figureCaption confidence="0.975877">
Figure 1
</figureCaption>
<bodyText confidence="0.955419333333333">
Example of a word alignment (VERBMOBIL task).
We propose to measure the quality of an alignment model by comparing the qual-
ity of the most probable alignment, the Viterbi alignment, with a manually produced
reference alignment. This has the advantage of enabling an automatic evaluation to be
performed. In addition, we shall show that this quality measure is a precise and reli-
able evaluation criterion that is well suited to guide designing and training statistical
alignment models.
The software used to train the statistical alignment models described in this article
is publicly available (Och 2000).
</bodyText>
<subsectionHeader confidence="0.996747">
1.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999961266666667">
We follow Brown, Della Pietra, Della Pietra, and Mercer (1993) to define alignment
as an object for indicating the corresponding words in a parallel text. Figure 1 shows
an example. Very often, it is difficult for a human to judge which words in a given
target string correspond to which words in its source string. Especially problematic
is the alignment of words within idiomatic expressions, free translations, and missing
function words. The problem is that the notion of “correspondence” between words
is subjective. It is important to keep this in mind in the evaluation of word alignment
quality. We shall deal with this problem in Section 5.
The alignment between two word strings can be quite complicated. Often, an
alignment includes effects such as reorderings, omissions, insertions, and word-to-
phrase alignments. Therefore, we need a very general representation of alignment.
Formally, we use the following definition for alignment in this article. We are given
a source (French) stringf1J = f1, ... , fj,. . . , fJ and a target language (English) string
eI1 = e1,. . . , ei, ... , eI that have to be aligned. We define an alignment between the two
word strings as a subset of the Cartesian product of the word positions; that is, an
</bodyText>
<page confidence="0.975543">
20
</page>
<note confidence="0.640465666666667">
Och and Ney Comparison of Statistical Alignment Models
alignment A is defined as
A C J(j,i): j = 1,...,J;i = 1,...,I} (1)
</note>
<bodyText confidence="0.999803625">
Modeling the alignment as an arbitrary relation between source and target language
positions is quite general. The development of alignment models that are able to deal
with this general representation, however, is hard. Typically, the alignment models pre-
sented in the literature impose additional constraints on the alignment representation.
Typically, the alignment representation is restricted in a way such that each source
word is assigned to exactly one target word. Alignment models restricted in this way
are similar to the concept of hidden Markov models (HMMs) in speech recognition.
The alignment mapping in such models consists of associations j → i = aj from source
position j to target position i = aj. The alignment aJ1 = a1, ... , aj, ... , aJ may contain
alignments aj = 0 with the “empty” word e0 to account for source words that are
not aligned with any target word. Constructed in such a way, the alignment is not
a relation between source and target language positions, but only a mapping from
source to target language positions.
In Melamed (2000), a further simplification is performed that enforces a one-to-one
alignment for nonempty words. This means that the alignment mapping aJ1 must be
injective for all word positions aj &gt; 0. Note that many translation phenomena cannot
be handled using restricted alignment representations such as this one. Especially,
methods such as Melamed’s are in principle not able to achieve a 100% recall. The
problem can be reduced through corpus preprocessing steps that perform grouping
and splitting of words.
Some papers report improvements in the alignment quality of statistical methods
when linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000). In
these methods, the linguistic knowledge is used mainly to filter out incorrect align-
ments. In this work, we shall avoid making explicit assumptions concerning the lan-
guage used. By avoiding these assumptions, we expect our approach to be applicable
to almost every language pair. The only assumptions we make are that the parallel
text is segmented into aligned sentences and that the sentences are segmented into
words. Obviously, there are additional implicit assumptions in the models that are
needed to obtain a good alignment quality. For example, in languages with a very
rich morphology, such as Finnish, a trivial segmentation produces a high number of
words that occur only once, and every learning method suffers from a significant data
sparseness problem.
</bodyText>
<subsectionHeader confidence="0.965878">
1.2 Applications
</subsectionHeader>
<bodyText confidence="0.957839307692308">
There are numerous applications for word alignments in natural language processing.
These applications crucially depend on the quality of the word alignment (Och and
Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word align-
ment methods is the automatic extraction of bilingual lexica and terminology from
corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000).
Statistical alignment models are often the basis of single-word-based statistical
machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998;
Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney
2001; Germann et al. 2001). In addition, these models are the starting point for re-
fined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999)
or example-based translation systems (Brown 1997). In such systems, the quality of
the machine translation output directly depends on the quality of the initial word
alignment (Och and Ney 2000).
</bodyText>
<page confidence="0.998378">
21
</page>
<note confidence="0.880772">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.9972836">
Another application of word alignments is in the field of word sense disambigua-
tion (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used
to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers
from a language, such as English, for which many tools already exist to languages for
which such resources are scarce.
</bodyText>
<subsectionHeader confidence="0.949694">
1.3 Overview
</subsectionHeader>
<bodyText confidence="0.999957944444445">
In Section 2, we review various statistical alignment models and heuristic models.
We present a new statistical alignment model, a log-linear combination of the best
models of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, and
Mercer (1993). In Section 3, we describe the training of the alignment models and
present a new training schedule that yields significantly better results. In addition,
we describe how to deal with overfitting, deficient models, and very small or very
large training corpora. In Section 4, we present some heuristic methods for improving
alignment quality by performing a symmetrization of word alignments. In Section 5,
we describe an evaluation methodology for word alignment methods dealing with
the ambiguities associated with the word alignment annotation based on generalized
precision and recall measures. In Section 6, we present a systematic comparison of the
various statistical alignment models with regard to alignment quality and translation
quality. We assess the effect of training corpora of various sizes and the use of a
conventional bilingual dictionary. In the literature, it is often claimed that the refined
alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993) are not
suitable for small corpora because of data sparseness problems. We show that this is
not the case if these models are parametrized suitably. In the Appendix, we describe
some methods for efficient training of fertility-based alignment models.
</bodyText>
<sectionHeader confidence="0.989811" genericHeader="keywords">
2. Review of Alignment Models
</sectionHeader>
<subsectionHeader confidence="0.835654">
2.1 General Approaches
</subsectionHeader>
<bodyText confidence="0.988465416666667">
We distinguish between two general approaches to computing word alignments: sta-
tistical alignment models and heuristic models. In the following, we describe both
types of models and compare them from a theoretical viewpoint.
The notational convention we employ is as follows. We use the symbol Pr(·)
to denote general probability distributions with (almost) no specific assumptions. In
contrast, for model-based probability distributions, we use the generic symbol p(·).
2.1.1 Statistical Alignment Models. In statistical machine translation, we try to model
the translation probability Pr(f1 J  |eI1), which describes the relationship between a
source language string fJ1 and a target language string eI1. In (statistical) alignment
models Pr(fJ1,aJ1  |eI1), a “hidden” alignment aJ1 is introduced that describes a mapping
from a source position j to a target position aj. The relationship between the translation
model and the alignment model is given by
</bodyText>
<equation confidence="0.988563">
Pr(fJ 1  |eI1) = � Pr(f J 1,aJ 1  |eI1) (2)
aJ 1
</equation>
<bodyText confidence="0.9985985">
The alignment aJ 1 may contain alignments aj = 0 with the empty word e0 to account
for source words that are not aligned with any target word.
In general, the statistical model depends on a set of unknown parameters θ that is
learned from training data. To express the dependence of the model on the parameter
</bodyText>
<page confidence="0.995111">
22
</page>
<note confidence="0.917634">
Och and Ney Comparison of Statistical Alignment Models
</note>
<bodyText confidence="0.925957">
set, we use the following notation:
</bodyText>
<equation confidence="0.990043">
Pr(fJ1, aJ1  |eI1) = pe(f J 1,aJ 1  |eI1) (3)
</equation>
<bodyText confidence="0.999765">
The art of statistical modeling is to develop specific statistical models that capture
the relevant properties of the considered problem domain. In our case, the statistical
alignment model has to describe the relationship between a source language string
and a target language string adequately.
To train the unknown parameters θ, we are given a parallel training corpus con-
sisting of S sentence pairs {(fs, es) : s = 1, ... , S}. For each sentence pair (fs, es), the
alignment variable is denoted by a = aJ1. The unknown parameters θ are determined
by maximizing the likelihood on the parallel training corpus:
</bodyText>
<equation confidence="0.650362">
θˆ = argmax S E pe(fs, a  |es) (4)
e H a
s=1
</equation>
<bodyText confidence="0.999781">
Typically, for the kinds of models we describe here, the expectation maximization (EM)
algorithm (Dempster, Laird, and Rubin 1977) or some approximate EM algorithm is
used to perform this maximization. To avoid a common misunderstanding, however,
note that the use of the EM algorithm is not essential for the statistical approach, but
only a useful tool for solving this parameter estimation problem.
Although for a given sentence pair there is a large number of alignments, we can
always find a best alignment:
</bodyText>
<equation confidence="0.994796">
ˆaJ1 = argmax pˆe(fJ1, aJ1  |eI1) (5)
aJ 1
</equation>
<bodyText confidence="0.985168047619048">
The alignment ˆaJ1 is also called the Viterbi alignment of the sentence pair(f J 1,eI 1). (For
the sake of simplicity, we shall drop the index θ if it is not explicitly needed.)
Later in the article, we evaluate the quality of this Viterbi alignment by comparing
it to a manually produced reference alignment. The parameters of the statistical align-
ment models are optimized with respect to a maximum-likelihood criterion, which
is not necessarily directly related to alignment quality. Such an approach, however,
requires training with manually defined alignments, which is not done in the research
presented in this article. Experimental evidence shows (Section 6) that the statistical
alignment models using this parameter estimation technique do indeed obtain a good
alignment quality.
In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della
Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel,
Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which
we call Model 6. All these models use a different decomposition of the probability
Pr(fJ1,aJ1  |eI1).
2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments
use a function of the similarity between the types of the two languages (Smadja, Mc-
Keown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently,
variations of the Dice coefficient (Dice 1945) are used as this similarity function. For
each sentence pair, a matrix including the association scores between every word at
every position is then obtained:
</bodyText>
<equation confidence="0.996177666666667">
2 · C(ei, fj)
dice(i,j) = (6)
C(ei) · C(fj)
</equation>
<page confidence="0.991856">
23
</page>
<note confidence="0.435132">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.9972426">
C(e,f) denotes the co-occurrence count of e and f in the parallel training corpus. C(e)
and C(f) denote the count of e in the target sentences and the count off in the source
sentences, respectively. From this association score matrix, the word alignment is then
obtained by applying suitable heuristics. One method is to choose as alignment aj = i
for position j the word with the largest association score:
</bodyText>
<equation confidence="0.996116">
aj = argmax{dice(i,j)} (7)
i
</equation>
<bodyText confidence="0.9996875">
A refinement of this method is the competitive linking algorithm (Melamed 2000).
In a first step, the highest-ranking word position (i, j) is aligned. Then, the correspond-
ing row and column are removed from the association score matrix. This procedure is
iteratively repeated until every source or target language word is aligned. The advan-
tage of this approach is that indirect associations (i.e., words that co-occur often but
are not translations of each other) occur less often. The resulting alignment contains
only one-to-one alignments and typically has a higher precision than the heuristic
model defined in equation (7).
</bodyText>
<subsectionHeader confidence="0.591941">
2.1.3 A Comparison of Statistical Models and Heuristic Models. The main advan-
</subsectionHeader>
<bodyText confidence="0.999853416666667">
tage of the heuristic models is their simplicity. They are very easy to implement and
understand. Therefore, variants of the heuristic models described above are widely
used in the word alignment literature.
One problem with heuristic models is that the use of a specific similarity function
seems to be completely arbitrary. The literature contains a large variety of different
scoring functions, some including empirically adjusted parameters. As we show in
Section 6, the Dice coefficient results in a worse alignment quality than the statistical
models.
In our view, the approach of using statistical alignment models is more coherent.
The general principle for coming up with an association score between words results
from statistical estimation theory, and the parameters of the models are adjusted such
that the likelihood of the models on the training corpus is maximized.
</bodyText>
<subsectionHeader confidence="0.99868">
2.2 Statistical Alignment Models
</subsectionHeader>
<bodyText confidence="0.6850385">
2.2.1 Hidden Markov Alignment Model. The alignment model Pr(fJ1,aJ 1  |eI1) can be
structured without loss of generality as follows:
</bodyText>
<equation confidence="0.989808363636364">
Pr(fJ1, aJ1  |eI1) = Pr(J  |eI1) · � J Pr(fj, aj  |f j−1
j=1 1 , aj−1
1 , eI1) (8)
= Pr(J  |eI1) · � J Pr(aj  |f j−1
j=1 1 ,aj−1
1 , eI1) · Pr(fj  |fj−1
1 , aj1, eI1) (9)
Using this decomposition, we obtain three different probabilities: a length probability
Pr(J  |eI1), an alignment probability Pr(aj  |f1j−1,aj1− 1,eI1) and a lexicon probability
Pr(fj  |fj−1
1 ,aj1,eI 1). In the hidden Markov alignment model, we assume a first-order
</equation>
<bodyText confidence="0.855119">
dependence for the alignments aj and that the lexicon probability depends only on the
word at position aj:
</bodyText>
<equation confidence="0.9969766">
Pr(aj  |f j−1
1 ,aj−1
1 ,eI 1) = p(aj  |aj−1,I) (10)
Pr(fj  |f j−1
1 ,aj 1,eI 1) = p(fj  |eaj) (11)
</equation>
<page confidence="0.984482">
24
</page>
<note confidence="0.601676">
Och and Ney Comparison of Statistical Alignment Models
</note>
<bodyText confidence="0.8604215">
Later in the article, we describe a refinement with a dependence on eaj−1 in the
alignment model. Putting everything together and assuming a simple length model
</bodyText>
<equation confidence="0.99706475">
Pr(J  |eI1) = p(J  |I), we obtain the following basic HMM-based decomposition of
p(fJ1  |eI1):
p(f1J |eI 1) =p(J |I)· � Il J [p(aj  |aj−1, I) · p(fj  |eaj)] (12)
aJ 1 j=1
</equation>
<bodyText confidence="0.9876594">
with the alignment probability p(i  |it, I) and the translation probability p(f  |e).
To make the alignment parameters independent of absolute word positions, we
assume that the alignment probabilities p(i  |i&apos;,I) depend only on the jump width
(i − i&apos;). Using a set of non-negative parameters {c(i − i&apos;)}, we can write the alignment
probabilities in the form
</bodyText>
<equation confidence="0.994912">
c(i − i~)
p(i  |i~, I) =rI(13)
i&amp;quot;=1 c(ill − i&apos;)
</equation>
<bodyText confidence="0.961175272727273">
This form ensures that the alignment probabilities satisfy the normalization constraint
for each conditioning word position it, it = 1, ... , I. This model is also referred to as a
homogeneous HMM (Vogel, Ney, and Tillmann 1996). A similar idea was suggested
by Dagan, Church, and Gale (1993).
In the original formulation of the hidden Markov alignment model, there is no
empty word that generates source words having no directly aligned target word. We
introduce the empty word by extending the HMM network by I empty words e2I
I+1.
The target word ei has a corresponding empty word ei+I (i.e., the position of the empty
word encodes the previously visited target word). We enforce the following constraints
on the transitions in the HMM network (i ≤ I, it ≤ I) involving the empty word e0:1
</bodyText>
<equation confidence="0.998442666666667">
p(i + I  |i&apos;,I) = p0 · b(i,i&apos;)
p(i + I  |i&apos;+I,I) = p0 · b(i,i&apos;)
p(i  |i&apos;+I,I) = p(i  |i&apos;,I)
</equation>
<bodyText confidence="0.97259675">
The parameter p0 is the probability of a transition to the empty word, which has to be
optimized on held-out data. In our experiments, we set p0 = 0.2.
Whereas the HMM is based on first-order dependencies p(i = aj  |aj−1,I) for the
alignment distribution, Models 1 and 2 use zero-order dependencies p(i = aj  |j, I, J):
</bodyText>
<listItem confidence="0.992609">
• Model 1 uses a uniform distribution p(i  |j,I,J) = 1/(I + 1):
</listItem>
<equation confidence="0.993833666666667">
J
Pr(fJ1,aJ1  |eI1) = (I(J  |i)J · Ilp(fj  |eaj) (17)
j=1
</equation>
<bodyText confidence="0.970598">
Hence, the word order does not affect the alignment probability.
</bodyText>
<listItem confidence="0.984169">
• In Model 2, we obtain
</listItem>
<equation confidence="0.995467666666667">
Pr(fJ1,aJ1  |eI1) = p(J  |I) · Il J [p(aj  |j,I,J) · p(fj  |eaj)] (18)
j=1
1 δ(i, i) is the Kronecker function, which is one if i = it and zero otherwise.
</equation>
<page confidence="0.989938">
25
</page>
<note confidence="0.434957">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.999198333333333">
To reduce the number of alignment parameters, we ignore the
dependence on J in the alignment model and use a distribution p(aj  |j, I)
instead of p(aj  |j, I, J).
</bodyText>
<subsectionHeader confidence="0.998981">
2.3 Fertility-Based Alignment Models
</subsectionHeader>
<bodyText confidence="0.999708">
In the following, we give a short description of the fertility-based alignment models
of Brown, Della Pietra, Della Pietra, and Mercer (1993). A gentle introduction can be
found in Knight (1999b).
The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra,
Della Pietra, and Mercer 1993) have a significantly more complicated structure than
the simple Models 1 and 2. The fertility Oi of a word ei in position i is defined as the
number of aligned source words:
</bodyText>
<equation confidence="0.9901465">
�Oi = δ(aj,i) (19)
j
</equation>
<bodyText confidence="0.9995661">
The fertility-based alignment models contain a probability p(O  |e) that the target word
e is aligned to O words. By including this probability, it is possible to explicitly describe
the fact that for instance the German word ¨ubermorgen produces four English words
(the day after tomorrow). In particular, the fertility O = 0 is used for prepositions
or articles that have no direct counterpart in the other language.
To describe the fertility-based alignment models in more detail, we introduce,
as an alternative alignment representation, the inverted alignments, which define a
mapping from target to source positions rather than the other way around. We allow
several positions in the source language to be covered; that is, we consider alignments
B of the form
</bodyText>
<equation confidence="0.809337">
B: i → Bi C {1,...,j,...,J}. (20)
</equation>
<bodyText confidence="0.98285175">
An important constraint for the inverted alignment is that all positions of the source
sentence must be covered exactly once; that is, the Bi have to form a partition of the
set {1, ... , j, ... ,J}. The number of words Oi = |Bi |is the fertility of the word ei. In the
following, Bik refers to the kth element of Bi in ascending order.
The inverted alignments BI0 are a different way to represent normal alignments
aJ1. The set B0 contains the positions of all source words that are aligned with the
empty word. Fertility-based alignment models use the following decomposition and
assumptions:2
</bodyText>
<equation confidence="0.9995288">
Pr(fJ1, aJ1  |eI1) = Pr(fJ1, BI0  |eI1) (21)
= Pr(B0  |BI1) · 11I Pr(Bi  |Bi−1
i=1 1 ,eI 1) · Pr(f 1J  |BI0, eI1) (22)
= p(B0  |BI1) · 11I p(Bi  |Bi−1,ei) · 11I 11 p(fj  |ei) (23)
i=1 i=0 j∈Bi
</equation>
<bodyText confidence="0.607026">
As might be seen from this equation, we have tacitly assumed that the set B0 of words
aligned with the empty word is generated only after the nonempty positions have
</bodyText>
<footnote confidence="0.794344">
2 The original description of the fertility-based alignment models in Brown, Della Pietra, Della Pietra,
and Mercer (1993) includes a more refined derivation of the fertility-based alignment models.
</footnote>
<page confidence="0.991821">
26
</page>
<note confidence="0.485771">
Och and Ney Comparison of Statistical Alignment Models
</note>
<listItem confidence="0.826104">
been covered. The distribution p(Bi  |Bi−1,ei) is different for Models 3, 4, and 5:
• In Model 3, the dependence of Bi on its predecessor Bi−1 is ignored:
</listItem>
<equation confidence="0.994792">
p(Bi  |Bi−1,ei) = p(Oi  |ei) Oi! 11 p(j  |i, J) (24)
jEBi
</equation>
<bodyText confidence="0.933063818181818">
We obtain an (inverted) zero-order alignment model p(j  |i, J).
• In Model 4, every word is dependent on the previous aligned word and
on the word classes of the surrounding words. First, we describe the
dependence of alignment positions. (The dependence on word classes is
for now ignored and will be introduced later.) We have two (inverted)
first-order alignment models: p=1(∆j  |· · ·) and p&gt;1(∆j  |· · ·). The
difference between this model and the first-order alignment model in the
HMM lies in the fact that here we now have a dependence along the
j-axis instead of a dependence along the i-axis. The model p=1(∆j  |· · ·) is
used to position the first word of a set Bi, and the model p&gt;1(∆j  |· · ·) is
used to position the remaining words from left to right:
</bodyText>
<equation confidence="0.9947245">
p(Bi  |Bi−1,ei) = p(Oi  |ei)·p=1(Bi1 −BP(i)  |···) 11φi p&gt;1(Bik − Bi,k−1  |···) (25)
k=2
</equation>
<bodyText confidence="0.971022">
The function i → i&apos; = p(i) gives the largest value i&apos; &lt; i for which |Bi, |&gt; 0.
The symbol BP(i) denotes the average of all elements in BP(i).
</bodyText>
<listItem confidence="0.988224">
• Both Model 3 and Model 4 ignore whether or not a source position has
been chosen. In addition, probability mass is reserved for source
positions outside the sentence boundaries. For both of these reasons, the
probabilities of all valid alignments do not sum to unity in these two
models. Such models are called deficient (Brown, Della Pietra, Della
Pietra, and Mercer 1993). Model 5 is a reformulation of Model 4 with a
suitably refined alignment model to avoid deficiency. (We omit the
specific formula. We note only that the number of alignment parameters
for Model 5 is significantly larger than for Model 4.)
</listItem>
<bodyText confidence="0.999606">
Models 3, 4, and 5 define the probability p(B0  |BI1) as uniformly distributed for the
O0! possibilities given the number of words aligned with the empty word O0 = |B0|.
Assuming a binomial distribution for the number of words aligned with the empty
word, we obtain the following distribution for B0:
</bodyText>
<equation confidence="0.9986202">
I
p(B0  |Bi) = p O0  |Oi· 1(26)
O0!
i=1
(�− O0 / (1 − p1)J−2φ0p110 · 1!(27)
</equation>
<bodyText confidence="0.999977333333333">
The free parameter p1 is associated with the number of words that are aligned with
the empty word. There are O0! ways to order the O0 words produced by the empty
word, and hence, the alignment model of the empty word is nondeficient. As we will
</bodyText>
<page confidence="0.974938">
27
</page>
<note confidence="0.415035">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.995119">
see in Section 3.2, this creates problems for Models 3 and 4. Therefore, we modify
Models 3 and 4 slightly by replacing 00! in equation (27) with Jφ0:
</bodyText>
<equation confidence="0.994964">
p(B0  |BI1) = l �0 00 I (1 − p1)J−2φ0poo · J10 (28)
</equation>
<bodyText confidence="0.999814636363636">
As a result of this modification, the alignment models for both nonempty words and
the empty word are deficient.
2.3.1 Model 6. As we shall see, the alignment models with a first-order dependence
(HMM, Models 4 and 5) produce significantly better results than the other alignment
models. The HMM predicts the distance between subsequent source language po-
sitions, whereas Model 4 predicts the distance between subsequent target language
positions. This implies that the HMM makes use of locality in the source language,
whereas Model 4 makes use of locality in the target language. We expect to achieve
better alignment quality by using a model that takes into account both types of de-
pendencies. Therefore, we combine HMM and Model 4 in a log-linear way and call
the resulting model Model 6:
</bodyText>
<equation confidence="0.940129">
p6(f,a  |e) = p4(f,a  |e)α ·pHMM(f,a  |e) (29)
&amp;,,f, p4(f&apos;, a&apos;  |e)α · pHMM(f&apos;, a&apos;  |e)
</equation>
<bodyText confidence="0.9989434">
Here, the interpolation parameter α is employed to weigh Model 4 relative to the
hidden Markov alignment model. In our experiments, we use Model 4 instead of
Model 5, as it is significantly more efficient in training and obtains better results.
In general, we can perform a log-linear combination of several models pk(f,a  |e),
k=1,...,Kby
</bodyText>
<equation confidence="0.9757">
HK k=1 pk(f, a  |e)αk
p6(f, a  |e) = � HK (30)
k=1 pk(f�, a~  |e)αk
a�,f�
</equation>
<bodyText confidence="0.999916">
The interpolation parameters αk are determined in such a way that the alignment
quality on held-out data is optimized.
We use a log-linear combination instead of the simpler linear combination be-
cause the values of Pr(f,a  |e) typically differ by orders of magnitude for HMM and
Model 4. In such a case, we expect the log-linear combination to be better than a linear
combination.
</bodyText>
<subsubsectionHeader confidence="0.875171">
2.3.2 Alignment Models Depending on Word Classes. For HMM and Models 4 and
</subsubsectionHeader>
<bodyText confidence="0.994761769230769">
5, it is straightforward to extend the alignment parameters to include a dependence
on the word classes of the surrounding words (Och and Ney 2000). In the hidden
Markov alignment model, we allow for a dependence of the position aj on the class
of the preceding target word C(eaj−1): p(aj  |aj−1,I,C(eaj−1)). Similarly, we can include
dependencies on source and target word classes in Models 4 and 5 (Brown, Della
Pietra, Della Pietra, and Mercer 1993). The categorization of the words into classes
(here: 50 classes) is performed automatically by using the statistical learning procedure
described in Kneser and Ney (1993).
2.3.3 Overview of Models. The main differences among the statistical alignment mod-
els lie in the alignment model they employ (zero-order or first-order), the fertility
model they employ, and the presence or absence of deficiency. In addition, the models
differ with regard to the efficiency of the E-step in the EM algorithm (Section 3.1).
Table 1 offers an overview of the properties of the various alignment models.
</bodyText>
<page confidence="0.996177">
28
</page>
<note confidence="0.833631">
Och and Ney Comparison of Statistical Alignment Models
</note>
<tableCaption confidence="0.995366">
Table 1
</tableCaption>
<bodyText confidence="0.694829">
Overview of the alignment models.
Model Alignment model Fertility model E-step Deficient
Model 1 uniform no exact no
Model 2 zero-order no exact no
HMM first-order no exact no
Model 3 zero-order yes approximative yes
Model 4 first-order yes approximative yes
Model 5 first-order yes approximative no
Model 6 first-order yes approximative yes
</bodyText>
<subsectionHeader confidence="0.998538">
2.4 Computation of the Viterbi Alignment
</subsectionHeader>
<bodyText confidence="0.9721266">
We now develop an algorithm to compute the Viterbi alignment for each alignment
model. Although there exist simple polynomial algorithms for the baseline Models 1
and 2, we are unaware of any efficient algorithm for computing the Viterbi alignment
for the fertility-based alignment models.
For Model 2 (also for Model 1 as a special case), we obtain
</bodyText>
<equation confidence="0.997408875">
ˆaJ1 = argmax Pr(f J 1, aJ 1  |eI 1) (31)
aJ 1
= argmax I p(J  |I) · � J [p(aj  |j,I) · p(fj  |eaj)] } (32)
aJ j=1
1
rr 11
largmax{p(aj  |j,I) · p(fj  |eaj)}J J (33)
LL aj j=1
</equation>
<bodyText confidence="0.999749411764706">
Hence, the maximization over the (I+1)J different alignments decomposes into J max-
imizations of (I + 1) lexicon probabilities. Similarly, the Viterbi alignment for Model 2
can be computed with a complexity of O(I · J).
Finding the optimal alignment for the HMM is more complicated than for Model 1
or Model 2. Using a dynamic programming approach, it is possible to obtain the Viterbi
alignment for the HMM with a complexity of O(I2 ·J) (Vogel, Ney, and Tillmann 1996).
For the refined alignment models, however, namely, Models 3, 4, 5, and 6, max-
imization over all alignments cannot be efficiently carried out. The corresponding
search problem is NP-complete (Knight 1990a). For short sentences, a possible so-
lution could be an A* search algorithm (Och, Ueffing, and Ney 2001). In the work
presented here, we use a more efficient greedy search algorithm for the best align-
ment, as suggested in Brown, Della Pietra, Della Pietra, and Mercer (1993). The basic
idea is to compute the Viterbi alignment of a simple model (such as Model 2 or HMM).
This alignment is then iteratively improved with respect to the alignment probability
of the refined alignment model. (For further details on the greedy search algorithm,
see Brown, Della Pietra, Della Pietra, and Mercer [1993].) In the Appendix, we present
methods for performing an efficient computation of this pseudo-Viterbi alignment.
</bodyText>
<sectionHeader confidence="0.980255" genericHeader="introduction">
3. Training
</sectionHeader>
<subsectionHeader confidence="0.991431">
3.1 EM Algorithm
</subsectionHeader>
<bodyText confidence="0.997384">
In this section, we describe our approach to determining the model parameters 0.
Every model has a specific set of free parameters. For example, the parameters 0 for
</bodyText>
<page confidence="0.984206">
29
</page>
<note confidence="0.396438">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.811428">
Model 4 consist of lexicon, alignment, and fertility parameters:
</bodyText>
<equation confidence="0.981883">
0 = {{p(f  |e)}, {p=1(∆j  |···)}, {p&gt;1(∆j  |···)},{p(φ  |e)},p1} (34)
</equation>
<bodyText confidence="0.995962166666667">
To train the model parameters 0, we use a maximum-likelihood approach, as described
in equation (4), by applying the EM algorithm (Baum 1972). The different models are
trained in succession on the same data; the final parameter values of a simpler model
serve as the starting point for a more complex model.
In the E-step of Model 1, the lexicon parameter counts for one sentence pair (e, f)
are calculated:
</bodyText>
<equation confidence="0.9877775">
c(f  |e;e,f) = � N(e, f) � Pr(a  |e,f) � 6(f,fj)6(e,eaj) (35)
e,f a j
</equation>
<bodyText confidence="0.647134">
Here, N(e, f) is the training corpus count of the sentence pair (f, e). In the M-step, the
lexicon parameters are computed:
</bodyText>
<equation confidence="0.995815">
� sc(f  |e; fs, es) (36)
p(f  |e) = Es,f c(f  |e; fs, es)
</equation>
<bodyText confidence="0.999958941176471">
Similarly, the alignment and fertility probabilities can be estimated for all other align-
ment models (Brown, Della Pietra, Della Pietra, and Mercer 1993). When bootstrapping
from a simpler model to a more complex model, the simpler model is used to weigh the
alignments, and the counts are accumulated for the parameters of the more complex
model.
In principle, the sum over all (I+ 1)J alignments has to be calculated in the E-step.
Evaluating this sum by explicitly enumerating all alignments would be infeasible.
Fortunately, Models 1 and 2 and HMM have a particularly simple mathematical form
such that the EM algorithm can be implemented efficiently (i.e., in the E-step, it is
possible to efficiently evaluate all alignments). For the HMM, this is referred to as the
Baum-Welch algorithm (Baum 1972).
Since we know of no efficient way to avoid the explicit summation over all align-
ments in the EM algorithm in the fertility-based alignment models, the counts are
collected only over a subset of promising alignments. For Models 3 to 6, we perform
the count collection only over a small number of good alignments. To keep the training
fast, we consider only a small fraction of all alignments. We compare three different
methods for using subsets of varying sizes:
</bodyText>
<listItem confidence="0.989819214285714">
• The simplest method is to perform Viterbi training using only the best
alignment found. As the Viterbi alignment computation itself is very
time consuming for Models 3 to 6, the Viterbi alignment is computed
only approximately, using the method described in Brown, Della Pietra,
Della Pietra, and Mercer (1993).
• Al-Onaizan et al. (1999) suggest using as well the neighboring
alignments of the best alignment found. (For an exact definition of the
neighborhood of an alignment, the reader is referred to the Appendix.)
• Brown, Della Pietra, Della Pietra, and Mercer (1993) use an even larger
set of alignments, including also the pegged alignments, a large set of
alignments with a high probability Pr(fJ 1, aJ1  |eI1). The method for
constructing these alignments (Brown, Della Pietra, Della Pietra, and
Mercer 1993) guarantees that for each lexical relationship in every
sentence pair, at least one alignment is considered.
</listItem>
<page confidence="0.972456">
30
</page>
<note confidence="0.500506">
Och and Ney Comparison of Statistical Alignment Models
</note>
<bodyText confidence="0.9997125">
In Section 6, we show that by using the HMM instead of Model 2 in bootstrap-
ping the fertility-based alignment models, the alignment quality can be significantly
improved. In the Appendix, we present an efficient training algorithm of the fertility-
based alignment models.
</bodyText>
<subsectionHeader confidence="0.999893">
3.2 Is Deficiency a Problem?
</subsectionHeader>
<bodyText confidence="0.999979611111111">
When using the EM algorithm on the standard versions of Models 3 and 4, we observe
that during the EM iterations more and more words are aligned with the empty word.
This results in a poor alignment quality, because too many words are aligned to the
empty word. This progressive increase in the number of words aligned with the empty
word does not occur when the other alignment models are used. We believe that this
is due to the deficiency of Model 3 and Model 4.
The use of the EM algorithm guarantees that the likelihood increases for each
iteration. This holds for both deficient and nondeficient models. For deficient models,
however, as the amount of deficiency in the model is reduced, the likelihood increases.
In Models 3 and 4 as defined in Brown, Della Pietra, Della Pietra, and Mercer (1993),
the alignment model for nonempty words is deficient, but the alignment model for
the empty word is nondeficient. Hence, the EM algorithm can increase likelihood by
simply aligning more and more words with the empty word.3
Therefore, we modify Models 3 and 4 slightly, such that the empty word also has
a deficient alignment model. The alignment probability is set to p(j  |i, J) = 1/J for each
source word aligned with the empty word. Another remedy, adopted in Och and Ney
(2000), is to choose a value for the parameter p1 of the empty-word fertility and keep
it fixed.
</bodyText>
<subsectionHeader confidence="0.99963">
3.3 Smoothing
</subsectionHeader>
<bodyText confidence="0.9936138">
To overcome the problem of overfitting on the training data and to enable the models
to cope better with rare words, we smooth the alignment and fertility probabilities. For
the alignment probabilities of the HMM (and similarly for Models 4 and 5), we perform
an interpolation with a uniform distribution p(i  |j,I) = 1/I using an interpolation
parameter α:
</bodyText>
<equation confidence="0.970597">
p&apos;(aj  |aj−1,I) = (1 − α) · p(aj  |aj−1,I) + α ·1 (37)
</equation>
<bodyText confidence="0.979859416666667">
For the fertility probabilities, we assume that there is a dependence on the number
of letters g(e) of e and estimate a fertility distribution p(φ  |g) using the EM algorithm.
Typically, longer words have a higher fertility. By making this assumption, the model
can learn that the longer words usually have a higher fertility than shorter words.
Using an interpolation parameter β, the fertility distribution is then computed as
� p&apos;(φ  |e)β0 (e)) · p(φ  |e) + 0 (e) · p(φ  |g(e)) (38)
Here, n(e) denotes the frequency of e in the training corpus. This linear interpolation
ensures that for frequent words (i.e., n(e) » β), the specific distribution p(φ  |e) dom-
inates, and that for rare words (i.e., n(e) « β), the general distribution p(φ  |g(e))
dominates.
The interpolation parameters α and β are determined in such a way that the
alignment quality on held-out data is optimized.
</bodyText>
<footnote confidence="0.9673345">
3 This effect did not occur in Brown, Della Pietra, Della Pietra, and Mercer (1993), as Models 3 and 4
were not trained directly.
</footnote>
<page confidence="0.999594">
31
</page>
<note confidence="0.437884">
Computational Linguistics Volume 29, Number 1
</note>
<subsectionHeader confidence="0.995337">
3.4 Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.9653164">
A conventional bilingual dictionary can be considered an additional knowledge source
that can be used in training. We assume that the dictionary is a list of word strings
(e, f). The entries for each language can be a single word or an entire phrase.
To integrate a dictionary into the EM algorithm, we compare two different
methods:
</bodyText>
<listItem confidence="0.972041">
• Brown, Della Pietra, Della Pietra, Goldsmith, et al. (1993) developed a
multinomial model for the process of constructing a dictionary (by a
human lexicographer). By applying suitable simplifications, the method
boils down to adding every dictionary entry (e, f) to the training corpus
with an entry-specific count called effective multiplicity, expressed as
µ(e,f):
</listItem>
<equation confidence="0.822597">
µ(e, f) = 1(e) - eλp) ·p(f|) (39)
</equation>
<bodyText confidence="0.974815666666667">
In this section, A(e) is an additional parameter describing the size of the
sample that is used to estimate the model p(f  |e). This count is then
used instead of N(e, f) in the EM algorithm as shown in equation (35).
</bodyText>
<listItem confidence="0.9986965">
• Och and Ney (2000) suggest that the effective multiplicity of a dictionary
entry be set to a large value µ+ » 1 if the lexicon entry actually occurs
in one of the sentence pairs of the bilingual corpus and to a low value
otherwise:
</listItem>
<equation confidence="0.9765325">
e f= I µ+ if e and f co-occur 40
µ (&apos;) l µ− otherwise ( )
</equation>
<bodyText confidence="0.998514">
As a result, only dictionary entries that indeed occur in the training
corpus have a large effect in training. The motivation behind this is to
avoid a deterioration of the alignment as a result of out-of-domain
dictionary entries. Every entry in the dictionary that does co-occur in the
training corpus can be assumed correct and should therefore obtain a
high count. We set µ− = 0.
</bodyText>
<sectionHeader confidence="0.986241" genericHeader="method">
4. Symmetrization
</sectionHeader>
<bodyText confidence="0.999944266666667">
In this section, we describe various methods for performing a symmetrization of our
directed statistical alignment models by applying a heuristic postprocessing step that
combines the alignments in both translation directions (source to target, target to
source).
The baseline alignment model does not allow a source word to be aligned with
more than one target word. Therefore, lexical correspondences like that of the German
compound word Zahnarzttermin with the English dentist’s appointment cause problems,
because a single source word must be mapped to two or more target words. Therefore,
the resulting Viterbi alignment of the standard alignment models has a systematic loss
in recall.
To solve this problem, we perform training in both translation directions (source to
target, target to source). As a result, we obtain two alignments aJ1 and bI1 for each pair
of sentences in the training corpus. Let A1 = {(aj, j)  |aj &gt; 01 and A2 = {(i, bi)  |bi &gt; 01
denote the sets of alignments in the two Viterbi alignments. To increase the quality
of the alignments, we combine A1 and A2 into one alignment matrix A using the
</bodyText>
<page confidence="0.992334">
32
</page>
<note confidence="0.745414">
Och and Ney Comparison of Statistical Alignment Models
</note>
<bodyText confidence="0.866903">
following combination methods:
</bodyText>
<listItem confidence="0.999989">
• Intersection: A = A1 ∩ A2.
• Union: A = A1 ∪ A2.
• Refined method: In a first step, the intersection A = A1 ∩ A2 is
</listItem>
<bodyText confidence="0.5144284">
determined. The elements of this intersection result from both Viterbi
alignments and are therefore very reliable. Then, we extend the
alignment A iteratively by adding alignments (i, j) occurring only in the
alignment A1 or in the alignment A2 if neither fj nor ei has an alignment
in A, or if both of the following conditions hold:
</bodyText>
<listItem confidence="0.99465025">
• The alignment (i, j) has a horizontal neighbor (i − 1,j), (i + 1,j)
or a vertical neighbor (i, j − 1), (i, j + 1) that is already in A.
• The set A ∪ {(i, j)} does not contain alignments with both
horizontal and vertical neighbors.
</listItem>
<bodyText confidence="0.9947062">
Obviously, the intersection of the two alignments yields an alignment consisting of
only one-to-one alignments with a higher precision and a lower recall than either
one separately. The union of the two alignments yields a higher recall and a lower
precision of the combined alignment than either one separately. Whether a higher
precision or a higher recall is preferred depends on the final application for which
the word alignment is intended. In applications such as statistical machine translation
(Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000),
so an alignment union would probably be chosen. In lexicography applications, we
might be interested in alignments with a very high precision obtained by performing
an alignment intersection.
</bodyText>
<sectionHeader confidence="0.991862" genericHeader="method">
5. Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.999979866666667">
In the following, we present an annotation scheme for single-word-based alignments
and a corresponding evaluation criterion.
It is well known that manually performing a word alignment is a complicated
and ambiguous task (Melamed 1998). Therefore, in performing the alignments for
the research presented here, we use an annotation scheme that explicitly allows for
ambiguous alignments. The persons conducting the annotation are asked to specify
alignments of two different kinds: an S (sure) alignment, for alignments that are un-
ambiguous, and a P (possible) alignment, for ambiguous alignments. The P label is
used especially to align words within idiomatic expressions and free translations and
missing function words (S ⊆ P).
The reference alignment thus obtained may contain many-to-one and one-to-many
relationships. Figure 2 shows an example of a manually aligned sentence with S and
P labels.
The quality of an alignment A = {(j, aj)  |aj &gt; 0} is then computed by appropriately
redefined precision and recall measures:
</bodyText>
<equation confidence="0.997932">
recall = |A ∩ S ||A ∩ P|
|S |, precision = (41)
|A|
</equation>
<bodyText confidence="0.9882445">
and the following alignment error rate (AER), which is derived from the well-known
F-measure:
</bodyText>
<equation confidence="0.994069">
AER(S,P;A) = 1 − |A ∩ S |+ |A ∩ P |(42)
|A |+ |S|
</equation>
<page confidence="0.992899">
33
</page>
<figure confidence="0.726896">
Computational Linguistics Volume 29, Number 1
</figure>
<figureCaption confidence="0.961923">
Figure 2
</figureCaption>
<bodyText confidence="0.995456">
A manual alignment with S (filled squares) and P (unfilled squares) connections.
These definitions of precision, recall and the AER are based on the assumption
that a recall error can occur only if an S alignment is not found and a precision error
can occur only if the found alignment is not even P.
The set of sentence pairs for which the manual alignment is produced is randomly
selected from the training corpus. It should be emphasized that all the training of the
models is performed in a completely unsupervised way (i.e., no manual alignments
are used). From this point of view, there is no need to have a test corpus separate from
the training corpus.
Typically, the annotation is performed by two human annotators, producing sets
S1, P1, S2, P2. To increase the quality of the resulting reference alignment, the anno-
tators are presented with the mutual errors and asked to improve their alignments
where possible. (Mutual errors of the two annotators A and B are the errors in the
alignment of annotator A if we assume the alignment of annotator B as reference and
the errors in the alignment of annotator B if we assume the alignment of annotator A
as reference.) From these alignments, we finally generate a reference alignment that
contains only those S connections on which both annotators agree and all P connec-
tions from both annotators. This can be accomplished by forming the intersection of
the sure alignments (S = S1∩S2) and the union of the possible alignments (P = P1∪P2),
respectively. By generating the reference alignment in this way, we obtain an alignment
error rate of 0 percent when we compare the S alignments of every single annotator
with the combined reference alignment.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="method">
6. Experiments
</sectionHeader>
<bodyText confidence="0.9983295">
We present in this section results of experiments involving the Verbmobil and Hansards
tasks. The Verbmobil task (Wahlster 2000) is a (German-English) speech translation task
</bodyText>
<page confidence="0.994328">
34
</page>
<note confidence="0.85528">
Och and Ney Comparison of Statistical Alignment Models
</note>
<tableCaption confidence="0.7354805">
Table 2
Corpus characteristics of the Verbmobil task.
</tableCaption>
<table confidence="0.994467777777778">
German English
Training corpus Sentences 34,446 ≈ 34K
Words 329,625 343,076
Vocabulary 5,936 3,505
Singletons 2,600 1,305
Bilingual dictionary Entries 4,404
Words 4,758 5,543
Test corpus Sentences 354
Words 3,233 3,109
</table>
<tableCaption confidence="0.848282">
Table 3
Corpus characteristics of the Hansards task.
</tableCaption>
<table confidence="0.990098777777778">
French English
Training corpus Sentences 1470K
Words 24.33M 22.16M
Vocabulary 100,269 78,332
Singletons 40,199 31,319
Bilingual dictionary Entries 28,701
Words 28,702 30,186
Test corpus Sentences 500
Words 8,749 7,946
</table>
<bodyText confidence="0.998362285714286">
in the domain of appointment scheduling, travel planning, and hotel reservation. The
bilingual sentences used in training are correct transcriptions of spoken dialogues.
However, they include spontaneous speech effects such as hesitations, false starts, and
ungrammatical phrases. The French-English Hansards task consists of the debates in
the Canadian parliament. This task has a very large vocabulary of about 100,000 French
words and 80,000 English words.4
Statistics for the two corpora are shown in Tables 2 and 3. The number of running
words and the vocabularies are based on full-form words and the punctuation marks.
We produced smaller training corpora by randomly choosing 500, 2,000 and 8,000
sentences from the Verbmobil task and 500, 8,000, and 128,000 sentences from the
Hansards task.
For both tasks, we manually aligned a randomly chosen subset of the training
corpus. From this subset of the corpus, the first 100 sentences are used as the de-
velopment corpus to optimize the model parameters that are not trained via the EM
</bodyText>
<footnote confidence="0.780544">
4 We do not use the Blinker annotated corpus described in Melamed (1998), since the domain is very
special (the Bible) and a different annotation methodology is used.
</footnote>
<page confidence="0.968912">
35
</page>
<table confidence="0.479916">
Computational Linguistics Volume 29, Number 1
</table>
<tableCaption confidence="0.997191">
Table 4
</tableCaption>
<table confidence="0.998499166666667">
Comparison of alignment error rate percentages for various training schemes (Verbmobil task;
Dice+C: Dice coefficient with competitive linking).
Size of training corpus
Model Training scheme 0.5K 2K 8K 34K
Dice 28.4 29.2 29.1 29.0
Dice+C 21.5 21.8 20.1 20.4
Model 1 15 19.3 19.0 17.8 17.0
Model 2 1525 27.7 21.0 15.8 13.5
HMM 15H5 19.1 15.4 11.4 9.2
Model 3 152533 25.8 18.4 13.4 10.3
15H533 18.1 14.3 10.5 8.1
Model 4 15253343 23.4 14.6 10.0 7.7
15H543 17.3 11.7 9.1 6.5
15H53343 16.8 11.7 8.4 6.3
Model 5 15H54353 17.3 11.4 8.7 6.2
15H5334353 16.9 11.8 8.5 5.8
Model 6 15H54363 17.2 11.3 8.8 6.1
15H5334363 16.4 11.7 8.0 5.7
</table>
<bodyText confidence="0.992531384615385">
algorithm (e.g., the smoothing parameters). The remaining sentences are used as the
test corpus.
The sequence of models used and the number of training iterations used for each
model is referred to in the following as the training scheme. Our standard train-
ing scheme on Verbmobil is 15H5334363. This notation indicates that five iterations
of Model 1, five iterations of HMM, three iterations of Model 3, three iterations
of Model 4, and three iterations of Model 6 are performed. On Hansards, we use
15H10334363. This training scheme typically gives very good results and does not lead
to overfitting. We use the slightly modified versions of Model 3 and Model 4 described
in Section 3.2 and smooth the fertility and the alignment parameters. In the E-step of
the EM algorithm for the fertility-based alignment models, we use the Viterbi align-
ment and its neighborhood. Unless stated otherwise, no bilingual dictionary is used
in training.
</bodyText>
<subsectionHeader confidence="0.999091">
6.1 Models and Training Schemes
</subsectionHeader>
<bodyText confidence="0.9998575">
Tables 4 and 5 compare the alignment quality achieved using various models and
training schemes. In general, we observe that the refined models (Models 4, 5, and 6)
yield significantly better results than the simple Model 1 or Dice coefficient. Typically,
the best results are obtained with Model 6. This holds across a wide range of sizes
for the training corpus, from an extremely small training corpus of only 500 sentences
up to a training corpus of 1.5 million sentences. The improvement that results from
using a larger training corpus is more significant, however, if more refined models are
used. Interestingly, even on a tiny corpus of only 500 sentences, alignment error rates
under 30% are achieved for all models, and the best models have error rates somewhat
under 20%.
We observe that the alignment quality obtained with a specific model heavily
depends on the training scheme that is used to bootstrap the model.
</bodyText>
<page confidence="0.998139">
36
</page>
<note confidence="0.896349">
Och and Ney Comparison of Statistical Alignment Models
</note>
<tableCaption confidence="0.998732">
Table 5
</tableCaption>
<table confidence="0.995828666666666">
Comparison of alignment error rate percentages for various training schemes (Hansards task;
Dice+C: Dice coefficient with competitive linking).
Size of training corpus
Model Training scheme 0.5K 8K 128K 1.47M
Dice 50.9 43.4 39.6 38.9
Dice+C 46.3 37.6 35.0 34.0
Model 1 15 40.6 33.6 28.6 25.9
Model 2 1525 46.7 29.3 22.0 19.5
HMM 15H5 26.3 23.3 15.0 10.8
Model 3 152533 43.6 27.5 20.5 18.0
15H533 27.5 22.5 16.6 13.2
Model 4 15253343 41.7 25.1 17.3 14.1
15H53343 26.1 20.2 13.1 9.4
15H543 26.3 21.8 13.3 9.3
Model 5 15H54353 26.5 21.5 13.7 9.6
15H5334353 26.5 20.4 13.4 9.4
Model 6 15H54363 26.0 21.6 12.8 8.8
15H5334363 25.9 20.3 12.5 8.7
</table>
<figureCaption confidence="0.595396">
Figure 3
</figureCaption>
<bodyText confidence="0.7667185">
Comparison of alignment error rate (in percent) for Model 1 and Dice coefficient (left: 34K
Verbmobil task, right: 128K Hansards task).
</bodyText>
<subsectionHeader confidence="0.997699">
6.2 Heuristic Models versus Model 1
</subsectionHeader>
<bodyText confidence="0.999879727272727">
We pointed out in Section 2 that from a theoretical viewpoint, the main advantage
of statistical alignment models in comparison to heuristic models is the well-founded
mathematical theory that underlies their parameter estimation. Tables 4 and 5 show
that the statistical alignment models significantly outperform the heuristic Dice coef-
ficient and the heuristic Dice coefficient with competitive linking (Dice+C). Even the
simple Model 1 achieves better results than the two Dice coefficient models.
It is instructive to analyze the alignment quality obtained in the EM training of
Model 1. Figure 3 shows the alignment quality over the iteration numbers of Model 1.
We see that the first iteration of Model 1 achieves significantly worse results than the
Dice coefficient, but by only the second iteration, Model 1 gives better results than the
Dice coefficient.
</bodyText>
<page confidence="0.998698">
37
</page>
<note confidence="0.52808">
Computational Linguistics Volume 29, Number 1
</note>
<tableCaption confidence="0.971232">
Table 6
Effect of using more alignments in training fertility models on alignment error rate (Verbmobil
task). Body of table presents error rate percentages.
</tableCaption>
<table confidence="0.999626">
Size of training corpus
Training scheme Alignment set 0.5K 2K 8K 34K
Viterbi 17.8 12.6 8.6 6.6
15H5334363 +neighbors 16.4 11.7 8.0 5.7
+pegging 16.4 11.2 8.2 5.7
Viterbi 24.1 16.0 11.6 8.6
1525334353 +neighbors 22.9 14.2 9.8 7.6
+pegging 22.0 13.3 9.7 6.9
</table>
<tableCaption confidence="0.989281333333333">
Table 7
Effect of using more alignments in training fertility models on alignment error rate (Hansards
task). Body of table presents error rate percentages.
</tableCaption>
<table confidence="0.999514">
Size of training corpus
Training scheme Alignment set 0.5K 8K 128K
Viterbi 25.8 20.3 12.6
15H10334363 +neighbors 25.9 20.3 12.5
+pegging 25.8 19.9 12.6
Viterbi 41.9 25.1 17.6
1525334353 +neighbors 41.7 24.8 16.1
+pegging 41.2 23.7 15.8
</table>
<subsectionHeader confidence="0.970891">
6.3 Model 2 versus HMM
</subsectionHeader>
<bodyText confidence="0.999965416666667">
An important result of these experiments is that the hidden Markov alignment model
achieves significantly better results than Model 2. We attribute this to the fact that the
HMM is a homogeneous first-order alignment model, and such models are able to
better represent the locality and monotonicity properties of natural languages. Both
models have the important property of allowing an efficient implementation of the
EM algorithm (Section 3). On the largest Verbmobil task, the HMM achieves an im-
provement of 3.8% over Model 2. On the largest Hansards task, the improvement is
8.7%. Interestingly, this advantage continues to hold after bootstrapping more refined
models. On Model 4, the improvement is 1.4% and 4.8%, respectively.
We conclude that it is important to bootstrap the refined alignment models with
good initial parameters. Obviously, if we use Model 2 for bootstrapping, we eventually
obtain a poor local optimum.
</bodyText>
<subsectionHeader confidence="0.999161">
6.4 The Number of Alignments in Training
</subsectionHeader>
<bodyText confidence="0.999811571428571">
In Tables 6 and 7, we compare the results obtained by using different numbers of
alignments in the training of the fertility-based alignment models. We compare the
three different approaches described in Section 3: using only the Viterbi alignment,
using in addition the neighborhood of the Viterbi alignment, and using the pegged
alignments. To reduce the training time, we restrict the number of pegged alignments
by using only those in which Pr(f, a I e) is not much smaller than the probability of the
Viterbi alignment. This reduces the training time drastically. For the large Hansards
</bodyText>
<page confidence="0.998647">
38
</page>
<note confidence="0.896391">
Och and Ney Comparison of Statistical Alignment Models
</note>
<tableCaption confidence="0.99758">
Table 8
</tableCaption>
<table confidence="0.983974666666667">
Computing time on the 34K Verbmobil task (on 600 MHz Pentium III machine).
Alignment set Seconds per iteration
Model 3 Model 4 Model 5
Viterbi 48.0 251.0 248.0
+neighbors 101.0 283.0 276.0
+pegging 129.0 3,348.0 3,356.0
</table>
<tableCaption confidence="0.661700333333333">
Table 9
Effect of smoothing on alignment error rate (Verbmobil task, Model 6). Body of table presents
error rate percentages.
</tableCaption>
<table confidence="0.9855135">
Size of training corpus
Smoothing method 0.5K 2K 8K 34K
None 19.7 14.9 10.9 8.3
Fertility 18.4 14.3 10.3 8.0
Alignment 16.8 13.2 9.1 6.4
Alignment and fertility 16.4 11.7 8.0 5.7
</table>
<bodyText confidence="0.997904785714286">
corpus, however, there still is an unacceptably large training time. Therefore, we report
the results for only up to 128,000 training sentences.
The effect of pegging strongly depends on the quality of the starting point used
for training the fertility-based alignment models. If we use Model 2 as the starting
point, we observe a significant improvement when we use the neighborhood align-
ments and the pegged alignments. If we use only the Viterbi alignment, the results are
significantly worse than using additionally the neighborhood of the Viterbi alignment.
If we use HMM as the starting point, we observe a much smaller effect. We conclude
that using more alignments in training is a way to avoid a poor local optimum.
Table 8 shows the computing time for performing one iteration of the EM algo-
rithm. Using a larger set of alignments increases the training time for Model 4 and
Model 5 significantly. Since using the pegging alignments yields only a moderate
improvement in performance, all following results are obtained by using the neigh-
borhood of the Viterbi alignment without pegging.
</bodyText>
<subsectionHeader confidence="0.999556">
6.5 Effect of Smoothing
</subsectionHeader>
<bodyText confidence="0.999966230769231">
Tables 9 and 10 show the effect on the alignment error rate of smoothing the alignment
and fertility probabilities. We observe a significant improvement when we smooth
the alignment probabilities and a minor improvement when we smooth the fertility
probabilities. An analysis of the alignments shows that smoothing the fertility proba-
bilities significantly reduces the frequently occurring problem of rare words forming
“garbage collectors” in that they tend to align with too many words in the other
language (Brown, Della Pietra, Della Pietra, Goldsmith, et al. 1993).
Without smoothing, we observe early overfitting: The alignment error rate in-
creases after the second iteration of HMM, as shown in Figure 4. On the Verbmobil
task, the best alignment error rate is obtained in the second iteration. On the Hansards
task, the best alignment error rate is obtained in the sixth iteration. In iterations sub-
sequent to the second on the Verbmobil task and the sixth on the Hansards task, the
alignment error rate increases significantly. With smoothing of the alignment param-
</bodyText>
<page confidence="0.995088">
39
</page>
<figure confidence="0.998243714285714">
Computational Linguistics Volume 29, Number 1
8.5
no smoothing
with smoothing
12.5
12
11.5
11
cc
w 10.5
10
9.5
2 4 6 8 10 12 14 16 18
HMM iterations
2 4 6 8 10 12 14 16 18
HMM iterations
19.5
19
18.5
18
17.5
CC
w 17
16.5
16
15.5
15
14.5
</figure>
<figureCaption confidence="0.995942">
Figure 4
</figureCaption>
<bodyText confidence="0.997959">
Overfitting on the training data with the hidden Markov alignment model using various
smoothing parameters (top: 34K Verbmobil task, bottom: 128K Hansards task).
</bodyText>
<page confidence="0.997128">
40
</page>
<note confidence="0.761018">
Och and Ney Comparison of Statistical Alignment Models
</note>
<tableCaption confidence="0.997345">
Table 10
</tableCaption>
<table confidence="0.933195625">
Effect of smoothing on alignment error rate (Hansards task, Model 6). Body of table presents
error rate percentages.
Size of training corpus
Smoothing method 0.5K 8K 128K 1470K
None 28.6 23.3 13.3 9.5
Fertility 28.3 22.5 12.7 9.3
Alignment 26.5 21.2 13.0 8.9
Alignment and fertility 25.9 20.3 12.5 8.7
</table>
<tableCaption confidence="0.994124">
Table 11
</tableCaption>
<table confidence="0.974166333333333">
Effect of word classes on alignment error rate (Verbmobil task). Body of table presents error
rate percentages.
Size of training corpus
Word classes 0.5K 2K 8K 34K
No 16.5 11.7 8.0 6.3
Yes 16.4 11.7 8.0 5.7
</table>
<tableCaption confidence="0.992466">
Table 12
</tableCaption>
<table confidence="0.984317666666667">
Effect of word classes on alignment error rate (Hansards task). Body of table presents error
rate percentages.
Size of training corpus
Word classes 0.5K 8K 128K 1470K
No 25.5 20.7 12.8 8.9
Yes 25.9 20.3 12.5 8.7
</table>
<bodyText confidence="0.9985245">
eters, we obtain a lower alignment error rate, overfitting occurs later in the process,
and its effect is smaller.
</bodyText>
<subsectionHeader confidence="0.998051">
6.6 Alignment Models Depending on Word Classes
</subsectionHeader>
<bodyText confidence="0.9999424">
Tables 11 and 12 show the effects of including a dependence on word classes in the
alignment model, as described in Section 2.3. The word classes are always trained
on the same subset of the training corpus as is used for the training of the align-
ment models. We observe no significant improvement in performance as a result
of including dependence on word classes when a small training corpus is used. A
possible reason for this lack of improvement is that either the word classes them-
selves or the resulting large number of alignment parameters cannot be estimated
reliably using a small training corpus. When a large training corpus is used, however,
there is a clear improvement in performance on both the Verbmobil and the Hansards
tasks.
</bodyText>
<subsectionHeader confidence="0.999702">
6.7 Using a Conventional Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.99809">
Tables 13 and 14 show the effect of using a conventional bilingual dictionary in training
on the Verbmobil and Hansards tasks, respectively. We compare the two methods for
using the dictionary described in Section 3.4. We observe that the method with a fixed
</bodyText>
<page confidence="0.998918">
41
</page>
<note confidence="0.601227">
Computational Linguistics Volume 29, Number 1
</note>
<tableCaption confidence="0.980282333333333">
Table 13
Effect of using a conventional dictionary on alignment error rate (Verbmobil task). Body of
table presents error rate percentages.
</tableCaption>
<table confidence="0.999597285714286">
Size of training corpus
Bilingual dictionary 0.5K 2K 8K 34K
No 16.4 11.7 8.0 5.7
Yes/p var. 10.9 9.0 6.9 5.1
Yes/p+ = 8 9.7 7.6 6.0 5.1
Yes/p+ = 16 10.0 7.8 6.0 4.6
Yes/p+ = 32 10.4 8.5 6.4 4.7
</table>
<tableCaption confidence="0.993058">
Table 14
Effect of using a conventional dictionary on alignment error rate (Hansards task). Body of
table presents error rate percentages.
</tableCaption>
<table confidence="0.986142428571429">
Size of training corpus
Bilingual dictionary 0.5K 8K 128K 1470K
No 25.9 20.3 12.5 8.7
Yes/p var. 23.3 18.3 12.3 8.6
Yes/p+ = 8 22.7 18.5 12.2 8.6
Yes/p+ = 16 23.1 18.7 12.1 8.6
Yes/p+ = 32 24.9 20.2 11.7 8.3
</table>
<bodyText confidence="0.9986385">
threshold of µ+ = 16 gives the best results. The method with a varying µ gives worse
results, but this method has one fewer parameter to be optimized on held-out data.
On small corpora, there is an improvement of up to 6.7% on the Verbmobil task
and 3.2% on the Hansards task, but when a larger training corpus is used, the im-
provements are reduced to 1.1% and 0.4%, respectively. Interestingly, the amount
of the overall improvement contributed by the use of a conventional dictionary is
small compared to the improvement achieved through the use of better alignment
models.
</bodyText>
<subsectionHeader confidence="0.995172">
6.8 Generalized Alignments
</subsectionHeader>
<bodyText confidence="0.999917533333333">
In this section, we compare the results obtained using different translation directions
and using the symmetrization methods described in Section 4. Tables 15 and 16 show
precision, recall, and alignment error rate for the last iteration of Model 6 for both
translation directions. In this experiment, we use the conventional dictionary as well.
Particularly for the Verbmobil task, with the language pair German-English, we ob-
serve that for German as the source language the alignment error rate is much higher
than for English as source language. A possible reason for this difference in the align-
ment error rates is that the baseline alignment representation as a vector aJ1 does not
allow German word compounds (which occur frequently) to be aligned with more
than one English word.
The effect of merging alignments by forming the intersection, the union, or the
refined combination of the Viterbi alignments in both translation directions is shown in
Tables 17 and 18. Figure 5 shows the corresponding precision/recall graphs. By using
the refined combination, we can increase precision and recall on the Hansards task. The
lowest alignment error rate on the Hansards task is obtained by using the intersection
</bodyText>
<page confidence="0.998505">
42
</page>
<note confidence="0.922206">
Och and Ney Comparison of Statistical Alignment Models
</note>
<tableCaption confidence="0.996326">
Table 15
</tableCaption>
<table confidence="0.9587875">
Effect of training corpus size and translation direction on precision, recall, and alignment error
rate (Verbmobil task + dictionary). All figures are percentages.
English → German German → English
Corpus size Precision Recall AER Precision Recall AER
0.5K 87.6 93.1 10.0 77.9 80.3 21.1
2K 90.5 94.4 7.8 88.1 88.1 11.9
8K 92.7 95.7 6.0 90.2 89.1 10.3
34K 94.6 96.3 4.6 92.5 89.5 8.8
</table>
<tableCaption confidence="0.995019">
Table 16
</tableCaption>
<table confidence="0.8735775">
Effect of training corpus size and translation direction on precision, recall, and alignment error
rate (Hansards task + dictionary). All figures are percentages.
English → French French → English
Corpus size Precision Recall AER Precision Recall AER
0.5K 73.0 83.8 23.1 68.5 79.1 27.8
8K 77.0 88.9 18.7 76.0 88.5 19.5
128K 84.5 93.5 12.1 84.6 93.3 12.2
1470K 89.4 94.7 8.6 89.1 95.2 8.6
</table>
<tableCaption confidence="0.991883">
Table 17
</tableCaption>
<table confidence="0.990158875">
Effect of alignment combination on precision, recall, and alignment error rate (Verbmobil task
+ dictionary). All figures are percentages.
Intersection Union Refined method
Corpus size Precision Recall AER Precision Recall AER Precision Recall AER
0.5K 97.5 76.8 13.6 74.8 96.1 16.9 87.8 92.9 9.9
2K 97.2 85.6 8.6 84.1 96.9 10.6 91.3 94.2 7.4
8K 97.5 86.6 8.0 87.0 97.7 8.5 92.8 96.0 5.8
34K 98.1 87.6 7.2 90.6 98.4 6.0 94.0 96.9 4.7
</table>
<tableCaption confidence="0.9936">
Table 18
</tableCaption>
<table confidence="0.95946575">
Effect of alignment combination on precision, recall, and alignment error rate (Hansards task +
dictionary). All figures are percentages.
Intersection Union Refined method
Corpus size Precision Recall AER Precision Recall AER Precision Recall AER
0.5K 91.5 71.3 18.7 63.4 91.6 29.0 75.5 84.9 21.1
8K 95.6 82.8 10.6 68.2 94.4 24.2 83.3 90.0 14.2
128K 96.7 90.0 6.3 77.8 96.9 16.1 89.4 94.4 8.7
1470K 96.8 92.3 5.2 84.2 97.6 11.3 91.5 95.5 7.0
</table>
<page confidence="0.995099">
43
</page>
<figure confidence="0.958993238095238">
Computational Linguistics Volume 29, Number 1
Figure 5
Effect of various symmetrization methods on precision and recall for different training corpus
sizes (top: Verbmobil task, bottom: Hansards task).
95
90
85
80
75
70
75
80
85
precision
90
95
100
70
75
80
85
precision
90
95
100
e &gt;g
g-&gt;e
intersection
union
refined --•-•-•
100
95
90
85
80
75
100
e &gt;f
f-&gt;e ----x--
intersection
union --id—
refined --.-
</figure>
<page confidence="0.99057">
44
</page>
<note confidence="0.818423">
Och and Ney Comparison of Statistical Alignment Models
</note>
<bodyText confidence="0.9857015">
method. By forming a union or intersection of the alignments, we can obtain very high
recall or precision values on both the Hansards task and the Verbmobil task.
</bodyText>
<subsectionHeader confidence="0.998498">
6.9 Effect of Alignment Quality on Translation Quality
</subsectionHeader>
<bodyText confidence="0.999947666666667">
Alignment models similar to those studied in this article have been used as a start-
ing point for refined phrase-based statistical machine translation systems (Alshawi,
Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000). In Och
and Ney (2000), the overall result of the experimental evaluation has been that an
improved alignment quality yields an improved subjective quality of the statistical
machine translation system as well.
</bodyText>
<sectionHeader confidence="0.886223" genericHeader="method">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9999735">
In this article, we have discussed in detail various statistical and heuristic word align-
ment models and described various modifications and extensions to models known in
the literature. We have developed a new statistical alignment model (Model 6) that has
yielded the best results among all the models we considered in the experiments we
have conducted. We have presented two methods for including a conventional bilin-
gual dictionary in training and described heuristic symmetrization algorithms that
combine alignments in both translation directions possible between two languages,
producing an alignment with a higher precision, a higher recall, or an improved align-
ment error rate.
We have suggested measuring the quality of an alignment model using the quality
of the Viterbi alignment compared to that achieved in a manually produced reference
alignment. This quality measure has the advantage of automatic evaluation. To pro-
duce the reference alignment, we have used a refined annotation scheme that reduces
the problems and ambiguities associated with the manual construction of a word
alignment.
We have performed various experiments to assess the effect of different alignment
models, training schemes, and knowledge sources. The key results of these experi-
ments are as follows:
</bodyText>
<listItem confidence="0.997668142857143">
• Statistical alignment models outperform the simple Dice coefficient.
• The best results are obtained with our Model 6. In general, very
important ingredients of a good model seem to be a first-order
dependence between word positions and a fertility model.
• Smoothing and symmetrization have a significant effect on the alignment
quality achieved by a particular model.
• The following methods have only a minor effect on the quality of
alignment achieved by a particular model:
• adding entries of a conventional bilingual dictionary to the
training data.
• making the alignment models dependent on word classes (as in
Models 4 and 5).
• increasing the number of alignments used in the approximation
of the EM algorithm for the fertility-based alignment models.
</listItem>
<bodyText confidence="0.69236">
Further improvements in alignments are expected to be produced through the
adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment
</bodyText>
<page confidence="0.99723">
45
</page>
<note confidence="0.704062">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.9994707">
models based on word groups rather than single words (Och, Tillmann, and Ney
1999). The use of models that explicitly deal with the hierarchical structures of natural
language is very promising (Wu 1996; Yamada and Knight 2001).
We plan to develop structured models for the lexicon, alignment, and fertility prob-
abilities using maximum-entropy models. This is expected to allow an easy integration
of more dependencies, such as in a second-order alignment model, without running
into the problem of the number of alignment parameters getting unmanageably large.
Furthermore, it will be important to verify the applicability of the statistical align-
ment models examined in this article to less similar language pairs such as Chinese-
English and Japanese-English.
</bodyText>
<sectionHeader confidence="0.755725" genericHeader="method">
Appendix: Efficient Training of Fertility-Based Alignment Models
</sectionHeader>
<bodyText confidence="0.995119888888889">
In this Appendix, we describe some methods for efficient training of fertility-based
alignment models. The core idea is to enumerate only a small subset of good align-
ments in the E-step of the EM algorithm instead of enumerating all (I + 1)J align-
ments. This small subset of alignments is the set of neighboring alignments of the
best alignment that can be found by a greedy search algorithm. We use two operators
to transform alignments: The move operator m[i,j](a) changes aj := i, and the swap
operator s[j1,j2](a) exchanges aj1 and aj2. The neighborhood N(a) of an alignment a is
then defined as the set of all alignments that differ by one move or one swap from
alignment a:
</bodyText>
<equation confidence="0.965562">
N(a) = {a&apos; : Eli,j : a&apos; = m[i,j](a) V Elj1,j2 : a&apos; = s[j1,j2](a)} (43)
</equation>
<bodyText confidence="0.991869333333333">
For one step of the greedy search algorithm, we define the following hill-climbing
operator (for Model 3), which yields for an alignment a the most probable alignment
b(a) in the neighborhood N(a):
</bodyText>
<equation confidence="0.981771">
b(a) = argmax p3(a&apos;  |e, f) (44)
a&apos;EN(a)
</equation>
<bodyText confidence="0.999021">
Similarly, we define a hill-climbing operator for the other alignment models.
</bodyText>
<subsectionHeader confidence="0.973192">
Straightforward Implementation
</subsectionHeader>
<bodyText confidence="0.999314">
A straightforward count collection procedure for a sentence pair (f,e) following the
description in Brown, Della Pietra, Della Pietra, and Mercer (1993) is as follows:5
</bodyText>
<listItem confidence="0.811156666666667">
1. Calculate the Viterbi alignment of Model 2: a0 := argmaxa p2(f, a  |e),
n := 0.
2. While in the neighborhood N(an) an alignment a&apos; exists with
</listItem>
<figure confidence="0.568648833333333">
p3(a&apos;  |e, f) &gt; p3(an  |e, f):
(a) Set an+1 to the best alignment in the neighborhood.
(b) n := n + 1.
3. Calculate
�s := Pr(f,a  |e) (45)
aEN(an)
</figure>
<footnote confidence="0.4422095">
5 To simplify the description, we ignore the process known as pegging, which generates a bigger number
of alignments considered in training.
</footnote>
<page confidence="0.97949">
46
</page>
<figure confidence="0.836633">
Och and Ney Comparison of Statistical Alignment Models
4. For each alignment a in the neighborhood N(an)
(a) Calculate
</figure>
<equation confidence="0.855855714285714">
p := Pr(a  |e,f) (46)
Pr(f,a  |e) = (47)
s
(b) For each j := 1 to J: Increase alignment counts
c(j  |aj,m,l;e,f) := c(j  |aj,m,l;e,f) + p (48)
(c) For each i := 1 to I: Increase the fertility counts with p:
c(Oi  |ei; e, f) := c(Oi  |ei; e, f) + p (49)
</equation>
<bodyText confidence="0.717368">
(d) Increase the counts for p1:
</bodyText>
<equation confidence="0.548825">
c(1; e, f) := c(1; e, f) + p · O0 (50)
</equation>
<bodyText confidence="0.998394333333333">
A major part of the time in this procedure is spent on calculating the probability
Pr(a&apos;  |e, f) of an alignment a&apos;. In general, this takes about (I + J) operations. Brown,
Della Pietra, Della Pietra, and Mercer (1993) describe a method for obtaining Pr(a&apos; |
e, f) incrementally from Pr(a  |e, f) if alignment a differs only by moves or swaps from
alignment a&apos;. This method results in a constant number of operations that is sufficient
to calculate the score of a move or the score of a swap.
</bodyText>
<sectionHeader confidence="0.595216" genericHeader="method">
Refined Implementation: Fast Hill Climbing
</sectionHeader>
<bodyText confidence="0.9999495">
Analyzing the training program reveals that most of the time is spent on the compu-
tation of the costs of moves and swaps. To reduce the number of operations required
in such computation, these values are cached in two matrices. We use one matrix for
the scores of a move aj := i:
</bodyText>
<equation confidence="0.9838595">
Mzj = Pr(m[i,j](a)  |e,f) · (1 − S(aj,i)) (51)
Pr(a  |e, f)
</equation>
<bodyText confidence="0.991429">
and an additional matrix for the scores of a swap of aj and aj�:
</bodyText>
<equation confidence="0.958595666666667">
Pr(s[j,j (a)  |e,f)
Pr(a  |e, f)
0 otherwise
</equation>
<bodyText confidence="0.999962333333333">
During the hill climbing, it is sufficient, after making a move or a swap, to update
only those rows or columns in the matrix that are affected by the move or swap. For
example, when performing a move aj := i, it is necessary to
</bodyText>
<listItem confidence="0.999678666666667">
• update in matrix M the columns j~ with aj� = aj or aj, = i.
• update in matrix M the rows aj and i.
• update in matrix S the rows and the columns j~ with aj� = aj or aj, = i.
</listItem>
<bodyText confidence="0.988897">
Similar updates have to be performed after a swap. In the count collection (step 3), it
is possible to use the same matrices as obtained in the last hill-climbing step.
By restricting in this way the number of matrix entries that need to be updated,
it is possible to reduce the number of operations in hill climbing by about one order
of magnitude.
</bodyText>
<equation confidence="0.989778">
Sjj, = I
· (1 − S(aj,aj&apos;)) if j &lt;j&apos; (52)
</equation>
<page confidence="0.998564">
47
</page>
<note confidence="0.8339135">
Computational Linguistics Volume 29, Number 1
Refined Implementation: Fast Count Collection
</note>
<bodyText confidence="0.9999146">
The straightforward algorithm given for performing the count collection has the dis-
advantage of requiring that all alignments in the neighborhood of alignment a be
enumerated explicitly. In addition, it is necessary to perform a loop over all targets
and a loop over all source positions to update the lexicon/alignment and the fertil-
ity counts. To perform the count collection in an efficient way, we use the fact that
the alignments in the neighborhood N(a) are very similar. This allows the sharing of
many operations in the count collection process.
To efficiently obtain the alignment and lexicon probability counts, we introduce the
following auxiliary quantities that use the move and swap matrices that are available
after performing the hill climbing described above:
</bodyText>
<listItem confidence="0.969473">
• probability of all alignments in the neighborhood N(a):
</listItem>
<equation confidence="0.9760005">
Pr(N (a)  |e, f) = E Pr(a&apos;  |e, f) (53)
a&apos;EN(a)
� �
E E
= Pr(a  |e, f) · �1 + Mij + Sjj&apos; �(54)
i,j j,j&apos;
</equation>
<listItem confidence="0.877492">
• probability of all alignments in the neighborhood N(a) that differ in
position j from alignment a:
</listItem>
<equation confidence="0.997718833333333">
Pr(Nj(a)  |e, f) = E Pr(a&apos;  |e,f)(1 − δ(aj,a&apos;j)) (55)
a&apos;EN(a)
� �
�E E
= Pr(a  |e,f) Mij + (Sjj� + Sj,j) �(56)
i j�
</equation>
<bodyText confidence="0.557346">
For the alignment counts c(j  |i; e, f) and the lexicon counts c(f  |e; e, f), we have
</bodyText>
<equation confidence="0.983673333333333">
1Pr(aPr(N(a)  |e,f)−Pr(Nj(a)  |e,f) if i=ajc(j  |i; e, f) =  |e,f) Mi.+Ej, b aj,,i 5j,+Sj,� (57)(� , (1 ) (j� 11)) � i#aj
c(f  |e;e,f) = E E c(j  |i;e,f) · δ(f,fj) · δ(e,ei) (58)
i j
</equation>
<bodyText confidence="0.9999275">
To obtain the fertility probability counts and the count for p1 efficiently, we intro-
duce the following auxiliary quantities:
</bodyText>
<listItem confidence="0.971206">
• probability of all alignments that have an increased fertility for position i:
</listItem>
<equation confidence="0.910808">
� �
Pr(N+1(a)  |e,f) = Pr(a  |f,e) E(1 − δ(aj,i)) · Mij �(59)
j
</equation>
<listItem confidence="0.954147">
• probability of all alignments that have a decreased fertility for position i:
</listItem>
<equation confidence="0.927246333333333">
� �
Pr(N−1(a)  |e, f) = Pr(a  |e, f) E δ(aj, i) E Mi&apos;j �(60)
j i�
</equation>
<page confidence="0.994747">
48
</page>
<note confidence="0.746626">
Och and Ney Comparison of Statistical Alignment Models
</note>
<listItem confidence="0.936636">
• probability of all alignments that have an unchanged fertility for posi-
tion i:
</listItem>
<equation confidence="0.9985956">
Pr(N +0
i (a)  |e,f) = Pr(N(a)  |e,f)
− Pr(N +1
i (a)  |e,f) − Pr(N −1
i (a)  |e,f) (61)
</equation>
<bodyText confidence="0.9838055">
These quantities do not depend on swaps, since a swap does not change the fertilities
of an alignment. For the fertility counts, we have:
</bodyText>
<table confidence="0.646850285714286">
� �δ(e, ei) Pr(N+k
c(φ  |e;e,f) = k i (a)  |e, f)δ(φi + k, φ) (62)
i
For p1, we have:
� Pr(N +k
c(1; e, f) = 0 (a)  |e, f)(φ0 + k) (63)
k
</table>
<bodyText confidence="0.9983605">
Using the auxiliary quantities, a count collection algorithm can be formulated that
requires about O(max(I, J)2) operations. This is one order of magnitude faster than the
straightforward algorithm described above. In practice, we observe that the resulting
training is 10–20 times faster.
</bodyText>
<sectionHeader confidence="0.985392" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999800785714286">
This work has been partially supported as
part of the Verbmobil project (contract
number 01 IV 701 T4) by the German
Federal Ministry of Education, Science,
Research and Technology and as part of the
EuTrans project (project number 30268) by
the European Union. In addition, this work
has been partially supported by the
National Science Foundation under grant
no. IIS-9820687 through the 1999 Workshop
on Language Engineering, Center for
Language and Speech Processing, Johns
Hopkins University. All work for this paper
was done at RWTH Aachen.
</bodyText>
<sectionHeader confidence="0.990828" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997686211538461">
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John D. Lafferty, I. Dan
Melamed, David Purdy, Franz J. Och,
Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation. Final
Report, JHU Workshop. Available at
http://www.clsp.jhu.edu/ws99/projects
/mt/final report/mt-final-report.ps.
Alshawi, Hiyan, Srinivas Bangalore, and
Shona Douglas. 1998. Automatic
acquisition of hierarchical transduction
models for machine translation. In
COLING–ACL ’98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, volume 1,
pages 41–47, Montreal, Canada, August.
Baum, L. E. 1972. An inequality and
associated maximization technique in
statistical estimation for probabilistic
functions of Markov processes.
Inequalities, 3:1–8.
Berger, Adam L., Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Harry
Printz, and Lubos Ures. 1994. The
Candide system for machine translation.
In Proceedings of the ARPA Workshop on
Human Language Technology,
pages 157–162, Plainsboro, New Jersey,
March.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, M. J. Goldsmith,
J. Hajic, R. L. Mercer, and S. Mohanty.
1993. But dictionaries are data too. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 202–205,
Plainsboro, New Jersey, March.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–311.
Brown, Ralf D. 1997. Automated dictionary
extraction for “knowledge-free”
example-based translation. In Seventh
International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-97), pages 111–118, Santa Fe, New
Mexico, July.
Dagan, Ido, Kenneth W. Church, and
</reference>
<page confidence="0.975062">
49
</page>
<note confidence="0.358963">
Computational Linguistics Volume 29, Number 1
</note>
<reference confidence="0.999953983606557">
William A. Gale. 1993. Robust bilingual
word alignment for machine aided
translation. In Proceedings of the Workshop
on Very Large Corpora, pages 1–8,
Columbus, Ohio, June.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society,
Series B, 39(1):1–22.
Diab, Mona. 2000. An unsupervised method
for multilingual word sense tagging using
parallel corpora: A preliminary
investigation. In ACL-2000 Workshop on
Word Senses and Multilinguality, pages 1–9,
Hong Kong, October.
Dice, Lee R. 1945. Measures of the amount
of ecologic association between species.
Journal of Ecology, 26:297–302.
Garc´ıa-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative,
DP-based search algorithm for statistical
machine translation. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP’98), pages 1235–1238,
Sydney, Australia, November.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 228–235, Toulouse, France, July.
Huang, Jin-Xia and Key-Sun Choi. 2000.
Chinese-Korean word alignment based on
linguistic comparison. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 392–399, Hong Kong, October.
Ker, Sue J. and Jason S. Chang. 1997. A
class-based approach to word alignment.
Computational Linguistics, 23(2):313–343.
Kneser, Reinhard and Hermann Ney. 1993.
Improved clustering techniques for
class-based statistical language modelling.
In European Conference on Speech
Communication and Technology,
pages 973–976, Berlin, Germany,
September.
Knight, Kevin. 1999a. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607–615.
Knight, Kevin. 1999b. A Statistical MT
Tutorial Workbook. Available at
http://www.isi.edu/natural-language/
mt/wkbk.rtf.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The blinker
project. Technical Report 98-07, Institute
for Research in Cognitive Science,
Philadelphia.
Melamed, I. Dan. 2000. Models of
translational equivalence among words.
Computational Linguistics, 26(2):221–249.
Ney, Hermann, Sonja Nießen, Franz J. Och,
Hassan Sawaf, Christoph Tillmann, and
Stephan Vogel. 2000. Algorithms for
statistical translation of spoken language.
IEEE Transactions on Speech and Audio
Processing, 8(1):24–36.
Nießen, Sonja, Stephan Vogel, Hermann
Ney, and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In COLING-ACL ’98:
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 960–967, Montreal,
Canada, August.
Och, Franz J. 2000. Giza++: Training of
statistical translation models. Available at
http://www-i6.informatik.rwth-
aachen.de/∼och/software/GIZA++.html.
Och, Franz J. and Hermann Ney. 2000. A
comparison of alignment models for
statistical machine translation. In COLING
’00: The 18th International Conference on
Computational Linguistics, pages 1086–1090,
Saarbr¨ucken, Germany, August.
Och, Franz J., Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20–28, University of Maryland,
College Park, June.
Och, Franz J., Nicola Ueffing, and Hermann
Ney. 2001. An efficient A* search
algorithm for statistical machine
translation. In Data-Driven Machine
Translation Workshop, pages 55–62,
Toulouse, France, July.
Och, Franz J. and Hans Weber. 1998.
Improving statistical natural language
translation with categories and rules. In
COLING-ACL ’98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 985–989,
Montreal, Canada, August.
Simard, M., G. Foster, and P. Isabelle. 1992.
Using cognates to align sentences in
bilingual corpora. In Fourth International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-92),
pages 67–81, Montreal, Canada.
Smadja, Frank, Kathleen R. McKeown, and
Vasileios Hatzivassiloglou. 1996.
Translating collocations for bilingual
lexicons: A statistical approach.
Computational Linguistics, 22(1):1–38.
</reference>
<page confidence="0.970115">
50
</page>
<note confidence="0.759409">
Och and Ney Comparison of Statistical Alignment Models
</note>
<reference confidence="0.999681976190476">
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In COLING ’96: The 16th International
Conference on Computational Linguistics,
pages 836–841, Copenhagen, Denmark,
August.
Wahlster, Wolfgang, editor. 2000. Verbmobil:
Foundations of speech-to-speech translations.
Springer Verlag, Berlin.
Wang, Ye-Yi and Alex Waibel. 1998. Fast
decoding for statistical machine
translation. In Proceedings of the
International Conference on Speech and
Language Processing, pages 1357–1363,
Sydney, Australia, November.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Conference of the Association for
Computational Linguistics (ACL ’96),
pages 152–158, Santa Cruz, California,
June.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523–530, Toulouse, France,
July.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Human
Language Technology Conference, pages
109–116, San Diego, California, March.
Yarowsky, David and Richard Wicentowski.
2000. Minimally supervised
morphological analysis by multimodal
alignment. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 207–216, Hong
Kong, October.
</reference>
<page confidence="0.999118">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961796">
<title confidence="0.9894035">A Systematic Comparison of Various Statistical Alignment Models</title>
<author confidence="0.999882">Josef Hermann</author>
<affiliation confidence="0.996733">University of Southern California RWTH Aachen</affiliation>
<abstract confidence="0.998330923076923">We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methodsfor combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John D Lafferty</author>
<author>I Dan Melamed</author>
<author>David Purdy</author>
<author>Franz J Och</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation. Final Report, JHU Workshop. Available at http://www.clsp.jhu.edu/ws99/projects /mt/final report/mt-final-report.ps.</title>
<date>1999</date>
<contexts>
<context position="33397" citStr="Al-Onaizan et al. (1999)" startWordPosition="5514" endWordPosition="5517">er a subset of promising alignments. For Models 3 to 6, we perform the count collection only over a small number of good alignments. To keep the training fast, we consider only a small fraction of all alignments. We compare three different methods for using subsets of varying sizes: • The simplest method is to perform Viterbi training using only the best alignment found. As the Viterbi alignment computation itself is very time consuming for Models 3 to 6, the Viterbi alignment is computed only approximately, using the method described in Brown, Della Pietra, Della Pietra, and Mercer (1993). • Al-Onaizan et al. (1999) suggest using as well the neighboring alignments of the best alignment found. (For an exact definition of the neighborhood of an alignment, the reader is referred to the Appendix.) • Brown, Della Pietra, Della Pietra, and Mercer (1993) use an even larger set of alignments, including also the pegged alignments, a large set of alignments with a high probability Pr(fJ 1, aJ1 |eI1). The method for constructing these alignments (Brown, Della Pietra, Della Pietra, and Mercer 1993) guarantees that for each lexical relationship in every sentence pair, at least one alignment is considered. 30 Och and </context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Purdy, Och, Smith, Yarowsky, 1999</marker>
<rawString>Al-Onaizan, Yaser, Jan Curin, Michael Jahr, Kevin Knight, John D. Lafferty, I. Dan Melamed, David Purdy, Franz J. Och, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Final Report, JHU Workshop. Available at http://www.clsp.jhu.edu/ws99/projects /mt/final report/mt-final-report.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Automatic acquisition of hierarchical transduction models for machine translation.</title>
<date>1998</date>
<booktitle>In COLING–ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>41--47</pages>
<location>Montreal, Canada,</location>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 1998. Automatic acquisition of hierarchical transduction models for machine translation. In COLING–ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, volume 1, pages 41–47, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<volume>3</volume>
<contexts>
<context position="31273" citStr="Baum 1972" startWordPosition="5154" endWordPosition="5155"> we present methods for performing an efficient computation of this pseudo-Viterbi alignment. 3. Training 3.1 EM Algorithm In this section, we describe our approach to determining the model parameters 0. Every model has a specific set of free parameters. For example, the parameters 0 for 29 Computational Linguistics Volume 29, Number 1 Model 4 consist of lexicon, alignment, and fertility parameters: 0 = {{p(f |e)}, {p=1(∆j |···)}, {p&gt;1(∆j |···)},{p(φ |e)},p1} (34) To train the model parameters 0, we use a maximum-likelihood approach, as described in equation (4), by applying the EM algorithm (Baum 1972). The different models are trained in succession on the same data; the final parameter values of a simpler model serve as the starting point for a more complex model. In the E-step of Model 1, the lexicon parameter counts for one sentence pair (e, f) are calculated: c(f |e;e,f) = � N(e, f) � Pr(a |e,f) � 6(f,fj)6(e,eaj) (35) e,f a j Here, N(e, f) is the training corpus count of the sentence pair (f, e). In the M-step, the lexicon parameters are computed: � sc(f |e; fs, es) (36) p(f |e) = Es,f c(f |e; fs, es) Similarly, the alignment and fertility probabilities can be estimated for all other al</context>
<context position="32592" citStr="Baum 1972" startWordPosition="5382" endWordPosition="5383">to a more complex model, the simpler model is used to weigh the alignments, and the counts are accumulated for the parameters of the more complex model. In principle, the sum over all (I+ 1)J alignments has to be calculated in the E-step. Evaluating this sum by explicitly enumerating all alignments would be infeasible. Fortunately, Models 1 and 2 and HMM have a particularly simple mathematical form such that the EM algorithm can be implemented efficiently (i.e., in the E-step, it is possible to efficiently evaluate all alignments). For the HMM, this is referred to as the Baum-Welch algorithm (Baum 1972). Since we know of no efficient way to avoid the explicit summation over all alignments in the EM algorithm in the fertility-based alignment models, the counts are collected only over a subset of promising alignments. For Models 3 to 6, we perform the count collection only over a small number of good alignments. To keep the training fast, we consider only a small fraction of all alignments. We compare three different methods for using subsets of varying sizes: • The simplest method is to perform Viterbi training using only the best alignment found. As the Viterbi alignment computation itself i</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, L. E. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. Inequalities, 3:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John R Gillett</author>
<author>John D Lafferty</author>
<author>Harry Printz</author>
<author>Lubos Ures</author>
</authors>
<title>The Candide system for machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<contexts>
<context position="8205" citStr="Berger et al. 1994" startWordPosition="1273" endWordPosition="1276">nd every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disam</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Lafferty, Printz, Ures, 1994</marker>
<rawString>Berger, Adam L., Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, John D. Lafferty, Harry Printz, and Lubos Ures. 1994. The Candide system for machine translation. In Proceedings of the ARPA Workshop on Human Language Technology,</rawString>
</citation>
<citation valid="false">
<date></date>
<pages>157--162</pages>
<location>Plainsboro, New Jersey,</location>
<marker></marker>
<rawString>pages 157–162, Plainsboro, New Jersey, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>M J Goldsmith</author>
<author>J Hajic</author>
<author>R L Mercer</author>
<author>S Mohanty</author>
</authors>
<title>But dictionaries are data too.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>202--205</pages>
<location>Plainsboro, New Jersey,</location>
<marker>Brown, Pietra, Pietra, Goldsmith, Hajic, Mercer, Mohanty, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, M. J. Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty. 1993. But dictionaries are data too. In Proceedings of the ARPA Workshop on Human Language Technology, pages 202–205, Plainsboro, New Jersey, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Automated dictionary extraction for “knowledge-free” example-based translation.</title>
<date>1997</date>
<booktitle>In Seventh International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-97),</booktitle>
<pages>111--118</pages>
<location>Santa Fe, New Mexico,</location>
<contexts>
<context position="8535" citStr="Brown 1997" startWordPosition="1327" endWordPosition="1328">gnment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to languages for which such resources are scarce. 1.3 Overview In Section 2, we revi</context>
</contexts>
<marker>Brown, 1997</marker>
<rawString>Brown, Ralf D. 1997. Automated dictionary extraction for “knowledge-free” example-based translation. In Seventh International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-97), pages 111–118, Santa Fe, New Mexico, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ido Dagan</author>
<author>Kenneth W Church</author>
</authors>
<location>and</location>
<marker>Dagan, Church, </marker>
<rawString>Dagan, Ido, Kenneth W. Church, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>1--8</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="18919" citStr="Gale (1993)" startWordPosition="3017" endWordPosition="3018">To make the alignment parameters independent of absolute word positions, we assume that the alignment probabilities p(i |i&apos;,I) depend only on the jump width (i − i&apos;). Using a set of non-negative parameters {c(i − i&apos;)}, we can write the alignment probabilities in the form c(i − i~) p(i |i~, I) =rI(13) i&amp;quot;=1 c(ill − i&apos;) This form ensures that the alignment probabilities satisfy the normalization constraint for each conditioning word position it, it = 1, ... , I. This model is also referred to as a homogeneous HMM (Vogel, Ney, and Tillmann 1996). A similar idea was suggested by Dagan, Church, and Gale (1993). In the original formulation of the hidden Markov alignment model, there is no empty word that generates source words having no directly aligned target word. We introduce the empty word by extending the HMM network by I empty words e2I I+1. The target word ei has a corresponding empty word ei+I (i.e., the position of the empty word encodes the previously visited target word). We enforce the following constraints on the transitions in the HMM network (i ≤ I, it ≤ I) involving the empty word e0:1 p(i + I |i&apos;,I) = p0 · b(i,i&apos;) p(i + I |i&apos;+I,I) = p0 · b(i,i&apos;) p(i |i&apos;+I,I) = p(i |i&apos;,I) The paramet</context>
</contexts>
<marker>Gale, 1993</marker>
<rawString>William A. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora, pages 1–8, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>An unsupervised method for multilingual word sense tagging using parallel corpora: A preliminary investigation.</title>
<date>2000</date>
<booktitle>In ACL-2000 Workshop on Word Senses and Multilinguality,</booktitle>
<pages>1--9</pages>
<location>Hong Kong,</location>
<contexts>
<context position="8826" citStr="Diab 2000" startWordPosition="1374" endWordPosition="1375">Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to languages for which such resources are scarce. 1.3 Overview In Section 2, we review various statistical alignment models and heuristic models. We present a new statistical alignment model, a log-linear combination of the best models of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, and Mercer (1993). In Section 3, we describe the training of the </context>
</contexts>
<marker>Diab, 2000</marker>
<rawString>Diab, Mona. 2000. An unsupervised method for multilingual word sense tagging using parallel corpora: A preliminary investigation. In ACL-2000 Workshop on Word Senses and Multilinguality, pages 1–9, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Journal of Ecology,</journal>
<pages>26--297</pages>
<contexts>
<context position="14762" citStr="Dice 1945" startWordPosition="2321" endWordPosition="2322"> 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr(fJ1,aJ1 |eI1). 2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefficient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained: 2 · C(ei, fj) dice(i,j) = (6) C(ei) · C(fj) 23 Computational Linguistics Volume 29, Number 1 C(e,f) denotes the co-occurrence count of e and f in the parallel training corpus. C(e) and C(f) denote the count of e in the target sentences and the count off in the source sentences, respectively. From this association score matrix, the word alignment is then obtained by applying suitable heuristics. One method is to choose as alignment aj = </context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Dice, Lee R. 1945. Measures of the amount of ecologic association between species. Journal of Ecology, 26:297–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismael Garc´ıa-Varea</author>
<author>Francisco Casacuberta</author>
<author>Hermann Ney</author>
</authors>
<title>An iterative, DP-based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP’98),</booktitle>
<pages>1235--1238</pages>
<location>Sydney, Australia,</location>
<marker>Garc´ıa-Varea, Casacuberta, Ney, 1998</marker>
<rawString>Garc´ıa-Varea, Ismael, Francisco Casacuberta, and Hermann Ney. 1998. An iterative, DP-based search algorithm for statistical machine translation. In Proceedings of the International Conference on Spoken Language Processing (ICSLP’98), pages 1235–1238, Sydney, Australia, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>228--235</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="8348" citStr="Germann et al. 2001" startWordPosition="1297" endWordPosition="1300">ents in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic an</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 228–235, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Xia Huang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Chinese-Korean word alignment based on linguistic comparison.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>392--399</pages>
<location>Hong Kong,</location>
<contexts>
<context position="6892" citStr="Huang and Choi 2000" startWordPosition="1076" endWordPosition="1079">ed that enforces a one-to-one alignment for nonempty words. This means that the alignment mapping aJ1 must be injective for all word positions aj &gt; 0. Note that many translation phenomena cannot be handled using restricted alignment representations such as this one. Especially, methods such as Melamed’s are in principle not able to achieve a 100% recall. The problem can be reduced through corpus preprocessing steps that perform grouping and splitting of words. Some papers report improvements in the alignment quality of statistical methods when linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000). In these methods, the linguistic knowledge is used mainly to filter out incorrect alignments. In this work, we shall avoid making explicit assumptions concerning the language used. By avoiding these assumptions, we expect our approach to be applicable to almost every language pair. The only assumptions we make are that the parallel text is segmented into aligned sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a very rich morphology, </context>
</contexts>
<marker>Huang, Choi, 2000</marker>
<rawString>Huang, Jin-Xia and Key-Sun Choi. 2000. Chinese-Korean word alignment based on linguistic comparison. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 392–399, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue J Ker</author>
<author>Jason S Chang</author>
</authors>
<title>A class-based approach to word alignment.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="6870" citStr="Ker and Chang 1997" startWordPosition="1072" endWordPosition="1075">ification is performed that enforces a one-to-one alignment for nonempty words. This means that the alignment mapping aJ1 must be injective for all word positions aj &gt; 0. Note that many translation phenomena cannot be handled using restricted alignment representations such as this one. Especially, methods such as Melamed’s are in principle not able to achieve a 100% recall. The problem can be reduced through corpus preprocessing steps that perform grouping and splitting of words. Some papers report improvements in the alignment quality of statistical methods when linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000). In these methods, the linguistic knowledge is used mainly to filter out incorrect alignments. In this work, we shall avoid making explicit assumptions concerning the language used. By avoiding these assumptions, we expect our approach to be applicable to almost every language pair. The only assumptions we make are that the parallel text is segmented into aligned sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a </context>
<context position="14687" citStr="Ker and Chang 1997" startWordPosition="2309" endWordPosition="2312">n technique do indeed obtain a good alignment quality. In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr(fJ1,aJ1 |eI1). 2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefficient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained: 2 · C(ei, fj) dice(i,j) = (6) C(ei) · C(fj) 23 Computational Linguistics Volume 29, Number 1 C(e,f) denotes the co-occurrence count of e and f in the parallel training corpus. C(e) and C(f) denote the count of e in the target sentences and the count off in the source sentences, respectively. From this association score matrix, the word alignment is then obtained </context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>Ker, Sue J. and Jason S. Chang. 1997. A class-based approach to word alignment. Computational Linguistics, 23(2):313–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved clustering techniques for class-based statistical language modelling.</title>
<date>1993</date>
<booktitle>In European Conference on Speech Communication and Technology,</booktitle>
<pages>973--976</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="27984" citStr="Kneser and Ney (1993)" startWordPosition="4607" endWordPosition="4610">, it is straightforward to extend the alignment parameters to include a dependence on the word classes of the surrounding words (Och and Ney 2000). In the hidden Markov alignment model, we allow for a dependence of the position aj on the class of the preceding target word C(eaj−1): p(aj |aj−1,I,C(eaj−1)). Similarly, we can include dependencies on source and target word classes in Models 4 and 5 (Brown, Della Pietra, Della Pietra, and Mercer 1993). The categorization of the words into classes (here: 50 classes) is performed automatically by using the statistical learning procedure described in Kneser and Ney (1993). 2.3.3 Overview of Models. The main differences among the statistical alignment models lie in the alignment model they employ (zero-order or first-order), the fertility model they employ, and the presence or absence of deficiency. In addition, the models differ with regard to the efficiency of the E-step in the EM algorithm (Section 3.1). Table 1 offers an overview of the properties of the various alignment models. 28 Och and Ney Comparison of Statistical Alignment Models Table 1 Overview of the alignment models. Model Alignment model Fertility model E-step Deficient Model 1 uniform no exact </context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Kneser, Reinhard and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language modelling. In European Conference on Speech Communication and Technology, pages 973–976, Berlin, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in word-replacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="20624" citStr="Knight (1999" startWordPosition="3327" endWordPosition="3328">n Model 2, we obtain Pr(fJ1,aJ1 |eI1) = p(J |I) · Il J [p(aj |j,I,J) · p(fj |eaj)] (18) j=1 1 δ(i, i) is the Kronecker function, which is one if i = it and zero otherwise. 25 Computational Linguistics Volume 29, Number 1 To reduce the number of alignment parameters, we ignore the dependence on J in the alignment model and use a distribution p(aj |j, I) instead of p(aj |j, I, J). 2.3 Fertility-Based Alignment Models In the following, we give a short description of the fertility-based alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993). A gentle introduction can be found in Knight (1999b). The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra, Della Pietra, and Mercer 1993) have a significantly more complicated structure than the simple Models 1 and 2. The fertility Oi of a word ei in position i is defined as the number of aligned source words: �Oi = δ(aj,i) (19) j The fertility-based alignment models contain a probability p(O |e) that the target word e is aligned to O words. By including this probability, it is possible to explicitly describe the fact that for instance the German word ¨ubermorgen produces four English words (the day after tomorrow).</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Knight, Kevin. 1999a. Decoding complexity in word-replacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>A Statistical MT Tutorial Workbook. Available at http://www.isi.edu/natural-language/ mt/wkbk.rtf.</title>
<date>1999</date>
<contexts>
<context position="20624" citStr="Knight (1999" startWordPosition="3327" endWordPosition="3328">n Model 2, we obtain Pr(fJ1,aJ1 |eI1) = p(J |I) · Il J [p(aj |j,I,J) · p(fj |eaj)] (18) j=1 1 δ(i, i) is the Kronecker function, which is one if i = it and zero otherwise. 25 Computational Linguistics Volume 29, Number 1 To reduce the number of alignment parameters, we ignore the dependence on J in the alignment model and use a distribution p(aj |j, I) instead of p(aj |j, I, J). 2.3 Fertility-Based Alignment Models In the following, we give a short description of the fertility-based alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993). A gentle introduction can be found in Knight (1999b). The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra, Della Pietra, and Mercer 1993) have a significantly more complicated structure than the simple Models 1 and 2. The fertility Oi of a word ei in position i is defined as the number of aligned source words: �Oi = δ(aj,i) (19) j The fertility-based alignment models contain a probability p(O |e) that the target word e is aligned to O words. By including this probability, it is possible to explicitly describe the fact that for instance the German word ¨ubermorgen produces four English words (the day after tomorrow).</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Knight, Kevin. 1999b. A Statistical MT Tutorial Workbook. Available at http://www.isi.edu/natural-language/ mt/wkbk.rtf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The blinker project.</title>
<date>1998</date>
<tech>Technical Report 98-07,</tech>
<institution>Institute for Research in Cognitive Science,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="41812" citStr="Melamed 1998" startWordPosition="6944" endWordPosition="6945">gnment is intended. In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen. In lexicography applications, we might be interested in alignments with a very high precision obtained by performing an alignment intersection. 5. Evaluation Methodology In the following, we present an annotation scheme for single-word-based alignments and a corresponding evaluation criterion. It is well known that manually performing a word alignment is a complicated and ambiguous task (Melamed 1998). Therefore, in performing the alignments for the research presented here, we use an annotation scheme that explicitly allows for ambiguous alignments. The persons conducting the annotation are asked to specify alignments of two different kinds: an S (sure) alignment, for alignments that are unambiguous, and a P (possible) alignment, for ambiguous alignments. The P label is used especially to align words within idiomatic expressions and free translations and missing function words (S ⊆ P). The reference alignment thus obtained may contain many-to-one and one-to-many relationships. Figure 2 sho</context>
<context position="46469" citStr="Melamed (1998)" startWordPosition="7698" endWordPosition="7699">n in Tables 2 and 3. The number of running words and the vocabularies are based on full-form words and the punctuation marks. We produced smaller training corpora by randomly choosing 500, 2,000 and 8,000 sentences from the Verbmobil task and 500, 8,000, and 128,000 sentences from the Hansards task. For both tasks, we manually aligned a randomly chosen subset of the training corpus. From this subset of the corpus, the first 100 sentences are used as the development corpus to optimize the model parameters that are not trained via the EM 4 We do not use the Blinker annotated corpus described in Melamed (1998), since the domain is very special (the Bible) and a different annotation methodology is used. 35 Computational Linguistics Volume 29, Number 1 Table 4 Comparison of alignment error rate percentages for various training schemes (Verbmobil task; Dice+C: Dice coefficient with competitive linking). Size of training corpus Model Training scheme 0.5K 2K 8K 34K Dice 28.4 29.2 29.1 29.0 Dice+C 21.5 21.8 20.1 20.4 Model 1 15 19.3 19.0 17.8 17.0 Model 2 1525 27.7 21.0 15.8 13.5 HMM 15H5 19.1 15.4 11.4 9.2 Model 3 152533 25.8 18.4 13.4 10.3 15H533 18.1 14.3 10.5 8.1 Model 4 15253343 23.4 14.6 10.0 7.7 1</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>Melamed, I. Dan. 1998. Manual annotation of translational equivalence: The blinker project. Technical Report 98-07, Institute for Research in Cognitive Science, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="6235" citStr="Melamed (2000)" startWordPosition="977" endWordPosition="978"> one target word. Alignment models restricted in this way are similar to the concept of hidden Markov models (HMMs) in speech recognition. The alignment mapping in such models consists of associations j → i = aj from source position j to target position i = aj. The alignment aJ1 = a1, ... , aj, ... , aJ may contain alignments aj = 0 with the “empty” word e0 to account for source words that are not aligned with any target word. Constructed in such a way, the alignment is not a relation between source and target language positions, but only a mapping from source to target language positions. In Melamed (2000), a further simplification is performed that enforces a one-to-one alignment for nonempty words. This means that the alignment mapping aJ1 must be injective for all word positions aj &gt; 0. Note that many translation phenomena cannot be handled using restricted alignment representations such as this one. Especially, methods such as Melamed’s are in principle not able to achieve a 100% recall. The problem can be reduced through corpus preprocessing steps that perform grouping and splitting of words. Some papers report improvements in the alignment quality of statistical methods when linguistic kn</context>
<context position="8074" citStr="Melamed 2000" startWordPosition="1258" endWordPosition="1259">s with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney</context>
<context position="14702" citStr="Melamed 2000" startWordPosition="2313" endWordPosition="2314">d obtain a good alignment quality. In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr(fJ1,aJ1 |eI1). 2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefficient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained: 2 · C(ei, fj) dice(i,j) = (6) C(ei) · C(fj) 23 Computational Linguistics Volume 29, Number 1 C(e,f) denotes the co-occurrence count of e and f in the parallel training corpus. C(e) and C(f) denote the count of e in the target sentences and the count off in the source sentences, respectively. From this association score matrix, the word alignment is then obtained by applying sui</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>Melamed, I. Dan. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Sonja Nießen</author>
<author>Franz J Och</author>
<author>Hassan Sawaf</author>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Algorithms for statistical translation of spoken language.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="63773" citStr="Ney et al. 2000" startWordPosition="10572" endWordPosition="10575">on refined --•-•-• 100 95 90 85 80 75 100 e &gt;f f-&gt;e ----x-- intersection union --id— refined --.- 44 Och and Ney Comparison of Statistical Alignment Models method. By forming a union or intersection of the alignments, we can obtain very high recall or precision values on both the Hansards task and the Verbmobil task. 6.9 Effect of Alignment Quality on Translation Quality Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000). In Och and Ney (2000), the overall result of the experimental evaluation has been that an improved alignment quality yields an improved subjective quality of the statistical machine translation system as well. 7. Conclusion In this article, we have discussed in detail various statistical and heuristic word alignment models and described various modifications and extensions to models known in the literature. We have developed a new statistical alignment model (Model 6) that has yielded the best results among all the models we considered in the experiments we have conducted. We have presented </context>
</contexts>
<marker>Ney, Nießen, Och, Sawaf, Tillmann, Vogel, 2000</marker>
<rawString>Ney, Hermann, Sonja Nießen, Franz J. Och, Hassan Sawaf, Christoph Tillmann, and Stephan Vogel. 2000. Algorithms for statistical translation of spoken language. IEEE Transactions on Speech and Audio Processing, 8(1):24–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>A DP-based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In COLING-ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>960--967</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="8256" citStr="Nießen et al. 1998" startWordPosition="1283" endWordPosition="1286"> data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicen</context>
</contexts>
<marker>Nießen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>Nießen, Sonja, Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1998. A DP-based search algorithm for statistical machine translation. In COLING-ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 960–967, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Giza++: Training of statistical translation models. Available at http://www-i6.informatik.rwthaachen.de/∼och/software/GIZA++.html.</title>
<date>2000</date>
<contexts>
<context position="3768" citStr="Och 2000" startWordPosition="562" endWordPosition="563">re 1 Example of a word alignment (VERBMOBIL task). We propose to measure the quality of an alignment model by comparing the quality of the most probable alignment, the Viterbi alignment, with a manually produced reference alignment. This has the advantage of enabling an automatic evaluation to be performed. In addition, we shall show that this quality measure is a precise and reliable evaluation criterion that is well suited to guide designing and training statistical alignment models. The software used to train the statistical alignment models described in this article is publicly available (Och 2000). 1.1 Problem Definition We follow Brown, Della Pietra, Della Pietra, and Mercer (1993) to define alignment as an object for indicating the corresponding words in a parallel text. Figure 1 shows an example. Very often, it is difficult for a human to judge which words in a given target string correspond to which words in its source string. Especially problematic is the alignment of words within idiomatic expressions, free translations, and missing function words. The problem is that the notion of “correspondence” between words is subjective. It is important to keep this in mind in the evaluatio</context>
</contexts>
<marker>Och, 2000</marker>
<rawString>Och, Franz J. 2000. Giza++: Training of statistical translation models. Available at http://www-i6.informatik.rwthaachen.de/∼och/software/GIZA++.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In COLING ’00: The 18th International Conference on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="7855" citStr="Och and Ney 2000" startWordPosition="1226" endWordPosition="1229">mented into aligned sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998</context>
<context position="14333" citStr="Och and Ney (2000)" startWordPosition="2254" endWordPosition="2257">espect to a maximum-likelihood criterion, which is not necessarily directly related to alignment quality. Such an approach, however, requires training with manually defined alignments, which is not done in the research presented in this article. Experimental evidence shows (Section 6) that the statistical alignment models using this parameter estimation technique do indeed obtain a good alignment quality. In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr(fJ1,aJ1 |eI1). 2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefficient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained: 2 · C(ei, f</context>
<context position="27509" citStr="Och and Ney 2000" startWordPosition="4532" endWordPosition="4535">a~ |e)αk a�,f� The interpolation parameters αk are determined in such a way that the alignment quality on held-out data is optimized. We use a log-linear combination instead of the simpler linear combination because the values of Pr(f,a |e) typically differ by orders of magnitude for HMM and Model 4. In such a case, we expect the log-linear combination to be better than a linear combination. 2.3.2 Alignment Models Depending on Word Classes. For HMM and Models 4 and 5, it is straightforward to extend the alignment parameters to include a dependence on the word classes of the surrounding words (Och and Ney 2000). In the hidden Markov alignment model, we allow for a dependence of the position aj on the class of the preceding target word C(eaj−1): p(aj |aj−1,I,C(eaj−1)). Similarly, we can include dependencies on source and target word classes in Models 4 and 5 (Brown, Della Pietra, Della Pietra, and Mercer 1993). The categorization of the words into classes (here: 50 classes) is performed automatically by using the statistical learning procedure described in Kneser and Ney (1993). 2.3.3 Overview of Models. The main differences among the statistical alignment models lie in the alignment model they emplo</context>
<context position="35629" citStr="Och and Ney (2000)" startWordPosition="5888" endWordPosition="5891">y in the model is reduced, the likelihood increases. In Models 3 and 4 as defined in Brown, Della Pietra, Della Pietra, and Mercer (1993), the alignment model for nonempty words is deficient, but the alignment model for the empty word is nondeficient. Hence, the EM algorithm can increase likelihood by simply aligning more and more words with the empty word.3 Therefore, we modify Models 3 and 4 slightly, such that the empty word also has a deficient alignment model. The alignment probability is set to p(j |i, J) = 1/J for each source word aligned with the empty word. Another remedy, adopted in Och and Ney (2000), is to choose a value for the parameter p1 of the empty-word fertility and keep it fixed. 3.3 Smoothing To overcome the problem of overfitting on the training data and to enable the models to cope better with rare words, we smooth the alignment and fertility probabilities. For the alignment probabilities of the HMM (and similarly for Models 4 and 5), we perform an interpolation with a uniform distribution p(i |j,I) = 1/I using an interpolation parameter α: p&apos;(aj |aj−1,I) = (1 − α) · p(aj |aj−1,I) + α ·1 (37) For the fertility probabilities, we assume that there is a dependence on the number o</context>
<context position="38200" citStr="Och and Ney (2000)" startWordPosition="6327" endWordPosition="6330"> Della Pietra, Goldsmith, et al. (1993) developed a multinomial model for the process of constructing a dictionary (by a human lexicographer). By applying suitable simplifications, the method boils down to adding every dictionary entry (e, f) to the training corpus with an entry-specific count called effective multiplicity, expressed as µ(e,f): µ(e, f) = 1(e) - eλp) ·p(f|) (39) In this section, A(e) is an additional parameter describing the size of the sample that is used to estimate the model p(f |e). This count is then used instead of N(e, f) in the EM algorithm as shown in equation (35). • Och and Ney (2000) suggest that the effective multiplicity of a dictionary entry be set to a large value µ+ » 1 if the lexicon entry actually occurs in one of the sentence pairs of the bilingual corpus and to a low value otherwise: e f= I µ+ if e and f co-occur 40 µ (&apos;) l µ− otherwise ( ) As a result, only dictionary entries that indeed occur in the training corpus have a large effect in training. The motivation behind this is to avoid a deterioration of the alignment as a result of out-of-domain dictionary entries. Every entry in the dictionary that does co-occur in the training corpus can be assumed correct a</context>
<context position="41358" citStr="Och and Ney 2000" startWordPosition="6877" endWordPosition="6880">zontal and vertical neighbors. Obviously, the intersection of the two alignments yields an alignment consisting of only one-to-one alignments with a higher precision and a lower recall than either one separately. The union of the two alignments yields a higher recall and a lower precision of the combined alignment than either one separately. Whether a higher precision or a higher recall is preferred depends on the final application for which the word alignment is intended. In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen. In lexicography applications, we might be interested in alignments with a very high precision obtained by performing an alignment intersection. 5. Evaluation Methodology In the following, we present an annotation scheme for single-word-based alignments and a corresponding evaluation criterion. It is well known that manually performing a word alignment is a complicated and ambiguous task (Melamed 1998). Therefore, in performing the alignments for the research presented here, we use an annotation scheme that explicitly allows for ambiguous alignm</context>
<context position="63796" citStr="Och and Ney (2000)" startWordPosition="10577" endWordPosition="10580">0 95 90 85 80 75 100 e &gt;f f-&gt;e ----x-- intersection union --id— refined --.- 44 Och and Ney Comparison of Statistical Alignment Models method. By forming a union or intersection of the alignments, we can obtain very high recall or precision values on both the Hansards task and the Verbmobil task. 6.9 Effect of Alignment Quality on Translation Quality Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000). In Och and Ney (2000), the overall result of the experimental evaluation has been that an improved alignment quality yields an improved subjective quality of the statistical machine translation system as well. 7. Conclusion In this article, we have discussed in detail various statistical and heuristic word alignment models and described various modifications and extensions to models known in the literature. We have developed a new statistical alignment model (Model 6) that has yielded the best results among all the models we considered in the experiments we have conducted. We have presented two methods for includi</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz J. and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In COLING ’00: The 18th International Conference on Computational Linguistics, pages 1086–1090, Saarbr¨ucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland, College Park,</institution>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Och, Franz J., Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, University of Maryland, College Park, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient A* search algorithm for statistical machine translation.</title>
<date>2001</date>
<booktitle>In Data-Driven Machine Translation Workshop,</booktitle>
<pages>55--62</pages>
<location>Toulouse, France,</location>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Och, Franz J., Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for statistical machine translation. In Data-Driven Machine Translation Workshop, pages 55–62, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hans Weber</author>
</authors>
<title>Improving statistical natural language translation with categories and rules.</title>
<date>1998</date>
<booktitle>In COLING-ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>985--989</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="8455" citStr="Och and Weber 1998" startWordPosition="1314" endWordPosition="1317"> (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to l</context>
</contexts>
<marker>Och, Weber, 1998</marker>
<rawString>Och, Franz J. and Hans Weber. 1998. Improving statistical natural language translation with categories and rules. In COLING-ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 985–989, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92),</booktitle>
<pages>67--81</pages>
<location>Montreal, Canada.</location>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Simard, M., G. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92), pages 67–81, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Smadja, Frank, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING ’96: The 16th International Conference on Computational Linguistics, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of speech-to-speech translations.</title>
<date>2000</date>
<editor>Wahlster, Wolfgang, editor.</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="6235" citStr="(2000)" startWordPosition="978" endWordPosition="978">get word. Alignment models restricted in this way are similar to the concept of hidden Markov models (HMMs) in speech recognition. The alignment mapping in such models consists of associations j → i = aj from source position j to target position i = aj. The alignment aJ1 = a1, ... , aj, ... , aJ may contain alignments aj = 0 with the “empty” word e0 to account for source words that are not aligned with any target word. Constructed in such a way, the alignment is not a relation between source and target language positions, but only a mapping from source to target language positions. In Melamed (2000), a further simplification is performed that enforces a one-to-one alignment for nonempty words. This means that the alignment mapping aJ1 must be injective for all word positions aj &gt; 0. Note that many translation phenomena cannot be handled using restricted alignment representations such as this one. Especially, methods such as Melamed’s are in principle not able to achieve a 100% recall. The problem can be reduced through corpus preprocessing steps that perform grouping and splitting of words. Some papers report improvements in the alignment quality of statistical methods when linguistic kn</context>
<context position="14333" citStr="(2000)" startWordPosition="2257" endWordPosition="2257">maximum-likelihood criterion, which is not necessarily directly related to alignment quality. Such an approach, however, requires training with manually defined alignments, which is not done in the research presented in this article. Experimental evidence shows (Section 6) that the statistical alignment models using this parameter estimation technique do indeed obtain a good alignment quality. In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr(fJ1,aJ1 |eI1). 2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefficient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained: 2 · C(ei, f</context>
<context position="35629" citStr="(2000)" startWordPosition="5891" endWordPosition="5891">el is reduced, the likelihood increases. In Models 3 and 4 as defined in Brown, Della Pietra, Della Pietra, and Mercer (1993), the alignment model for nonempty words is deficient, but the alignment model for the empty word is nondeficient. Hence, the EM algorithm can increase likelihood by simply aligning more and more words with the empty word.3 Therefore, we modify Models 3 and 4 slightly, such that the empty word also has a deficient alignment model. The alignment probability is set to p(j |i, J) = 1/J for each source word aligned with the empty word. Another remedy, adopted in Och and Ney (2000), is to choose a value for the parameter p1 of the empty-word fertility and keep it fixed. 3.3 Smoothing To overcome the problem of overfitting on the training data and to enable the models to cope better with rare words, we smooth the alignment and fertility probabilities. For the alignment probabilities of the HMM (and similarly for Models 4 and 5), we perform an interpolation with a uniform distribution p(i |j,I) = 1/I using an interpolation parameter α: p&apos;(aj |aj−1,I) = (1 − α) · p(aj |aj−1,I) + α ·1 (37) For the fertility probabilities, we assume that there is a dependence on the number o</context>
<context position="38200" citStr="(2000)" startWordPosition="6330" endWordPosition="6330">a, Goldsmith, et al. (1993) developed a multinomial model for the process of constructing a dictionary (by a human lexicographer). By applying suitable simplifications, the method boils down to adding every dictionary entry (e, f) to the training corpus with an entry-specific count called effective multiplicity, expressed as µ(e,f): µ(e, f) = 1(e) - eλp) ·p(f|) (39) In this section, A(e) is an additional parameter describing the size of the sample that is used to estimate the model p(f |e). This count is then used instead of N(e, f) in the EM algorithm as shown in equation (35). • Och and Ney (2000) suggest that the effective multiplicity of a dictionary entry be set to a large value µ+ » 1 if the lexicon entry actually occurs in one of the sentence pairs of the bilingual corpus and to a low value otherwise: e f= I µ+ if e and f co-occur 40 µ (&apos;) l µ− otherwise ( ) As a result, only dictionary entries that indeed occur in the training corpus have a large effect in training. The motivation behind this is to avoid a deterioration of the alignment as a result of out-of-domain dictionary entries. Every entry in the dictionary that does co-occur in the training corpus can be assumed correct a</context>
<context position="63796" citStr="(2000)" startWordPosition="10580" endWordPosition="10580">0 75 100 e &gt;f f-&gt;e ----x-- intersection union --id— refined --.- 44 Och and Ney Comparison of Statistical Alignment Models method. By forming a union or intersection of the alignments, we can obtain very high recall or precision values on both the Hansards task and the Verbmobil task. 6.9 Effect of Alignment Quality on Translation Quality Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000). In Och and Ney (2000), the overall result of the experimental evaluation has been that an improved alignment quality yields an improved subjective quality of the statistical machine translation system as well. 7. Conclusion In this article, we have discussed in detail various statistical and heuristic word alignment models and described various modifications and extensions to models known in the literature. We have developed a new statistical alignment model (Model 6) that has yielded the best results among all the models we considered in the experiments we have conducted. We have presented two methods for includi</context>
</contexts>
<marker>2000</marker>
<rawString>Wahlster, Wolfgang, editor. 2000. Verbmobil: Foundations of speech-to-speech translations. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Fast decoding for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Speech and Language Processing,</booktitle>
<pages>1357--1363</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="8236" citStr="Wang and Waibel 1998" startWordPosition="1279" endWordPosition="1282">ers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarow</context>
</contexts>
<marker>Wang, Waibel, 1998</marker>
<rawString>Wang, Ye-Yi and Alex Waibel. 1998. Fast decoding for statistical machine translation. In Proceedings of the International Conference on Speech and Language Processing, pages 1357–1363, Sydney, Australia, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Conference of the Association for Computational Linguistics (ACL ’96),</booktitle>
<contexts>
<context position="8214" citStr="Wu 1996" startWordPosition="1277" endWordPosition="1278">thod suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation</context>
<context position="66468" citStr="Wu 1996" startWordPosition="10989" endWordPosition="10990">alignment models dependent on word classes (as in Models 4 and 5). • increasing the number of alignments used in the approximation of the EM algorithm for the fertility-based alignment models. Further improvements in alignments are expected to be produced through the adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment 45 Computational Linguistics Volume 29, Number 1 models based on word groups rather than single words (Och, Tillmann, and Ney 1999). The use of models that explicitly deal with the hierarchical structures of natural language is very promising (Wu 1996; Yamada and Knight 2001). We plan to develop structured models for the lexicon, alignment, and fertility probabilities using maximum-entropy models. This is expected to allow an easy integration of more dependencies, such as in a second-order alignment model, without running into the problem of the number of alignment parameters getting unmanageably large. Furthermore, it will be important to verify the applicability of the statistical alignment models examined in this article to less similar language pairs such as ChineseEnglish and Japanese-English. Appendix: Efficient Training of Fertility</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Wu, Dekai. 1996. A polynomial-time algorithm for statistical machine translation. In Proceedings of the 34th Annual Conference of the Association for Computational Linguistics (ACL ’96),</rawString>
</citation>
<citation valid="false">
<date></date>
<pages>152--158</pages>
<location>Santa Cruz, California,</location>
<marker></marker>
<rawString>pages 152–158, Santa Cruz, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>523--530</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="66493" citStr="Yamada and Knight 2001" startWordPosition="10991" endWordPosition="10994"> models dependent on word classes (as in Models 4 and 5). • increasing the number of alignments used in the approximation of the EM algorithm for the fertility-based alignment models. Further improvements in alignments are expected to be produced through the adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment 45 Computational Linguistics Volume 29, Number 1 models based on word groups rather than single words (Och, Tillmann, and Ney 1999). The use of models that explicitly deal with the hierarchical structures of natural language is very promising (Wu 1996; Yamada and Knight 2001). We plan to develop structured models for the lexicon, alignment, and fertility probabilities using maximum-entropy models. This is expected to allow an easy integration of more dependencies, such as in a second-order alignment model, without running into the problem of the number of alignment parameters getting unmanageably large. Furthermore, it will be important to verify the applicability of the statistical alignment models examined in this article to less similar language pairs such as ChineseEnglish and Japanese-English. Appendix: Efficient Training of Fertility-Based Alignment Models I</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 523–530, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Human Language Technology Conference,</booktitle>
<pages>109--116</pages>
<location>San Diego, California,</location>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>Yarowsky, David, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Human Language Technology Conference, pages 109–116, San Diego, California, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>207--216</pages>
<location>Hong Kong,</location>
<contexts>
<context position="7887" citStr="Yarowsky and Wicentowski 2000" startWordPosition="1230" endWordPosition="1233">d sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) o</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>Yarowsky, David and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 207–216, Hong Kong, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>