<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000411">
<title confidence="0.998478">
Robust PCFG-Based Generation using Automatically Acquired LFG
Approximations
</title>
<author confidence="0.998693">
Aoife Cahill1 and Josef van Genabith1,2
</author>
<affiliation confidence="0.9961295">
1 National Centre for Language Technology (NCLT)
School of Computing, Dublin City University, Dublin 9, Ireland
</affiliation>
<address confidence="0.45911">
2 Center for Advanced Studies, IBM Dublin, Ireland
</address>
<email confidence="0.99309">
{acahill,josef}@computing.dcu.ie
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999886416666667">
We present a novel PCFG-based archi-
tecture for robust probabilistic generation
based on wide-coverage LFG approxima-
tions (Cahill et al., 2004) automatically
extracted from treebanks, maximising the
probability of a tree given an f-structure.
We evaluate our approach using string-
based evaluation. We currently achieve
coverage of 95.26%, a BLEU score of
0.7227 and string accuracy of 0.7476 on
the Penn-II WSJ Section 23 sentences of
length &lt;20.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905275862069">
Wide coverage grammars automatically extracted
from treebanks are a corner-stone technology
in state-of-the-art probabilistic parsing. They
achieve robustness and coverage at a fraction of
the development cost of hand-crafted grammars. It
is surprising to note that to date, such grammars do
not usually figure in the complementary operation
to parsing – natural language surface realisation.
Research on statistical natural language surface
realisation has taken three broad forms, differ-
ing in where statistical information is applied in
the generation process. Langkilde (2000), for ex-
ample, uses n-gram word statistics to rank alter-
native output strings from symbolic hand-crafted
generators to select paths in parse forest repre-
sentations. Bangalore and Rambow (2000) use
n-gram word sequence statistics in a TAG-based
generation model to rank output strings and ad-
ditional statistical and symbolic resources at in-
termediate generation stages. Ratnaparkhi (2000)
uses maximum entropy models to drive generation
with word bigram or dependency representations
taking into account (unrealised) semantic features.
Valldal and Oepen (2005) present a discriminative
disambiguation model using a hand-crafted HPSG
grammar for generation. Belz (2005) describes
a method for building statistical generation mod-
els using an automatically created generation tree-
bank for weather forecasts. None of these prob-
abilistic approaches to NLG uses a full treebank
grammar to drive generation.
Bangalore et al. (2001) investigate the ef-
fect of training size on performance while using
grammars automatically extracted from the Penn-
II Treebank (Marcus et al., 1994) for generation.
Using an automatically extracted XTAG grammar,
they achieve a string accuracy of 0.749 on their
test set. Nakanishi et al. (2005) present proba-
bilistic models for a chart generator using a HPSG
grammar acquired from the Penn-II Treebank (the
Enju HPSG). They investigate discriminative dis-
ambiguation models following Valldal and Oepen
(2005) and their best model achieves coverage of
90.56% and a BLEU score of 0.7723 on Penn-II
WSJ Section 23 sentences of length &lt;20.
In this paper we present a novel PCFG-based
architecture for probabilistic generation based on
wide-coverage, robust Lexical Functional Gram-
mar (LFG) approximations automatically ex-
tracted from treebanks (Cahill et al., 2004). In
Section 2 we briefly describe LFG (Kaplan and
Bresnan, 1982). Section 3 presents our genera-
tion architecture. Section 4 presents evaluation re-
sults on the Penn-II WSJ Section 23 test set us-
ing string-based metrics. Section 5 compares our
approach with alternative approaches in the litera-
ture. Section 6 concludes and outlines further re-
search.
</bodyText>
<sectionHeader confidence="0.990722" genericHeader="introduction">
2 Lexical Functional Grammar
</sectionHeader>
<bodyText confidence="0.998707666666667">
Lexical Functional Grammar (LFG) (Kaplan and
Bresnan, 1982) is a constraint-based theory of
grammar. It (minimally) posits two levels of repre-
sentation, c(onstituent)-structure and f(unctional)-
structure. C-structure is represented by context-
free phrase-structure trees, and captures surface
</bodyText>
<page confidence="0.898497">
1033
</page>
<note confidence="0.716324">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1033–1040,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<equation confidence="0.749782">
John resigned
(T PRED) = ‘John’ (T PRED) = ‘resign’
(T NUM) = SG (T TENSE) = PAST
(T PERS) = 3
</equation>
<figureCaption confidence="0.978756">
Figure 1: C- and f-structures for the sentence They believe John resigned.
</figureCaption>
<equation confidence="0.956005166666667">
S
T=1
NP VP
(T SUBJ)= 1 T=1
NNP V SBAR
T=1 T=1 (T COMP)= 1
They believe S
(T PRED) = ‘pro’ (T PRED) = ‘believe’ T=1
(T NUM) = PL (T TENSE) = present
(T PERS) = 3 NP VP
(T SUBJ)= 1 T=1
NNP V
T=1 T=1
PRED ‘BELIEVE((TSUBJ)(TCOMP))’
� PRED ‘PRO’ �
SUBJ f2: NUM PL
 PERS 3 �
PRED ‘JOHN’
SUBJ f4: NUM SG
COMP fg:PERS 3
PRED RESIGN((TSUBJ))’
TENSE PAST
TENSE PRESENT
f1:

      



 
</equation>
<bodyText confidence="0.999858">
grammatical configurations such as word order.
The nodes in the trees are annotated with func-
tional equations (attribute-value structure con-
straints) which are resolved to produce an f-
structure. F-structures are recursive attribute-
value matrices, representing abstract syntactic
functions. F-structures approximate to basic
predicate-argument-adjunct structures or depen-
dency relations. Figure 1 shows the c- and f-
structures for the sentence “They believe John re-
signed”.
</bodyText>
<sectionHeader confidence="0.8648795" genericHeader="method">
3 PCFG-Based Generation for
Treebank-Based LFG Resources
</sectionHeader>
<bodyText confidence="0.999930941176471">
Cahill et al. (2004) present a method to au-
tomatically acquire wide-coverage robust proba-
bilistic LFG approximations1 from treebanks. The
method is based on an automatic f-structure an-
notation algorithm that associates nodes in tree-
bank trees with f-structure equations. For each
tree, the equations are collected and passed on to
a constraint solver which produces an f-structure
for the tree. Cahill et al. (2004) present two
parsing architectures: the pipeline and the inte-
grated parsing architecture. In the pipeline ar-
chitecture, a PCFG (or a history-based lexicalised
generative parser) is extracted from the treebank
and used to parse unseen text into trees, the result-
ing trees are annotated with f-structure equations
by the f-structure annotation algorithm and a con-
straint solver produces an f-structure. In the in-
</bodyText>
<footnote confidence="0.99344425">
1The resources are approximations in that (i) they do not
enforce LFG completeness and coherence constraints and (ii)
PCFG-based models can only approximate LFG and similar
constraint-based formalisms (Abney, 1997).
</footnote>
<bodyText confidence="0.999658833333333">
tegrated architecture, first the treebank trees are
automatically annotated with f-structure informa-
tion, f-structure annotated PCFGs with rules of
the form NP(TOBJ=j)—*DT(T=j) NN(T=j) are
extracted, syntactic categories followed by equa-
tions are treated as monadic CFG categories dur-
ing grammar extraction and parsing, unseen text is
parsed into trees with f-structure annotations, the
annotations are collected and a constraint solver
produces an f-structure.
The generation architecture presented here
builds on the integrated parsing architecture re-
sources of Cahill et al. (2004). The generation
process takes an f-structure (such as the f-structure
on the right in Figure 1) as input and outputs the
most likely f-structure annotated tree (such as the
tree on the left in Figure 1) given the input f-
structure
</bodyText>
<subsectionHeader confidence="0.293654">
argmaxTreeP (Tree F-Str)
</subsectionHeader>
<bodyText confidence="0.965595666666667">
where the probability of a tree given an f-
structure is decomposed as the product of the
probabilities of all f-structure annotated produc-
tions contributing to the tree but where in addi-
tion to conditioning on the LHS of the produc-
tion (as in the integrated parsing architecture of
Cahill et al. (2004)) each production X —* Y is
now also conditioned on the set of f-structure fea-
tures Feats 0-linked2 to the LHS of the rule. For
an f-structure annotated tree Tree and f-structure
F-Str with -b(Tree)=F-Str:3
20 links LFG’s c-structure to f-structure in terms of many-
to-one functions from tree nodes into f-structure.
34) resolves the equations in Tree into F-Str (if satisfiable)
in terms of the piece-wise function 0.
</bodyText>
<page confidence="0.968764">
1034
</page>
<table confidence="0.999954285714286">
Conditioning F-Structure Features Grammar Rules Probability
{PRED, SUBJ, COMP, TENSE} VP(↑=↓) → VBD(↑=↓) SBAR(↑COMP=↓) 0.4998
{PRED, SUBJ, COMP, TENSE} VP(↑=↓) → VBP(↑=↓) SBAR(↑COMP=↓) 0.0366
{PRED, SUBJ, COMP, TENSE} VP(↑=↓) → VBD(↑=↓) , S(↑COMP=↓) 6.48e-6
{PRED, SUBJ, COMP, TENSE} VP(↑=↓) → VBD(↑=↓) S(↑COMP=↓) 3.88e-6
{PRED, SUBJ, COMP, TENSE} VP(↑=↓) → VBP(↑=↓) , SBARQ(↑COMP=↓) 7.86e-7
{PRED, SUBJ, COMP, TENSE} VP(↑=↓) → VBD(↑=↓) SBARQ(↑COMP=↓) 1.59e-7
</table>
<tableCaption confidence="0.999771">
Table 1: Example VP Generation rules automatically extracted from Sections 02–21 of the Penn-II
</tableCaption>
<equation confidence="0.939632625">
Treebank
�P(Tree|F-Str) := P(X → Y |X, Feats) (1)
X → Y in Tree
O(X) = Feats
P(X → Y, X, Feats)
P(X → Y |X, Feats) = P(X, Feats) = (2)
P(X → Y, Feats) ≈ #(X → Y, Feats) (3)
P(X, Feats) #(X → ... , Feats)
</equation>
<bodyText confidence="0.999923739130435">
and where probabilities are estimated using a
simple MLE and rule counts (#) from the auto-
matically f-structure annotated treebank resource
of Cahill et al. (2004). Lexical rules (rules ex-
panding preterminals) are conditioned on the full
set of (atomic) feature-value pairs 0-linked to the
RHS. The intuition for conditioning rules in this
way is that local f-structure components of the in-
put f-structure drive the generation process. This
conditioning effectively turns the f-structure an-
notated PCFGs of Cahill et al. (2004) into prob-
abilistic generation grammars. For example, in
Figure 1 (where 0-links are represented as ar-
rows), we automatically extract the rule S(T=1) —*
NP(TSUBJ=1) VP(T=1) conditioned on the feature
set {PRED,SUBJ,COMP,TENSE}. The probability
of the rule is then calculated by counting the num-
ber of occurrences of that rule (and the associated
set of features), divided by the number of occur-
rences of rules with the same LHS and set of fea-
tures. Table 1 gives example VP rule expansions
with their probabilities when we train a grammar
from Sections 02–21 of the Penn Treebank.
</bodyText>
<subsectionHeader confidence="0.999576">
3.1 Chart Generation Algorithm
</subsectionHeader>
<bodyText confidence="0.935158035714286">
The generation algorithm is based on chart gen-
eration as first introduced by Kay (1996) with
Viterbi-pruning. The generation grammar is first
converted into Chomsky Normal Form (CNF). We
recursively build a chart-like data structure in a
bottom-up fashion. In contrast to packing of lo-
cally equivalent edges (Carroll and Oepen, 2005),
in our approach if two chart items have equiva-
lent rule left-hand sides and lexical coverage, only
the most probable one is kept. Each grammatical
function-labelled (sub-)f-structure in the overall f-
structure indexes a (sub-)chart. The chart for each
f-structure generates the most probable tree for
that f-structure, given the internal set of condition-
ing f-structure features and its grammatical func-
tion label. At each level, grammatical function in-
dexed charts are initially unordered. Charts are
linearised by generation grammar rules once the
charts themselves have produced the most prob-
able tree for the chart. Our example in Figure 1
generates the following grammatical function in-
dexed, embedded and (at each level of embedding)
unordered (sub-)chart configuration:
TOP f 1:
For each local subchart, the following algorithm
is applied:
Add lexical rules
While subchart is Changing
</bodyText>
<sectionHeader confidence="0.284136" genericHeader="method">
Apply unary productions
</sectionHeader>
<subsectionHeader confidence="0.655988666666667">
Apply binary productions
Propagate compatible rules
3.2 A Worked Example
</subsectionHeader>
<bodyText confidence="0.999910583333333">
As an example, we step through the construc-
tion of the COMP-indexed chart at level f3 of
the f-structure in Figure 1. For lexical rules,
we check the feature set at the sub-f-structure
level and the values of the features. Only fea-
tures associated with lexical material are consid-
ered. The SUBJ-indexed sub-chart f4 is con-
structed by first adding the rule NNP(T=1) —*
John(TPRED=‘John’,TNUM=pl,TPERS=3). If more
than one lexical rule corresponds to a particular set
of features and values in the f-structure, we add all
rules with different LHS categories. If two or more
</bodyText>
<figure confidence="0.94732">
SUBJ f 4:
COMP fj
SUBJ fi
</figure>
<page confidence="0.972091">
1035
</page>
<bodyText confidence="0.999899600000001">
rules with equal LHS categories match the feature
set, we only add the most probable one.
Unary productions are applied if the RHS of the
unary production matches the LHS of an item al-
ready in the chart and the feature set of the unary
production matches the conditioning feature set of
the local sub-f-structure. In our example, this re-
sults in the rule NP(↑SUBJ=↓) → NNP(↑=↓), con-
ditioned on {NUM, PERS, PRED}, being added to
the sub-chart at level f4 (the probability associated
with this item is the probability of the rule multi-
plied by the probability of the previous chart item
which combines with the new rule). When a rule
is added to the chart, it is automatically associated
with the yield of the rule, allowing us to propa-
gate chunks of generated material upwards in the
chart. If two items in the chart have the same LHS
(and the same yield independent of word order),
only the item with the highest probability is kept.
This Viterbi-style pruning ensures that processing
is efficient.
At sub-chart f4 there are no binary rules that
can be applied. At this stage, it is not possible
to add any more items to the sub-chart, therefore
we propagate items in the chart that are compat-
ible with the sub-chart index SUBJ. In our ex-
ample, only the rule NP(↑SUBJ=↓) → NNP(↑=↓)
(which yields the string John) is propagated to the
next level up in the overall chart for consideration
in the next iteration. If the yield of an item be-
ing propagated upwards in the chart is subsumed
by an element already at that level, the subsumed
item is removed. This results in efficiently treat-
ing the well known problem originally described
in Kay (1996), where one unnecessarily retains
sub-optimal strings. For example, generating the
string “The very tall strong athletic man”, one
does not want to keep variations such as “The very
tall man”, or “The athletic man”, if one can gener-
ate the entire string. Our method ensures that only
the most probable tree with the longest yield will
be propagated upwards.
The COMP-indexed chart at level f3 of the f-
structure is constructed in a similar fashion. First
the lexical rule V(↑=↓) → resigned is added.
Next, conditioning on {PRED, SUBJ, TENSE}, the
unary rule VP(↑=↓) → V(↑=↓) (with yield re-
signed) is added. We combine the new VP(↑=↓)
rule with the NP(↑SUBJ=↓) already present from
the previous iteration to enable us to add the rule
S(↑=↓) → NP(↑SUBJ=↓) VP(↑=↓), conditioned
on {PRED, SUBJ, TENSE}. The yield of this rule
is John resigned. Next, conditioning on the same
feature set, we add the rule SBAR(↑comp=↓) →
S(↑=↓) with yield John resigned to the chart. It is
not possible to add any more new rules, so at this
stage, only the SBAR(↑COMP=↓) rule with yield
John resigned is propagated up to the next level.
The process continues until at the outermost
level of the f-structure, there are no more rules to
be added to the chart. At this stage, we search for
the most probable rule with TOP as its LHS cate-
gory and return the yield of this rule as the output
of the generation process. Generation fails if there
is no rule with LHS TOP at this level in the chart.
</bodyText>
<subsectionHeader confidence="0.998404">
3.3 Lexical Smoothing
</subsectionHeader>
<bodyText confidence="0.999984137931034">
Currently, the only smoothing in the system ap-
plies at the lexical level. Our backoff uses
the built-in lexical macros4 of the automatic f-
structure annotation algorithm of Cahill et al.
(2004) to identify potential part-of-speech cate-
gories corresponding to a particular set of features.
Following Baayen and Sproat (1996) we assume
that unknown words have a probability distribu-
tion similar to hapax legomena. We add a lexical
rule for each POS tag that corresponds to the f-
structure features at that level to the chart with a
probability computed from the original POS tag
probability distribution multiplied by a very small
constant. This means that lexical rules seen during
training have a much higher probability than lexi-
cal rules added during the smoothing phase. Lexi-
cal smoothing has the advantage of boosting cov-
erage (as shown in Tables 3, 4, 5 and 6 below) but
slightly degrades the quality of the strings gener-
ated. We believe that the tradeoff in terms of qual-
ity is worth the increase in coverage.
Smoothing is not carried out when there is no
suitable phrasal grammar rule that applies during
the process of generation. This can lead to the gen-
eration of partial strings, since some f-structure
components may fail to generate a corresponding
string. In such cases, generation outputs the con-
catenation of the strings generated by the remain-
ing components.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998455">
We train our system on WSJ Sections 02–21 of
the Penn-II Treebank and evaluate against the raw
</bodyText>
<footnote confidence="0.978359333333333">
4The lexical macros associate POS tags with sets of fea-
tures, for example the tag NNS (plural noun) is associated
with the features IPRED=$LEMMA and INUM=pl.
</footnote>
<page confidence="0.954542">
1036
</page>
<table confidence="0.998019333333333">
S. length &lt; 20 &lt; 25 &lt; 30 &lt; 40 all
Training 16667 23597 29647 36765 39832
Test 1034 1464 1812 2245 2416
</table>
<tableCaption confidence="0.960023">
Table 2: Number of training and test sentences per
sentence length
</tableCaption>
<bodyText confidence="0.9968985">
strings from Section 23. We use Section 22 as our
development set. As part of our evaluation, we ex-
periment with sentences of varying length (20, 25,
30, 40, all), both in training and testing. Table 2
gives the number of training and test sentences for
each sentence length. In each case, we use the au-
tomatically generated f-structures from Cahill et
al. (2004) from the original Section 23 treebank
trees as f-structure input to our generation experi-
ments. We automatically mark adjunct and coor-
dination scope in the input f-structure. Notice that
these automatically generated f-structures are not
“perfect”, i.e. they are not guaranteed to be com-
plete and coherent (Kaplan and Bresnan, 1982): a
local f-structure may contain material that is not
supposed to be there (incoherence) and/or may be
missing material that is supposed to be there (in-
completeness). The results presented below show
that our method is robust with respect to the qual-
ity of the f-structure input and will always attempt
to generate partial output rather than fail. We con-
sider this an important property as pristine gen-
eration input cannot always be guaranteed in re-
alistic application scenarios, such as probabilistic
transfer-based machine translation where genera-
tion input may contain a certain amount of noise.
</bodyText>
<subsectionHeader confidence="0.997977">
4.1 Pre-Training Treebank Transformations
</subsectionHeader>
<bodyText confidence="0.999954386363637">
During the development of the generation system,
we carried out error analysis on our development
set WSJ Section 22 of the Penn-II Treebank. We
identified some initial pre-training transformations
to the treebank that help generation.
Punctuation: Punctuation is not usually en-
coded in f-structure representations. Because our
architecture is completely driven by rules con-
ditioned by f-structure information automatically
extracted from an f-structure annotated treebank,
its placement of punctuation is not principled.
This led to anomalies such as full stops appear-
ing mid sentence and quotation marks appearing
in undesired locations. One partial solution to this
was to reduce the amount of punctuation that the
system trained on. We removed all punctuation
apart from commas and full stops from the train-
ing data. We did not remove any punctuation from
the evaluation test set (Section 23), but our system
will ever only produce commas and full stops. In
the evaluation (Tables 3, 4, 5 and 6) we are pe-
nalised for the missing punctuation. To solve the
problem of full stops appearing mid sentence, we
carry out a punctuation post-processing step on all
generated strings. This removes mid-sentence full
stops and adds missing full stops at the end of gen-
erated sentences prior to evaluation. We are work-
ing on a more appropriate solution allowing the
system to generate all punctuation.
Case: English does not have much case mark-
ing, and for parsing no special treatment was en-
coded. However, when generating, it is very
important that the first person singular pronoun
is I in the nominative case and me in the ac-
cusative. Given the original grammar used in pars-
ing, our generation system was not able to distin-
guish nominative from accusative contexts. The
solution we implemented was to carry out a gram-
mar transformation in a pre-processing step, to au-
tomatically annotate personal pronouns with their
case information. This resulted in phrasal and lex-
ical rules such as NP(TSUBJ) —* PRP&amp;quot;nom(T=I)
and PRP&amp;quot;nom(T=I) —* I and greatly improved the
accuracy of the pronouns generated.
</bodyText>
<subsectionHeader confidence="0.944706">
4.2 String-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.999747571428572">
We evaluate the output of our generation system
against the raw strings of Section 23 using the
Simple String Accuracy and BLEU (Papineni et
al., 2002) evaluation metrics. Simple String Accu-
racy is based on the string edit distance between
the output of the generation system and the gold
standard sentence. BLEU is the weighted average
of n-gram precision against the gold standard sen-
tences. We also measure coverage as the percent-
age of input f-structures that generate a string. For
evaluation, we automatically expand all contracted
words. We only evaluate strings produced by the
system (similar to Nakanishi et al. (2005)).
We conduct a total of four experiments. The
parameters we investigate are lexical smoothing
(Section 3.3) and partial output. Partial output
is a robustness feature for cases where a sub-f-
structure component fails to generate a string and
the system outputs a concatenation of the strings
generated by the remaining components, rather
than fail completely.
</bodyText>
<page confidence="0.975799">
1037
</page>
<table confidence="0.996399529411765">
Sentence length of Evaluation &lt; 20 Section 23 Sentences of length: all
Training Data Metric &lt; 25 &lt; 30 &lt; 40
&lt; 20 BLEU 0.6812 0.6601 0.6373 0.6013 0.5793
String Accuracy 0.7274 0.7052 0.6875 0.6572 0.6431
Coverage 96.52 95.83 94.59 93.76 93.92
&lt; 25 BLEU 0.6915 0.6800 0.6696 0.6396 0.6233
String Accuracy 0.7262 0.7095 0.6983 0.6731 0.6618
Coverage 96.52 95.83 94.59 93.76 93.92
&lt; 30 BLEU 0.6979 0.6881 0.6792 0.6576 0.6445
String Accuracy 0.7317 0.7169 0.7075 0.6853 0.6749
Coverage 97.97 97.95 97.41 97.15 97.31
&lt; 40 BLEU 0.7045 0.6951 0.6852 0.6715 0.6605
String Accuracy 0.7349 0.7212 0.7074 0.6881 0.6788
Coverage 98.45 98.36 98.01 97.82 97.93
all BLEU 0.7077 0.6974 0.6859 0.6734 0.6651
String Accuracy 0.7373 0.7221 0.7087 0.6894 0.6808
Coverage 98.65 98.5 98.12 97.95 98.05
</table>
<tableCaption confidence="0.990151">
Table 3: Generation +partial output +lexical smoothing
</tableCaption>
<table confidence="0.9972">
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric &lt; 20 &lt; 25 &lt; 30 &lt; 40 all
all BLEU 0.6253 0.6097 0.5887 0.5730 0.5590
String Accuracy 0.6886 0.6688 0.6513 0.6317 0.6207
Coverage 91.20 91.19 90.84 90.33 90.11
</table>
<tableCaption confidence="0.999883">
Table 4: Generation +partial output -lexical smoothing
</tableCaption>
<bodyText confidence="0.999964333333334">
Varying the length of the sentences included in
the training data (Tables 3 and 5) shows that re-
sults improve (both in terms of coverage and string
quality) as the length of sentence included in the
training data increases.
Tables 3 and 5 give the results for the exper-
iments including lexical smoothing and varying
partial output. Table 3 (+partial, +smoothing)
shows that training on sentences of all lengths and
evaluating all strings (including partial outputs),
our system achieves coverage of 98.05%, a BLEU
score of 0.6651 and string accuracy of 0.6808. Ta-
ble 5 (-partial, +smoothing) shows that coverage
drops to 89.49%, BLEU score increases to 0.6979
and string accuracy to 0.7012, when the system
is trained on sentences of all lengths. Similarly,
for strings &lt;20, coverage drops from 98.65% to
95.26%, BLEU increases from 0.7077 to 0.7227
and String Accuracy from 0.7373 to 0.7476. In-
cluding partial output increases coverage (by more
than 8.5 percentage points for all sentences) and
hence robustness while slightly decreasing quality.
Tables 3 (+partial, +smoothing) and 4 (+partial,
-smoothing) give results for the experiments in-
cluding partial output but varying lexical smooth-
ing. With no lexical smoothing (Table 4), the
system (trained on all sentence lengths) produces
strings for 90.11% of the input f-structures and
achieves a BLEU score of 0.5590 and string ac-
curacy of 0.6207. Switching off lexical smooth-
ing has a negative effect on all evaluation met-
rics (coverage and quality), because many more
strings produced are now partial (since for FRED
values unseen during training, no lexical entries
are added to the chart).
Comparing Tables 5 (-partial, +smoothing)
and 6 (-partial, -smoothing), where the system
does not produce any partial outputs and lexi-
cal smoothing is varied, shows that training on
all sentence lengths, BLEU score increases from
0.6979 to 0.7147 and string accuracy increases
from 0.7012 to 0.7192. At the same time, cover-
age drops dramatically from 89.49% (Table 5) to
47.60% (Table 6).
Comparing Tables 4 and 6 shows that while par-
tial output almost doubles coverage, this comes
at a price of a severe drop in quality (BLEU
score drops from 0.7147 to 0.5590). On the other
hand, comparing Tables 5 and 6 shows that lexical
smoothing achieves a similar increase in coverage
with only a very slight drop in quality.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.97219">
Nakanishi et al. (2005) achieve 90.56% cover-
age and a BLEU score of 0.7723 on Section 23
</bodyText>
<page confidence="0.965498">
1038
</page>
<table confidence="0.996431882352941">
Sentence length of Evaluation &lt; 20 Section 23 Sentences of length: all
Training Data Metric &lt; 25 &lt; 30 &lt; 40
&lt; 20 BLEU 0.7326 0.7185 0.7165 0.7082 0.7052
String Accuracy 0.76 0.7428 0.7363 0.722 0.7175
Coverage 85.49 81.56 77.26 71.94 69.08
&lt; 25 BLEU 0.7300 0.7235 0.7218 0.7118 0.7077
String Accuracy 0.7517 0.7382 0.7315 0.7172 0.7116
Coverage 89.65 87.77 84.38 80.31 78.56
&lt; 30 BLEU 0.7207 0.7125 0.7107 0.6991 0.6946
String Accuracy 0.747 0.7336 0.7275 0.711 0.7045
Coverage 93.23 92.14 89.74 86.59 85.18
&lt; 40 BLEU 0.7221 0.7140 0.7106 0.7016 0.6976
String Accuracy 0.746 0.7331 0.7236 0.7072 0.7001
Coverage 94.58 93.85 91.89 89.62 88.33
all BLEU 0.7227 0.7145 0.7095 0.7011 0.6979
String Accuracy 0.7476 0.7331 0.7239 0.7077 0.7012
Coverage 95.26 94.40 92.55 90.69 89.49
</table>
<tableCaption confidence="0.995381">
Table 5: Generation -partial output +lexical smoothing
</tableCaption>
<table confidence="0.9969226">
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric &lt; 20 &lt; 25 &lt; 30 &lt; 40 all
all BLEU 0.7272 0.7237 0.7201 0.7160 0.7147
String Accuracy 0.7547 0.7436 0.7361 0.7237 0.7192
Coverage 61.99 57.38 53.64 47.60 47.60
</table>
<tableCaption confidence="0.999567">
Table 6: Generation -partial output -lexical smoothing
</tableCaption>
<bodyText confidence="0.999950133333333">
sentences, restricted to length ≤20 for efficiency
reasons. Langkilde-Geary’s (2002) best system
achieves 82.8% coverage, a BLEU score of 0.924
and string accuracy of 0.945 against Section 23
sentences of all lengths. Callaway (2003) achieves
98.7% coverage and a string accuracy of 0.6607
on sentences of all lengths. Our best results for
sentences of length :5 20 are coverage of 95.26%,
BLEU score of 0.7227 and string accuracy of
0.7476. For all sentence lengths, our best results
are coverage of 89.49%, a BLEU score of 0.6979
and string accuracy of 0.7012.
Using hand-crafted grammar-based genera-
tion systems (Langkilde-Geary, 2002; Callaway,
2003), it is possible to achieve very high results.
However, hand-crafted systems are expensive to
construct and not easily ported to new domains or
other languages. Our methodology, on the other
hand, is based on resources automatically acquired
from treebanks and easily ported to new domains
and languages, simply by retraining on suitable
data. Recent work on the automatic acquisition
of multilingual LFG resources from treebanks for
Chinese, German and Spanish (Burke et al., 2004;
Cahill et al., 2005; O’Donovan et al., 2005) has
shown that given a suitable treebank, it is possi-
ble to automatically acquire high quality LFG re-
sources in a very short space of time. The genera-
tion architecture presented here is easily ported to
those different languages and treebanks.
</bodyText>
<sectionHeader confidence="0.996626" genericHeader="conclusions">
6 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.99997247826087">
We present a new architecture for stochastic LFG
surface realisation using the automatically anno-
tated treebanks and extracted PCFG-based LFG
approximations of Cahill et al. (2004). Our model
maximises the probability of a tree given an f-
structure, supporting a simple and efficient imple-
mentation that scales to wide-coverage treebank-
based resources. An improved model would
maximise the probability of a string given an f-
structure by summing over trees with the same
yield. More research is required to implement
such a model efficiently using packed representa-
tions (Carroll and Oepen, 2005). Simple PCFG-
based models, while effective and computationally
efficient, can only provide approximations to LFG
and similar constraint-based formalisms (Abney,
1997). Research on discriminative disambigua-
tion methods (Valldal and Oepen, 2005; Nakanishi
et al., 2005) is important. Kaplan and Wedekind
(2000) show that for certain linguistically interest-
ing classes of LFG (and PATR etc.) grammars,
generation from f-structures yields a context free
language. Their proof involves the notion of a
</bodyText>
<page confidence="0.989573">
1039
</page>
<bodyText confidence="0.9999617">
“refinement” grammar where f-structure informa-
tion is compiled into CFG rules. Our probabilis-
tic generation grammars bear a conceptual similar-
ity to Kaplan and Wedekind’s “refinement” gram-
mars. It would be interesting to explore possible
connections between the treebank-based empirical
work presented here and the theoretical constructs
in Kaplan and Wedekind’s proofs.
We presented a full set of generation experi-
ments on varying sentence lengths training on Sec-
tions 02–21 of the Penn Treebank and evaluat-
ing on Section 23 strings. Sentences of length
≤20 achieve coverage of 95.26%, BLEU score
of 0.7227 and string accuracy of 0.7476 against
the raw Section 23 text. Sentences of all lengths
achieve coverage of 89.49%, BLEU score of
0.6979 and string accuracy of 0.7012. Our method
is robust and can cope with noise in the f-structure
input to generation and will attempt to produce
partial output rather than fail.
</bodyText>
<sectionHeader confidence="0.998196" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999755">
We gratefully acknowledge support from Science
Foundation Ireland grant 04/BR/CS0370 for the
research reported in this paper.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987561627907">
Stephen Abney. 1997. Stochastic Attribute-Value Gram-
mars. Computational Linguistics, 23(4):597–618.
Harald Baayen and Richard Sproat. 1996. Estimating lexi-
cal priors for low-frequency morphologically ambiguous
forms. Computational Linguistics, 22(2):155–166.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation. In
Proceedings of COLING 2000, pages 42–48, Saarbrcken,
Germany.
Srinivas Bangalore, John Chen, and Owen Rambow. 2001.
Impact of quality and quantity of corpora on stochastic
generation. In Proceedings of EMNLP 2001, pages 159–
166.
Anja Belz. 2005. Statistical generation: Three methods com-
pared and evaluated. In Proceedings of the 10th European
Workshop on Natural Language Generation (ENLG’ 05),
pages 15–23, Aberdeen, Scotland.
Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill,
Ruth O’Donovan, Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-Based Acquisition of a
Chinese Lexical-Functional Grammar. In Proceedings of
the 18th Pacific Asia Conference on Language, Informa-
tion and Computation, pages 161–172, Tokyo, Japan.
Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef van
Genabith, and Andy Way. 2004. Long-Distance De-
pendency Resolution in Automatically Acquired Wide-
Coverage PCFG-Based LFG Approximations. In Pro-
ceedings ofACL-04, pages 320–327, Barcelona, Spain.
Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-
Carthy, Ruth O’Donovan, Christian Rohrer, Josef van
Genabith, and Andy Way. 2005. Treebank-based acquisi-
tion of multilingual unification grammar resources. Jour-
nal of Research on Language and Computation; Special
Issue on “Shared Representations in Multilingual Gram-
mar Engineering”, pages 247–279.
Charles B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the Eigh-
teenth International Joint Conference on ArtificialIntelli-
gence, pages 811–817, Acapulco, Mexico.
John Carroll and Stephan Oepen. 2005. High efficiency real-
ization for a wide-coverage unification grammar. In Pro-
ceedings of IJCNLP05, pages 165–176, Jeju Island, Ko-
rea.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar, a Formal System for Grammatical Representa-
tion. In Joan Bresnan, editor, The Mental Representation
of Grammatical Relations, pages 173–281. MIT Press,
Cambridge, MA.
Ron Kaplan and Juergen Wedekind. 2000. LFG Generation
produces Context-free languages. In Proceedings of COL-
ING 2000, pages 141–148, Saarbruecken, Germany.
Martin Kay. 1996. Chart Generation. In Proceedings of the
34th Annual Meeting of the Associationfor Computational
Linguistics, pages 200–204, Santa Cruz, CA.
Irene Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sentence
generator. In Second International Natural Language
Generation Conference, pages 17–24, Harriman, NY.
Irene Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proceedings of NAACL 2000, pages 170–177,
Seattle, WA.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn Treebank: An-
notating Predicate Argument Structure. In Proceedings
of the ARPA Workshop on Human Language Technology,
pages 110–115, Princton, NJ.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005.
Probabilistic models for disambiguation of an HPSG-
based chart generator. In Proceedings of the International
Workshop on Parsing Technology, Vancouver, Canada.
Ruth O’Donovan, Aoife Cahill, Josef van Genabith, and
Andy Way. 2005. Automatic Acquisition of Spanish LFG
Resources from the CAST3LB Treebank. In Proceedings
ofLFG 05, pages 334–352, Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of
Machine Translation. In Proceedings ofACL 2002, pages
311–318, Philadelphia, PA.
Adwait Ratnaparkhi. 2000. Trainable methods for natu-
ral language generation. In Proceedings of NAACL 2000,
pages 194–201, Seattle, WA.
Erik Valldal and Stephan Oepen. 2005. Maximum En-
tropy Models for Realization Reranking. In Proceedings
of the 10th Machine Translation Summit, pages 109–116,
Phuket, Thailand.
</reference>
<page confidence="0.986967">
1040
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286987">
<title confidence="0.994917">Robust PCFG-Based Generation using Automatically Acquired LFG Approximations</title>
<author confidence="0.997698">Josef van</author>
<note confidence="0.74549">1National Centre for Language Technology (NCLT) School of Computing, Dublin City University, Dublin 9, Ireland 2Center for Advanced Studies, IBM Dublin, Ireland</note>
<abstract confidence="0.994039090909091">We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al., 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure. We evaluate our approach using stringbased evaluation. We currently achieve coverage of 95.26%, a BLEU score of 0.7227 and string accuracy of 0.7476 on</abstract>
<note confidence="0.495054">the Penn-II WSJ Section 23 sentences of</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Abney</author>
</authors>
<title>Stochastic Attribute-Value Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="6160" citStr="Abney, 1997" startWordPosition="934" endWordPosition="935"> parsing architectures: the pipeline and the integrated parsing architecture. In the pipeline architecture, a PCFG (or a history-based lexicalised generative parser) is extracted from the treebank and used to parse unseen text into trees, the resulting trees are annotated with f-structure equations by the f-structure annotation algorithm and a constraint solver produces an f-structure. In the in1The resources are approximations in that (i) they do not enforce LFG completeness and coherence constraints and (ii) PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). tegrated architecture, first the treebank trees are automatically annotated with f-structure information, f-structure annotated PCFGs with rules of the form NP(TOBJ=j)—*DT(T=j) NN(T=j) are extracted, syntactic categories followed by equations are treated as monadic CFG categories during grammar extraction and parsing, unseen text is parsed into trees with f-structure annotations, the annotations are collected and a constraint solver produces an f-structure. The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al. (2004). The generati</context>
<context position="27897" citStr="Abney, 1997" startWordPosition="4477" endWordPosition="4478">imations of Cahill et al. (2004). Our model maximises the probability of a tree given an fstructure, supporting a simple and efficient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekind’s “refinement” grammars. It would be interesting to explore possible connections between the tr</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Stephen Abney. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Richard Sproat</author>
</authors>
<title>Estimating lexical priors for low-frequency morphologically ambiguous forms.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="15009" citStr="Baayen and Sproat (1996)" startWordPosition="2396" endWordPosition="2399">e are no more rules to be added to the chart. At this stage, we search for the most probable rule with TOP as its LHS category and return the yield of this rule as the output of the generation process. Generation fails if there is no rule with LHS TOP at this level in the chart. 3.3 Lexical Smoothing Currently, the only smoothing in the system applies at the lexical level. Our backoff uses the built-in lexical macros4 of the automatic fstructure annotation algorithm of Cahill et al. (2004) to identify potential part-of-speech categories corresponding to a particular set of features. Following Baayen and Sproat (1996) we assume that unknown words have a probability distribution similar to hapax legomena. We add a lexical rule for each POS tag that corresponds to the fstructure features at that level to the chart with a probability computed from the original POS tag probability distribution multiplied by a very small constant. This means that lexical rules seen during training have a much higher probability than lexical rules added during the smoothing phase. Lexical smoothing has the advantage of boosting coverage (as shown in Tables 3, 4, 5 and 6 below) but slightly degrades the quality of the strings gen</context>
</contexts>
<marker>Baayen, Sproat, 1996</marker>
<rawString>Harald Baayen and Richard Sproat. 1996. Estimating lexical priors for low-frequency morphologically ambiguous forms. Computational Linguistics, 22(2):155–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING 2000,</booktitle>
<pages>42--48</pages>
<location>Saarbrcken, Germany.</location>
<contexts>
<context position="1555" citStr="Bangalore and Rambow (2000)" startWordPosition="216" endWordPosition="219">ve robustness and coverage at a fraction of the development cost of hand-crafted grammars. It is surprising to note that to date, such grammars do not usually figure in the complementary operation to parsing – natural language surface realisation. Research on statistical natural language surface realisation has taken three broad forms, differing in where statistical information is applied in the generation process. Langkilde (2000), for example, uses n-gram word statistics to rank alternative output strings from symbolic hand-crafted generators to select paths in parse forest representations. Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of COLING 2000, pages 42–48, Saarbrcken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Impact of quality and quantity of corpora on stochastic generation.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>159--166</pages>
<contexts>
<context position="2286" citStr="Bangalore et al. (2001)" startWordPosition="320" endWordPosition="323">tatistical and symbolic resources at intermediate generation stages. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts. None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation. Bangalore et al. (2001) investigate the effect of training size on performance while using grammars automatically extracted from the PennII Treebank (Marcus et al., 1994) for generation. Using an automatically extracted XTAG grammar, they achieve a string accuracy of 0.749 on their test set. Nakanishi et al. (2005) present probabilistic models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG). They investigate discriminative disambiguation models following Valldal and Oepen (2005) and their best model achieves coverage of 90.56% and a BLEU score of 0.7723 on Penn-II WSJ Se</context>
</contexts>
<marker>Bangalore, Chen, Rambow, 2001</marker>
<rawString>Srinivas Bangalore, John Chen, and Owen Rambow. 2001. Impact of quality and quantity of corpora on stochastic generation. In Proceedings of EMNLP 2001, pages 159– 166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Statistical generation: Three methods compared and evaluated.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Workshop on Natural Language Generation (ENLG’ 05),</booktitle>
<pages>15--23</pages>
<location>Aberdeen, Scotland.</location>
<contexts>
<context position="2030" citStr="Belz (2005)" startWordPosition="283" endWordPosition="284">native output strings from symbolic hand-crafted generators to select paths in parse forest representations. Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts. None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation. Bangalore et al. (2001) investigate the effect of training size on performance while using grammars automatically extracted from the PennII Treebank (Marcus et al., 1994) for generation. Using an automatically extracted XTAG grammar, they achieve a string accuracy of 0.749 on their test set. Nakanishi et al. (2005) present probabilistic models for a chart generator</context>
</contexts>
<marker>Belz, 2005</marker>
<rawString>Anja Belz. 2005. Statistical generation: Three methods compared and evaluated. In Proceedings of the 10th European Workshop on Natural Language Generation (ENLG’ 05), pages 15–23, Aberdeen, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Burke</author>
<author>Olivia Lam</author>
<author>Rowena Chan</author>
<author>Aoife Cahill</author>
<author>Ruth O’Donovan</author>
<author>Adams Bodomo</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Treebank-Based Acquisition of a Chinese Lexical-Functional Grammar.</title>
<date>2004</date>
<booktitle>In Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>161--172</pages>
<location>Tokyo, Japan.</location>
<marker>Burke, Lam, Chan, Cahill, O’Donovan, Bodomo, van Genabith, Way, 2004</marker>
<rawString>Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill, Ruth O’Donovan, Adams Bodomo, Josef van Genabith, and Andy Way. 2004. Treebank-Based Acquisition of a Chinese Lexical-Functional Grammar. In Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation, pages 161–172, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Long-Distance Dependency Resolution in Automatically Acquired WideCoverage PCFG-Based LFG Approximations.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL-04,</booktitle>
<pages>320--327</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef van Genabith, and Andy Way. 2004. Long-Distance Dependency Resolution in Automatically Acquired WideCoverage PCFG-Based LFG Approximations. In Proceedings ofACL-04, pages 320–327, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Michael Burke</author>
<author>Mairead McCarthy</author>
<author>Ruth O’Donovan</author>
<author>Christian Rohrer</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Treebank-based acquisition of multilingual unification grammar resources.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation; Special Issue on “Shared Representations in Multilingual Grammar Engineering”,</journal>
<pages>247--279</pages>
<marker>Cahill, Forst, Burke, McCarthy, O’Donovan, Rohrer, van Genabith, Way, 2005</marker>
<rawString>Aoife Cahill, Martin Forst, Michael Burke, Mairead McCarthy, Ruth O’Donovan, Christian Rohrer, Josef van Genabith, and Andy Way. 2005. Treebank-based acquisition of multilingual unification grammar resources. Journal of Research on Language and Computation; Special Issue on “Shared Representations in Multilingual Grammar Engineering”, pages 247–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on ArtificialIntelligence,</booktitle>
<pages>811--817</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="25914" citStr="Callaway (2003)" startWordPosition="4169" endWordPosition="4170"> 95.26 94.40 92.55 90.69 89.49 Table 5: Generation -partial output +lexical smoothing Sentence length of Evaluation Section 23 Sentences of length: Training Data Metric &lt; 20 &lt; 25 &lt; 30 &lt; 40 all all BLEU 0.7272 0.7237 0.7201 0.7160 0.7147 String Accuracy 0.7547 0.7436 0.7361 0.7237 0.7192 Coverage 61.99 57.38 53.64 47.60 47.60 Table 6: Generation -partial output -lexical smoothing sentences, restricted to length ≤20 for efficiency reasons. Langkilde-Geary’s (2002) best system achieves 82.8% coverage, a BLEU score of 0.924 and string accuracy of 0.945 against Section 23 sentences of all lengths. Callaway (2003) achieves 98.7% coverage and a string accuracy of 0.6607 on sentences of all lengths. Our best results for sentences of length :5 20 are coverage of 95.26%, BLEU score of 0.7227 and string accuracy of 0.7476. For all sentence lengths, our best results are coverage of 89.49%, a BLEU score of 0.6979 and string accuracy of 0.7012. Using hand-crafted grammar-based generation systems (Langkilde-Geary, 2002; Callaway, 2003), it is possible to achieve very high results. However, hand-crafted systems are expensive to construct and not easily ported to new domains or other languages. Our methodology, o</context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proceedings of the Eighteenth International Joint Conference on ArtificialIntelligence, pages 811–817, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Stephan Oepen</author>
</authors>
<title>High efficiency realization for a wide-coverage unification grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP05,</booktitle>
<pages>165--176</pages>
<location>Jeju Island,</location>
<contexts>
<context position="9958" citStr="Carroll and Oepen, 2005" startWordPosition="1539" endWordPosition="1542">rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features. Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 02–21 of the Penn Treebank. 3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. The generation grammar is first converted into Chomsky Normal Form (CNF). We recursively build a chart-like data structure in a bottom-up fashion. In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept. Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart. The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditioning f-structure features and its grammatical function label. At each level, grammatical function indexed charts are initially unordered. Charts are linearised by generation grammar rules once the charts themselves have produced the most probable</context>
<context position="27731" citStr="Carroll and Oepen, 2005" startWordPosition="4454" endWordPosition="4457">6 Conclusion and Further Work We present a new architecture for stochastic LFG surface realisation using the automatically annotated treebanks and extracted PCFG-based LFG approximations of Cahill et al. (2004). Our model maximises the probability of a tree given an fstructure, supporting a simple and efficient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic g</context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>John Carroll and Stephan Oepen. 2005. High efficiency realization for a wide-coverage unification grammar. In Proceedings of IJCNLP05, pages 165–176, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical Functional Grammar, a Formal System for Grammatical Representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3210" citStr="Kaplan and Bresnan, 1982" startWordPosition="462" endWordPosition="465">nt probabilistic models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG). They investigate discriminative disambiguation models following Valldal and Oepen (2005) and their best model achieves coverage of 90.56% and a BLEU score of 0.7723 on Penn-II WSJ Section 23 sentences of length &lt;20. In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from treebanks (Cahill et al., 2004). In Section 2 we briefly describe LFG (Kaplan and Bresnan, 1982). Section 3 presents our generation architecture. Section 4 presents evaluation results on the Penn-II WSJ Section 23 test set using string-based metrics. Section 5 compares our approach with alternative approaches in the literature. Section 6 concludes and outlines further research. 2 Lexical Functional Grammar Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) is a constraint-based theory of grammar. It (minimally) posits two levels of representation, c(onstituent)-structure and f(unctional)- structure. C-structure is represented by contextfree phrase-structure trees, and captures s</context>
<context position="17199" citStr="Kaplan and Bresnan, 1982" startWordPosition="2770" endWordPosition="2773">art of our evaluation, we experiment with sentences of varying length (20, 25, 30, 40, all), both in training and testing. Table 2 gives the number of training and test sentences for each sentence length. In each case, we use the automatically generated f-structures from Cahill et al. (2004) from the original Section 23 treebank trees as f-structure input to our generation experiments. We automatically mark adjunct and coordination scope in the input f-structure. Notice that these automatically generated f-structures are not “perfect”, i.e. they are not guaranteed to be complete and coherent (Kaplan and Bresnan, 1982): a local f-structure may contain material that is not supposed to be there (incoherence) and/or may be missing material that is supposed to be there (incompleteness). The results presented below show that our method is robust with respect to the quality of the f-structure input and will always attempt to generate partial output rather than fail. We consider this an important property as pristine generation input cannot always be guaranteed in realistic application scenarios, such as probabilistic transfer-based machine translation where generation input may contain a certain amount of noise. </context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ron Kaplan and Joan Bresnan. 1982. Lexical Functional Grammar, a Formal System for Grammatical Representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–281. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Juergen Wedekind</author>
</authors>
<title>LFG Generation produces Context-free languages.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING 2000,</booktitle>
<pages>141--148</pages>
<location>Saarbruecken, Germany.</location>
<contexts>
<context position="28039" citStr="Kaplan and Wedekind (2000)" startWordPosition="4495" endWordPosition="4498">icient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekind’s “refinement” grammars. It would be interesting to explore possible connections between the treebank-based empirical work presented here and the theoretical constructs in Kaplan and Wedekind’s proofs. We presented a full set of generati</context>
</contexts>
<marker>Kaplan, Wedekind, 2000</marker>
<rawString>Ron Kaplan and Juergen Wedekind. 2000. LFG Generation produces Context-free languages. In Proceedings of COLING 2000, pages 141–148, Saarbruecken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart Generation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Associationfor Computational Linguistics,</booktitle>
<pages>200--204</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="9712" citStr="Kay (1996)" startWordPosition="1504" endWordPosition="1505">esented as arrows), we automatically extract the rule S(T=1) —* NP(TSUBJ=1) VP(T=1) conditioned on the feature set {PRED,SUBJ,COMP,TENSE}. The probability of the rule is then calculated by counting the number of occurrences of that rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features. Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 02–21 of the Penn Treebank. 3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. The generation grammar is first converted into Chomsky Normal Form (CNF). We recursively build a chart-like data structure in a bottom-up fashion. In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept. Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart. The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditio</context>
<context position="13196" citStr="Kay (1996)" startWordPosition="2085" endWordPosition="2086">rules that can be applied. At this stage, it is not possible to add any more items to the sub-chart, therefore we propagate items in the chart that are compatible with the sub-chart index SUBJ. In our example, only the rule NP(↑SUBJ=↓) → NNP(↑=↓) (which yields the string John) is propagated to the next level up in the overall chart for consideration in the next iteration. If the yield of an item being propagated upwards in the chart is subsumed by an element already at that level, the subsumed item is removed. This results in efficiently treating the well known problem originally described in Kay (1996), where one unnecessarily retains sub-optimal strings. For example, generating the string “The very tall strong athletic man”, one does not want to keep variations such as “The very tall man”, or “The athletic man”, if one can generate the entire string. Our method ensures that only the most probable tree with the longest yield will be propagated upwards. The COMP-indexed chart at level f3 of the fstructure is constructed in a similar fashion. First the lexical rule V(↑=↓) → resigned is added. Next, conditioning on {PRED, SUBJ, TENSE}, the unary rule VP(↑=↓) → V(↑=↓) (with yield resigned) is a</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart Generation. In Proceedings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 200–204, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Second International Natural Language Generation Conference,</booktitle>
<pages>17--24</pages>
<location>Harriman, NY.</location>
<contexts>
<context position="26318" citStr="Langkilde-Geary, 2002" startWordPosition="4234" endWordPosition="4235">icted to length ≤20 for efficiency reasons. Langkilde-Geary’s (2002) best system achieves 82.8% coverage, a BLEU score of 0.924 and string accuracy of 0.945 against Section 23 sentences of all lengths. Callaway (2003) achieves 98.7% coverage and a string accuracy of 0.6607 on sentences of all lengths. Our best results for sentences of length :5 20 are coverage of 95.26%, BLEU score of 0.7227 and string accuracy of 0.7476. For all sentence lengths, our best results are coverage of 89.49%, a BLEU score of 0.6979 and string accuracy of 0.7012. Using hand-crafted grammar-based generation systems (Langkilde-Geary, 2002; Callaway, 2003), it is possible to achieve very high results. However, hand-crafted systems are expensive to construct and not easily ported to new domains or other languages. Our methodology, on the other hand, is based on resources automatically acquired from treebanks and easily ported to new domains and languages, simply by retraining on suitable data. Recent work on the automatic acquisition of multilingual LFG resources from treebanks for Chinese, German and Spanish (Burke et al., 2004; Cahill et al., 2005; O’Donovan et al., 2005) has shown that given a suitable treebank, it is possibl</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Second International Natural Language Generation Conference, pages 17–24, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>170--177</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1363" citStr="Langkilde (2000)" startWordPosition="189" endWordPosition="190">ences of length &lt;20. 1 Introduction Wide coverage grammars automatically extracted from treebanks are a corner-stone technology in state-of-the-art probabilistic parsing. They achieve robustness and coverage at a fraction of the development cost of hand-crafted grammars. It is surprising to note that to date, such grammars do not usually figure in the complementary operation to parsing – natural language surface realisation. Research on statistical natural language surface realisation has taken three broad forms, differing in where statistical information is applied in the generation process. Langkilde (2000), for example, uses n-gram word statistics to rank alternative output strings from symbolic hand-crafted generators to select paths in parse forest representations. Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation </context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proceedings of NAACL 2000, pages 170–177, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>110--115</pages>
<location>Princton, NJ.</location>
<contexts>
<context position="2433" citStr="Marcus et al., 1994" startWordPosition="343" endWordPosition="346">am or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts. None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation. Bangalore et al. (2001) investigate the effect of training size on performance while using grammars automatically extracted from the PennII Treebank (Marcus et al., 1994) for generation. Using an automatically extracted XTAG grammar, they achieve a string accuracy of 0.749 on their test set. Nakanishi et al. (2005) present probabilistic models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG). They investigate discriminative disambiguation models following Valldal and Oepen (2005) and their best model achieves coverage of 90.56% and a BLEU score of 0.7723 on Penn-II WSJ Section 23 sentences of length &lt;20. In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage, rob</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proceedings of the ARPA Workshop on Human Language Technology, pages 110–115, Princton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSGbased chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technology,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2579" citStr="Nakanishi et al. (2005)" startWordPosition="366" endWordPosition="369">uation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts. None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation. Bangalore et al. (2001) investigate the effect of training size on performance while using grammars automatically extracted from the PennII Treebank (Marcus et al., 1994) for generation. Using an automatically extracted XTAG grammar, they achieve a string accuracy of 0.749 on their test set. Nakanishi et al. (2005) present probabilistic models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG). They investigate discriminative disambiguation models following Valldal and Oepen (2005) and their best model achieves coverage of 90.56% and a BLEU score of 0.7723 on Penn-II WSJ Section 23 sentences of length &lt;20. In this paper we present a novel PCFG-based architecture for probabilistic generation based on wide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from treebanks (Cahill et al., 2004). In Section 2 we briefly describe</context>
<context position="20588" citStr="Nakanishi et al. (2005)" startWordPosition="3312" endWordPosition="3315">aluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al., 2002) evaluation metrics. Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence. BLEU is the weighted average of n-gram precision against the gold standard sentences. We also measure coverage as the percentage of input f-structures that generate a string. For evaluation, we automatically expand all contracted words. We only evaluate strings produced by the system (similar to Nakanishi et al. (2005)). We conduct a total of four experiments. The parameters we investigate are lexical smoothing (Section 3.3) and partial output. Partial output is a robustness feature for cases where a sub-fstructure component fails to generate a string and the system outputs a concatenation of the strings generated by the remaining components, rather than fail completely. 1037 Sentence length of Evaluation &lt; 20 Section 23 Sentences of length: all Training Data Metric &lt; 25 &lt; 30 &lt; 40 &lt; 20 BLEU 0.6812 0.6601 0.6373 0.6013 0.5793 String Accuracy 0.7274 0.7052 0.6875 0.6572 0.6431 Coverage 96.52 95.83 94.59 93.76</context>
<context position="24484" citStr="Nakanishi et al. (2005)" startWordPosition="3939" endWordPosition="3942">lexical smoothing is varied, shows that training on all sentence lengths, BLEU score increases from 0.6979 to 0.7147 and string accuracy increases from 0.7012 to 0.7192. At the same time, coverage drops dramatically from 89.49% (Table 5) to 47.60% (Table 6). Comparing Tables 4 and 6 shows that while partial output almost doubles coverage, this comes at a price of a severe drop in quality (BLEU score drops from 0.7147 to 0.5590). On the other hand, comparing Tables 5 and 6 shows that lexical smoothing achieves a similar increase in coverage with only a very slight drop in quality. 5 Discussion Nakanishi et al. (2005) achieve 90.56% coverage and a BLEU score of 0.7723 on Section 23 1038 Sentence length of Evaluation &lt; 20 Section 23 Sentences of length: all Training Data Metric &lt; 25 &lt; 30 &lt; 40 &lt; 20 BLEU 0.7326 0.7185 0.7165 0.7082 0.7052 String Accuracy 0.76 0.7428 0.7363 0.722 0.7175 Coverage 85.49 81.56 77.26 71.94 69.08 &lt; 25 BLEU 0.7300 0.7235 0.7218 0.7118 0.7077 String Accuracy 0.7517 0.7382 0.7315 0.7172 0.7116 Coverage 89.65 87.77 84.38 80.31 78.56 &lt; 30 BLEU 0.7207 0.7125 0.7107 0.6991 0.6946 String Accuracy 0.747 0.7336 0.7275 0.711 0.7045 Coverage 93.23 92.14 89.74 86.59 85.18 &lt; 40 BLEU 0.7221 0.714</context>
<context position="27998" citStr="Nakanishi et al., 2005" startWordPosition="4489" endWordPosition="4492">structure, supporting a simple and efficient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekind’s “refinement” grammars. It would be interesting to explore possible connections between the treebank-based empirical work presented here and the theoretical constructs in Kaplan and Wedekind’s pr</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSGbased chart generator. In Proceedings of the International Workshop on Parsing Technology, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth O’Donovan</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<date>2005</date>
<booktitle>Automatic Acquisition of Spanish LFG Resources from the CAST3LB Treebank. In Proceedings ofLFG 05,</booktitle>
<pages>334--352</pages>
<location>Bergen,</location>
<marker>O’Donovan, Cahill, van Genabith, Way, 2005</marker>
<rawString>Ruth O’Donovan, Aoife Cahill, Josef van Genabith, and Andy Way. 2005. Automatic Acquisition of Spanish LFG Resources from the CAST3LB Treebank. In Proceedings ofLFG 05, pages 334–352, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="20111" citStr="Papineni et al., 2002" startWordPosition="3236" endWordPosition="3239">l grammar used in parsing, our generation system was not able to distinguish nominative from accusative contexts. The solution we implemented was to carry out a grammar transformation in a pre-processing step, to automatically annotate personal pronouns with their case information. This resulted in phrasal and lexical rules such as NP(TSUBJ) —* PRP&amp;quot;nom(T=I) and PRP&amp;quot;nom(T=I) —* I and greatly improved the accuracy of the pronouns generated. 4.2 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al., 2002) evaluation metrics. Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence. BLEU is the weighted average of n-gram precision against the gold standard sentences. We also measure coverage as the percentage of input f-structures that generate a string. For evaluation, we automatically expand all contracted words. We only evaluate strings produced by the system (similar to Nakanishi et al. (2005)). We conduct a total of four experiments. The parameters we investigate are lexical smoothing (Section 3.3) and partial ou</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings ofACL 2002, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL 2000,</booktitle>
<pages>194--201</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1750" citStr="Ratnaparkhi (2000)" startWordPosition="246" endWordPosition="247">ing – natural language surface realisation. Research on statistical natural language surface realisation has taken three broad forms, differing in where statistical information is applied in the generation process. Langkilde (2000), for example, uses n-gram word statistics to rank alternative output strings from symbolic hand-crafted generators to select paths in parse forest representations. Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts. None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation. Bangalore et al. (2001) investigate the effect of training size on performance while us</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for natural language generation. In Proceedings of NAACL 2000, pages 194–201, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Valldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum Entropy Models for Realization Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>109--116</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="1922" citStr="Valldal and Oepen (2005)" startWordPosition="267" endWordPosition="270">nformation is applied in the generation process. Langkilde (2000), for example, uses n-gram word statistics to rank alternative output strings from symbolic hand-crafted generators to select paths in parse forest representations. Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Valldal and Oepen (2005) present a discriminative disambiguation model using a hand-crafted HPSG grammar for generation. Belz (2005) describes a method for building statistical generation models using an automatically created generation treebank for weather forecasts. None of these probabilistic approaches to NLG uses a full treebank grammar to drive generation. Bangalore et al. (2001) investigate the effect of training size on performance while using grammars automatically extracted from the PennII Treebank (Marcus et al., 1994) for generation. Using an automatically extracted XTAG grammar, they achieve a string acc</context>
<context position="27973" citStr="Valldal and Oepen, 2005" startWordPosition="4485" endWordPosition="4488">lity of a tree given an fstructure, supporting a simple and efficient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekind’s “refinement” grammars. It would be interesting to explore possible connections between the treebank-based empirical work presented here and the theoretical constructs in</context>
</contexts>
<marker>Valldal, Oepen, 2005</marker>
<rawString>Erik Valldal and Stephan Oepen. 2005. Maximum Entropy Models for Realization Reranking. In Proceedings of the 10th Machine Translation Summit, pages 109–116, Phuket, Thailand.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>