<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007330">
<title confidence="0.997408">
Towards a Unified Approach to Memory- and Statistical-Based
Machine Translation
</title>
<author confidence="0.99839">
Daniel Marcu
</author>
<affiliation confidence="0.92491275">
Information Sciences Institute and
Department of Computer Science
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.623391">
Marina del Rey, CA 90292
</address>
<email confidence="0.999311">
marcu@isi.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987631578948">
We present a set of algorithms that en-
able us to translate natural language
sentences by exploiting both a trans-
lation memory and a statistical-based
translation model. Our results show
that an automatically derived transla-
tion memory can be used within a sta-
tistical framework to often find trans-
lations of higher probability than those
found using solely a statistical model.
The translations produced using both
the translation memory and the sta-
tistical model are significantly better
than translations produced by two com-
mercial systems: our hybrid system
translated perfectly 58% of the 505
sentences in a test collection, while
the commercial systems translated per-
fectly only 40-42% of them.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980744186046">
Over the last decade, much progress has been
made in the fields of example-based (EBMT) and
statistical machine translation (SMT). EBMT sys-
tems work by modifying existing, human pro-
duced translation instances, which are stored in
a translation memory (TMEM). Many methods
have been proposed for storing translation pairs
in a TMEM, finding translation examples that
are relevant for translating unseen sentences, and
modifying and integrating translation fragments
to produce correct outputs. Sato (1992), for ex-
ample, stores complete parse trees in the TMEM
and selects and generates new translations by
performing similarity matchings on these trees.
Veale and Way (1997) store complete sentences;
new translations are generated by modifying the
TMEM translation that is most similar to the in-
put sentence. Others store phrases; new trans-
lations are produced by optimally partitioning
the input into phrases that match examples from
the TMEM (Maruyana and Watanabe, 1992), or
by finding all partial matches and then choosing
the best possible translation using a multi-engine
translation system (Brown, 1999).
With a few exceptions (Wu and Wong, 1998),
most SMT systems are couched in the noisy chan-
nel framework (see Figure 1). In this framework,
the source language, let’s say English, is assumed
to be generated by a noisy probabilistic source.1
Most of the current statistical MT systems treat
this source as a sequence of words (Brown et al.,
1993). (Alternative approaches exist, in which the
source is taken to be, for example, a sequence of
aligned templates/phrases (Wang, 1998; Och et
al., 1999) or a syntactic tree (Yamada and Knight,
2001).) In the noisy-channel framework, a mono-
lingual corpus is used to derive a statistical lan-
guage model that assigns a probability to a se-
quence of words or phrases, thus enabling one to
distinguish between sequences of words that are
grammatically correct and sequences that are not.
A sentence-aligned parallel corpus is then used
in order to build a probabilistic translation model
</bodyText>
<footnote confidence="0.983603">
1For the rest of this paper, we use the terms source
and target languages according to the jargon specific to the
noisy-channel framework. In this framework, the source lan-
guage is the language into which the machine translation
system translates.
</footnote>
<figure confidence="0.652923">
argmax P(e  |f) = argmax P(f  |e) P(e)
e e
</figure>
<figureCaption confidence="0.999955">
Figure 1: The noisy channel model.
</figureCaption>
<bodyText confidence="0.999730536585366">
that explains how the source can be turned into
the target and that assigns a probability to every
way in which a source e can be mapped into a tar-
get f. Once the parameters of the language and
translation models are estimated using traditional
maximum likelihood and EM techniques (Demp-
ster et al., 1977), one can take as input any string
in the target language f, and find the source e of
highest probability that could have generated the
target, a process called decoding (see Figure 1).
It is clear that EBMT and SMT systems have
different strengths and weaknesses. If a sen-
tence to be translated or a very similar one can be
found in the TMEM, an EBMT system has a good
chance of producing a good translation. How-
ever, if the sentence to be translated has no close
matches in the TMEM, then an EBMT system is
less likely to succeed. In contrast, an SMT sys-
tem may be able to produce perfect translations
even when the sentence given as input does not
resemble any sentence from the training corpus.
However, such a system may be unable to gener-
ate translations that use idioms and phrases that
reflect long-distance dependencies and contexts,
which are usually not captured by current transla-
tion models.
This paper advances the state-of-the-art in two
respects. First, we show how one can use an ex-
isting statistical translation model (Brown et al.,
1993) in order to automatically derive a statistical
TMEM. Second, we adapt a decoding algorithm
so that it can exploit information specific both to
the statistical TMEM and the translation model.
Our experiments show that the automatically de-
rived translation memory can be used within the
statistical framework to often find translations of
higher probability than those found using solely
the statistical model. The translations produced
ufsing both the translation memory and the statisti-
cal model are significantly better than translations
produced by two commercial systems.
</bodyText>
<sectionHeader confidence="0.975749" genericHeader="introduction">
2 The IBM Model 4
</sectionHeader>
<bodyText confidence="0.999440285714286">
For the work described in this paper we used a
modified version of the statistical machine trans-
lation tool developed in the context of the 1999
Johns Hopkins’ Summer Workshop (Al-Onaizan
et al., 1999), which implements IBM translation
model 4 (Brown et al., 1993).
IBM model 4 revolves around the notion of
word alignment over a pair of sentences (see Fig-
ure 2). The word alignment is a graphical repre-
sentation of an hypothetical stochastic process by
which a source string e is converted into a target
string f. The probability of a given alignment a
and target sentence f given a source sentence e is
given by
</bodyText>
<equation confidence="0.9279305">
P(a, fe) =
n e t e
</equation>
<bodyText confidence="0.946463533333333">
d e
d
NULL
where the factors delineated by symbols corre-
spond to hypothetical steps in the following gen-
erative process:
Each English word eis assigned with prob-
ability n e a fertility, which corre-
sponds to the number of French words into
which e is going to be translated.
Each English word eis then translated with
probability t einto a French word,
where ranges from 1 to the number of
words(fertility of e) into which eis
translated. For example, the English word
</bodyText>
<figure confidence="0.832041714285714">
best Decoder observed
e f
Source
P(e)
Channel
P(f  |e)
e
</figure>
<bodyText confidence="0.989094333333333">
“no” in Figure 2 is a word of fertility 2 that
is translated into “aucun” and “ne”.
The rest of the factors denote distorsion
probabilities (d), which capture the proba-
bility that words change their position when
translated from one language into another;
the probability of some French words being
generated from an invisible English NULL
element (p), etc. See (Brown et al., 1993)
or (Germann et al., 2001) for a detailed dis-
cussion of this translation model and a de-
scription of its parameters.
</bodyText>
<sectionHeader confidence="0.988813" genericHeader="method">
3 Building a statistical translation
memory
</sectionHeader>
<bodyText confidence="0.989811648648649">
Companies that specialize in producing high-
quality human translations of documentation and
news rely often on translation memory tools to in-
crease their productivity (Sprung, 2000). Build-
ing high-quality TMEM is an expensive process
that requires many person-years of work. Since
we are not in the fortunate position of having ac-
cess to an existing TMEM, we decided to build
one automatically.
We trained IBM translation model 4 on
500,000 English-French sentence pairs from
the Hansard corpus. We then used the Viterbi
alignment of each sentence, i.e., the alignment of
highest probability, to extract tuples of the form
, where represents
a contiguous English phrase,
represents a contiguous French phrase, and
represents the Viterbi align-
ment between the two phrases. We selected
only “contiguous” alignments, i.e., alignments in
which the words in the English phrase generated
only words in the French phrase and each word
in the French phrase was generated either by the
NULL word or a word from the English phrase.
We extracted only tuples in which the English
and French phrases contained at least two words.
For example, in the Viterbi alignment of the
two sentences in Figure 2, which was produced
automatically, “there” and “.” are words of fertil-
ity 0, NULL generates the French lexeme “.”, “is”
generates “est”, “no” generates “aucun” and “ne”,
and so on. From this alignment we extracted the
Figure 2: Example of Viterbi alignment produced
by IBM model 4.
six tuples shown in Table 1, because they were the
only ones that satisfied all conditions mentioned
above. For example, the pairno one ; aucun syn-
dicat particulier nedoes not occur in the transla-
tion memory because the French word “syndicat”
is generated by the word “union”, which does not
occur in the English phrase “no one”.
By extracting all tuples of the form
from the training corpus, we ended up with many
duplicates and with French phrases that were
paired with multiple English translations. We
chose for each French phrase only one possible
English translation equivalent. We tried out two
distinct methods for choosing a translation equiv-
alent, thus constructing two different probabilistic
TMEMs:
The Frequency-based Translation MEMory
(FTMEM) was created by associating with
each French phrase the English equivalent
that occurred most often in the collection of
phrases that we extracted.
The Probability-based Translation MEMory
(PTMEM) was created by associating with
each French phrase the English equivalent
that corresponded to the alignment of high-
est probability.
In contrast to other TMEMs, our TMEMs explic-
itly encode not only the mutual translation pairs
but also their corresponding word-level align-
ments, which are derived according to a certain
translation model (in our case, IBM model 4).
The mutual translations can be anywhere between
two words long to complete sentences. Both
methods yielded translation memories that con-
tained around 11.8 million word-aligned transla-
tion pairs. Due to efficiency considerations and
memory limitations — the software we wrote
loads a complete TMEM into the memory — we
used in our experiments only a fraction of the
TMEMs, those that contained phrases at most 10
</bodyText>
<table confidence="0.988203647058823">
English French Alignment
one union syndicat particulier one particulier; union syndicat
no one union aucun syndicat particulier ne no aucun, ne;
one particulier; union syndicat
is no one union aucun syndicat particulier ne est is est; no aucun, ne;
one particulier; union syndicat
there is no one union aucun syndicat particulier ne est is est; no aucun, ne;
one particulier; union syndicat
is no one union involved aucun syndicat particulier ne est en cause is est; no aucun, ne;
one particulier; union syndicat
involved en cause
there is no one union involved aucun syndicat particulier ne est en cause is est; no aucun, ne;
one particulier; union syndicat
involved en cause
there is no one union involved. aucun syndicat particulier ne est en cause . is est; no aucun, ne;
one particulier; union syndicat
involved en cause; NULL .
</table>
<tableCaption confidence="0.996177">
Table 1: Examples of automatically constructed statistical translation memory entries.
</tableCaption>
<table confidence="0.9998715">
TMEM Perfect Almost Incorrect Unable
perfect to judge
FTMEM 62.5% 8.5% 27.0% 2.0%
PTMEM 57.5% 7.5% 33.5% 1.5%
</table>
<tableCaption confidence="0.9092515">
Table 2: Accuracy of automatically constructed
TMEMs.
</tableCaption>
<bodyText confidence="0.99037575">
words long. This yielded a working FTMEM of
4.1 million and a PTMEM of 5.7 million phrase
translation pairs aligned at the word level using
IBM statistical model 4.
To evaluate the quality of both TMEMs we
built, we extracted randomly 200 phrase pairs
from each TMEM. These phrases were judged by
a bilingual speaker as
perfect translations if she could imagine con-
texts in which the aligned phrases could be
mutual translations of each other;
almost perfect translations if the aligned
phrases were mutual translations of each
other and one phrase contained one single
word with no equivalent in the other lan-
guage2;
incorrect translations if the judge could not
imagine any contexts in which the aligned
phrases could be mutual translations of each
other.
</bodyText>
<footnote confidence="0.92289125">
2For example, the translation pair “final , le secr´etaire
de” and “final act, the secretary of” were labeled as almost
perfect because the English word “act” has no French equiv-
alent.
</footnote>
<bodyText confidence="0.9989428">
The results of the evaluation are shown in Ta-
ble 2. A visual inspection of the phrases in our
TMEMs and the judgments made by the evaluator
suggest that many of the translations labeled as in-
correct make sense when assessed in a larger con-
text. For example, “autres r´egions de le pays que”
and “other parts of Canada than” were judged as
incorrect. However, when considered in a con-
text in which it is clear that “Canada” and “pays”
corefer, it would be reasonable to assume that the
translation is correct. Table 3 shows a few exam-
ples of phrases from our FTMEM and their corre-
sponding correctness judgments.
Although we found our evaluation to be ex-
tremely conservative, we decided nevertheless to
stick to it as it adequately reflects constraints spe-
cific to high-standard translation environments in
which TMEMs are built manually and constantly
checked for quality by specialized teams (Sprung,
2000).
</bodyText>
<sectionHeader confidence="0.491709333333333" genericHeader="method">
4 Statistical decoding using both a
statistical TMEM and a statistical
translation model
</sectionHeader>
<bodyText confidence="0.935603055555556">
The results in Table 2 show that about 70% of the
entries in our translation memory are correct or
almost correct (very easy to fix). It is, though, an
empirical question to what extend such TMEMs
can be used to improve the performance of cur-
rent translation systems. To determine this, we
modified an existing decoding algorithm so that it
can exploit information specific both to a statisti-
cal translation model and a statistical TMEM.
English French Judgment
, but I cannot say , mais je ne puis dire correct
how did this all come about ? comment est-ce arriv´ee ? correct
but , I humbly believe mais , a` mon humble avis correct
final act , the secretary of final , le secr´etaire de almost correct
other parts of Canada than autres r´egions de le pays que incorrect
what is the total amount accumulated a combien se ´el`eve la incorrect
that party present this ce parti pr´esent aujourd’hui incorrect
the airraft company to present further studies de autre ´etudes incorrect
</bodyText>
<tableCaption confidence="0.995022">
Table 3: Examples of TMEM entries with correctness judgments.
</tableCaption>
<bodyText confidence="0.999984523809524">
The decoding algorithm that we use is a greedy
one — see (Germann et al., 2001) for details. The
decoder guesses first an English translation for
the French sentence given as input and then at-
tempts to improve it by exploring greedily alter-
native translations from the immediate translation
space. We modified the greedy decoder described
by Germann et al. (2001) so that it attempts to
find good translation starting from two distinct
points in the space of possible translations: one
point corresponds to a word-for-word “gloss” of
the French input; the other point corresponds to
a translation that resembles most closely transla-
tions stored in the TMEM.
As discussed by Germann et al. (2001), the
word-for-word gloss is constructed by aligning
each French word f with its most likely En-
glish translation e f (e f argmax t(e f )).
For example, in translating the French sentence
“Bien entendu , il parle de une belle victoire .”,
the greedy decoder initially assumes that a good
translation of it is “Well heard, it talking a beauti-
ful victory” because the best translation of “bien”
is “well”, the best translation of “entendu” is
“heard”, and so on. A word-for-word gloss re-
sults (at best) in English words written in French
word order.
The translation that resembles most closely
translations stored in the TMEM is constructed
by deriving a “cover” for the input sentence using
phrases from the TMEM. The derivation attempts
to cover with translation pairs from the TMEM
as much of the input sentence as possible, using
the longest phrases in the TMEM. The words in
the input that are not part of any phrase extracted
from the TMEM are glossed. For example, this
approach may start the translation process from
the phrase “well , he is talking a beautiful victory”
if the TMEM contains the pairswell , ; bien en-
tendu ,andhe is talking; il parlebut no pair
with the French phrase “belle victoire”.
If the input sentence is found “as is” in the
translation memory, its translation is simply re-
turned and there is no further processing. Oth-
erwise, once an initial alignment is created, the
greedy decoder tries to improve it, i.e., it tries to
find an alignment (and implicitly a translation) of
higher probability by modifying locally the initial
alignment. The decoder attempts to find align-
ments and translations of higher probability by
employing a set of simple operations, such as
changing the translation of one or two words in
the alignment under consideration, inserting into
or deleting from the alignment words of fertility
zero, and swapping words or segments.
In a stepwise fashion, starting from the ini-
tial gloss or initial cover, the greedy decoder iter-
ates exhaustively over all alignments that are one
such simple operation away from the alignment
under consideration. At every step, the decoder
chooses the alignment of highest probability, un-
til the probability of the current alignment can no
longer be improved.
</bodyText>
<sectionHeader confidence="0.998123" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999374384615385">
We extracted from the test corpus a collection
of 505 French sentences, uniformly distributed
across the lengths 6, 7, 8, 9, and 10. For each
French sentence, we had access to the human-
generated English translation in the test corpus,
and to translations generated by two commercial
systems. We produced translations using three
versions of the greedy decoder: one used only the
statistical translation model, one used the trans-
lation model and the FTMEM, and one used the
translation model and the PTMEM.
We initially assessed how often the translations
obtained from TMEM seeds had higher proba-
</bodyText>
<table confidence="0.9923627">
Sent. Found Higher Same Higher
length in prob. result prob.
FTMEM from from
FTMEM gloss
6 33 9 43 16
7 27 9 48 17
8 29 16 42 14
9 31 15 28 27
10 31 9 43 18
All (%) 30% 12% 40% 18%
</table>
<tableCaption confidence="0.998873">
Table 4: The utility of the FTMEM.
</tableCaption>
<table confidence="0.9993765">
Sent. Found Higher Same Higher
length in prob. result prob.
FTMEM from from
FTMEM gloss
6 33 9 43 16
7 27 10 50 14
8 30 16 41 14
9 31 15 36 19
10 31 15 31 13
All (%) 31% 13% 41% 15%
</table>
<tableCaption confidence="0.999841">
Table 5: The utility of the PTMEM.
</tableCaption>
<bodyText confidence="0.999844">
bility than the translations obtained from simple
glosses. Tables 4 and 5 show that the transla-
tion memories significantly help the decoder find
translations of high probability. In about 30%
of the cases, the translations are simply copied
from a TMEM and in about 13% of the cases
the translations obtained from a TMEM seed have
higher probability that the best translations ob-
tained from a simple gloss. In 40% of the cases
both seeds (the TMEM and the gloss) yield the
same translation. Only in about 15-18% of the
cases the translations obtained from the gloss
are better than the translations obtained from the
TMEM seeds. It appears that both TMEMs help
the decoder find translations of higher probability
consistently, across all sentence lengths.
In a second experiment, a bilingual judge
scored the human translations extracted from the
automatically aligned test corpus; the transla-
tions produced by a greedy decoder that use both
TMEM and gloss seeds; the translations produced
by a greedy decoder that uses only the statistical
model and the gloss seed; and translations pro-
duced by two commercial systems (A and B).
If an English translation had the very same
meaning as the French original, it was con-
sidered semantically correct. If the mean-
ing was just a little different, the transla-
tion was considered semantically incorrect.
For example, “this is rather provision dis-
turbing” was judged as a correct semantical
translation of “voil`a une disposition plotˆot
inqui´etante”, but “this disposal is rather dis-
turbing” was judged as incorrect.
If a translation was perfect from a gram-
matical perspective, it was considered to be
grammatical. Otherwise, it was considered
incorrect. For example, “this is rather pro-
vision disturbing” was judged as ungram-
matical, although one may very easily make
sense of it.
We decided to use such harsh evaluation criteria
because, in previous experiments, we repeatedly
found that harsh criteria can be applied consis-
tently. To ensure consistency during evaluation,
the judge used a specialized interface: once the
correctness of a translation produced by a system
S was judged, the same judgment was automati-
cally recorded with respect to the other systems as
well. This way, it became impossible for a trans-
lation to be judged as correct when produced by
one system and incorrect when produced by an-
other system.
Table 6, which summarizes the results, displays
the percent of perfect translations (both semanti-
cally and grammatically) produced by a variety of
systems. Table 6 shows that translations produced
using both TMEM and gloss seeds are much bet-
ter than translations that do not use TMEMs.
The translation systems that use both a TMEM
and the statistical model outperform significantly
the two commercial systems. The figures in Ta-
ble 6 also reflect the harshness of our evaluation
metric: only 82% of the human translations ex-
tracted from the test corpus were considered per-
fect translation. A few of the errors were gen-
uine, and could be explained by failures of the
sentence alignment program that was used to cre-
ate the corpus (Melamed, 1999). Most of the er-
rors were judged as semantic, reflecting directly
the harshness of our evaluation metric.
</bodyText>
<sectionHeader confidence="0.998061" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.977656333333333">
The approach to translation described in this pa-
per is quite general. It can be applied in con-
junction with other statistical translation mod-
</bodyText>
<table confidence="0.995484375">
Sentence Humans Greedy with Greedy with Greedy without Commercial Commercial
length FTMEM PTMEM TMEM system A system B
6 92 72 70 52 55 59
7 73 58 52 37 42 43
8 80 53 52 30 38 29
9 84 53 53 37 40 35
10 85 57 60 36 40 37
All(%) 82% 58% 57% 38% 42% 40%
</table>
<tableCaption confidence="0.999255">
Table 6: Percent of perfect translations produced by various translation systems and algorithms.
</tableCaption>
<bodyText confidence="0.999790444444445">
els. And it can be applied in conjunction with
existing translation memories. To do this, one
would simply have to train the statistical model on
the translation memory provided as input, deter-
mine the Viterbi alignments, and enhance the ex-
isting translation memory with word-level align-
ments as produced by the statistical translation
model. We suspect that using manually produced
TMEMs can only increase the performance as
such TMEMs undergo periodic checks for qual-
ity assurance.
The work that comes closest to using a sta-
tistical TMEM similar to the one we propose
here is that of Vogel and Ney (2000), who au-
tomatically derive from a parallel corpus a hier-
archical TMEM. The hierarchical TMEM con-
sists of a set of transducers that encode a sim-
ple grammar. The transducers are automatically
constructed: they reflect common patterns of us-
age at levels of abstractions that are higher than
the words. Vogel and Ney (2000) do not evaluate
their TMEM-based system, so it is difficult to em-
pirically compare their approach with ours. From
a theoretical perspective, it appears though that
the two approaches are complementary: Vogel
and Ney (2000) identify abstract patterns of usage
and then use them during translation. This may
address the data sparseness problem that is char-
acteristic to any statistical modeling effort and
produce better translation parameters.
In contrast, our approach attempts to stir the
statistical decoding process into directions that
are difficult to reach when one relies only on
the parameters of a particular translation model.
For example, the two phrases “il est mort” and
“he kicked the bucket” may appear only in one
sentence in an arbitrary large corpus. The pa-
rameters learned from the entire corpus will very
likely associate very low probability to the words
“kicked” and “bucket” being translated into “est”
and “mort”. Because of this, a statistical-based
MT system will have trouble producing a trans-
lation that uses the phrase “kick the bucket”, no
matter what decoding technique it employs. How-
ever, if the two phrases are stored in the TMEM,
producing such a translation becomes feasible.
If optimal decoding algorithms capable of
searching exhaustively the space of all possible
translations existed, using TMEMs in the style
presented in this paper would never improve the
performance of a system. Our approach works
because it biases the decoder to search in sub-
spaces that are likely to yield translations of high
probability, subspaces which otherwise may not
be explored. The bias introduced by TMEMs is
a practical alternative to finding optimal transla-
tions, which is NP-complete (Knight, 1999).
It is clear that one of the main strengths of the
TMEM is its ability to encode contextual, long-
distance dependencies that are incongruous with
the parameters learned by current context poor,
reductionist channel models. Unfortunately, the
criterion used by the decoder in order to choose
between a translation produced starting from a
gloss and one produced starting from a TMEM
is biased in favor of the gloss-based translation. It
is possible for the decoder to produce a perfect
translation using phrases from the TMEM, and
yet, to discard the perfect translation in favor of
an incorrect translation of higher probability that
was obtained from a gloss (or from the TMEM).
It would be desirable to develop alternative rank-
ing techniques that would permit one to prefer in
some instances a TMEM-based translation, even
though that translation is not the best according
to the probabilistic channel model. The examples
in Table 7 shows though that this is not trivial: it
is not always the case that the translation of high-
Translations Does this translation Is this Is this the translation
use TMEM translation of highest
phrases? correct? probability?
monsieur le pr´esident , je aimerais savoir . yes yes yes
mr. speaker, i would like to know. no yes yes
mr. speaker, i would like to know.
je ne peux vous entendre , brian . yes yes yes
i cannot hear you, brian . no no no
i can you listen, brian .
alors , je termine l`a - dessus . yes yes no
therefore, i will conclude my remarks. no no yes
therefore , i conclude - over.
</bodyText>
<tableCaption confidence="0.958292">
Table 7: Example of system outputs, obtained with or without TMEM help.
</tableCaption>
<bodyText confidence="0.998651384615385">
est probability is the perfect one. The first French
sentence in Table 7 is correctly translated with or
without help from the translation memory. The
second sentence is correctly translated only when
the system uses a TMEM seed; and fortunately,
the translation of highest probability is the one
obtained using the TMEM seed. The translation
obtained from the TMEM seed is also correct for
the third sentence. But unfortunately, in this case,
the TMEM-based translation is not the most prob-
able.
Acknowledgments. This work was supported
by DARPA-ITO grant N66001-00-1-9814.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990015625">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Final Report, JHU Summer Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Ralph D. Brown. 1999. Adding linguistic knowledge
to a lexical example-based translation system. In
Proceedings of TMI’99, pages 22–32, Chester, Eng-
land.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical So-
ciety, 39(Ser B):1–38.
Ulrich Germann, Mike Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding
and optimal decoding for machine translation. In
Proceedings ofACL’01, Toulouse, France.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4).
H. Maruyana and H. Watanabe. 1992. Tree cover
search algorithm for example-based translation. In
Proceedings of TMI’92, pages 173–184.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107–130.
Franz Josef Och, Christoph Tillmann, and Herman
Ney. 1999. Improved alignment models for sta-
tistical machine translation. In Proceedings of
the EMNLP and VLC, pages 20–28, University of
Maryland, Maryland.
S. Sato. 1992. CTM: an example-based transla-
tion aid system using the character-based match re-
trieval method. In Proceedings of the 14th Inter-
national Conference on Computational Linguistics
(COLING’92), Nantes, France.
Robert C. Sprung, editor. 2000. Translating Into Suc-
cess: Cutting-Edge Strategies For Going Multilin-
gual In A Global Age. John Benjamins Publishers.
Tony Veale and Andy Way. 1997. Gaijin: A
template-based bootstrapping approach to example-
based machine translation. In Proceedings of New
Methods in Natural Language Processing”, Sofia,
Bulgaria.
S. Vogel and Herman Ney. 2000. Construction of a
hierarchical translation memory. In Proceedings of
COLING’00, pages 1131–1135, Saarbr¨ucken, Ger-
many.
Ye-Yi Wang. 1998. Grammar Inference and Statis-
tical Machine Translation. Ph.D. thesis, Carnegie
Mellon University. Also available as CMU-LTI
Technical Report 98-160.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In
Proceedings of ACL’98, pages 1408–1414, Mon-
treal, Canada.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
ofACL’01, Toulouse, France.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851578">
<title confidence="0.9996405">Towards a Unified Approach to Memoryand Statistical-Based Machine Translation</title>
<author confidence="0.999394">Daniel Marcu</author>
<affiliation confidence="0.966051666666667">Information Sciences Institute and Department of Computer Science University of Southern California</affiliation>
<address confidence="0.9948755">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.997971">marcu@isi.edu</email>
<abstract confidence="0.99789275">We present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statistical-based translation model. Our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model. The translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection, while the commercial systems translated perfectly only 40-42% of them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation. Final Report, JHU Summer Workshop.</title>
<date>1999</date>
<contexts>
<context position="5504" citStr="Al-Onaizan et al., 1999" startWordPosition="886" endWordPosition="889">e translation model. Our experiments show that the automatically derived translation memory can be used within the statistical framework to often find translations of higher probability than those found using solely the statistical model. The translations produced ufsing both the translation memory and the statistical model are significantly better than translations produced by two commercial systems. 2 The IBM Model 4 For the work described in this paper we used a modified version of the statistical machine translation tool developed in the context of the 1999 Johns Hopkins’ Summer Workshop (Al-Onaizan et al., 1999), which implements IBM translation model 4 (Brown et al., 1993). IBM model 4 revolves around the notion of word alignment over a pair of sentences (see Figure 2). The word alignment is a graphical representation of an hypothetical stochastic process by which a source string e is converted into a target string f. The probability of a given alignment a and target sentence f given a source sentence e is given by P(a, fe) = n e t e d e d NULL where the factors delineated by symbols correspond to hypothetical steps in the following generative process: Each English word eis assigned with probability</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Final Report, JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2442" citStr="Brown et al., 1993" startWordPosition="371" endWordPosition="374">ranslations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not. A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model 1For the rest of t</context>
<context position="4709" citStr="Brown et al., 1993" startWordPosition="762" endWordPosition="765"> be translated has no close matches in the TMEM, then an EBMT system is less likely to succeed. In contrast, an SMT system may be able to produce perfect translations even when the sentence given as input does not resemble any sentence from the training corpus. However, such a system may be unable to generate translations that use idioms and phrases that reflect long-distance dependencies and contexts, which are usually not captured by current translation models. This paper advances the state-of-the-art in two respects. First, we show how one can use an existing statistical translation model (Brown et al., 1993) in order to automatically derive a statistical TMEM. Second, we adapt a decoding algorithm so that it can exploit information specific both to the statistical TMEM and the translation model. Our experiments show that the automatically derived translation memory can be used within the statistical framework to often find translations of higher probability than those found using solely the statistical model. The translations produced ufsing both the translation memory and the statistical model are significantly better than translations produced by two commercial systems. 2 The IBM Model 4 For th</context>
<context position="6842" citStr="Brown et al., 1993" startWordPosition="1125" endWordPosition="1128">word eis then translated with probability t einto a French word, where ranges from 1 to the number of words(fertility of e) into which eis translated. For example, the English word best Decoder observed e f Source P(e) Channel P(f |e) e “no” in Figure 2 is a word of fertility 2 that is translated into “aucun” and “ne”. The rest of the factors denote distorsion probabilities (d), which capture the probability that words change their position when translated from one language into another; the probability of some French words being generated from an invisible English NULL element (p), etc. See (Brown et al., 1993) or (Germann et al., 2001) for a detailed discussion of this translation model and a description of its parameters. 3 Building a statistical translation memory Companies that specialize in producing highquality human translations of documentation and news rely often on translation memory tools to increase their productivity (Sprung, 2000). Building high-quality TMEM is an expensive process that requires many person-years of work. Since we are not in the fortunate position of having access to an existing TMEM, we decided to build one automatically. We trained IBM translation model 4 on 500,000 </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph D Brown</author>
</authors>
<title>Adding linguistic knowledge to a lexical example-based translation system.</title>
<date>1999</date>
<booktitle>In Proceedings of TMI’99,</booktitle>
<pages>22--32</pages>
<location>Chester, England.</location>
<contexts>
<context position="2097" citStr="Brown, 1999" startWordPosition="313" endWordPosition="314">(1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus i</context>
</contexts>
<marker>Brown, 1999</marker>
<rawString>Ralph D. Brown. 1999. Adding linguistic knowledge to a lexical example-based translation system. In Proceedings of TMI’99, pages 22–32, Chester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="3653" citStr="Dempster et al., 1977" startWordPosition="575" endWordPosition="579">e rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this framework, the source language is the language into which the machine translation system translates. argmax P(e |f) = argmax P(f |e) P(e) e e Figure 1: The noisy channel model. that explains how the source can be turned into the target and that assigns a probability to every way in which a source e can be mapped into a target f. Once the parameters of the language and translation models are estimated using traditional maximum likelihood and EM techniques (Dempster et al., 1977), one can take as input any string in the target language f, and find the source e of highest probability that could have generated the target, a process called decoding (see Figure 1). It is clear that EBMT and SMT systems have different strengths and weaknesses. If a sentence to be translated or a very similar one can be found in the TMEM, an EBMT system has a good chance of producing a good translation. However, if the sentence to be translated has no close matches in the TMEM, then an EBMT system is less likely to succeed. In contrast, an SMT system may be able to produce perfect translati</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39(Ser B):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Mike Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL’01,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="6868" citStr="Germann et al., 2001" startWordPosition="1130" endWordPosition="1133"> with probability t einto a French word, where ranges from 1 to the number of words(fertility of e) into which eis translated. For example, the English word best Decoder observed e f Source P(e) Channel P(f |e) e “no” in Figure 2 is a word of fertility 2 that is translated into “aucun” and “ne”. The rest of the factors denote distorsion probabilities (d), which capture the probability that words change their position when translated from one language into another; the probability of some French words being generated from an invisible English NULL element (p), etc. See (Brown et al., 1993) or (Germann et al., 2001) for a detailed discussion of this translation model and a description of its parameters. 3 Building a statistical translation memory Companies that specialize in producing highquality human translations of documentation and news rely often on translation memory tools to increase their productivity (Sprung, 2000). Building high-quality TMEM is an expensive process that requires many person-years of work. Since we are not in the fortunate position of having access to an existing TMEM, we decided to build one automatically. We trained IBM translation model 4 on 500,000 English-French sentence pa</context>
<context position="14349" citStr="Germann et al., 2001" startWordPosition="2357" endWordPosition="2360">ire correct how did this all come about ? comment est-ce arriv´ee ? correct but , I humbly believe mais , a` mon humble avis correct final act , the secretary of final , le secr´etaire de almost correct other parts of Canada than autres r´egions de le pays que incorrect what is the total amount accumulated a combien se ´el`eve la incorrect that party present this ce parti pr´esent aujourd’hui incorrect the airraft company to present further studies de autre ´etudes incorrect Table 3: Examples of TMEM entries with correctness judgments. The decoding algorithm that we use is a greedy one — see (Germann et al., 2001) for details. The decoder guesses first an English translation for the French sentence given as input and then attempts to improve it by exploring greedily alternative translations from the immediate translation space. We modified the greedy decoder described by Germann et al. (2001) so that it attempts to find good translation starting from two distinct points in the space of possible translations: one point corresponds to a word-for-word “gloss” of the French input; the other point corresponds to a translation that resembles most closely translations stored in the TMEM. As discussed by Germa</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Mike Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings ofACL’01, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="24615" citStr="Knight, 1999" startWordPosition="4093" endWordPosition="4094"> However, if the two phrases are stored in the TMEM, producing such a translation becomes feasible. If optimal decoding algorithms capable of searching exhaustively the space of all possible translations existed, using TMEMs in the style presented in this paper would never improve the performance of a system. Our approach works because it biases the decoder to search in subspaces that are likely to yield translations of high probability, subspaces which otherwise may not be explored. The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999). It is clear that one of the main strengths of the TMEM is its ability to encode contextual, longdistance dependencies that are incongruous with the parameters learned by current context poor, reductionist channel models. Unfortunately, the criterion used by the decoder in order to choose between a translation produced starting from a gloss and one produced starting from a TMEM is biased in favor of the gloss-based translation. It is possible for the decoder to produce a perfect translation using phrases from the TMEM, and yet, to discard the perfect translation in favor of an incorrect trans</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyana</author>
<author>H Watanabe</author>
</authors>
<title>Tree cover search algorithm for example-based translation.</title>
<date>1992</date>
<booktitle>In Proceedings of TMI’92,</booktitle>
<pages>173--184</pages>
<contexts>
<context position="1960" citStr="Maruyana and Watanabe, 1992" startWordPosition="291" endWordPosition="294">lation examples that are relevant for translating unseen sentences, and modifying and integrating translation fragments to produce correct outputs. Sato (1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phra</context>
</contexts>
<marker>Maruyana, Watanabe, 1992</marker>
<rawString>H. Maruyana and H. Watanabe. 1992. Tree cover search algorithm for example-based translation. In Proceedings of TMI’92, pages 173–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="21356" citStr="Melamed, 1999" startWordPosition="3549" endWordPosition="3550">ed by a variety of systems. Table 6 shows that translations produced using both TMEM and gloss seeds are much better than translations that do not use TMEMs. The translation systems that use both a TMEM and the statistical model outperform significantly the two commercial systems. The figures in Table 6 also reflect the harshness of our evaluation metric: only 82% of the human translations extracted from the test corpus were considered perfect translation. A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999). Most of the errors were judged as semantic, reflecting directly the harshness of our evaluation metric. 6 Discussion The approach to translation described in this paper is quite general. It can be applied in conjunction with other statistical translation modSentence Humans Greedy with Greedy with Greedy without Commercial Commercial length FTMEM PTMEM TMEM system A system B 6 92 72 70 52 55 59 7 73 58 52 37 42 43 8 80 53 52 30 38 29 9 84 53 53 37 40 35 10 85 57 60 36 40 37 All(%) 82% 58% 57% 38% 42% 40% Table 6: Percent of perfect translations produced by various translation systems and algo</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Herman Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the EMNLP and VLC,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>Maryland.</location>
<contexts>
<context position="2594" citStr="Och et al., 1999" startWordPosition="395" endWordPosition="398">ll partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not. A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model 1For the rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this framework, the source l</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Herman Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the EMNLP and VLC, pages 20–28, University of Maryland, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
</authors>
<title>CTM: an example-based translation aid system using the character-based match retrieval method.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING’92),</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="1491" citStr="Sato (1992)" startWordPosition="220" endWordPosition="221">n a test collection, while the commercial systems translated perfectly only 40-42% of them. 1 Introduction Over the last decade, much progress has been made in the fields of example-based (EBMT) and statistical machine translation (SMT). EBMT systems work by modifying existing, human produced translation instances, which are stored in a translation memory (TMEM). Many methods have been proposed for storing translation pairs in a TMEM, finding translation examples that are relevant for translating unseen sentences, and modifying and integrating translation fragments to produce correct outputs. Sato (1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown,</context>
</contexts>
<marker>Sato, 1992</marker>
<rawString>S. Sato. 1992. CTM: an example-based translation aid system using the character-based match retrieval method. In Proceedings of the 14th International Conference on Computational Linguistics (COLING’92), Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Sprung</author>
<author>editor</author>
</authors>
<title>Translating Into Success: Cutting-Edge Strategies For Going Multilingual In A Global Age.</title>
<date>2000</date>
<publisher>John Benjamins Publishers.</publisher>
<marker>Sprung, editor, 2000</marker>
<rawString>Robert C. Sprung, editor. 2000. Translating Into Success: Cutting-Edge Strategies For Going Multilingual In A Global Age. John Benjamins Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
<author>Andy Way</author>
</authors>
<title>Gaijin: A template-based bootstrapping approach to examplebased machine translation.</title>
<date>1997</date>
<booktitle>In Proceedings of New Methods in Natural Language Processing”,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1660" citStr="Veale and Way (1997)" startWordPosition="245" endWordPosition="248">the fields of example-based (EBMT) and statistical machine translation (SMT). EBMT systems work by modifying existing, human produced translation instances, which are stored in a translation memory (TMEM). Many methods have been proposed for storing translation pairs in a TMEM, finding translation examples that are relevant for translating unseen sentences, and modifying and integrating translation fragments to produce correct outputs. Sato (1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, le</context>
</contexts>
<marker>Veale, Way, 1997</marker>
<rawString>Tony Veale and Andy Way. 1997. Gaijin: A template-based bootstrapping approach to examplebased machine translation. In Proceedings of New Methods in Natural Language Processing”, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>Herman Ney</author>
</authors>
<title>Construction of a hierarchical translation memory.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING’00,</booktitle>
<pages>1131--1135</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="22570" citStr="Vogel and Ney (2000)" startWordPosition="3764" endWordPosition="3767">d algorithms. els. And it can be applied in conjunction with existing translation memories. To do this, one would simply have to train the statistical model on the translation memory provided as input, determine the Viterbi alignments, and enhance the existing translation memory with word-level alignments as produced by the statistical translation model. We suspect that using manually produced TMEMs can only increase the performance as such TMEMs undergo periodic checks for quality assurance. The work that comes closest to using a statistical TMEM similar to the one we propose here is that of Vogel and Ney (2000), who automatically derive from a parallel corpus a hierarchical TMEM. The hierarchical TMEM consists of a set of transducers that encode a simple grammar. The transducers are automatically constructed: they reflect common patterns of usage at levels of abstractions that are higher than the words. Vogel and Ney (2000) do not evaluate their TMEM-based system, so it is difficult to empirically compare their approach with ours. From a theoretical perspective, it appears though that the two approaches are complementary: Vogel and Ney (2000) identify abstract patterns of usage and then use them dur</context>
</contexts>
<marker>Vogel, Ney, 2000</marker>
<rawString>S. Vogel and Herman Ney. 2000. Construction of a hierarchical translation memory. In Proceedings of COLING’00, pages 1131–1135, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
</authors>
<title>Grammar Inference and Statistical Machine Translation.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<note>Also available as</note>
<contexts>
<context position="2575" citStr="Wang, 1998" startWordPosition="393" endWordPosition="394">by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not. A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model 1For the rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this fram</context>
</contexts>
<marker>Wang, 1998</marker>
<rawString>Ye-Yi Wang. 1998. Grammar Inference and Statistical Machine Translation. Ph.D. thesis, Carnegie Mellon University. Also available as CMU-LTI Technical Report 98-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Hongsing Wong</author>
</authors>
<title>Machine translation with a stochastic grammatical channel.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL’98,</booktitle>
<pages>1408--1414</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2140" citStr="Wu and Wong, 1998" startWordPosition="319" endWordPosition="322">parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language mod</context>
</contexts>
<marker>Wu, Wong, 1998</marker>
<rawString>Dekai Wu and Hongsing Wong. 1998. Machine translation with a stochastic grammatical channel. In Proceedings of ACL’98, pages 1408–1414, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL’01,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="2640" citStr="Yamada and Knight, 2001" startWordPosition="403" endWordPosition="406">e best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not. A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model 1For the rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this framework, the source language is the language into which the machine</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings ofACL’01, Toulouse, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>