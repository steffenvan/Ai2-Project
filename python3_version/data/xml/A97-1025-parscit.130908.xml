<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.055057">
<title confidence="0.988307">
Contextual Spelling Correction Using Latent Semantic Analysis
</title>
<author confidence="0.93601">
Michael P. Jones and James H. Martin
</author>
<affiliation confidence="0.9986815">
Dept. of Computer Science and Institute of Cognitive Science
University of Colorado
</affiliation>
<address confidence="0.542215">
Boulder, CO 80309-0430
</address>
<email confidence="0.904745">
{injones,martin}Qcs.colorado.edu
</email>
<sectionHeader confidence="0.992658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999717454545455">
Contextual spelling errors are defined as
the use of an incorrect, though valid, word
in a particular sentence or context. Tra-
ditional spelling checkers flag misspelled
words, but they do not typically attempt to
identify words that are used incorrectly in a
sentence. We explore the use of Latent Se-
mantic Analysis for correcting these incor-
rectly used words and the results are com-
pared to earlier work based on a Bayesian
classifier.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997215217391304">
Spelling checkers are now available for all major
word processing systems. However, these spelling
checkers only catch errors that result in misspelled
words. If an error results in a different, but incor-
rect word, it will go undetected. For example, quite
may easily be mistyped as quiet. Another type of er-
ror occurs when a writer simply doesn&apos;t know which
word of a set of homophones1 (or near homophones)
is the proper one for a particular context. For ex-
ample, the usage of affect and effect is commonly
confused.
Though the cause is different for the two types
of errors, we can treat them similarly by examining
the contexts in which they appear. Consequently,
no effort is made to distinguish between the two er-
ror types and both are called contextual spelling er-
rors. Kukich (1992a; 1992b) reports that 40% to
45% of observed spelling errors are contextual er-
rors. Sets of words which are frequently misused or
mistyped for one another are identified as confusion
sets. Thus, from our earlier examples, {quiet, quite}
and {affect, effect} are two separate confusion sets.
In this paper, we introduce Latent Semantic Anal-
ysis (LSA) as a method for correcting contextual
spelling errors for a given collection of confusion sets.
&apos;Homophones are words that sound the same, but are
spelled differently.
LSA was originally developed as a model for infor-
mation retrieval (Dumais et al., 1988; Deerwester et
al., 1990), but it has proven useful in other tasks
too. Some examples include an expert Expert lo-
cator (Streeter and Lochbaum, 1988) and a confer-
ence proceedings indexer (Foltz, 1995) which per-
forms better than a simple keyword-based index.
Recently, LSA has been proposed as a theory of se-
mantic learning (Landauer and Dumais, (In press)).
Our motivation in using LSA was to test its effec-
tiveness at predicting words based on a given sen-
tence and to compare it to a Bayesian classifier. LSA
makes predictions by building a high-dimensional,
&amp;quot;semantic&amp;quot; space which is used to compare the sim-
ilarity of the words from a confusion set to a given
context. The experimental results from LSA predic-
tion are then compared to both a baseline predic-
tor and a hybrid predictor based on trigrams and a
Bayesian classifier.
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998885636363637">
Latent Semantic Analysis has been applied to the
problem of spelling correction previously (Kukich,
1992b). However, this work focused on detect-
ing misspelled words, not contextual spelling errors.
The approach taken used letter n-grams to build the
semantic space. In this work, we use the words di-
rectly.
Yarowsky (1994) notes that conceptual spelling
correction is part of a closely related class of prob-
lems which include word sense disambiguation, word
choice selection in machine translation, and accent
and capitalization restoration. This class of prob-
lems has been attacked by many others. A number
of feature-based methods have been tried, including
Bayesian classifiers (Gale, Church, and Yarowsky,
1992; Golding, 1995), decision lists (Yarowsky,
1994), and knowledge-based approaches (McRoy,
1992). Recently, Golding and Schabes (1996) de-
scribed a system, Tribayes, that combines a trigram
model of the words&apos; parts of speech with a Bayesian
classifier. The trigram component of the system is
used to make decisions for those confusion sets that
</bodyText>
<page confidence="0.96415">
166
</page>
<equation confidence="0.940044125">
documents
terms
t x d
To
t x r
r xi
Do&apos;
r x d
</equation>
<figureCaption confidence="0.99953">
Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;.
</figureCaption>
<bodyText confidence="0.999419166666667">
contain words with different parts of speech. The
Bayesian component is used to predict the correct
word from among same part-of-speech words.
Golding and Schabes selected 18 confusion sets
from a list of commonly confused words plus a few
that represent typographical errors. They trained
their system using a random 80% of the Brown cor-
pus (Kueera and Francis, 1967). The remaining 20%
of the corpus was used to test how well the system
performed. We have chosen to use the same 18 con-
fusion sets and the Brown corpus in order to compare
LSA to Tribayes.
</bodyText>
<sectionHeader confidence="0.93889" genericHeader="method">
3 Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.998892888888889">
Latent Semantic Analysis (LSA) was developed at
Bellcore for use in information retrieval tasks (for
which it is also known as LSI) (Dumais et al., 1988;
Deerwester et al., 1990). The premise of the LSA
model is that an author begins with some idea or
information to be communicated. The selection of
particular lexical items in a collection of texts is
simply evidence for the underlying ideas or informa-
tion being presented. The goal of LSA, then, is to
take the &amp;quot;evidence&amp;quot; (i.e., words) presented and un-
cover the underlying semantics of the text passage.
Because many words are polysemous (have multi-
ple meanings) and synonymous (have meanings in
common with other words), the evidence available
in the text tends to be somewhat &amp;quot;noisy.&amp;quot; LSA at-
tempts to eliminate the noise from the data by first
representing the texts in a high-dimensional space
and then reducing the dimensionality of the space
to only the most important dimensions. This pro-
cess is described in more detail in Dumais (1988)
or Deerwester (1990), but a brief description is pro-
vided here.
A collection of texts is represented in matrix for-
mat. The rows of the matrix correspond to terms
and the columns represent documents. The indi-
vidual cell values are based on some function of the
term&apos;s frequency in the corresponding document and
its frequency in the whole collection. The func-
tion for selecting cell values will be discussed in sec-
tion 4.2. A singular value decomposition (SVD) is
performed on this matrix. SVD factors the origi-
nal matrix into the product of three matrices. We&apos;ll
identify these matrices as T, S, and D&apos; (see Figure 1).
The T matrix is a representation of the original term
vectors as vectors of derived orthogonal factor val-
ues. D&apos; is a similar representation for the original
document vectors. S is a diagonal matrix&apos; of rank r.
It is also called the singular value matrix. The sin-
gular values are sorted in decreasing order along the
diagonal. They represent a scaling factor for each
dimension in the T and D&apos; matrices.
Multiplying T, S, and D&apos; together perfectly repro-
duces the original representation of the text collec-
tion. Recall, however, that the original representa-
tion is expected to be noisy. What we really want
is an approximation of the original space that elim-
inates the majority of the noise and captures the
most important ideas or semantics of the texts.
An approximation of the original matrix is created
by eliminating some number of the least important
singular values in S. They correspond to the least
important (and hopefully, most noisy) dimensions
in the space. This step leaves a new matrix (S0) of
rank k.3 A similar reduction is made in T and D
by retaining the first k columns of T and the first
k rows of D&apos; as depicted in Figure 2. The product
of the resulting To, So, and D&apos;o matrices is a least
squares best fit reconstruction of the original matrix
(Eckart and Young, 1939). The reconstructed ma-
trix defines a space that represents or predicts the
frequency with which each term in the space would
appear in a given document or text segment given
an infinite sample of semantically similar texts (Lan-
</bodyText>
<footnote confidence="0.994906">
2A diagonal matrix is a square matrix that contains
non-zero values only along the diagonal running from the
upper left to the lower right.
3The number of factors k to be retained is generally
selected empirically.
</footnote>
<page confidence="0.97992">
167
</page>
<figure confidence="0.995339">
terms
documents
A
t x d
Do&apos;
k x k k x d
t x k
</figure>
<figureCaption confidence="0.9601465">
Figure 2: Results of reducing the T, S and D&apos; matrices produced by SVD to rank k. Recombining
the reduced matrices gives X, a least squares best fit reconstruction of the original matrix.
</figureCaption>
<bodyText confidence="0.997539181818182">
dauer and Dumais, (In press)).
New text passages can be projected into the space
by computing a weighted average of the term vectors
which correspond to the words in the new text. In
the contextual spelling correction task, we can gen-
erate a vector representation for each text passage
in which a confusion word appears. The similarity
between this text passage vector and the confusion
word vectors can be used to predict the most likely
word given the context or text in which it will ap-
pear.
</bodyText>
<sectionHeader confidence="0.996177" genericHeader="method">
4 Experimental Method
</sectionHeader>
<subsectionHeader confidence="0.984504">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99995294117647">
Separate corpora for training and testing LSA&apos;s abil-
ity to correct contextual word usage errors were cre-
ated from the Brown corpus (Kueera and Francis,
1967). The Brown corpus was parsed into individ-
ual sentences which are randomly assigned to either
a training corpus or a test corpus. Roughly 80%
of the original corpus was assigned as the training
corpus and the other 20% was reserved as the test
corpus. For each confusion set, only those sentences
in the training corpus which contained words in the
confusion set were extracted for construction of an
LSA space. Similarly, the sentences used to test the
LSA space&apos;s predictions were those extracted from
the test corpus which contained words from the con-
fusion set being examined. The details of the space
construction and testing method are described be-
low.
</bodyText>
<subsectionHeader confidence="0.993044">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.998468925925926">
Training the system consists of processing the train-
ing sentences and constructing an LSA space from
them. LSA requires the corpus to be segmented into
documents. For a given confusion set, an LSA space
is constructed by treating each training sentence as
a document. In other words, each training sentence
is used as a column in the LSA matrix. Before be-
. 1 68
ing processed by LSA, each sentence undergoes the
following transformations: context reduction, stem-
ming, bigram creation, and term weighting.
Context reduction is a step in which the sen-
tence is reduced in size to the confusion word plus
the seven words on either side of the word or up to
the sentence boundary. The average sentence length
in the corpus is 28 words, so this step has the effect
of reducing the size of the data to approximately
half the original. Intuitively, the reduction ought to
improve performance by disallowing the distantly lo-
cated words in long sentences to have any influence
on the prediction of the confusion word because they
usually have little or nothing to do with the selec-
tion of the proper word. In practice, however, the
reduction we use had little effect on the predictions
obtained from the LSA space.
We ran some experiments in which we built LSA
spaces using the whole sentence as well as other con-
text window sizes. Smaller context sizes didn&apos;t seem
to contain enough information to produce good pre-
dictions. Larger context sizes (up to the size of the
entire sentence) produced results which were not sig-
nificantly different from the results reported here.
However, using a smaller context size reduces the
total number of unique terms by an average of 13%.
Correspondingly, using fewer terms in the initial ma-
trix reduces the average running time and storage
space requirements by 17% and 10% respectively.
Stemming is the process of reducing each word to
its morphological root. The goal is to treat the dif-
ferent morphological variants of a word as the same
entity. For example, the words smile, smiled, smiles,
smiling, and smilingly (all from the corpus) are re-
duced to the root smile and treated equally. We
tried different stemming algorithms and all improved
the predictive performance of LSA. The results pre-
sented in this paper are based on Porter&apos;s (Porter,
1980) algorithm.
Bigram creation is performed for the words that
were not removed in the context reduction step.
Bigrams are formed between all adjacent pairs of
words. The bigrams are treated as additional terms
during the LSA space construction process. In other
words, the bigrams fill their own row in the LSA ma-
trix.
Term weighting is an effort to increase the
weight or importance of certain terms in the high
dimensional space. A local and global weighting
is given to each term in each sentence. The local
weight is a combination of the raw count of the par-
ticular term in the sentence and the term&apos;s prox-
imity to the confusion word. Terms located nearer
to the confusion word are given additional weight
in a linearly decreasing manner. The local weight
of each term is then flattened by taking its log2.
The global weight given to each term is an attempt
to measure its predictive power in the corpus as a
whole. We found that entropy (see also (Lochbaum
and Streeter, 1989)) performed best as a global mea-
sure. Furthermore, terms which did not appear in
more than one sentence in the training corpus were
removed.
While LSA can be used to quickly obtain satisfac-
tory results, some tuning of the parameters involved
can improve its performance. For example, we chose
(somewhat arbitrarily) to retain 100 factors for each
LSA space. We wanted to fix this variable for all
confusion sets and this number gives a good average
performance. However, tuning the number of factors
to select the &amp;quot;best&amp;quot; number for each space shows an
average of 2% improvement over all the results and
up to 8% for some confusion sets.
</bodyText>
<subsectionHeader confidence="0.989763">
4.3 Testing
</subsectionHeader>
<bodyText confidence="0.999823071428571">
Once the LSA space for a confusion set has been cre-
ated, it can be used to predict the word (from the
confusion set) most likely to appear in a given sen-
tence. We tested the predictive accuracy of the LSA
space in the following manner. A sentence from the
test corpus is selected and the location of the confu-
sion word in the sentence is treated as an unknown
word which must be predicted. One at a time, the
words from the confusion set are inserted into the
sentence at the location of the word to be predicted
and the same transformations that the training sen-
tences undergo are applied to the test sentence. The
inserted confusion word is then removed from the
sentence (but not the bigrams of which it is a part)
because its presence biases the comparison which oc-
curs later. A vector in LSA space is constructed from
the resulting terms.
The word predicted most likely to appear in a sen-
tence is determined by comparing the similarity of
each test sentence vector to each confusion word vec-
tor from the LSA space. Vector similarity is evalu-
ated by computing the cosine between two vectors.
The pair of sentence and confusion word vectors with
the largest cosine is identified and the corresponding
confusion word is chosen as the most likely word for
the test sentence. The predicted word is compared
to the correct word and a tally of correct predictions
is kept.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999991">
The results described in this section are based on the
18 confusion sets selected by Golding (1995; 1996).
Seven of the 18 confusion sets contain words that are
all the same part of speech and the remaining 11 con-
tain words with different parts of speech. Golding
and Schabes (1996) have already shown that using a
trigram model to predict words from a confusion set
based on the expected part of speech is very effec-
tive. Consequently, we will focus most of our atten-
tion on the seven confusion sets containing words of
the same part of speech. These seven sets are listed
first in all of our tables and figures. We also show
the results for the remaining 11 confusion sets for
comparison purposes, but as expected, these aren&apos;t
as good. We, therefore, consider our system com-
plementary to one (such as Tribayes) that predicts
based on part of speech when possible.
</bodyText>
<subsectionHeader confidence="0.967501">
5.1 Baseline Prediction System
</subsectionHeader>
<bodyText confidence="0.999983">
We describe our results in terms of a baseline predic-
tion system that ignores the context contained in the
test sentence and always predicts the confusion word
that occurred most frequently in the training corpus.
Table 1 shows the performance of this baseline pre-
dictor. The left half of the table lists the various con-
fusion sets. The next two columns show the training
and testing corpus sentence counts for each confu-
sion set. Because the sentences in the Brown corpus
are not tagged with a markup language, we identi-
fied individual sentences automatically based on a
small set of heuristics. Consequently, our sentence
counts for the various confusion sets differ slightly
from the counts reported in (Golding and Schabes,
1996).
The right half of Table 1 shows the most frequent
word in the training corpus from each confusion set.
Following the most frequent word is the baseline
performance data. Baseline performance is the per-
centage of correct predictions made by choosing the
given (most frequent) word. The percentage of cor-
rect predictions also represents the frequency of sen-
tences in the test corpus that contain the given word.
The final column lists the training corpus frequency
of the given word. The difference between the base-
line performance column and the training corpus
frequency column gives some indication about how
evenly distributed the words are between the two
corpora.
For example, there are 158 training sentences for
the confusion set {principal, principle} and 34 test
sentences. Since the word principle is listed in the
right half of the table, it must have appeared more
frequently in the training set. From the final column,
</bodyText>
<page confidence="0.99669">
169
</page>
<table confidence="0.999641368421053">
Confusion Set Train Test Most Freq. Base (Train Freq.)
principal principle 158 34 principle 41.2 (57.6)
raise rise • 117 36 rise 72.2 (65.0)
affect effect 193 53 effect 88.7 (85.0)
peace piece 257 62 peace 58.1 (59.5)
country county 389 91 country 59.3 (71.0)
amount number 480 122 number 75.4 (73.8)
among between 853 203 between 62.1 (66.7)
accept except 189 62 except 67.7 (73.5)
begin being 623 161 being 88.8 (89.4)
lead led 197 63 led 50.8 (52.3)
passed past 353 81 past 64.2 (63.2)
quiet quite 280 76 quite 88.2 (76.1)
weather whether 267 67 whether 73.1 (79.0)
cite sight site 128 32 sight 62.5 (54.7)
it&apos;s its 1577 391 its 84.7 (84.9)
than then 2497 578 than 58.8 (55.3)
you&apos;re your 734 220 your 86.8 (84.5)
their there they&apos;re 4176 978 there 53.4 (53.1)
</table>
<tableCaption confidence="0.868697">
Table 1: Baseline performance for 18 confusion sets. The table is divided into confusion sets
containing words of the same part of speech and those which have different parts of speech.
</tableCaption>
<bodyText confidence="0.97861175">
we can see that it occurred in almost 58% of the
training sentences. However, it occurs in only 41%
of the test sentences and thus the baseline predictor
scores only 41% for this confusion set.
</bodyText>
<subsectionHeader confidence="0.999787">
5.2 Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999982693548387">
Table 2 shows the performance of LSA on the con-
textual spelling correction task. The table provides
the baseline performance information for compari-
son to LSA. In all but the case of {amount, number),
LSA improves upon the baseline performance. The
improvement provided by LSA averaged over all con-
fusion sets is about 14% and for the sets with the
same part of speech, the average improvement is
16%.
Table 2 also gives the results obtained by Tribayes
as reported in (Golding and Schabes, 1996). The
baseline performance given in connection with Trib-
ayes corresponds to the partitioning of the Brown
corpus used to test Tribayes. It should be noted that
we did not implement Tribayes nor did we use the
same partitioning of the Brown corpus as Tribayes.
Thus, the comparison between LSA and Tribayes is
an indirect one.
The differences in the baseline predictor for each
system are a result of different partitions of the
Brown corpus. Both systems randomly split the
data such that roughly 80% is allocated to the train-
ing corpus and the remaining 20% is reserved for
the test corpus. Due to the random nature of this
process, however, the corpora must differ between
the two systems. The baseline predictor presented
in this paper and in (Golding and Schabes, 1996)
are based on the same method so the correspond-
ing columns in Table 2 can be compared to get an
idea of the distribution of sentences that contain the
most frequent word for each confusion set.
Examination of Table 2 reveals that it is difficult
to make a direct comparison between the results of
LSA and Tribayes due to the differences in the par-
titioning of the Brown corpus. Each system should
perform well on the most frequent confusion word
in the training data. Thus, the distribution of the
most frequent word between the the training and
the test corpus will affect the performance of the
system. Because the baseline score captures infor-
mation about the percentage of the test corpus that
should be easily predicted (i.e., the portion that con-
tains the most frequent word), we propose a com-
parison of the results by examination of the respec-
tive systems&apos; improvement over the baseline score
reported for each. The results of this comparison
are charted in Figure 3. The horizontal axis in the
figure represents the baseline predictor performance
for each system (even though it varies between the
two systems). The vertical bar thus represents the
performance above (or below) the baseline predictor
for each system on each confusion set.
LSA performs slightly better, on average, than
Tribayes for those confusion sets which contain
words of the same part of speech. Tribayes clearly
out-performs LSA for those words of a different part
of speech. Thus, LSA is doing better than the
Bayesian component of Tribayes, but it doesn&apos;t. in-
clude part of speech information and is therefore not.
capable of performing as well as the part. of speech
trigram component of Tribayes. Consequently, we
believe that LSA is a competitive alternative to
</bodyText>
<page confidence="0.988409">
170
</page>
<table confidence="0.9999583">
Confusion Set LSA Tribayes
Baseline LSA Baseline Tribayes
principal principle 41.2 91.2 58.8 88.2
raise rise 72.2 80.6 64.1 76.9
affect effect 88.7 94.3 91.8 95.9
peace piece 58.1 83.9 44.0 90.0
country county 59.3 81.3 91.9 85.5
amount number 75.4 56.6 71.5 82.9
among between 62.1 80.8 71.5 75.3
accept except 67.7 82.3 70.0 82.0
begin being 88.8 93.2 93.2 97.3
lead led 50.8 73.0 46.9 83.7
passed past 64.2 80.3 68.9 95.9
quiet quite 88.2 90.8 83.3 95.5
weather whether 73.1 85.1 86.9 93.4
cite sight site 62.5 78.1 64.7 70.6
it&apos;s its 84.7 92.8 91.3 98.1
than then 58.8 90.5 63.4 94.9
you&apos;re your 86.8 91.4 89.3 98.9
their there they&apos;re 53.4 73.9 56.8 97.6
</table>
<tableCaption confidence="0.9000025">
Table 2: LSA performance for 18 confusion sets. The results of Tribayes (Golding and Schabes,
1996) are also given.
</tableCaption>
<figureCaption confidence="0.977797">
Figure 3: Comparison of Tribayes vs. LSA performance above the baseline metric.
</figureCaption>
<figure confidence="0.997635647058824">
o_
a)
a)
.o-
4■•
- 1 0
a)
0
a)
Tribayes and LSA performance compared to baseline predictor
50
O Tribayes
0 LSA
40 -
3
° 20
10
</figure>
<page confidence="0.9443095">
-20
171
</page>
<bodyText confidence="0.997613">
a Bayesian classifier for making predictions among
words of the same part of speech.
</bodyText>
<subsectionHeader confidence="0.992919">
5.3 Performance Tuning
</subsectionHeader>
<bodyText confidence="0.999983090909091">
The results that have been presented here are based
on uniform treatment for each confusion set. That is,
the initial data processing steps and LSA space con-
struction parameters have all been the same. How-
ever, the model does not require equivalent treat-
ment of all confusion sets. In theory, we should be
able to increase the performance for each confusion
set by tuning the various parameters for each confu-
sion set.
In order to explore this idea further, we selected
the confusion set {amount, number} as a testbed
for performance tuning to a particular confusion set.
As previously mentioned, we can tune the number
of factors to a particular confusion set. In the case
of this confusion set, using 120 factors increases the
performance by 6%. However, tuning this param-
eter alone still leaves the performance short of the
baseline predictor.
A quick examination of the context in which both
words appear reveals that a significant percentage
(82%) of all training instances contain either the bi-
gram of the confusion word preceded by the, fol-
lowed by of, or in some cases, both. For exam-
ple, there are many instances of the collocation
the+number+of in the training data. However, there
are only one third as many training instances for
amount (the less frequent word) as there are for
number. This situation leads LSA to believe that the
bigrams the+amount and amount+of have more dis-
crimination power than the corresponding bigrams
which contain number. As a result, LSA gives them
a higher weight and LSA almost always predicts
amount when the confusion word in the test sen-
tence appears in this context. This local context is
a poor predictor of the confusion word and its pres-
ence tends to dominate the decision made by LSA.
By eliminating the words the and of from the train-
ing and testing process, we permit the remaining
context to be used for prediction. The elimination
of the poor local context combined with the larger
number of factors increases the performance of LSA
to 13% above the baseline predictor (compared to
11% for Tribayes). This is a net increase in perfor-
mance of 32%!
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999972806451613">
We&apos;ve shown that LSA can be used to attack the
problem of identifying contextual misuses of words,
particularly when those words are the same part of
speech. It has proven to be an effective alternative
to Bayesian classifiers. Confusions sets whose words
are different parts of speech are more effectively han-
dled using a method which incorporates the word&apos;s
part of speech as a feature. We are exploring tech-
niques for introducing part of speech information
into the LSA space so that the system can make
better predictions for those sets on which it doesn&apos;t
yet measure up to Tribayes. We&apos;ve also shown that
for the cost of experimentation with different param-
eter combinations, LSA&apos;s performance can be tuned
for individual confusion sets.
While the results of this experiment look very nice,
they still don&apos;t tell us anything about how useful the
technique is when applied to unedited text. The
testing procedure assumes that a confusion word
must be predicted as if the author of the text hadn&apos;t
supplied a word or that writers misuse the confusion
words nearly 50% of the time. For example, consider
the case of the confusion set {principal, principle}.
The LSA prediction accuracy for this set is 91%.
However, it might be the case that, in practice, peo-
ple tend to use the correct word 95% of the time.
LSA has thus introduced a 4% error into the writing
process. Our continuing work is to explore the error
rate that occurs in unedited text as a means of as-
sessing the &amp;quot;true&amp;quot; performance of contextual spelling
correction systems.
</bodyText>
<sectionHeader confidence="0.997619" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998705">
The first author is supported under DARPA con-
tract SOL BAA95-10. We gratefully acknowledge
the comments and suggestions of Thomas Landauer
and the anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.997739" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995296">
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harsh-
man. 1990. Indexing by Latent Semantic Analy-
sis. Journal of the American Society for Informa-
tion Science, 41(6):391-407, September.
Susan T. Dumais, George W. Furnas, Thomas K.
Landauer, Scott Deerwester, and Richard Harsh-
man. 1988. Using Latent Semantic Analysis to
improve access to textual information. In Human
Factors in Computing Systems, CHI&apos;88 Confer-
ence Proceedings (Washington, D.C.), pages 281-
285, New York, May. ACM.
Carl Eckart and Gale Young. 1939. A principle
axis transformation for non-hermitian matrices.
American Mathematical Society Bulletin, 45:118-
121.
Peter W. Foltz. 1995. Improving human-
proceedings interaction: Indexing the CHI index.
In Human Factors in Computing Systems: CHI95
Conference Companion, pages 101-102. Associa-
tions for Computing Machinery (ACM), May.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. A method for disambiguating
word senses in a large corpus. Computers and the
Humanities, 26(5-6):415-439, Dec.
</reference>
<page confidence="0.976959">
172
</page>
<reference confidence="0.968605041666666">
Andrew R. Golding. 1995. A Bayesian hybrid
method for context-sensitive spelling correction.
In Proceedings of the Third Workshop on Very
Large Corpora, Cambridge, MA.
Andrew R. Golding and Yves Schabes. 1996. Com-
bining trigram-based and feature-based methods
for context-sensitive spelling correction. In Pro-
ceedings of the 34th Annual Meeting of the Associ-
ation for Computational Linguistics, Santa Clara,
CA, June. Association for Computational Linguis-
tics.
Karen Kukich. 1992a. Spelling correction for the
telecommunications network for the deaf. Com-
munications of the ACM, 35(5):80-90, May.
Karen Kukich. 1992b. Techniques for automatically
correcting words in text. ACM Computing Sur-
veys, 24(4):377-439, Dec.
Henry Kueera and W. Nelson Francis. 1967. Com-
putational Analysis of Present-Day American En-
glish. Brown University Press, Providence, RI. -
Thomas K. Landauer and Susan T. Dumais. (In
press). A solution to Plato&apos;s problem: The La-
tent Semantic Analysis theory of acquisition, in-
duction, and representation of knowledge. Psy-
chological Review.
Karen E. Lochbaum and Lynn A. Streeter. 1989.
Comparing and combining the effectiveness of La-
tent Semantic Indexing and the ordinary vector
space model for information retrieval. Informa-
tion Processing e4 Management, 25(6):665-676.
Susan W. McRoy. 1992. Using multiple knowledge
sources for word sense disambiguation. Computa-
tional Linguistics, 18(1):1-30, March.
M. F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130-137, July.
Lynn A. Streeter and Karen E. Lochbaum. 1988.
An expert/expert-locating system based on au-
tomatic representation of semantic structure. In
Proceedings of the Fourth Conference on Artifi-
cial Intelligence Applications, pages 345-350, San
Diego, CA, March. IEEE.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restora-
tion in Spanish and French. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 88-95, Las Cruces,
NM, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999082">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.559750">
<title confidence="0.999618">Contextual Spelling Correction Using Latent Semantic Analysis</title>
<author confidence="0.997943">P Jones H Martin</author>
<affiliation confidence="0.999944">Dept. of Computer Science and Institute of Cognitive Science University of Colorado</affiliation>
<address confidence="0.999755">Boulder, CO 80309-0430</address>
<email confidence="0.999718">injonesQcs.colorado.edu</email>
<email confidence="0.999718">martinQcs.colorado.edu</email>
<abstract confidence="0.9384335">Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a We explore the use of Se- Analysis correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="2120" citStr="Deerwester et al., 1990" startWordPosition="339" endWordPosition="342">b) reports that 40% to 45% of observed spelling errors are contextual errors. Sets of words which are frequently misused or mistyped for one another are identified as confusion sets. Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Analysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. &apos;Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for information retrieval (Dumais et al., 1988; Deerwester et al., 1990), but it has proven useful in other tasks too. Some examples include an expert Expert locator (Streeter and Lochbaum, 1988) and a conference proceedings indexer (Foltz, 1995) which performs better than a simple keyword-based index. Recently, LSA has been proposed as a theory of semantic learning (Landauer and Dumais, (In press)). Our motivation in using LSA was to test its effectiveness at predicting words based on a given sentence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, &amp;quot;semantic&amp;quot; space which is used to compare the similarity of the wo</context>
<context position="4898" citStr="Deerwester et al., 1990" startWordPosition="802" endWordPosition="805">rds. Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that represent typographical errors. They trained their system using a random 80% of the Brown corpus (Kueera and Francis, 1967). The remaining 20% of the corpus was used to test how well the system performed. We have chosen to use the same 18 confusion sets and the Brown corpus in order to compare LSA to Tribayes. 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) was developed at Bellcore for use in information retrieval tasks (for which it is also known as LSI) (Dumais et al., 1988; Deerwester et al., 1990). The premise of the LSA model is that an author begins with some idea or information to be communicated. The selection of particular lexical items in a collection of texts is simply evidence for the underlying ideas or information being presented. The goal of LSA, then, is to take the &amp;quot;evidence&amp;quot; (i.e., words) presented and uncover the underlying semantics of the text passage. Because many words are polysemous (have multiple meanings) and synonymous (have meanings in common with other words), the evidence available in the text tends to be somewhat &amp;quot;noisy.&amp;quot; LSA attempts to eliminate the noise f</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard A. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391-407, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Scott Deerwester</author>
<author>Richard Harshman</author>
</authors>
<title>Using Latent Semantic Analysis to improve access to textual information.</title>
<date>1988</date>
<booktitle>In Human Factors in Computing Systems, CHI&apos;88 Conference Proceedings (Washington, D.C.),</booktitle>
<pages>281--285</pages>
<publisher>ACM.</publisher>
<location>New York,</location>
<contexts>
<context position="2094" citStr="Dumais et al., 1988" startWordPosition="335" endWordPosition="338">. Kukich (1992a; 1992b) reports that 40% to 45% of observed spelling errors are contextual errors. Sets of words which are frequently misused or mistyped for one another are identified as confusion sets. Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Analysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. &apos;Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for information retrieval (Dumais et al., 1988; Deerwester et al., 1990), but it has proven useful in other tasks too. Some examples include an expert Expert locator (Streeter and Lochbaum, 1988) and a conference proceedings indexer (Foltz, 1995) which performs better than a simple keyword-based index. Recently, LSA has been proposed as a theory of semantic learning (Landauer and Dumais, (In press)). Our motivation in using LSA was to test its effectiveness at predicting words based on a given sentence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, &amp;quot;semantic&amp;quot; space which is used to compar</context>
<context position="4872" citStr="Dumais et al., 1988" startWordPosition="798" endWordPosition="801">ame part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that represent typographical errors. They trained their system using a random 80% of the Brown corpus (Kueera and Francis, 1967). The remaining 20% of the corpus was used to test how well the system performed. We have chosen to use the same 18 confusion sets and the Brown corpus in order to compare LSA to Tribayes. 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) was developed at Bellcore for use in information retrieval tasks (for which it is also known as LSI) (Dumais et al., 1988; Deerwester et al., 1990). The premise of the LSA model is that an author begins with some idea or information to be communicated. The selection of particular lexical items in a collection of texts is simply evidence for the underlying ideas or information being presented. The goal of LSA, then, is to take the &amp;quot;evidence&amp;quot; (i.e., words) presented and uncover the underlying semantics of the text passage. Because many words are polysemous (have multiple meanings) and synonymous (have meanings in common with other words), the evidence available in the text tends to be somewhat &amp;quot;noisy.&amp;quot; LSA attempt</context>
</contexts>
<marker>Dumais, Furnas, Landauer, Deerwester, Harshman, 1988</marker>
<rawString>Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman. 1988. Using Latent Semantic Analysis to improve access to textual information. In Human Factors in Computing Systems, CHI&apos;88 Conference Proceedings (Washington, D.C.), pages 281-285, New York, May. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Eckart</author>
<author>Gale Young</author>
</authors>
<title>A principle axis transformation for non-hermitian matrices.</title>
<date>1939</date>
<journal>American Mathematical Society Bulletin,</journal>
<pages>45--118</pages>
<contexts>
<context position="7627" citStr="Eckart and Young, 1939" startWordPosition="1275" endWordPosition="1278"> majority of the noise and captures the most important ideas or semantics of the texts. An approximation of the original matrix is created by eliminating some number of the least important singular values in S. They correspond to the least important (and hopefully, most noisy) dimensions in the space. This step leaves a new matrix (S0) of rank k.3 A similar reduction is made in T and D by retaining the first k columns of T and the first k rows of D&apos; as depicted in Figure 2. The product of the resulting To, So, and D&apos;o matrices is a least squares best fit reconstruction of the original matrix (Eckart and Young, 1939). The reconstructed matrix defines a space that represents or predicts the frequency with which each term in the space would appear in a given document or text segment given an infinite sample of semantically similar texts (Lan2A diagonal matrix is a square matrix that contains non-zero values only along the diagonal running from the upper left to the lower right. 3The number of factors k to be retained is generally selected empirically. 167 terms documents A t x d Do&apos; k x k k x d t x k Figure 2: Results of reducing the T, S and D&apos; matrices produced by SVD to rank k. Recombining the reduced ma</context>
</contexts>
<marker>Eckart, Young, 1939</marker>
<rawString>Carl Eckart and Gale Young. 1939. A principle axis transformation for non-hermitian matrices. American Mathematical Society Bulletin, 45:118-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
</authors>
<title>Improving humanproceedings interaction: Indexing the CHI index.</title>
<date>1995</date>
<booktitle>In Human Factors in Computing Systems: CHI95 Conference Companion,</booktitle>
<pages>101--102</pages>
<institution>Associations for Computing Machinery (ACM),</institution>
<contexts>
<context position="2294" citStr="Foltz, 1995" startWordPosition="370" endWordPosition="371">s, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Analysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. &apos;Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for information retrieval (Dumais et al., 1988; Deerwester et al., 1990), but it has proven useful in other tasks too. Some examples include an expert Expert locator (Streeter and Lochbaum, 1988) and a conference proceedings indexer (Foltz, 1995) which performs better than a simple keyword-based index. Recently, LSA has been proposed as a theory of semantic learning (Landauer and Dumais, (In press)). Our motivation in using LSA was to test its effectiveness at predicting words based on a given sentence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, &amp;quot;semantic&amp;quot; space which is used to compare the similarity of the words from a confusion set to a given context. The experimental results from LSA prediction are then compared to both a baseline predictor and a hybrid predictor based on trigr</context>
</contexts>
<marker>Foltz, 1995</marker>
<rawString>Peter W. Foltz. 1995. Improving humanproceedings interaction: Indexing the CHI index. In Human Factors in Computing Systems: CHI95 Conference Companion, pages 101-102. Associations for Computing Machinery (ACM), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--5</pages>
<contexts>
<context position="3654" citStr="Gale, Church, and Yarowsky, 1992" startWordPosition="587" endWordPosition="591">eviously (Kukich, 1992b). However, this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that 166 documents terms t x d To t x r r xi Do&apos; r x d Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict the correct word from among sa</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26(5-6):415-439, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
</authors>
<title>A Bayesian hybrid method for context-sensitive spelling correction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="3670" citStr="Golding, 1995" startWordPosition="592" endWordPosition="593"> this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that 166 documents terms t x d To t x r r xi Do&apos; r x d Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict the correct word from among same part-of-speec</context>
<context position="15050" citStr="Golding (1995" startWordPosition="2568" endWordPosition="2569">t likely to appear in a sentence is determined by comparing the similarity of each test sentence vector to each confusion word vector from the LSA space. Vector similarity is evaluated by computing the cosine between two vectors. The pair of sentence and confusion word vectors with the largest cosine is identified and the corresponding confusion word is chosen as the most likely word for the test sentence. The predicted word is compared to the correct word and a tally of correct predictions is kept. 5 Results The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996). Seven of the 18 confusion sets contain words that are all the same part of speech and the remaining 11 contain words with different parts of speech. Golding and Schabes (1996) have already shown that using a trigram model to predict words from a confusion set based on the expected part of speech is very effective. Consequently, we will focus most of our attention on the seven confusion sets containing words of the same part of speech. These seven sets are listed first in all of our tables and figures. We also show the results for the remaining 11 confusion sets for comparison purposes</context>
</contexts>
<marker>Golding, 1995</marker>
<rawString>Andrew R. Golding. 1995. A Bayesian hybrid method for context-sensitive spelling correction. In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Yves Schabes</author>
</authors>
<title>Combining trigram-based and feature-based methods for context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Clara, CA,</location>
<contexts>
<context position="3787" citStr="Golding and Schabes (1996)" startWordPosition="604" endWordPosition="607"> letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that 166 documents terms t x d To t x r r xi Do&apos; r x d Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict the correct word from among same part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that repres</context>
<context position="15234" citStr="Golding and Schabes (1996)" startWordPosition="2599" endWordPosition="2602"> is evaluated by computing the cosine between two vectors. The pair of sentence and confusion word vectors with the largest cosine is identified and the corresponding confusion word is chosen as the most likely word for the test sentence. The predicted word is compared to the correct word and a tally of correct predictions is kept. 5 Results The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996). Seven of the 18 confusion sets contain words that are all the same part of speech and the remaining 11 contain words with different parts of speech. Golding and Schabes (1996) have already shown that using a trigram model to predict words from a confusion set based on the expected part of speech is very effective. Consequently, we will focus most of our attention on the seven confusion sets containing words of the same part of speech. These seven sets are listed first in all of our tables and figures. We also show the results for the remaining 11 confusion sets for comparison purposes, but as expected, these aren&apos;t as good. We, therefore, consider our system complementary to one (such as Tribayes) that predicts based on part of speech when possible. 5.1 Baseline Pr</context>
<context position="16583" citStr="Golding and Schabes, 1996" startWordPosition="2827" endWordPosition="2830">test sentence and always predicts the confusion word that occurred most frequently in the training corpus. Table 1 shows the performance of this baseline predictor. The left half of the table lists the various confusion sets. The next two columns show the training and testing corpus sentence counts for each confusion set. Because the sentences in the Brown corpus are not tagged with a markup language, we identified individual sentences automatically based on a small set of heuristics. Consequently, our sentence counts for the various confusion sets differ slightly from the counts reported in (Golding and Schabes, 1996). The right half of Table 1 shows the most frequent word in the training corpus from each confusion set. Following the most frequent word is the baseline performance data. Baseline performance is the percentage of correct predictions made by choosing the given (most frequent) word. The percentage of correct predictions also represents the frequency of sentences in the test corpus that contain the given word. The final column lists the training corpus frequency of the given word. The difference between the baseline performance column and the training corpus frequency column gives some indicatio</context>
<context position="19189" citStr="Golding and Schabes, 1996" startWordPosition="3273" endWordPosition="3276">ly 41% of the test sentences and thus the baseline predictor scores only 41% for this confusion set. 5.2 Latent Semantic Analysis Table 2 shows the performance of LSA on the contextual spelling correction task. The table provides the baseline performance information for comparison to LSA. In all but the case of {amount, number), LSA improves upon the baseline performance. The improvement provided by LSA averaged over all confusion sets is about 14% and for the sets with the same part of speech, the average improvement is 16%. Table 2 also gives the results obtained by Tribayes as reported in (Golding and Schabes, 1996). The baseline performance given in connection with Tribayes corresponds to the partitioning of the Brown corpus used to test Tribayes. It should be noted that we did not implement Tribayes nor did we use the same partitioning of the Brown corpus as Tribayes. Thus, the comparison between LSA and Tribayes is an indirect one. The differences in the baseline predictor for each system are a result of different partitions of the Brown corpus. Both systems randomly split the data such that roughly 80% is allocated to the training corpus and the remaining 20% is reserved for the test corpus. Due to t</context>
<context position="22473" citStr="Golding and Schabes, 1996" startWordPosition="3832" endWordPosition="3835">ct effect 88.7 94.3 91.8 95.9 peace piece 58.1 83.9 44.0 90.0 country county 59.3 81.3 91.9 85.5 amount number 75.4 56.6 71.5 82.9 among between 62.1 80.8 71.5 75.3 accept except 67.7 82.3 70.0 82.0 begin being 88.8 93.2 93.2 97.3 lead led 50.8 73.0 46.9 83.7 passed past 64.2 80.3 68.9 95.9 quiet quite 88.2 90.8 83.3 95.5 weather whether 73.1 85.1 86.9 93.4 cite sight site 62.5 78.1 64.7 70.6 it&apos;s its 84.7 92.8 91.3 98.1 than then 58.8 90.5 63.4 94.9 you&apos;re your 86.8 91.4 89.3 98.9 their there they&apos;re 53.4 73.9 56.8 97.6 Table 2: LSA performance for 18 confusion sets. The results of Tribayes (Golding and Schabes, 1996) are also given. Figure 3: Comparison of Tribayes vs. LSA performance above the baseline metric. o_ a) a) .o4■• - 1 0 a) 0 a) Tribayes and LSA performance compared to baseline predictor 50 O Tribayes 0 LSA 40 - 3 ° 20 10 -20 171 a Bayesian classifier for making predictions among words of the same part of speech. 5.3 Performance Tuning The results that have been presented here are based on uniform treatment for each confusion set. That is, the initial data processing steps and LSA space construction parameters have all been the same. However, the model does not require equivalent treatment of a</context>
</contexts>
<marker>Golding, Schabes, 1996</marker>
<rawString>Andrew R. Golding and Yves Schabes. 1996. Combining trigram-based and feature-based methods for context-sensitive spelling correction. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Clara, CA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Spelling correction for the telecommunications network for the deaf.</title>
<date>1992</date>
<journal>Communications of the ACM,</journal>
<pages>35--5</pages>
<contexts>
<context position="1489" citStr="Kukich (1992" startWordPosition="240" endWordPosition="241">s in a different, but incorrect word, it will go undetected. For example, quite may easily be mistyped as quiet. Another type of error occurs when a writer simply doesn&apos;t know which word of a set of homophones1 (or near homophones) is the proper one for a particular context. For example, the usage of affect and effect is commonly confused. Though the cause is different for the two types of errors, we can treat them similarly by examining the contexts in which they appear. Consequently, no effort is made to distinguish between the two error types and both are called contextual spelling errors. Kukich (1992a; 1992b) reports that 40% to 45% of observed spelling errors are contextual errors. Sets of words which are frequently misused or mistyped for one another are identified as confusion sets. Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Analysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. &apos;Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for information retrieval (Dumais et al.,</context>
<context position="3044" citStr="Kukich, 1992" startWordPosition="497" endWordPosition="498">umais, (In press)). Our motivation in using LSA was to test its effectiveness at predicting words based on a given sentence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, &amp;quot;semantic&amp;quot; space which is used to compare the similarity of the words from a confusion set to a given context. The experimental results from LSA prediction are then compared to both a baseline predictor and a hybrid predictor based on trigrams and a Bayesian classifier. 2 Related Work Latent Semantic Analysis has been applied to the problem of spelling correction previously (Kukich, 1992b). However, this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yaro</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992a. Spelling correction for the telecommunications network for the deaf. Communications of the ACM, 35(5):80-90, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<pages>24--4</pages>
<contexts>
<context position="1489" citStr="Kukich (1992" startWordPosition="240" endWordPosition="241">s in a different, but incorrect word, it will go undetected. For example, quite may easily be mistyped as quiet. Another type of error occurs when a writer simply doesn&apos;t know which word of a set of homophones1 (or near homophones) is the proper one for a particular context. For example, the usage of affect and effect is commonly confused. Though the cause is different for the two types of errors, we can treat them similarly by examining the contexts in which they appear. Consequently, no effort is made to distinguish between the two error types and both are called contextual spelling errors. Kukich (1992a; 1992b) reports that 40% to 45% of observed spelling errors are contextual errors. Sets of words which are frequently misused or mistyped for one another are identified as confusion sets. Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Analysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. &apos;Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for information retrieval (Dumais et al.,</context>
<context position="3044" citStr="Kukich, 1992" startWordPosition="497" endWordPosition="498">umais, (In press)). Our motivation in using LSA was to test its effectiveness at predicting words based on a given sentence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, &amp;quot;semantic&amp;quot; space which is used to compare the similarity of the words from a confusion set to a given context. The experimental results from LSA prediction are then compared to both a baseline predictor and a hybrid predictor based on trigrams and a Bayesian classifier. 2 Related Work Latent Semantic Analysis has been applied to the problem of spelling correction previously (Kukich, 1992b). However, this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yaro</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992b. Techniques for automatically correcting words in text. ACM Computing Surveys, 24(4):377-439, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kueera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI. -</location>
<contexts>
<context position="4504" citStr="Kueera and Francis, 1967" startWordPosition="731" endWordPosition="734">ith a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that 166 documents terms t x d To t x r r xi Do&apos; r x d Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict the correct word from among same part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that represent typographical errors. They trained their system using a random 80% of the Brown corpus (Kueera and Francis, 1967). The remaining 20% of the corpus was used to test how well the system performed. We have chosen to use the same 18 confusion sets and the Brown corpus in order to compare LSA to Tribayes. 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) was developed at Bellcore for use in information retrieval tasks (for which it is also known as LSI) (Dumais et al., 1988; Deerwester et al., 1990). The premise of the LSA model is that an author begins with some idea or information to be communicated. The selection of particular lexical items in a collection of texts is simply evidence for the underl</context>
<context position="8990" citStr="Kueera and Francis, 1967" startWordPosition="1517" endWordPosition="1520">ojected into the space by computing a weighted average of the term vectors which correspond to the words in the new text. In the contextual spelling correction task, we can generate a vector representation for each text passage in which a confusion word appears. The similarity between this text passage vector and the confusion word vectors can be used to predict the most likely word given the context or text in which it will appear. 4 Experimental Method 4.1 Data Separate corpora for training and testing LSA&apos;s ability to correct contextual word usage errors were created from the Brown corpus (Kueera and Francis, 1967). The Brown corpus was parsed into individual sentences which are randomly assigned to either a training corpus or a test corpus. Roughly 80% of the original corpus was assigned as the training corpus and the other 20% was reserved as the test corpus. For each confusion set, only those sentences in the training corpus which contained words in the confusion set were extracted for construction of an LSA space. Similarly, the sentences used to test the LSA space&apos;s predictions were those extracted from the test corpus which contained words from the confusion set being examined. The details of the </context>
</contexts>
<marker>Kueera, Francis, 1967</marker>
<rawString>Henry Kueera and W. Nelson Francis. 1967. Computational Analysis of Present-Day American English. Brown University Press, Providence, RI. -</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>(In press). A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge.</title>
<journal>Psychological Review.</journal>
<marker>Landauer, Dumais, </marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. (In press). A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen E Lochbaum</author>
<author>Lynn A Streeter</author>
</authors>
<title>Comparing and combining the effectiveness of Latent Semantic Indexing and the ordinary vector space model for information retrieval.</title>
<date>1989</date>
<booktitle>Information Processing e4 Management,</booktitle>
<pages>25--6</pages>
<contexts>
<context position="12922" citStr="Lochbaum and Streeter, 1989" startWordPosition="2189" endWordPosition="2192">rease the weight or importance of certain terms in the high dimensional space. A local and global weighting is given to each term in each sentence. The local weight is a combination of the raw count of the particular term in the sentence and the term&apos;s proximity to the confusion word. Terms located nearer to the confusion word are given additional weight in a linearly decreasing manner. The local weight of each term is then flattened by taking its log2. The global weight given to each term is an attempt to measure its predictive power in the corpus as a whole. We found that entropy (see also (Lochbaum and Streeter, 1989)) performed best as a global measure. Furthermore, terms which did not appear in more than one sentence in the training corpus were removed. While LSA can be used to quickly obtain satisfactory results, some tuning of the parameters involved can improve its performance. For example, we chose (somewhat arbitrarily) to retain 100 factors for each LSA space. We wanted to fix this variable for all confusion sets and this number gives a good average performance. However, tuning the number of factors to select the &amp;quot;best&amp;quot; number for each space shows an average of 2% improvement over all the results a</context>
</contexts>
<marker>Lochbaum, Streeter, 1989</marker>
<rawString>Karen E. Lochbaum and Lynn A. Streeter. 1989. Comparing and combining the effectiveness of Latent Semantic Indexing and the ordinary vector space model for information retrieval. Information Processing e4 Management, 25(6):665-676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Using multiple knowledge sources for word sense disambiguation.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="3749" citStr="McRoy, 1992" startWordPosition="601" endWordPosition="602"> The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that 166 documents terms t x d To t x r r xi Do&apos; r x d Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict the correct word from among same part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>Susan W. McRoy. 1992. Using multiple knowledge sources for word sense disambiguation. Computational Linguistics, 18(1):1-30, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<pages>14--3</pages>
<contexts>
<context position="11943" citStr="Porter, 1980" startWordPosition="2019" endWordPosition="2020">respondingly, using fewer terms in the initial matrix reduces the average running time and storage space requirements by 17% and 10% respectively. Stemming is the process of reducing each word to its morphological root. The goal is to treat the different morphological variants of a word as the same entity. For example, the words smile, smiled, smiles, smiling, and smilingly (all from the corpus) are reduced to the root smile and treated equally. We tried different stemming algorithms and all improved the predictive performance of LSA. The results presented in this paper are based on Porter&apos;s (Porter, 1980) algorithm. Bigram creation is performed for the words that were not removed in the context reduction step. Bigrams are formed between all adjacent pairs of words. The bigrams are treated as additional terms during the LSA space construction process. In other words, the bigrams fill their own row in the LSA matrix. Term weighting is an effort to increase the weight or importance of certain terms in the high dimensional space. A local and global weighting is given to each term in each sentence. The local weight is a combination of the raw count of the particular term in the sentence and the ter</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130-137, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn A Streeter</author>
<author>Karen E Lochbaum</author>
</authors>
<title>An expert/expert-locating system based on automatic representation of semantic structure.</title>
<date>1988</date>
<booktitle>In Proceedings of the Fourth Conference on Artificial Intelligence Applications,</booktitle>
<pages>345--350</pages>
<publisher>IEEE.</publisher>
<location>San Diego, CA,</location>
<contexts>
<context position="2243" citStr="Streeter and Lochbaum, 1988" startWordPosition="360" endWordPosition="363">d or mistyped for one another are identified as confusion sets. Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Analysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. &apos;Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for information retrieval (Dumais et al., 1988; Deerwester et al., 1990), but it has proven useful in other tasks too. Some examples include an expert Expert locator (Streeter and Lochbaum, 1988) and a conference proceedings indexer (Foltz, 1995) which performs better than a simple keyword-based index. Recently, LSA has been proposed as a theory of semantic learning (Landauer and Dumais, (In press)). Our motivation in using LSA was to test its effectiveness at predicting words based on a given sentence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, &amp;quot;semantic&amp;quot; space which is used to compare the similarity of the words from a confusion set to a given context. The experimental results from LSA prediction are then compared to both a basel</context>
</contexts>
<marker>Streeter, Lochbaum, 1988</marker>
<rawString>Lynn A. Streeter and Karen E. Lochbaum. 1988. An expert/expert-locating system based on automatic representation of semantic structure. In Proceedings of the Fourth Conference on Artificial Intelligence Applications, pages 345-350, San Diego, CA, March. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM,</location>
<contexts>
<context position="3262" citStr="Yarowsky (1994)" startWordPosition="532" endWordPosition="533">ional, &amp;quot;semantic&amp;quot; space which is used to compare the similarity of the words from a confusion set to a given context. The experimental results from LSA prediction are then compared to both a baseline predictor and a hybrid predictor based on trigrams and a Bayesian classifier. 2 Related Work Latent Semantic Analysis has been applied to the problem of spelling correction previously (Kukich, 1992b). However, this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; </context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 88-95, Las Cruces, NM, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>