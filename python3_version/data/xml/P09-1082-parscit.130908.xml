<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001100">
<title confidence="0.999045333333333">
Combining Lexical Semantic Resources
with Question &amp; Answer Archives
for Translation-Based Answer Finding
</title>
<author confidence="0.623179">
Delphine Bernhard and Iryna Gurevych
</author>
<affiliation confidence="0.67509675">
Ubiquitous Knowledge Processing (UKP) Lab
Computer Science Department
Technische Universit¨at Darmstadt, Hochschulstraße 10
D-64289 Darmstadt, Germany
</affiliation>
<email confidence="0.965767">
http://www.ukp.tu-darmstadt.de/
</email>
<sectionHeader confidence="0.993087" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999726304347826">
Monolingual translation probabilities have
recently been introduced in retrieval mod-
els to solve the lexical gap problem.
They can be obtained by training statisti-
cal translation models on parallel mono-
lingual corpora, such as question-answer
pairs, where answers act as the “source”
language and questions as the “target”
language. In this paper, we propose
to use as a parallel training dataset the
definitions and glosses provided for the
same term by different lexical semantic re-
sources. We compare monolingual trans-
lation models built from lexical semantic
resources with two other kinds of datasets:
manually-tagged question reformulations
and question-answer pairs. We also show
that the monolingual translation probabil-
ities obtained (i) are comparable to tradi-
tional semantic relatedness measures and
(ii) significantly improve the results over
the query likelihood and the vector-space
model for answer finding.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986384">
The lexical gap (or lexical chasm) often observed
between queries and documents or questions and
answers is a pervasive problem both in Informa-
tion Retrieval (IR) and Question Answering (QA).
This problem arises from alternative ways of con-
veying the same information, due to synonymy
or paraphrasing, and is especially severe for re-
trieval over shorter documents, such as sentence
retrieval or question retrieval in Question &amp; An-
swer archives. Several solutions to this problem
have been proposed including query expansion
(Riezler et al., 2007; Fang, 2008), query refor-
mulation or paraphrasing (Hermjakob et al., 2002;
Tomuro, 2003; Zukerman and Raskutti, 2002)
and semantic information retrieval (M¨uller et al.,
2007).
Berger and Lafferty (1999) have formulated a
further solution to the lexical gap problem con-
sisting in integrating monolingual statistical trans-
lation models in the retrieval process. Monolin-
gual translation models encode statistical word as-
sociations which are trained on parallel monolin-
gual corpora. The major drawback of this ap-
proach lies in the limited availability of truly par-
allel monolingual corpora. In practice, training
data for translation-based retrieval often consist in
question-answer pairs, usually extracted from the
evaluation corpus itself (Riezler et al., 2007; Xue
et al., 2008; Lee et al., 2008). While collection-
specific translation models effectively encode sta-
tistical word associations for the target document
collection, it also introduces a bias in the evalua-
tion and makes it difficult to assess the quality of
the translation model per se, independently from a
specific task and document collection.
In this paper, we propose new kinds of
datasets for training domain-independent mono-
lingual translation models. We use the defini-
tions and glosses provided for the same term
by different lexical semantic resources to auto-
matically train the translation models. This ap-
proach has been very recently made possible by
the emergence of new kinds of lexical seman-
tic and encyclopedic resources such as Wikipedia
and Wiktionary. These resources are freely avail-
able, up-to-date and have a broad coverage and
good quality. Thanks to the combination of sev-
eral resources, it is possible to obtain monolin-
gual parallel corpora which are large enough to
train domain-independent translation models. In
addition, we collected question-answer pairs and
manually-tagged question reformulations from a
social Q&amp;A site. We use these datasets to build
further translation models.
Translation-based retrieval models have been
</bodyText>
<page confidence="0.955148">
728
</page>
<note confidence="0.999613">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999068230769231">
widely used in practice by the IR and QA commu-
nity. However, the quality of the semantic infor-
mation encoded in the translation tables has never
been assessed intrinsically. To do so, we com-
pare translation probabilities with concept vector
based semantic relatedness measures with respect
to human relatedness rankings for reference word
pairs. This study provides empirical evidence for
the high quality of the semantic information en-
coded in statistical word translation tables. We
then use the translation models in an answer find-
ing task based on a new question-answer dataset
which is totally independent from the resources
used for training the translation models. This ex-
trinsic evaluation shows that our translation mod-
els significantly improve the results over the query
likelihood and the vector-space model.
The remainder of the paper is organised as fol-
lows. Section 2 discusses related work on seman-
tic relatedness and statistical translation models
for retrieval. Section 3 presents the monolingual
parallel datasets we used for obtaining monolin-
gual translation probabilities. Semantic related-
ness experiments are detailed in Section 4. Section
5 presents answer finding experiments. Finally, we
conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.977612">
2.1 Statistical Translation Models for
Retrieval
</subsectionHeader>
<bodyText confidence="0.999791793650794">
Statistical translation models for retrieval have
first been introduced by Berger and Lafferty
(1999). These models attempt to address syn-
onymy and polysemy problems by encoding sta-
tistical word associations trained on monolingual
parallel corpora. This method offers several ad-
vantages. First, it bases upon a sound mathe-
matical formulation of the retrieval model. Sec-
ond, it is not as computationally expensive as
other semantic retrieval models, since it only re-
lies on a word translation table which can easily
be computed before retrieval. The main draw-
back lies in the availability of suitable training data
for the translation probabilities. Berger and Laf-
ferty (1999) initially built synthetic training data
consisting of queries automatically generated from
documents. Berger et al. (2000) proposed to train
translation models on question-answer pairs taken
from Usenet FAQs and call-center dialogues, with
answers corresponding to the “source” language
and questions to the “target” language.
Subsequent work in this area often used simi-
lar kinds of training data such as question-answer
pairs from Yahoo! Answers (Lee et al., 2008) or
from the Wondir site (Xue et al., 2008). Lee et
al. (2008) tried to further improve translation mod-
els based on question-answer pairs by selecting the
most important terms to build compact translation
models.
Other kinds of training data have also been pro-
posed. Jeon et al. (2005) automatically clustered
semantically similar questions based on their an-
swers. Murdock and Croft (2005) created a first
parallel corpus of synonym pairs extracted from
WordNet, and an additional parallel corpus of En-
glish words translating to the same Arabic term in
a parallel English-Arabic corpus.
Similar work has also been performed in the
area of query expansion using training data con-
sisting of FAQ pages (Riezler et al., 2007) or
queries and clicked snippets from query logs (Rie-
zler et al., 2008).
All in all, translation models have been shown
to significantly improve the retrieval results
over traditional baselines for document retrieval
(Berger and Lafferty, 1999), question retrieval in
Question &amp; Answer archives (Jeon et al., 2005;
Lee et al., 2008; Xue et al., 2008) and for sentence
retrieval (Murdock and Croft, 2005).
Many of the approaches previously described
have used parallel data extracted from the retrieval
corpus itself. The translation models obtained are
therefore domain and collection-specific, which
introduces a bias in the evaluation and makes
it difficult to assess to what extent the transla-
tion model may be re-used for other tasks and
document collections. We henceforth propose a
new approach for building monolingual transla-
tion models relying on domain-independent lexi-
cal semantic resources. Moreover, we extensively
compare the results obtained by these models with
models obtained from a different type of dataset,
namely Question &amp; Answer archives.
</bodyText>
<subsectionHeader confidence="0.998723">
2.2 Semantic Relatedness
</subsectionHeader>
<bodyText confidence="0.99992">
The rationale behind translation-based retrieval
models is that monolingual translation probabil-
ities encode some form of semantic knowledge.
The semantic similarity and relatedness of words
has traditionally been assessed through corpus-
based and knowledge-based measures. Corpus-
based measures include Hyperspace Analogue to
</bodyText>
<page confidence="0.997052">
729
</page>
<bodyText confidence="0.99609425">
Language (HAL) (Lund and Burgess, 1996) and
Latent Semantic Analysis (LSA) (Landauer et al.,
1998). Knowledge-based measures rely on lexical
semantic resources such as WordNet and comprise
path length based measures (Rada et al., 1989)
and concept vector based measures (Qiu and Frei,
1993). These measures have recently also been ap-
plied to new collaboratively constructed resources
such as Wikipedia (Zesch et al., 2007) and Wik-
tionary (Zesch et al., 2008), with good results.
While classical measures of semantic related-
ness have been extensively studied and compared,
based on comparisons with human relatedness
judgements or word-choice problems, there is no
comparable intrinsic study of the relatedness mea-
sures obtained through word translation probabil-
ities. In this study, we use the correlation with
human rankings for reference word pairs to inves-
tigate how word translation probabilities compare
with traditional semantic relatedness measures. To
our knowledge, this is the first time that word-to-
word translation probabilities are used for ranking
word-pairs with respect to their semantic related-
ness.
</bodyText>
<sectionHeader confidence="0.989003" genericHeader="method">
3 Parallel Datasets
</sectionHeader>
<bodyText confidence="0.999971857142857">
In order to obtain parallel training data for the
translation models, we collected three different
datasets: manually-tagged question reformula-
tions and question-answer pairs from the WikiAn-
swers social Q&amp;A site (Section 3.1), and glosses
from WordNet, Wiktionary, Wikipedia and Simple
Wikipedia (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999318">
3.1 Social Q&amp;A Sites
</subsectionHeader>
<bodyText confidence="0.999956933333333">
Social Q&amp;A sites, such as Yahoo! Answers and
AnswerBag, provide portals where users can ask
their own questions as well as answer questions
from other users.
For our experiments we collected a dataset of
questions and answers, as well as question refor-
mulations, from the WikiAnswers1 (WA) web site.
WikiAnswers is a social Q&amp;A site similar to Ya-
hoo! Answers and AnswerBag. The main orig-
inality of WikiAnswers is that users might manu-
ally tag question reformulations in order to prevent
the duplication of answers to questions asking the
same thing in a different way. When a user enters
a question that is not already part of the question
repository, the web site displays a list of already
</bodyText>
<footnote confidence="0.9059">
1http://wiki.answers.com/
</footnote>
<bodyText confidence="0.999626404761905">
existing questions similar to the one just asked by
the user. The user may then freely select the ques-
tion which paraphrases her question, if available.
The question reformulations thus labelled by the
users are stored in order to retrieve the same an-
swer when a given question reformulation is asked
again.
We collected question-answer pairs and ques-
tion reformulations from the WikiAnswers site.
The resulting dataset contains 480,190 questions
with answers.2 We use this dataset in order to train
two different translation models:
Question-Answer Pairs (WAQA) In this set-
ting, question-answer pairs are considered as a
parallel corpus. Two different forms of combi-
nations are possible: (Q,A), where questions act
as source and answers as target, and (A,Q), where
answers act as source and questions as target. Re-
cent work by Xue et al. (2008) has shown that the
best results are obtained by pooling the question-
answer pairs {(q, a)1, ..., (q, a)n} and the answer-
question pairs {(a, q)1, ..., (a, q)n} for training,
so that we obtain the following parallel corpus:
{(q, a)1, ..., (q, a)n}∪{(a, q)1, ..., (a, q)n}. Over-
all, this corpus contains 1,227,362 parallel pairs
and will be referred to as WAQA (WikiAnswers
Question-Answers) in the rest of the paper.
Question Reformulations (WAQ) In this set-
ting, question and question reformulation pairs
are considered as a parallel corpus, e.g. ‘How
long do polar bears live?’ and ‘What is
the polar bear lifespan?’. For a given
user question q1, we retrieve its stored re-
formulations from the WikiAnswers dataset;
q11, q12, .... The original question and reformu-
lations are subsequently combined and pooled to
obtain a parallel corpus of question reformula-
tion pairs: {(q1, q11), (q1, q12), ..., (qn, qnm)} ∪
{(q11, q1), (q12, q1), ..., (qnm, qn)}. This corpus
contains 4,379,620 parallel pairs and will be re-
ferred to as WAQ (WikiAnswers Questions) in the
rest of the paper.
</bodyText>
<subsectionHeader confidence="0.999866">
3.2 Lexical Semantic Resources
</subsectionHeader>
<bodyText confidence="0.99996775">
Glosses and definitions for the same lexeme in dif-
ferent lexical semantic and encyclopedic resources
can actually be considered as near-paraphrases,
since they define the same terms and hence have
</bodyText>
<footnote confidence="0.483819">
2A question may have more than one answer.
</footnote>
<page confidence="0.910878">
730
</page>
<bodyText confidence="0.9976955">
gem moon
WAQ WAQA LSR ALLPool WAQ WAQA LSR ALLPool
gem explorer gem gem moon moon moon moon
95 ford diamonds xlt land earth lunar land
xlt gem gemstone 95 foot lunar sun earth
module xlt diamond explorer armstrong apollo earth landed
stones demand natural gemstone set landed tides armstrong
expedition lists facets diamonds actually neil moons neil
ring dash rare natural neil 1969 phase apollo
gemstone center synthetic diamond landed armstrong crescent set
modual play ruby ford apollo space astronomical foot
crystal lights usage ruby walked surface occurs actually
</bodyText>
<tableCaption confidence="0.97997">
Table 1: Sample top translations for different training data. ALL corresponds to WAQ+WAQA+LSR.
</tableCaption>
<bodyText confidence="0.624923">
the same meaning, as shown by the following ex-
ample for the lexeme “moon”:
</bodyText>
<listItem confidence="0.965059857142857">
• Wordnet (sense 1): the natural satellite of the
Earth.
• English Wiktionary: The Moon, the satellite
of planet Earth.
• English Wikipedia: The Moon (Latin: Luna)
is Earth’s only natural satellite and the fifth
largest natural satellite in the Solar System.
</listItem>
<bodyText confidence="0.988469">
We use glosses and definitions contained in the
following resources to build a parallel corpus:
</bodyText>
<listItem confidence="0.996108125">
• WordNet (Fellbaum, 1998). We use a freely
available API for WordNet (JWNL3) to ac-
cess WordNet 3.0.
• English Wiktionary. We use the Wiktionary
dump from January 11, 2009.
• English and Simple English Wikipedia. We
use the Wikipedia dump from February
6, 2007 and the Simple Wikipedia dump
</listItem>
<bodyText confidence="0.926642416666667">
from July 24, 2008. The Simple English
Wikipedia is an English Wikipedia targeted
at non-native speakers of English which uses
simpler words than the English Wikipedia.
Wikipedia and Simple Wikipedia articles do
not directly correspond to glosses such as
those found in dictionaries, we therefore con-
sidered the first paragraph in articles as a sur-
rogate for glosses.
Given a list of 86,584 seed lexemes extracted
from WordNet, we collected the glosses for each
lexeme from the four English resources described
</bodyText>
<footnote confidence="0.976073">
3http://sourceforge.net/projects/
jwordnet/
</footnote>
<bodyText confidence="0.999922875">
above. We then built pairs of glosses by consid-
ering each possible pair of resource. Given that a
lexeme might have different senses, and hence dif-
ferent glosses, it is possible to extract several gloss
pairs for one and the same lexeme and one and the
same pair of resources. It is therefore necessary to
perform word sense alignment. As we do not need
perfect training data, but rather large amounts of
training data, we used a very simple method con-
sisting in eliminating gloss pairs which did not at
least have one lemma in common (excluding stop
words and the seed lexeme itself).
The final pooled parallel corpus contains
307,136 pairs and is henceforth much smaller
than the previous datasets extracted from WikiAn-
swers. This corpus will be referred to as LSR.
</bodyText>
<subsectionHeader confidence="0.894678">
3.3 Translation Model Training
</subsectionHeader>
<bodyText confidence="0.99986625">
We used the GIZA++ SMT Toolkit4 (Och and
Ney, 2003) in order to obtain word-to-word
translation probabilities from the parallel datasets
described above. As is common practice in
translation-based retrieval, we utilised the IBM
translation model 1. The only pre-processing steps
performed for all parallel datasets were tokenisa-
tion and stop word removal.5
</bodyText>
<subsectionHeader confidence="0.945383">
3.4 Comparison of Word-to-Word
Translations
</subsectionHeader>
<bodyText confidence="0.930277">
Table 1 gives some examples of word-to-word
translations obtained for the different parallel cor-
pora used (the column ALLPool will be described
in the next section). As evidenced by this table,
</bodyText>
<footnote confidence="0.998823">
4http://code.google.com/p/giza-pp/
5For stop word removal we used the list avail-
ableat:http://truereader.com/manuals/onix/
stopwords1.html.
</footnote>
<page confidence="0.994529">
731
</page>
<bodyText confidence="0.999974285714286">
the different kinds of data encode different types
of information, including semantic relatedness and
similarity, as well as morphological relatedness.
As could be expected, the quality of the “trans-
lations” is variable and heavily dependent on the
training data: the WAQ and WAQA models reveal
the users’ interests, while the LSR model encodes
lexicographic and encyclopedic knowledge. For
instance, “gem” is an acronym for “generic elec-
tronic module”, which is found in Ford vehicles.
Since many question-answer pairs in WA are re-
lated to cars, this very particular use of “gem” is
predominant in the WAQ and WAQA translation
tables.
</bodyText>
<subsectionHeader confidence="0.993657">
3.5 Combination of the Datasets
</subsectionHeader>
<bodyText confidence="0.999887333333333">
In order to investigate the role played by differ-
ent kinds of training data, we combined the sev-
eral translation models, using the two methods de-
scribed by Xue et al. (2008). The first method con-
sists in a linear combination of the word-to-word
translation probabilities after training:
</bodyText>
<equation confidence="0.999339">
PLin(wilwj) = αPWAQ(wi|wj)
+ γPWAQA(wi|wj)
+ δPLSR(wi|wj) (1)
</equation>
<bodyText confidence="0.999937545454545">
where α + γ + δ = 1. This approach will be
labelled with the Lin subscript.
The second method consists in pooling the
training datasets, i.e. concatenating the parallel
corpora, before training. This approach will be
labelled with the Pool subscript. Examples for
word-to-word translations obtained with this type
of combination can be found in the last column for
each word in Table 1. The ALLPool setting corre-
sponds to the pooling of all three parallel datasets:
WAQ+WAQA+LSR.
</bodyText>
<sectionHeader confidence="0.997022" genericHeader="method">
4 Semantic Relatedness Experiments
</sectionHeader>
<bodyText confidence="0.98720115">
The aim of this first experiment is to perform an
intrinsic evaluation of the word translation proba-
bilities obtained by comparing them to traditional
semantic relatedness measures on the task of rank-
ing word pairs. Human judgements of semantic re-
latedness can be used to evaluate how well seman-
tic relatedness measures reflect human rankings by
correlating their ranking results with Spearman’s
rank correlation coefficient. Several evaluation
datasets are available for English, but we restrict
our study to the larger dataset created by Finkel-
stein et al. (2002) due to the low coverage of many
pairs in the word-to-word translation tables. This
dataset comprises two subsets, which have been
annotated by different annotators: Fin1–153, con-
taining 153 word pairs, and Fin2–200, containing
200 word pairs.
Word-to-word translation probabilities are com-
pared with a concept vector based measure relying
on Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), since this approach has been
shown to yield very good results (Zesch et al.,
2008). The method consists in representing words
as a concept vector, where concepts correspond to
WordNet synsets, Wikipedia article titles or Wik-
tionary entry names. Concept vectors for each
word are derived from the textual representation
available for each concept, i.e. glosses in Word-
Net, the full article or the first paragraph of the
article in Wikipedia or the full contents of a Wik-
tionary entry. We refer the reader to (Gabrilovich
and Markovitch, 2007; Zesch et al., 2008) for tech-
nical details on how the concept vectors are built
and used to obtain semantic relatedness values.
Table 2 lists Spearman’s rank correlation coeffi-
cients obtained for concept vector based measures
and translation probabilities. In order to ensure
a fair evaluation, we limit the comparison to the
word pairs which are contained in all resources
and translation tables.
</bodyText>
<table confidence="0.999815">
Dataset Fin1-153 Fin2-200
Word pairs used 46 42
Concept vectors
WordNet .26 .46
Wikipedia .27 .03
WikipediaFirst .30 .38
Wiktionary .39 .58
Translation probabilities
WAQ .43 .65
WAQA .54 .37
LSR .51 .29
ALLPool .52 .57
</table>
<tableCaption confidence="0.986491">
Table 2: Spearman’s rank correlation coefficients
</tableCaption>
<bodyText confidence="0.997010625">
on the Fin1-153 and Fin2-200 datasets. Best val-
ues for each dataset are in bold format. For
WikipediaFirst, the concept vectors are based on
the first paragraph of each article.
The first observation is that the coverage over
the two evaluation datasets is rather small: only 46
pairs have been evaluated for the Fin1-153 dataset
and 42 for the Fin2-200 dataset. This is mainly
</bodyText>
<page confidence="0.993359">
732
</page>
<bodyText confidence="0.999977733333333">
due to the natural absence of many word pairs in
the translation tables. Indeed, translation proba-
bilities can only be obtained from observed paral-
lel pairs in the training data. Concept vector based
measures are more flexible in that respect since the
relatedness value is based on a common represen-
tation in a concept vector space. It is therefore
possible to measure relatedness for a far greater
number of word pairs, as long as they share some
concept vector dimensions. The second observa-
tion is that, on the restricted subset of word pairs
considered, the results obtained by word-to-word
translation probabilities are most of the time better
than those of concept vector measures. However,
the differences are not statistically significant.6
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="method">
5 Answer Finding Experiments
</sectionHeader>
<subsectionHeader confidence="0.998191">
5.1 Retrieval based on Translation Models
</subsectionHeader>
<bodyText confidence="0.99989875">
The second experiment aims at providing an ex-
trinsic evaluation of the translation probabilities
by employing them in an answer finding task.
In order to perform retrieval, we use a rank-
ing function similar to the one proposed by Xue
et al. (2008), which builds upon previous work
on translation-based retrieval models and tries to
overcome some of their flaws:
</bodyText>
<equation confidence="0.9995678">
P(q|D) = 11 P(w|D) (2)
wEq
P(w|D) = (1 − A)Pm,,(w|D) + AP(w|C) (3)
Pm.(w|D) = (1 − Q)Pml(w|D) +
P(w|t)Pml(t|D) (4)
</equation>
<bodyText confidence="0.9999595">
where q is the query, D the document, A the
smoothing parameter for the document collection
C and P(w|t) is the probability of translating a
document term t to the query term w.
The only difference to the original model by
Xue et al. (2008) is that we use Jelinek-Mercer
smoothing for equation 3 instead of Dirichlet
Smoothing, as it has been done by Jeon et al.
(2005). In all our experiments, 0 was set to 0.8
and A to 0.5.
</bodyText>
<subsectionHeader confidence="0.996805">
5.2 The Microsoft Research QA Corpus
</subsectionHeader>
<bodyText confidence="0.9999065">
We performed an extrinsic evaluation of mono-
lingual word translation probabilities by integrat-
ing them in the retrieval model previously de-
scribed for an answer finding task. To this aim,
</bodyText>
<footnote confidence="0.421458">
6Fisher-Z transformation, two-tailed test with α=.05.
</footnote>
<bodyText confidence="0.996871564102564">
we used the questions and answers contained in
the Microsoft Research Question Answering Cor-
pus.7 This corpus comprises approximately 1.4K
questions collected from 10-13 year old school-
children, who were asked “If you could talk to an
encyclopedia, what would you ask it?”. The an-
swers to the questions have been manually identi-
fied in the full text of Encarta 98 and annotated
with the following relevance judgements: exact
answer (1), off topic (3), on topic - off target (4),
partial answer (5). In order to use this dataset for
an answer finding task, we consider the annotated
answers as the documents to be retrieved and use
the questions as the set of test queries.
This corpus is particularly well suited to con-
duct experiments targeted at the lexical gap prob-
lem: only 28% of the question-answer pairs corre-
spond to a strong match (two or more query terms
in the same answer sentence), while about a half
(52%) are a weak match (only one query term
matched in the answer sentence) and 16 % are in-
direct answers which do not explicitly contain the
answer but provide enough information for deduc-
ing it. Moreover, the Microsoft QA corpus is not
limited to a specific topic and entirely indepen-
dent from the datasets used to build our translation
models.
The original corpus contained some inconsis-
tencies due to duplicated data and non-labelled
entries. After cleaning, we obtained a corpus of
1,364 questions and 9,780 answers. Table 3 gives
one example of a question with different answers
and relevance judgements.
We report the retrieval performance in terms
of Mean Average Precision (MAP) and Mean R-
Precision (R-prec), MAP being our primary evalu-
ation metric. We consider the following relevance
categories, corresponding to increasing levels of
tolerance for inexact or partial answers:
</bodyText>
<listItem confidence="0.9959884">
• MAP1, R-Prec1: exact answer (1)
• MAP1,5, R-Prec1,5: exact answer (1) or par-
tial answer (5)
• MAP1,4,5, R-Prec1,4,5: exact answer (1) or
partial answer (5) or on topic - off target (4)
</listItem>
<bodyText confidence="0.941229">
Similarly to the training data for translation
models, the only pre-processing steps performed
</bodyText>
<footnote confidence="0.97048175">
7http://research.microsoft.
com/en-us/downloads/
88c0021c-328a-4148-a158-a42d7331c6cf/
default.aspx
</footnote>
<figure confidence="0.635239">
�
�
tED
</figure>
<page confidence="0.986161">
733
</page>
<bodyText confidence="0.9781495">
Question Why is the sun bright?
Exact answer Star, large celestial body composed of gravitationally contained hot gases
emitting electromagnetic radiation, especially light, as a result of nuclear
reactions inside the star. The sun is a star.
Partial answer Solar Energy, radiant energy produced in the sun as a result of nuclear fu-
sion reactions (see Nuclear Energy; Sun).
On topic - off target The sun has a magnitude of -26.7, inasmuch as it is about 10 billion times
as bright as Sirius in the earth’s sky.
</bodyText>
<tableCaption confidence="0.99904">
Table 3: Example relevance judgements in the Microsoft QA corpus.
</tableCaption>
<table confidence="0.999886363636364">
Model MAP1 R-Prec1 MAP1,5 R-Prec1,5 MAP1,4,5 R-Prec1,4,5
QLM 0.2679 0.1941 0.3179 0.2963 0.3215 0.3057
Lucene 0.2705 0.2002 0.3167 0.2956 0.3192 0.3030
WAQ 0.3002 0.2149* 0.3557 0.3269 0.3583 0.3375
WAQA 0.3000 0.2211 0.3640 0.3328 0.3664 0.3405
LSR 0.3046 0.2171* 0.3666 0.3327 0.3723 0.3464
WAQ+WAQAPool 0.3062 0.2259 0.3685 0.3339 0.3716 0.3454
WAQ+LSRPool 0.3117 0.2224 0.3736 0.3399 0.3766 0.3487
WAQA+LSRPool 0.3135 0.2267 0.3818 0.3444 0.3840 0.3515
WAQ+WAQA+LSRPool 0.3152 0.2286 0.3832 0.3495 0.3848 0.3569
WAQ+WAQA+LSRLin 0.3215 0.2343 0.3921 0.3536 0.3967 0.3673
</table>
<tableCaption confidence="0.983805">
Table 4: Answer retrieval results. The WAQ+WAQA+LSRLin results have been obtained with α=0.2
</tableCaption>
<bodyText confidence="0.9939277">
-y=0.2 and 5=0.6 (the parameter values have been determined empirically based on MAP and R-Prec).
The performance gaps between the translation-based models and the baseline models are statistically
significant, except for those marked with a ‘*’ (two-tailed paired t-test, p &lt; 0.05).
for this corpus were tokenisation and stop word
removal. Due to the small size of the answer
corpus, we built an open vocabulary background
collection model to deal with out of vocabulary
words by smoothing the unigram probabilities
with Good-Turing discounting, using the SRILM
toolkit8 (Stolcke, 2002).
</bodyText>
<subsectionHeader confidence="0.902509">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999847818181818">
As baselines, we consider the query-likelihood
model (QLM), corresponding to equation 4 with
Q = 0, and Lucene.9
The results reported in Table 4 show that models
incorporating monolingual translation probabili-
ties perform consistently better than both baseline
systems especially when they are used in combi-
nation. It is however difficult to provide a ranking
of the different types of training data based on the
retrieval results: it seems that LSR is slightly more
performant than WAQ and WAQA, both alone and
</bodyText>
<footnote confidence="0.999719666666667">
8http://www.speech.sri.com/projects/
srilm/
9http://lucene.apache.org
</footnote>
<bodyText confidence="0.999831565217391">
in combination, but the improvement is minor. It
is worth noticing that while the LSR training data
are comparatively smaller than WAQ and WAQA,
they however yield comparable results. The linear
combination of datasets (WAQ+WAQA+LSRLin)
yields statistically significant performance im-
provement when compared to the models without
combinations (except when compared to WAQA
for R-Prec1, p&gt;0.05), which shows that the differ-
ent datasets and resources used are complemen-
tary and each contribute to the overall result.
Three answer retrieval examples are given in
Figure 1. They provide further evidence for
the results obtained. The correct answer to the
first question “Who invented Halloween?” is
retrieved by the WAQ+WAQA+LSRLin model,
but not by the QLM. This is a case of a weak
match with only “Halloween” as matching term.
The WAQ+WAQA+LSRLin model is however able
to establish the connection between the ques-
tion term “invented” and the answer term “orig-
inated”. Questions 2 and 3 show that transla-
tion probabilities can also replace word normali-
</bodyText>
<page confidence="0.995992">
734
</page>
<figureCaption confidence="0.996207952380952">
QLM top answer WAQ+WAQA+LSRLin top answer
Question 1: Who invented Halloween?
Halloween occurs on October 31 and is observed The observances connected with Halloween are
in the U.S. and other countries with masquerad- thought to have originated among the ancient
ing, bonfires, and games. Druids, who believed that on that evening,
Saman, the lord of the dead, called forth hosts
of evil spirits.
Question 2: Can mosquito bites spread AIDS?
Another species, the Asian tiger mosquito, has Studies have shown no evidence of HIV trans-
caused health experts concern since it was first mission through insects – even in areas where
detected in the United States in 1985. Proba- there are many cases of AIDS and large popu-
bly arriving in shipments of used tire casings, lations of insects such as mosquitoes.
this fierce biter can spread a type of encephalitis,
dengue fever, and other diseases.
Question 3: How do the mountains form into a shape?
In 1985, scientists vaporized graphite to produce Geologists believe that most mountains are
a stable form of carbon molecule consisting of formed by movements in the earth’s crust.
60 carbon atoms in a roughly spherical shape,
looking like a soccer ball.
Figure 1: Top answer retrieved by QLM and WAQ+WAQA+LSRLin. Lexical overlaps between question
and answer are in bold, morphological relations are in italics.
</figureCaption>
<bodyText confidence="0.999906">
sation techniques such as stemming and lemmati-
sation, since the answers do not contain the ques-
tion terms “mosquito” (for question 2) and “form”
(for question 3), but only their inflected forms
“mosquitoes” and “formed”.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999855023255814">
We have presented three datasets for training sta-
tistical word translation models for use in answer
finding: question-answer pairs, manually-tagged
question reformulations and glosses for the same
term extracted from several lexical semantic re-
sources. It is the first time that the two latter types
of datasets have been used for this task. We have
also provided the first intrinsic evaluation of word
translation probabilities with respect to human re-
latedness rankings for reference word pairs. This
evaluation has shown that, despite the simplicity
of the method, monolingual translation models are
comparable to concept vector semantic relatedness
measures for this task. Moreover, models based on
translation probabilities yield significant improve-
ment over baseline approaches for answer finding,
especially when different types of training data are
combined. The experiments bear strong evidence
that several datasets encode different and comple-
mentary types of knowledge, which are all use-
ful for retrieval. In order to integrate semantics
in retrieval, it is therefore advisable to combine
both knowledge specific to the task at hand, e.g.
question-answer pairs, and external knowledge, as
contained in lexical semantic resources.
In the future, we would like to further evalu-
ate the models presented in this paper for different
tasks, such as question paraphrase retrieval, and
larger datasets. We also plan to improve ques-
tion analysis by automatically identifying question
topic and question focus.
Acknowledgments We thank Konstantina
Garoufi, Nada Mimouni, Christof M¨uller and
Torsten Zesch for contributions to this work.
We also thank Mark-Christoph M¨uller and the
anonymous reviewers for insightful comments.
We are grateful to Bill Dolan for making us
aware of the Microsoft Research QA Corpus.
This work has been supported by the German
Research Foundation (DFG) under the grant No.
GU 798/3-1, and by the Volkswagen Foundation
as part of the Lichtenberg-Professorship Program
under the grant No. I/82806.
</bodyText>
<sectionHeader confidence="0.995791" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.824724333333333">
Adam Berger and John Lafferty. 1999. Information
Retrieval as Statistical Translation. In Proceedings
of the 22nd Annual International Conference on Re-
</reference>
<page confidence="0.993177">
735
</page>
<reference confidence="0.998612526785714">
search and Development in Information Retrieval
(SIGIR ’99), pages 222–229.
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding.
In Proceedings of the 23rd Annual International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR ’00), pages 192–199.
Hui Fang. 2008. A Re-examination of Query Expan-
sion Using Lexical Resources. In Proceedings of
ACL-08: HLT, pages 139–147, Columbus, Ohio.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing Search in Context: the
Concept Revisited. ACM Transactions on Informa-
tion Systems (TOIS), 20(1):116–131, January.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI), pages 1606–1611.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural Language Based Reformu-
lation Resource and Wide Exploitation for Question
Answering. In Proceedings of the Eleventh Text Re-
trieval Conference (TREC 2002).
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005. Finding Similar Questions in Large Question
and Answer Archives. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge Management (CIKM ’05), pages 84–90.
Thomas K. Landauer, Darrell Laham, and Peter Foltz.
1998. Learning Human-like Knowledge by Singu-
lar Value Decomposition: A Progress Report. Ad-
vances in Neural Information Processing Systems,
10:45–51.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging Lexical Gaps be-
tween Queries and Questions on Large Online Q&amp;A
Collections with Compact Translation Models. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
410–418, Honolulu, Hawaii.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments &amp; Computers, 28(2):203–208.
Christof M¨uller, Iryna Gurevych, and Max M¨uhlh¨auser.
2007. Integrating Semantic Knowledge into Text
Similarity and Information Retrieval. In Proceed-
ings of the First IEEE International Conference on
Semantic Computing (ICSC), pages 257–264.
Vanessa Murdock and W. Bruce Croft. 2005. A Trans-
lation Model for Sentence Retrieval. In Proceedings
of the Conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP’05), pages 684–691.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19–51.
Yonggang Qiu and Hans-Peter Frei. 1993. Concept
Based Query Expansion. In Proceedings of the 16th
Annual International Conference on Research and
Development in Information Retrieval (SIGIR ’93),
pages 160–169.
Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria
Blettner. 1989. Development and Application of
a Metric on Semantic Nets. IEEE Transactions on
Systems, Man and Cybernetics, 19(1):17–30.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical Machine Translation for Query Ex-
pansion in Answer Retrieval. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL’ 07), pages
464–471.
Stefan Riezler, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Im-
proved Query Expansion. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING 2008), pages 737–744.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901–904.
Noriko Tomuro. 2003. Interrogative Reformulation
Patterns and Acquisition of Question Paraphrases.
In Proceedings of the International Workshop on
Paraphrasing, pages 33–40.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.
2008. Retrieval Models for Question and Answer
Archives. In Proceedings of the 31st Annual Inter-
national Conference on Research and Development
in Information Retrieval (SIGIR ’08), pages 475–
482.
Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser.
2007. Analyzing and Accessing Wikipedia as a Lex-
ical Semantic Resource. In Data Structures for Lin-
guistic Resources and Applications, pages 197–205.
Gunter Narr, T¨ubingen.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence (AAAI
2008), pages 861–867.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lex-
ical Query Paraphrasing for Document Retrieval.
In Proceedings of the 19th International Confer-
ence on Computational linguistics, pages 1177–
1183, Taipei, Taiwan.
</reference>
<page confidence="0.998561">
736
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.745816">
<title confidence="0.99945">Combining Lexical Semantic Resources with Question &amp; Answer Archives for Translation-Based Answer Finding</title>
<author confidence="0.998462">Bernhard Gurevych</author>
<affiliation confidence="0.917781333333333">Ubiquitous Knowledge Processing (UKP) Lab Computer Science Department Technische Universit¨at Darmstadt, Hochschulstraße 10</affiliation>
<address confidence="0.9975">D-64289 Darmstadt, Germany</address>
<web confidence="0.992205">http://www.ukp.tu-darmstadt.de/</web>
<abstract confidence="0.99935375">Monolingual translation probabilities have recently been introduced in retrieval modto solve the gap They can be obtained by training statistical translation models on parallel monolingual corpora, such as question-answer pairs, where answers act as the “source” language and questions as the “target” language. In this paper, we propose to use as a parallel training dataset the definitions and glosses provided for the same term by different lexical semantic resources. We compare monolingual translation models built from lexical semantic resources with two other kinds of datasets: manually-tagged question reformulations and question-answer pairs. We also show that the monolingual translation probabilities obtained (i) are comparable to traditional semantic relatedness measures and (ii) significantly improve the results over the query likelihood and the vector-space model for answer finding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International Conference on Research and Development in Information Retrieval (SIGIR ’99),</booktitle>
<pages>222--229</pages>
<contexts>
<context position="2022" citStr="Berger and Lafferty (1999)" startWordPosition="283" endWordPosition="286">blem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question &amp; Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) and semantic information retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). Whil</context>
<context position="5397" citStr="Berger and Lafferty (1999)" startWordPosition="795" endWordPosition="798"> query likelihood and the vector-space model. The remainder of the paper is organised as follows. Section 2 discusses related work on semantic relatedness and statistical translation models for retrieval. Section 3 presents the monolingual parallel datasets we used for obtaining monolingual translation probabilities. Semantic relatedness experiments are detailed in Section 4. Section 5 presents answer finding experiments. Finally, we conclude in Section 6. 2 Related Work 2.1 Statistical Translation Models for Retrieval Statistical translation models for retrieval have first been introduced by Berger and Lafferty (1999). These models attempt to address synonymy and polysemy problems by encoding statistical word associations trained on monolingual parallel corpora. This method offers several advantages. First, it bases upon a sound mathematical formulation of the retrieval model. Second, it is not as computationally expensive as other semantic retrieval models, since it only relies on a word translation table which can easily be computed before retrieval. The main drawback lies in the availability of suitable training data for the translation probabilities. Berger and Lafferty (1999) initially built synthetic</context>
<context position="7403" citStr="Berger and Lafferty, 1999" startWordPosition="1109" endWordPosition="1112">s based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in Question &amp; Answer archives (Jeon et al., 2005; Lee et al., 2008; Xue et al., 2008) and for sentence retrieval (Murdock and Croft, 2005). Many of the approaches previously described have used parallel data extracted from the retrieval corpus itself. The translation models obtained are therefore domain and collection-specific, which introduces a bias in the evaluation and makes it difficult to assess to what extent the translation model may be re-used for other tasks and document collections. We henceforth propose a new approach for building monolingual translation models</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Adam Berger and John Lafferty. 1999. Information Retrieval as Statistical Translation. In Proceedings of the 22nd Annual International Conference on Research and Development in Information Retrieval (SIGIR ’99), pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Rich Caruana</author>
<author>David Cohn</author>
<author>Dayne Freitag</author>
<author>Vibhu Mittal</author>
</authors>
<title>Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International Conference on Research and Development in Information Retrieval (SIGIR ’00),</booktitle>
<pages>192--199</pages>
<contexts>
<context position="6094" citStr="Berger et al. (2000)" startWordPosition="902" endWordPosition="905">statistical word associations trained on monolingual parallel corpora. This method offers several advantages. First, it bases upon a sound mathematical formulation of the retrieval model. Second, it is not as computationally expensive as other semantic retrieval models, since it only relies on a word translation table which can easily be computed before retrieval. The main drawback lies in the availability of suitable training data for the translation probabilities. Berger and Lafferty (1999) initially built synthetic training data consisting of queries automatically generated from documents. Berger et al. (2000) proposed to train translation models on question-answer pairs taken from Usenet FAQs and call-center dialogues, with answers corresponding to the “source” language and questions to the “target” language. Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been</context>
</contexts>
<marker>Berger, Caruana, Cohn, Freitag, Mittal, 2000</marker>
<rawString>Adam Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. 2000. Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding. In Proceedings of the 23rd Annual International Conference on Research and Development in Information Retrieval (SIGIR ’00), pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Fang</author>
</authors>
<title>A Re-examination of Query Expansion Using Lexical Resources.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>139--147</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1831" citStr="Fang, 2008" startWordPosition="258" endWordPosition="259">ector-space model for answer finding. 1 Introduction The lexical gap (or lexical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question &amp; Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) and semantic information retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training da</context>
</contexts>
<marker>Fang, 2008</marker>
<rawString>Hui Fang. 2008. A Re-examination of Query Expansion Using Lexical Resources. In Proceedings of ACL-08: HLT, pages 139–147, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing Search in Context: the Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="18537" citStr="Finkelstein et al. (2002)" startWordPosition="2850" endWordPosition="2854"> datasets: WAQ+WAQA+LSR. 4 Semantic Relatedness Experiments The aim of this first experiment is to perform an intrinsic evaluation of the word translation probabilities obtained by comparing them to traditional semantic relatedness measures on the task of ranking word pairs. Human judgements of semantic relatedness can be used to evaluate how well semantic relatedness measures reflect human rankings by correlating their ranking results with Spearman’s rank correlation coefficient. Several evaluation datasets are available for English, but we restrict our study to the larger dataset created by Finkelstein et al. (2002) due to the low coverage of many pairs in the word-to-word translation tables. This dataset comprises two subsets, which have been annotated by different annotators: Fin1–153, containing 153 word pairs, and Fin2–200, containing 200 word pairs. Word-to-word translation probabilities are compared with a concept vector based measure relying on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), since this approach has been shown to yield very good results (Zesch et al., 2008). The method consists in representing words as a concept vector, where concepts correspond to WordNet synsets, W</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing Search in Context: the Concept Revisited. ACM Transactions on Information Systems (TOIS), 20(1):116–131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="18941" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2909" endWordPosition="2912">nkings by correlating their ranking results with Spearman’s rank correlation coefficient. Several evaluation datasets are available for English, but we restrict our study to the larger dataset created by Finkelstein et al. (2002) due to the low coverage of many pairs in the word-to-word translation tables. This dataset comprises two subsets, which have been annotated by different annotators: Fin1–153, containing 153 word pairs, and Fin2–200, containing 200 word pairs. Word-to-word translation probabilities are compared with a concept vector based measure relying on Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), since this approach has been shown to yield very good results (Zesch et al., 2008). The method consists in representing words as a concept vector, where concepts correspond to WordNet synsets, Wikipedia article titles or Wiktionary entry names. Concept vectors for each word are derived from the textual representation available for each concept, i.e. glosses in WordNet, the full article or the first paragraph of the article in Wikipedia or the full contents of a Wiktionary entry. We refer the reader to (Gabrilovich and Markovitch, 2007; Zesch et al., 2008) for technical details on how the con</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>Natural Language Based Reformulation Resource and Wide Exploitation for Question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh Text Retrieval Conference (TREC</booktitle>
<contexts>
<context position="1892" citStr="Hermjakob et al., 2002" startWordPosition="265" endWordPosition="268">on The lexical gap (or lexical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question &amp; Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) and semantic information retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-</context>
</contexts>
<marker>Hermjakob, Echihabi, Marcu, 2002</marker>
<rawString>Ulf Hermjakob, Abdessamad Echihabi, and Daniel Marcu. 2002. Natural Language Based Reformulation Resource and Wide Exploitation for Question Answering. In Proceedings of the Eleventh Text Retrieval Conference (TREC 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding Similar Questions in Large Question and Answer Archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (CIKM ’05),</booktitle>
<pages>84--90</pages>
<contexts>
<context position="6723" citStr="Jeon et al. (2005)" startWordPosition="1003" endWordPosition="1006">o train translation models on question-answer pairs taken from Usenet FAQs and call-center dialogues, with answers corresponding to the “source” language and questions to the “target” language. Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval resul</context>
<context position="22200" citStr="Jeon et al. (2005)" startWordPosition="3448" endWordPosition="3451">y Xue et al. (2008), which builds upon previous work on translation-based retrieval models and tries to overcome some of their flaws: P(q|D) = 11 P(w|D) (2) wEq P(w|D) = (1 − A)Pm,,(w|D) + AP(w|C) (3) Pm.(w|D) = (1 − Q)Pml(w|D) + P(w|t)Pml(t|D) (4) where q is the query, D the document, A the smoothing parameter for the document collection C and P(w|t) is the probability of translating a document term t to the query term w. The only difference to the original model by Xue et al. (2008) is that we use Jelinek-Mercer smoothing for equation 3 instead of Dirichlet Smoothing, as it has been done by Jeon et al. (2005). In all our experiments, 0 was set to 0.8 and A to 0.5. 5.2 The Microsoft Research QA Corpus We performed an extrinsic evaluation of monolingual word translation probabilities by integrating them in the retrieval model previously described for an answer finding task. To this aim, 6Fisher-Z transformation, two-tailed test with α=.05. we used the questions and answers contained in the Microsoft Research Question Answering Corpus.7 This corpus comprises approximately 1.4K questions collected from 10-13 year old schoolchildren, who were asked “If you could talk to an encyclopedia, what would you </context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding Similar Questions in Large Question and Answer Archives. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (CIKM ’05), pages 84–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Peter Foltz</author>
</authors>
<title>Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>10--45</pages>
<contexts>
<context position="8674" citStr="Landauer et al., 1998" startWordPosition="1294" endWordPosition="1297">esources. Moreover, we extensively compare the results obtained by these models with models obtained from a different type of dataset, namely Question &amp; Answer archives. 2.2 Semantic Relatedness The rationale behind translation-based retrieval models is that monolingual translation probabilities encode some form of semantic knowledge. The semantic similarity and relatedness of words has traditionally been assessed through corpusbased and knowledge-based measures. Corpusbased measures include Hyperspace Analogue to 729 Language (HAL) (Lund and Burgess, 1996) and Latent Semantic Analysis (LSA) (Landauer et al., 1998). Knowledge-based measures rely on lexical semantic resources such as WordNet and comprise path length based measures (Rada et al., 1989) and concept vector based measures (Qiu and Frei, 1993). These measures have recently also been applied to new collaboratively constructed resources such as Wikipedia (Zesch et al., 2007) and Wiktionary (Zesch et al., 2008), with good results. While classical measures of semantic relatedness have been extensively studied and compared, based on comparisons with human relatedness judgements or word-choice problems, there is no comparable intrinsic study of the </context>
</contexts>
<marker>Landauer, Laham, Foltz, 1998</marker>
<rawString>Thomas K. Landauer, Darrell Laham, and Peter Foltz. 1998. Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report. Advances in Neural Information Processing Systems, 10:45–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Tae Lee</author>
<author>Sang-Bum Kim</author>
<author>Young-In Song</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Bridging Lexical Gaps between Queries and Questions on Large Online Q&amp;A Collections with Compact Translation Models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--418</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="2616" citStr="Lee et al., 2008" startWordPosition="372" endWordPosition="375">ger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection. In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models. We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translation models. This approach has been ver</context>
<context position="6438" citStr="Lee et al., 2008" startWordPosition="954" endWordPosition="957">d before retrieval. The main drawback lies in the availability of suitable training data for the translation probabilities. Berger and Lafferty (1999) initially built synthetic training data consisting of queries automatically generated from documents. Berger et al. (2000) proposed to train translation models on question-answer pairs taken from Usenet FAQs and call-center dialogues, with answers corresponding to the “source” language and questions to the “target” language. Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work </context>
</contexts>
<marker>Lee, Kim, Song, Rim, 2008</marker>
<rawString>Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and Hae-Chang Rim. 2008. Bridging Lexical Gaps between Queries and Questions on Large Online Q&amp;A Collections with Compact Translation Models. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410–418, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="8615" citStr="Lund and Burgess, 1996" startWordPosition="1285" endWordPosition="1288">tion models relying on domain-independent lexical semantic resources. Moreover, we extensively compare the results obtained by these models with models obtained from a different type of dataset, namely Question &amp; Answer archives. 2.2 Semantic Relatedness The rationale behind translation-based retrieval models is that monolingual translation probabilities encode some form of semantic knowledge. The semantic similarity and relatedness of words has traditionally been assessed through corpusbased and knowledge-based measures. Corpusbased measures include Hyperspace Analogue to 729 Language (HAL) (Lund and Burgess, 1996) and Latent Semantic Analysis (LSA) (Landauer et al., 1998). Knowledge-based measures rely on lexical semantic resources such as WordNet and comprise path length based measures (Rada et al., 1989) and concept vector based measures (Qiu and Frei, 1993). These measures have recently also been applied to new collaboratively constructed resources such as Wikipedia (Zesch et al., 2007) and Wiktionary (Zesch et al., 2008), with good results. While classical measures of semantic relatedness have been extensively studied and compared, based on comparisons with human relatedness judgements or word-choi</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments &amp; Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
</authors>
<title>Integrating Semantic Knowledge into Text Similarity and Information Retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the First IEEE International Conference on Semantic Computing (ICSC),</booktitle>
<pages>257--264</pages>
<marker>M¨uller, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>Christof M¨uller, Iryna Gurevych, and Max M¨uhlh¨auser. 2007. Integrating Semantic Knowledge into Text Similarity and Information Retrieval. In Proceedings of the First IEEE International Conference on Semantic Computing (ICSC), pages 257–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Murdock</author>
<author>W Bruce Croft</author>
</authors>
<title>A Translation Model for Sentence Retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP’05),</booktitle>
<pages>684--691</pages>
<contexts>
<context position="6827" citStr="Murdock and Croft (2005)" startWordPosition="1017" endWordPosition="1020">es, with answers corresponding to the “source” language and questions to the “target” language. Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in </context>
</contexts>
<marker>Murdock, Croft, 2005</marker>
<rawString>Vanessa Murdock and W. Bruce Croft. 2005. A Translation Model for Sentence Retrieval. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP’05), pages 684–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="15752" citStr="Och and Ney, 2003" startWordPosition="2427" endWordPosition="2430">xeme and one and the same pair of resources. It is therefore necessary to perform word sense alignment. As we do not need perfect training data, but rather large amounts of training data, we used a very simple method consisting in eliminating gloss pairs which did not at least have one lemma in common (excluding stop words and the seed lexeme itself). The final pooled parallel corpus contains 307,136 pairs and is henceforth much smaller than the previous datasets extracted from WikiAnswers. This corpus will be referred to as LSR. 3.3 Translation Model Training We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) in order to obtain word-to-word translation probabilities from the parallel datasets described above. As is common practice in translation-based retrieval, we utilised the IBM translation model 1. The only pre-processing steps performed for all parallel datasets were tokenisation and stop word removal.5 3.4 Comparison of Word-to-Word Translations Table 1 gives some examples of word-to-word translations obtained for the different parallel corpora used (the column ALLPool will be described in the next section). As evidenced by this table, 4http://code.google.com/p/giza-pp/ 5For stop word remova</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Qiu</author>
<author>Hans-Peter Frei</author>
</authors>
<title>Concept Based Query Expansion.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International Conference on Research and Development in Information Retrieval (SIGIR ’93),</booktitle>
<pages>160--169</pages>
<contexts>
<context position="8866" citStr="Qiu and Frei, 1993" startWordPosition="1323" endWordPosition="1326">The rationale behind translation-based retrieval models is that monolingual translation probabilities encode some form of semantic knowledge. The semantic similarity and relatedness of words has traditionally been assessed through corpusbased and knowledge-based measures. Corpusbased measures include Hyperspace Analogue to 729 Language (HAL) (Lund and Burgess, 1996) and Latent Semantic Analysis (LSA) (Landauer et al., 1998). Knowledge-based measures rely on lexical semantic resources such as WordNet and comprise path length based measures (Rada et al., 1989) and concept vector based measures (Qiu and Frei, 1993). These measures have recently also been applied to new collaboratively constructed resources such as Wikipedia (Zesch et al., 2007) and Wiktionary (Zesch et al., 2008), with good results. While classical measures of semantic relatedness have been extensively studied and compared, based on comparisons with human relatedness judgements or word-choice problems, there is no comparable intrinsic study of the relatedness measures obtained through word translation probabilities. In this study, we use the correlation with human rankings for reference word pairs to investigate how word translation pro</context>
</contexts>
<marker>Qiu, Frei, 1993</marker>
<rawString>Yonggang Qiu and Hans-Peter Frei. 1993. Concept Based Query Expansion. In Proceedings of the 16th Annual International Conference on Research and Development in Information Retrieval (SIGIR ’93), pages 160–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Rada</author>
<author>Hafedh Mili</author>
<author>Ellen Bicknell</author>
<author>Maria Blettner</author>
</authors>
<title>Development and Application of a Metric on Semantic Nets.</title>
<date>1989</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8811" citStr="Rada et al., 1989" startWordPosition="1314" endWordPosition="1317"> Question &amp; Answer archives. 2.2 Semantic Relatedness The rationale behind translation-based retrieval models is that monolingual translation probabilities encode some form of semantic knowledge. The semantic similarity and relatedness of words has traditionally been assessed through corpusbased and knowledge-based measures. Corpusbased measures include Hyperspace Analogue to 729 Language (HAL) (Lund and Burgess, 1996) and Latent Semantic Analysis (LSA) (Landauer et al., 1998). Knowledge-based measures rely on lexical semantic resources such as WordNet and comprise path length based measures (Rada et al., 1989) and concept vector based measures (Qiu and Frei, 1993). These measures have recently also been applied to new collaboratively constructed resources such as Wikipedia (Zesch et al., 2007) and Wiktionary (Zesch et al., 2008), with good results. While classical measures of semantic relatedness have been extensively studied and compared, based on comparisons with human relatedness judgements or word-choice problems, there is no comparable intrinsic study of the relatedness measures obtained through word translation probabilities. In this study, we use the correlation with human rankings for refer</context>
</contexts>
<marker>Rada, Mili, Bicknell, Blettner, 1989</marker>
<rawString>Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria Blettner. 1989. Development and Application of a Metric on Semantic Nets. IEEE Transactions on Systems, Man and Cybernetics, 19(1):17–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical Machine Translation for Query Expansion in Answer Retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’ 07),</booktitle>
<pages>464--471</pages>
<contexts>
<context position="1818" citStr="Riezler et al., 2007" startWordPosition="254" endWordPosition="257">y likelihood and the vector-space model for answer finding. 1 Introduction The lexical gap (or lexical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question &amp; Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) and semantic information retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice</context>
<context position="7159" citStr="Riezler et al., 2007" startWordPosition="1072" endWordPosition="1075">models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in Question &amp; Answer archives (Jeon et al., 2005; Lee et al., 2008; Xue et al., 2008) and for sentence retrieval (Murdock and Croft, 2005). Many of the approaches previously described have used parallel data extracted from the retrieval corpus itself. The translation models obtained are therefore domain and collection-specific, which</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical Machine Translation for Query Expansion in Answer Retrieval. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’ 07), pages 464–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Yi Liu</author>
<author>Alexander Vasserman</author>
</authors>
<title>Translating Queries into Snippets for Improved Query Expansion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING</booktitle>
<pages>737--744</pages>
<contexts>
<context position="7230" citStr="Riezler et al., 2008" startWordPosition="1084" endWordPosition="1088">erms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in Question &amp; Answer archives (Jeon et al., 2005; Lee et al., 2008; Xue et al., 2008) and for sentence retrieval (Murdock and Croft, 2005). Many of the approaches previously described have used parallel data extracted from the retrieval corpus itself. The translation models obtained are therefore domain and collection-specific, which introduces a bias in the evaluation and makes it difficult to assess t</context>
</contexts>
<marker>Riezler, Liu, Vasserman, 2008</marker>
<rawString>Stefan Riezler, Yi Liu, and Alexander Vasserman. 2008. Translating Queries into Snippets for Improved Query Expansion. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pages 737–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="26562" citStr="Stolcke, 2002" startWordPosition="4136" endWordPosition="4137">e been obtained with α=0.2 -y=0.2 and 5=0.6 (the parameter values have been determined empirically based on MAP and R-Prec). The performance gaps between the translation-based models and the baseline models are statistically significant, except for those marked with a ‘*’ (two-tailed paired t-test, p &lt; 0.05). for this corpus were tokenisation and stop word removal. Due to the small size of the answer corpus, we built an open vocabulary background collection model to deal with out of vocabulary words by smoothing the unigram probabilities with Good-Turing discounting, using the SRILM toolkit8 (Stolcke, 2002). 5.3 Results As baselines, we consider the query-likelihood model (QLM), corresponding to equation 4 with Q = 0, and Lucene.9 The results reported in Table 4 show that models incorporating monolingual translation probabilities perform consistently better than both baseline systems especially when they are used in combination. It is however difficult to provide a ranking of the different types of training data based on the retrieval results: it seems that LSR is slightly more performant than WAQ and WAQA, both alone and 8http://www.speech.sri.com/projects/ srilm/ 9http://lucene.apache.org in c</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
</authors>
<title>Interrogative Reformulation Patterns and Acquisition of Question Paraphrases.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Paraphrasing,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1906" citStr="Tomuro, 2003" startWordPosition="269" endWordPosition="270">exical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question &amp; Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) and semantic information retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, </context>
</contexts>
<marker>Tomuro, 2003</marker>
<rawString>Noriko Tomuro. 2003. Interrogative Reformulation Patterns and Acquisition of Question Paraphrases. In Proceedings of the International Workshop on Paraphrasing, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Retrieval Models for Question and Answer Archives.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International Conference on Research and Development in Information Retrieval (SIGIR ’08),</booktitle>
<pages>475--482</pages>
<contexts>
<context position="2597" citStr="Xue et al., 2008" startWordPosition="368" endWordPosition="371">et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection. In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models. We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translation models. This ap</context>
<context position="6481" citStr="Xue et al., 2008" startWordPosition="963" endWordPosition="966">in the availability of suitable training data for the translation probabilities. Berger and Lafferty (1999) initially built synthetic training data consisting of queries automatically generated from documents. Berger et al. (2000) proposed to train translation models on question-answer pairs taken from Usenet FAQs and call-center dialogues, with answers corresponding to the “source” language and questions to the “target” language. Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of quer</context>
<context position="11608" citStr="Xue et al. (2008)" startWordPosition="1750" endWordPosition="1753">to retrieve the same answer when a given question reformulation is asked again. We collected question-answer pairs and question reformulations from the WikiAnswers site. The resulting dataset contains 480,190 questions with answers.2 We use this dataset in order to train two different translation models: Question-Answer Pairs (WAQA) In this setting, question-answer pairs are considered as a parallel corpus. Two different forms of combinations are possible: (Q,A), where questions act as source and answers as target, and (A,Q), where answers act as source and questions as target. Recent work by Xue et al. (2008) has shown that the best results are obtained by pooling the questionanswer pairs {(q, a)1, ..., (q, a)n} and the answerquestion pairs {(a, q)1, ..., (a, q)n} for training, so that we obtain the following parallel corpus: {(q, a)1, ..., (q, a)n}∪{(a, q)1, ..., (a, q)n}. Overall, this corpus contains 1,227,362 parallel pairs and will be referred to as WAQA (WikiAnswers Question-Answers) in the rest of the paper. Question Reformulations (WAQ) In this setting, question and question reformulation pairs are considered as a parallel corpus, e.g. ‘How long do polar bears live?’ and ‘What is the polar</context>
<context position="17281" citStr="Xue et al. (2008)" startWordPosition="2656" endWordPosition="2659">heavily dependent on the training data: the WAQ and WAQA models reveal the users’ interests, while the LSR model encodes lexicographic and encyclopedic knowledge. For instance, “gem” is an acronym for “generic electronic module”, which is found in Ford vehicles. Since many question-answer pairs in WA are related to cars, this very particular use of “gem” is predominant in the WAQ and WAQA translation tables. 3.5 Combination of the Datasets In order to investigate the role played by different kinds of training data, we combined the several translation models, using the two methods described by Xue et al. (2008). The first method consists in a linear combination of the word-to-word translation probabilities after training: PLin(wilwj) = αPWAQ(wi|wj) + γPWAQA(wi|wj) + δPLSR(wi|wj) (1) where α + γ + δ = 1. This approach will be labelled with the Lin subscript. The second method consists in pooling the training datasets, i.e. concatenating the parallel corpora, before training. This approach will be labelled with the Pool subscript. Examples for word-to-word translations obtained with this type of combination can be found in the last column for each word in Table 1. The ALLPool setting corresponds to th</context>
<context position="21601" citStr="Xue et al. (2008)" startWordPosition="3341" endWordPosition="3344">ncept vector dimensions. The second observation is that, on the restricted subset of word pairs considered, the results obtained by word-to-word translation probabilities are most of the time better than those of concept vector measures. However, the differences are not statistically significant.6 5 Answer Finding Experiments 5.1 Retrieval based on Translation Models The second experiment aims at providing an extrinsic evaluation of the translation probabilities by employing them in an answer finding task. In order to perform retrieval, we use a ranking function similar to the one proposed by Xue et al. (2008), which builds upon previous work on translation-based retrieval models and tries to overcome some of their flaws: P(q|D) = 11 P(w|D) (2) wEq P(w|D) = (1 − A)Pm,,(w|D) + AP(w|C) (3) Pm.(w|D) = (1 − Q)Pml(w|D) + P(w|t)Pml(t|D) (4) where q is the query, D the document, A the smoothing parameter for the document collection C and P(w|t) is the probability of translating a document term t to the query term w. The only difference to the original model by Xue et al. (2008) is that we use Jelinek-Mercer smoothing for equation 3 instead of Dirichlet Smoothing, as it has been done by Jeon et al. (2005).</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008. Retrieval Models for Question and Answer Archives. In Proceedings of the 31st Annual International Conference on Research and Development in Information Retrieval (SIGIR ’08), pages 475– 482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
</authors>
<title>Analyzing and Accessing Wikipedia as a Lexical Semantic Resource.</title>
<date>2007</date>
<booktitle>In Data Structures for Linguistic Resources and Applications,</booktitle>
<pages>197--205</pages>
<publisher>Gunter Narr, T¨ubingen.</publisher>
<marker>Zesch, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser. 2007. Analyzing and Accessing Wikipedia as a Lexical Semantic Resource. In Data Structures for Linguistic Resources and Applications, pages 197–205. Gunter Narr, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using Wiktionary for Computing Semantic Relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI</booktitle>
<pages>861--867</pages>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Using Wiktionary for Computing Semantic Relatedness. In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI 2008), pages 861–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Bhavani Raskutti</author>
</authors>
<title>Lexical Query Paraphrasing for Document Retrieval.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics,</booktitle>
<pages>1177--1183</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1936" citStr="Zukerman and Raskutti, 2002" startWordPosition="271" endWordPosition="274">often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question &amp; Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) and semantic information retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the eva</context>
</contexts>
<marker>Zukerman, Raskutti, 2002</marker>
<rawString>Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical Query Paraphrasing for Document Retrieval. In Proceedings of the 19th International Conference on Computational linguistics, pages 1177– 1183, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>