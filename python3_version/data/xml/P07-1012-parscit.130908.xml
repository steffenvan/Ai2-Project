<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.094480">
<title confidence="0.9941235">
Vocabulary Decomposition for Estonian Open Vocabulary Speech
Recognition
</title>
<author confidence="0.990417">
Antti Puurula and Mikko Kurimo
</author>
<affiliation confidence="0.9975495">
Adaptive Informatics Research Centre
Helsinki University of Technology
</affiliation>
<address confidence="0.915761">
P.O.Box 5400, FIN-02015 HUT, Finland
</address>
<email confidence="0.995883">
{puurula, mikkok}@cis.hut.fi
</email>
<sectionHeader confidence="0.995591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999195">
Speech recognition in many morphologi-
cally rich languages suffers from a very high
out-of-vocabulary (OOV) ratio. Earlier work
has shown that vocabulary decomposition
methods can practically solve this problem
for a subset of these languages. This pa-
per compares various vocabulary decompo-
sition approaches to open vocabulary speech
recognition, using Estonian speech recogni-
tion as a benchmark. Comparisons are per-
formed utilizing large models of 60000 lex-
ical items and smaller vocabularies of 5000
items. A large vocabulary model based on
a manually constructed morphological tag-
ger is shown to give the lowest word er-
ror rate, while the unsupervised morphol-
ogy discovery method Morfessor Baseline
gives marginally weaker results. Only the
Morfessor-based approach is shown to ade-
quately scale to smaller vocabulary sizes.
</bodyText>
<sectionHeader confidence="0.998899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.972511">
1.1 OOV problem
</subsectionHeader>
<bodyText confidence="0.999133875">
Open vocabulary speech recognition refers to au-
tomatic speech recognition (ASR) of continuous
speech, or “speech-to-text” of spoken language,
where the recognizer is expected to recognize any
word spoken in that language. This capability is a re-
cent development in ASR, and is required or benefi-
cial in many of the current applications of ASR tech-
nology. Moreover, large vocabulary speech recogni-
</bodyText>
<page confidence="0.802142">
89
</page>
<bodyText confidence="0.99993425">
tion is not possible in most languages of the world
without first developing the tools needed for open
vocabulary speech recognition. This is due to a fun-
damental obstacle in current ASR called the out-of-
vocabulary (OOV) problem.
The OOV problem refers to the existence of words
encountered that a speech recognizer is unable to
recognize, as they are not covered in the vocabu-
lary. The OOV problem is caused by three inter-
twined issues. Firstly, the language model training
data and the test data always come from different
samplings of the language, and the mismatch be-
tween test and training data introduces some OOV
words, the amount depending on the difference be-
tween the data sets. Secondly, ASR systems always
use finite and preferably small sized vocabularies,
since the speed of decoding rapidly slows down as
the vocabulary size is increased. Vocabulary sizes
depend on the application domain, sizes larger than
60000 being very rare. As some of the words en-
countered in the training data are left out of the vo-
cabulary, there will be OOV words during recogni-
tion. The third and final issue is the fundamental
one; languages form novel sentences not only by
combining words, but also by combining sub-word
items called morphs to make up the words them-
selves. These morphs in turn correspond to abstract
grammatical items called morphemes, and morphs
of the same morpheme are called allomorphs of that
morpheme. The study of these facets of language
is aptly called morphology, and has been largely ne-
glected in modern ASR technology. This is due to
</bodyText>
<note confidence="0.902967">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 89–95,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.996780333333333">
ASR having been developed primarily for English,
where the OOV problem is not as severe as in other
languages of the world.
</bodyText>
<subsectionHeader confidence="0.891158">
1.2 Relevance of morphology for ASR
</subsectionHeader>
<bodyText confidence="0.999884066666667">
Morphologies in natural languages are character-
ized typologically using two parameters, called in-
dexes of synthesis and fusion. Index of synthesis
has been loosely defined as the ratio of morphs per
word forms in the language(Comrie, 1989), while
index of fusion refers to the ratio of morphs per mor-
pheme. High frequency of verb paradigms such as
“hear, hear + d, hear + d” would result in a high syn-
thesis, low fusion language, whereas high frequency
of paradigms such as “sing, sang, sung” would re-
sult in almost the opposite. Counting distinct item
types and not instances of the types, the first ex-
ample would have 2 word forms, 2 morphs and 2
morphemes, the second 3 word forms, 3 morphs and
1 morpheme. Although in the first example, there
are 3 word instances of the 2 word forms, the lat-
ter word form being an ambiguous one referring to
two distinct grammatical constructions. It should
also be noted that the first morph of the first ex-
ample has 2 pronunciations. Pronunciational bound-
aries do not always follow morphological ones, and
a morph may and will have several pronunciations
that depend on context, if the language in question
has significant orthographic irregularity.
As can be seen, both types of morphological com-
plexity increase the amount of distinct word forms,
resulting in an increase in the OOV rate of any fi-
nite sized vocabulary for that language. In prac-
tice, the OOV increase caused by synthesis is much
larger, as languages can have thousands of differ-
ent word forms per word that are caused by addi-
tion of processes of word formation followed by in-
flections. Thus the OOV problem in ASR has been
most pronounced in languages with much synthesis,
regardless of the amount of fusion. The morpheme-
based modeling approaches evaluated in this work
are primarily intended for fixing the problem caused
by synthesis, and should work less well or even ad-
versely when attempted with low synthesis, high fu-
sion languages. It should be noted that models based
on finite state transducers have been shown to be ad-
equate for describing fusion as well(Koskenniemi,
1983), and further work should evaluate these types
of models in ASR of languages with higher indexes
of fusion.
</bodyText>
<subsectionHeader confidence="0.933609">
1.3 Approaches for solving the OOV problem
</subsectionHeader>
<bodyText confidence="0.999991181818182">
The traditional method for reducing OOV would be
to simply increase the vocabulary size so that the rate
of OOV words becomes sufficiently low. Naturally
this method fails when the words are derived, com-
pounded or inflected forms of rarer words. While
this approach might still be practical in languages
with a low index of synthesis such as English, it
fails with most languages in the world. For exam-
ple, in English with language models (LM) of 60k
words trained from the Gigaword Corpus V.2(Graff
et al., 2005), and testing on a very similar Voice
of America -portion of TDT4 speech corpora(Kong
and Graff, 2005), this gives a OOV rate of 1.5%.
It should be noted that every OOV causes roughly
two errors in recognition, and vocabulary decompo-
sition approaches such as the ones evaluated here
give some benefits to word error rate (WER) even
in recognizing languages such as English(Bisani and
Ney, 2005).
Four different approaches to lexical unit selec-
tion are evaluated in this work, all of which have
been presented previously. These are hence called
“word”, “hybrid”, “morph” and “grammar”. The
word approach is the default approach to lexical
item selection, and is provided here as a baseline for
the alternative approaches. The alternatives tested
here are all based on decomposing the in-vocabulary
words, OOV words, or both, in LM training data into
sequences of sub-word fragments. During recogni-
tion the decoder can then construct the OOV words
encountered as combinations of these fragments.
Word boundaries are marked in LMs with tokens so
that the words can be reconstructed from the sub-
word fragments after decoding simply by removing
spaces between fragments, and changing the word
boundaries tokens to spaces. As splitting to sub-
word items makes the span of LM histories shorter,
higher order n-grams must be used to correct this.
Varigrams(Siivola and Pellom, 2005) are used in
this work, and to make LMs trained with each ap-
proach comparable, the varigrams have been grown
to roughly sizes of 5 million counts. It should be
noted that the names for the approaches here are
somewhat arbitrary, as from a theoretical perspec-
</bodyText>
<page confidence="0.988816">
90
</page>
<bodyText confidence="0.999928428571429">
tive both morph- and grammar-based approaches try
to model the grammatical morph set of a language,
difference being that “morph” does this with an un-
supervised data-driven machine learning algorithm,
whereas “grammar” does this using segmentations
from a manually constructed rule-based morpholog-
ical tagger.
</bodyText>
<sectionHeader confidence="0.637633" genericHeader="method">
2 Modeling approaches
</sectionHeader>
<subsectionHeader confidence="0.99867">
2.1 Word approach
</subsectionHeader>
<bodyText confidence="0.999728777777778">
The first approach evaluated in this work is the tra-
ditional word based LM, where items are simply the
most frequent words in the language model training
data. OOV words are simply treated as unknown
words in language model training. This has been
the default approach to selection of lexical items in
speech recognition for several decades, and as it has
been sufficient in English ASR, there has been lim-
ited interest in any alternatives.
</bodyText>
<subsectionHeader confidence="0.999642">
2.2 Hybrid approach
</subsectionHeader>
<bodyText confidence="0.999971133333333">
The second approach is a recent refinement of the
traditional word-based approach. This is similar to
what was introduced as “flat hybrid model”(Bisani
and Ney, 2005), and it tries to model OOV-words
as sequences of words and fragments. “Hybrid”
refers to the LM histories being composed of hy-
brids of words and fragments, while “flat” refers to
the model being composed of one n-gram model in-
stead of several models for the different item types.
The models tested in this work differ in that since
Estonian has a very regular phonemic orthography,
grapheme sequences can be directly used instead
of more complex pronunciation modeling. Subse-
quently the fragments used are just one grapheme in
length.
</bodyText>
<subsectionHeader confidence="0.999719">
2.3 Morph approach
</subsectionHeader>
<bodyText confidence="0.9999766">
The morph-based approach has shown superior re-
sults to word-based models in languages of high
synthesis and low fusion, including Estonian. This
approach, called “Morfessor Baseline” is described
in detail in (Creutz et al., 2007). An unsupervised
machine learning algorithm is used to discover the
morph set of the language in question, using mini-
mum description length (MDL) as an optimization
criterion. The algorithm is given a word list of the
language, usually pruned to about 100 000 words,
that it proceeds to recursively split to smaller items,
using gains in MDL to optimize the item set. The
resulting set of morphs models the morph set well in
languages of high synthesis, but as it does not take
fusion into account any manner, it should not work
in languages of high fusion. It neither preserves in-
formation about pronunciations, and as these do not
follow morph boundaries, the approach is unsuitable
in its basic form to languages of high orthographic
irregularity.
</bodyText>
<subsectionHeader confidence="0.998396">
2.4 Grammar approach
</subsectionHeader>
<bodyText confidence="0.999976727272727">
The final approach applies a manually constructed
rule-based morphological tagger(Alumäe, 2006).
This approach is expected to give the best results,
as the tagger should give the ideal segmentation
along the grammatical morphs that the unsupervised
and language-independent morph approach tries to
find. To make this approach more comparable to
the morph models, OOV morphs are modeled as
sequences of graphemes similar to the hybrid ap-
proach. Small changes to the original approach
were also made to make the model comparable to
the other models presented here, such as using the
tagger segmentations as such and not using pseudo-
morphemes, as well as not tagging the items in any
manner. This approach suffers from the same handi-
caps as the morph approach, as well as from some
additional ones: morphological analyzers are not
readily available for most languages, they must be
tailored by linguists for new datasets, and it is an
open problem as to how pronunciation dictionaries
should be written for grammatical morphs in lan-
guages with significant orthographic irregularity.
</bodyText>
<subsectionHeader confidence="0.997431">
2.5 Text segmentation and language modeling
</subsectionHeader>
<bodyText confidence="0.999992545454546">
For training the LMs, a subset of 43 mil-
lion words from the Estonian Segakorpus was
used(Segakorpus, 2005), preprocessed with a mor-
phological analyzer(Alumäe, 2006). After selecting
the item types, segmenting the training corpora and
generation of a pronunciation dictionary, LMs were
trained for each lexical item type. Table 1 shows
the text format for LM training data after segmen-
tation with each model. As can be seen, the word-
based approach doesn’t use word boundary tokens.
To keep the LMs comparable between each model-
</bodyText>
<page confidence="0.987747">
91
</page>
<table confidence="0.999107888888889">
model text segmentation
word 5k voodis reeglina loeme
word 60k voodis reeglina loeme
hybrid 5k v o o d i s &lt;w&gt; reeglina &lt;w&gt; l o e m e
hybrid 60k voodis &lt;w&gt; reeglina &lt;w&gt; loeme
morph 5k voodi s &lt;w&gt; re e g lina &lt;w&gt; loe me
morph 60k voodi s &lt;w&gt; reegli na &lt;w&gt; loe me
grammar 5k voodi s &lt;w&gt; reegli na &lt;w&gt; loe me
grammar 60k voodi s &lt;w&gt; reegli na &lt;w&gt; loe me
</table>
<tableCaption confidence="0.999968">
Table 1. Sample segmented texts for each model.
</tableCaption>
<bodyText confidence="0.999321363636364">
ing approach, growing varigram models(Siivola and
Pellom, 2005) were used with no limits as to the or-
der of n-grams, but limiting the number of counts to
4.8 and 5 million counts. In some models this grow-
ing method resulted in the inclusion of very frequent
long item sequences to the varigram, up to a 28-
gram. Models of both 5000 and 60000 lexical items
were trained in order to test if and how the model-
ing approaches would scale to smaller and therefore
much faster vocabularies. Distribution of counts in
n-gram orders can be seen in figure 1.
</bodyText>
<figureCaption confidence="0.660377">
Figure 1. Number of counts included for each n-
gram order in the 60k varigram models.
</figureCaption>
<bodyText confidence="0.996729857142857">
The performance of the statistical language mod-
els is often evaluated by perplexity or cross-entropy.
However, we decided to only report the real ASR
performance, because perplexity does not suit well
to the comparison of models that use different lex-
ica, have different OOV rates and have lexical units
of different lengths.
</bodyText>
<sectionHeader confidence="0.99606" genericHeader="method">
3 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.995785">
3.1 Evaluation set
</subsectionHeader>
<bodyText confidence="0.999969869565217">
Acoustic models for Estonian ASR were trained on
the Estonian Speechdat-like corpus(Meister et al.,
2002). This consists of spoken newspaper sentences
and shorter utterances, read over a telephone by
1332 different speakers. The data therefore was
quite clearly articulated, but suffered from 8kHz
sample rate, different microphones, channel noises
and occasional background noises. On top of this
the speakers were selected to give a very broad cov-
erage of different dialectal varieties of Estonian and
were of different age groups. For these reasons, in
spite of consisting of relatively common word forms
from newspaper sentences, the database can be con-
sidered challenging for ASR.
Held-out sentences were from the same corpus
used as development and evaluation set. 8 different
sentences from 50 speakers each were used for eval-
uation, while sentences from 15 speakers were used
for development. LM scaling factor was optimized
for each model separately on the development set.
On total over 200 hours of data from the database
was used for acoustic model training, of which less
than half was speech.
</bodyText>
<subsectionHeader confidence="0.997839">
3.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999972666666667">
The acoustic models were Hidden Markov Models
(HMM) with Gaussian Mixture Models (GMM)
for state modeling based on 39-dimensional
MFCC+P+D+DD features, with windowed cepstral
mean subtraction (CMS) of 1.25 second window.
Maximum likelihood linear transformation (MLLT)
was used during training. State-tied cross-word
triphones and 3 left-to-right states were used, state
durations were modeled using gamma distributions.
On total 3103 tied states and 16 Gaussians per state
were used.
Decoding was done with the decoder developed
at TKK(Pylkkönen, 2005), which is based on a one-
pass Viterbi beam search with token passing on a
lexical prefix tree. The lexical prefix tree included a
cross-word network for modeling triphone contexts,
and the nodes in the prefix tree were tied at the tri-
phone state level. Bigram look-ahead models were
</bodyText>
<page confidence="0.990805">
92
</page>
<bodyText confidence="0.999979538461538">
used in speeding up decoding, in addition to prun-
ing with global beam, history, histogram and word
end pruning. Due to the properties of the decoder
and varigram models, very high order n-grams could
be used without significant degradation in decoding
speed.
As the decoder was run with only one pass, adap-
tation was not used in this work. In preliminary
experiments simple adaptation with just constrained
maximum likelihood linear regression (CMLLR)
was shown to give as much as 20 % relative word
error rate reductions (RWERR) with this dataset.
Adaptation was not used, since it interacts with the
model types, as well as with the WER from the first
round of decoding, providing larger RWERR for the
better models. With high WER models, adaptation
matrices are less accurate, and it is also probable that
the decomposition methods yield more accurate ma-
trices, as they produce results where fewer HMM-
states are misrecognized. These issues should be in-
vestigated in future research.
After decoding, the results were post-processed
by removing words that seemed to be sequences of
junk fragments: consonant-only sequences and 1-
phoneme words. This treatment should give very
significant improvements with noisy data, but in pre-
liminary experiments it was noted that the use of
sentence boundaries resulted in almost 10% RW-
ERR weaker results for the approaches using frag-
ments, as that almost negates the gains achieved
from this post-processing. Since sentence bound-
ary forcing is done prior to junk removal, it seems
to work erroneously when it is forced to operate on
noisy data. Sentence boundaries were nevertheless
used, as in the same experiments the word-based
models gained significantly from their use, most
likely because they cannot use the fragment items
for detection of acoustic junk, as the models with
fragments can.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999198538461538">
Results of the experiments were consistent with ear-
lier findings(Hirsimäki et al., 2006; Kurimo et al.,
2006). Traditional word based LMs showed the
worst performance, with all of the recently proposed
alternatives giving better results. Hybrid LMs con-
sistently outperformed traditional word-based LMs
in both large and small vocabulary conditions. The
two morphology-driven approaches gave similar and
clearly superior results. Only the morph approach
seems to scale down well to smaller vocabulary
sizes, as the WER for the grammar approach in-
creased rapidly as size of the vocabulary was de-
creased.
</bodyText>
<table confidence="0.983354333333333">
size word hybrid morph grammar
60000 53.1 47.1 39.4 38.7
5000 82.0 63.0 43.5 47.6
</table>
<tableCaption confidence="0.999674">
Table 2. Word error rates for the models (WER %).
</tableCaption>
<bodyText confidence="0.998450411764706">
Table 2 shows the WER for the large (60000) and
small (5000) vocabulary sizes and different mod-
eling approaches. Table 3 shows the correspond-
ing letter error rates (LER). LERs are more compa-
rable across some languages than WERs, as WER
depends more on factors such as length, morpho-
logical complexity, and OOV of the words. How-
ever, for within-language and between-model com-
parisons, the RWERR should still be a valid met-
ric, and is also usable in languages that do not use a
phonemic writing system. The RWERRs of differ-
ent novel methods seems to be comparable between
different languages as well. Both WER and LER are
high considering the task. However, standard meth-
ods such as adaptation were not used, as the inten-
tion was only to study the RWERR of the different
approaches.
</bodyText>
<table confidence="0.967793333333333">
size word hybrid morph grammar
60000 17.8 15.8 12.4 12.3
5000 35.5 20.8 14.4 15.4
</table>
<tableCaption confidence="0.996698">
Table 3. Letter error rates for the models (LER %).
</tableCaption>
<sectionHeader confidence="0.994744" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999984">
Four different approaches to lexical item selection
for large and open vocabulary ASR in Estonian
were evaluated. It was shown that the three ap-
proaches utilizing vocabulary decomposition give
substantial improvements over the traditional word
based approach, and make large vocabulary ASR
technology possible for languages similar to Esto-
nian, where the traditional approach fails due to very
</bodyText>
<page confidence="0.997222">
93
</page>
<bodyText confidence="0.9997215">
high OOV rates. These include memetic relatives
Finnish and Turkish, among other languages that
have morphologies of high fusion, low synthesis and
low orthographic irregularity.
</bodyText>
<subsectionHeader confidence="0.996353">
5.1 Performance of the approaches
</subsectionHeader>
<bodyText confidence="0.999996966666667">
The morpheme-based approaches outperformed the
word- and hybrid-based approaches clearly. The re-
sults for “hybrid” are in in the range suggested by
earlier work(Bisani and Ney, 2005). One possi-
ble explanation for the discrepancy between the hy-
brid and morpheme-based approaches would be that
the morpheme-based approaches capture items that
make sense in n-gram modeling, as morphs are items
that the system of language naturally operates on.
These items would then be of more use when try-
ing to predict unseen data(Creutz et al., 2007). As
modeling pronunciations is much more straightfor-
ward in Estonian, the morpheme-based approaches
do not suffer from erroneous pronunciations, result-
ing in clearly superior performance.
As for the superiority of the “grammar” over the
unsupervised “morph”, the difference is marginal in
terms of RWERR. The grammatical tagger was tai-
lored by hand for that particular language, whereas
Morfessor method is meant to be unsupervised and
language independent. There are further arguments
that would suggest that the unsupervised approach
is one that should be followed; only “morph” scaled
well to smaller vocabulary sizes, the usual practice
of pruning the word list to produce smaller morph
sets gives better results than here and most impor-
tantly, it is questionable if “grammar” can be taken
to languages with high indexes of fusion and ortho-
graphic irregularity, as the models have to take these
into account as well.
</bodyText>
<subsectionHeader confidence="0.999768">
5.2 Comparison to previous results
</subsectionHeader>
<bodyText confidence="0.999980666666666">
There are few previous results published on Estonian
open vocabulary ASR. In (Alumäe, 2006) a WER of
44.5% was obtained with word-based trigrams and
a WER of 37.2% with items similar to ones from
“grammar” using the same speech corpus as in this
work. Compared to the present work, the WER
for the morpheme-based models was measured with
compound words split in both hypothesis and ref-
erence texts, making the task slightly easier than
here. In (Kurimo et al., 2006) a WER of 57.6% was
achieved with word-based varigrams and a WER of
49.0% with morphs-based ones. This used the same
evaluation set as this work, but had slightly different
LMs and different acoustic modelling which is the
main reason for the higher WER levels. In summary,
morpheme-based approaches seem to consistently
outperform the traditional word based one in Esto-
nian ASR, regardless of the specifics of the recogni-
tion system, test set and models.
In (Hirsimäki et al., 2006) a corresponding com-
parison of unsupervised and grammar-based morphs
was presented in Finnish, and the grammar-based
model gave a significantly higher WER in one of the
tasks. This result is interesting, and may stem from a
number of factors, among them the different decoder
and acoustic models, 4-grams versus varigrams, as
well as differences in post-processing. Most likely
the difference is due to lack of coverage for domain-
specific words in the Finnish tagger, as it has a 4.2%
OOV rate on the training data. On top of this the
OOV words are modeled simply as grapheme se-
quences, instead of modeling only OOV morphs in
that manner, as is done in this work.
</bodyText>
<subsectionHeader confidence="0.7878775">
5.3 Open problems in vocabulary
decomposition
</subsectionHeader>
<bodyText confidence="0.999991363636364">
As stated in the introduction, modeling languages
with high indexes of fusion such as Arabic will re-
quire more complex vocabulary decomposition ap-
proaches. This is verified by recent empirical re-
sults, where gains obtained from simple morpholog-
ical decomposition seem to be marginal(Kirchhoff
et al., 2006; Creutz et al., 2007). These languages
would possibly need novel LM inference algorithms
and decoder architectures. Current research seems
to be heading in this direction, with weighted finite
state transducers becoming standard representations
for the vocabulary instead of the lexical prefix tree.
Another issue in vocabulary decomposition is or-
thographic irregularity, as the items resulting from
decomposition do not necessarily have unambigu-
ous pronunciations. As most modern recognizers
use the Viterbi approximation with vocabularies of
one pronunciation per item, this is problematic. One
solution to this is expanding the different items with
tags according to pronunciation, shifting the prob-
lem to language modeling(Creutz et al., 2007). For
example, English plural “s” would expand to “s#1”
</bodyText>
<page confidence="0.997829">
94
</page>
<bodyText confidence="0.999992230769231">
with pronunciation “/s/”, and “s#2” with pronunci-
ation “/z/”, and so on. In this case the vocabulary
size increases by the amount of different pronunci-
ations added. The new items will have pronuncia-
tions that depend on their language model context,
enabling the prediction of pronunciations with lan-
guage model probabilities. The only downside to
this is complicating the search for optimal vocabu-
lary decomposition, as the items should make sense
in both pronunciational and morphological terms.
One can consider the originally presented hybrid
approach as an approach to vocabulary decompo-
sition that tries to keep the pronunciations of the
items as good as possible, whereas the morph ap-
proach tries to find items that make sense in terms
of morphology. This is obviously due to the meth-
ods having been developed on very different types
of languages. The morph approach was developed
for the needs of Finnish speech recognition, which
is a high synthesis, moderate fusion and very low or-
thographic irregularity language, whereas the hybrid
approach in (Bisani and Ney, 2005) was developed
for English, which has low synthesis, moderate fu-
sion, and very high orthographic irregularity. A uni-
versal approach to vocabulary decomposition would
have to take all of these factors into account.
</bodyText>
<sectionHeader confidence="0.994408" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999341333333333">
The authors would like to thank Dr. Tanel Alumäe
from Tallinn University of Technology for help in
performing experiments with Estonian speech and
text databases. This work was supported by the
Academy of Finland in the project: New adaptive
and learning methods in speech recognition.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991798629032258">
Bernard Comrie. 1972. Language Universals and Lin-
guistic Typology, Second Edition. Athenæum Press
Ltd, Gateshead, UK.
Kimmo Koskenniemi. 1983. Two-level Morphol-
ogy: a General Computational Model for Word-Form
Recognition and Production. University of Helsinki,
Helsinki, Finland.
Tanel Alumäe. 2006. Methods for Estonian Large Vo-
cabulary Speech Recognition. PhD Thesis. Tallinn
University of Technology. Tallinn, Estonia.
Maximilian Bisani, Hermann Ney. 2005. Open Vocab-
ulary Speech Recognition with Flat Hybrid Models.
INTERSPEECH-2005, 725–728.
Janne Pylkkönen. 2005. An Efficient One-pass Decoder
for Finnish Large Vocabulary Continuous Speech
Recognition. Proceedings of The 2nd Baltic Con-
ference on Human Language Technologies, 167–172.
HLT’2005. Tallinn, Estonia.
Vesa Siivola, Bryan L. Pellom. 2005. Growing an n-
Gram Language Model. INTERSPEECH-2005, 1309–
1312.
David Graff, Junbo Kong, Ke Chen and Kazuaki
Maeda. 2005. LDC Gigaword Corpora: En-
glish Gigaword Second Edition. In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
Junbo Kong and David Graff. 2005. TDT4 Multilin-
gual Broadcast News Speech Corpus. In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
Segakorpus. 2005. Segakorpus - Mixed Corpus of Esto-
nian. Tartu University. http://test.cl.ut.ee/korpused/.
Einar Meister, Jürgen Lasn and Lya Meister 2002. Esto-
nian SpeechDat: a project in progress. In Proceedings
of the Fonetiikan Päivät - Phonetics Symposium 2002
in Finland, 21–26.
Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, Kevin
Duh and Andreas Stolcke 2006. Morphology-
based language modeling for conversational Arabic
speech recognition. Computer Speech &amp; Language
20(4):589–608.
Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti
Puurula, Janne Pylkkönen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Saraclar and Andreas
Stolcke 2007. Analysis of Morph-Based Speech
Recognition and the Modeling of Out-of-Vocabulary
Words Across Languages To appear in Proceedings
of Human Language Technologies: The Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics. NAACL-HLT
2007, Rochester, NY, USA
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Siivola,
Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumae
and Murat Saraclar 2006. Unlimited vocabulary
speech recognition for agglutinative languages. In Hu-
man Language Technology, Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. HLT-NAACL 2006. New York,
USA
Teemu Hirsimäki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja and Janne Pylkkönen 2006.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
&amp; Language 20(4):515–541.
</reference>
<page confidence="0.998866">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969425">
<title confidence="0.999379">Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition</title>
<author confidence="0.993398">Antti Puurula</author>
<author confidence="0.993398">Mikko Kurimo</author>
<affiliation confidence="0.999559">Adaptive Informatics Research Centre Helsinki University of Technology</affiliation>
<address confidence="0.9999">P.O.Box 5400, FIN-02015 HUT, Finland</address>
<email confidence="0.995756">puurula@cis.hut.fi</email>
<email confidence="0.995756">mikkok@cis.hut.fi</email>
<abstract confidence="0.998906523809524">Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernard Comrie</author>
</authors>
<title>Language Universals and Linguistic Typology, Second Edition.</title>
<date>1972</date>
<publisher>Athenæum Press Ltd,</publisher>
<location>Gateshead, UK.</location>
<marker>Comrie, 1972</marker>
<rawString>Bernard Comrie. 1972. Language Universals and Linguistic Typology, Second Edition. Athenæum Press Ltd, Gateshead, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level Morphology: a General Computational Model for Word-Form Recognition and Production.</title>
<date>1983</date>
<institution>University of Helsinki,</institution>
<location>Helsinki, Finland.</location>
<contexts>
<context position="5503" citStr="Koskenniemi, 1983" startWordPosition="892" endWordPosition="893">nds of different word forms per word that are caused by addition of processes of word formation followed by inflections. Thus the OOV problem in ASR has been most pronounced in languages with much synthesis, regardless of the amount of fusion. The morphemebased modeling approaches evaluated in this work are primarily intended for fixing the problem caused by synthesis, and should work less well or even adversely when attempted with low synthesis, high fusion languages. It should be noted that models based on finite state transducers have been shown to be adequate for describing fusion as well(Koskenniemi, 1983), and further work should evaluate these types of models in ASR of languages with higher indexes of fusion. 1.3 Approaches for solving the OOV problem The traditional method for reducing OOV would be to simply increase the vocabulary size so that the rate of OOV words becomes sufficiently low. Naturally this method fails when the words are derived, compounded or inflected forms of rarer words. While this approach might still be practical in languages with a low index of synthesis such as English, it fails with most languages in the world. For example, in English with language models (LM) of 60</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-level Morphology: a General Computational Model for Word-Form Recognition and Production. University of Helsinki, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanel Alumäe</author>
</authors>
<title>Methods for Estonian Large Vocabulary Speech Recognition. PhD Thesis.</title>
<date>2006</date>
<institution>Tallinn University of Technology.</institution>
<location>Tallinn, Estonia.</location>
<contexts>
<context position="10431" citStr="Alumäe, 2006" startWordPosition="1694" endWordPosition="1695">0 000 words, that it proceeds to recursively split to smaller items, using gains in MDL to optimize the item set. The resulting set of morphs models the morph set well in languages of high synthesis, but as it does not take fusion into account any manner, it should not work in languages of high fusion. It neither preserves information about pronunciations, and as these do not follow morph boundaries, the approach is unsuitable in its basic form to languages of high orthographic irregularity. 2.4 Grammar approach The final approach applies a manually constructed rule-based morphological tagger(Alumäe, 2006). This approach is expected to give the best results, as the tagger should give the ideal segmentation along the grammatical morphs that the unsupervised and language-independent morph approach tries to find. To make this approach more comparable to the morph models, OOV morphs are modeled as sequences of graphemes similar to the hybrid approach. Small changes to the original approach were also made to make the model comparable to the other models presented here, such as using the tagger segmentations as such and not using pseudomorphemes, as well as not tagging the items in any manner. This a</context>
<context position="21050" citStr="Alumäe, 2006" startWordPosition="3422" endWordPosition="3423">d language independent. There are further arguments that would suggest that the unsupervised approach is one that should be followed; only “morph” scaled well to smaller vocabulary sizes, the usual practice of pruning the word list to produce smaller morph sets gives better results than here and most importantly, it is questionable if “grammar” can be taken to languages with high indexes of fusion and orthographic irregularity, as the models have to take these into account as well. 5.2 Comparison to previous results There are few previous results published on Estonian open vocabulary ASR. In (Alumäe, 2006) a WER of 44.5% was obtained with word-based trigrams and a WER of 37.2% with items similar to ones from “grammar” using the same speech corpus as in this work. Compared to the present work, the WER for the morpheme-based models was measured with compound words split in both hypothesis and reference texts, making the task slightly easier than here. In (Kurimo et al., 2006) a WER of 57.6% was achieved with word-based varigrams and a WER of 49.0% with morphs-based ones. This used the same evaluation set as this work, but had slightly different LMs and different acoustic modelling which is the ma</context>
</contexts>
<marker>Alumäe, 2006</marker>
<rawString>Tanel Alumäe. 2006. Methods for Estonian Large Vocabulary Speech Recognition. PhD Thesis. Tallinn University of Technology. Tallinn, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Open Vocabulary Speech Recognition with Flat Hybrid Models.</title>
<date>2005</date>
<tech>INTERSPEECH-2005,</tech>
<pages>725--728</pages>
<contexts>
<context position="6561" citStr="Bisani and Ney, 2005" startWordPosition="1069" endWordPosition="1072">ical in languages with a low index of synthesis such as English, it fails with most languages in the world. For example, in English with language models (LM) of 60k words trained from the Gigaword Corpus V.2(Graff et al., 2005), and testing on a very similar Voice of America -portion of TDT4 speech corpora(Kong and Graff, 2005), this gives a OOV rate of 1.5%. It should be noted that every OOV causes roughly two errors in recognition, and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate (WER) even in recognizing languages such as English(Bisani and Ney, 2005). Four different approaches to lexical unit selection are evaluated in this work, all of which have been presented previously. These are hence called “word”, “hybrid”, “morph” and “grammar”. The word approach is the default approach to lexical item selection, and is provided here as a baseline for the alternative approaches. The alternatives tested here are all based on decomposing the in-vocabulary words, OOV words, or both, in LM training data into sequences of sub-word fragments. During recognition the decoder can then construct the OOV words encountered as combinations of these fragments. </context>
<context position="8778" citStr="Bisani and Ney, 2005" startWordPosition="1425" endWordPosition="1428">rst approach evaluated in this work is the traditional word based LM, where items are simply the most frequent words in the language model training data. OOV words are simply treated as unknown words in language model training. This has been the default approach to selection of lexical items in speech recognition for several decades, and as it has been sufficient in English ASR, there has been limited interest in any alternatives. 2.2 Hybrid approach The second approach is a recent refinement of the traditional word-based approach. This is similar to what was introduced as “flat hybrid model”(Bisani and Ney, 2005), and it tries to model OOV-words as sequences of words and fragments. “Hybrid” refers to the LM histories being composed of hybrids of words and fragments, while “flat” refers to the model being composed of one n-gram model instead of several models for the different item types. The models tested in this work differ in that since Estonian has a very regular phonemic orthography, grapheme sequences can be directly used instead of more complex pronunciation modeling. Subsequently the fragments used are just one grapheme in length. 2.3 Morph approach The morph-based approach has shown superior r</context>
<context position="19648" citStr="Bisani and Ney, 2005" startWordPosition="3199" endWordPosition="3202">ry decomposition give substantial improvements over the traditional word based approach, and make large vocabulary ASR technology possible for languages similar to Estonian, where the traditional approach fails due to very 93 high OOV rates. These include memetic relatives Finnish and Turkish, among other languages that have morphologies of high fusion, low synthesis and low orthographic irregularity. 5.1 Performance of the approaches The morpheme-based approaches outperformed the word- and hybrid-based approaches clearly. The results for “hybrid” are in in the range suggested by earlier work(Bisani and Ney, 2005). One possible explanation for the discrepancy between the hybrid and morpheme-based approaches would be that the morpheme-based approaches capture items that make sense in n-gram modeling, as morphs are items that the system of language naturally operates on. These items would then be of more use when trying to predict unseen data(Creutz et al., 2007). As modeling pronunciations is much more straightforward in Estonian, the morpheme-based approaches do not suffer from erroneous pronunciations, resulting in clearly superior performance. As for the superiority of the “grammar” over the unsuperv</context>
</contexts>
<marker>Bisani, Ney, 2005</marker>
<rawString>Maximilian Bisani, Hermann Ney. 2005. Open Vocabulary Speech Recognition with Flat Hybrid Models. INTERSPEECH-2005, 725–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janne Pylkkönen</author>
</authors>
<title>An Efficient One-pass Decoder for Finnish Large Vocabulary Continuous Speech Recognition.</title>
<date>2005</date>
<booktitle>Proceedings of The 2nd Baltic Conference on Human Language Technologies, 167–172. HLT’2005.</booktitle>
<location>Tallinn, Estonia.</location>
<contexts>
<context position="15062" citStr="Pylkkönen, 2005" startWordPosition="2458" endWordPosition="2459">del training, of which less than half was speech. 3.2 Decoding The acoustic models were Hidden Markov Models (HMM) with Gaussian Mixture Models (GMM) for state modeling based on 39-dimensional MFCC+P+D+DD features, with windowed cepstral mean subtraction (CMS) of 1.25 second window. Maximum likelihood linear transformation (MLLT) was used during training. State-tied cross-word triphones and 3 left-to-right states were used, state durations were modeled using gamma distributions. On total 3103 tied states and 16 Gaussians per state were used. Decoding was done with the decoder developed at TKK(Pylkkönen, 2005), which is based on a onepass Viterbi beam search with token passing on a lexical prefix tree. The lexical prefix tree included a cross-word network for modeling triphone contexts, and the nodes in the prefix tree were tied at the triphone state level. Bigram look-ahead models were 92 used in speeding up decoding, in addition to pruning with global beam, history, histogram and word end pruning. Due to the properties of the decoder and varigram models, very high order n-grams could be used without significant degradation in decoding speed. As the decoder was run with only one pass, adaptation w</context>
</contexts>
<marker>Pylkkönen, 2005</marker>
<rawString>Janne Pylkkönen. 2005. An Efficient One-pass Decoder for Finnish Large Vocabulary Continuous Speech Recognition. Proceedings of The 2nd Baltic Conference on Human Language Technologies, 167–172. HLT’2005. Tallinn, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Bryan L Pellom</author>
</authors>
<title>Growing an nGram Language Model.</title>
<date>2005</date>
<volume>2005</volume>
<pages>1312</pages>
<contexts>
<context position="7540" citStr="Siivola and Pellom, 2005" startWordPosition="1224" endWordPosition="1227">e are all based on decomposing the in-vocabulary words, OOV words, or both, in LM training data into sequences of sub-word fragments. During recognition the decoder can then construct the OOV words encountered as combinations of these fragments. Word boundaries are marked in LMs with tokens so that the words can be reconstructed from the subword fragments after decoding simply by removing spaces between fragments, and changing the word boundaries tokens to spaces. As splitting to subword items makes the span of LM histories shorter, higher order n-grams must be used to correct this. Varigrams(Siivola and Pellom, 2005) are used in this work, and to make LMs trained with each approach comparable, the varigrams have been grown to roughly sizes of 5 million counts. It should be noted that the names for the approaches here are somewhat arbitrary, as from a theoretical perspec90 tive both morph- and grammar-based approaches try to model the grammatical morph set of a language, difference being that “morph” does this with an unsupervised data-driven machine learning algorithm, whereas “grammar” does this using segmentations from a manually constructed rule-based morphological tagger. 2 Modeling approaches 2.1 Wor</context>
<context position="12451" citStr="Siivola and Pellom, 2005" startWordPosition="2037" endWordPosition="2040"> data after segmentation with each model. As can be seen, the wordbased approach doesn’t use word boundary tokens. To keep the LMs comparable between each model91 model text segmentation word 5k voodis reeglina loeme word 60k voodis reeglina loeme hybrid 5k v o o d i s &lt;w&gt; reeglina &lt;w&gt; l o e m e hybrid 60k voodis &lt;w&gt; reeglina &lt;w&gt; loeme morph 5k voodi s &lt;w&gt; re e g lina &lt;w&gt; loe me morph 60k voodi s &lt;w&gt; reegli na &lt;w&gt; loe me grammar 5k voodi s &lt;w&gt; reegli na &lt;w&gt; loe me grammar 60k voodi s &lt;w&gt; reegli na &lt;w&gt; loe me Table 1. Sample segmented texts for each model. ing approach, growing varigram models(Siivola and Pellom, 2005) were used with no limits as to the order of n-grams, but limiting the number of counts to 4.8 and 5 million counts. In some models this growing method resulted in the inclusion of very frequent long item sequences to the varigram, up to a 28- gram. Models of both 5000 and 60000 lexical items were trained in order to test if and how the modeling approaches would scale to smaller and therefore much faster vocabularies. Distribution of counts in n-gram orders can be seen in figure 1. Figure 1. Number of counts included for each ngram order in the 60k varigram models. The performance of the stati</context>
</contexts>
<marker>Siivola, Pellom, 2005</marker>
<rawString>Vesa Siivola, Bryan L. Pellom. 2005. Growing an nGram Language Model. INTERSPEECH-2005, 1309– 1312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2005</date>
<booktitle>LDC Gigaword Corpora: English Gigaword Second Edition. In LDC link:</booktitle>
<pages>http://www.ldc.upenn.edu/Catalog/index.jsp.</pages>
<contexts>
<context position="6167" citStr="Graff et al., 2005" startWordPosition="1003" endWordPosition="1006">s of models in ASR of languages with higher indexes of fusion. 1.3 Approaches for solving the OOV problem The traditional method for reducing OOV would be to simply increase the vocabulary size so that the rate of OOV words becomes sufficiently low. Naturally this method fails when the words are derived, compounded or inflected forms of rarer words. While this approach might still be practical in languages with a low index of synthesis such as English, it fails with most languages in the world. For example, in English with language models (LM) of 60k words trained from the Gigaword Corpus V.2(Graff et al., 2005), and testing on a very similar Voice of America -portion of TDT4 speech corpora(Kong and Graff, 2005), this gives a OOV rate of 1.5%. It should be noted that every OOV causes roughly two errors in recognition, and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate (WER) even in recognizing languages such as English(Bisani and Ney, 2005). Four different approaches to lexical unit selection are evaluated in this work, all of which have been presented previously. These are hence called “word”, “hybrid”, “morph” and “grammar”. The word approa</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2005</marker>
<rawString>David Graff, Junbo Kong, Ke Chen and Kazuaki Maeda. 2005. LDC Gigaword Corpora: English Gigaword Second Edition. In LDC link: http://www.ldc.upenn.edu/Catalog/index.jsp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junbo Kong</author>
<author>David Graff</author>
</authors>
<title>TDT4 Multilingual Broadcast News Speech Corpus.</title>
<date>2005</date>
<booktitle>In LDC link:</booktitle>
<pages>http://www.ldc.upenn.edu/Catalog/index.jsp.</pages>
<contexts>
<context position="6269" citStr="Kong and Graff, 2005" startWordPosition="1020" endWordPosition="1023">oblem The traditional method for reducing OOV would be to simply increase the vocabulary size so that the rate of OOV words becomes sufficiently low. Naturally this method fails when the words are derived, compounded or inflected forms of rarer words. While this approach might still be practical in languages with a low index of synthesis such as English, it fails with most languages in the world. For example, in English with language models (LM) of 60k words trained from the Gigaword Corpus V.2(Graff et al., 2005), and testing on a very similar Voice of America -portion of TDT4 speech corpora(Kong and Graff, 2005), this gives a OOV rate of 1.5%. It should be noted that every OOV causes roughly two errors in recognition, and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate (WER) even in recognizing languages such as English(Bisani and Ney, 2005). Four different approaches to lexical unit selection are evaluated in this work, all of which have been presented previously. These are hence called “word”, “hybrid”, “morph” and “grammar”. The word approach is the default approach to lexical item selection, and is provided here as a baseline for the alter</context>
</contexts>
<marker>Kong, Graff, 2005</marker>
<rawString>Junbo Kong and David Graff. 2005. TDT4 Multilingual Broadcast News Speech Corpus. In LDC link: http://www.ldc.upenn.edu/Catalog/index.jsp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Segakorpus</author>
</authors>
<title>Segakorpus -</title>
<date>2005</date>
<institution>Mixed Corpus of Estonian. Tartu University.</institution>
<note>http://test.cl.ut.ee/korpused/.</note>
<contexts>
<context position="11566" citStr="Segakorpus, 2005" startWordPosition="1877" endWordPosition="1878"> not using pseudomorphemes, as well as not tagging the items in any manner. This approach suffers from the same handicaps as the morph approach, as well as from some additional ones: morphological analyzers are not readily available for most languages, they must be tailored by linguists for new datasets, and it is an open problem as to how pronunciation dictionaries should be written for grammatical morphs in languages with significant orthographic irregularity. 2.5 Text segmentation and language modeling For training the LMs, a subset of 43 million words from the Estonian Segakorpus was used(Segakorpus, 2005), preprocessed with a morphological analyzer(Alumäe, 2006). After selecting the item types, segmenting the training corpora and generation of a pronunciation dictionary, LMs were trained for each lexical item type. Table 1 shows the text format for LM training data after segmentation with each model. As can be seen, the wordbased approach doesn’t use word boundary tokens. To keep the LMs comparable between each model91 model text segmentation word 5k voodis reeglina loeme word 60k voodis reeglina loeme hybrid 5k v o o d i s &lt;w&gt; reeglina &lt;w&gt; l o e m e hybrid 60k voodis &lt;w&gt; reeglina &lt;w&gt; loeme mo</context>
</contexts>
<marker>Segakorpus, 2005</marker>
<rawString>Segakorpus. 2005. Segakorpus - Mixed Corpus of Estonian. Tartu University. http://test.cl.ut.ee/korpused/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Einar Meister</author>
</authors>
<title>Jürgen Lasn and Lya Meister 2002. Estonian SpeechDat: a project in progress.</title>
<booktitle>In Proceedings of the Fonetiikan Päivät - Phonetics Symposium 2002 in Finland,</booktitle>
<pages>21--26</pages>
<marker>Meister, </marker>
<rawString>Einar Meister, Jürgen Lasn and Lya Meister 2002. Estonian SpeechDat: a project in progress. In Proceedings of the Fonetiikan Päivät - Phonetics Symposium 2002 in Finland, 21–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Dimitra Vergyri</author>
<author>Jeff Bilmes</author>
<author>Kevin Duh</author>
<author>Andreas Stolcke</author>
</authors>
<title>Morphologybased language modeling for conversational Arabic speech recognition.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="22923" citStr="Kirchhoff et al., 2006" startWordPosition="3733" endWordPosition="3736">is due to lack of coverage for domainspecific words in the Finnish tagger, as it has a 4.2% OOV rate on the training data. On top of this the OOV words are modeled simply as grapheme sequences, instead of modeling only OOV morphs in that manner, as is done in this work. 5.3 Open problems in vocabulary decomposition As stated in the introduction, modeling languages with high indexes of fusion such as Arabic will require more complex vocabulary decomposition approaches. This is verified by recent empirical results, where gains obtained from simple morphological decomposition seem to be marginal(Kirchhoff et al., 2006; Creutz et al., 2007). These languages would possibly need novel LM inference algorithms and decoder architectures. Current research seems to be heading in this direction, with weighted finite state transducers becoming standard representations for the vocabulary instead of the lexical prefix tree. Another issue in vocabulary decomposition is orthographic irregularity, as the items resulting from decomposition do not necessarily have unambiguous pronunciations. As most modern recognizers use the Viterbi approximation with vocabularies of one pronunciation per item, this is problematic. One so</context>
</contexts>
<marker>Kirchhoff, Vergyri, Bilmes, Duh, Stolcke, 2006</marker>
<rawString>Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, Kevin Duh and Andreas Stolcke 2006. Morphologybased language modeling for conversational Arabic speech recognition. Computer Speech &amp; Language 20(4):589–608.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mathias Creutz</author>
<author>Teemu Hirsimäki</author>
<author>Mikko Kurimo</author>
</authors>
<title>Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar and Andreas Stolcke</title>
<date>2007</date>
<booktitle>Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics. NAACL-HLT 2007,</booktitle>
<location>Rochester, NY, USA</location>
<contexts>
<context position="9563" citStr="Creutz et al., 2007" startWordPosition="1552" endWordPosition="1555">lat” refers to the model being composed of one n-gram model instead of several models for the different item types. The models tested in this work differ in that since Estonian has a very regular phonemic orthography, grapheme sequences can be directly used instead of more complex pronunciation modeling. Subsequently the fragments used are just one grapheme in length. 2.3 Morph approach The morph-based approach has shown superior results to word-based models in languages of high synthesis and low fusion, including Estonian. This approach, called “Morfessor Baseline” is described in detail in (Creutz et al., 2007). An unsupervised machine learning algorithm is used to discover the morph set of the language in question, using minimum description length (MDL) as an optimization criterion. The algorithm is given a word list of the language, usually pruned to about 100 000 words, that it proceeds to recursively split to smaller items, using gains in MDL to optimize the item set. The resulting set of morphs models the morph set well in languages of high synthesis, but as it does not take fusion into account any manner, it should not work in languages of high fusion. It neither preserves information about pr</context>
<context position="20002" citStr="Creutz et al., 2007" startWordPosition="3257" endWordPosition="3260"> low synthesis and low orthographic irregularity. 5.1 Performance of the approaches The morpheme-based approaches outperformed the word- and hybrid-based approaches clearly. The results for “hybrid” are in in the range suggested by earlier work(Bisani and Ney, 2005). One possible explanation for the discrepancy between the hybrid and morpheme-based approaches would be that the morpheme-based approaches capture items that make sense in n-gram modeling, as morphs are items that the system of language naturally operates on. These items would then be of more use when trying to predict unseen data(Creutz et al., 2007). As modeling pronunciations is much more straightforward in Estonian, the morpheme-based approaches do not suffer from erroneous pronunciations, resulting in clearly superior performance. As for the superiority of the “grammar” over the unsupervised “morph”, the difference is marginal in terms of RWERR. The grammatical tagger was tailored by hand for that particular language, whereas Morfessor method is meant to be unsupervised and language independent. There are further arguments that would suggest that the unsupervised approach is one that should be followed; only “morph” scaled well to sma</context>
<context position="22945" citStr="Creutz et al., 2007" startWordPosition="3737" endWordPosition="3740">ge for domainspecific words in the Finnish tagger, as it has a 4.2% OOV rate on the training data. On top of this the OOV words are modeled simply as grapheme sequences, instead of modeling only OOV morphs in that manner, as is done in this work. 5.3 Open problems in vocabulary decomposition As stated in the introduction, modeling languages with high indexes of fusion such as Arabic will require more complex vocabulary decomposition approaches. This is verified by recent empirical results, where gains obtained from simple morphological decomposition seem to be marginal(Kirchhoff et al., 2006; Creutz et al., 2007). These languages would possibly need novel LM inference algorithms and decoder architectures. Current research seems to be heading in this direction, with weighted finite state transducers becoming standard representations for the vocabulary instead of the lexical prefix tree. Another issue in vocabulary decomposition is orthographic irregularity, as the items resulting from decomposition do not necessarily have unambiguous pronunciations. As most modern recognizers use the Viterbi approximation with vocabularies of one pronunciation per item, this is problematic. One solution to this is expa</context>
</contexts>
<marker>Creutz, Hirsimäki, Kurimo, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar and Andreas Stolcke 2007. Analysis of Morph-Based Speech Recognition and the Modeling of Out-of-Vocabulary Words Across Languages To appear in Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics. NAACL-HLT 2007, Rochester, NY, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
</authors>
<title>Antti Puurula, Ebru Arisoy, Vesa Siivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumae and Murat Saraclar</title>
<date>2006</date>
<booktitle>In Human Language Technology, Conference of the North American Chapter of the Association for Computational Linguistics. HLT-NAACL</booktitle>
<location>New York, USA</location>
<marker>Kurimo, 2006</marker>
<rawString>Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Siivola, Teemu Hirsimäki, Janne Pylkkönen, Tanel Alumae and Murat Saraclar 2006. Unlimited vocabulary speech recognition for agglutinative languages. In Human Language Technology, Conference of the North American Chapter of the Association for Computational Linguistics. HLT-NAACL 2006. New York, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Hirsimäki</author>
<author>Mathias Creutz</author>
</authors>
<title>Vesa Siivola, Mikko Kurimo, Sami Virpioja and Janne Pylkkönen</title>
<date>2006</date>
<journal>Computer Speech &amp; Language</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Hirsimäki, Creutz, 2006</marker>
<rawString>Teemu Hirsimäki, Mathias Creutz, Vesa Siivola, Mikko Kurimo, Sami Virpioja and Janne Pylkkönen 2006. Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech &amp; Language 20(4):515–541.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>