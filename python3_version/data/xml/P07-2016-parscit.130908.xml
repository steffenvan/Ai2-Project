<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000126">
<title confidence="0.999639">
A Joint Statistical Model for Simultaneous Word Spacing and
Spelling Error Correction for Korean
</title>
<author confidence="0.999653">
Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee*
</author>
<affiliation confidence="0.9275702">
*Department of Computer Science and Engineering
Pohang University of Science &amp; Technology (POSTECH)
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea
** Changwon National University
Department of Computer information &amp; Communication
</affiliation>
<address confidence="0.94204">
9 Sarim-dong, Changwon Gyeongnam, Korea 641-773
</address>
<email confidence="0.992733">
nohhj@postech.ac.kr jcha@changwon.ac.kr gblee@postech.ac.kr
</email>
<sectionHeader confidence="0.995538" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953666666667">
This paper presents noisy-channel based
Korean preprocessor system, which cor-
rects word spacing and typographical errors.
The proposed algorithm corrects both er-
rors simultaneously. Using Eojeol transi-
tion pattern dictionary and statistical data
such as Eumjeol n-gram and Jaso transition
probabilities, the algorithm minimizes the
usage of huge word dictionaries.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882764705882">
With increasing usages of messenger and SMS, we
need an efficient text normalizer that processes
colloquial style sentences. As in the case of general
literary sentences, correcting word spacing error
and spelling error is the very essential problem
with colloquial style sentences.
In order to correct word spacing errors, many
algorithms were used, which can be divided into
statistical algorithms and rule-based algorithms.
Statistical algorithms generally use character n-
gram (Eojeol1 or Eumjeol2 n-gram in Korean)
(Kang and Woo, 2001; Kwon, 2002) or noisy-
channel model (Gao et. al., 2003). Rule-based al-
gorithms are mostly heuristic algorithms that re-
flect linguistic knowledge (Yang et al., 2005) to
solve word spacing problem. Word spacing prob-
lem is treated especially in Japanese or Chinese,
</bodyText>
<footnote confidence="0.993456">
1 Eojeol is a Korean spacing unit which consists of one or
more Eumjeols (morphemes).
2 Eumjeol is a Korean syllable.
</footnote>
<page confidence="0.996226">
61
</page>
<bodyText confidence="0.991343111111111">
which does not use word boundary, or Korean,
which is normally segmented into Eojeols, not into
words or morphemes.
The previous algorithms for spelling error cor-
rection basically use a word dictionary. Each word
in a sentence is compared to word dictionary en-
tries, and if the word is not in the dictionary, then
the system assumes that the word has spelling er-
rors. Then corrected candidate words are suggested
by the system from the word dictionary, according
to some metric to measure the similarity between
the target word and its candidate word, such as
edit-distance (Kashyap and Oommen, 1984; Mays
et al., 1991).
But these previous algorithms have a critical li-
mitation: They all corrected word spacing errors
and spelling errors separately. Word spacing algo-
rithms define the problem as a task for determining
whether to insert the delimiter between characters
or not. Since the determination is made according
to the characters, the algorithms cannot work if the
characters have spelling errors. Likewise, algo-
rithms for solving spelling error problem cannot
work well with word spacing errors.
To cope with the limitation, there is an algo-
rithm proposed for Japanese (Nagata, 1996). Japa-
nese sentence cannot be divided into words, but
into chunks (bunsetsu in Japanese), like Eojeol in
Korean. The proposed system is for sentences rec-
ognized by OCR, and it uses character transition
probabilities and POS (part of speech) tag n-gram.
However it needs a word dictionary and takes long
time for searching many character combinations.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 61–64,
Prague, June 2007. c�2007 Association for Computational Linguistics
We propose a new algorithm which can correct
both word spacing error and spelling error simulta-
neously for Korean. This algorithm is based on
noisy-channel model, which uses Jaso3 transition
probabilities and Eojeol transition probabilities to
create spelling correction candidates. Candidates
are increased in number by inserting the blank cha-
racters on the created candidates, which cover the
spacing error correction candidates. We find the
best candidate sentence from the networks of Ja-
so/Eojeol candidates. This method decreases the
size of Eojeol transition pattern dictionary and cor-
rects the patterns which are not in the dictionary.
The remainder of this paper is as follows: Sec-
tion 2 describes why we use Jaso transition prob-
ability for Korean. Section 3 describes the pro-
posed model in detail. Section 4 provides the ex-
periment results and analyses. Finally, section 5
presents our conclusion.
</bodyText>
<sectionHeader confidence="0.9604345" genericHeader="method">
2 Spelling Error Correction with Jaso
Transition4 Probabilities
</sectionHeader>
<bodyText confidence="0.999924333333333">
We can use Eumjeol transition probabilities or Jaso
transition probabilities for spelling error correction
for Korean. We choose Jaso transition probabilities
because there are several advantages. Since an
Eumjeol is a combination of 3 Jasos, the number of
all possible Eumjeols is much larger than that of all
possible Jasos. In other words, Jaso-based
language model is smaller than Eumjeol-based
language model. Various errors in Eumjeol (even if
they do not appear as an Eumjeol pattern in a
training corpus) can be corrected by correction in
Jaso unit. Also, Jaso transition probabilities can be
extracted from relatively small corpus. This merit
is very important since we do not normally have
such a huge corpus which is very hard to collect,
since we have to pair the spelling errors with
corresponding corrections.
We obtain probabilities differently for each
case: single Jaso transition case, two Jaso’s transi-
tion case, and more than two Jasos transition case.
In single Jaso transition case, the spelling errors
are corrected by only one Jaso transition (e.g.
7JOAdq_Æ7Jb}dq_ / NÆ�). The case of correcting
by deleting Jaso is also one of the single Jaso tran-
</bodyText>
<footnote confidence="0.614019">
3 Jaso is a Korean character.
4 ‘Transition’ means the correct character is changed to other
character due to some causes, such as typographical errors.
</footnote>
<bodyText confidence="0.999748769230769">
sition case (���Æ��dq_ / �ÆX5). The Jaso
transition probabilities are calculated by counting
the transition frequencies in a training corpus.
In two Jaso’s transition case, the spelling errors
are corrected by adjacent two Jasos transition
(&amp;$Æ2L.VL / HdÆX H). In this case, we treat
two Jaso’s as one transition unit. The transition
probability calculation is the same as above.
In more than two Jaso’s transition case, the spel-
ling errors cannot be corrected only by Jaso transi-
tion (�Æ��). In this case, we treat the whole
Eojeols as one transition unit, and build an Eojeol
transition pattern dictionary for these special cases.
</bodyText>
<sectionHeader confidence="0.994425" genericHeader="method">
3 A Joint Statistical Model for Word
</sectionHeader>
<subsectionHeader confidence="0.7692355">
Spacing and Spelling Error Correction
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.875298857142857">
Given a sentence T which includes both word
spacing errors and spelling errors, we create
correction candidates C from T , and find the best
candidate that has the highest transition
C&apos;
probability from C .
C&apos;= argmaxC P(C  |T). (1)
</bodyText>
<subsectionHeader confidence="0.974905">
3.2 Model Description
</subsectionHeader>
<bodyText confidence="0.728585333333333">
A given sentence T and candidates consist of
C
Eumjeol and the blank character .
</bodyText>
<equation confidence="0.522742666666667">
si bi
T = s1b1s2b 2s3b3 ...snbn .
(2)
</equation>
<bodyText confidence="0.962737625">
(n is the number of Eumjeols)
Eumjeol consists of 3 Jasos, Choseong (on-
si
set), Jungseong (nucleus), and Jongseong (coda).
The empty Jaso is defined as ‘X’. is ‘
bi
the blank exists, and ‘Φ ’ when the blank does not
exist.
</bodyText>
<equation confidence="0.850693727272727">
si = ji 1 ji2ji3 . (3)
(
ji1: Choseong, ji 2: Jungseong, ji3: Jongseong)
Now we apply Bayes’ Rule for C&apos;:
C&apos;= argmaxC P(C  |T)
C P
(T|C)P(C) /P(T)
(  |) ( ).
C P T C P C
5 ‘X’ indicates that there is no Jaso in that position.
C = s1b1s2b 2 s3b3 ...snbn.
</equation>
<figure confidence="0.96634925">
B ’ when
arg max
arg max
(4)
</figure>
<page confidence="0.988792">
62
</page>
<bodyText confidence="0.9294195">
P(C) can be obtained using trigrams of Eum-
jeols (with the blank character) that C includes.
</bodyText>
<equation confidence="0.983832733333333">
n
P(C) = ∏ P ci ci c i
(  |1 2 ) , c = s or b . (5)
− −
i=1
And P(T  |C) can be written as multiplication
of each Jaso transition probability and the blank
character transition probability.
n
P T C
(  |) = ∏ P(si  |sii=1
n
∏ [P(ji1  |ji1)P(ji2  |ji2)P(ji3  |ji3)P(bi  |bi
i = 1
(6)
</equation>
<bodyText confidence="0.986504333333333">
We use logarithm of P(C  |T) in implementa-
tion. Figure 1 shows how the system creates the
Jaso candidates network.
</bodyText>
<figureCaption confidence="0.997177">
Figure 1: An example6 of Jaso candidate network.
</figureCaption>
<bodyText confidence="0.9419025">
In Figure 1, the topmost line is the sequence of
Jasos of the input sentence. Each Eumjeol in the
sentence is decomposed into 3 Jasos as above, and
each Jaso has its own correction candidates. For
example, Jaso ‘ㅇ’ at 4th column has its candidates
‘ㅎ’, ‘ㄴ’ and ‘X’. And two jaso’s ‘Xㅋ’ at 13th
and 14th column has its candidates ‘ㅎㄱ’,
‘ㅎㅋ’, ’ㄱㅎ’, ’ㅋㅎ’, and ‘ㄱㅇ’. The undermost
gray square is an Eojeol (which is decomposed into
Jasos) candidate ‘ㅇㅓXㄸㅓㅎㄱㅔX’ created
from ‘ㅇㅓXㅋㅔX’. Each jaso candidate has its
own transition probability, log (  |&apos; )
P jik jik 7, that is
used for calculating P(C  |T) .
In order to calculate P(C), we need Eumjeol-
based candidate network. Hence, we convert the
above Jaso candidate network into Eumjeol/Eojeol
candidate network. Figure 2 shows part of the final
</bodyText>
<footnote confidence="0.87821975">
6 The example sentence is “데체메일을어케보내는거지”.
7 In real implementation, we used “a*logP(jik|j’ik) + b” by
determining constants a and b with parameter optimization
(a = 1.0, b = 3.0).
</footnote>
<bodyText confidence="0.998849230769231">
network briefly. At this time, the blank characters
‘ B ’ and ‘ Φ ’ are inserted into each Eum-
jeol/Eojeol candidates. To find the best path from
the candidates, we conduct viterbi-search from
leftmost node corresponding to the beginning of
the sentence. When Eumjeol/Eojeol candidates are
selected, the algorithm prunes the candidates ac-
cording to the accumulated probabilities, doing
beam search. Once the best path is found, the sen-
tence corrected by both spacing and spelling errors
is extracted by backtracking the path. In Figure 2,
thick squares represent the nodes selected by the
best path.
</bodyText>
<figureCaption confidence="0.990279">
Figure 2: A final Eumjeol/Eojeol candidate network8
</figureCaption>
<sectionHeader confidence="0.934371" genericHeader="evaluation">
4 Experiments and Analyses
</sectionHeader>
<subsectionHeader confidence="0.832441">
4.1 Corpus Information
</subsectionHeader>
<table confidence="0.998326714285714">
Training Test
Sentences 60076 6006
Eojeols 302397 30376
Error Sentences (%) 15335 1512
(25.53) (25.17)
Error Eojeols (%) 31297 3111
(10.35) (10.24)
</table>
<tableCaption confidence="0.999908">
Table 1: Corpus information
</tableCaption>
<bodyText confidence="0.983642375">
Table 1 shows the information of corpus which is
used for experiments. All corpora are obtained
from Korean web chatting site log. Each corpus
has pair of sentences, sentences containing errors
and sentences with those errors corrected. Jaso
transition patterns and Eojeol transition patterns
are extracted from training corpus. Also, Eumjeol
n-grams are also obtained as a language model.
</bodyText>
<page confidence="0.4914525">
8 The final corrected sentence is “대체 메일을 어떻게
보내는 거지”.
</page>
<figure confidence="0.683642666666667">
)
.
)]
</figure>
<page confidence="0.991687">
63
</page>
<subsectionHeader confidence="0.999138">
4.2 Experiment Results and Analyses
</subsectionHeader>
<bodyText confidence="0.997930428571429">
We used two separate Eumjeol n-grams as lan-
guage models for experiments. N-gram A is ob-
tained from only training corpus and n-gram B is
obtained from all training and test corpora. All ac-
curacies are measured based on Eojeol unit.
Table 2 shows the results of word spacing error
correction only for the test corpus.
</bodyText>
<table confidence="0.9513225">
n-gram A n-gram B
Accuracy 91.03% 96.00%
</table>
<tableCaption confidence="0.999674">
Table 2: The word spacing error correction results
</tableCaption>
<bodyText confidence="0.96449725">
The results of both word spacing error and spell-
ing error correction are shown in Table 3. Error
containing test corpus (the blank characters are all
deleted) was applied to this evaluation.
</bodyText>
<table confidence="0.9948525">
System n-gram A n-gram B
Basic joint model 88.34% 93.83%
</table>
<tableCaption confidence="0.999911">
Table 3: The joint model results
</tableCaption>
<bodyText confidence="0.9949346">
Table 4 shows the results of the same experi-
ment, without deleting the blank characters in the
test corpus. The experiment shows that our joint
model has a flexibility of utilizing already existing
blanks (spacing) in the input sentence.
</bodyText>
<table confidence="0.9931855">
System n-gram A n-gram B
Baseline 89.35% 89.35%
Basic joint model with keep- 90.35% 95.25%
ing the blank characters
</table>
<tableCaption confidence="0.977783">
Table 4: The joint model results without deleting the
exist spaces
</tableCaption>
<bodyText confidence="0.999983733333333">
As shown above, the performance is dependent
of the language model (n-gram) performance. Jaso
transition probabilities can be obtained easily from
small corpus because the number of Jaso is very
small, under 100, in contrast with Eumjeol.
Using the existing blank information is also an
important factor. If test sentences have no or few
blank characters, then we simply use joint algo-
rithm to correct both errors. But when the test sen-
tences already have some blank characters, we can
use the information since some of the spacing can
be given by the user. By keeping the blank charac-
ters, we can get better accuracy because blank in-
sertion errors are generally fewer than the blank
deletion errors in the corpus.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999939375">
We proposed a joint text preprocessing model
that can correct both word spacing and spelling
errors simultaneously for Korean. To our best
knowledge, this is the first model which can handle
inter-related errors between spacing and spelling in
Korean. The usage and size of the word dictionar-
ies are decreased by using Jaso statistical prob-
abilities effectively.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9869435">
This work was supported in part by MIC &amp; IITA
through IT Leading R&amp;D Support Project.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99911065625">
Jianfeng Gao, Mu Li and Chang-Ning Huang. 2003.
Improved Source-Channel Models for Chinese Word
Segmentation. Proceedings of the 41st Annual Meet-
ing of the ACL, pp. 272-279
Seung-Shik Kang and Chong-Woo Woo. 2001. Auto-
matic Segmentation of Words Using Syllable Bigram
Statistics. Proceedings of 6th Natural Language Proc-
essing Pacific Rim Symposium, pp. 729-732
R. L Kashyap, B. J. Oommen. 1984. Spelling Correc-
tion Using Probabilistic Methods. Pattern Recogni-
tion Letters, pp. 147-154
Oh-Wook Kwon. 2002. Korean Word Segmentation and
Compound-noun Decomposition Using Markov
Chain and Syllable N-gram. The Journal of the
Acoustical Society of Korea, pp. 274-283.
Mu Li, Muhua Zhu, Yang Zhang and Ming Zhou. 2006.
Exploring Distributional Similarity Based Models for
Query Spelling Correction. Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pp. 1025-
1032
Eric Mays, Fred J. Damerau and Robert L. Mercer.
1991. Context Based Spelling Correction. IP&amp;M, pp.
517-522.
Masaaki Nagata. 1996. Context-Based Spelling Correc-
tion for Japanese OCR. Proceedings of the 16th con-
ference on Computational Linguistics, pp. 806-811
Christoper C. Yang and K. W. Li. 2005. A Heuristic
Method Based on a Statistical Approach for Chinese
Text Segmentation. Journal of the American Society
for Information Science and Technology, pp. 1438-
1447.
</reference>
<page confidence="0.999416">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.393398">
<title confidence="0.9960785">A Joint Statistical Model for Simultaneous Word Spacing and Spelling Error Correction for Korean</title>
<author confidence="0.968702">Hyungjong Noh Jeong-Won Cha Gary Geunbae Lee</author>
<affiliation confidence="0.994399">Department of Computer Science and Engineering Pohang University of Science &amp; Technology (POSTECH)</affiliation>
<address confidence="0.67216">San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea</address>
<affiliation confidence="0.996482">Changwon National University Department of Computer information &amp; Communication</affiliation>
<address confidence="0.696376">9 Sarim-dong, Changwon Gyeongnam, Korea 641-773</address>
<email confidence="0.779028">nohhj@postech.ac.krjcha@changwon.ac.krgblee@postech.ac.kr</email>
<abstract confidence="0.9994865">This paper presents noisy-channel based Korean preprocessor system, which corrects word spacing and typographical errors. The proposed algorithm corrects both errors simultaneously. Using Eojeol transition pattern dictionary and statistical data such as Eumjeol n-gram and Jaso transition probabilities, the algorithm minimizes the usage of huge word dictionaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improved Source-Channel Models for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>272--279</pages>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Jianfeng Gao, Mu Li and Chang-Ning Huang. 2003. Improved Source-Channel Models for Chinese Word Segmentation. Proceedings of the 41st Annual Meeting of the ACL, pp. 272-279</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seung-Shik Kang</author>
<author>Chong-Woo Woo</author>
</authors>
<title>Automatic Segmentation of Words Using Syllable Bigram Statistics.</title>
<date>2001</date>
<booktitle>Proceedings of 6th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>729--732</pages>
<contexts>
<context position="1419" citStr="Kang and Woo, 2001" startWordPosition="190" endWordPosition="193">es, the algorithm minimizes the usage of huge word dictionaries. 1 Introduction With increasing usages of messenger and SMS, we need an efficient text normalizer that processes colloquial style sentences. As in the case of general literary sentences, correcting word spacing error and spelling error is the very essential problem with colloquial style sentences. In order to correct word spacing errors, many algorithms were used, which can be divided into statistical algorithms and rule-based algorithms. Statistical algorithms generally use character ngram (Eojeol1 or Eumjeol2 n-gram in Korean) (Kang and Woo, 2001; Kwon, 2002) or noisychannel model (Gao et. al., 2003). Rule-based algorithms are mostly heuristic algorithms that reflect linguistic knowledge (Yang et al., 2005) to solve word spacing problem. Word spacing problem is treated especially in Japanese or Chinese, 1 Eojeol is a Korean spacing unit which consists of one or more Eumjeols (morphemes). 2 Eumjeol is a Korean syllable. 61 which does not use word boundary, or Korean, which is normally segmented into Eojeols, not into words or morphemes. The previous algorithms for spelling error correction basically use a word dictionary. Each word in </context>
</contexts>
<marker>Kang, Woo, 2001</marker>
<rawString>Seung-Shik Kang and Chong-Woo Woo. 2001. Automatic Segmentation of Words Using Syllable Bigram Statistics. Proceedings of 6th Natural Language Processing Pacific Rim Symposium, pp. 729-732</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Kashyap</author>
<author>B J Oommen</author>
</authors>
<title>Spelling Correction Using Probabilistic Methods.</title>
<date>1984</date>
<journal>Pattern Recognition Letters,</journal>
<pages>147--154</pages>
<contexts>
<context position="2402" citStr="Kashyap and Oommen, 1984" startWordPosition="354" endWordPosition="357">is a Korean syllable. 61 which does not use word boundary, or Korean, which is normally segmented into Eojeols, not into words or morphemes. The previous algorithms for spelling error correction basically use a word dictionary. Each word in a sentence is compared to word dictionary entries, and if the word is not in the dictionary, then the system assumes that the word has spelling errors. Then corrected candidate words are suggested by the system from the word dictionary, according to some metric to measure the similarity between the target word and its candidate word, such as edit-distance (Kashyap and Oommen, 1984; Mays et al., 1991). But these previous algorithms have a critical limitation: They all corrected word spacing errors and spelling errors separately. Word spacing algorithms define the problem as a task for determining whether to insert the delimiter between characters or not. Since the determination is made according to the characters, the algorithms cannot work if the characters have spelling errors. Likewise, algorithms for solving spelling error problem cannot work well with word spacing errors. To cope with the limitation, there is an algorithm proposed for Japanese (Nagata, 1996). Japan</context>
</contexts>
<marker>Kashyap, Oommen, 1984</marker>
<rawString>R. L Kashyap, B. J. Oommen. 1984. Spelling Correction Using Probabilistic Methods. Pattern Recognition Letters, pp. 147-154</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oh-Wook Kwon</author>
</authors>
<title>Korean Word Segmentation and Compound-noun Decomposition Using Markov Chain and Syllable N-gram.</title>
<date>2002</date>
<journal>The Journal of the Acoustical Society of Korea,</journal>
<pages>274--283</pages>
<contexts>
<context position="1432" citStr="Kwon, 2002" startWordPosition="194" endWordPosition="195">nimizes the usage of huge word dictionaries. 1 Introduction With increasing usages of messenger and SMS, we need an efficient text normalizer that processes colloquial style sentences. As in the case of general literary sentences, correcting word spacing error and spelling error is the very essential problem with colloquial style sentences. In order to correct word spacing errors, many algorithms were used, which can be divided into statistical algorithms and rule-based algorithms. Statistical algorithms generally use character ngram (Eojeol1 or Eumjeol2 n-gram in Korean) (Kang and Woo, 2001; Kwon, 2002) or noisychannel model (Gao et. al., 2003). Rule-based algorithms are mostly heuristic algorithms that reflect linguistic knowledge (Yang et al., 2005) to solve word spacing problem. Word spacing problem is treated especially in Japanese or Chinese, 1 Eojeol is a Korean spacing unit which consists of one or more Eumjeols (morphemes). 2 Eumjeol is a Korean syllable. 61 which does not use word boundary, or Korean, which is normally segmented into Eojeols, not into words or morphemes. The previous algorithms for spelling error correction basically use a word dictionary. Each word in a sentence is</context>
</contexts>
<marker>Kwon, 2002</marker>
<rawString>Oh-Wook Kwon. 2002. Korean Word Segmentation and Compound-noun Decomposition Using Markov Chain and Syllable N-gram. The Journal of the Acoustical Society of Korea, pp. 274-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Muhua Zhu</author>
<author>Yang Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>Exploring Distributional Similarity Based Models for Query Spelling Correction.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>1025--1032</pages>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>Mu Li, Muhua Zhu, Yang Zhang and Ming Zhou. 2006. Exploring Distributional Similarity Based Models for Query Spelling Correction. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 1025-1032</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mays</author>
<author>Fred J Damerau</author>
<author>Robert L Mercer</author>
</authors>
<title>Context Based Spelling Correction. IP&amp;M,</title>
<date>1991</date>
<pages>517--522</pages>
<contexts>
<context position="2422" citStr="Mays et al., 1991" startWordPosition="358" endWordPosition="361">hich does not use word boundary, or Korean, which is normally segmented into Eojeols, not into words or morphemes. The previous algorithms for spelling error correction basically use a word dictionary. Each word in a sentence is compared to word dictionary entries, and if the word is not in the dictionary, then the system assumes that the word has spelling errors. Then corrected candidate words are suggested by the system from the word dictionary, according to some metric to measure the similarity between the target word and its candidate word, such as edit-distance (Kashyap and Oommen, 1984; Mays et al., 1991). But these previous algorithms have a critical limitation: They all corrected word spacing errors and spelling errors separately. Word spacing algorithms define the problem as a task for determining whether to insert the delimiter between characters or not. Since the determination is made according to the characters, the algorithms cannot work if the characters have spelling errors. Likewise, algorithms for solving spelling error problem cannot work well with word spacing errors. To cope with the limitation, there is an algorithm proposed for Japanese (Nagata, 1996). Japanese sentence cannot </context>
</contexts>
<marker>Mays, Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred J. Damerau and Robert L. Mercer. 1991. Context Based Spelling Correction. IP&amp;M, pp. 517-522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Context-Based Spelling Correction for Japanese OCR.</title>
<date>1996</date>
<booktitle>Proceedings of the 16th conference on Computational Linguistics,</booktitle>
<pages>806--811</pages>
<contexts>
<context position="2995" citStr="Nagata, 1996" startWordPosition="449" endWordPosition="450">yap and Oommen, 1984; Mays et al., 1991). But these previous algorithms have a critical limitation: They all corrected word spacing errors and spelling errors separately. Word spacing algorithms define the problem as a task for determining whether to insert the delimiter between characters or not. Since the determination is made according to the characters, the algorithms cannot work if the characters have spelling errors. Likewise, algorithms for solving spelling error problem cannot work well with word spacing errors. To cope with the limitation, there is an algorithm proposed for Japanese (Nagata, 1996). Japanese sentence cannot be divided into words, but into chunks (bunsetsu in Japanese), like Eojeol in Korean. The proposed system is for sentences recognized by OCR, and it uses character transition probabilities and POS (part of speech) tag n-gram. However it needs a word dictionary and takes long time for searching many character combinations. Proceedings of the ACL 2007 Demo and Poster Sessions, pages 61–64, Prague, June 2007. c�2007 Association for Computational Linguistics We propose a new algorithm which can correct both word spacing error and spelling error simultaneously for Korean.</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996. Context-Based Spelling Correction for Japanese OCR. Proceedings of the 16th conference on Computational Linguistics, pp. 806-811</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoper C Yang</author>
<author>K W Li</author>
</authors>
<title>A Heuristic Method Based on a Statistical Approach for Chinese Text Segmentation.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>1438--1447</pages>
<marker>Yang, Li, 2005</marker>
<rawString>Christoper C. Yang and K. W. Li. 2005. A Heuristic Method Based on a Statistical Approach for Chinese Text Segmentation. Journal of the American Society for Information Science and Technology, pp. 1438-1447.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>