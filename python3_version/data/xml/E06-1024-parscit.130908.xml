<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002342">
<title confidence="0.686495">
Keeping the initiative: an empirically-motivated approach to predicting
user-initiated dialogue contributions in HCI
Kerstin Fischer and John A. Bateman
Faculty of Linguistics and Literary Sciences and SFB/TR8
University of Bremen
Bremen, Germany
</title>
<email confidence="0.717692">
{kerstinf,bateman}@uni-bremen.de
</email>
<sectionHeader confidence="0.987031" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996515">
In this paper, we address the problem
of reducing the unpredictability of user-
initiated dialogue contributions in human-
computer interaction without explicitly re-
stricting the user’s interactive possibili-
ties. We demonstrate that it is possible to
identify conditions under which particular
classes of user-initiated contributions will
occur and discuss consequences for dia-
logue system design.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950486111112">
It is increasingly recognised that human-computer
dialogue situations can benefit considerably from
mixed-initiative interaction (Allen, 1999). Interac-
tion where there is, or appears to be, little restric-
tion on just when and how the user may make a di-
alogue contribution increases the perceived natu-
ralness of an interaction, itself a valuable goal, and
also opens up the application of human-computer
interaction (HCI) to tasks where both system and
user are contributing more equally to the task be-
ing addressed.
Problematic with the acceptance of mixed-
initiative dialogue, however, is the radically in-
creased interpretation load placed on the dialogue
system. This flexibility impacts negatively on
performance at all levels of system design, from
speech recognition to intention interpretation. In
particular, clarification questions initiated by the
user are difficult to process because they may ap-
pear off-topic and can occur at any point. But pre-
venting users from posing such questions leads to
stilted interaction and a reduced sense of control
over how things are proceeding.
In this paper we pursue a partial solution to the
problem of user-initiated contributions that takes
its lead from detailed empirical studies of how
such situations are handled in human-human inter-
action. Most proposed computational treatments
of this situation up until now rely on formalised
notions of relevance: a system attempts to inter-
pret a user contribution by relating it to shared
goals of the system and user. When a connection
can be found, then even an apparently off-topic
clarification can be accomodated. In our approach,
we show how the search space for relevant connec-
tions can be constrained considerably by incorpo-
rating the generic conversation analytic principle
of recipient design (Sacks et al., 1974, p727). This
treats user utterances as explicit instructions for
how they are to be incorporated into the unfold-
ing discourse—an approach that can itself be ac-
comodated within much current discourse seman-
tic work whereby potential discourse interpreta-
tion is facilitated by drawing tighter structural and
semantic constraints from each discourse contri-
bution (Webber et al., 1999; Asher and Lascarides,
2003). We extend this here to include constraints
and conditions for the use of clarification subdia-
logues.
Our approach is empirically driven through-
out. In Section 2, we establish to what extent
the principles of recipient design uncovered for
natural human interaction can be adopted for the
still artificial situation of human-computer inter-
action. Although it is commonly assumed that re-
sults concerning human-human interaction can be
applied to human-computer interaction (Horvitz,
1999), there are also revealing differences (Amal-
berti et al., 1993). We report on a targetted com-
parison of adopted dialogic strategies in natural
human interaction (termed below HHC: human-
human communication) and human-computer in-
teraction (HCI). The study shows significant and
reliable differences in how dialogue is being man-
aged. In Section 3, we interpret these results with
respect to their implications for recipient design.
The results demonstrate not only that recipient de-
sign is relevant for HCI, but also that it leads to
specific and predictable kinds of clarification dia-
logues being taken up by users confronted with an
artificial dialogue system. Finally, in Section 4, we
discuss the implications of the results for dialogic
</bodyText>
<page confidence="0.998032">
185
</page>
<bodyText confidence="0.993506666666667">
system design in general and briefly indicate how
the required mechanisms are being incorporated in
our own dialogue system.
</bodyText>
<sectionHeader confidence="0.7504685" genericHeader="method">
2 A targetted comparison of HHC and
HCI dialogues
</sectionHeader>
<bodyText confidence="0.99993330952381">
In order to ascertain the extent to which tech-
niques of recipient design established on the ba-
sis of human-human natural interaction can be
transferred to HCI, we investigated comparable
task-oriented dialogues that varied according to
whether the users believed that that they were in-
teracting with another human or with an artificial
agent. The data for our investigation were taken
from three German corpora collected in the mid-
1990s within a toy plane building scenario used
for a range of experiments in the German Collab-
orative Research Centre Situated Artificial Com-
municators (SFB 360) at the University of Biele-
feld (Sagerer et al., 1994). In these experiments,
one participant is the ‘constructor’ who actually
builds the model plane, the other participant is the
‘instructor’, who provides instructions for the con-
structor.
The corpora differ in that the constructor in the
HHC setting was another human interlocutor; in
the other scenario, the participants were seated in
front of a computer but were informed that they
were actually talking to an automatic speech pro-
cessing system (HCI).1 In all cases, there was no
visual contact between constructor and instructor.
Previous work on human-human task-
oriented dialogues going back to, for example,
Grosz (1982), has shown that dialogue structure
commonly follows task structure. Moreover,
it is well known that human-human interaction
employs a variety of dialogue structuring mech-
anisms, ranging from meta-talk to discourse
markers, and that some of these can usefully be
employed for automatic analysis (Marcu, 2000).
If dialogue with artificial agents were then to be
structured as it is with human interlocutors, there
would be many useful linguistic surface cues
available for guiding interpretation. And, indeed,
a common way of designing dialogue structure in
HCI is to have it follow the structure of the task,
since this defines the types of actions necessary
and their sequencing.
</bodyText>
<footnote confidence="0.9974115">
1In fact, the interlocutors were always humans, as the ar-
tificial agent in the HCI conditions was simulated employing
standard Wizard-of-Oz methods allowing tighter control of
the linguistic responses received by the user.
</footnote>
<figureCaption confidence="0.991949">
Figure 1: Contrasting dialogue structures for HHC
and HCI conditions
</figureCaption>
<bodyText confidence="0.999632625">
Previous studies have not, however, addressed
the issue of dialogue structure in HCI system-
atically, although a decrease in framing signals
has been noted by Hitzenberger and Womser-
Hacker (1995)—indicating either that the dis-
course structure is marked less often or that there
is less structure to be marked. A more precise
characterisation of how task-structure is used or
expressed in HCI situations is then critical for fur-
ther design. For our analysis here, we focused
on properties of the overall dialogue structure and
how this is signalled via linguistic cues. Our re-
sults show that there are in fact significant differ-
ences in HCI and HHC and that it is not possi-
ble simply to take the human-human interaction
results and transpose results for one situation to
the other.
The structuring devices of the human-to-human
construction dialogues can be described as fol-
lows. The instructors first inform their communi-
cation partners about the general goal of the con-
struction. Subsequently, and as would be expected
for a task-oriented dialogue from previous stud-
ies, the discourse structure is hierarchical. At the
top level, there is discussion of the assembly of
the whole toy airplane, which is divided into in-
dividual functional parts, such as the wings or
the wheels. The individual constructional steps
then usually comprise a request to identify one or
more parts and a request to combine them. Each
step is generally acknowledged by the communi-
cation partner, and the successful combination of
the parts as a larger structure is signalled as well.
All the human-to-human dialogues were similar in
these respects. This discourse structure is shown
graphically in the outer box of Figure 1.
Instructors mark changes between phases with
signals of attention, often the constructor’s first
name, and discourse particles or speech routines
that mark the beginning of a new phase such as
</bodyText>
<page confidence="0.992635">
186
</page>
<table confidence="0.549671181818182">
usage goal discourse marker explicit marking
HHC HCI HHC HCI HHC HCI
none 27.3 100 0 52.5 13.6 52.5
single 40.9 0 9.1 25.0 54.5 27.5
frequent 31.8 0 90.9 22.5 31.8 20.0
Percentage of speakers making no,
single or frequent use of a particular
structuring strategy.
HCI: N=40; HHC: N=22. All differ-
ences are highly significant (ANOVA
p&lt;0.005).
</table>
<tableCaption confidence="0.994708">
Table 1: Distribution of dialogue structuring devices across experimental conditions
</tableCaption>
<bodyText confidence="0.999886">
also [so] or jetzt geht’s los [now]. This structur-
ing function of discourse markers has been shown
in several studies and so can be assumed to be
quite usual for human-human interaction (Swerts,
1998). Furthermore, individual constructional
steps are explicitly marked by means of als er-
stes, dann [first of all, then] or der erste Schritt
[the first step]. In addition to the marking of the
construction phases, we also find marking of the
different activities, such as description of the main
goal versus description of the main architecture,
or different phases that arise through the address-
ing of different addressees, such as asides to the
experimenters.
Speakers in dialogues directed at human inter-
locutors are therefore attending to the following
three aspects of discourse structure:
</bodyText>
<listItem confidence="0.94485">
• marking the beginning of the task-oriented
phase of the dialogue;
• marking the individual constructional steps;
• providing orientations for the hearer as to the
goals and subgoals of the communication.
</listItem>
<bodyText confidence="0.985716758064516">
When we turn to the HCI condition, however,
we find a very different picture—indicating that a
straightforward tuning of dialogue structure for an
artificial agent on the basis of the HHC condition
will not produce an effective system.
These dialogues generally start as the HHC di-
alogues do, i.e., with a signal for getting the com-
munication partner’s attention, but then diverge by
giving very low-level instructions, such as to find
a particular kind of component, often even before
the system has itself given any feedback. Since
this behaviour is divorced from any possible feed-
back or input produced by the artificial system, it
can only be adopted because of the speaker’s ini-
tial assumptions about the computer. When this
strategy is successful, the speaker continues to use
it in following turns. Instructors in the HCI condi-
tion do not then attempt to give a general orienta-
tion to their hearer. This is true of all the human-
computer dialogues in the corpus. Moreover, the
dialogue phases of the HCI dialogues do not cor-
respond to the assembly of an identifiable part of
the airplane, such as a wing, the wheels, or the
propeller, but to much smaller units that consist
of successfully identifying and combining some
parts. The divergent dialogue structure of the HCI
condition is shown graphically in the inner dashed
box of Figure 1.
These differences between the experimental
conditions are quantified in Table 1, which shows
for each condition the frequencies of occurrence
for the use of general orienting goal instructions,
describing what task the constructor/instructor is
about to address, the use of discourse markers,
and the use of explicit signals of changes in task
phase. These differences prove (a) that users are
engaging in recipient design with respect to their
partner in these comparable situations and (b) that
the linguistic cues available for structuring an in-
terpretation of the dialogue in the HCI case are
considerably impoverished. This can itself obvi-
ously lead to problems given the difficulty of the
interpretation task.
3 Interpretation of the observed
differences in terms of recipient design
Examining the results of the previous section more
closely, we find signs that the concept of the com-
munication partner to which participants were ori-
enting was not the same for all participants. Some
speakers believed structural marking also to be
useful in the HCI situation, for example. In this
section, we turn to a more exact consideration of
the reasons for these differences and show that di-
rectly employing the mechanisms of recipient de-
sign developed by Schegloff (1972) is a beneficial
strategy. The full range of variation observed, in-
cluding intra-corpus variation that space precluded
us describing in detail above, is seen to arise from
a single common mechanism. Furthermore, we
show that precisely the same mechanism leads to
a predictive account of user-initiated clarificatory
dialogues.
</bodyText>
<page confidence="0.989902">
187
</page>
<bodyText confidence="0.99991719">
The starting point for the discussion is the
conversation analytic notion of the insertion se-
quence. An insertion sequence is a subdialogue
inserted between the first and second parts of an
adjacency pair. They are problematic for artificial
agents precisely because they are places where the
user takes the initiative and demands information
from the system. Clarificatory subdialogues are
regularly of this kind. Schegloff (1972) analyses
the kinds of discourse contents that may constitute
insertion sequences in human-to-human conversa-
tions involving spatial reference. His results im-
ply a strong connection between recipient design
and discourse structure. This means that we can
describe the kind of local sequential organisation
problematic for mixed-initiative dialogue interpre-
tation on the basis of more general principles.
Insertion sequences have been found to address
the following kinds of dialogue work:
Location Analysis: Speakers check upon spa-
tial information regarding the communica-
tion partners, such as where they are when on
a mobile phone, which may lead to an inser-
tion sequence and is also responsible for one
of the most common types of utterances when
beginning a conversation by mobile phone:
i.e., “I’m just on the bus/train/tram”.
Membership Analysis: Speakers check upon
information about the recipient because the
communication partner’s knowledge may
render some formulations more relevant than
others. As a ‘member’ of a particular class of
people, such as the class of locals, or of the
class of those who have visited the place be-
fore, the addressee may be expected to know
some landmarks that the speaker may use for
spatial description. Membership groups may
also include differentiation according to ca-
pabilities (e.g., perceptual) of the interlocu-
tors.
Topic or Activity Analysis: Speakers attend to
which aspects of the location addressed are
relevant for the given topic and activity. They
have a number of choices at their disposal
among which they can select: geographical
descriptions, e.g. 2903 Main Street, descrip-
tions with relation to members, e.g. John’s
place, descriptions by means of landmarks,
or place names.
These three kinds of interactional activity each
give rise to potential insertion sequences; that is,
they serve as the functional motivation for par-
ticular clarificatory subdialogues being explored
rather than others. In the HCI situation, however,
one of them stands out. The task of membership
analysis is extremely challenging for a user faced
with an unknown artificial agent. There is little ba-
sis for assigning group membership; indeed, there
are not even grounds for knowing which kind of
groups would be applicable, due to lack of experi-
ence with artificial communication partners.
Since membership analysis constitutes a pre-
requisite for the formulation of instructions, recip-
ient design can be expected to be an essential force
both for the discourse structure and for the motiva-
tion of particular types of clarification questions in
HCI. We tested this prediction by means of a fur-
ther empirical study involving a scenario in which
the users’ task was to instruct a robot to measure
the distance between two objects out of a set of
seven. These objects differed only in their spatial
position. The users had an overview of the robot
and the objects to be referred to and typed their in-
structions into a notebook. The relevant objects
were pointed at by the instructor of the experi-
ments. The users were not given any information
about the system and so were explicitly faced with
a considerable problem of membership analysis,
making the need for clarification dialogues partic-
ularly obvious. The results of the study confirmed
the predicted effect and, moreover, provide a clas-
sification of clarification question types. Thus, the
particular kinds of analysis found to initiate inser-
tion sequences in HHC situations are clearly active
in HCI clarification questions as well.
21 subjects from varied professions and with
different experience with artificial systems partic-
ipated in the study. The robot’s output was gener-
ated by a simple script that displayed answers in
a fixed order after a particular ‘processing’ time.
The dialogues were all, therefore, absolutely com-
parable regarding the robot’s linguistic material;
moreover, the users’ instructions had no impact on
the robot’s linguistic behaviour. The robot, a Pio-
neer 2, did not move, but the participants were told
that it could measure distances and that they were
connected to the robot’s dialogue processing sys-
tem by means of a wireless LAN connection. The
robot’s output was either “error” (or later in the
dialogues a natural language variant) or a distance
</bodyText>
<page confidence="0.990307">
188
</page>
<table confidence="0.835380888888889">
usr11-1 hallo# [hello#]
sys ERROR
usr11-2 siehst du was [do you see anything?]
sys ERROR
usr11-3 was siehst du [what do you see?]
sys ERROR 652-a: input is invalid.
usr11-4 miB den abstand zwischen der vordersten tasse und
der linken tasse [measure the distance between
the frontmost cup and the left cup]
</table>
<figureCaption confidence="0.983614">
Figure 2: Example dialogue extract showing
membership analysis clarification questions
</figureCaption>
<bodyText confidence="0.99973888">
in centimeters. This forced users to reformulate
their dialogue contributions—an effective method-
ology for obtaining users’ hypotheses about the
functioning and capabilities of a system (Fischer,
2003). In our terms, this leads directly to an ex-
plicit exploration of a user’s membership analysis.
As expected in a joint attention scenario, very
limited location analysis occurred. Topic analysis
is also restricted; spatial formulations were chosen
on the basis of what users believed to be ‘most un-
derstandable’ for the robot, which also leads back
to the task of membership analysis.
In contrast, there were many cases of member-
ship analysis. There was clearly great uncertainty
about the robot’s prerequisites for carrying out the
spatial task and this was explicitly specified in the
users’ varied formulations. A simple example is
given in Figure 2.
The complete list of types of questions related
to membership analysis and which digress from
the task instructions in our corpus is given in Ta-
ble 2. Each of these instances of membership anal-
ysis constitutes a clarification question that would
initiate an off-topic subdialogue if the robot had
reacted to it.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="method">
4 Consequences for system design
</sectionHeader>
<bodyText confidence="0.999941">
So far our empirical studies have shown that there
are particular kinds of interactional problems that
will regularly trigger user-initiated clarification
subdialogues. These might appear off-topic or
out of place but when understood in terms of
the membership and topic/activity analysis, it be-
comes clear that all such contributions are, in a
very strong sense, ‘predictable’. These results can,
and arguably should,2 be exploited in the follow-
ing ways. One is to extend dialogue system de-
sign to be able to meet these contingently rele-
</bodyText>
<note confidence="0.473087">
2Doran et al. (2001) demonstrate a negative relationship
between number of initiative attempts and their success rate.
</note>
<bodyText confidence="0.999978960784314">
vant contributions whenever they occur. That is,
we adapt dialogue manager, lexical database etc.
so that precisely these apparently out-of-domain
topics are covered. A second strategy is to de-
termine discourse conditions that can be used to
alert the dialogue system to the likely occurrence
or absence of these kinds of clarificatory subdia-
logues (see below). Third, we can design explicit
strategies for interaction that will reduce the like-
lihood that a user will employ them: for example,
by providing information about the agent’s capa-
bilities, etc. as listed in Table 2 in advance by
means of system-initiated assertions. That is, we
can guide, or shape, to use the terminology intro-
duced by Zoltan-Ford (1991), the users’ linguistic
behaviour. A combination of these three capabil-
ities promises to improve the overall quality of a
dialogue system and forms the basis for a signifi-
cant part of our current research.
We have already ascertained empirically dis-
course conditions that support the second strat-
egy above, and these follow again directly from
the basic notions of recipient design and mem-
bership analysis. If a user already has a strong
membership analysis in place—for example, due
to preconceptions concerning the abilities (or,
more commonly, lack of abilities) of the artifi-
cial agent—then this influences the design of that
user’s utterances throughout the dialogue. As a
consequence, we have been able to define distinc-
tive linguistic profiles that lead to the identifica-
tion of distinct user groups that differ reliably in
their dialogue strategies, particularly in their ini-
tiation of subdialogues. In the human-robot dia-
logues just considered, for example, we found that
eight out of 21 users did not employ any clarifica-
tion questions at all and an additional four users
asked only a single clarification question. Provid-
ing these users with additional information about
the robot’s capabilities is of limited utility because
these users found ways to deal with the situation
without asking clarification questions. The sec-
ond group of participants consisted of nine users;
this group used many questions that would have
led into potentially problematic clarification dia-
logues if the system had been real. For these users,
the presentation of additional information on the
robot’s capabilities would be very useful.
It proved possible to distinguish the members
of these two groups reliably simply by attend-
ing to their initial dialogue contributions. This is
</bodyText>
<page confidence="0.996857">
189
</page>
<table confidence="0.991856833333333">
domain example (translation)
perception VP7-3 [do you see the cups?]
readiness VP4-25 [Are you ready for another task?]
functional capabilities VP19-11 [what can you do?]
linguistic capabilities VP18-7 [Or do you only know mugs?]
cognitive capabilities VP20-15 [do you know where is left and right of you?]
</table>
<tableCaption confidence="0.991763">
Table 2: Membership analysis related clarification questions
</tableCaption>
<table confidence="0.99832">
use of task-oriented greetings
clarification beginnings
none 58.3 11.1
single 25.0 11.1
frequent 16.7 77.8
</table>
<tableCaption confidence="0.570530285714286">
N = 21; average number of clarification questions
for task-oriented group: 1.17 clarification ques-
tions per dialogue; average number for ‘greeting’-
group 3.2; significance by t-test p&lt;0.01
Table 3: Percentage of speakers using no, a sin-
gle, or frequent clarification questions depending
on first utterance
</tableCaption>
<bodyText confidence="0.999982647887324">
where their pre-interaction membership analysis
was most clearly expressed. In the human-robot
dialogues investigated, there is no initial utterance
from the robot, the user has to initiate the inter-
action. Two principally different types of first ut-
terance were apparent: whereas one group of users
begins the interaction with task-instructions, a sec-
ond group begins the dialogue by means of a greet-
ing, an appeal for help, or a question with regard
to the capabilities of the system. These two dif-
ferent ways of approaching the system had sys-
tematic consequences for the dialogue structure.
The dependent variable investigated is the num-
ber of utterances that initiate clarification subdia-
logues. The results of the analysis show that those
who greet the robot or interact with it other than
by issuing commands initiate clarificatory subdi-
alogues significantly more often than those who
start with an instruction (cf. Table 3). Thus,
user modelling on the basis of the first utterance
in these dialogues can be used to predict much
of users’ linguistic behaviour with respect to the
initiation of clarification dialogues. Note that for
this type of user modelling no previous informa-
tion about the user is necessary and group assign-
ment can be carried out unobtrusively by means of
simple key word spotting on the first utterance.
Whereas the avoidance of clarificatory user-
initiated subdialogues is clearly a benefit, we can
also use the results of our empirical investigations
to motivate improvements in the other areas of in-
teractive work undertaken by speakers. In particu-
lar topic and activity analysis can become prob-
lematic when the decompositions adopted by a
user are either insufficient to structure dialogue ap-
propriately for interpretation or, worse, are incom-
patible with the domain models maintained by the
artificial agent. In the latter case, communication
will either fail or invoke rechecking of member-
ship categories to find a basis for understanding
(e.g., ‘do you know what cups are?’). Thus, what
can be seen on the part of a user as reducing the
complexity of a task can in fact be removing in-
formation vital for the artificial agent to effect suc-
cessful interpretation.
The results of a user’s topic and activity analy-
sis make themselves felt in the divergent dialogue
structures observed. As shown above in Figure 1,
the structure of the dialogues is thus much flatter
than the one found in the corresponding HHC dia-
logues, such that goal description and marking of
subtasks is missing, and the only structure results
from the division into selection and combination
of parts. In our second study, precisely the same
effects are observed. The task of measuring dis-
tances between objects is often decomposed into
‘simpler’ subtasks; for example, the complexity of
the task is reduced by achieving reference to each
of the objects first before the robot is requested to
measure the distance between them.
This potential mismatch between user and sys-
tem can also be identified on the basis of the inter-
action. Proceeding directly to issuing low-level in-
structions rather than providing background gen-
eral goal information is a clear linguistically
recognisable cue that a nonaligned topic/activity
analysis has been adopted. A successful dialogue
system can therefore rely on this dialogue tran-
sition as providing an indication of problems to
come, which can again be avoided in advance by
explicit system-initiated assertions of information.
</bodyText>
<page confidence="0.99356">
190
</page>
<bodyText confidence="0.999929684210526">
Our main focus in this paper has been on setting
out and motivating some generic principles for di-
alogue system design. These principles could find
diverse computational instantiations and it has not
been our aim to argue for any one instantation
rather than another. However, to conclude, we
summarise briefly the approach that we are adopt-
ing to incorporating these mechanisms within our
own dialogue system (Ross et al., 2005).
Our system augments an information-state
based approach with a distinguished vocabulary
of discourse transitions between states. We attach
‘conceptualisation-conditions’ to these transitions
which serve to post discourse goals whose partic-
ular function is to head off user-initiated clarifi-
cation. The presence of a greeting is one such
condition; the immediate transition to basic-level
instructions is another. Recognition and produc-
tion of instructions is aided by treating the seman-
tic types that occur (‘cups’, ‘measure’, ‘move’,
etc.) as elements of a domain ontology. The di-
verse topic/activity analyses then correspond to
the specification of the granularity and decom-
position of activated domain ontologies. Sim-
ilarly, location analyses correspond to common
sense geographies, which we model in terms simi-
lar to those of ontologies now being developed for
Geographic Information Systems (Fonseca et al.,
2002).
The specification of conceptualisation-
conditions triggered by discourse transitions
and classifications of the topic/activity analysis
given by the semantic types provided in user ut-
terances represents a direct transfer of the implicit
strategies found in conversation analyses to the
design of our dialogue system. For example, in
our case many simple clarifications like ‘do you
see the cups?,’ ‘how many cups do you see?’ as
well as ‘what can you do?’ are prevented by pro-
viding information in advance on what the robot
can perceive to those users that use greetings.
Similarly, during a scene description where the
system has the initiative, the opportunity is taken
to introduce terms for the objects it perceives as
well as appropriate ways of describing the scene,
e.g., by means of ‘There are two groups of cups.
What do you want me to do?’ a range of otherwise
necessary clarificatory questions is avoided. Even
in the case of failure, users will not doubt those
capabilities of the system that it has displayed it-
self, due to alignment processes also observable in
human-to-human dialogical interaction (Pickering
and Garrod, 2004). After a successful interaction,
users expect the system to be able to process
parallel instructions because they reliably expect
the system to behave consistently (Fischer and
Batliner, 2000).
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99973123255814">
In this paper, the discourse structure initiated by
users in HCI situations has been investigated and
the results have been three-fold. The structures
initiated in HCI are much flatter than in HHC; no
general orientation with respect to the aims of a
sub-task are presented to the artificial communica-
tion partner, and marking is usually reduced. This
needs to be accounted for in the mapping of the
task-structure onto the discourse model, irrespec-
tive of the kind of representation chosen. Sec-
ondly, the contents of clarification subdialogues
have also been identified as particularly depen-
dent on recipient design. That is, they concern
the preconditions for formulating utterances par-
ticularly for the respective hearer. Here, the less
that is known about the communication partner,
the more needs to be elicited in clarification dia-
logues: however, crucially, we can now state pre-
cisely which kinds of elicitations will be found
(cf. Table 2). Thirdly, users have been shown to
differ in the strategies that they take to solve the
uncertainty about the speech situation and we can
predict which strategies they in fact will follow in
their employment of clarification dialogues on the
basis of their initial interaction with the system (cf.
Table 3).
Since the likelihood for users to initiate such
clarificatory subdialogues has been found to be
predictable, we have a basis for a range of implicit
strategies for addressing the users’ subsequent lin-
guistic behaviour. Recipient design has therefore
been shown to be a powerful mechanism that, with
the appropriate methods, can be incorporated in
user-adapted dialogue management design.
Information of the kind that we have uncovered
empirically in the work reported in this paper can
be used to react appropriately to the different types
of users in two ways: either one can adapt the
system or one can try to adapt the user (Ogden
and Bernick, 1996). Although techniques for both
strategies are supported by our results, in general
we favour attempting to influence the user’s be-
haviour without restricting it a priori by means
</bodyText>
<page confidence="0.995596">
191
</page>
<bodyText confidence="0.99999565">
of computer-initiated dialogue structure. Since the
reasons for the users’ behaviour have been shown
to be located on the level of their conceptualisation
of the communication partner, explicit instruction
may in any case not be useful—explicit guidance
of users is not only often impractical but also is
not received well by users. The preferred choice is
then to influence the users’ concepts of their com-
munication partner and thus their linguistic be-
haviour by shaping (Zoltan-Ford, 1991). In par-
ticular, Schegloff’s analysis shows in detail the
human interlocutors’ preference for those location
terms that express group membership. Therefore,
in natural dialogues the speakers constantly signal
to each other who they are, what the other per-
son can expect them to know. Effective system
design should therefore provide users with pre-
cisely those kinds of information that constitute
their most frequent clarification questions initially
and in the manner that we have discussed.
</bodyText>
<sectionHeader confidence="0.981296" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999864333333333">
The authors gratefully acknowledge the support of
the Deutsche Forschungsgemeinschaft (DFG) for
the work reported in this paper.
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999770012658227">
Christine Doran, John Aberdeen, Laurie Damianos and
Lynette Hirschman. 2001. Comparing Several As-
pects of Human-Computer and Human-Huamn Di-
alogues. Proceedings of the 2nd SIGdial Workshop
on Discourse and Dialogue, Aalborg, Denmark.
James Allen. 1999. Mixed-initiative interaction. IEEE
Intelligent Systems, Sept./Oct.:14–16.
R. Amalberti, N. Carbonell, and P. Falzon. 1993.
User representations of computer systems in human-
computer speech interaction. International Journal
of Man-Machine Studies, 38:547–566.
Nicholas Asher and Alex Lascarides. 2003. Logics
of conversation. Cambridge University Press, Cam-
bridge.
Kerstin Fischer and Anton Batliner. 2000. What
makes speakers angry in human-computer conver-
sation. In Proceedings of the Third Workshop on
Human-Computer Conversation, Bellagio, Italy.
Kerstin Fischer. 2003. Linguistic methods for in-
vestigating concepts in use. In Thomas Stolz and
Katja Kolbe, editors, Methodologie in der Linguis-
tik. Frankfurt a.M.: Peter Lang.
Frederico T. Fonseca, Max J. Egenhofer, Peggy
Agouris, and Gilberto Cˆamara. 2002. Using ontolo-
gies for integrated geographic information systems.
Transactions in GIS, 6(3).
Barbara J. Grosz. 1982. Discourse analysis. In
Richard Kittredge and John Lehrberger, editors,
Sublanguage. Studies of Language in Restricted Se-
mantic Domains, pages 138–174. Berlin, New York:
De Gruyter.
L. Hitzenberger and C. Womser-Hacker. 1995.
Experimentelle Untersuchungen zu multimodalen
naturlichsprachigen Dialogen in der Mensch-
Computer-Interaktion. Sprache und Datenverar-
beitung, 19(1):51–61.
Eric Horvitz. 1999. Uncertainty, action, and interac-
tion: In pursuit of mixed-initiative computing. IEEE
Intelligent Systems, Sept./Oct.:17–20.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: a surface-based approach. Computa-
tional Linguistics, 26(3):395–448, Sep.
W.C. Ogden and P. Bernick. 1996. Using natural lan-
guage interfaces. In M. Helander, editor, Handbook
of Human-Computer Interaction. Elsevier Science
Publishers, North Holland.
Martin J. Pickering and Simon Garrod. 2004. Towards
a mechanistic psychology of dialogue. Behavioural
and Brain Sciences, 27(2):169–190.
R.J. Ross, J. Bateman, and H. Shi. 2005. Applying
Generic Dialogue Models to the Information State
Approach. In Proceedings of Symposium on Dia-
logue Modelling and Generation. Amsterdam.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organisation of turn-taking
for conversation. Language, 50:696–735.
Gerhard Sagerer, Hans-Jurgen Eikmeyer, and Gert
Rickheit. 1994. ”Wir bauen jetzt ein Flugzeug”:
Konstruieren im Dialog. Arbeitsmaterialien. Report
SFB 360, University of Bielefeld.
E. A. Schegloff. 1972. Notes on a conversational prac-
tice: formulating place. In D. Sudnow, editor, Stud-
ies in Social Interaction, pages 75–119. The Free
Press, New York.
Marc Swerts. 1998. Filled pauses as markers of dis-
course structure. Journal of Pragmatics, 30:485–
496.
Bonnie Webber, Alistair Knott, Matthew Stone, and
Aravind Joshi. 1999. Discourse relations: a struc-
tural and presuppositional account using lexicalized
TAG. In Proceedings of the 37th. Annual Meeting
of the American Association for Computational Lin-
guistics (ACL’99), pages 41–48, University of Mary-
land. American Association for Computational Lin-
guistics.
Elizabeth Zoltan-Ford. 1991. How to get people to
say and type what computers can understand. Inter-
national journal of Man-Machine Studies, 34:527–
647.
</reference>
<page confidence="0.998167">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848235">
<title confidence="0.995064">Keeping the initiative: an empirically-motivated approach to predicting user-initiated dialogue contributions in HCI</title>
<author confidence="0.999983">Kerstin Fischer</author>
<author confidence="0.999983">John A Bateman</author>
<affiliation confidence="0.9385025">of Linguistics and Literary Sciences SFB/TR8 University of Bremen</affiliation>
<address confidence="0.996867">Bremen, Germany</address>
<abstract confidence="0.998080454545455">In this paper, we address the problem of reducing the unpredictability of userinitiated dialogue contributions in humancomputer interaction without explicitly restricting the user’s interactive possibilities. We demonstrate that it is possible to identify conditions under which particular classes of user-initiated contributions will occur and discuss consequences for dialogue system design.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>John Aberdeen</author>
<author>Laurie Damianos</author>
<author>Lynette Hirschman</author>
</authors>
<title>Comparing Several Aspects of Human-Computer and Human-Huamn Dialogues.</title>
<date>2001</date>
<booktitle>Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="19701" citStr="Doran et al. (2001)" startWordPosition="3089" endWordPosition="3093">he robot had reacted to it. 4 Consequences for system design So far our empirical studies have shown that there are particular kinds of interactional problems that will regularly trigger user-initiated clarification subdialogues. These might appear off-topic or out of place but when understood in terms of the membership and topic/activity analysis, it becomes clear that all such contributions are, in a very strong sense, ‘predictable’. These results can, and arguably should,2 be exploited in the following ways. One is to extend dialogue system design to be able to meet these contingently rele2Doran et al. (2001) demonstrate a negative relationship between number of initiative attempts and their success rate. vant contributions whenever they occur. That is, we adapt dialogue manager, lexical database etc. so that precisely these apparently out-of-domain topics are covered. A second strategy is to determine discourse conditions that can be used to alert the dialogue system to the likely occurrence or absence of these kinds of clarificatory subdialogues (see below). Third, we can design explicit strategies for interaction that will reduce the likelihood that a user will employ them: for example, by prov</context>
</contexts>
<marker>Doran, Aberdeen, Damianos, Hirschman, 2001</marker>
<rawString>Christine Doran, John Aberdeen, Laurie Damianos and Lynette Hirschman. 2001. Comparing Several Aspects of Human-Computer and Human-Huamn Dialogues. Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Mixed-initiative interaction.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems, Sept./Oct.:14–16.</journal>
<contexts>
<context position="840" citStr="Allen, 1999" startWordPosition="105" endWordPosition="106">en Bremen, Germany {kerstinf,bateman}@uni-bremen.de Abstract In this paper, we address the problem of reducing the unpredictability of userinitiated dialogue contributions in humancomputer interaction without explicitly restricting the user’s interactive possibilities. We demonstrate that it is possible to identify conditions under which particular classes of user-initiated contributions will occur and discuss consequences for dialogue system design. 1 Introduction It is increasingly recognised that human-computer dialogue situations can benefit considerably from mixed-initiative interaction (Allen, 1999). Interaction where there is, or appears to be, little restriction on just when and how the user may make a dialogue contribution increases the perceived naturalness of an interaction, itself a valuable goal, and also opens up the application of human-computer interaction (HCI) to tasks where both system and user are contributing more equally to the task being addressed. Problematic with the acceptance of mixedinitiative dialogue, however, is the radically increased interpretation load placed on the dialogue system. This flexibility impacts negatively on performance at all levels of system des</context>
</contexts>
<marker>Allen, 1999</marker>
<rawString>James Allen. 1999. Mixed-initiative interaction. IEEE Intelligent Systems, Sept./Oct.:14–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Amalberti</author>
<author>N Carbonell</author>
<author>P Falzon</author>
</authors>
<title>User representations of computer systems in humancomputer speech interaction.</title>
<date>1993</date>
<journal>International Journal of Man-Machine Studies,</journal>
<pages>38--547</pages>
<contexts>
<context position="3471" citStr="Amalberti et al., 1993" startWordPosition="510" endWordPosition="514">discourse contribution (Webber et al., 1999; Asher and Lascarides, 2003). We extend this here to include constraints and conditions for the use of clarification subdialogues. Our approach is empirically driven throughout. In Section 2, we establish to what extent the principles of recipient design uncovered for natural human interaction can be adopted for the still artificial situation of human-computer interaction. Although it is commonly assumed that results concerning human-human interaction can be applied to human-computer interaction (Horvitz, 1999), there are also revealing differences (Amalberti et al., 1993). We report on a targetted comparison of adopted dialogic strategies in natural human interaction (termed below HHC: humanhuman communication) and human-computer interaction (HCI). The study shows significant and reliable differences in how dialogue is being managed. In Section 3, we interpret these results with respect to their implications for recipient design. The results demonstrate not only that recipient design is relevant for HCI, but also that it leads to specific and predictable kinds of clarification dialogues being taken up by users confronted with an artificial dialogue system. Fin</context>
</contexts>
<marker>Amalberti, Carbonell, Falzon, 1993</marker>
<rawString>R. Amalberti, N. Carbonell, and P. Falzon. 1993. User representations of computer systems in humancomputer speech interaction. International Journal of Man-Machine Studies, 38:547–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2920" citStr="Asher and Lascarides, 2003" startWordPosition="429" endWordPosition="432">e accomodated. In our approach, we show how the search space for relevant connections can be constrained considerably by incorporating the generic conversation analytic principle of recipient design (Sacks et al., 1974, p727). This treats user utterances as explicit instructions for how they are to be incorporated into the unfolding discourse—an approach that can itself be accomodated within much current discourse semantic work whereby potential discourse interpretation is facilitated by drawing tighter structural and semantic constraints from each discourse contribution (Webber et al., 1999; Asher and Lascarides, 2003). We extend this here to include constraints and conditions for the use of clarification subdialogues. Our approach is empirically driven throughout. In Section 2, we establish to what extent the principles of recipient design uncovered for natural human interaction can be adopted for the still artificial situation of human-computer interaction. Although it is commonly assumed that results concerning human-human interaction can be applied to human-computer interaction (Horvitz, 1999), there are also revealing differences (Amalberti et al., 1993). We report on a targetted comparison of adopted </context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Nicholas Asher and Alex Lascarides. 2003. Logics of conversation. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Fischer</author>
<author>Anton Batliner</author>
</authors>
<title>What makes speakers angry in human-computer conversation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Third Workshop on Human-Computer Conversation,</booktitle>
<location>Bellagio, Italy.</location>
<contexts>
<context position="29234" citStr="Fischer and Batliner, 2000" startWordPosition="4586" endWordPosition="4589">t perceives as well as appropriate ways of describing the scene, e.g., by means of ‘There are two groups of cups. What do you want me to do?’ a range of otherwise necessary clarificatory questions is avoided. Even in the case of failure, users will not doubt those capabilities of the system that it has displayed itself, due to alignment processes also observable in human-to-human dialogical interaction (Pickering and Garrod, 2004). After a successful interaction, users expect the system to be able to process parallel instructions because they reliably expect the system to behave consistently (Fischer and Batliner, 2000). 5 Conclusions In this paper, the discourse structure initiated by users in HCI situations has been investigated and the results have been three-fold. The structures initiated in HCI are much flatter than in HHC; no general orientation with respect to the aims of a sub-task are presented to the artificial communication partner, and marking is usually reduced. This needs to be accounted for in the mapping of the task-structure onto the discourse model, irrespective of the kind of representation chosen. Secondly, the contents of clarification subdialogues have also been identified as particular</context>
</contexts>
<marker>Fischer, Batliner, 2000</marker>
<rawString>Kerstin Fischer and Anton Batliner. 2000. What makes speakers angry in human-computer conversation. In Proceedings of the Third Workshop on Human-Computer Conversation, Bellagio, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Fischer</author>
</authors>
<title>Linguistic methods for investigating concepts in use.</title>
<date>2003</date>
<booktitle>In Thomas Stolz and Katja Kolbe, editors, Methodologie in der Linguistik. Frankfurt a.M.: Peter Lang.</booktitle>
<contexts>
<context position="18144" citStr="Fischer, 2003" startWordPosition="2841" endWordPosition="2842">) or a distance 188 usr11-1 hallo# [hello#] sys ERROR usr11-2 siehst du was [do you see anything?] sys ERROR usr11-3 was siehst du [what do you see?] sys ERROR 652-a: input is invalid. usr11-4 miB den abstand zwischen der vordersten tasse und der linken tasse [measure the distance between the frontmost cup and the left cup] Figure 2: Example dialogue extract showing membership analysis clarification questions in centimeters. This forced users to reformulate their dialogue contributions—an effective methodology for obtaining users’ hypotheses about the functioning and capabilities of a system (Fischer, 2003). In our terms, this leads directly to an explicit exploration of a user’s membership analysis. As expected in a joint attention scenario, very limited location analysis occurred. Topic analysis is also restricted; spatial formulations were chosen on the basis of what users believed to be ‘most understandable’ for the robot, which also leads back to the task of membership analysis. In contrast, there were many cases of membership analysis. There was clearly great uncertainty about the robot’s prerequisites for carrying out the spatial task and this was explicitly specified in the users’ varied</context>
</contexts>
<marker>Fischer, 2003</marker>
<rawString>Kerstin Fischer. 2003. Linguistic methods for investigating concepts in use. In Thomas Stolz and Katja Kolbe, editors, Methodologie in der Linguistik. Frankfurt a.M.: Peter Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederico T Fonseca</author>
<author>Max J Egenhofer</author>
<author>Peggy Agouris</author>
<author>Gilberto Cˆamara</author>
</authors>
<title>Using ontologies for integrated geographic information systems.</title>
<date>2002</date>
<journal>Transactions in GIS,</journal>
<volume>6</volume>
<issue>3</issue>
<marker>Fonseca, Egenhofer, Agouris, Cˆamara, 2002</marker>
<rawString>Frederico T. Fonseca, Max J. Egenhofer, Peggy Agouris, and Gilberto Cˆamara. 2002. Using ontologies for integrated geographic information systems. Transactions in GIS, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
</authors>
<title>Discourse analysis.</title>
<date>1982</date>
<booktitle>Studies of Language in Restricted Semantic Domains,</booktitle>
<pages>138--174</pages>
<editor>In Richard Kittredge and John Lehrberger, editors, Sublanguage.</editor>
<location>Berlin, New York: De Gruyter.</location>
<contexts>
<context position="5599" citStr="Grosz (1982)" startWordPosition="850" endWordPosition="851">eriments, one participant is the ‘constructor’ who actually builds the model plane, the other participant is the ‘instructor’, who provides instructions for the constructor. The corpora differ in that the constructor in the HHC setting was another human interlocutor; in the other scenario, the participants were seated in front of a computer but were informed that they were actually talking to an automatic speech processing system (HCI).1 In all cases, there was no visual contact between constructor and instructor. Previous work on human-human taskoriented dialogues going back to, for example, Grosz (1982), has shown that dialogue structure commonly follows task structure. Moreover, it is well known that human-human interaction employs a variety of dialogue structuring mechanisms, ranging from meta-talk to discourse markers, and that some of these can usefully be employed for automatic analysis (Marcu, 2000). If dialogue with artificial agents were then to be structured as it is with human interlocutors, there would be many useful linguistic surface cues available for guiding interpretation. And, indeed, a common way of designing dialogue structure in HCI is to have it follow the structure of t</context>
</contexts>
<marker>Grosz, 1982</marker>
<rawString>Barbara J. Grosz. 1982. Discourse analysis. In Richard Kittredge and John Lehrberger, editors, Sublanguage. Studies of Language in Restricted Semantic Domains, pages 138–174. Berlin, New York: De Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hitzenberger</author>
<author>C Womser-Hacker</author>
</authors>
<date>1995</date>
<booktitle>Experimentelle Untersuchungen zu multimodalen naturlichsprachigen Dialogen in der MenschComputer-Interaktion. Sprache und Datenverarbeitung,</booktitle>
<marker>Hitzenberger, Womser-Hacker, 1995</marker>
<rawString>L. Hitzenberger and C. Womser-Hacker. 1995. Experimentelle Untersuchungen zu multimodalen naturlichsprachigen Dialogen in der MenschComputer-Interaktion. Sprache und Datenverarbeitung, 19(1):51–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Horvitz</author>
</authors>
<title>Uncertainty, action, and interaction: In pursuit of mixed-initiative computing.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems, Sept./Oct.:17–20.</journal>
<contexts>
<context position="3408" citStr="Horvitz, 1999" startWordPosition="503" endWordPosition="504">tighter structural and semantic constraints from each discourse contribution (Webber et al., 1999; Asher and Lascarides, 2003). We extend this here to include constraints and conditions for the use of clarification subdialogues. Our approach is empirically driven throughout. In Section 2, we establish to what extent the principles of recipient design uncovered for natural human interaction can be adopted for the still artificial situation of human-computer interaction. Although it is commonly assumed that results concerning human-human interaction can be applied to human-computer interaction (Horvitz, 1999), there are also revealing differences (Amalberti et al., 1993). We report on a targetted comparison of adopted dialogic strategies in natural human interaction (termed below HHC: humanhuman communication) and human-computer interaction (HCI). The study shows significant and reliable differences in how dialogue is being managed. In Section 3, we interpret these results with respect to their implications for recipient design. The results demonstrate not only that recipient design is relevant for HCI, but also that it leads to specific and predictable kinds of clarification dialogues being taken</context>
</contexts>
<marker>Horvitz, 1999</marker>
<rawString>Eric Horvitz. 1999. Uncertainty, action, and interaction: In pursuit of mixed-initiative computing. IEEE Intelligent Systems, Sept./Oct.:17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of unrestricted texts: a surface-based approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="5907" citStr="Marcu, 2000" startWordPosition="895" endWordPosition="896">were seated in front of a computer but were informed that they were actually talking to an automatic speech processing system (HCI).1 In all cases, there was no visual contact between constructor and instructor. Previous work on human-human taskoriented dialogues going back to, for example, Grosz (1982), has shown that dialogue structure commonly follows task structure. Moreover, it is well known that human-human interaction employs a variety of dialogue structuring mechanisms, ranging from meta-talk to discourse markers, and that some of these can usefully be employed for automatic analysis (Marcu, 2000). If dialogue with artificial agents were then to be structured as it is with human interlocutors, there would be many useful linguistic surface cues available for guiding interpretation. And, indeed, a common way of designing dialogue structure in HCI is to have it follow the structure of the task, since this defines the types of actions necessary and their sequencing. 1In fact, the interlocutors were always humans, as the artificial agent in the HCI conditions was simulated employing standard Wizard-of-Oz methods allowing tighter control of the linguistic responses received by the user. Figu</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The rhetorical parsing of unrestricted texts: a surface-based approach. Computational Linguistics, 26(3):395–448, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Ogden</author>
<author>P Bernick</author>
</authors>
<title>Using natural language interfaces.</title>
<date>1996</date>
<booktitle>Handbook of Human-Computer Interaction.</booktitle>
<editor>In M. Helander, editor,</editor>
<publisher>Elsevier Science Publishers, North Holland.</publisher>
<contexts>
<context position="31158" citStr="Ogden and Bernick, 1996" startWordPosition="4897" endWordPosition="4900">rs to initiate such clarificatory subdialogues has been found to be predictable, we have a basis for a range of implicit strategies for addressing the users’ subsequent linguistic behaviour. Recipient design has therefore been shown to be a powerful mechanism that, with the appropriate methods, can be incorporated in user-adapted dialogue management design. Information of the kind that we have uncovered empirically in the work reported in this paper can be used to react appropriately to the different types of users in two ways: either one can adapt the system or one can try to adapt the user (Ogden and Bernick, 1996). Although techniques for both strategies are supported by our results, in general we favour attempting to influence the user’s behaviour without restricting it a priori by means 191 of computer-initiated dialogue structure. Since the reasons for the users’ behaviour have been shown to be located on the level of their conceptualisation of the communication partner, explicit instruction may in any case not be useful—explicit guidance of users is not only often impractical but also is not received well by users. The preferred choice is then to influence the users’ concepts of their communication</context>
</contexts>
<marker>Ogden, Bernick, 1996</marker>
<rawString>W.C. Ogden and P. Bernick. 1996. Using natural language interfaces. In M. Helander, editor, Handbook of Human-Computer Interaction. Elsevier Science Publishers, North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Pickering</author>
<author>Simon Garrod</author>
</authors>
<title>Towards a mechanistic psychology of dialogue.</title>
<date>2004</date>
<journal>Behavioural and Brain Sciences,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="29041" citStr="Pickering and Garrod, 2004" startWordPosition="4558" endWordPosition="4561">he robot can perceive to those users that use greetings. Similarly, during a scene description where the system has the initiative, the opportunity is taken to introduce terms for the objects it perceives as well as appropriate ways of describing the scene, e.g., by means of ‘There are two groups of cups. What do you want me to do?’ a range of otherwise necessary clarificatory questions is avoided. Even in the case of failure, users will not doubt those capabilities of the system that it has displayed itself, due to alignment processes also observable in human-to-human dialogical interaction (Pickering and Garrod, 2004). After a successful interaction, users expect the system to be able to process parallel instructions because they reliably expect the system to behave consistently (Fischer and Batliner, 2000). 5 Conclusions In this paper, the discourse structure initiated by users in HCI situations has been investigated and the results have been three-fold. The structures initiated in HCI are much flatter than in HHC; no general orientation with respect to the aims of a sub-task are presented to the artificial communication partner, and marking is usually reduced. This needs to be accounted for in the mappin</context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>Martin J. Pickering and Simon Garrod. 2004. Towards a mechanistic psychology of dialogue. Behavioural and Brain Sciences, 27(2):169–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Ross</author>
<author>J Bateman</author>
<author>H Shi</author>
</authors>
<title>Applying Generic Dialogue Models to the Information State Approach.</title>
<date>2005</date>
<booktitle>In Proceedings of Symposium on Dialogue Modelling and Generation.</booktitle>
<location>Amsterdam.</location>
<contexts>
<context position="26978" citStr="Ross et al., 2005" startWordPosition="4241" endWordPosition="4244">therefore rely on this dialogue transition as providing an indication of problems to come, which can again be avoided in advance by explicit system-initiated assertions of information. 190 Our main focus in this paper has been on setting out and motivating some generic principles for dialogue system design. These principles could find diverse computational instantiations and it has not been our aim to argue for any one instantation rather than another. However, to conclude, we summarise briefly the approach that we are adopting to incorporating these mechanisms within our own dialogue system (Ross et al., 2005). Our system augments an information-state based approach with a distinguished vocabulary of discourse transitions between states. We attach ‘conceptualisation-conditions’ to these transitions which serve to post discourse goals whose particular function is to head off user-initiated clarification. The presence of a greeting is one such condition; the immediate transition to basic-level instructions is another. Recognition and production of instructions is aided by treating the semantic types that occur (‘cups’, ‘measure’, ‘move’, etc.) as elements of a domain ontology. The diverse topic/activ</context>
</contexts>
<marker>Ross, Bateman, Shi, 2005</marker>
<rawString>R.J. Ross, J. Bateman, and H. Shi. 2005. Applying Generic Dialogue Models to the Information State Approach. In Proceedings of Symposium on Dialogue Modelling and Generation. Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
<author>E Schegloff</author>
<author>G Jefferson</author>
</authors>
<title>A simplest systematics for the organisation of turn-taking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<pages>50--696</pages>
<contexts>
<context position="2511" citStr="Sacks et al., 1974" startWordPosition="368" endWordPosition="371">ad from detailed empirical studies of how such situations are handled in human-human interaction. Most proposed computational treatments of this situation up until now rely on formalised notions of relevance: a system attempts to interpret a user contribution by relating it to shared goals of the system and user. When a connection can be found, then even an apparently off-topic clarification can be accomodated. In our approach, we show how the search space for relevant connections can be constrained considerably by incorporating the generic conversation analytic principle of recipient design (Sacks et al., 1974, p727). This treats user utterances as explicit instructions for how they are to be incorporated into the unfolding discourse—an approach that can itself be accomodated within much current discourse semantic work whereby potential discourse interpretation is facilitated by drawing tighter structural and semantic constraints from each discourse contribution (Webber et al., 1999; Asher and Lascarides, 2003). We extend this here to include constraints and conditions for the use of clarification subdialogues. Our approach is empirically driven throughout. In Section 2, we establish to what extent</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>H. Sacks, E. Schegloff, and G. Jefferson. 1974. A simplest systematics for the organisation of turn-taking for conversation. Language, 50:696–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Sagerer</author>
<author>Hans-Jurgen Eikmeyer</author>
<author>Gert Rickheit</author>
</authors>
<title>Wir bauen jetzt ein Flugzeug”: Konstruieren im Dialog.</title>
<date>1994</date>
<tech>Arbeitsmaterialien. Report SFB 360,</tech>
<institution>University of Bielefeld.</institution>
<contexts>
<context position="4973" citStr="Sagerer et al., 1994" startWordPosition="751" endWordPosition="754">extent to which techniques of recipient design established on the basis of human-human natural interaction can be transferred to HCI, we investigated comparable task-oriented dialogues that varied according to whether the users believed that that they were interacting with another human or with an artificial agent. The data for our investigation were taken from three German corpora collected in the mid1990s within a toy plane building scenario used for a range of experiments in the German Collaborative Research Centre Situated Artificial Communicators (SFB 360) at the University of Bielefeld (Sagerer et al., 1994). In these experiments, one participant is the ‘constructor’ who actually builds the model plane, the other participant is the ‘instructor’, who provides instructions for the constructor. The corpora differ in that the constructor in the HHC setting was another human interlocutor; in the other scenario, the participants were seated in front of a computer but were informed that they were actually talking to an automatic speech processing system (HCI).1 In all cases, there was no visual contact between constructor and instructor. Previous work on human-human taskoriented dialogues going back to,</context>
</contexts>
<marker>Sagerer, Eikmeyer, Rickheit, 1994</marker>
<rawString>Gerhard Sagerer, Hans-Jurgen Eikmeyer, and Gert Rickheit. 1994. ”Wir bauen jetzt ein Flugzeug”: Konstruieren im Dialog. Arbeitsmaterialien. Report SFB 360, University of Bielefeld.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Schegloff</author>
</authors>
<title>Notes on a conversational practice: formulating place. In</title>
<date>1972</date>
<booktitle>Studies in Social Interaction,</booktitle>
<pages>75--119</pages>
<editor>D. Sudnow, editor,</editor>
<publisher>The Free Press,</publisher>
<location>New York.</location>
<contexts>
<context position="12513" citStr="Schegloff (1972)" startWordPosition="1961" endWordPosition="1962"> to problems given the difficulty of the interpretation task. 3 Interpretation of the observed differences in terms of recipient design Examining the results of the previous section more closely, we find signs that the concept of the communication partner to which participants were orienting was not the same for all participants. Some speakers believed structural marking also to be useful in the HCI situation, for example. In this section, we turn to a more exact consideration of the reasons for these differences and show that directly employing the mechanisms of recipient design developed by Schegloff (1972) is a beneficial strategy. The full range of variation observed, including intra-corpus variation that space precluded us describing in detail above, is seen to arise from a single common mechanism. Furthermore, we show that precisely the same mechanism leads to a predictive account of user-initiated clarificatory dialogues. 187 The starting point for the discussion is the conversation analytic notion of the insertion sequence. An insertion sequence is a subdialogue inserted between the first and second parts of an adjacency pair. They are problematic for artificial agents precisely because th</context>
</contexts>
<marker>Schegloff, 1972</marker>
<rawString>E. A. Schegloff. 1972. Notes on a conversational practice: formulating place. In D. Sudnow, editor, Studies in Social Interaction, pages 75–119. The Free Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Swerts</author>
</authors>
<title>Filled pauses as markers of discourse structure.</title>
<date>1998</date>
<journal>Journal of Pragmatics,</journal>
<volume>30</volume>
<pages>496</pages>
<contexts>
<context position="9097" citStr="Swerts, 1998" startWordPosition="1409" endWordPosition="1410">course marker explicit marking HHC HCI HHC HCI HHC HCI none 27.3 100 0 52.5 13.6 52.5 single 40.9 0 9.1 25.0 54.5 27.5 frequent 31.8 0 90.9 22.5 31.8 20.0 Percentage of speakers making no, single or frequent use of a particular structuring strategy. HCI: N=40; HHC: N=22. All differences are highly significant (ANOVA p&lt;0.005). Table 1: Distribution of dialogue structuring devices across experimental conditions also [so] or jetzt geht’s los [now]. This structuring function of discourse markers has been shown in several studies and so can be assumed to be quite usual for human-human interaction (Swerts, 1998). Furthermore, individual constructional steps are explicitly marked by means of als erstes, dann [first of all, then] or der erste Schritt [the first step]. In addition to the marking of the construction phases, we also find marking of the different activities, such as description of the main goal versus description of the main architecture, or different phases that arise through the addressing of different addressees, such as asides to the experimenters. Speakers in dialogues directed at human interlocutors are therefore attending to the following three aspects of discourse structure: • mark</context>
</contexts>
<marker>Swerts, 1998</marker>
<rawString>Marc Swerts. 1998. Filled pauses as markers of discourse structure. Journal of Pragmatics, 30:485– 496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Alistair Knott</author>
<author>Matthew Stone</author>
<author>Aravind Joshi</author>
</authors>
<title>Discourse relations: a structural and presuppositional account using lexicalized TAG.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th. Annual Meeting of the American Association for Computational Linguistics (ACL’99),</booktitle>
<pages>41--48</pages>
<institution>University of Maryland. American Association for Computational Linguistics.</institution>
<contexts>
<context position="2891" citStr="Webber et al., 1999" startWordPosition="425" endWordPosition="428">c clarification can be accomodated. In our approach, we show how the search space for relevant connections can be constrained considerably by incorporating the generic conversation analytic principle of recipient design (Sacks et al., 1974, p727). This treats user utterances as explicit instructions for how they are to be incorporated into the unfolding discourse—an approach that can itself be accomodated within much current discourse semantic work whereby potential discourse interpretation is facilitated by drawing tighter structural and semantic constraints from each discourse contribution (Webber et al., 1999; Asher and Lascarides, 2003). We extend this here to include constraints and conditions for the use of clarification subdialogues. Our approach is empirically driven throughout. In Section 2, we establish to what extent the principles of recipient design uncovered for natural human interaction can be adopted for the still artificial situation of human-computer interaction. Although it is commonly assumed that results concerning human-human interaction can be applied to human-computer interaction (Horvitz, 1999), there are also revealing differences (Amalberti et al., 1993). We report on a tar</context>
</contexts>
<marker>Webber, Knott, Stone, Joshi, 1999</marker>
<rawString>Bonnie Webber, Alistair Knott, Matthew Stone, and Aravind Joshi. 1999. Discourse relations: a structural and presuppositional account using lexicalized TAG. In Proceedings of the 37th. Annual Meeting of the American Association for Computational Linguistics (ACL’99), pages 41–48, University of Maryland. American Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Zoltan-Ford</author>
</authors>
<title>How to get people to say and type what computers can understand.</title>
<date>1991</date>
<journal>International journal of Man-Machine Studies,</journal>
<volume>34</volume>
<pages>647</pages>
<contexts>
<context position="20517" citStr="Zoltan-Ford (1991)" startWordPosition="3220" endWordPosition="3221">o that precisely these apparently out-of-domain topics are covered. A second strategy is to determine discourse conditions that can be used to alert the dialogue system to the likely occurrence or absence of these kinds of clarificatory subdialogues (see below). Third, we can design explicit strategies for interaction that will reduce the likelihood that a user will employ them: for example, by providing information about the agent’s capabilities, etc. as listed in Table 2 in advance by means of system-initiated assertions. That is, we can guide, or shape, to use the terminology introduced by Zoltan-Ford (1991), the users’ linguistic behaviour. A combination of these three capabilities promises to improve the overall quality of a dialogue system and forms the basis for a significant part of our current research. We have already ascertained empirically discourse conditions that support the second strategy above, and these follow again directly from the basic notions of recipient design and membership analysis. If a user already has a strong membership analysis in place—for example, due to preconceptions concerning the abilities (or, more commonly, lack of abilities) of the artificial agent—then this </context>
<context position="31833" citStr="Zoltan-Ford, 1991" startWordPosition="5005" endWordPosition="5006">y our results, in general we favour attempting to influence the user’s behaviour without restricting it a priori by means 191 of computer-initiated dialogue structure. Since the reasons for the users’ behaviour have been shown to be located on the level of their conceptualisation of the communication partner, explicit instruction may in any case not be useful—explicit guidance of users is not only often impractical but also is not received well by users. The preferred choice is then to influence the users’ concepts of their communication partner and thus their linguistic behaviour by shaping (Zoltan-Ford, 1991). In particular, Schegloff’s analysis shows in detail the human interlocutors’ preference for those location terms that express group membership. Therefore, in natural dialogues the speakers constantly signal to each other who they are, what the other person can expect them to know. Effective system design should therefore provide users with precisely those kinds of information that constitute their most frequent clarification questions initially and in the manner that we have discussed. Acknowledgement The authors gratefully acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG)</context>
</contexts>
<marker>Zoltan-Ford, 1991</marker>
<rawString>Elizabeth Zoltan-Ford. 1991. How to get people to say and type what computers can understand. International journal of Man-Machine Studies, 34:527– 647.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>