<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001150">
<sectionHeader confidence="0.886123666666667" genericHeader="method">
BOOK REVIEWS
COMPUTATIONAL LEXICOGRAPHY FOR NATURAL
LANGUAGE PROCESSING
</sectionHeader>
<subsectionHeader confidence="0.617295">
Bran Boguraev and Ted Briscoe (eds.)
</subsectionHeader>
<bodyText confidence="0.7336845">
(University of Cambridge and University of Lancaster)
London: Longman, 1989, xv + 310 pp.
(Copublished in the United States with John Wiley &amp;
Sons)
Hardbound, ISBN 0-582-02248-7 (Longman) and
0-470-21187-3 (Wiley), $49.95, £24.00
</bodyText>
<figure confidence="0.830698">
Reviewed by
Geoffrey Sampson
</figure>
<affiliation confidence="0.529674">
University of Leeds
</affiliation>
<bodyText confidence="0.996798009900991">
As automatic natural language processing (NLP) moves
out of the era of toy pilot projects and begins to grapple
with real-life language in all its complexity, it needs access
to quantities of information about individual lexical items.
The only plausible source of such information lies in ma-
chine-readable versions of ordinary published dictionaries;
although they are designed for other purposes and are far
from ideal for computer use, they represent an investment
of resources that the computational linguistics research
community is in no position to match. This book, whose
various chapters are co-authored by a total of sixteen
contributors drawn from research groups at Cambridge,
Amsterdam, and New Mexico State University, is about
the issues that have arisen in exploiting one dictionary, the
Longman Dictionary of Contemporary English (LDOCE),
for NLP purposes.
Topics covered include: the translation of what was
originally a typesetting tape into a format enabling the
computer to locate efficiently the various categories of
information specified for a given word; the relationship
between grammatical information as listed for entries in
the dictionary and the categories required by the theoreti-
cal linguist; the use of pronunciation information in speech-
recognition systems; and various tentative experiments in
deriving computer-usable semantic information from the
definition portions of dictionary entries. A series of appendi-
ces gives useful factual information about the dictionary
and its vocabulary. Since all the work discussed relates to
LDOCE, it concerns English exclusively; at one point (p. 8)
the editors offer a French example that is truly hair-raising
in spelling and pronunciation, but this is a rare blemish in a
book whose standard of editing and production is high.
The research reported has a solid and professional qual-
ity. These are no dilettantes musing on how in principle one
might aim to achieve a given task, but people who have set
about getting the job done, for the whole language, and tell
us here what problems they encountered and how far they
were successful. Most of the problems will be common to
any published dictionary, and the introductory chapter and
the bibliography give very full coverage of dictionary-based
NLP research in general; for a research group aiming to
work with a different English dictionary, or a dictionary of
another language, this book would be an excellent place to
start. Although much of the emphasis is rightly on practical
computing considerations, there are also many novel and
valuable theoretical insights. David Carter, discussing
speech-recognition systems that use partial phonetic infor-
mation to produce classes of candidate words, points out
that standard ways of measuring the systems&apos; performance
are grossly misleading, both because they ignore relative
frequencies of words in a class and because measurement
ought to be logarithmic rather than linear—to whittle a
vocabulary of 10,000 words down to a class of 100 candi-
dates is to do half, not 99%, of the task of identifying the
stimulus. Theoretical linguists have claimed that various
aspects of grammatical subcategorization of lexical items
are predictable from meaning and thematic structure, as
one might well expect (how can a person master a language
unless such matters are governed by general rules?), but
Boguraev and Briscoe exploit that fact that LDOCE codes
individual verb-senses for susceptibility to dative alterna-
tion (Sally slid the drink to Susan Sally slid Susan the
drink) in order to submit this idea to detailed testing, and
they find that none of the proposed rules holds; whether
dative alternation is permissible seems to be an aribtrary
fact about individual senses of individual words (a point
that underlines the crucial importance of full-scale diction-
aries in NLP).
Rather than discussing each chapter of this book in
detail, in the rest of my review I shall take up two issues
that recur in many chapters and on which I am inclined to
question the position put forward by the contributors.
The first has to do with how far an ordinary published
dictionary comparable to LDOCE (which has 55,000 en-
tries) provides adequate coverage of text, in view of the
open-ended quality of any natural language and the fact
that published dictionaries do not try to cover proper
names. Many contributors to the book reviewed are quite
pessimistic, suggesting that while published dictionaries
are the best resources we have, they fall very far short of
adequate coverage of the language. The writers repeatedly
refer to Walker and Amsler (1986), who studied the extent
of correspondence between a large sample (over 8 million
word-tokens) of material from the New York Times news
service, and the (approximately 70,000) entries of Web-
ster&apos;s Seventh New Collegiate Dictionary. Walker and
Computational Linguistics Volume 16, Number 2, June 1990 113
Book Reviews Computational Lexicography for Natural Language Processing
Amsler say that as many as 64% of the word-types in their
corpus do not occur in the dictionary.
This surprised me, since it seemed to contradict the
findings of a smaller-scale piece of research that I carried
out (Sampson 1989) before learning of Walker and Am-
sler&apos;s work. I ran the computer-usable version prepared by
Roger Mitton (1986) of the third edition of the Oxford
Advanced Learner&apos;s Dictionary (OALD3) against a 45,622
word-token subset of the LOB Corpus, in order to analyze
in detail the nature of the gaps in the dictionary (like
Amsler and Walker, I eliminated punctuation, &amp;quot;words&amp;quot;
consisting of digits, etc.). Only 1,477 word-tokens, or 3.24%
of my sample, were missing from the dictionary. Although
I was counting tokens while Walker and Amsler counted
types, in itself this can hardly explain more than a fraction
of the difference between our results. Zipfs Law, taken in
conjunction with the rank/frequency information about
the LOB Corpus in Hofland and Johansson (1982), sug-
gests that 44,145 tokens found in the dictionary might
represent on the order of 18,000 types, and the 1,477
missing tokens represent 1,176 types, implying that in my
sample the non-dictionary word types would be in the
region of 6%. Walker and Amsler&apos;s dictionary lacked in-
flected forms and also some high-frequency proper names
(though its total number of entries is nevertheless slightly
lower than that of the Webster dictionary); furthermore,
the news service corpus is likely to have a higher density of
proper names than the LOB Corpus. But these factors do
not account for the difference between Walker and Am-
sler&apos;s and my figures; they state that inflected forms and
proper names comprise only half of their nondictionary
word types, so that about one-third of all word types in their
sample is missing from their dictionary without being
names or inflected forms. (Because of the large scale of
their experiment, Walker and Amsler are not in a position
to give detailed analyses of this one-third.)
I can only suppose that the discrepancy between Walker
and Amsler&apos;s figure of one-third and my 6% stems from the
fact that relative size of sample matters when counting
missing types, rather than missing tokens. Even if Walker
and Amsler had no greater a proportion of nondictionary
tokens than I, in their sample that would be a quarter of a
million tokens, many of which would be unique types, while
for either sample the bulk of tokens that are in the dictio-
nary will be concentrated on the same few very common
types. If Walker and Amsler had counted tokens rather
than types, then (and had used a dictionary that listed
inflected forms), it is not clear that they would have reached
a figure much higher than my 3.24%. This puts a different
gloss on the question of dictionary adequacy.
The second issue I should like to take up is the claim,
made by several contributors to this book, that LDOCE is
&amp;quot;uniquely suitable for computational lexicography&amp;quot; (p. 2).
There has been an idea in the air for some time now that
LDOCE is not just one lexical resource among others but,
for computer applications, has a clear superiority over all
alternatives. The point has not been easy to assess, since it
relates in part to material found only in the electronic
version of LDOCE, and (unlike Oxford, who have made
OALD3 relatively freely available to computational re-
searchers), Longman have strictly guarded their electronic
copyright, allowing the dictionary to be used only by a few
groups whose work is centered on it ard who as a natural
consequence have tended to champion it against its rivals.
My own loyalties lie rather with OALD, edited by my
Leeds/ CCALAS colleague Anthony Cowie; this book of-
fers an opportunity to try to establish how much of the
claimed superiority of LDOCE is real and how much hype.
(I shall compare LDOCE only with OALD; the book under
review itself treats OALD as &amp;quot;the competition,&amp;quot; and I have
little experience with other dictionaries such as Webster&apos;s
or the Collins/University of Birmingham COBUILD dic-
tionary.)
Relevant points made by contributors fall under four
headings: file format; &amp;quot;controlled vocabulary&amp;quot;; subject and
&amp;quot;box&amp;quot; codes; and grammatical classification schemes.
So far as file format is concerned, Eric Akkerman claims
(p. 66) that &amp;quot;the computer-tape version of LDOCE turned
out to be much more structured than that of OALD, which
was basically a typesetting tape.&amp;quot; This is certainly fair
comment with respect to the tapes originally produced by
the two publishers. For some time now, though, it has been
of historical interest only, since more than one research
group have produced parsed versions of the OALD3 tape.
Furthermore, in this and other respects the critique of
OALD in this book has been overtaken by the publication,
also in 1989, of the fourth edition of OALD, which was
designed from the start to be computer-tractable and has a
format that I suspect is superior to that of the LDOCE tape
(though, certainly, by no means as sophisticated as that of
the resource that some of these researchers have created
using LDOCE as their raw material).
&amp;quot;Controlled vocabulary&amp;quot; refers to the fact that the lan-
guage of LDOCE definitions is restricted to a specified set
of some 2,200 words, each of which is supposed to be used
only in its central sense(s). This might make it easier to
deduce formalized representations of word meanings from
the definitions, and several of the contributors who work on
computational semantics express enthusiasm for this fea-
ture of LDOCE. The editors are much more cautious,
however (see pp. 16, 34); and research by Jansen et al.
(1987) makes it questionable how far one can accurately
describe the LDOCE definition vocabulary as &amp;quot;controlled&amp;quot;
at all. Furthermore, an explicit controlled-vocabulary pol-
icy is presumably an advantage only if, without it, lexicog-
raphers tend to range more widely in phrasing their defini-
tions. To test this, I looked at the examples of LDOCE
definitions quoted by Hiyan Alshawi (p. 157ff), who has
developed a system that attempts to assign word senses to
general semantic categories by locating the grammatical
head word of the definition, commonly a superordinate of
the definiendum. In many cases the same head word oc-
cured in both OALD4 and LDOCE definitions; and in every
case the OALD4 head word seemed at least as intuitively
</bodyText>
<page confidence="0.806572">
114 Computational Linguistics Volume 16, Number 2, June 1990
</page>
<note confidence="0.397462">
Book Reviews Computational Lexicography for Natural Language Processing
</note>
<bodyText confidence="0.9990779">
suitable as that in LDOCE as a core-vocabulary superordi-
nate term—in the case of club (verb), one might think that
OALD4 hit is more straightforward and unambiguous than
LDOCE beat. OALD4 has no definition for bring out
corresponding to LDOCE&apos;s &amp;quot;to introduce (usu. a young
lady) into the social life of a great city.&amp;quot;
Probably more important are the &amp;quot;subject&amp;quot; and &amp;quot;box&amp;quot;
codes, which are included (in the subentries for individual
word senses) only in the electronic LDOCE, not in the
published version; this book seems to be the only public
source of information about them. Subject codes are four-
letter sequences that represent the topic and/or geographic
domain in which a word or word sense is used, e.g. NAZV
&amp;quot;nautical,&amp;quot; GAQA &amp;quot;games/Argentina.&amp;quot; The ten-charac-
ter box codes express semantic selection restrictions; an
example from page 14 is --L-X----S for sandwich &amp;quot;to put
tightly in between,&amp;quot; where X in place 5 means preference
for abstract or human subject, and S in place 10 means
preference for solid object. Unfortunately, very little detail
is given about the box code system; thus, although place 3
is said to be associated with &amp;quot;level of attitude,&amp;quot; we are not
told what the L in the sandwich box code means, and in
general there is no statement of the range of categories
expressed by these codes. Subject and box codes both look
as though they might be genuinely significant NLP re-
sources having no equivalent in other dictionaries; the
subject codes might be usable for disambiguating words in
context, the box codes could serve that purpose and also,
conceivably, might make a contribution in grammatical
parsing. But the book does not tell us enough to permit their
value to be assessed, and the editors note that &amp;quot;there are
problems concerning the accuracy and completeness of this
type of information in LDOCE. . . . None of the work
reported in this book makes significant use of these codes.&amp;quot;
The singlemost important reason for the book&apos;s claims
about the superiority of LDOCE is its system of grammati-
cal classification; Chapter 3, by Eric Akkerman, is an
extended comparison between the classification schemes of
LDOCE and OALD3, very much to the latter&apos;s disfavor.
LDOCE uses a system of two- or three-part codes in which,
broadly, a capital letter indicates part of speech and a digit
indicates valency; in some cases a lowercase letter is added
to indicate a finer subdivision. Thus the use of know as in &amp;quot;I
know (that) he&apos;ll come&amp;quot; is coded T5a, in which T means
&amp;quot;transitive verb with one object,&amp;quot; 5 means &amp;quot;followed by a
that-clause,&amp;quot; and a means &amp;quot;that can be omitted.&amp;quot; OALD3
uses intuitive abbreviations (n, vt, prep, etc.) for the sort of
information coded by capital letters in LDOCE; verb valen-
cies are indicated by specifying one or more of the 51 &amp;quot;verb
patterns&amp;quot; of a system worked out by A. S. Hornby, the first
editor of OALD. The use of know just quoted is VP9, i.e.,
&amp;quot;S + vt + that-clause.&amp;quot; One respect in which the LDOCE
system is more flexible is that its valency digits can combine
with letters for any part of speech to which they apply, not
just with verb letters; thus 5 combines also with F (attribu-
tive adjective or adverb) to give a code F5a for sure as in
&amp;quot;He was sure (that) she knew.&amp;quot; Only a few combinations of
nonverb letter with number are meaningful, but those that
are give information that is not coded in OALD at all.
It is also fair to say, as Akkerman does, that the Hornby
verb-pattern system is in some cases linguistically naive,
failing to distinguish cases in which items occur together
through a grammatical construction from cases of acciden-
tal juxtaposition. He quotes VP14A, &amp;quot;S + vi + to-
infinitive,&amp;quot; used in OALD3 both for come in &amp;quot;How did you
come to know her?&amp;quot;, where the to-clause is in construction
with come, and for stop in &amp;quot;We stopped to have a rest,&amp;quot;
where the to-clause is an immediate constituent of the
sentence. For reasons like this, the Hornby system has been
abandoned in OALD4, which uses a more rational method
of classifying verbs (it is also more advanced than the
OALD3 system in other areas of grammar; for instance, it
codes nine grammatical types of noun rather than just
dividing nouns into countable and uncountable as in
OALD3). Working through Akkerman&apos;s detailed compari-
son of the two dictionaries, I find that in most of the
examples he quotes to illustrate the superiority of LDOCE
over OALD3, the OALD4 coding of the relevant word
senses escapes his criticism.
On the other hand, Akkerman also appears to me to be
less than fair even to OALD3. He often seems to begin the
axiom that the LDOCE grammatical distinctions are ideal,
so that any deviation in OALD, whether by conflating
classes that LDOCE distinguishes or by drawing distinc-
tions that LDOCE does not make, must be bad. He com-
plains (p. 75) that OALD3 has no code corresponding to the
a of LDOCE T5a, showing that that can be left out from a
finite clause following the verb; but this code is useful only
if verbs differ with respect to omissibility of that, and
omission of that is normally governed by considerations
other than verb identity. (I thought ascertain might be a
verb not tolerating omission of that; but LDOCE codes it as
T5a.) Conversely, Akkerman says that there is no grammat-
ical motivation for the Hornby-code distinction between
verbs of physical perception and other verbs; but in (Brit-
ish) English the former verbs are grammatically distinctive
in requiring can in the present tense (&amp;quot;I can see the car&amp;quot; but
not *&amp;quot;I see the car&amp;quot;), though I am not sure whether this
explains the feature of the Hornby system to which Akker-
man refers. At one point (p. 74), Akkerman even criticizes
the OALD3 system as inferior to that of LDOCE by refer-
ence to a point on which they seem to agree. Akkerman
says that it is illogical for OALD3 to use a single Hornby
code VP19C to cover the two uses of a verb like understand
as in &amp;quot;I can&apos;t understand him behaving so foolishly&amp;quot; and &amp;quot;I
can&apos;t understand his behaving so foolishly,&amp;quot; arguing that
the syntax of the two examples is very different. Again it is
not clear that verbs differ in their propensity to occur in one
of these patterns rather than in the other; but, more to the
point, I can find only a single code, Ti, applicable to either
pattern in the LDOCE entry for understand.
In sum, I am not convinced that researchers without
access to the Longman dictionary are doomed to second-
class citizenship in the emerging world of natural language
Computational Linguistics Volume 16, Number 2, June 1990 115
Book Reviews Natural Language Processing in LISP, POP-11, and PROLOG
processing. LDOCE has some attractive special features,
but so do other dictionaries. There can be little doubt, on
the other hand, about the importance and value of the kind
of research reported in this book.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="method">
REFERENCES
</sectionHeader>
<reference confidence="0.8866514">
Hofland, Knut and Johansson, Stig. 1982 Word Frequencies in British
and American English. Norwegian Computing Centre for the Humani-
ties, Bergen.
Jansen, J.; Mergeai, J.P.; and Vanandroye, J. 1987 Controlling LDOCE&apos;s
Controlled Vocabulary. In: Cowie, A.P., ed., The Dictionary and the
Language Learner (Lexicographica, series maior, 17). Niemeyer, Tu-
bingen, 78-94.
Mitton, Roger. 1986 A partial dictionary of English in Computer-Usable
Form. Literary and Linguistic Computing 1:214-215.
Sampson, G.R. 1989 How Fully Does a Machine-Usable Dictionary
Cover English Text? Literary and Linguistic Computing. 4:29-35.
Walker, D.E. and Amsler, R.A. 1986 The Use of Machine-Readable
Dictionaries in Sublanguage Analysis. In: Grishman, Ralph and Kit-
tredge, Richard, eds., Analyzing Language in Restricted Domains:
Sublanguage Description and Processing. Lawrence Erlbaum, Hills-
dale, NJ: 69-83.
Geoffrey Sampson is Professor of Linguistics and Director of the
Centre for Computer Analysis of Language and Speech at the
University of Leeds, Britain&apos;s largest unitary university. His
computational linguistics research is corpus-based, and includes
development of a system of robust parsing by stochastic optimiza-
tion (Project APRIL). With Roger Garside and Geoffrey Leech
he coedited The Computational Analysis of English (Longman,
1987). Sampson&apos;s address is: Department of Linguistics and
Phonetics, University of Leeds, Leeds LS2 9JT, U.K.
</reference>
<sectionHeader confidence="0.992976666666667" genericHeader="method">
NATURAL LANGUAGE PROCESSING IN LISP: AN
INTRODUCTION TO COMPUTATIONAL LINGUISTICS
NATURAL LANGUAGE PROCESSING IN POP-11: AN
INTRODUCTION TO COMPUTATIONAL LINGUISTICS
NATURAL LANGUAGE PROCESSING IN PROLOG: AN
INTRODUCTION TO COMPUTATIONAL LINGUISTICS
</sectionHeader>
<subsectionHeader confidence="0.961995">
Gerald Gazdar and Chris Mellish
</subsectionHeader>
<bodyText confidence="0.551750111111111">
(University of Sussex and University of Edinburgh,
respectively)
Workingham, England: Addison-Wesley, 1989
Lisp volume: xv + 524 pp.
Hardbound, ISBN 0-201-17825-7, £17.95
Pop-11 volume: xv 524 pp.
Hardbound, ISBN 0-201-17448-0, £17.95
Prolog volume: xv + 504 pp.
Hardbound, ISBN 0-201-18053-7, £17.95
</bodyText>
<footnote confidence="0.560385">
Reviewed by
Kwee TjoeLiong
</footnote>
<subsubsectionHeader confidence="0.659125">
University of Amsterdam
</subsubsectionHeader>
<bodyText confidence="0.995631719298246">
This is a very interesting and intriguing array of textbooks
to read, to compare, and to review. It also has been a rather
hard job for me to do so. The last paragraphs try to explain
why. First of all, however, an objective and factual sum-
mary of the contents and form of Gazdar and Mellish&apos;s
NLP in X: An Introduction to CL, where Xis instantiated
to one of (PROLOG, POP-11, LISP} (reviewer&apos;s short-
hand).
Quotations can be helpful as the shortest way to give you
a rapid impression. From the letter that the book review
editor sent me is this encouraging line: &amp;quot;They are really
three separate versions of the same book, so there&apos;s not
nearly as much reading as there first appears.&amp;quot; Therefore,
one of the titles is treated here as prototypical (to wit, the
Prolog volume). Whenever they differ, the other two are,
subjectively, considered as derivative.
I am going to quote amply from the authors&apos; Preface,
since it is a characterization of the book in their own words.
It is neatly split into sections, and I distinguish three
aspects: &apos;What,&apos; What Exactly,&apos; and &apos;In What Way,&apos; each
aspect being handled in a pair of consecutive sections of the
preface.
What: From the first two sections, Audience and Cover-
age:
This book is aimed at computer scientists and linguists
at undergraduate, postgraduate or faculty level, who
have taken, or are concurrently taking, a programming
course in X. . . . The book is specifically intended to
teach NLP and computational linguistics: it does not
attempt to teach programming or computer science to
linguists, or to provide more than an implicit introduc-
tion to linguistics for computer scientists. . .
The major focus of this book, as of the field to which it
provides an introduction, is on the processing of the
orthographic forms of natural language utterances and
text. [No issues in speech, because those are] topics that
deserve books to themselves, books that we would not be
competent to write. Most of the book deals with the
parsing and understanding of natural language, much
less on the production of it. This bias reflects the present
shape of the field, and of the state of knowledge. . .
The book is formally oriented and technical in charac-
ter, and organized, for the most part, around formal
techniques. The perspective adopted is that of computer
science, not cognitive science. . . . We concentrate on
areas that are beginning to be well understood, and for
which standard techniques. . . have begun to emerge. . . .
[Hence,] a good deal more time on syntactic processing
than on semantic or pragmatic processing. . . . Discus-
sion of developments at the leading edge of NLP re-
search, on such topics as parallel parsing algorithms, the
new style categorial grammars, connectionist approaches
or the emerging implementations of situation semantics
and discourse representation theory are excluded alto-
gether or relegated to the further reading sections.. . . A
less readily excusable omission is any consideration of
the role of probabilistic techniques in NLP. [But . . . ]
</bodyText>
<page confidence="0.93225">
116 Computational Linguistics Volume 16, Number 2, June 1990
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.004742">
<title confidence="0.993472666666667">BOOK REVIEWS COMPUTATIONAL LEXICOGRAPHY FOR NATURAL LANGUAGE PROCESSING</title>
<author confidence="0.997518">Bran Boguraev</author>
<author confidence="0.997518">Ted Briscoe</author>
<affiliation confidence="0.973393">(University of Cambridge and University of Lancaster)</affiliation>
<note confidence="0.909872833333333">London: Longman, 1989, xv + 310 pp. (Copublished in the United States with John Wiley &amp; Sons) Hardbound, ISBN 0-582-02248-7 (Longman) and 0-470-21187-3 (Wiley), $49.95, £24.00 Reviewed by</note>
<author confidence="0.999716">Geoffrey Sampson</author>
<affiliation confidence="0.999169">University of Leeds</affiliation>
<abstract confidence="0.992870034055728">As automatic natural language processing (NLP) moves out of the era of toy pilot projects and begins to grapple with real-life language in all its complexity, it needs access to quantities of information about individual lexical items. The only plausible source of such information lies in machine-readable versions of ordinary published dictionaries; although they are designed for other purposes and are far from ideal for computer use, they represent an investment of resources that the computational linguistics research community is in no position to match. This book, whose various chapters are co-authored by a total of sixteen contributors drawn from research groups at Cambridge, Amsterdam, and New Mexico State University, is about the issues that have arisen in exploiting one dictionary, the Longman Dictionary of Contemporary English (LDOCE), Topics covered include: the translation of what was originally a typesetting tape into a format enabling the computer to locate efficiently the various categories of information specified for a given word; the relationship between grammatical information as listed for entries in the dictionary and the categories required by the theoretical linguist; the use of pronunciation information in speechrecognition systems; and various tentative experiments in deriving computer-usable semantic information from the definition portions of dictionary entries. A series of appendices gives useful factual information about the dictionary and its vocabulary. Since all the work discussed relates to concerns English exclusively; at one point (p. 8) the editors offer a French example that is truly hair-raising in spelling and pronunciation, but this is a rare blemish in a book whose standard of editing and production is high. The research reported has a solid and professional quality. These are no dilettantes musing on how in principle one might aim to achieve a given task, but people who have set about getting the job done, for the whole language, and tell us here what problems they encountered and how far they were successful. Most of the problems will be common to any published dictionary, and the introductory chapter and the bibliography give very full coverage of dictionary-based NLP research in general; for a research group aiming to work with a different English dictionary, or a dictionary of another language, this book would be an excellent place to start. Although much of the emphasis is rightly on practical computing considerations, there are also many novel and valuable theoretical insights. David Carter, discussing speech-recognition systems that use partial phonetic information to produce classes of candidate words, points out that standard ways of measuring the systems&apos; performance are grossly misleading, both because they ignore relative frequencies of words in a class and because measurement ought to be logarithmic rather than linear—to whittle a vocabulary of 10,000 words down to a class of 100 candidates is to do half, not 99%, of the task of identifying the stimulus. Theoretical linguists have claimed that various aspects of grammatical subcategorization of lexical items are predictable from meaning and thematic structure, as one might well expect (how can a person master a language unless such matters are governed by general rules?), but and Briscoe exploit that fact that individual verb-senses for susceptibility to dative alternaslid the drink to Susan Sally slid Susan the order to submit this idea to detailed testing, and they find that none of the proposed rules holds; whether dative alternation is permissible seems to be an aribtrary fact about individual senses of individual words (a point that underlines the crucial importance of full-scale dictionaries in NLP). Rather than discussing each chapter of this book in detail, in the rest of my review I shall take up two issues that recur in many chapters and on which I am inclined to question the position put forward by the contributors. The first has to do with how far an ordinary published comparable to has 55,000 entries) provides adequate coverage of text, in view of the open-ended quality of any natural language and the fact that published dictionaries do not try to cover proper names. Many contributors to the book reviewed are quite pessimistic, suggesting that while published dictionaries are the best resources we have, they fall very far short of adequate coverage of the language. The writers repeatedly refer to Walker and Amsler (1986), who studied the extent of correspondence between a large sample (over 8 million of material from the York Times and the (approximately 70,000) entries of Web- Seventh New Collegiate Dictionary. and Linguistics Volume 16, Number 2, June 1990 Book Reviews Computational Lexicography for Natural Language Processing Amsler say that as many as 64% of the word-types in their corpus do not occur in the dictionary. This surprised me, since it seemed to contradict the findings of a smaller-scale piece of research that I carried out (Sampson 1989) before learning of Walker and Amsler&apos;s work. I ran the computer-usable version prepared by Mitton (1986) of the third edition of Oxford Learner&apos;s Dictionary (OALD3) a 45,622 word-token subset of the LOB Corpus, in order to analyze in detail the nature of the gaps in the dictionary (like Amsler and Walker, I eliminated punctuation, &amp;quot;words&amp;quot; consisting of digits, etc.). Only 1,477 word-tokens, or 3.24% of my sample, were missing from the dictionary. Although I was counting tokens while Walker and Amsler counted types, in itself this can hardly explain more than a fraction of the difference between our results. Zipfs Law, taken in conjunction with the rank/frequency information about the LOB Corpus in Hofland and Johansson (1982), suggests that 44,145 tokens found in the dictionary might represent on the order of 18,000 types, and the 1,477 missing tokens represent 1,176 types, implying that in my sample the non-dictionary word types would be in the region of 6%. Walker and Amsler&apos;s dictionary lacked inflected forms and also some high-frequency proper names (though its total number of entries is nevertheless slightly lower than that of the Webster dictionary); furthermore, the news service corpus is likely to have a higher density of proper names than the LOB Corpus. But these factors do not account for the difference between Walker and Amsler&apos;s and my figures; they state that inflected forms and proper names comprise only half of their nondictionary word types, so that about one-third of all word types in their sample is missing from their dictionary without being names or inflected forms. (Because of the large scale of their experiment, Walker and Amsler are not in a position to give detailed analyses of this one-third.) I can only suppose that the discrepancy between Walker and Amsler&apos;s figure of one-third and my 6% stems from the fact that relative size of sample matters when counting missing types, rather than missing tokens. Even if Walker and Amsler had no greater a proportion of nondictionary tokens than I, in their sample that would be a quarter of a million tokens, many of which would be unique types, while for either sample the bulk of tokens that are in the dictionary will be concentrated on the same few very common types. If Walker and Amsler had counted tokens rather than types, then (and had used a dictionary that listed inflected forms), it is not clear that they would have reached a figure much higher than my 3.24%. This puts a different gloss on the question of dictionary adequacy. The second issue I should like to take up is the claim, by several contributors to this book, that &amp;quot;uniquely suitable for computational lexicography&amp;quot; (p. 2). There has been an idea in the air for some time now that not just one lexical resource among others but, for computer applications, has a clear superiority over all alternatives. The point has not been easy to assess, since it relates in part to material found only in the electronic of (unlike Oxford, who have made freely available to computational researchers), Longman have strictly guarded their electronic copyright, allowing the dictionary to be used only by a few groups whose work is centered on it ard who as a natural consequence have tended to champion it against its rivals. own loyalties lie rather with by my Leeds/ CCALAS colleague Anthony Cowie; this book offers an opportunity to try to establish how much of the superiority of is and how much hype. shall compare with book under itself treats &amp;quot;the competition,&amp;quot; and I have little experience with other dictionaries such as Webster&apos;s the Collins/University of Birmingham dictionary.) Relevant points made by contributors fall under four headings: file format; &amp;quot;controlled vocabulary&amp;quot;; subject and &amp;quot;box&amp;quot; codes; and grammatical classification schemes. So far as file format is concerned, Eric Akkerman claims 66) that &amp;quot;the computer-tape version of to be much more structured than that of was basically a typesetting tape.&amp;quot; This is certainly fair comment with respect to the tapes originally produced by the two publishers. For some time now, though, it has been of historical interest only, since more than one research have produced parsed versions of the Furthermore, in this and other respects the critique of this book has been overtaken by the publication, in 1989, of the fourth edition of was designed from the start to be computer-tractable and has a that I suspect is superior to that of the (though, certainly, by no means as sophisticated as that of the resource that some of these researchers have created their raw material). &amp;quot;Controlled vocabulary&amp;quot; refers to the fact that the lanof is restricted to a specified set of some 2,200 words, each of which is supposed to be used only in its central sense(s). This might make it easier to deduce formalized representations of word meanings from the definitions, and several of the contributors who work on computational semantics express enthusiasm for this feaof editors are much more cautious, (see pp. 16, 34); and research by Jansen al. (1987) makes it questionable how far one can accurately the vocabulary as &amp;quot;controlled&amp;quot; at all. Furthermore, an explicit controlled-vocabulary policy is presumably an advantage only if, without it, lexicographers tend to range more widely in phrasing their defini- To test this, I looked at the examples of definitions quoted by Hiyan Alshawi (p. 157ff), who has developed a system that attempts to assign word senses to general semantic categories by locating the grammatical head word of the definition, commonly a superordinate of the definiendum. In many cases the same head word ocin both and in every the word seemed at least as intuitively 114 Computational Linguistics Volume 16, Number 2, June 1990 Book Reviews Computational Lexicography for Natural Language Processing as that in a core-vocabulary superorditerm—in the case of one might think that hit more straightforward and unambiguous than beat. OALD4 no definition for out to introduce (usu. a young lady) into the social life of a great city.&amp;quot; Probably more important are the &amp;quot;subject&amp;quot; and &amp;quot;box&amp;quot; codes, which are included (in the subentries for individual senses) only in the electronic in the published version; this book seems to be the only public source of information about them. Subject codes are fourletter sequences that represent the topic and/or geographic domain in which a word or word sense is used, e.g. NAZV &amp;quot;nautical,&amp;quot; GAQA &amp;quot;games/Argentina.&amp;quot; The ten-character box codes express semantic selection restrictions; an from page 14 is --L-X----S for put in between,&amp;quot; where place 5 means preference abstract or human subject, and place 10 means preference for solid object. Unfortunately, very little detail is given about the box code system; thus, although place 3 is said to be associated with &amp;quot;level of attitude,&amp;quot; we are not what the the code means, and in general there is no statement of the range of categories expressed by these codes. Subject and box codes both look as though they might be genuinely significant NLP resources having no equivalent in other dictionaries; the subject codes might be usable for disambiguating words in context, the box codes could serve that purpose and also, conceivably, might make a contribution in grammatical parsing. But the book does not tell us enough to permit their value to be assessed, and the editors note that &amp;quot;there are problems concerning the accuracy and completeness of this of information in . . . of the work reported in this book makes significant use of these codes.&amp;quot; The singlemost important reason for the book&apos;s claims the superiority of is system of grammatical classification; Chapter 3, by Eric Akkerman, is an extended comparison between the classification schemes of much to the latter&apos;s disfavor. a system of twoor three-part codes in which, broadly, a capital letter indicates part of speech and a digit indicates valency; in some cases a lowercase letter is added indicate a finer subdivision. Thus the use of in &amp;quot;I (that) he&apos;ll come&amp;quot; is coded T5a, in which &amp;quot;transitive verb with one object,&amp;quot; 5 means &amp;quot;followed by a and be omitted.&amp;quot; intuitive abbreviations (n, prep, for the sort of coded by capital letters in valencies are indicated by specifying one or more of the 51 &amp;quot;verb patterns&amp;quot; of a system worked out by A. S. Hornby, the first of use of quoted is VP9, i.e., + that-clause.&amp;quot; One respect in which the system is more flexible is that its valency digits can combine with letters for any part of speech to which they apply, not just with verb letters; thus 5 combines also with F (attribuadjective or adverb) to give a code F5a for in &amp;quot;He was sure (that) she knew.&amp;quot; Only a few combinations of nonverb letter with number are meaningful, but those that give information that is not coded in all. It is also fair to say, as Akkerman does, that the Hornby verb-pattern system is in some cases linguistically naive, failing to distinguish cases in which items occur together through a grammatical construction from cases of accidenjuxtaposition. He quotes VP14A, &amp;quot;S + + toused in for &amp;quot;How did you come to know her?&amp;quot;, where the to-clause is in construction for in &amp;quot;We to have a rest,&amp;quot; where the to-clause is an immediate constituent of the sentence. For reasons like this, the Hornby system has been in uses a more rational method of classifying verbs (it is also more advanced than the in other areas of grammar; for instance, it codes nine grammatical types of noun rather than just dividing nouns into countable and uncountable as in through Akkerman&apos;s detailed comparison of the two dictionaries, I find that in most of the he quotes to illustrate the superiority of of the relevant word senses escapes his criticism. On the other hand, Akkerman also appears to me to be than fair even to often seems to begin the that the distinctions are ideal, that any deviation in by conflating that or by drawing distincthat not make, must be bad. He com- (p. 75) that no code corresponding to the showing that be left out from a finite clause following the verb; but this code is useful only verbs differ with respect to omissibility of of normally governed by considerations than verb identity. (I thought be a not tolerating omission of it as T5a.) Conversely, Akkerman says that there is no grammatical motivation for the Hornby-code distinction between verbs of physical perception and other verbs; but in (British) English the former verbs are grammatically distinctive requiring the present tense (&amp;quot;I can see the car&amp;quot; but not *&amp;quot;I see the car&amp;quot;), though I am not sure whether this explains the feature of the Hornby system to which Akkerman refers. At one point (p. 74), Akkerman even criticizes as inferior to that of reference to a point on which they seem to agree. Akkerman that it is illogical for use a single Hornby VP19C to cover the two uses of a verb like in &amp;quot;I understand him behaving so foolishly&amp;quot; and &amp;quot;I can&apos;t understand his behaving so foolishly,&amp;quot; arguing that the syntax of the two examples is very different. Again it is not clear that verbs differ in their propensity to occur in one of these patterns rather than in the other; but, more to the point, I can find only a single code, Ti, applicable to either in the for In sum, I am not convinced that researchers without access to the Longman dictionary are doomed to secondclass citizenship in the emerging world of natural language Computational Linguistics Volume 16, Number 2, June 1990 115 Reviews Language Processing in LISP, POP-11, and PROLOG some attractive special features, but so do other dictionaries. There can be little doubt, on the other hand, about the importance and value of the kind of research reported in this book.</abstract>
<note confidence="0.740157666666667">REFERENCES Knut and Johansson, Stig. 1982 Frequencies in British American English. Computing Centre for the Humanities, Bergen. J.; Mergeai, J.P.; and Vanandroye, J. 1987 Controlling Vocabulary. In: Cowie, A.P., ed., Dictionary and the Learner series maior, 17). Niemeyer, Tubingen, 78-94. Mitton, Roger. 1986 A partial dictionary of English in Computer-Usable and Linguistic Computing 1:214-215. Sampson, G.R. 1989 How Fully Does a Machine-Usable Dictionary English Text? and Linguistic Computing. Walker, D.E. and Amsler, R.A. 1986 The Use of Machine-Readable Dictionaries in Sublanguage Analysis. In: Grishman, Ralph and Kit- Richard, eds., Language in Restricted Domains: Description and Processing. Erlbaum, Hillsdale, NJ: 69-83. Sampson Professor of Linguistics and Director of the</note>
<affiliation confidence="0.7078275">Centre for Computer Analysis of Language and Speech at the University of Leeds, Britain&apos;s largest unitary university. His</affiliation>
<abstract confidence="0.894534">computational linguistics research is corpus-based, and includes development of a system of robust parsing by stochastic optimization (Project APRIL). With Roger Garside and Geoffrey Leech coedited Computational Analysis of English 1987). Sampson&apos;s address is: Department of Linguistics and Phonetics, University of Leeds, Leeds LS2 9JT, U.K.</abstract>
<intro confidence="0.895456">LANGUAGE PROCESSING IN</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Knut Hofland</author>
<author>Stig Johansson</author>
</authors>
<date>1982</date>
<booktitle>Word Frequencies in British and American English. Norwegian Computing Centre for the Humanities,</booktitle>
<location>Bergen.</location>
<contexts>
<context position="6370" citStr="Hofland and Johansson (1982)" startWordPosition="995" endWordPosition="998"> the Oxford Advanced Learner&apos;s Dictionary (OALD3) against a 45,622 word-token subset of the LOB Corpus, in order to analyze in detail the nature of the gaps in the dictionary (like Amsler and Walker, I eliminated punctuation, &amp;quot;words&amp;quot; consisting of digits, etc.). Only 1,477 word-tokens, or 3.24% of my sample, were missing from the dictionary. Although I was counting tokens while Walker and Amsler counted types, in itself this can hardly explain more than a fraction of the difference between our results. Zipfs Law, taken in conjunction with the rank/frequency information about the LOB Corpus in Hofland and Johansson (1982), suggests that 44,145 tokens found in the dictionary might represent on the order of 18,000 types, and the 1,477 missing tokens represent 1,176 types, implying that in my sample the non-dictionary word types would be in the region of 6%. Walker and Amsler&apos;s dictionary lacked inflected forms and also some high-frequency proper names (though its total number of entries is nevertheless slightly lower than that of the Webster dictionary); furthermore, the news service corpus is likely to have a higher density of proper names than the LOB Corpus. But these factors do not account for the difference</context>
</contexts>
<marker>Hofland, Johansson, 1982</marker>
<rawString>Hofland, Knut and Johansson, Stig. 1982 Word Frequencies in British and American English. Norwegian Computing Centre for the Humanities, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jansen</author>
<author>J P Mergeai</author>
<author>J Vanandroye</author>
</authors>
<title>Controlling LDOCE&apos;s Controlled Vocabulary.</title>
<date>1987</date>
<booktitle>The Dictionary and the Language Learner (Lexicographica, series maior, 17). Niemeyer, Tubingen,</booktitle>
<pages>78--94</pages>
<editor>In: Cowie, A.P., ed.,</editor>
<contexts>
<context position="11009" citStr="Jansen et al. (1987)" startWordPosition="1777" endWordPosition="1780">s that of the resource that some of these researchers have created using LDOCE as their raw material). &amp;quot;Controlled vocabulary&amp;quot; refers to the fact that the language of LDOCE definitions is restricted to a specified set of some 2,200 words, each of which is supposed to be used only in its central sense(s). This might make it easier to deduce formalized representations of word meanings from the definitions, and several of the contributors who work on computational semantics express enthusiasm for this feature of LDOCE. The editors are much more cautious, however (see pp. 16, 34); and research by Jansen et al. (1987) makes it questionable how far one can accurately describe the LDOCE definition vocabulary as &amp;quot;controlled&amp;quot; at all. Furthermore, an explicit controlled-vocabulary policy is presumably an advantage only if, without it, lexicographers tend to range more widely in phrasing their definitions. To test this, I looked at the examples of LDOCE definitions quoted by Hiyan Alshawi (p. 157ff), who has developed a system that attempts to assign word senses to general semantic categories by locating the grammatical head word of the definition, commonly a superordinate of the definiendum. In many cases the s</context>
</contexts>
<marker>Jansen, Mergeai, Vanandroye, 1987</marker>
<rawString>Jansen, J.; Mergeai, J.P.; and Vanandroye, J. 1987 Controlling LDOCE&apos;s Controlled Vocabulary. In: Cowie, A.P., ed., The Dictionary and the Language Learner (Lexicographica, series maior, 17). Niemeyer, Tubingen, 78-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Mitton</author>
</authors>
<title>A partial dictionary of English</title>
<date>1986</date>
<booktitle>in Computer-Usable Form. Literary and Linguistic Computing</booktitle>
<pages>1--214</pages>
<contexts>
<context position="5718" citStr="Mitton (1986)" startWordPosition="893" endWordPosition="894">ial from the New York Times news service, and the (approximately 70,000) entries of Webster&apos;s Seventh New Collegiate Dictionary. Walker and Computational Linguistics Volume 16, Number 2, June 1990 113 Book Reviews Computational Lexicography for Natural Language Processing Amsler say that as many as 64% of the word-types in their corpus do not occur in the dictionary. This surprised me, since it seemed to contradict the findings of a smaller-scale piece of research that I carried out (Sampson 1989) before learning of Walker and Amsler&apos;s work. I ran the computer-usable version prepared by Roger Mitton (1986) of the third edition of the Oxford Advanced Learner&apos;s Dictionary (OALD3) against a 45,622 word-token subset of the LOB Corpus, in order to analyze in detail the nature of the gaps in the dictionary (like Amsler and Walker, I eliminated punctuation, &amp;quot;words&amp;quot; consisting of digits, etc.). Only 1,477 word-tokens, or 3.24% of my sample, were missing from the dictionary. Although I was counting tokens while Walker and Amsler counted types, in itself this can hardly explain more than a fraction of the difference between our results. Zipfs Law, taken in conjunction with the rank/frequency information </context>
</contexts>
<marker>Mitton, 1986</marker>
<rawString>Mitton, Roger. 1986 A partial dictionary of English in Computer-Usable Form. Literary and Linguistic Computing 1:214-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Sampson</author>
</authors>
<title>How Fully Does a Machine-Usable Dictionary Cover English Text? Literary and Linguistic Computing.</title>
<date>1989</date>
<pages>4--29</pages>
<contexts>
<context position="5607" citStr="Sampson 1989" startWordPosition="875" endWordPosition="876">r (1986), who studied the extent of correspondence between a large sample (over 8 million word-tokens) of material from the New York Times news service, and the (approximately 70,000) entries of Webster&apos;s Seventh New Collegiate Dictionary. Walker and Computational Linguistics Volume 16, Number 2, June 1990 113 Book Reviews Computational Lexicography for Natural Language Processing Amsler say that as many as 64% of the word-types in their corpus do not occur in the dictionary. This surprised me, since it seemed to contradict the findings of a smaller-scale piece of research that I carried out (Sampson 1989) before learning of Walker and Amsler&apos;s work. I ran the computer-usable version prepared by Roger Mitton (1986) of the third edition of the Oxford Advanced Learner&apos;s Dictionary (OALD3) against a 45,622 word-token subset of the LOB Corpus, in order to analyze in detail the nature of the gaps in the dictionary (like Amsler and Walker, I eliminated punctuation, &amp;quot;words&amp;quot; consisting of digits, etc.). Only 1,477 word-tokens, or 3.24% of my sample, were missing from the dictionary. Although I was counting tokens while Walker and Amsler counted types, in itself this can hardly explain more than a fract</context>
</contexts>
<marker>Sampson, 1989</marker>
<rawString>Sampson, G.R. 1989 How Fully Does a Machine-Usable Dictionary Cover English Text? Literary and Linguistic Computing. 4:29-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Walker</author>
<author>R A Amsler</author>
</authors>
<title>The Use of Machine-Readable Dictionaries in Sublanguage Analysis.</title>
<date>1986</date>
<booktitle>Analyzing Language in Restricted Domains: Sublanguage Description and Processing. Lawrence Erlbaum,</booktitle>
<pages>69--83</pages>
<editor>In: Grishman, Ralph and Kittredge, Richard, eds.,</editor>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="5002" citStr="Walker and Amsler (1986)" startWordPosition="778" endWordPosition="781">nd on which I am inclined to question the position put forward by the contributors. The first has to do with how far an ordinary published dictionary comparable to LDOCE (which has 55,000 entries) provides adequate coverage of text, in view of the open-ended quality of any natural language and the fact that published dictionaries do not try to cover proper names. Many contributors to the book reviewed are quite pessimistic, suggesting that while published dictionaries are the best resources we have, they fall very far short of adequate coverage of the language. The writers repeatedly refer to Walker and Amsler (1986), who studied the extent of correspondence between a large sample (over 8 million word-tokens) of material from the New York Times news service, and the (approximately 70,000) entries of Webster&apos;s Seventh New Collegiate Dictionary. Walker and Computational Linguistics Volume 16, Number 2, June 1990 113 Book Reviews Computational Lexicography for Natural Language Processing Amsler say that as many as 64% of the word-types in their corpus do not occur in the dictionary. This surprised me, since it seemed to contradict the findings of a smaller-scale piece of research that I carried out (Sampson </context>
</contexts>
<marker>Walker, Amsler, 1986</marker>
<rawString>Walker, D.E. and Amsler, R.A. 1986 The Use of Machine-Readable Dictionaries in Sublanguage Analysis. In: Grishman, Ralph and Kittredge, Richard, eds., Analyzing Language in Restricted Domains: Sublanguage Description and Processing. Lawrence Erlbaum, Hillsdale, NJ: 69-83.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Geoffrey</author>
</authors>
<title>Sampson is Professor of Linguistics and Director of the Centre for Computer Analysis of Language and Speech at the University of Leeds, Britain&apos;s largest unitary university. His computational linguistics research is corpus-based, and includes development of a system of robust parsing by stochastic optimization</title>
<date>1987</date>
<booktitle>(Project APRIL). With Roger Garside and Geoffrey Leech he coedited The Computational Analysis of English (Longman,</booktitle>
<institution>Department of Linguistics and Phonetics, University of Leeds,</institution>
<location>Leeds LS2 9JT, U.K.</location>
<marker>Geoffrey, 1987</marker>
<rawString>Geoffrey Sampson is Professor of Linguistics and Director of the Centre for Computer Analysis of Language and Speech at the University of Leeds, Britain&apos;s largest unitary university. His computational linguistics research is corpus-based, and includes development of a system of robust parsing by stochastic optimization (Project APRIL). With Roger Garside and Geoffrey Leech he coedited The Computational Analysis of English (Longman, 1987). Sampson&apos;s address is: Department of Linguistics and Phonetics, University of Leeds, Leeds LS2 9JT, U.K.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>