<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000186">
<title confidence="0.9862935">
Phrase-Based Backoff Models for Machine Translation of Highly Inflected
Languages
</title>
<author confidence="0.989282">
Mei Yang
</author>
<affiliation confidence="0.9941425">
Department of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.952376">
Seattle, WA, USA
</address>
<email confidence="0.999678">
yangmei@ee.washington.edu
</email>
<author confidence="0.935981">
Katrin Kirchhoff
</author>
<affiliation confidence="0.9936">
Department of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.952349">
Seattle, WA, USA
</address>
<email confidence="0.999759">
katrin@ee.washington.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998435">
We propose a backoff model for phrase-
based machine translation that translates
unseen word forms in foreign-language
text by hierarchical morphological ab-
stractions at the word and the phrase level.
The model is evaluated on the Europarl
corpus for German-English and Finnish-
English translation and shows improve-
ments over state-of-the-art phrase-based
models.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985825">
Current statistical machine translation (SMT) usu-
ally works well in cases where the domain is
fixed, the training and test data match, and a large
amount of training data is available. Nevertheless,
standard SMT models tend to perform much bet-
ter on languages that are morphologically simple,
whereas highly inflected languages with a large
number of potential word forms are more prob-
lematic, particularly when training data is sparse.
SMT attempts to find a sentence eˆ in the desired
output language given the corresponding sentence
f in the source language, according to
</bodyText>
<equation confidence="0.994934">
eˆ = argmaxeP(f|e)P(e) (1)
</equation>
<bodyText confidence="0.999861244444445">
Most state-of-the-art SMT adopt a phrase-based
approach such that e is chunked into I phrases
¯e1, ..., ¯eI and the translation model is defined
over mappings between phrases in e and in f.
i.e. P(¯f|¯e). Typically, phrases are extracted from
a word-aligned training corpus. Different inflected
forms of the same lemma are treated as different
words, and there is no provision for unseen forms,
i.e. unknown words encountered in the test data
are not translated at all but appear verbatim in the
output. Although the percentage of such unseen
word forms may be negligible when the training
set is large and matches the test set well, it may rise
drastically when training data is limited or from
a different domain. Many current and future ap-
plications of machine translation require the rapid
porting of existing systems to new languages and
domains without being able to collect appropri-
ate training data; this problem can therefore be
expected to become increasingly more important.
Furthermore, untranslated words can be one of the
main factors contributing to low user satisfaction
in practical applications.
Several previous studies (see Section 2 below)
have addressed issues of morphology in SMT, but
most of these have focused on the problem of word
alignment and vocabulary size reduction. Princi-
pled ways of incorporating different levels of mor-
phological abstraction into phrase-based models
have mostly been ignored so far. In this paper we
propose a hierarchical backoff model for phrase-
based translation that integrates several layers of
morphological operations, such that more specific
models are preferred over more general models.
We experimentally evaluate the model on transla-
tion from two highly-inflected languages, German
and Finnish, into English and present improve-
ments over a state-of-the-art system. The rest of
the paper is structured as follows: The following
section discusses related background work. Sec-
tion 4 describes the proposed model; Sections 5
and 6 provide details about the data and baseline
system used in this study. Section 7 provides ex-
perimental results and discussion. Section 8 con-
cludes.
</bodyText>
<page confidence="0.999378">
41
</page>
<sectionHeader confidence="0.856739" genericHeader="method">
2 Morphology in SMT Systems
</sectionHeader>
<bodyText confidence="0.999687606557377">
Previous approaches have used morpho-syntactic
knowledge mainly at the low-level stages of a ma-
chine translation system, i.e. for preprocessing.
(Niessen and Ney, 2001a) use morpho-syntactic
knowledge for reordering certain syntactic con-
structions that differ in word order in the source
vs. target language (German and English). Re-
ordering is applied before training and after gener-
ating the output in the target language. Normaliza-
tion of English/German inflectional morphology
to base forms for the purpose of word alignment is
performed in (Corston-Oliver and Gamon, 2004)
and (Koehn, 2005), demonstrating that the vocab-
ulary size can be reduced significantly without af-
fecting performance.
Similar morphological simplifications have
been applied to other languages such as Roma-
nian (Fraser and Marcu, 2005) in order to de-
crease word alignment error rate. In (Niessen
and Ney, 2001b), a hierarchical lexicon model is
used that represents words as combinations of full
forms, base forms, and part-of-speech tags, and
that allows the word alignment training procedure
to interpolate counts based on the different lev-
els of representation. (Goldwater and McCloskey,
2005) investigate various morphological modifi-
cations for Czech-English translations: a subset
of the vocabulary was converted to stems, pseu-
dowords consisting of morphological tags were in-
troduced, and combinations of stems and morpho-
logical tags were used as new word forms. Small
improvements were found in combination with a
word-to-word translation model. Most of these
techniques have focused on improving word align-
ment or reducing vocabulary size; however, it is
often the case that better word alignment does not
improve the overall translation performance of a
standard phrase-based SMT system.
Phrase-based models themselves have not ben-
efited much from additional morpho-syntactic
knowledge; e.g. (Lioma and Ounis, 2005) do not
report any improvement from integrating part-of-
speech information at the phrase level. One suc-
cessful application of morphological knowledge is
(de Gispert et al., 2005), where knowledge-based
morphological techniques are used to identify un-
seen verb forms in the test text and to generate
inflected forms in the target language based on
annotated POS tags and lemmas. Phrase predic-
tion in the target language is conditioned on the
phrase in the source language as well the corre-
sponding tuple of lemmatized phrases. This tech-
nique worked well for translating from a morpho-
logically poor language (English) to a more highly
inflected language (Spanish) when applied to un-
seen verb forms. Treating both known and un-
known verbs in this way, however, did not result
in additional improvements. Here we extend the
notion of treating known and unknown words dif-
ferently and propose a backoff model for phrase-
based translation.
</bodyText>
<sectionHeader confidence="0.991912" genericHeader="method">
3 Backoff Models
</sectionHeader>
<bodyText confidence="0.999526">
Generally speaking, backoff models exploit rela-
tionships between more general and more spe-
cific probability distributions. They specify under
which conditions the more specific model is used
and when the model “backs off” to the more gen-
eral distribution. Backoff models have been used
in a variety of ways in natural language process-
ing, most notably in statistical language modeling.
In language modeling, a higher-order n-gram dis-
tribution is used when it is deemed reliable (deter-
mined by the number of occurrences in the train-
ing data); otherwise, the model backs off to the
next lower-order n-gram distribution. For the case
of trigrams, this can be expressed as:
</bodyText>
<equation confidence="0.951200666666667">
pBO(wt|wt−1, wt−2) (2)
JdcpML(wt|wt−1, wt−2) if c &gt; T
α(wt−1,wt−2)pBO(wt|wt−1)oth
</equation>
<bodyText confidence="0.971181095238095">
where pML denotes the maximum-likelihood
estimate, c denotes the count of the triple
(wi, wi−1, wi−2) in the training data, T is the count
threshold above which the maximum-likelihood
estimate is retained, and dN(wi,wi−1,wi−2) is a dis-
counting factor (generally between 0 and 1) that is
applied to the higher-order distribution. The nor-
malization factor α(wi−1, wi−2) ensures that the
distribution sums to one. In (Bilmes and Kirch-
hoff, 2003) this method was generalized to a back-
off model with multiple paths, allowing the com-
bination of different backed-off probability esti-
mates. Hierarchical backoff schemes have also
been used by (Zitouni et al., 2003) for language
modeling and by (Gildea, 2001) for semantic role
labeling. (Resnik et al., 2001) used backoff trans-
lation lexicons for cross-language information re-
trieval. More recently, (Xi and Hwa, 2005) have
used backoff models for combining in-domain and
=
erwise
</bodyText>
<page confidence="0.991833">
42
</page>
<bodyText confidence="0.999342333333333">
out-of-domain data for the purpose of bootstrap-
ping a part-of-speech tagger for Chinese, outper-
forming standard methods such as EM.
</bodyText>
<sectionHeader confidence="0.965026" genericHeader="method">
4 Backoff Models in MT
</sectionHeader>
<bodyText confidence="0.995075393939394">
In order to handle unseen words in the test data
we propose a hierarchical backoff model that uses
morphological information. Several morphologi-
cal operations, in particular stemming and com-
pound splitting, are interleaved such that a more
specific form (i.e. a form closer to the full word
form) is chosen before a more general form (i.e. a
form that has undergone morphological process-
ing). The procedure is shown in Figure 1 and can
be described as follows: First, a standard phrase
table based on full word forms is trained. If an
unknown word fi is encountered in the test data
with context cfi = fi−n, ..., fi−1, fi+1, ..., fi+m,
the word is first stemmed, i.e. fi0 = stem(fi).
The phrase table entries for words sharing the
same stem are then modified by replacing the
respective words with their stems. If an en-
try can be found among these such that the
source language side of the phrase pair consists of
fi−n, ..., fi−1, stem(fi), fi+1, ..., fi+m, the corre-
sponding translation is used (or, if several pos-
sible translations occur, the one with the high-
est probability is chosen). Note that the con-
text may be empty, in which case a single-word
phrase is used. If this step fails, the model backs
off to the next level and applies compound split-
ting to the unknown word (further described be-
low), i.e.(f00
i1, f00
i2) = split(fi). The match with
the original word-based phrase table is then per-
formed again. If this step fails for either of the
two parts of f00, stemming is applied again: f 000
</bodyText>
<equation confidence="0.99986825">
i1 =
stem(f00
i1) and f000
i2 = stem(f00
</equation>
<bodyText confidence="0.999966666666667">
i2), and a match with
the stemmed phrase table entries is carried out.
Only if the attempted match fails at this level is the
input passed on verbatim in the translation output.
The backoff procedure could in principle be
performed on demand by a specialized decoder;
however, since we use an off-the-shelf decoder
(Pharaoh (Koehn, 2004)), backoff is implicitly en-
forced by providing a phrase-table that includes
all required backoff levels and by preprocessing
the test data accordingly. The phrase table will
thus include entries for phrases based on full word
forms as well as for their stemmed and/or split
counterparts.
For each entry with decomposed morphological
forms, four probabilities need to be provided: two
phrasal translation scores for both translation di-
rections, p(¯e |¯f) and p(¯f|¯e), and two correspond-
ing lexical scores, which are computed as a prod-
uct of the word-by-word translation probabilities
under the given alignment a:
</bodyText>
<equation confidence="0.990169">
p(fj|ei) (3)
</equation>
<bodyText confidence="0.930438368421053">
where j ranges of words in phrase f and i ranges
of words in phrase ¯e. In the case of unknown
words in the foreign language, we need the prob-
abilities p(¯e|stem(¯f)), p(stem(¯f)|¯e) (where the
stemming operation stem(¯f) applies to the un-
known words in the phrase), and their lexical
equivalents. These are computed by relative fre-
quency estimation, e.g.
¯f)) = count(¯e, stem(
count(stem(
¯f
The other translation probabilities are computed
analogously. Since normalization is performed
over the entire phrase table, this procedure has
the effect of discounting the original probability
porig(¯e|¯f) since e¯ may now have been generated
by either f or by stem( f). In the standard formu-
lation of backoff models shown in Equation 3, this
amounts to:
</bodyText>
<equation confidence="0.9830328">
pBO(¯e |f) (5)
�
d¯e, ¯fporig(¯e |¯f) if c(¯e, ¯f) &gt; 0
=
p(¯e|stem(¯f)) otherwise
</equation>
<figureCaption confidence="0.824908">
Figure 1: Backoff procedure.
</figureCaption>
<equation confidence="0.996937416666667">
i, i2 i
i, i,
i2 i2
i i
i
i, i2
plex(¯e |¯f) = J 1 I
j=1 a(i)=j
|j|a(i) = j|
p(¯e|stem(
)) (4)
f))
</equation>
<page confidence="0.994577">
43
</page>
<bodyText confidence="0.999953866666667">
where
is the amount by which the word-based phrase
translation probability is discounted. Equiva-
lent probability computations are carried out for
the lexical translation probabilities. Similar to
the backoff level that uses stemming, the trans-
lation probabilities need to be recomputed for
the levels that use splitting and combined split-
ting/stemming.
In order to derive the morphological decompo-
sition we use existing tools. For stemming we
use the TreeTagger (Schmid, 1994) for German
and the Snowball stemmer1 for Finnish. A vari-
ety of ways for compound splitting have been in-
vestigated in machine translation (Koehn, 2003).
Here we use a simple technique that considers all
possible ways of segmenting a word into two sub-
parts (with a minimum-length constraint of three
characters on each subpart). A segmentation is ac-
cepted if the subparts appear as individual items
in the training data vocabulary. The only linguis-
tic knowledge used in the segmentation process is
the removal of final &lt;s&gt; from the first part of the
compound before trying to match it to an existing
word. This character (Fugen-s) is often inserted as
“glue” when forming German compounds. Other
glue characters were not considered for simplic-
ity (but could be added in the future). The seg-
mentation method is clearly not linguistically ad-
equate: first, words may be split into more than
two parts. Second, the method may generate mul-
tiple possible segmentations without a principled
way of choosing among them; third, it may gener-
ate invalid splits. However, a manual analysis of
300 unknown compounds in the German develop-
ment set (see next section) showed that 95.3% of
them were decomposed correctly: for the domain
at hand, most compounds need not be split into
more than two parts; if one part is itself a com-
pound it is usually frequent enough in the train-
ing data to have a translation. Furthermore, lexi-
calized compounds, whose decomposition would
lead to wrong translations, are also typically fre-
quent words and have an appropriate translation in
the training data.
</bodyText>
<footnote confidence="0.97569">
1http://snowball.tartarus.org
</footnote>
<sectionHeader confidence="0.987096" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.999968147058824">
Our data consists of the Europarl training, devel-
opment and test definitions for German-English
and Finnish-English of the 2005 ACL shared data
task (Koehn and Monz, 2005). Both German
and Finnish are morphologically rich languages:
German has four cases and three genders and
shows number, gender and case distinctions not
only on verbs, nouns, and adjectives, but also
on determiners. In addition, it has notoriously
many compounds. Finnish is a highly agglutina-
tive language with a large number of inflectional
paradigms (e.g. one for each of its 15 cases). Noun
compounds are also frequent. On the 2005 ACL
shared MT data task, Finnish to English trans-
lation showed the lowest average performance
(17.9% BLEU) and German had the second low-
est (21.9%), while the average BLEU scores for
French-to-English and Spanish-to-English were
much higher (27.1% and 27.8%, respectively).
The data was preprocessed by lowercasing and
filtering out sentence pairs whose length ratio
(number of words in the source language divided
by the number of words in the target language,
or vice versa) was &gt; 9. The development and
test sets consist of 2000 sentences each. In order
to study the effect of varying amounts of training
data we created several training partitions consist-
ing of random selections of a subset of the full
training set. The sizes of the partitions are shown
in Table 1, together with the resulting percentage
of out-of-vocabulary (OOV) words in the develop-
ment and test sets (“type” refers to a unique word
in the vocabulary, “token” to an instance in the ac-
tual text).
</bodyText>
<sectionHeader confidence="0.998446" genericHeader="method">
6 System
</sectionHeader>
<bodyText confidence="0.999944142857143">
We use a two-pass phrase-based statistical MT
system using GIZA++ (Och and Ney, 2000) for
word alignment and Pharaoh (Koehn, 2004) for
phrase extraction and decoding. Word alignment
is performed in both directions using the IBM-
4 model. Phrases are then extracted from the
word alignments using the method described in
(Och and Ney, 2003). For first-pass decoding we
use Pharaoh in n-best mode. The decoder uses a
weighted combination of seven scores: 4 transla-
tion model scores (phrase-based and lexical scores
for both directions), a trigram language model
score, a distortion score, and a word penalty. Non-
monotonic decoding is used, with no limit on the
</bodyText>
<figure confidence="0.951807428571429">
d¯e,
f¯=
1 − p(¯e, stem(
p(¯e,
(6)
f)
f))
</figure>
<page confidence="0.987199">
44
</page>
<table confidence="0.999704785714286">
German-English
Set # sent # words oov dev oov test
train1 5K 101K 7.9/42.6 7.9/42.7
train2 25K 505K 3.8/22.1 3.7/21.9
train3 50K 1013K 2.7/16.1 2.7/16.1
train4 250K 5082K 1.3/8.1 1.2/7.5
train5 751K 15258K 0.8/4.9 0.7/4.4
Finnish-English
Set # sent # words oov dev oov test
train1 5K 78K 16.6/50.6 16.4/50.6
train2 25K 395K 8.6/28.2 8.4/27.8
train3 50K 790K 6.3/21.0 6.2/20.8
train4 250K 3945K 3.1/10.4 3.0/10.2
train5 717K 11319K 1.8/6.2 1.8/6.1
</table>
<tableCaption confidence="0.990666">
Table 1: Training set sizes and percentages of
OOV words (types/tokens) on the development
and test sets.
</tableCaption>
<table confidence="0.998380666666667">
dev test
Finnish-English 22.2 22.0
German-English 24.6 24.8
</table>
<tableCaption confidence="0.843904">
Table 2: Baseline system BLEU scores (%) on dev
and test sets.
</tableCaption>
<bodyText confidence="0.999898615384615">
number of moves. The score combination weights
are trained by a minimum error rate training pro-
cedure similar to (Och and Ney, 2003). The tri-
gram language model uses modified Kneser-Ney
smoothing and interpolation of trigram and bigram
estimates and was trained on the English side of
the bitext. In the first pass, 2000 hypotheses are
generated per sentence. In the second pass, the
seven scores described above are combined with
4-gram language model scores. The performance
of the baseline system on the development and test
sets is shown in Table 2. The BLEU scores ob-
tained are state-of-the-art for this task.
</bodyText>
<sectionHeader confidence="0.995714" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9997795">
We first investigated to what extent the OOV rate
on the development data could be reduced by our
backoff procedure. Table 3 shows the percentage
of words that are still untranslatable after back-
off. A comparison with Table 1 shows that the
backoff model reduces the OOV rate, with a larger
reduction effect observed when the training set
is smaller. We next performed translation with
backoff systems trained on each data partition. In
each case, the combination weights for the indi-
</bodyText>
<table confidence="0.999586857142857">
German-English
dev set test set
train1 5.2/27.7 5.1/27.3
train2 2.0/11.7 2.0/11.6
train3 1.4/8.1 1.3/7.6
train4 0.5/3.1 0.5/2.9
train5 0.3/1.7 0.2/1.3
Finnish-English
dev set test set
train1 9.1/28.5 9.2/28.9
train2 3.8/12.4 3.7/12.3
train3 2.5/8.2 2.4/8.0
train4 0.9/3.2 0.9/3.0
train5 0.4/1.4 0.4/1.5
</table>
<tableCaption confidence="0.996739">
Table 3: OOV rates (%) on the development
</tableCaption>
<bodyText confidence="0.993206151515151">
and test sets under the backoff model (word
types/tokens).
vidual model scores were re-optimized. Table 4
shows the evaluation results on the dev set. Since
the BLEU score alone is often not a good indi-
cator of successful translations of unknown words
(the unigram or bigram precision may be increased
but may not have a strong effect on the over-
all BLEU score), position-independent word error
rate (PER) rate was measured as well. We see im-
provements in BLEU score and PERs in almost
all cases. Statistical significance was measured on
PER using a difference of proportions significance
test and on BLEU using a segment-level paired
t-test. PER improvements are significant almost
all training conditions for both languages; BLEU
improvements are significant in all conditions for
Finnish and for the two smallest training sets for
German. The effect on the overall development set
(consisting of both sentences with known words
only and sentences with unknown words) is shown
in Table 5. As expected, the impact on overall per-
formance is smaller, especially for larger training
data sets, due to the relatively small percentage of
OOV tokens (see Table 1). The evaluation results
for the test set are shown in Tables 6 (for the sub-
set of sentences with OOVs) and 7 (for the entire
test set), with similar conclusions.
The examples A and B in Figure 2 demon-
strate higher-scoring translations produced by the
backoff system as opposed to the baseline sys-
tem. An analysis of the backoff system output
showed that in some cases (e.g. examples C and
</bodyText>
<page confidence="0.997765">
45
</page>
<table confidence="0.999679294117647">
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 14.2 56.9 15.4 55.5
train2 16.3 55.2 17.3 51.8
train3 17.8 51.1 18.4 49.7
train4 19.6 51.1 19.9 47.6
train5 21.9 46.6 22.6 46.0
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
Set BLEU PER BLEU PER
train1 12.4 59.9 13.6 57.8
train2 13.0 61.2 13.9 59.1
train3 14.0 58.0 14.7 57.8
train4 17.4 52.7 18.4 50.8
train5 16.8 52.7 18.7 50.2
</table>
<tableCaption confidence="0.989916">
Table 4: BLEU (%) and position-independent
</tableCaption>
<bodyText confidence="0.809185">
word error rate (PER) on the subset of the devel-
opment data containing unknown words (second-
pass output). Here and in the following tables,
statistically significant differences to the baseline
model are shown in boldface (p &lt; 0.05).
</bodyText>
<table confidence="0.999646625">
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 15.3 56.4 16.3 55.1
train2 19.0 53.0 19.5 51.6
train3 20.0 49.9 20.5 49.3
train4 22.2 49.0 22.4 48.1
train5 24.6 46.5 24.7 45.6
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 13.1 59.3 14.4 57.4
train2 14.5 59.7 15.4 58.3
train3 16.0 56.5 16.5 56.5
train4 21.0 50.0 21.4 49.2
train5 22.2 50.5 22.5 49.7
</table>
<tableCaption confidence="0.990410333333333">
Table 5: BLEU (%) and position-independent
word error rate (PER) for the entire development
set.
</tableCaption>
<table confidence="0.9999381875">
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 14.3 56.2 15.5 55.1
train2 17.1 54.3 17.6 50.7
train3 17.4 50.8 18.1 49.7
train4 18.9 49.8 18.8 48.2
train5 19.1 46.3 19.4 46.2
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 12.4 59.5 13.5 57.5
train2 13.3 60.7 14.2 59.0
train3 14.1 58.2 15.1 57.3
train4 17.2 54.0 18.4 50.2
train5 16.6 51.8 19.0 49.4
</table>
<tableCaption confidence="0.916294">
Table 6: BLEU (%) and position-independent
word error rate (PER) for the test set (subset with
OOV words).
</tableCaption>
<bodyText confidence="0.998984793103448">
D in Figure 2), the backoff model produced a
good translation, but the translation was a para-
phrase rather than an identical match to the ref-
erence translation. Since only a single reference
translation is available for the Europarl data (pre-
venting the computation of a BLEU score based
on multiple hand-annotated references), good but
non-matching translations are not taken into ac-
count by our evaluation method. In other cases
the unknown word was translated correctly, but
since it was translated as single-word phrase the
segmentation of the entire sentence was affected.
This may cause greater distortion effects since the
sentence is segmented into a larger number of
smaller phrases, each of which can be reordered.
We therefore added the possibility of translating
an unknown word in its phrasal context by stem-
ming up to m words to the left and right in the
original sentence and finding translations for the
entire stemmed phrase (i.e. the function stem()
is now applied to the entire phrase). This step
is inserted before the stemming of a single word
f in the backoff model described above. How-
ever, since translations for entire stemmed phrases
were found only in about 1% of all cases, there
was no significant effect on the BLEU score. An-
other possibility of limiting reordering effects re-
sulting from single-word translations of OOVs is
to restrict the distortion limit of the decoder. Our
</bodyText>
<page confidence="0.998011">
46
</page>
<table confidence="0.999826875">
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 15.3 55.8 16.3 54.8
train2 19.4 52.3 19.6 50.9
train3 20.3 49.6 20.7 49.2
train4 22.5 48.1 22.5 47.9
train5 24.8 46.3 25.1 45.5
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 12.9 58.7 14.0 57.0
train2 14.5 59.5 15.3 58.4
train3 15.6 56.6 16.4 56.2
train4 20.6 50.3 21.0 49.6
train5 22.0 50.0 22.3 49.5
</table>
<tableCaption confidence="0.726890333333333">
Table 7: BLEU (%) and position-independent
word error rate (PER) for the test set (entire test
set).
</tableCaption>
<bodyText confidence="0.99960625">
experiments showed that this improves the BLEU
score slightly for both the baseline and the backoff
system; the relative difference, however, remained
the same.
</bodyText>
<sectionHeader confidence="0.999106" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.986923782608696">
We have presented a backoff model for phrase-
based SMT that uses morphological abstractions
to translate unseen word forms in the foreign lan-
guage input. When a match for an unknown word
in the test set cannot be found in the trained phrase
table, the model relies instead on translation prob-
abilities derived from stemmed or split versions
of the word in its phrasal context. An evalua-
tion of the model on German-English and Finnish-
English translations of parliamentary proceedings
showed statistically significant improvements in
PER for almost all training conditions and signifi-
cant improvements in BLEU when the training set
is small (100K words), with larger improvements
for Finnish than for German. This demonstrates
that our method is mainly relevant for highly in-
flected languages and sparse training data condi-
tions. It is also designed to improve human accep-
tance of machine translation output, which is par-
ticularly adversely affected by untranslated words.
Acknowledgments
This work was funded by NSF grant no. IIS-
0308297. We thank Ilona Pitk¨anen for help with
</bodyText>
<subsectionHeader confidence="0.901909">
Example A: (German-English):
</subsectionHeader>
<bodyText confidence="0.939080875">
SRC: wir sind berzeugt davon, dass ein europa des friedens
nicht durch milit¨arb¨undnisse geschaffen wird.
BASE: we are convinced that a europe of peace, not by
milit¨arb¨undnisse is created.
BACKOFF: we are convinced that a europe of peace, not
by military alliance is created.
REF: we are convinced that a europe of peace will not be
created through military alliances.
</bodyText>
<subsectionHeader confidence="0.30651">
Example B. (Finnish-English):
</subsectionHeader>
<bodyText confidence="0.180071">
SRC: arvoisa puhemies, puhuimme t¨a¨all¨a eilisiltana
serviasta ja siell¨a tapahtuvista vallankumouksellisista
muutoksista.
BASE: mr president, we talked about here last night, on
the subject of serbia and there, of vallankumouksellisista
changes.
BACKOFF: mr president, we talked about here last
night, on the subject of serbia and there, of revolutionary
changes.
REF: mr. president, last night we discussed the topic of
serbia and the revolutionary changes that are taking place
there.
</bodyText>
<subsectionHeader confidence="0.34955">
Example C. (Finnish-English):
</subsectionHeader>
<bodyText confidence="0.844806714285714">
SRC: toivon t¨alt¨a osin, ett¨a yhdistyneiden kansakuntien
alaisuudessa k¨ayt¨aviss¨a neuvotteluissa p¨a¨ast¨aisiin sell-
aiseen lopputulokseen, ett¨a kyproksen kreikkalainen ja
turkkilainen v¨aest¨onosa voisivat yhdess¨a nauttia liittymisen
mukanaan tuomista eduista yhdistetyss¨a tasavallassa.
BASE: i hope that the united nations in the negotiations
to reach a conclusion that the greek and turkish accession
to the benefi t of the benefits of the republic of ydistetyss¨a
brings together v¨aest¨onosa could, in this respect, under the
auspices.
BACKOFF: i hope that the united nations in the nego-
tiations to reach a conclusion that the greek and turkish
communities can work together to bring the benefits of the
accession of the republic of ydistetyss¨a. in this respect,
under the
REF: in this connection, i would hope that the talks
conducted under the auspices of the united nations will be
able to come to a successful conclusion enabling the greek
and turkish cypriot populations to enjoy the advantages
of membership of the european union in the context of a
reunifi ed republic.
</bodyText>
<subsectionHeader confidence="0.934718">
Example D. (German-English):
</subsectionHeader>
<bodyText confidence="0.967708090909091">
SRC:so sind wir beim durcharbeiten des textes verfahren,
wobei wir bei einer reihe von punkten versucht haben, noch
einige straffungen vorzunehmen.
BASE: we are in the durcharbeiten procedures of the text,
although we have tried to make a few straffungen to carry
out on a number of issues.
BACKOFF: we are in the durcharbeiten procedures, and
we have tried to make a few streamlining of the text in a
number of points.
REF: this is how we came to go through the text, and
attempted to cut down on certain items in the process.
</bodyText>
<figureCaption confidence="0.80262575">
Figure 2: Translation examples (SRC = source,
BASE = baseline system, BACKOFF = backoff
system, REF = reference). OOVs and their trans-
lation are marked in boldface.
</figureCaption>
<page confidence="0.997035">
47
</page>
<bodyText confidence="0.591253">
the Finnish language.
</bodyText>
<sectionHeader confidence="0.948454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999927">
J.A. Bilmes and K. Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 4–6, Edmonton, Canada.
S. Corston-Oliver and M. Gamon. 2004. Normaliz-
ing German and English inflectional morphology to
improve statistical word alignment. In Robert E.
Frederking and Kathryn Taylor, editors, Proceedings
of the Conference of the Association for Machine
Translation in the Americas, pages 48–57, Washing-
ton, DC.
A. de Gispert, J.B. Mari˜no, and J.M. Crego. 2005. Im-
proving statistical machine translation by classifying
and generalizing inflected verb forms. In Proceed-
ings of 9th European Conference on Speech Commu-
nication and Technology, pages 3193–3196, Lisboa,
Portugal.
A. Fraser and D. Marcu. 2005. ISI’s participation in
the Romanian-English alignment task. In Proceed-
ings of the 2005 ACL Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Transla-
tion and Beyond, pages 91–94, Ann Arbor, Michi-
gan.
D. Gildea. 2001. Statistical Language Understanding
Using Frame Semantics. Ph.D. thesis, University of
California, Berkeley, California.
S. Goldwater and D. McCloskey. 2005. Improving sta-
tistical MT through morphological analysis. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 676–683, Vancou-
ver, British Columbia, Canada.
P. Koehn and C. Monz. 2005. Shared task: statistical
machine translation between European languages.
In Proceedings of the 2005 ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 119–124, Ann Ar-
bor, Michigan.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. the-
sis, Information Sciences Institute, USC, Los Ange-
les, California.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Robert E. Frederking and Kathryn Taylor, editors,
Proceedings of the Conference of the Association for
Machine Translation in the Americas, pages 115–
124, Washington, DC.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of MT
Summit X, Phuket, Thailand.
C. Lioma and I. Ounis. 2005. Deploying part-of-
speech patterns to enhance statistical phrase-based
machine translation resources. In Proceedings of the
2005 ACL Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, pages 163–166, Ann Arbor, Michigan.
S. Niessen and H. Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine trans-
lation. In Proceedings of MT Summit VIII, Santiago
de Compostela, Galicia, Spain.
S. Niessen and H. Ney. 2001b. Toward hierar-
chical models for statistical machine translation of
inflected languages. In Proceedings of the ACL
2001 Workshop on Data-Driven Methods in Ma-
chine Translation, pages 47–54, Toulouse, France.
F.J. Och and H. Ney. 2000. Giza++:
Training of statistical translation mod-
els. http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
F.J. Och and H. Ney. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan.
P. Resnik, D. Oard, and G.A. Levow. 2001. Improved
cross-language retrieval using backoff translation.
In Proceedings of the First International Conference
on Human Language Technology Research, pages
153–155, San Diego, California.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing, pages 44–49, Manchester, UK.
C. Xi and R. Hwa. 2005. A backoff model for boot-
strapping resources for non-English languages. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 851–858, Van-
couver, British Columbia, Canada.
I. Zitouni, O. Siohan, and C.-H. Lee. 2003. Hierar-
chical class n-gram language models: towards bet-
ter estimation of unseen events in speech recogni-
tion. In Proceedings of 8th European Conference on
Speech Communication and Technology, pages 237–
240, Geneva, Switzerland.
</reference>
<page confidence="0.999354">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953071">
<title confidence="0.9968855">Phrase-Based Backoff Models for Machine Translation of Highly Inflected Languages</title>
<author confidence="0.999997">Mei Yang</author>
<affiliation confidence="0.9998675">Department of Electrical Engineering University of Washington</affiliation>
<address confidence="0.999578">Seattle, WA, USA</address>
<email confidence="0.999747">yangmei@ee.washington.edu</email>
<author confidence="0.998948">Katrin Kirchhoff</author>
<affiliation confidence="0.9998715">Department of Electrical Engineering University of Washington</affiliation>
<address confidence="0.9997">Seattle, WA, USA</address>
<email confidence="0.99985">katrin@ee.washington.edu</email>
<abstract confidence="0.996210727272727">We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and Finnish- English translation and shows improvements over state-of-the-art phrase-based models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J A Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>4--6</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7514" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="1147" endWordPosition="1151">s off to the next lower-order n-gram distribution. For the case of trigrams, this can be expressed as: pBO(wt|wt−1, wt−2) (2) JdcpML(wt|wt−1, wt−2) if c &gt; T α(wt−1,wt−2)pBO(wt|wt−1)oth where pML denotes the maximum-likelihood estimate, c denotes the count of the triple (wi, wi−1, wi−2) in the training data, T is the count threshold above which the maximum-likelihood estimate is retained, and dN(wi,wi−1,wi−2) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1, wi−2) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and = erwise 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard met</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J.A. Bilmes and K. Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 4–6, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
</authors>
<title>Normalizing German and English inflectional morphology to improve statistical word alignment.</title>
<date>2004</date>
<booktitle>Proceedings of the Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>48--57</pages>
<editor>In Robert E. Frederking and Kathryn Taylor, editors,</editor>
<location>Washington, DC.</location>
<contexts>
<context position="4050" citStr="Corston-Oliver and Gamon, 2004" startWordPosition="610" endWordPosition="613">n. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) inv</context>
</contexts>
<marker>Corston-Oliver, Gamon, 2004</marker>
<rawString>S. Corston-Oliver and M. Gamon. 2004. Normalizing German and English inflectional morphology to improve statistical word alignment. In Robert E. Frederking and Kathryn Taylor, editors, Proceedings of the Conference of the Association for Machine Translation in the Americas, pages 48–57, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A de Gispert</author>
<author>J B Mari˜no</author>
<author>J M Crego</author>
</authors>
<title>Improving statistical machine translation by classifying and generalizing inflected verb forms.</title>
<date>2005</date>
<booktitle>In Proceedings of 9th European Conference on Speech Communication and Technology,</booktitle>
<pages>3193--3196</pages>
<location>Lisboa, Portugal.</location>
<marker>de Gispert, Mari˜no, Crego, 2005</marker>
<rawString>A. de Gispert, J.B. Mari˜no, and J.M. Crego. 2005. Improving statistical machine translation by classifying and generalizing inflected verb forms. In Proceedings of 9th European Conference on Speech Communication and Technology, pages 3193–3196, Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>ISI’s participation in the Romanian-English alignment task.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</booktitle>
<pages>91--94</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="4285" citStr="Fraser and Marcu, 2005" startWordPosition="644" endWordPosition="647">ic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various morphological modifications for Czech-English translations: a subset of the vocabulary was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological t</context>
</contexts>
<marker>Fraser, Marcu, 2005</marker>
<rawString>A. Fraser and D. Marcu. 2005. ISI’s participation in the Romanian-English alignment task. In Proceedings of the 2005 ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, pages 91–94, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Statistical Language Understanding Using Frame Semantics.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley,</institution>
<location>California.</location>
<contexts>
<context position="7773" citStr="Gildea, 2001" startWordPosition="1191" endWordPosition="1192">, wi−1, wi−2) in the training data, T is the count threshold above which the maximum-likelihood estimate is retained, and dN(wi,wi−1,wi−2) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1, wi−2) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and = erwise 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular stemming and compound splitting, are interlea</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Statistical Language Understanding Using Frame Semantics. Ph.D. thesis, University of California, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>D McCloskey</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>676--683</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4646" citStr="Goldwater and McCloskey, 2005" startWordPosition="700" endWordPosition="703">n (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various morphological modifications for Czech-English translations: a subset of the vocabulary was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological tags were used as new word forms. Small improvements were found in combination with a word-to-word translation model. Most of these techniques have focused on improving word alignment or reducing vocabulary size; however, it is often the case that better word alignment does not improve the overall translation performance of a standard phrase-based SMT system. </context>
</contexts>
<marker>Goldwater, McCloskey, 2005</marker>
<rawString>S. Goldwater and D. McCloskey. 2005. Improving statistical MT through morphological analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 676–683, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Shared task: statistical machine translation between European languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</booktitle>
<pages>119--124</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="13887" citStr="Koehn and Monz, 2005" startWordPosition="2212" endWordPosition="2215">95.3% of them were decomposed correctly: for the domain at hand, most compounds need not be split into more than two parts; if one part is itself a compound it is usually frequent enough in the training data to have a translation. Furthermore, lexicalized compounds, whose decomposition would lead to wrong translations, are also typically frequent words and have an appropriate translation in the training data. 1http://snowball.tartarus.org 5 Data Our data consists of the Europarl training, development and test definitions for German-English and Finnish-English of the 2005 ACL shared data task (Koehn and Monz, 2005). Both German and Finnish are morphologically rich languages: German has four cases and three genders and shows number, gender and case distinctions not only on verbs, nouns, and adjectives, but also on determiners. In addition, it has notoriously many compounds. Finnish is a highly agglutinative language with a large number of inflectional paradigms (e.g. one for each of its 15 cases). Noun compounds are also frequent. On the 2005 ACL shared MT data task, Finnish to English translation showed the lowest average performance (17.9% BLEU) and German had the second lowest (21.9%), while the avera</context>
</contexts>
<marker>Koehn, Monz, 2005</marker>
<rawString>P. Koehn and C. Monz. 2005. Shared task: statistical machine translation between European languages. In Proceedings of the 2005 ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, pages 119–124, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Noun Phrase Translation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Information Sciences Institute,</institution>
<location>USC, Los Angeles, California.</location>
<contexts>
<context position="12254" citStr="Koehn, 2003" startWordPosition="1945" endWordPosition="1946">ount by which the word-based phrase translation probability is discounted. Equivalent probability computations are carried out for the lexical translation probabilities. Similar to the backoff level that uses stemming, the translation probabilities need to be recomputed for the levels that use splitting and combined splitting/stemming. In order to derive the morphological decomposition we use existing tools. For stemming we use the TreeTagger (Schmid, 1994) for German and the Snowball stemmer1 for Finnish. A variety of ways for compound splitting have been investigated in machine translation (Koehn, 2003). Here we use a simple technique that considers all possible ways of segmenting a word into two subparts (with a minimum-length constraint of three characters on each subpart). A segmentation is accepted if the subparts appear as individual items in the training data vocabulary. The only linguistic knowledge used in the segmentation process is the removal of final &lt;s&gt; from the first part of the compound before trying to match it to an existing word. This character (Fugen-s) is often inserted as “glue” when forming German compounds. Other glue characters were not considered for simplicity (but </context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, Information Sciences Institute, USC, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>Proceedings of the Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<editor>In Robert E. Frederking and Kathryn Taylor, editors,</editor>
<location>Washington, DC.</location>
<contexts>
<context position="10037" citStr="Koehn, 2004" startWordPosition="1584" endWordPosition="1585">wn word (further described below), i.e.(f00 i1, f00 i2) = split(fi). The match with the original word-based phrase table is then performed again. If this step fails for either of the two parts of f00, stemming is applied again: f 000 i1 = stem(f00 i1) and f000 i2 = stem(f00 i2), and a match with the stemmed phrase table entries is carried out. Only if the attempted match fails at this level is the input passed on verbatim in the translation output. The backoff procedure could in principle be performed on demand by a specialized decoder; however, since we use an off-the-shelf decoder (Pharaoh (Koehn, 2004)), backoff is implicitly enforced by providing a phrase-table that includes all required backoff levels and by preprocessing the test data accordingly. The phrase table will thus include entries for phrases based on full word forms as well as for their stemmed and/or split counterparts. For each entry with decomposed morphological forms, four probabilities need to be provided: two phrasal translation scores for both translation directions, p(¯e |¯f) and p(¯f|¯e), and two corresponding lexical scores, which are computed as a product of the word-by-word translation probabilities under the given </context>
<context position="15434" citStr="Koehn, 2004" startWordPosition="2469" endWordPosition="2470">nt and test sets consist of 2000 sentences each. In order to study the effect of varying amounts of training data we created several training partitions consisting of random selections of a subset of the full training set. The sizes of the partitions are shown in Table 1, together with the resulting percentage of out-of-vocabulary (OOV) words in the development and test sets (“type” refers to a unique word in the vocabulary, “token” to an instance in the actual text). 6 System We use a two-pass phrase-based statistical MT system using GIZA++ (Och and Ney, 2000) for word alignment and Pharaoh (Koehn, 2004) for phrase extraction and decoding. Word alignment is performed in both directions using the IBM4 model. Phrases are then extracted from the word alignments using the method described in (Och and Ney, 2003). For first-pass decoding we use Pharaoh in n-best mode. The decoder uses a weighted combination of seven scores: 4 translation model scores (phrase-based and lexical scores for both directions), a trigram language model score, a distortion score, and a word penalty. Nonmonotonic decoding is used, with no limit on the d¯e, f¯= 1 − p(¯e, stem( p(¯e, (6) f) f)) 44 German-English Set # sent # </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Robert E. Frederking and Kathryn Taylor, editors, Proceedings of the Conference of the Association for Machine Translation in the Americas, pages 115– 124, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="4068" citStr="Koehn, 2005" startWordPosition="615" endWordPosition="616">gy in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various m</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lioma</author>
<author>I Ounis</author>
</authors>
<title>Deploying part-ofspeech patterns to enhance statistical phrase-based machine translation resources.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</booktitle>
<pages>163--166</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="5373" citStr="Lioma and Ounis, 2005" startWordPosition="806" endWordPosition="809">ry was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological tags were used as new word forms. Small improvements were found in combination with a word-to-word translation model. Most of these techniques have focused on improving word alignment or reducing vocabulary size; however, it is often the case that better word alignment does not improve the overall translation performance of a standard phrase-based SMT system. Phrase-based models themselves have not benefited much from additional morpho-syntactic knowledge; e.g. (Lioma and Ounis, 2005) do not report any improvement from integrating part-ofspeech information at the phrase level. One successful application of morphological knowledge is (de Gispert et al., 2005), where knowledge-based morphological techniques are used to identify unseen verb forms in the test text and to generate inflected forms in the target language based on annotated POS tags and lemmas. Phrase prediction in the target language is conditioned on the phrase in the source language as well the corresponding tuple of lemmatized phrases. This technique worked well for translating from a morphologically poor lang</context>
</contexts>
<marker>Lioma, Ounis, 2005</marker>
<rawString>C. Lioma and I. Ounis. 2005. Deploying part-ofspeech patterns to enhance statistical phrase-based machine translation resources. In Proceedings of the 2005 ACL Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, pages 163–166, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>H Ney</author>
</authors>
<title>Morpho-syntactic analysis for reordering in statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of MT Summit VIII, Santiago de</booktitle>
<location>Compostela, Galicia,</location>
<contexts>
<context position="3641" citStr="Niessen and Ney, 2001" startWordPosition="549" endWordPosition="552">y-inflected languages, German and Finnish, into English and present improvements over a state-of-the-art system. The rest of the paper is structured as follows: The following section discusses related background work. Section 4 describes the proposed model; Sections 5 and 6 provide details about the data and baseline system used in this study. Section 7 provides experimental results and discussion. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languag</context>
</contexts>
<marker>Niessen, Ney, 2001</marker>
<rawString>S. Niessen and H. Ney. 2001a. Morpho-syntactic analysis for reordering in statistical machine translation. In Proceedings of MT Summit VIII, Santiago de Compostela, Galicia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>H Ney</author>
</authors>
<title>Toward hierarchical models for statistical machine translation of inflected languages.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation,</booktitle>
<pages>47--54</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3641" citStr="Niessen and Ney, 2001" startWordPosition="549" endWordPosition="552">y-inflected languages, German and Finnish, into English and present improvements over a state-of-the-art system. The rest of the paper is structured as follows: The following section discusses related background work. Section 4 describes the proposed model; Sections 5 and 6 provide details about the data and baseline system used in this study. Section 7 provides experimental results and discussion. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languag</context>
</contexts>
<marker>Niessen, Ney, 2001</marker>
<rawString>S. Niessen and H. Ney. 2001b. Toward hierarchical models for statistical machine translation of inflected languages. In Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation, pages 47–54, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<note>http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html.</note>
<contexts>
<context position="15389" citStr="Och and Ney, 2000" startWordPosition="2460" endWordPosition="2463">get language, or vice versa) was &gt; 9. The development and test sets consist of 2000 sentences each. In order to study the effect of varying amounts of training data we created several training partitions consisting of random selections of a subset of the full training set. The sizes of the partitions are shown in Table 1, together with the resulting percentage of out-of-vocabulary (OOV) words in the development and test sets (“type” refers to a unique word in the vocabulary, “token” to an instance in the actual text). 6 System We use a two-pass phrase-based statistical MT system using GIZA++ (Och and Ney, 2000) for word alignment and Pharaoh (Koehn, 2004) for phrase extraction and decoding. Word alignment is performed in both directions using the IBM4 model. Phrases are then extracted from the word alignments using the method described in (Och and Ney, 2003). For first-pass decoding we use Pharaoh in n-best mode. The decoder uses a weighted combination of seven scores: 4 translation model scores (phrase-based and lexical scores for both directions), a trigram language model score, a distortion score, and a word penalty. Nonmonotonic decoding is used, with no limit on the d¯e, f¯= 1 − p(¯e, stem( p(¯</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F.J. Och and H. Ney. 2000. Giza++: Training of statistical translation models. http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="15641" citStr="Och and Ney, 2003" startWordPosition="2501" endWordPosition="2504">he full training set. The sizes of the partitions are shown in Table 1, together with the resulting percentage of out-of-vocabulary (OOV) words in the development and test sets (“type” refers to a unique word in the vocabulary, “token” to an instance in the actual text). 6 System We use a two-pass phrase-based statistical MT system using GIZA++ (Och and Ney, 2000) for word alignment and Pharaoh (Koehn, 2004) for phrase extraction and decoding. Word alignment is performed in both directions using the IBM4 model. Phrases are then extracted from the word alignments using the method described in (Och and Ney, 2003). For first-pass decoding we use Pharaoh in n-best mode. The decoder uses a weighted combination of seven scores: 4 translation model scores (phrase-based and lexical scores for both directions), a trigram language model score, a distortion score, and a word penalty. Nonmonotonic decoding is used, with no limit on the d¯e, f¯= 1 − p(¯e, stem( p(¯e, (6) f) f)) 44 German-English Set # sent # words oov dev oov test train1 5K 101K 7.9/42.6 7.9/42.7 train2 25K 505K 3.8/22.1 3.7/21.9 train3 50K 1013K 2.7/16.1 2.7/16.1 train4 250K 5082K 1.3/8.1 1.2/7.5 train5 751K 15258K 0.8/4.9 0.7/4.4 Finnish-Engli</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>D Oard</author>
<author>G A Levow</author>
</authors>
<title>Improved cross-language retrieval using backoff translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research,</booktitle>
<pages>153--155</pages>
<location>San Diego, California.</location>
<contexts>
<context position="7823" citStr="Resnik et al., 2001" startWordPosition="1197" endWordPosition="1200">e count threshold above which the maximum-likelihood estimate is retained, and dN(wi,wi−1,wi−2) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1, wi−2) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and = erwise 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular stemming and compound splitting, are interleaved such that a more specific form (i.e. a form cl</context>
</contexts>
<marker>Resnik, Oard, Levow, 2001</marker>
<rawString>P. Resnik, D. Oard, and G.A. Levow. 2001. Improved cross-language retrieval using backoff translation. In Proceedings of the First International Conference on Human Language Technology Research, pages 153–155, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="12103" citStr="Schmid, 1994" startWordPosition="1920" endWordPosition="1921">rwise Figure 1: Backoff procedure. i, i2 i i, i, i2 i2 i i i i, i2 plex(¯e |¯f) = J 1 I j=1 a(i)=j |j|a(i) = j| p(¯e|stem( )) (4) f)) 43 where is the amount by which the word-based phrase translation probability is discounted. Equivalent probability computations are carried out for the lexical translation probabilities. Similar to the backoff level that uses stemming, the translation probabilities need to be recomputed for the levels that use splitting and combined splitting/stemming. In order to derive the morphological decomposition we use existing tools. For stemming we use the TreeTagger (Schmid, 1994) for German and the Snowball stemmer1 for Finnish. A variety of ways for compound splitting have been investigated in machine translation (Koehn, 2003). Here we use a simple technique that considers all possible ways of segmenting a word into two subparts (with a minimum-length constraint of three characters on each subpart). A segmentation is accepted if the subparts appear as individual items in the training data vocabulary. The only linguistic knowledge used in the segmentation process is the removal of final &lt;s&gt; from the first part of the compound before trying to match it to an existing w</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Xi</author>
<author>R Hwa</author>
</authors>
<title>A backoff model for bootstrapping resources for non-English languages.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>851--858</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="7933" citStr="Xi and Hwa, 2005" startWordPosition="1213" endWordPosition="1216">factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1, wi−2) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and = erwise 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular stemming and compound splitting, are interleaved such that a more specific form (i.e. a form closer to the full word form) is chosen before a more general form (i.e. a form that has undergone morphological</context>
</contexts>
<marker>Xi, Hwa, 2005</marker>
<rawString>C. Xi and R. Hwa. 2005. A backoff model for bootstrapping resources for non-English languages. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 851–858, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>O Siohan</author>
<author>C-H Lee</author>
</authors>
<title>Hierarchical class n-gram language models: towards better estimation of unseen events in speech recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of 8th European Conference on Speech Communication and Technology,</booktitle>
<pages>237--240</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7729" citStr="Zitouni et al., 2003" startWordPosition="1182" endWordPosition="1185">hood estimate, c denotes the count of the triple (wi, wi−1, wi−2) in the training data, T is the count threshold above which the maximum-likelihood estimate is retained, and dN(wi,wi−1,wi−2) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1, wi−2) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and = erwise 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular s</context>
</contexts>
<marker>Zitouni, Siohan, Lee, 2003</marker>
<rawString>I. Zitouni, O. Siohan, and C.-H. Lee. 2003. Hierarchical class n-gram language models: towards better estimation of unseen events in speech recognition. In Proceedings of 8th European Conference on Speech Communication and Technology, pages 237– 240, Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>